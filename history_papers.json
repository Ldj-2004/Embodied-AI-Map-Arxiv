{
  "卡内基-机器人研究所": [
    {
      "title": "Hierarchical Entity-centric Reinforcement Learning with Factored Subgoal Diffusion",
      "url": "https://arxiv.org/abs/2602.02722",
      "date": "2026-02-02",
      "authors_text": "Dan Haramati, Carl Qi, Tal Daniel, Amy Zhang, Aviv Tamar",
      "is_highlight": false,
      "score": 74.0,
      "summary": "This paper presents a hierarchical entity-centric framework for offline Goal-Conditioned Reinforcement Learning that enhances performance in multi-entity long-horizon tasks through modular subgoal generation.",
      "teaser_image": "https://arxiv.org/html/2602.02722/figs/alg1_illustration_blue.png",
      "debug_abstract": "We propose a hierarchical entity-centric framework for offline Goal-Conditioned Reinforcement Learning (GCRL) that combines subgoal decomposition with factored structure to solve long-horizon tasks in domains with multiple entities. Achieving long-horizon goals in complex environments remains a core challenge in Reinforcement Learning (RL). Domains with multiple entities are particularly difficult due to their combinatorial complexity. GCRL facilitates generalization across goals and the use of subgoal structure, but struggles with high-dimensional observations and combinatorial state-spaces, especially under sparse reward. We employ a two-level hierarchy composed of a value-based GCRL agent and a factored subgoal-generating conditional diffusion model. The RL agent and subgoal generator are trained independently and composed post hoc through selective subgoal generation based on the value function, making the approach modular and compatible with existing GCRL algorithms. We introduce new variations to benchmark tasks that highlight the challenges of multi-entity domains, and show that our method consistently boosts performance of the underlying RL agent on image-based long-horizon tasks with sparse rewards, achieving over 150% higher success rates on the hardest task in our suite and generalizing to increasing horizons and numbers of entities. Rollout videos are provided at: this https URL"
    },
    {
      "title": "Simulating Human Audiovisual Search Behavior",
      "url": "https://arxiv.org/abs/2602.02790",
      "date": "2026-02-02",
      "authors_text": "Hyunsung Cho, Xuejing Luo, Byungjoo Lee, David Lindlbauer, Antti Oulasvirta",
      "is_highlight": true,
      "score": 77.0,
      "summary": "The paper introduces Sensonaut, a computational model simulating human audiovisual search behavior by optimizing movement and sensory strategies under uncertainty to enhance target location efficiency.",
      "teaser_image": "https://arxiv.org/html/2602.02790/x2.png",
      "debug_abstract": "Locating a target based on auditory and visual cues$\\unicode{x2013}$such as finding a car in a crowded parking lot or identifying a speaker in a virtual meeting$\\unicode{x2013}$requires balancing effort, time, and accuracy under uncertainty. Existing models of audiovisual search often treat perception and action in isolation, overlooking how people adaptively coordinate movement and sensory strategies. We present Sensonaut, a computational model of embodied audiovisual search. The core assumption is that people deploy their body and sensory systems in ways they believe will most efficiently improve their chances of locating a target, trading off time and effort under perceptual constraints. Our model formulates this as a resource-rational decision-making problem under partial observability. We validate the model against newly collected human data, showing that it reproduces both adaptive scaling of search time and effort under task complexity, occlusion, and distraction, and characteristic human errors. Our simulation of human-like resource-rational search informs the design of audiovisual interfaces that minimize search cost and cognitive load."
    },
    {
      "title": "Mirage2Matter: A Physically Grounded Gaussian World Model from Video",
      "url": "https://arxiv.org/abs/2602.00096",
      "date": "2026-01-24",
      "authors_text": "Zhengqing Gao, Ziwen Li, Xin Wang, Jiaxin Huang, Zhenyang Ren",
      "is_highlight": false,
      "score": 85.0,
      "summary": "The paper presents Mirage2Matter, a framework that generates high-fidelity embodied training data from videos, enabling scalable and practical world modeling for embodied intelligence.",
      "teaser_image": "https://arxiv.org/html/2602.00096/figures/teaser.png",
      "debug_abstract": "The scalability of embodied intelligence is fundamentally constrained by the scarcity of real-world interaction data. While simulation platforms provide a promising alternative, existing approaches often suffer from a substantial visual and physical gap to real environments and rely on expensive sensors, precise robot calibration, or depth measurements, limiting their practicality at scale. We present Simulate Anything, a graphics-driven world modeling and simulation framework that enables efficient generation of high-fidelity embodied training data using only multi-view environment videos and off-the-shelf assets. Our approach reconstructs real-world environments into a photorealistic scene representation using 3D Gaussian Splatting (3DGS), seamlessly capturing fine-grained geometry and appearance from video. We then leverage generative models to recover a physically realistic representation and integrate it into a simulation environment via a precision calibration target, enabling accurate scale alignment between the reconstructed scene and the real world. Together, these components provide a unified, editable, and physically grounded world model. Vision Language Action (VLA) models trained on our simulated data achieve strong zero-shot performance on downstream tasks, matching or even surpassing results obtained with real-world data, highlighting the potential of reconstruction-driven world modeling for scalable and practical embodied intelligence training."
    },
    {
      "title": "Llama-3.1-FoundationAI-SecurityLLM-Reasoning-8B Technical Report",
      "url": "https://arxiv.org/abs/2601.21051",
      "date": "2026-01-28",
      "authors_text": "Zhuoran Yang, Ed Li, Jianliang He, Aman Priyanshu, Baturay Saglam",
      "is_highlight": false,
      "score": 70.0,
      "summary": "Foundation-Sec-8B-Reasoning is an open-source cybersecurity reasoning model that excels in specialized tasks while retaining strong general capabilities through innovative training methods.",
      "teaser_image": "https://arxiv.org/html/2601.21051/x2.png",
      "debug_abstract": "We present Foundation-Sec-8B-Reasoning, the first open-source native reasoning model for cybersecurity. Built upon our previously released Foundation-Sec-8B base model (derived from Llama-3.1-8B-Base), the model is trained through a two-stage process combining supervised fine-tuning (SFT) and reinforcement learning from verifiable rewards (RLVR). Our training leverages proprietary reasoning data spanning cybersecurity analysis, instruction-following, and mathematical reasoning. Evaluation across 10 cybersecurity benchmarks and 10 general-purpose benchmarks demonstrates performance competitive with significantly larger models on cybersecurity tasks while maintaining strong general capabilities. The model shows effective generalization on multi-hop reasoning tasks and strong safety performance when deployed with appropriate system prompts and guardrails. This work demonstrates that domain-specialized reasoning models can achieve strong performance on specialized tasks while maintaining broad general capabilities. We release the model publicly at this https URL."
    },
    {
      "title": "Large-Scale Continual Scheduling and Execution for Dynamic Distributed Satellite Constellation Observation Allocation",
      "url": "https://arxiv.org/abs/2601.06188",
      "date": "2026-01-26",
      "authors_text": "Itai Zilberstein, Steve Chien",
      "is_highlight": false,
      "score": 70.0,
      "summary": "This paper introduces the Dynamic Incremental Neighborhood Stochastic Search algorithm for efficient scheduling in large-scale satellite constellations, enhancing autonomy and performance in dynamic environments.",
      "debug_abstract": "The size and capabilities of Earth-observing satellite constellations are rapidly increasing. Leveraging distributed onboard control, we can enable novel time-sensitive measurements and responses. However, deploying autonomy to large multiagent satellite systems necessitates algorithms with efficient computation and communication. We tackle this challenge and propose new, online algorithms for large-scale dynamic distributed constraint optimization problems (DDCOP). We present the Dynamic Multi-Satellite Constellation Observation Scheduling Problem (DCOSP), a new formulation of DDCOPs that models integrated scheduling and execution. We construct an omniscient offline algorithm to compute the novel optimality condition of DCOSP and present the Dynamic Incremental Neighborhood Stochastic Search (D-NSS) algorithm, an incomplete online decomposition-based DDCOP approach. We show through simulation that D-NSS converges to near-optimal solutions and outperforms DDCOP baselines in terms of solution quality, computation time, and message volume. Our work forms the foundation of the largest in-space demonstration of distributed multiagent AI to date: the NASA FAME mission."
    },
    {
      "title": "How Does Delegation in Social Interaction Evolve Over Time? Navigation with a Robot for Blind People",
      "url": "https://arxiv.org/abs/2601.19851",
      "date": "2026-01-27",
      "authors_text": "Rayna Hata, Masaki Kuribayashi, Allan Wang, Hironobu Takagi, Chieko Asakawa",
      "is_highlight": false,
      "score": 83.0,
      "summary": "The study reveals that blind participants refined their navigation strategies and preferences for robot assistance through repeated interactions, highlighting the importance of adaptive design in assistive robotics.",
      "debug_abstract": "Autonomy and independent navigation are vital to daily life but remain challenging for individuals with blindness. Robotic systems can enhance mobility and confidence by providing intelligent navigation assistance. However, fully autonomous systems may reduce users&#39; sense of control, even when they wish to remain actively involved. Although collaboration between user and robot has been recognized as important, little is known about how perceptions of this relationship change with repeated use. We present a repeated exposure study with six blind participants who interacted with a navigation-assistive robot in a real-world museum. Participants completed tasks such as navigating crowds, approaching lines, and encountering obstacles. Findings show that participants refined their strategies over time, developing clearer preferences about when to rely on the robot versus act independently. This work provides insights into how strategies and preferences evolve with repeated interaction and offers design implications for robots that adapt to user needs over time."
    },
    {
      "title": "Sentipolis: Emotion-Aware Agents for Social Simulations",
      "url": "https://arxiv.org/abs/2601.18027",
      "date": "2026-01-25",
      "authors_text": "Chiyuan Fu, Lyuhao Chen, Yunze Xiao, Weihao Xuan, Carlos Busso",
      "is_highlight": false,
      "score": 72.0,
      "summary": "Sentipolis introduces emotionally stateful agents using PAD representation and memory coupling, enhancing emotional continuity and communication in social simulations while revealing model-dependent behavior dynamics.",
      "debug_abstract": "LLM agents are increasingly used for social simulation, yet emotion is often treated as a transient cue, causing emotional amnesia and weak long-horizon continuity. We present Sentipolis, a framework for emotionally stateful agents that integrates continuous Pleasure-Arousal-Dominance (PAD) representation, dual-speed emotion dynamics, and emotion--memory coupling. Across thousands of interactions over multiple base models and evaluators, Sentipolis improves emotionally grounded behavior, boosting communication, and emotional continuity. Gains are model-dependent: believability increases for higher-capacity models but can drop for smaller ones, and emotion-awareness can mildly reduce adherence to social norms, reflecting a human-like tension between emotion-driven behavior and rule compliance in social simulation. Network-level diagnostics show reciprocal, moderately clustered, and temporally stable relationship structures, supporting the study of cumulative social dynamics such as alliance formation and gradual relationship change."
    },
    {
      "title": "Constraint-Aware Discrete-Time PID Gain Optimization for Robotic Joint Control Under Actuator Saturation",
      "url": "https://arxiv.org/abs/2601.18639",
      "date": "2026-01-26",
      "authors_text": "Ojasva Mishra, Xiaolong Wu, Min Xu",
      "is_highlight": false,
      "score": 78.0,
      "summary": "This paper presents a robust optimization method for PID gain tuning in robotic joint control, addressing actuator saturation and discrete-time execution while enhancing performance and stability.",
      "debug_abstract": "The precise regulation of rotary actuation is fundamental in autonomous robotics, yet practical PID loops deviate from continuous-time theory due to discrete-time execution, actuator saturation, and small delays and measurement imperfections. We present an implementation-aware analysis and tuning workflow for saturated discrete-time joint control. We (i) derive PI stability regions under Euler and exact zero-order-hold (ZOH) discretizations using the Jury criterion, (ii) evaluate a discrete back-calculation anti-windup realization under saturation-dominant regimes, and (iii) propose a hybrid-certified Bayesian optimization workflow that screens analytically unstable candidates and behaviorally unsafe transients while optimizing a robust IAE objective with soft penalties on overshoot and saturation duty. Baseline sweeps ($\\tau=1.0$~s, $\\Delta t=0.01$~s, $u\\in[-10,10]$) quantify rise/settle trends for P/PI/PID. Under a randomized model family emulating uncertainty, delay, noise, quantization, and tighter saturation, robustness-oriented tuning improves median IAE from $0.843$ to $0.430$ while keeping median overshoot below $2\\%$. In simulation-only tuning, the certification screen rejects $11.6\\%$ of randomly sampled gains within bounds before full robust evaluation, improving sample efficiency without hardware experiments."
    },
    {
      "title": "POPE: Learning to Reason on Hard Problems via Privileged On-Policy Exploration",
      "url": "https://arxiv.org/abs/2601.18779",
      "date": "2026-01-26",
      "authors_text": "Yuxiao Qu, Amrith Setlur, Virginia Smith, Ruslan Salakhutdinov, Aviral Kumar",
      "is_highlight": false,
      "score": 59.0,
      "summary": "POPE enhances reinforcement learning by using oracle solutions to guide exploration in hard problems, improving solvability and performance on challenging reasoning tasks.",
      "debug_abstract": "Reinforcement learning (RL) has improved the reasoning abilities of large language models (LLMs), yet state-of-the-art methods still fail to learn on many training problems. On hard problems, on-policy RL rarely explores even a single correct rollout, yielding zero reward and no learning signal for driving improvement. We find that natural solutions to remedy this exploration problem from classical RL, such as entropy bonuses, more permissive clipping of the importance ratio, or direct optimization of pass@k objectives, do not resolve this issue and often destabilize optimization without improving solvability. A natural alternative is to leverage transfer from easier problems. However, we show that mixing easy and hard problems during RL training is counterproductive due to ray interference, where optimization focuses on already-solvable problems in a way that actively inhibits progress on harder ones. To address this challenge, we introduce Privileged On-Policy Exploration (POPE), an approach that leverages human- or other oracle solutions as privileged information to guide exploration on hard problems, unlike methods that use oracle solutions as training targets (e.g., off-policy RL methods or warmstarting from SFT). POPE augments hard problems with prefixes of oracle solutions, enabling RL to obtain non-zero rewards during guided rollouts. Crucially, the resulting behaviors transfer back to the original, unguided problems through a synergy between instruction-following and reasoning. Empirically, POPE expands the set of solvable problems and substantially improves performance on challenging reasoning benchmarks."
    },
    {
      "title": "Reuse your FLOPs: Scaling RL on Hard Problems by Conditioning on Very Off-Policy Prefixes",
      "url": "https://arxiv.org/abs/2601.18795",
      "date": "2026-01-26",
      "authors_text": "Amrith Setlur, Zijian Wang, Andrew Cohen, Paria Rashidinejad, Sang Michael Xie",
      "is_highlight": false,
      "score": 63.0,
      "summary": "PrefixRL enhances reinforcement learning efficiency on challenging problems by leveraging off-policy traces to improve learning stability and sample efficiency, achieving faster and superior performance.",
      "debug_abstract": "Typical reinforcement learning (RL) methods for LLM reasoning waste compute on hard problems, where correct on-policy traces are rare, policy gradients vanish, and learning stalls. To bootstrap more efficient RL, we consider reusing old sampling FLOPs (from prior inference or RL training) in the form of off-policy traces. Standard off-policy methods supervise against off-policy data, causing instabilities during RL optimization. We introduce PrefixRL, where we condition on the prefix of successful off-policy traces and run on-policy RL to complete them, side-stepping off-policy instabilities. PrefixRL boosts the learning signal on hard problems by modulating the difficulty of the problem through the off-policy prefix length. We prove that the PrefixRL objective is not only consistent with the standard RL objective but also more sample efficient. Empirically, we discover back-generalization: training only on prefixed problems generalizes to out-of-distribution unprefixed performance, with learned strategies often differing from those in the prefix. In our experiments, we source the off-policy traces by rejection sampling with the base model, creating a self-improvement loop. On hard reasoning problems, PrefixRL reaches the same training reward 2x faster than the strongest baseline (SFT on off-policy data then RL), even after accounting for the compute spent on the initial rejection sampling, and increases the final reward by 3x. The gains transfer to held-out benchmarks, and PrefixRL is still effective when off-policy traces are derived from a different model family, validating its flexibility in practical settings."
    },
    {
      "title": "Vision-as-Inverse-Graphics Agent via Interleaved Multimodal Reasoning",
      "url": "https://arxiv.org/abs/2601.11109",
      "date": "2026-01-22",
      "authors_text": "Shaofeng Yin, Jiaxin Ge, Zora Zhiruo Wang, Xiuyu Li, Michael J. Black",
      "is_highlight": false,
      "score": 88.9,
      "summary": "The paper presents VIGA, a Vision-as-Inverse-Graphics Agent that utilizes interleaved multimodal reasoning for effective scene reconstruction and editing, outperforming existing models significantly.",
      "debug_abstract": "Vision-as-inverse-graphics, the concept of reconstructing an image as an editable graphics program is a long-standing goal of computer vision. Yet even strong VLMs aren&#39;t able to achieve this in one-shot as they lack fine-grained spatial and physical grounding capability. Our key insight is that closing this gap requires interleaved multimodal reasoning through iterative execution and verification. Stemming from this, we present VIGA (Vision-as-Inverse-Graphic Agent) that starts from an empty world and reconstructs or edits scenes through a closed-loop write-run-render-compare-revise procedure. To support long-horizon reasoning, VIGA combines (i) a skill library that alternates generator and verifier roles and (ii) an evolving context memory that contains plans, code diffs, and render history. VIGA is task-agnostic as it doesn&#39;t require auxiliary modules, covering a wide range of tasks such as 3D reconstruction, multi-step scene editing, 4D physical interaction, and 2D document editing, etc. Empirically, we found VIGA substantially improves one-shot baselines on BlenderGym (35.32%) and SlideBench (117.17%). Moreover, VIGA is also model-agnostic as it doesn&#39;t require finetuning, enabling a unified protocol to evaluate heterogeneous foundation VLMs. To better support this protocol, we introduce BlenderBench, a challenging benchmark that stress-tests interleaved multimodal reasoning with graphics engine, where VIGA improves by 124.70%."
    },
    {
      "title": "Airflow Source Seeking on Small Quadrotors Using a Single Flow Sensor",
      "url": "https://arxiv.org/abs/2601.15607",
      "date": "2026-01-22",
      "authors_text": "Lenworth Thomas, Tjaden Bridges, Sarah Bergbreiter",
      "is_highlight": false,
      "score": 72.3,
      "summary": "This paper presents a novel airflow source-seeking method for small quadrotors using a custom flow sensor, enhancing plume tracking capabilities in confined environments.",
      "debug_abstract": "As environmental disasters happen more frequently and severely, seeking the source of pollutants or harmful particulates using plume tracking becomes even more important. Plume tracking on small quadrotors would allow these systems to operate around humans and fly in more confined spaces, but can be challenging due to poor sensitivity and long response times from gas sensors that fit on small quadrotors. In this work, we present an approach to complement chemical plume tracking with airflow source-seeking behavior using a custom flow sensor that can sense both airflow magnitude and direction on small quadrotors &lt; 100 g. We use this sensor to implement a modified version of the `Cast and Surge&#39; algorithm that takes advantage of flow direction sensing to find and navigate towards flow sources. A series of characterization experiments verified that the system can detect airflow while in flight and reorient the quadrotor toward the airflow. Several trials with random starting locations and orientations were used to show that our source-seeking algorithm can reliably find a flow source. This work aims to provide a foundation for future platforms that can use flow sensors in concert with other sensors to enable richer plume tracking data collection and source-seeking."
    }
  ],
  "NUS AI LAB": [
    {
      "title": "FARTrack: Fast Autoregressive Visual Tracking with High Performance",
      "url": "https://arxiv.org/abs/2602.03214",
      "date": "2026-02-03",
      "authors_text": "Guijie Wang, Tong Lin, Yifan Bai, Anjia Cao, Shiyi Liang",
      "is_highlight": false,
      "score": 79.0,
      "summary": "FARTrack is a fast autoregressive visual tracking framework that enhances inference speed and performance through task-specific self-distillation and inter-frame autoregressive sparsification, achieving 343 FPS on GPU.",
      "teaser_image": "https://arxiv.org/html/2602.03214/x1.png",
      "debug_abstract": "Inference speed and tracking performance are two critical evaluation metrics in the field of visual tracking. However, high-performance trackers often suffer from slow processing speeds, making them impractical for deployment on resource-constrained devices. To alleviate this issue, we propose FARTrack, a Fast Auto-Regressive Tracking framework. Since autoregression emphasizes the temporal nature of the trajectory sequence, it can maintain high performance while achieving efficient execution across various devices. FARTrack introduces Task-Specific Self-Distillation and Inter-frame Autoregressive Sparsification, designed from the perspectives of shallow-yet-accurate distillation and redundant-to-essential token optimization, respectively. Task-Specific Self-Distillation achieves model compression by distilling task-specific tokens layer by layer, enhancing the model&#39;s inference speed while avoiding suboptimal manual teacher-student layer pairs assignments. Meanwhile, Inter-frame Autoregressive Sparsification sequentially condenses multiple templates, avoiding additional runtime overhead while learning a temporally-global optimal sparsification strategy. FARTrack demonstrates outstanding speed and competitive performance. It delivers an AO of 70.6% on GOT-10k in real-time. Beyond, our fastest model achieves a speed of 343 FPS on the GPU and 121 FPS on the CPU."
    },
    {
      "title": "PWAVEP: Purifying Imperceptible Adversarial Perturbations in 3D Point Clouds via Spectral Graph Wavelets",
      "url": "https://arxiv.org/abs/2602.03333",
      "date": "2026-02-03",
      "authors_text": "Haoran Li, Renyang Liu, Hongjia Liu, Chen Wang, Long Yin",
      "is_highlight": false,
      "score": 59.0,
      "summary": "PWAVEP introduces a non-invasive defense mechanism using spectral graph wavelets to purify adversarial perturbations in 3D point clouds, enhancing accuracy and robustness.",
      "teaser_image": "https://arxiv.org/html/2602.03333/x1.png",
      "debug_abstract": "Recent progress in adversarial attacks on 3D point clouds, particularly in achieving spatial imperceptibility and high attack performance, presents significant challenges for defenders. Current defensive approaches remain cumbersome, often requiring invasive model modifications, expensive training procedures or auxiliary data access. To address these threats, in this paper, we propose a plug-and-play and non-invasive defense mechanism in the spectral domain, grounded in a theoretical and empirical analysis of the relationship between imperceptible perturbations and high-frequency spectral components. Building upon these insights, we introduce a novel purification framework, termed PWAVEP, which begins by computing a spectral graph wavelet domain saliency score and local sparsity score for each point. Guided by these values, PWAVEP adopts a hierarchical strategy, it eliminates the most salient points, which are identified as hardly recoverable adversarial outliers. Simultaneously, it applies a spectral filtering process to a broader set of moderately salient points. This process leverages a graph wavelet transform to attenuate high-frequency coefficients associated with the targeted points, thereby effectively suppressing adversarial noise. Extensive evaluations demonstrate that the proposed PWAVEP achieves superior accuracy and robustness compared to existing approaches, advancing the state-of-the-art in 3D point cloud purification. Code and datasets are available at this https URL"
    },
    {
      "title": "Inject Once Survive Later: Backdooring Vision-Language-Action Models to Persist Through Downstream Fine-tuning",
      "url": "https://arxiv.org/abs/2602.00500",
      "date": "2026-01-31",
      "authors_text": "Jianyi Zhou, Yujie Wei, Ruichen Zhen, Bo Zhao, Xiaobo Xia",
      "is_highlight": true,
      "score": 60.0,
      "summary": "The paper introduces INFUSE, a robust backdoor attack framework for Vision-Language-Action models that survives user fine-tuning, maintaining high attack success rates while preserving performance.",
      "teaser_image": "https://arxiv.org/html/2602.00500/x2.png",
      "debug_abstract": "Vision-Language-Action (VLA) models have become foundational to modern embodied AI systems. By integrating visual perception, language understanding, and action planning, they enable general-purpose task execution across diverse environments. Despite their importance, the security of VLA models remains underexplored -- particularly in the context of backdoor attacks, which pose realistic threats in physical-world deployments. While recent methods attempt to inject backdoors into VLA models, these backdoors are easily erased during downstream adaptation, as user-side fine-tuning with clean data significantly alters model parameters, rendering them impractical for real-world applications. To address these challenges, we propose INFUSE (INjection into Fine-tUne-inSensitive modulEs), the first backdoor attack framework for VLA base models that remains effective even with arbitrary user fine-tuning. INFUSE begins by analyzing parameter sensitivity across diverse fine-tuning scenarios to identify modules that remain largely unchanged -- the fine-tune-insensitive modules. It then injects backdoors into these stable modules while freezing the rest, ensuring malicious behavior persists after extensive user fine-tuning. Comprehensive experiments across multiple VLA architectures demonstrate INFUSE&#39;s effectiveness. After user-side fine-tuning, INFUSE maintains mean attack success rates of 91.0% on simulation environments and 79.8% on real-world robot tasks, substantially surpassing BadVLA (38.8% and 36.6%, respectively), while preserving clean-task performance comparable to standard models. These results uncover a critical threat: backdoors implanted before distribution can persist through fine-tuning and remain effective at deployment."
    },
    {
      "title": "APEX: A Decoupled Memory-based Explorer for Asynchronous Aerial Object Goal Navigation",
      "url": "https://arxiv.org/abs/2602.00551",
      "date": "2026-01-31",
      "authors_text": "Daoxuan Zhang, Ping Chen, Xiaobo Xia, Xiu Su, Ruichen Zhen",
      "is_highlight": false,
      "score": 88.0,
      "summary": "APEX is a novel hierarchical agent that enhances UAV navigation through efficient exploration and target acquisition using dynamic memory, reinforcement learning, and open-vocabulary detection.",
      "teaser_image": "https://arxiv.org/html/2602.00551/x1.png",
      "debug_abstract": "Aerial Object Goal Navigation, a challenging frontier in Embodied AI, requires an Unmanned Aerial Vehicle (UAV) agent to autonomously explore, reason, and identify a specific target using only visual perception and language description. However, existing methods struggle with the memorization of complex spatial representations in aerial environments, reliable and interpretable action decision-making, and inefficient exploration and information gathering. To address these challenges, we introduce \\textbf{APEX} (Aerial Parallel Explorer), a novel hierarchical agent designed for efficient exploration and target acquisition in complex aerial settings. APEX is built upon a modular, three-part architecture: 1) Dynamic Spatio-Semantic Mapping Memory, which leverages the zero-shot capability of a Vision-Language Model (VLM) to dynamically construct high-resolution 3D Attraction, Exploration, and Obstacle maps, serving as an interpretable memory mechanism. 2) Action Decision Module, trained with reinforcement learning, which translates this rich spatial understanding into a fine-grained and robust control policy. 3) Target Grounding Module, which employs an open-vocabulary detector to achieve definitive and generalizable target identification. All these components are integrated into a hierarchical, asynchronous, and parallel framework, effectively bypassing the VLM&#39;s inference latency and boosting the agent&#39;s proactivity in exploration. Extensive experiments show that APEX outperforms the previous state of the art by +4.2\\% SR and +2.8\\% SPL on challenging UAV-ON benchmarks, demonstrating its superior efficiency and the effectiveness of its hierarchical asynchronous design. Our source code is provided in \\href{this https URL}{GitHub}"
    },
    {
      "title": "Self-Guard: Defending Large Reasoning Models via enhanced self-reflection",
      "url": "https://arxiv.org/abs/2602.00707",
      "date": "2026-01-31",
      "authors_text": "Jingnan Zheng, Jingjun Xu, Yanzhen Luo, Chenhang Cui, Gelei Deng",
      "is_highlight": false,
      "score": 50.0,
      "summary": "Self-Guard is a lightweight framework enhancing safety compliance in Large Reasoning Models by promoting self-reflection and mitigating risks without sacrificing model utility.",
      "teaser_image": "https://arxiv.org/html/2602.00707/materials/workflow.png",
      "debug_abstract": "The emergence of Large Reasoning Models (LRMs) introduces a new paradigm of explicit reasoning, enabling remarkable advances yet posing unique risks such as reasoning manipulation and information leakage. To mitigate these risks, current alignment strategies predominantly rely on heavy post-training paradigms or external interventions. However, these approaches are often computationally intensive and fail to address the inherent awareness-compliance gap, a critical misalignment where models recognize potential risks yet prioritize following user instructions due to their sycophantic tendencies. To address these limitations, we propose Self-Guard, a lightweight safety defense framework that reinforces safety compliance at the representational level. Self-Guard operates through two principal stages: (1) safety-oriented prompting, which activates the model&#39;s latent safety awareness to evoke spontaneous reflection, and (2) safety activation steering, which extracts the resulting directional shift in the hidden state space and amplifies it to ensure that safety compliance prevails over sycophancy during inference. Experiments demonstrate that Self-Guard effectively bridges the awareness-compliance gap, achieving robust safety performance without compromising model utility. Furthermore, Self-Guard exhibits strong generalization across diverse unseen risks and varying model scales, offering a cost-efficient solution for LRM safety alignment."
    },
    {
      "title": "Unveiling the Cognitive Compass: Theory-of-Mind-Guided Multimodal Emotion Reasoning",
      "url": "https://arxiv.org/abs/2602.00971",
      "date": "2026-02-01",
      "authors_text": "Meng Luo, Bobo Li, Shanqing Xu, Shize Zhang, Qiuchan Chen",
      "is_highlight": false,
      "score": 62.0,
      "summary": "The paper presents HitEmotion, a benchmark for assessing and enhancing multimodal large language models' emotional reasoning through Theory of Mind-guided methods and reinforcement learning.",
      "teaser_image": "https://arxiv.org/html/2602.00971/figures/teaser.png",
      "debug_abstract": "Despite rapid progress in multimodal large language models (MLLMs), their capability for deep emotional understanding remains limited. We argue that genuine affective intelligence requires explicit modeling of Theory of Mind (ToM), the cognitive substrate from which emotions arise. To this end, we introduce HitEmotion, a ToM-grounded hierarchical benchmark that diagnoses capability breakpoints across increasing levels of cognitive depth. Second, we propose a ToM-guided reasoning chain that tracks mental states and calibrates cross-modal evidence to achieve faithful emotional reasoning. We further introduce TMPO, a reinforcement learning method that uses intermediate mental states as process-level supervision to guide and strengthen model reasoning. Extensive experiments show that HitEmotion exposes deep emotional reasoning deficits in state-of-the-art models, especially on cognitively demanding tasks. In evaluation, the ToM-guided reasoning chain and TMPO improve end-task accuracy and yield more faithful, more coherent rationales. In conclusion, our work provides the research community with a practical toolkit for evaluating and enhancing the cognition-based emotional understanding capabilities of MLLMs. Our dataset and code are available at: this https URL."
    },
    {
      "title": "Do All Individual Layers Help? An Empirical Study of Task-Interfering Layers in Vision-Language Models",
      "url": "https://arxiv.org/abs/2602.01167",
      "date": "2026-02-01",
      "authors_text": "Zhiming Liu, Yujie Wei, Lei Feng, Xiu Su, Xiaobo Xia",
      "is_highlight": false,
      "score": 52.0,
      "summary": "This study identifies Task-Interfering Layers in vision-language models, demonstrating that selectively bypassing certain layers can enhance task performance without retraining.",
      "teaser_image": "https://arxiv.org/html/2602.01167/x1.png",
      "debug_abstract": "Current VLMs have demonstrated capabilities across a wide range of multimodal tasks. Typically, in a pretrained VLM, all layers are engaged by default to make predictions on downstream tasks. We find that intervening on a single layer, such as by zeroing its parameters, can improve the performance on certain tasks, indicating that some layers hinder rather than help downstream tasks. We systematically investigate how individual layers influence different tasks via layer intervention. Specifically, we measure the change in performance relative to the base model after intervening on each layer and observe improvements when bypassing specific layers. This improvement can be generalizable across models and datasets, indicating the presence of Task-Interfering Layers that harm downstream tasks&#39; performance. We introduce Task-Layer Interaction Vector, which quantifies the effect of intervening on each layer of a VLM given a task. These task-interfering layers exhibit task-specific sensitivity patterns: tasks requiring similar capabilities show consistent response trends under layer interventions, as evidenced by the high similarity in their task-layer interaction vectors. Inspired by these findings, we propose TaLo (Task-Adaptive Layer Knockout), a training-free, test-time adaptation method that dynamically identifies and bypasses the most interfering layer for a given task. Without parameter updates, TaLo improves performance across various models and datasets, including boosting Qwen-VL&#39;s accuracy on the Maps task in ScienceQA by up to 16.6%. Our work reveals an unexpected form of modularity in pretrained VLMs and provides a plug-and-play, training-free mechanism to unlock hidden capabilities at inference time. The source code will be publicly available."
    },
    {
      "title": "PRISM: Festina Lente Proactivity -- Risk-Sensitive, Uncertainty-Aware Deliberation for Proactive Agents",
      "url": "https://arxiv.org/abs/2602.01532",
      "date": "2026-02-02",
      "authors_text": "Yuxuan Fu, Xiaoyu Tan, Teqi Hao, Chen Zhan, Xihe Qiu",
      "is_highlight": false,
      "score": 45.0,
      "summary": "PRISM introduces a decision-theoretic framework for proactive agents that optimizes intervention timing and accuracy, significantly reducing false alarms and enhancing performance on ProactiveBench.",
      "teaser_image": "https://arxiv.org/html/2602.01532/fig2.png",
      "debug_abstract": "Proactive agents must decide not only what to say but also whether and when to intervene. Many current systems rely on brittle heuristics or indiscriminate long reasoning, which offers little control over the benefit-burden tradeoff. We formulate the problem as cost-sensitive selective intervention and present PRISM, a novel framework that couples a decision-theoretic gate with a dual-process reasoning architecture. At inference time, the agent intervenes only when a calibrated probability of user acceptance exceeds a threshold derived from asymmetric costs of missed help and false alarms. Inspired by festina lente (Latin: &#34;make haste slowly&#34;), we gate by an acceptance-calibrated, cost-derived threshold and invoke a resource-intensive Slow mode with counterfactual checks only near the decision boundary, concentrating computation on ambiguous and high-stakes cases. Training uses gate-aligned, schema-locked distillation: a teacher running the full PRISM pipeline provides dense, executable supervision on unlabeled interaction traces, while the student learns a response policy that is explicitly decoupled from the intervention gate to enable tunable and auditable control. On ProactiveBench, PRISM reduces false alarms by 22.78% and improves F1 by 20.14% over strong baselines. These results show that principled decision-theoretic gating, paired with selective slow reasoning and aligned distillation, yields proactive agents that are precise, computationally efficient, and controllable. To facilitate reproducibility, we release our code, models, and resources at this https URL all experiments use the open-source ProactiveBench benchmark."
    },
    {
      "title": "ReCALL: Recalibrating Capability Degradation for MLLM-based Composed Image Retrieval",
      "url": "https://arxiv.org/abs/2602.01639",
      "date": "2026-02-02",
      "authors_text": "Tianyu Yang, ChenWei He, Xiangzhao Hao, Tianyue Wang, Jiarui Guo",
      "is_highlight": false,
      "score": 42.0,
      "summary": "ReCALL addresses capability degradation in MLLM-based Composed Image Retrieval by diagnosing issues, generating corrective instructions, and refining the retriever through continual training.",
      "teaser_image": "https://arxiv.org/html/2602.01639/x1.png",
      "debug_abstract": "Composed Image Retrieval (CIR) aims to retrieve target images based on a hybrid query comprising a reference image and a modification text. Early dual-tower Vision-Language Models (VLMs) struggle with cross-modality compositional reasoning required for this task. Recently, adapting generative Multimodal Large Language Models (MLLMs) for retrieval offers a promising direction. However, we identify that this adaptation strategy overlooks a fundamental issue: adapting a generative MLLM into a single-embedding discriminative retriever triggers a paradigm conflict, which leads to Capability Degradation - the deterioration of native fine-grained reasoning after retrieval adaptation. To address this challenge, we propose ReCALL (Recalibrating Capability Degradation), a model-agnostic framework that follows a diagnose-generate-refine pipeline: Firstly, we diagnose cognitive blind spots of the retriever via self-guided informative instance mining. Next, we generate corrective instructions and triplets by CoT prompting the foundation MLLM and conduct quality control with VQA-based consistency filtering. Finally, we refine the retriever through continual training on these triplets with a grouped contrastive scheme, thereby internalizing fine-grained visual-semantic distinctions and realigning the discriminative embedding space of retriever with intrinsic compositional reasoning within the MLLM. Extensive experiments on CIRR and FashionIQ show that ReCALL consistently recalibrates degraded capabilities and achieves state-of-the-art performance. Code will be released soon."
    },
    {
      "title": "Beyond Precision: Training-Inference Mismatch is an Optimization Problem and Simple LR Scheduling Fixes It",
      "url": "https://arxiv.org/abs/2602.01826",
      "date": "2026-02-02",
      "authors_text": "Yaxiang Zhang, Yingru Li, Jiacai Liu, Jiawei Xu, Ziniu Li",
      "is_highlight": false,
      "score": 35.0,
      "summary": "This paper presents a dynamic learning rate scheduling method to mitigate training-inference mismatch and stabilize reinforcement learning in large language models by addressing gradient noise.",
      "teaser_image": "https://arxiv.org/html/2602.01826/Qwen3-4B_baseline_vs_importance_sampling_patches.png",
      "debug_abstract": "Reinforcement Learning (RL) for training Large Language Models is notoriously unstable. While recent studies attribute this to &#34;training inference mismatch stemming&#34; from inconsistent hybrid engines, standard remedies, such as Importance Sampling, might fail during extended training runs. In this work, we analyze this instability through the lens of optimization, demonstrating that gradient noise and training-inference mismatch escalate in tandem as training progresses. Meanwhile, we find that the mismatch can be effectively suppressed by shrinking the update size. Taken together, we deduce that the mismatch is not merely a static numerical discrepancy, but a dynamic failure coupled with the model&#39;s optimization. Based on this insight, we propose a simple yet effective solution: a specialized Learning Rate (LR) scheduler. Instead of pre-defined decay schedule in traditional LR scheduler, our method dynamically triggers LR decay based on response length, which we identify as a reliable early-warning signal for impending instability. Empirical evidence suggests that by reducing the learning rate as gradient noise rises, we can consistently stabilize RL training and keep the training-inference mismatch at a safe level."
    },
    {
      "title": "UrbanGS: A Scalable and Efficient Architecture for Geometrically Accurate Large-Scene Reconstruction",
      "url": "https://arxiv.org/abs/2602.02089",
      "date": "2026-02-02",
      "authors_text": "Changbai Li, Haodong Zhu, Hanlin Chen, Xiuping Liang, Tongfei Chen",
      "is_highlight": false,
      "score": 22.0,
      "summary": "UrbanGS is a scalable framework that enhances geometric accuracy and memory efficiency in large-scale urban scene reconstruction through innovative regularization and adaptive Gaussian pruning techniques.",
      "teaser_image": "https://arxiv.org/html/2602.02089/figure/main.png",
      "debug_abstract": "While 3D Gaussian Splatting (3DGS) enables high-quality, real-time rendering for bounded scenes, its extension to large-scale urban environments gives rise to critical challenges in terms of geometric consistency, memory efficiency, and computational scalability. To address these issues, we present UrbanGS, a scalable reconstruction framework that effectively tackles these challenges for city-scale applications. First, we propose a Depth-Consistent D-Normal Regularization module. Unlike existing approaches that rely solely on monocular normal estimators, which can effectively update rotation parameters yet struggle to update position parameters, our method integrates D-Normal constraints with external depth supervision. This allows for comprehensive updates of all geometric parameters. By further incorporating an adaptive confidence weighting mechanism based on gradient consistency and inverse depth deviation, our approach significantly enhances multi-view depth alignment and geometric coherence, which effectively resolves the issue of geometric accuracy in complex large-scale scenes. To improve scalability, we introduce a Spatially Adaptive Gaussian Pruning (SAGP) strategy, which dynamically adjusts Gaussian density based on local geometric complexity and visibility to reduce redundancy. Additionally, a unified partitioning and view assignment scheme is designed to eliminate boundary artifacts and optimize computational load. Extensive experiments on multiple urban datasets demonstrate that UrbanGS achieves superior performance in rendering quality, geometric accuracy, and memory efficiency, providing a systematic solution for high-fidelity large-scale scene reconstruction."
    },
    {
      "title": "TIDE: Trajectory-based Diagnostic Evaluation of Test-Time Improvement in LLM Agents",
      "url": "https://arxiv.org/abs/2602.02196",
      "date": "2026-02-02",
      "authors_text": "Hang Yan, Xinyu Che, Fangzhi Xu, Qiushi Sun, Zichen Ding",
      "is_highlight": false,
      "score": 20.0,
      "summary": "The TIDE framework evaluates Test-Time Improvement in LLM agents by analyzing task completion dynamics, recursive behaviors, and memory constraints to enhance performance optimization.",
      "teaser_image": "https://arxiv.org/html/2602.02196/x1.png",
      "debug_abstract": "Recent advances in autonomous LLM agents demonstrate their ability to improve performance through iterative interaction with the environment. We define this paradigm as Test-Time Improvement (TTI). However, the mechanisms under how and why TTI succeed or fail remain poorly understood, and existing evaluation metrics fail to capture their task optimization efficiency, behavior adaptation after erroneous actions, and the specific utility of working memory for task completion. To address these gaps, we propose Test-time Improvement Diagnostic Evaluation (TIDE), an agent-agnostic and environment-agnostic framework that decomposes TTI into three comprehensive and interconnected dimensions. The framework measures (1) the overall temporal dynamics of task completion and (2) identifies whether performance is primarily constrained by recursive looping behaviors or (3) by burdensome accumulated memory. Through extensive experiments across diverse agents and environments, TIDE highlights that improving agent performance requires more than scaling internal reasoning, calling for explicitly optimizing the interaction dynamics between the agent and the environment."
    },
    {
      "title": "A Low-Cost Vision-Based Tactile Gripper with Pretraining Learning for Contact-Rich Manipulation",
      "url": "https://arxiv.org/abs/2602.00514",
      "date": "2026-01-31",
      "authors_text": "Yaohua Liu, Binkai Ou, Zicheng Qiu, Ce Hao, Yemin Wang",
      "is_highlight": false,
      "score": 11.0,
      "summary": "The LVTG gripper enhances contact-rich robotic manipulation through low-cost, durable visuo-tactile integration and improved policy learning via contrastive pretraining, achieving higher success rates.",
      "teaser_image": "https://arxiv.org/html/2602.00514/x1.png",
      "debug_abstract": "Robotic manipulation in contact-rich environments remains challenging, particularly when relying on conventional tactile sensors that suffer from limited sensing range, reliability, and cost-effectiveness. In this work, we present LVTG, a low-cost visuo-tactile gripper designed for stable, robust, and efficient physical interaction. Unlike existing visuo-tactile sensors, LVTG enables more effective and stable grasping of larger and heavier everyday objects, thanks to its enhanced tactile sensing area and greater opening angle. Its surface skin is made of highly wear-resistant material, significantly improving durability and extending operational lifespan. The integration of vision and tactile feedback allows LVTG to provide rich, high-fidelity sensory data, facilitating reliable perception during complex manipulation tasks. Furthermore, LVTG features a modular design that supports rapid maintenance and replacement. To effectively fuse vision and touch, We adopt a CLIP-inspired contrastive learning objective to align tactile embeddings with their corresponding visual observations, enabling a shared cross-modal representation space for visuo-tactile perception. This alignment improves the performance of an Action Chunking Transformer (ACT) policy in contact-rich manipulation, leading to more efficient data collection and more effective policy learning. Compared to the original ACT method, the proposed LVTG with pretraining achieves significantly higher success rates in manipulation tasks."
    },
    {
      "title": "FD-VLA: Force-Distilled Vision-Language-Action Model for Contact-Rich Manipulation",
      "url": "https://arxiv.org/abs/2602.02142",
      "date": "2026-02-02",
      "authors_text": "Ruiteng Zhao, Wenshuo Wang, Yicheng Ma, Xiaocong Li, Francis E. H. Tay",
      "is_highlight": false,
      "score": 0,
      "summary": "The FD-VLA framework enhances contact-rich manipulation by integrating force awareness through a novel distillation method, improving performance without physical force sensors.",
      "teaser_image": "https://arxiv.org/html/2602.02142/x1.png",
      "debug_abstract": "Force sensing is a crucial modality for Vision-Language-Action (VLA) frameworks, as it enables fine-grained perception and dexterous manipulation in contact-rich tasks. We present Force-Distilled VLA (FD-VLA), a novel framework that integrates force awareness into contact-rich manipulation without relying on physical force sensors. The core of our approach is a Force Distillation Module (FDM), which distills force by mapping a learnable query token, conditioned on visual observations and robot states, into a predicted force token aligned with the latent representation of actual force signals. During inference, this distilled force token is injected into the pretrained VLM, enabling force-aware reasoning while preserving the integrity of its vision-language semantics. This design provides two key benefits: first, it allows practical deployment across a wide range of robots that lack expensive or fragile force-torque sensors, thereby reducing hardware cost and complexity; second, the FDM introduces an additional force-vision-state fusion prior to the VLM, which improves cross-modal alignment and enhances perception-action robustness in contact-rich scenarios. Surprisingly, our physical experiments show that the distilled force token outperforms direct sensor force measurements as well as other baselines, which highlights the effectiveness of this force-distilled VLA approach."
    },
    {
      "title": "Abstracting Robot Manipulation Skills via Mixture-of-Experts Diffusion Policies",
      "url": "https://arxiv.org/abs/2601.21251",
      "date": "2026-01-29",
      "authors_text": "Ce Hao, Xuanran Zhai, Yaohua Liu, Harold Soh",
      "is_highlight": false,
      "score": 90.0,
      "summary": "The Skill Mixture-of-Experts Policy (SMP) enhances robot manipulation by efficiently utilizing a compact skill basis and adaptive expert activation for improved multi-task performance.",
      "teaser_image": "https://arxiv.org/html/2601.21251/x1.png",
      "debug_abstract": "Diffusion-based policies have recently shown strong results in robot manipulation, but their extension to multi-task scenarios is hindered by the high cost of scaling model size and demonstrations. We introduce Skill Mixture-of-Experts Policy (SMP), a diffusion-based mixture-of-experts policy that learns a compact orthogonal skill basis and uses sticky routing to compose actions from a small, task-relevant subset of experts at each step. A variational training objective supports this design, and adaptive expert activation at inference yields fast sampling without oversized backbones. We validate SMP in simulation and on a real dual-arm platform with multi-task learning and transfer learning tasks, where SMP achieves higher success rates and markedly lower inference cost than large diffusion baselines. These results indicate a practical path toward scalable, transferable multi-task manipulation: learn reusable skills once, activate only what is needed, and adapt quickly when tasks change."
    },
    {
      "title": "TraceRouter: Robust Safety for Large Foundation Models via Path-Level Intervention",
      "url": "https://arxiv.org/abs/2601.21900",
      "date": "2026-01-29",
      "authors_text": "Chuancheng Shi, Shangze Li, Wenjun Lu, Wenhua Wu, Cong Wang",
      "is_highlight": false,
      "score": 73.0,
      "summary": "TraceRouter enhances the safety of large foundation models by disconnecting harmful semantic pathways through a novel path-level intervention framework, outperforming existing defenses.",
      "teaser_image": "https://arxiv.org/html/2601.21900/x8.png",
      "debug_abstract": "Despite their capabilities, large foundation models (LFMs) remain susceptible to adversarial manipulation. Current defenses predominantly rely on the &#34;locality hypothesis&#34;, suppressing isolated neurons or features. However, harmful semantics act as distributed, cross-layer circuits, rendering such localized interventions brittle and detrimental to utility. To bridge this gap, we propose \\textbf{TraceRouter}, a path-level framework that traces and disconnects the causal propagation circuits of illicit semantics. TraceRouter operates in three stages: (1) it pinpoints a sensitive onset layer by analyzing attention divergence; (2) it leverages sparse autoencoders (SAEs) and differential activation analysis to disentangle and isolate malicious features; and (3) it maps these features to downstream causal pathways via feature influence scores (FIS) derived from zero-out interventions. By selectively suppressing these causal chains, TraceRouter physically severs the flow of harmful information while leaving orthogonal computation routes intact. Extensive experiments demonstrate that TraceRouter significantly outperforms state-of-the-art baselines, achieving a superior trade-off between adversarial robustness and general utility. Our code will be publicly released. WARNING: This paper contains unsafe model responses."
    },
    {
      "title": "GPO: Growing Policy Optimization for Legged Robot Locomotion and Whole-Body Control",
      "url": "https://arxiv.org/abs/2601.20668",
      "date": "2026-01-28",
      "authors_text": "Shuhao Liao, Peizhuo Li, Xinrong Yang, Linnan Chang, Zhaoxin Fan",
      "is_highlight": false,
      "score": 92.0,
      "summary": "The paper presents Growing Policy Optimization (GPO), a framework that enhances reinforcement learning for legged robots by progressively expanding the action space to improve data collection and policy performance.",
      "teaser_image": "https://arxiv.org/html/2601.20668/pic/framework.png",
      "debug_abstract": "Training reinforcement learning (RL) policies for legged robots remains challenging due to high-dimensional continuous actions, hardware constraints, and limited exploration. Existing methods for locomotion and whole-body control work well for position-based control with environment-specific heuristics (e.g., reward shaping, curriculum design, and manual initialization), but are less effective for torque-based control, where sufficiently exploring the action space and obtaining informative gradient signals for training is significantly more difficult. We introduce Growing Policy Optimization (GPO), a training framework that applies a time-varying action transformation to restrict the effective action space in the early stage, thereby encouraging more effective data collection and policy learning, and then progressively expands it to enhance exploration and achieve higher expected return. We prove that this transformation preserves the PPO update rule and introduces only bounded, vanishing gradient distortion, thereby ensuring stable training. We evaluate GPO on both quadruped and hexapod robots, including zero-shot deployment of simulation-trained policies on hardware. Policies trained with GPO consistently achieve better performance. These results suggest that GPO provides a general, environment-agnostic optimization framework for learning legged locomotion."
    },
    {
      "title": "Curiosity Driven Knowledge Retrieval for Mobile Agents",
      "url": "https://arxiv.org/abs/2601.19306",
      "date": "2026-01-27",
      "authors_text": "Sijia Li, Xiaoyu Tan, Shahir Ali, Niels Schmidt, Gengchen Ma",
      "is_highlight": false,
      "score": 80.0,
      "summary": "The paper presents a curiosity-driven framework for mobile agents that enhances performance by retrieving and integrating structured knowledge (AppCards) when faced with uncertainty, achieving improved planning reliability.",
      "debug_abstract": "Mobile agents have made progress toward reliable smartphone automation, yet performance in complex applications remains limited by incomplete knowledge and weak generalization to unseen environments. We introduce a curiosity driven knowledge retrieval framework that formalizes uncertainty during execution as a curiosity score. When this score exceeds a threshold, the system retrieves external information from documentation, code repositories, and historical trajectories. Retrieved content is organized into structured AppCards, which encode functional semantics, parameter conventions, interface mappings, and interaction patterns. During execution, an enhanced agent selectively integrates relevant AppCards into its reasoning process, thereby compensating for knowledge blind spots and improving planning reliability. Evaluation on the AndroidWorld benchmark shows consistent improvements across backbones, with an average gain of six percentage points and a new state of the art success rate of 88.8\\% when combined with GPT-5. Analysis indicates that AppCards are particularly effective for multi step and cross application tasks, while improvements depend on the backbone model. Case studies further confirm that AppCards reduce ambiguity, shorten exploration, and support stable execution trajectories. Task trajectories are publicly available at this https URL."
    },
    {
      "title": "Video-KTR: Reinforcing Video Reasoning via Key Token Attribution",
      "url": "https://arxiv.org/abs/2601.19686",
      "date": "2026-01-27",
      "authors_text": "Ziyue Wang, Sheng Jin, Zhongrong Zuo, Jiawei Wu, Han Qiu",
      "is_highlight": false,
      "score": 66.0,
      "summary": "Video-KTR enhances video reasoning in multimodal models by selectively reinforcing key tokens through a novel attribution framework, achieving state-of-the-art performance across multiple benchmarks.",
      "debug_abstract": "Reinforcement learning (RL) has shown strong potential for enhancing reasoning in multimodal large language models, yet existing video reasoning methods often rely on coarse sequence-level rewards or single-factor token selection, neglecting fine-grained links among visual inputs, temporal dynamics, and linguistic outputs, limiting both accuracy and interpretability. We propose Video-KTR, a modality-aware policy shaping framework that performs selective, token-level RL by combining three attribution signals: (1) visual-aware tokens identified via counterfactual masking to reveal perceptual dependence; (2) temporal-aware tokens detected through frame shuffling to expose temporal sensitivity; and (3) high-entropy tokens signaling predictive uncertainty. By reinforcing only these key tokens, Video-KTR focuses learning on semantically informative, modality-sensitive content while filtering out low-value tokens. Across five challenging benchmarks, Video-KTR achieves state-of-the-art or highly competitive results, achieving 42.7\\% on Video-Holmes (surpassing GPT-4o) with consistent gains on both reasoning and general video understanding tasks. Ablation studies verify the complementary roles of the attribution signals and the robustness of targeted token-level updates. Overall, Video-KTR improves accuracy and interpretability, offering a simple, drop-in extension to RL for complex video reasoning. Our code and models are available at this https URL."
    },
    {
      "title": "ORION: Option-Regularized Deep Reinforcement Learning for Cooperative Multi-Agent Online Navigation",
      "url": "https://arxiv.org/abs/2601.01155",
      "date": "2026-01-26",
      "authors_text": "Shizhe Zhang, Jingsong Liang, Zhitao Zhou, Shuhan Ye, Yizhuo Wang",
      "is_highlight": false,
      "score": 88.0,
      "summary": "ORION is a deep reinforcement learning framework that enables decentralized, cooperative navigation for multi-agent systems in partially known environments by integrating prior maps and online observations.",
      "debug_abstract": "Existing methods for multi-agent navigation typically assume fully known environments, offering limited support for partially known scenarios such as warehouses or factory floors. There, agents may need to plan trajectories that balance their own path optimality with their ability to collect and share information about the environment that can help their teammates reach their own goals. To these ends, we propose ORION, a novel deep reinforcement learning framework for cooperative multi-agent online navigation in partially known environments. Starting from an imperfect prior map, ORION trains agents to make decentralized decisions, coordinate to reach their individual targets, and actively reduce map uncertainty by sharing online observations in a closed perception-action loop. We first design a shared graph encoder that fuses prior map with online perception into a unified representation, providing robust state embeddings under dynamic map discrepancies. At the core of ORION is an option-critic framework that learns to reason about a set of high-level cooperative modes that translate into sequences of low-level actions, allowing agents to switch between individual navigation and team-level exploration adaptively. We further introduce a dual-stage cooperation strategy that enables agents to assist teammates under map uncertainty, thereby reducing the overall makespan. Across extensive maze-like maps and large-scale warehouse environments, our simulation results show that ORION achieves high-quality, real-time decentralized cooperation over varying team sizes, outperforming state-of-the-art classical and learning-based baselines. Finally, we validate ORION on physical robot teams, demonstrating its robustness and practicality for real-world cooperative navigation."
    },
    {
      "title": "Fast and Safe Trajectory Optimization for Mobile Manipulators With Neural Configuration Space Distance Field",
      "url": "https://arxiv.org/abs/2601.18548",
      "date": "2026-01-26",
      "authors_text": "Yulin Li, Zhiyuan Song, Yiming Li, Zhicheng Song, Kai Chen",
      "is_highlight": false,
      "score": 86.0,
      "summary": "This paper introduces Generalized Configuration Space Distance Fields (GCDF) for efficient trajectory optimization in mobile manipulators, enhancing collision reasoning and enabling agile motion in complex environments.",
      "debug_abstract": "Mobile manipulators promise agile, long-horizon behavior by coordinating base and arm motion, yet whole-body trajectory optimization in cluttered, confined spaces remains difficult due to high-dimensional nonconvexity and the need for fast, accurate collision reasoning. Configuration Space Distance Fields (CDF) enable fixed-base manipulators to model collisions directly in configuration space via smooth, implicit distances. This representation holds strong potential to bypass the nonlinear configuration-to-workspace mapping while preserving accurate whole-body geometry and providing optimization-friendly collision costs. Yet, extending this capability to mobile manipulators is hindered by unbounded workspaces and tighter base-arm coupling. We lift this promise to mobile manipulation with Generalized Configuration Space Distance Fields (GCDF), extending CDF to robots with both translational and rotational joints in unbounded workspaces with tighter base-arm coupling. We prove that GCDF preserves Euclidean-like local distance structure and accurately encodes whole-body geometry in configuration space, and develop a data generation and training pipeline that yields continuous neural GCDFs with accurate values and gradients, supporting efficient GPU-batched queries. Building on this representation, we develop a high-performance sequential convex optimization framework centered on GCDF-based collision reasoning. The solver scales to large numbers of implicit constraints through (i) online specification of neural constraints, (ii) sparsity-aware active-set detection with parallel batched evaluation across thousands of constraints, and (iii) incremental constraint management for rapid replanning under scene changes."
    },
    {
      "title": "AdaReasoner: Dynamic Tool Orchestration for Iterative Visual Reasoning",
      "url": "https://arxiv.org/abs/2601.18631",
      "date": "2026-01-26",
      "authors_text": "Mingyang Song, Haoyu Sun, Jiawei Gu, Linjie Li, Luxin Xu",
      "is_highlight": false,
      "score": 61.0,
      "summary": "AdaReasoner is a multimodal model that autonomously optimizes tool selection and usage for iterative visual reasoning, achieving state-of-the-art performance on various benchmarks.",
      "debug_abstract": "When humans face problems beyond their immediate capabilities, they rely on tools, providing a promising paradigm for improving visual reasoning in multimodal large language models (MLLMs). Effective reasoning, therefore, hinges on knowing which tools to use, when to invoke them, and how to compose them over multiple steps, even when faced with new tools or new tasks. We introduce \\textbf{AdaReasoner}, a family of multimodal models that learn tool use as a general reasoning skill rather than as tool-specific or explicitly supervised behavior. AdaReasoner is enabled by (i) a scalable data curation pipeline exposing models to long-horizon, multi-step tool interactions; (ii) Tool-GRPO, a reinforcement learning algorithm that optimizes tool selection and sequencing based on end-task success; and (iii) an adaptive learning mechanism that dynamically regulates tool usage. Together, these components allow models to infer tool utility from task context and intermediate outcomes, enabling coordination of multiple tools and generalization to unseen tools. Empirically, AdaReasoner exhibits strong tool-adaptive and generalization behaviors: it autonomously adopts beneficial tools, suppresses irrelevant ones, and adjusts tool usage frequency based on task demands, despite never being explicitly trained to do so. These capabilities translate into state-of-the-art performance across challenging benchmarks, improving the 7B base model by +24.9\\% on average and surpassing strong proprietary systems such as GPT-5 on multiple tasks, including VSP and Jigsaw."
    },
    {
      "title": "Emotion-LLaMAv2 and MMEVerse: A New Framework and Benchmark for Multimodal Emotion Understanding",
      "url": "https://arxiv.org/abs/2601.16449",
      "date": "2026-01-23",
      "authors_text": "Xiaojiang Peng, Jingyi Chen, Zebang Cheng, Bao Peng, Fengyi Wu",
      "is_highlight": false,
      "score": 58.0,
      "summary": "Emotion-LLaMAv2 and MMEVerse introduce a comprehensive framework and benchmark for enhanced multimodal emotion understanding through advanced encoding, feature interaction, and large-scale dataset integration.",
      "debug_abstract": "Understanding human emotions from multimodal signals poses a significant challenge in affective computing and human-robot interaction. While multimodal large language models (MLLMs) have excelled in general vision-language tasks, their capabilities in emotional reasoning remain limited. The field currently suffers from a scarcity of large-scale datasets with high-quality, descriptive emotion annotations and lacks standardized benchmarks for evaluation. Our preliminary framework, Emotion-LLaMA, pioneered instruction-tuned multimodal learning for emotion reasoning but was restricted by explicit face detectors, implicit fusion strategies, and low-quality training data with limited scale. To address these limitations, we present Emotion-LLaMAv2 and the MMEVerse benchmark, establishing an end-to-end pipeline together with a standardized evaluation setting for emotion recognition and reasoning. Emotion-LLaMAv2 introduces three key advances. First, an end-to-end multiview encoder eliminates external face detection and captures nuanced emotional cues via richer spatial and temporal multiview tokens. Second, a Conv Attention pre-fusion module is designed to enable simultaneous local and global multimodal feature interactions external to the LLM backbone. Third, a perception-to-cognition curriculum instruction tuning scheme within the LLaMA2 backbone unifies emotion recognition and free-form emotion reasoning. To support large-scale training and reproducible evaluation, MMEVerse aggregates twelve publicly available emotion datasets, including IEMOCAP, MELD, DFEW, and MAFW, into a unified multimodal instruction format. The data are re-annotated via a multi-agent pipeline involving Qwen2 Audio, Qwen2.5 VL, and GPT 4o, producing 130k training clips and 36k testing clips across 18 evaluation benchmarks."
    },
    {
      "title": "Virtual Traffic Police: Large Language Model-Augmented Traffic Signal Control for Unforeseen Incidents",
      "url": "https://arxiv.org/abs/2601.15816",
      "date": "2026-01-22",
      "authors_text": "Shiqi Wei, Qiqing Wang, Kaidi Yang",
      "is_highlight": false,
      "score": 84.0,
      "summary": "The paper presents a hierarchical framework that enhances traditional traffic signal control with Large Language Models to efficiently manage unforeseen traffic incidents.",
      "debug_abstract": "Adaptive traffic signal control (TSC) has demonstrated strong effectiveness in managing dynamic traffic flows. However, conventional methods often struggle when unforeseen traffic incidents occur (e.g., accidents and road maintenance), which typically require labor-intensive and inefficient manual interventions by traffic police officers. Large Language Models (LLMs) appear to be a promising solution thanks to their remarkable reasoning and generalization capabilities. Nevertheless, existing works often propose to replace existing TSC systems with LLM-based systems, which can be (i) unreliable due to the inherent hallucinations of LLMs and (ii) costly due to the need for system replacement. To address the issues of existing works, we propose a hierarchical framework that augments existing TSC systems with LLMs, whereby a virtual traffic police agent at the upper level dynamically fine-tunes selected parameters of signal controllers at the lower level in response to real-time traffic incidents. To enhance domain-specific reliability in response to unforeseen traffic incidents, we devise a self-refined traffic language retrieval system (TLRS), whereby retrieval-augmented generation is employed to draw knowledge from a tailored traffic language database that encompasses traffic conditions and controller operation principles. Moreover, we devise an LLM-based verifier to update the TLRS continuously over the reasoning process. Our results show that LLMs can serve as trustworthy virtual traffic police officers that can adapt conventional TSC methods to unforeseen traffic incidents with significantly improved operational efficiency and reliability."
    },
    {
      "title": "PhysicsMind: Sim and Real Mechanics Benchmarking for Physical Reasoning and Prediction in Foundational VLMs and World Models",
      "url": "https://arxiv.org/abs/2601.16007",
      "date": "2026-01-22",
      "authors_text": "Chak-Wing Mak, Guanyu Zhu, Boyi Zhang, Hongji Li, Xiaowei Chi",
      "is_highlight": false,
      "score": 94.1,
      "summary": "PhysicsMind introduces a benchmark for evaluating physical reasoning in multimodal models through real and simulated tasks focused on fundamental physics principles, revealing significant gaps in current models' understanding.",
      "debug_abstract": "Modern foundational Multimodal Large Language Models (MLLMs) and video world models have advanced significantly in mathematical, common-sense, and visual reasoning, but their grasp of the underlying physics remains underexplored. Existing benchmarks attempting to measure this matter rely on synthetic, Visual Question Answer templates or focus on perceptual video quality that is tangential to measuring how well the video abides by physical laws. To address this fragmentation, we introduce PhysicsMind, a unified benchmark with both real and simulation environments that evaluates law-consistent reasoning and generation over three canonical principles: Center of Mass, Lever Equilibrium, and Newton&#39;s First Law. PhysicsMind comprises two main tasks: i) VQA tasks, testing whether models can reason and determine physical quantities and values from images or short videos, and ii) Video Generation(VG) tasks, evaluating if predicted motion trajectories obey the same center-of-mass, torque, and inertial constraints as the ground truth. A broad range of recent models and video generation models is evaluated on PhysicsMind and found to rely on appearance heuristics while often violating basic mechanics. These gaps indicate that current scaling and training are still insufficient for robust physical understanding, underscoring PhysicsMind as a focused testbed for physics-aware multimodal models. Our data will be released upon acceptance."
    },
    {
      "title": "AION: Aerial Indoor Object-Goal Navigation Using Dual-Policy Reinforcement Learning",
      "url": "https://arxiv.org/abs/2601.15614",
      "date": "2026-01-22",
      "authors_text": "Zichen Yan, Yuchen Hou, Shenao Wang, Yichao Gao, Rui Huang",
      "is_highlight": false,
      "score": 85.6,
      "summary": "AION introduces a dual-policy reinforcement learning framework for aerial object-goal navigation, enhancing exploration and navigation efficiency without external localization or global maps.",
      "debug_abstract": "Object-Goal Navigation (ObjectNav) requires an agent to autonomously explore an unknown environment and navigate toward target objects specified by a semantic label. While prior work has primarily studied zero-shot ObjectNav under 2D locomotion, extending it to aerial platforms with 3D locomotion capability remains underexplored. Aerial robots offer superior maneuverability and search efficiency, but they also introduce new challenges in spatial perception, dynamic control, and safety assurance. In this paper, we propose AION for vision-based aerial ObjectNav without relying on external localization or global maps. AION is an end-to-end dual-policy reinforcement learning (RL) framework that decouples exploration and goal-reaching behaviors into two specialized policies. We evaluate AION on the AI2-THOR benchmark and further assess its real-time performance in IsaacSim using high-fidelity drone models. Experimental results show that AION achieves superior performance across comprehensive evaluation metrics in exploration, navigation efficiency, and safety. The video can be found at this https URL."
    }
  ],
  "Advanced Robotics Centre": [
    {
      "title": "FARTrack: Fast Autoregressive Visual Tracking with High Performance",
      "url": "https://arxiv.org/abs/2602.03214",
      "date": "2026-02-03",
      "authors_text": "Guijie Wang, Tong Lin, Yifan Bai, Anjia Cao, Shiyi Liang",
      "is_highlight": false,
      "score": 79.0,
      "summary": "FARTrack is a fast autoregressive visual tracking framework that enhances inference speed and performance through task-specific self-distillation and inter-frame autoregressive sparsification, achieving 343 FPS on GPU.",
      "teaser_image": "https://arxiv.org/html/2602.03214/x1.png",
      "debug_abstract": "Inference speed and tracking performance are two critical evaluation metrics in the field of visual tracking. However, high-performance trackers often suffer from slow processing speeds, making them impractical for deployment on resource-constrained devices. To alleviate this issue, we propose FARTrack, a Fast Auto-Regressive Tracking framework. Since autoregression emphasizes the temporal nature of the trajectory sequence, it can maintain high performance while achieving efficient execution across various devices. FARTrack introduces Task-Specific Self-Distillation and Inter-frame Autoregressive Sparsification, designed from the perspectives of shallow-yet-accurate distillation and redundant-to-essential token optimization, respectively. Task-Specific Self-Distillation achieves model compression by distilling task-specific tokens layer by layer, enhancing the model&#39;s inference speed while avoiding suboptimal manual teacher-student layer pairs assignments. Meanwhile, Inter-frame Autoregressive Sparsification sequentially condenses multiple templates, avoiding additional runtime overhead while learning a temporally-global optimal sparsification strategy. FARTrack demonstrates outstanding speed and competitive performance. It delivers an AO of 70.6% on GOT-10k in real-time. Beyond, our fastest model achieves a speed of 343 FPS on the GPU and 121 FPS on the CPU."
    },
    {
      "title": "PWAVEP: Purifying Imperceptible Adversarial Perturbations in 3D Point Clouds via Spectral Graph Wavelets",
      "url": "https://arxiv.org/abs/2602.03333",
      "date": "2026-02-03",
      "authors_text": "Haoran Li, Renyang Liu, Hongjia Liu, Chen Wang, Long Yin",
      "is_highlight": false,
      "score": 59.0,
      "summary": "PWAVEP introduces a non-invasive defense mechanism using spectral graph wavelets to purify adversarial perturbations in 3D point clouds, enhancing accuracy and robustness.",
      "teaser_image": "https://arxiv.org/html/2602.03333/x1.png",
      "debug_abstract": "Recent progress in adversarial attacks on 3D point clouds, particularly in achieving spatial imperceptibility and high attack performance, presents significant challenges for defenders. Current defensive approaches remain cumbersome, often requiring invasive model modifications, expensive training procedures or auxiliary data access. To address these threats, in this paper, we propose a plug-and-play and non-invasive defense mechanism in the spectral domain, grounded in a theoretical and empirical analysis of the relationship between imperceptible perturbations and high-frequency spectral components. Building upon these insights, we introduce a novel purification framework, termed PWAVEP, which begins by computing a spectral graph wavelet domain saliency score and local sparsity score for each point. Guided by these values, PWAVEP adopts a hierarchical strategy, it eliminates the most salient points, which are identified as hardly recoverable adversarial outliers. Simultaneously, it applies a spectral filtering process to a broader set of moderately salient points. This process leverages a graph wavelet transform to attenuate high-frequency coefficients associated with the targeted points, thereby effectively suppressing adversarial noise. Extensive evaluations demonstrate that the proposed PWAVEP achieves superior accuracy and robustness compared to existing approaches, advancing the state-of-the-art in 3D point cloud purification. Code and datasets are available at this https URL"
    },
    {
      "title": "Inject Once Survive Later: Backdooring Vision-Language-Action Models to Persist Through Downstream Fine-tuning",
      "url": "https://arxiv.org/abs/2602.00500",
      "date": "2026-01-31",
      "authors_text": "Jianyi Zhou, Yujie Wei, Ruichen Zhen, Bo Zhao, Xiaobo Xia",
      "is_highlight": true,
      "score": 60.0,
      "summary": "The paper introduces INFUSE, a robust backdoor attack framework for Vision-Language-Action models that survives user fine-tuning, maintaining high attack success rates while preserving performance.",
      "teaser_image": "https://arxiv.org/html/2602.00500/x2.png",
      "debug_abstract": "Vision-Language-Action (VLA) models have become foundational to modern embodied AI systems. By integrating visual perception, language understanding, and action planning, they enable general-purpose task execution across diverse environments. Despite their importance, the security of VLA models remains underexplored -- particularly in the context of backdoor attacks, which pose realistic threats in physical-world deployments. While recent methods attempt to inject backdoors into VLA models, these backdoors are easily erased during downstream adaptation, as user-side fine-tuning with clean data significantly alters model parameters, rendering them impractical for real-world applications. To address these challenges, we propose INFUSE (INjection into Fine-tUne-inSensitive modulEs), the first backdoor attack framework for VLA base models that remains effective even with arbitrary user fine-tuning. INFUSE begins by analyzing parameter sensitivity across diverse fine-tuning scenarios to identify modules that remain largely unchanged -- the fine-tune-insensitive modules. It then injects backdoors into these stable modules while freezing the rest, ensuring malicious behavior persists after extensive user fine-tuning. Comprehensive experiments across multiple VLA architectures demonstrate INFUSE&#39;s effectiveness. After user-side fine-tuning, INFUSE maintains mean attack success rates of 91.0% on simulation environments and 79.8% on real-world robot tasks, substantially surpassing BadVLA (38.8% and 36.6%, respectively), while preserving clean-task performance comparable to standard models. These results uncover a critical threat: backdoors implanted before distribution can persist through fine-tuning and remain effective at deployment."
    },
    {
      "title": "APEX: A Decoupled Memory-based Explorer for Asynchronous Aerial Object Goal Navigation",
      "url": "https://arxiv.org/abs/2602.00551",
      "date": "2026-01-31",
      "authors_text": "Daoxuan Zhang, Ping Chen, Xiaobo Xia, Xiu Su, Ruichen Zhen",
      "is_highlight": false,
      "score": 88.0,
      "summary": "APEX is a novel hierarchical agent that enhances UAV navigation through efficient exploration and target acquisition using dynamic memory, reinforcement learning, and open-vocabulary detection.",
      "teaser_image": "https://arxiv.org/html/2602.00551/x1.png",
      "debug_abstract": "Aerial Object Goal Navigation, a challenging frontier in Embodied AI, requires an Unmanned Aerial Vehicle (UAV) agent to autonomously explore, reason, and identify a specific target using only visual perception and language description. However, existing methods struggle with the memorization of complex spatial representations in aerial environments, reliable and interpretable action decision-making, and inefficient exploration and information gathering. To address these challenges, we introduce \\textbf{APEX} (Aerial Parallel Explorer), a novel hierarchical agent designed for efficient exploration and target acquisition in complex aerial settings. APEX is built upon a modular, three-part architecture: 1) Dynamic Spatio-Semantic Mapping Memory, which leverages the zero-shot capability of a Vision-Language Model (VLM) to dynamically construct high-resolution 3D Attraction, Exploration, and Obstacle maps, serving as an interpretable memory mechanism. 2) Action Decision Module, trained with reinforcement learning, which translates this rich spatial understanding into a fine-grained and robust control policy. 3) Target Grounding Module, which employs an open-vocabulary detector to achieve definitive and generalizable target identification. All these components are integrated into a hierarchical, asynchronous, and parallel framework, effectively bypassing the VLM&#39;s inference latency and boosting the agent&#39;s proactivity in exploration. Extensive experiments show that APEX outperforms the previous state of the art by +4.2\\% SR and +2.8\\% SPL on challenging UAV-ON benchmarks, demonstrating its superior efficiency and the effectiveness of its hierarchical asynchronous design. Our source code is provided in \\href{this https URL}{GitHub}"
    },
    {
      "title": "Self-Guard: Defending Large Reasoning Models via enhanced self-reflection",
      "url": "https://arxiv.org/abs/2602.00707",
      "date": "2026-01-31",
      "authors_text": "Jingnan Zheng, Jingjun Xu, Yanzhen Luo, Chenhang Cui, Gelei Deng",
      "is_highlight": false,
      "score": 50.0,
      "summary": "Self-Guard is a lightweight framework enhancing safety compliance in Large Reasoning Models by promoting self-reflection and mitigating risks without sacrificing model utility.",
      "teaser_image": "https://arxiv.org/html/2602.00707/materials/workflow.png",
      "debug_abstract": "The emergence of Large Reasoning Models (LRMs) introduces a new paradigm of explicit reasoning, enabling remarkable advances yet posing unique risks such as reasoning manipulation and information leakage. To mitigate these risks, current alignment strategies predominantly rely on heavy post-training paradigms or external interventions. However, these approaches are often computationally intensive and fail to address the inherent awareness-compliance gap, a critical misalignment where models recognize potential risks yet prioritize following user instructions due to their sycophantic tendencies. To address these limitations, we propose Self-Guard, a lightweight safety defense framework that reinforces safety compliance at the representational level. Self-Guard operates through two principal stages: (1) safety-oriented prompting, which activates the model&#39;s latent safety awareness to evoke spontaneous reflection, and (2) safety activation steering, which extracts the resulting directional shift in the hidden state space and amplifies it to ensure that safety compliance prevails over sycophancy during inference. Experiments demonstrate that Self-Guard effectively bridges the awareness-compliance gap, achieving robust safety performance without compromising model utility. Furthermore, Self-Guard exhibits strong generalization across diverse unseen risks and varying model scales, offering a cost-efficient solution for LRM safety alignment."
    },
    {
      "title": "Unveiling the Cognitive Compass: Theory-of-Mind-Guided Multimodal Emotion Reasoning",
      "url": "https://arxiv.org/abs/2602.00971",
      "date": "2026-02-01",
      "authors_text": "Meng Luo, Bobo Li, Shanqing Xu, Shize Zhang, Qiuchan Chen",
      "is_highlight": false,
      "score": 62.0,
      "summary": "The paper presents HitEmotion, a benchmark for assessing and enhancing multimodal large language models' emotional reasoning through Theory of Mind-guided methods and reinforcement learning.",
      "teaser_image": "https://arxiv.org/html/2602.00971/figures/teaser.png",
      "debug_abstract": "Despite rapid progress in multimodal large language models (MLLMs), their capability for deep emotional understanding remains limited. We argue that genuine affective intelligence requires explicit modeling of Theory of Mind (ToM), the cognitive substrate from which emotions arise. To this end, we introduce HitEmotion, a ToM-grounded hierarchical benchmark that diagnoses capability breakpoints across increasing levels of cognitive depth. Second, we propose a ToM-guided reasoning chain that tracks mental states and calibrates cross-modal evidence to achieve faithful emotional reasoning. We further introduce TMPO, a reinforcement learning method that uses intermediate mental states as process-level supervision to guide and strengthen model reasoning. Extensive experiments show that HitEmotion exposes deep emotional reasoning deficits in state-of-the-art models, especially on cognitively demanding tasks. In evaluation, the ToM-guided reasoning chain and TMPO improve end-task accuracy and yield more faithful, more coherent rationales. In conclusion, our work provides the research community with a practical toolkit for evaluating and enhancing the cognition-based emotional understanding capabilities of MLLMs. Our dataset and code are available at: this https URL."
    },
    {
      "title": "Do All Individual Layers Help? An Empirical Study of Task-Interfering Layers in Vision-Language Models",
      "url": "https://arxiv.org/abs/2602.01167",
      "date": "2026-02-01",
      "authors_text": "Zhiming Liu, Yujie Wei, Lei Feng, Xiu Su, Xiaobo Xia",
      "is_highlight": false,
      "score": 52.0,
      "summary": "This study identifies Task-Interfering Layers in vision-language models, demonstrating that selectively bypassing certain layers can enhance task performance without retraining.",
      "teaser_image": "https://arxiv.org/html/2602.01167/x1.png",
      "debug_abstract": "Current VLMs have demonstrated capabilities across a wide range of multimodal tasks. Typically, in a pretrained VLM, all layers are engaged by default to make predictions on downstream tasks. We find that intervening on a single layer, such as by zeroing its parameters, can improve the performance on certain tasks, indicating that some layers hinder rather than help downstream tasks. We systematically investigate how individual layers influence different tasks via layer intervention. Specifically, we measure the change in performance relative to the base model after intervening on each layer and observe improvements when bypassing specific layers. This improvement can be generalizable across models and datasets, indicating the presence of Task-Interfering Layers that harm downstream tasks&#39; performance. We introduce Task-Layer Interaction Vector, which quantifies the effect of intervening on each layer of a VLM given a task. These task-interfering layers exhibit task-specific sensitivity patterns: tasks requiring similar capabilities show consistent response trends under layer interventions, as evidenced by the high similarity in their task-layer interaction vectors. Inspired by these findings, we propose TaLo (Task-Adaptive Layer Knockout), a training-free, test-time adaptation method that dynamically identifies and bypasses the most interfering layer for a given task. Without parameter updates, TaLo improves performance across various models and datasets, including boosting Qwen-VL&#39;s accuracy on the Maps task in ScienceQA by up to 16.6%. Our work reveals an unexpected form of modularity in pretrained VLMs and provides a plug-and-play, training-free mechanism to unlock hidden capabilities at inference time. The source code will be publicly available."
    },
    {
      "title": "PRISM: Festina Lente Proactivity -- Risk-Sensitive, Uncertainty-Aware Deliberation for Proactive Agents",
      "url": "https://arxiv.org/abs/2602.01532",
      "date": "2026-02-02",
      "authors_text": "Yuxuan Fu, Xiaoyu Tan, Teqi Hao, Chen Zhan, Xihe Qiu",
      "is_highlight": false,
      "score": 45.0,
      "summary": "PRISM introduces a decision-theoretic framework for proactive agents that optimizes intervention timing and accuracy, significantly reducing false alarms and enhancing performance on ProactiveBench.",
      "teaser_image": "https://arxiv.org/html/2602.01532/fig2.png",
      "debug_abstract": "Proactive agents must decide not only what to say but also whether and when to intervene. Many current systems rely on brittle heuristics or indiscriminate long reasoning, which offers little control over the benefit-burden tradeoff. We formulate the problem as cost-sensitive selective intervention and present PRISM, a novel framework that couples a decision-theoretic gate with a dual-process reasoning architecture. At inference time, the agent intervenes only when a calibrated probability of user acceptance exceeds a threshold derived from asymmetric costs of missed help and false alarms. Inspired by festina lente (Latin: &#34;make haste slowly&#34;), we gate by an acceptance-calibrated, cost-derived threshold and invoke a resource-intensive Slow mode with counterfactual checks only near the decision boundary, concentrating computation on ambiguous and high-stakes cases. Training uses gate-aligned, schema-locked distillation: a teacher running the full PRISM pipeline provides dense, executable supervision on unlabeled interaction traces, while the student learns a response policy that is explicitly decoupled from the intervention gate to enable tunable and auditable control. On ProactiveBench, PRISM reduces false alarms by 22.78% and improves F1 by 20.14% over strong baselines. These results show that principled decision-theoretic gating, paired with selective slow reasoning and aligned distillation, yields proactive agents that are precise, computationally efficient, and controllable. To facilitate reproducibility, we release our code, models, and resources at this https URL all experiments use the open-source ProactiveBench benchmark."
    },
    {
      "title": "ReCALL: Recalibrating Capability Degradation for MLLM-based Composed Image Retrieval",
      "url": "https://arxiv.org/abs/2602.01639",
      "date": "2026-02-02",
      "authors_text": "Tianyu Yang, ChenWei He, Xiangzhao Hao, Tianyue Wang, Jiarui Guo",
      "is_highlight": false,
      "score": 42.0,
      "summary": "ReCALL addresses capability degradation in MLLM-based Composed Image Retrieval by diagnosing issues, generating corrective instructions, and refining the retriever through continual training.",
      "teaser_image": "https://arxiv.org/html/2602.01639/x1.png",
      "debug_abstract": "Composed Image Retrieval (CIR) aims to retrieve target images based on a hybrid query comprising a reference image and a modification text. Early dual-tower Vision-Language Models (VLMs) struggle with cross-modality compositional reasoning required for this task. Recently, adapting generative Multimodal Large Language Models (MLLMs) for retrieval offers a promising direction. However, we identify that this adaptation strategy overlooks a fundamental issue: adapting a generative MLLM into a single-embedding discriminative retriever triggers a paradigm conflict, which leads to Capability Degradation - the deterioration of native fine-grained reasoning after retrieval adaptation. To address this challenge, we propose ReCALL (Recalibrating Capability Degradation), a model-agnostic framework that follows a diagnose-generate-refine pipeline: Firstly, we diagnose cognitive blind spots of the retriever via self-guided informative instance mining. Next, we generate corrective instructions and triplets by CoT prompting the foundation MLLM and conduct quality control with VQA-based consistency filtering. Finally, we refine the retriever through continual training on these triplets with a grouped contrastive scheme, thereby internalizing fine-grained visual-semantic distinctions and realigning the discriminative embedding space of retriever with intrinsic compositional reasoning within the MLLM. Extensive experiments on CIRR and FashionIQ show that ReCALL consistently recalibrates degraded capabilities and achieves state-of-the-art performance. Code will be released soon."
    },
    {
      "title": "Beyond Precision: Training-Inference Mismatch is an Optimization Problem and Simple LR Scheduling Fixes It",
      "url": "https://arxiv.org/abs/2602.01826",
      "date": "2026-02-02",
      "authors_text": "Yaxiang Zhang, Yingru Li, Jiacai Liu, Jiawei Xu, Ziniu Li",
      "is_highlight": false,
      "score": 35.0,
      "summary": "This paper presents a dynamic learning rate scheduling method to mitigate training-inference mismatch and stabilize reinforcement learning in large language models by addressing gradient noise.",
      "teaser_image": "https://arxiv.org/html/2602.01826/Qwen3-4B_baseline_vs_importance_sampling_patches.png",
      "debug_abstract": "Reinforcement Learning (RL) for training Large Language Models is notoriously unstable. While recent studies attribute this to &#34;training inference mismatch stemming&#34; from inconsistent hybrid engines, standard remedies, such as Importance Sampling, might fail during extended training runs. In this work, we analyze this instability through the lens of optimization, demonstrating that gradient noise and training-inference mismatch escalate in tandem as training progresses. Meanwhile, we find that the mismatch can be effectively suppressed by shrinking the update size. Taken together, we deduce that the mismatch is not merely a static numerical discrepancy, but a dynamic failure coupled with the model&#39;s optimization. Based on this insight, we propose a simple yet effective solution: a specialized Learning Rate (LR) scheduler. Instead of pre-defined decay schedule in traditional LR scheduler, our method dynamically triggers LR decay based on response length, which we identify as a reliable early-warning signal for impending instability. Empirical evidence suggests that by reducing the learning rate as gradient noise rises, we can consistently stabilize RL training and keep the training-inference mismatch at a safe level."
    },
    {
      "title": "UrbanGS: A Scalable and Efficient Architecture for Geometrically Accurate Large-Scene Reconstruction",
      "url": "https://arxiv.org/abs/2602.02089",
      "date": "2026-02-02",
      "authors_text": "Changbai Li, Haodong Zhu, Hanlin Chen, Xiuping Liang, Tongfei Chen",
      "is_highlight": false,
      "score": 22.0,
      "summary": "UrbanGS is a scalable framework that enhances geometric accuracy and memory efficiency in large-scale urban scene reconstruction through innovative regularization and adaptive Gaussian pruning techniques.",
      "teaser_image": "https://arxiv.org/html/2602.02089/figure/main.png",
      "debug_abstract": "While 3D Gaussian Splatting (3DGS) enables high-quality, real-time rendering for bounded scenes, its extension to large-scale urban environments gives rise to critical challenges in terms of geometric consistency, memory efficiency, and computational scalability. To address these issues, we present UrbanGS, a scalable reconstruction framework that effectively tackles these challenges for city-scale applications. First, we propose a Depth-Consistent D-Normal Regularization module. Unlike existing approaches that rely solely on monocular normal estimators, which can effectively update rotation parameters yet struggle to update position parameters, our method integrates D-Normal constraints with external depth supervision. This allows for comprehensive updates of all geometric parameters. By further incorporating an adaptive confidence weighting mechanism based on gradient consistency and inverse depth deviation, our approach significantly enhances multi-view depth alignment and geometric coherence, which effectively resolves the issue of geometric accuracy in complex large-scale scenes. To improve scalability, we introduce a Spatially Adaptive Gaussian Pruning (SAGP) strategy, which dynamically adjusts Gaussian density based on local geometric complexity and visibility to reduce redundancy. Additionally, a unified partitioning and view assignment scheme is designed to eliminate boundary artifacts and optimize computational load. Extensive experiments on multiple urban datasets demonstrate that UrbanGS achieves superior performance in rendering quality, geometric accuracy, and memory efficiency, providing a systematic solution for high-fidelity large-scale scene reconstruction."
    },
    {
      "title": "TIDE: Trajectory-based Diagnostic Evaluation of Test-Time Improvement in LLM Agents",
      "url": "https://arxiv.org/abs/2602.02196",
      "date": "2026-02-02",
      "authors_text": "Hang Yan, Xinyu Che, Fangzhi Xu, Qiushi Sun, Zichen Ding",
      "is_highlight": false,
      "score": 20.0,
      "summary": "The TIDE framework evaluates Test-Time Improvement in LLM agents by analyzing task completion dynamics, recursive behaviors, and memory constraints to enhance performance optimization.",
      "teaser_image": "https://arxiv.org/html/2602.02196/x1.png",
      "debug_abstract": "Recent advances in autonomous LLM agents demonstrate their ability to improve performance through iterative interaction with the environment. We define this paradigm as Test-Time Improvement (TTI). However, the mechanisms under how and why TTI succeed or fail remain poorly understood, and existing evaluation metrics fail to capture their task optimization efficiency, behavior adaptation after erroneous actions, and the specific utility of working memory for task completion. To address these gaps, we propose Test-time Improvement Diagnostic Evaluation (TIDE), an agent-agnostic and environment-agnostic framework that decomposes TTI into three comprehensive and interconnected dimensions. The framework measures (1) the overall temporal dynamics of task completion and (2) identifies whether performance is primarily constrained by recursive looping behaviors or (3) by burdensome accumulated memory. Through extensive experiments across diverse agents and environments, TIDE highlights that improving agent performance requires more than scaling internal reasoning, calling for explicitly optimizing the interaction dynamics between the agent and the environment."
    },
    {
      "title": "A Low-Cost Vision-Based Tactile Gripper with Pretraining Learning for Contact-Rich Manipulation",
      "url": "https://arxiv.org/abs/2602.00514",
      "date": "2026-01-31",
      "authors_text": "Yaohua Liu, Binkai Ou, Zicheng Qiu, Ce Hao, Yemin Wang",
      "is_highlight": false,
      "score": 11.0,
      "summary": "The LVTG gripper enhances contact-rich robotic manipulation through low-cost, durable visuo-tactile integration and improved policy learning via contrastive pretraining, achieving higher success rates.",
      "teaser_image": "https://arxiv.org/html/2602.00514/x1.png",
      "debug_abstract": "Robotic manipulation in contact-rich environments remains challenging, particularly when relying on conventional tactile sensors that suffer from limited sensing range, reliability, and cost-effectiveness. In this work, we present LVTG, a low-cost visuo-tactile gripper designed for stable, robust, and efficient physical interaction. Unlike existing visuo-tactile sensors, LVTG enables more effective and stable grasping of larger and heavier everyday objects, thanks to its enhanced tactile sensing area and greater opening angle. Its surface skin is made of highly wear-resistant material, significantly improving durability and extending operational lifespan. The integration of vision and tactile feedback allows LVTG to provide rich, high-fidelity sensory data, facilitating reliable perception during complex manipulation tasks. Furthermore, LVTG features a modular design that supports rapid maintenance and replacement. To effectively fuse vision and touch, We adopt a CLIP-inspired contrastive learning objective to align tactile embeddings with their corresponding visual observations, enabling a shared cross-modal representation space for visuo-tactile perception. This alignment improves the performance of an Action Chunking Transformer (ACT) policy in contact-rich manipulation, leading to more efficient data collection and more effective policy learning. Compared to the original ACT method, the proposed LVTG with pretraining achieves significantly higher success rates in manipulation tasks."
    },
    {
      "title": "FD-VLA: Force-Distilled Vision-Language-Action Model for Contact-Rich Manipulation",
      "url": "https://arxiv.org/abs/2602.02142",
      "date": "2026-02-02",
      "authors_text": "Ruiteng Zhao, Wenshuo Wang, Yicheng Ma, Xiaocong Li, Francis E. H. Tay",
      "is_highlight": false,
      "score": 0,
      "summary": "The FD-VLA framework enhances contact-rich manipulation by integrating force awareness through a novel distillation method, improving performance without physical force sensors.",
      "teaser_image": "https://arxiv.org/html/2602.02142/x1.png",
      "debug_abstract": "Force sensing is a crucial modality for Vision-Language-Action (VLA) frameworks, as it enables fine-grained perception and dexterous manipulation in contact-rich tasks. We present Force-Distilled VLA (FD-VLA), a novel framework that integrates force awareness into contact-rich manipulation without relying on physical force sensors. The core of our approach is a Force Distillation Module (FDM), which distills force by mapping a learnable query token, conditioned on visual observations and robot states, into a predicted force token aligned with the latent representation of actual force signals. During inference, this distilled force token is injected into the pretrained VLM, enabling force-aware reasoning while preserving the integrity of its vision-language semantics. This design provides two key benefits: first, it allows practical deployment across a wide range of robots that lack expensive or fragile force-torque sensors, thereby reducing hardware cost and complexity; second, the FDM introduces an additional force-vision-state fusion prior to the VLM, which improves cross-modal alignment and enhances perception-action robustness in contact-rich scenarios. Surprisingly, our physical experiments show that the distilled force token outperforms direct sensor force measurements as well as other baselines, which highlights the effectiveness of this force-distilled VLA approach."
    },
    {
      "title": "Abstracting Robot Manipulation Skills via Mixture-of-Experts Diffusion Policies",
      "url": "https://arxiv.org/abs/2601.21251",
      "date": "2026-01-29",
      "authors_text": "Ce Hao, Xuanran Zhai, Yaohua Liu, Harold Soh",
      "is_highlight": false,
      "score": 90.0,
      "summary": "The Skill Mixture-of-Experts Policy (SMP) enhances robot manipulation by efficiently utilizing a compact skill basis and adaptive expert activation for improved multi-task performance.",
      "teaser_image": "https://arxiv.org/html/2601.21251/x1.png",
      "debug_abstract": "Diffusion-based policies have recently shown strong results in robot manipulation, but their extension to multi-task scenarios is hindered by the high cost of scaling model size and demonstrations. We introduce Skill Mixture-of-Experts Policy (SMP), a diffusion-based mixture-of-experts policy that learns a compact orthogonal skill basis and uses sticky routing to compose actions from a small, task-relevant subset of experts at each step. A variational training objective supports this design, and adaptive expert activation at inference yields fast sampling without oversized backbones. We validate SMP in simulation and on a real dual-arm platform with multi-task learning and transfer learning tasks, where SMP achieves higher success rates and markedly lower inference cost than large diffusion baselines. These results indicate a practical path toward scalable, transferable multi-task manipulation: learn reusable skills once, activate only what is needed, and adapt quickly when tasks change."
    },
    {
      "title": "TraceRouter: Robust Safety for Large Foundation Models via Path-Level Intervention",
      "url": "https://arxiv.org/abs/2601.21900",
      "date": "2026-01-29",
      "authors_text": "Chuancheng Shi, Shangze Li, Wenjun Lu, Wenhua Wu, Cong Wang",
      "is_highlight": false,
      "score": 73.0,
      "summary": "TraceRouter enhances the safety of large foundation models by disconnecting harmful semantic pathways through a novel path-level intervention framework, outperforming existing defenses.",
      "teaser_image": "https://arxiv.org/html/2601.21900/x8.png",
      "debug_abstract": "Despite their capabilities, large foundation models (LFMs) remain susceptible to adversarial manipulation. Current defenses predominantly rely on the &#34;locality hypothesis&#34;, suppressing isolated neurons or features. However, harmful semantics act as distributed, cross-layer circuits, rendering such localized interventions brittle and detrimental to utility. To bridge this gap, we propose \\textbf{TraceRouter}, a path-level framework that traces and disconnects the causal propagation circuits of illicit semantics. TraceRouter operates in three stages: (1) it pinpoints a sensitive onset layer by analyzing attention divergence; (2) it leverages sparse autoencoders (SAEs) and differential activation analysis to disentangle and isolate malicious features; and (3) it maps these features to downstream causal pathways via feature influence scores (FIS) derived from zero-out interventions. By selectively suppressing these causal chains, TraceRouter physically severs the flow of harmful information while leaving orthogonal computation routes intact. Extensive experiments demonstrate that TraceRouter significantly outperforms state-of-the-art baselines, achieving a superior trade-off between adversarial robustness and general utility. Our code will be publicly released. WARNING: This paper contains unsafe model responses."
    },
    {
      "title": "GPO: Growing Policy Optimization for Legged Robot Locomotion and Whole-Body Control",
      "url": "https://arxiv.org/abs/2601.20668",
      "date": "2026-01-28",
      "authors_text": "Shuhao Liao, Peizhuo Li, Xinrong Yang, Linnan Chang, Zhaoxin Fan",
      "is_highlight": false,
      "score": 92.0,
      "summary": "The paper presents Growing Policy Optimization (GPO), a framework that enhances reinforcement learning for legged robots by progressively expanding the action space to improve data collection and policy performance.",
      "teaser_image": "https://arxiv.org/html/2601.20668/pic/framework.png",
      "debug_abstract": "Training reinforcement learning (RL) policies for legged robots remains challenging due to high-dimensional continuous actions, hardware constraints, and limited exploration. Existing methods for locomotion and whole-body control work well for position-based control with environment-specific heuristics (e.g., reward shaping, curriculum design, and manual initialization), but are less effective for torque-based control, where sufficiently exploring the action space and obtaining informative gradient signals for training is significantly more difficult. We introduce Growing Policy Optimization (GPO), a training framework that applies a time-varying action transformation to restrict the effective action space in the early stage, thereby encouraging more effective data collection and policy learning, and then progressively expands it to enhance exploration and achieve higher expected return. We prove that this transformation preserves the PPO update rule and introduces only bounded, vanishing gradient distortion, thereby ensuring stable training. We evaluate GPO on both quadruped and hexapod robots, including zero-shot deployment of simulation-trained policies on hardware. Policies trained with GPO consistently achieve better performance. These results suggest that GPO provides a general, environment-agnostic optimization framework for learning legged locomotion."
    },
    {
      "title": "Curiosity Driven Knowledge Retrieval for Mobile Agents",
      "url": "https://arxiv.org/abs/2601.19306",
      "date": "2026-01-27",
      "authors_text": "Sijia Li, Xiaoyu Tan, Shahir Ali, Niels Schmidt, Gengchen Ma",
      "is_highlight": false,
      "score": 80.0,
      "summary": "The paper presents a curiosity-driven framework for mobile agents that enhances performance by retrieving and integrating structured knowledge (AppCards) when faced with uncertainty, achieving improved planning reliability.",
      "debug_abstract": "Mobile agents have made progress toward reliable smartphone automation, yet performance in complex applications remains limited by incomplete knowledge and weak generalization to unseen environments. We introduce a curiosity driven knowledge retrieval framework that formalizes uncertainty during execution as a curiosity score. When this score exceeds a threshold, the system retrieves external information from documentation, code repositories, and historical trajectories. Retrieved content is organized into structured AppCards, which encode functional semantics, parameter conventions, interface mappings, and interaction patterns. During execution, an enhanced agent selectively integrates relevant AppCards into its reasoning process, thereby compensating for knowledge blind spots and improving planning reliability. Evaluation on the AndroidWorld benchmark shows consistent improvements across backbones, with an average gain of six percentage points and a new state of the art success rate of 88.8\\% when combined with GPT-5. Analysis indicates that AppCards are particularly effective for multi step and cross application tasks, while improvements depend on the backbone model. Case studies further confirm that AppCards reduce ambiguity, shorten exploration, and support stable execution trajectories. Task trajectories are publicly available at this https URL."
    },
    {
      "title": "Video-KTR: Reinforcing Video Reasoning via Key Token Attribution",
      "url": "https://arxiv.org/abs/2601.19686",
      "date": "2026-01-27",
      "authors_text": "Ziyue Wang, Sheng Jin, Zhongrong Zuo, Jiawei Wu, Han Qiu",
      "is_highlight": false,
      "score": 66.0,
      "summary": "Video-KTR enhances video reasoning in multimodal models by selectively reinforcing key tokens through a novel attribution framework, achieving state-of-the-art performance across multiple benchmarks.",
      "debug_abstract": "Reinforcement learning (RL) has shown strong potential for enhancing reasoning in multimodal large language models, yet existing video reasoning methods often rely on coarse sequence-level rewards or single-factor token selection, neglecting fine-grained links among visual inputs, temporal dynamics, and linguistic outputs, limiting both accuracy and interpretability. We propose Video-KTR, a modality-aware policy shaping framework that performs selective, token-level RL by combining three attribution signals: (1) visual-aware tokens identified via counterfactual masking to reveal perceptual dependence; (2) temporal-aware tokens detected through frame shuffling to expose temporal sensitivity; and (3) high-entropy tokens signaling predictive uncertainty. By reinforcing only these key tokens, Video-KTR focuses learning on semantically informative, modality-sensitive content while filtering out low-value tokens. Across five challenging benchmarks, Video-KTR achieves state-of-the-art or highly competitive results, achieving 42.7\\% on Video-Holmes (surpassing GPT-4o) with consistent gains on both reasoning and general video understanding tasks. Ablation studies verify the complementary roles of the attribution signals and the robustness of targeted token-level updates. Overall, Video-KTR improves accuracy and interpretability, offering a simple, drop-in extension to RL for complex video reasoning. Our code and models are available at this https URL."
    },
    {
      "title": "ORION: Option-Regularized Deep Reinforcement Learning for Cooperative Multi-Agent Online Navigation",
      "url": "https://arxiv.org/abs/2601.01155",
      "date": "2026-01-26",
      "authors_text": "Shizhe Zhang, Jingsong Liang, Zhitao Zhou, Shuhan Ye, Yizhuo Wang",
      "is_highlight": false,
      "score": 88.0,
      "summary": "ORION is a deep reinforcement learning framework that enables decentralized, cooperative navigation for multi-agent systems in partially known environments by integrating prior maps and online observations.",
      "debug_abstract": "Existing methods for multi-agent navigation typically assume fully known environments, offering limited support for partially known scenarios such as warehouses or factory floors. There, agents may need to plan trajectories that balance their own path optimality with their ability to collect and share information about the environment that can help their teammates reach their own goals. To these ends, we propose ORION, a novel deep reinforcement learning framework for cooperative multi-agent online navigation in partially known environments. Starting from an imperfect prior map, ORION trains agents to make decentralized decisions, coordinate to reach their individual targets, and actively reduce map uncertainty by sharing online observations in a closed perception-action loop. We first design a shared graph encoder that fuses prior map with online perception into a unified representation, providing robust state embeddings under dynamic map discrepancies. At the core of ORION is an option-critic framework that learns to reason about a set of high-level cooperative modes that translate into sequences of low-level actions, allowing agents to switch between individual navigation and team-level exploration adaptively. We further introduce a dual-stage cooperation strategy that enables agents to assist teammates under map uncertainty, thereby reducing the overall makespan. Across extensive maze-like maps and large-scale warehouse environments, our simulation results show that ORION achieves high-quality, real-time decentralized cooperation over varying team sizes, outperforming state-of-the-art classical and learning-based baselines. Finally, we validate ORION on physical robot teams, demonstrating its robustness and practicality for real-world cooperative navigation."
    },
    {
      "title": "Fast and Safe Trajectory Optimization for Mobile Manipulators With Neural Configuration Space Distance Field",
      "url": "https://arxiv.org/abs/2601.18548",
      "date": "2026-01-26",
      "authors_text": "Yulin Li, Zhiyuan Song, Yiming Li, Zhicheng Song, Kai Chen",
      "is_highlight": false,
      "score": 86.0,
      "summary": "This paper introduces Generalized Configuration Space Distance Fields (GCDF) for efficient trajectory optimization in mobile manipulators, enhancing collision reasoning and enabling agile motion in complex environments.",
      "debug_abstract": "Mobile manipulators promise agile, long-horizon behavior by coordinating base and arm motion, yet whole-body trajectory optimization in cluttered, confined spaces remains difficult due to high-dimensional nonconvexity and the need for fast, accurate collision reasoning. Configuration Space Distance Fields (CDF) enable fixed-base manipulators to model collisions directly in configuration space via smooth, implicit distances. This representation holds strong potential to bypass the nonlinear configuration-to-workspace mapping while preserving accurate whole-body geometry and providing optimization-friendly collision costs. Yet, extending this capability to mobile manipulators is hindered by unbounded workspaces and tighter base-arm coupling. We lift this promise to mobile manipulation with Generalized Configuration Space Distance Fields (GCDF), extending CDF to robots with both translational and rotational joints in unbounded workspaces with tighter base-arm coupling. We prove that GCDF preserves Euclidean-like local distance structure and accurately encodes whole-body geometry in configuration space, and develop a data generation and training pipeline that yields continuous neural GCDFs with accurate values and gradients, supporting efficient GPU-batched queries. Building on this representation, we develop a high-performance sequential convex optimization framework centered on GCDF-based collision reasoning. The solver scales to large numbers of implicit constraints through (i) online specification of neural constraints, (ii) sparsity-aware active-set detection with parallel batched evaluation across thousands of constraints, and (iii) incremental constraint management for rapid replanning under scene changes."
    },
    {
      "title": "AdaReasoner: Dynamic Tool Orchestration for Iterative Visual Reasoning",
      "url": "https://arxiv.org/abs/2601.18631",
      "date": "2026-01-26",
      "authors_text": "Mingyang Song, Haoyu Sun, Jiawei Gu, Linjie Li, Luxin Xu",
      "is_highlight": false,
      "score": 61.0,
      "summary": "AdaReasoner is a multimodal model that autonomously optimizes tool selection and usage for iterative visual reasoning, achieving state-of-the-art performance on various benchmarks.",
      "debug_abstract": "When humans face problems beyond their immediate capabilities, they rely on tools, providing a promising paradigm for improving visual reasoning in multimodal large language models (MLLMs). Effective reasoning, therefore, hinges on knowing which tools to use, when to invoke them, and how to compose them over multiple steps, even when faced with new tools or new tasks. We introduce \\textbf{AdaReasoner}, a family of multimodal models that learn tool use as a general reasoning skill rather than as tool-specific or explicitly supervised behavior. AdaReasoner is enabled by (i) a scalable data curation pipeline exposing models to long-horizon, multi-step tool interactions; (ii) Tool-GRPO, a reinforcement learning algorithm that optimizes tool selection and sequencing based on end-task success; and (iii) an adaptive learning mechanism that dynamically regulates tool usage. Together, these components allow models to infer tool utility from task context and intermediate outcomes, enabling coordination of multiple tools and generalization to unseen tools. Empirically, AdaReasoner exhibits strong tool-adaptive and generalization behaviors: it autonomously adopts beneficial tools, suppresses irrelevant ones, and adjusts tool usage frequency based on task demands, despite never being explicitly trained to do so. These capabilities translate into state-of-the-art performance across challenging benchmarks, improving the 7B base model by +24.9\\% on average and surpassing strong proprietary systems such as GPT-5 on multiple tasks, including VSP and Jigsaw."
    },
    {
      "title": "Emotion-LLaMAv2 and MMEVerse: A New Framework and Benchmark for Multimodal Emotion Understanding",
      "url": "https://arxiv.org/abs/2601.16449",
      "date": "2026-01-23",
      "authors_text": "Xiaojiang Peng, Jingyi Chen, Zebang Cheng, Bao Peng, Fengyi Wu",
      "is_highlight": false,
      "score": 58.0,
      "summary": "Emotion-LLaMAv2 and MMEVerse introduce a comprehensive framework and benchmark for enhanced multimodal emotion understanding through advanced encoding, feature interaction, and large-scale dataset integration.",
      "debug_abstract": "Understanding human emotions from multimodal signals poses a significant challenge in affective computing and human-robot interaction. While multimodal large language models (MLLMs) have excelled in general vision-language tasks, their capabilities in emotional reasoning remain limited. The field currently suffers from a scarcity of large-scale datasets with high-quality, descriptive emotion annotations and lacks standardized benchmarks for evaluation. Our preliminary framework, Emotion-LLaMA, pioneered instruction-tuned multimodal learning for emotion reasoning but was restricted by explicit face detectors, implicit fusion strategies, and low-quality training data with limited scale. To address these limitations, we present Emotion-LLaMAv2 and the MMEVerse benchmark, establishing an end-to-end pipeline together with a standardized evaluation setting for emotion recognition and reasoning. Emotion-LLaMAv2 introduces three key advances. First, an end-to-end multiview encoder eliminates external face detection and captures nuanced emotional cues via richer spatial and temporal multiview tokens. Second, a Conv Attention pre-fusion module is designed to enable simultaneous local and global multimodal feature interactions external to the LLM backbone. Third, a perception-to-cognition curriculum instruction tuning scheme within the LLaMA2 backbone unifies emotion recognition and free-form emotion reasoning. To support large-scale training and reproducible evaluation, MMEVerse aggregates twelve publicly available emotion datasets, including IEMOCAP, MELD, DFEW, and MAFW, into a unified multimodal instruction format. The data are re-annotated via a multi-agent pipeline involving Qwen2 Audio, Qwen2.5 VL, and GPT 4o, producing 130k training clips and 36k testing clips across 18 evaluation benchmarks."
    },
    {
      "title": "Virtual Traffic Police: Large Language Model-Augmented Traffic Signal Control for Unforeseen Incidents",
      "url": "https://arxiv.org/abs/2601.15816",
      "date": "2026-01-22",
      "authors_text": "Shiqi Wei, Qiqing Wang, Kaidi Yang",
      "is_highlight": false,
      "score": 84.0,
      "summary": "The paper presents a hierarchical framework that enhances traditional traffic signal control with Large Language Models to efficiently manage unforeseen traffic incidents.",
      "debug_abstract": "Adaptive traffic signal control (TSC) has demonstrated strong effectiveness in managing dynamic traffic flows. However, conventional methods often struggle when unforeseen traffic incidents occur (e.g., accidents and road maintenance), which typically require labor-intensive and inefficient manual interventions by traffic police officers. Large Language Models (LLMs) appear to be a promising solution thanks to their remarkable reasoning and generalization capabilities. Nevertheless, existing works often propose to replace existing TSC systems with LLM-based systems, which can be (i) unreliable due to the inherent hallucinations of LLMs and (ii) costly due to the need for system replacement. To address the issues of existing works, we propose a hierarchical framework that augments existing TSC systems with LLMs, whereby a virtual traffic police agent at the upper level dynamically fine-tunes selected parameters of signal controllers at the lower level in response to real-time traffic incidents. To enhance domain-specific reliability in response to unforeseen traffic incidents, we devise a self-refined traffic language retrieval system (TLRS), whereby retrieval-augmented generation is employed to draw knowledge from a tailored traffic language database that encompasses traffic conditions and controller operation principles. Moreover, we devise an LLM-based verifier to update the TLRS continuously over the reasoning process. Our results show that LLMs can serve as trustworthy virtual traffic police officers that can adapt conventional TSC methods to unforeseen traffic incidents with significantly improved operational efficiency and reliability."
    },
    {
      "title": "PhysicsMind: Sim and Real Mechanics Benchmarking for Physical Reasoning and Prediction in Foundational VLMs and World Models",
      "url": "https://arxiv.org/abs/2601.16007",
      "date": "2026-01-22",
      "authors_text": "Chak-Wing Mak, Guanyu Zhu, Boyi Zhang, Hongji Li, Xiaowei Chi",
      "is_highlight": false,
      "score": 94.1,
      "summary": "PhysicsMind introduces a benchmark for evaluating physical reasoning in multimodal models through real and simulated tasks focused on fundamental physics principles, revealing significant gaps in current models' understanding.",
      "debug_abstract": "Modern foundational Multimodal Large Language Models (MLLMs) and video world models have advanced significantly in mathematical, common-sense, and visual reasoning, but their grasp of the underlying physics remains underexplored. Existing benchmarks attempting to measure this matter rely on synthetic, Visual Question Answer templates or focus on perceptual video quality that is tangential to measuring how well the video abides by physical laws. To address this fragmentation, we introduce PhysicsMind, a unified benchmark with both real and simulation environments that evaluates law-consistent reasoning and generation over three canonical principles: Center of Mass, Lever Equilibrium, and Newton&#39;s First Law. PhysicsMind comprises two main tasks: i) VQA tasks, testing whether models can reason and determine physical quantities and values from images or short videos, and ii) Video Generation(VG) tasks, evaluating if predicted motion trajectories obey the same center-of-mass, torque, and inertial constraints as the ground truth. A broad range of recent models and video generation models is evaluated on PhysicsMind and found to rely on appearance heuristics while often violating basic mechanics. These gaps indicate that current scaling and training are still insufficient for robust physical understanding, underscoring PhysicsMind as a focused testbed for physics-aware multimodal models. Our data will be released upon acceptance."
    },
    {
      "title": "AION: Aerial Indoor Object-Goal Navigation Using Dual-Policy Reinforcement Learning",
      "url": "https://arxiv.org/abs/2601.15614",
      "date": "2026-01-22",
      "authors_text": "Zichen Yan, Yuchen Hou, Shenao Wang, Yichao Gao, Rui Huang",
      "is_highlight": false,
      "score": 85.6,
      "summary": "AION introduces a dual-policy reinforcement learning framework for aerial object-goal navigation, enhancing exploration and navigation efficiency without external localization or global maps.",
      "debug_abstract": "Object-Goal Navigation (ObjectNav) requires an agent to autonomously explore an unknown environment and navigate toward target objects specified by a semantic label. While prior work has primarily studied zero-shot ObjectNav under 2D locomotion, extending it to aerial platforms with 3D locomotion capability remains underexplored. Aerial robots offer superior maneuverability and search efficiency, but they also introduce new challenges in spatial perception, dynamic control, and safety assurance. In this paper, we propose AION for vision-based aerial ObjectNav without relying on external localization or global maps. AION is an end-to-end dual-policy reinforcement learning (RL) framework that decouples exploration and goal-reaching behaviors into two specialized policies. We evaluate AION on the AI2-THOR benchmark and further assess its real-time performance in IsaacSim using high-fidelity drone models. Experimental results show that AION achieves superior performance across comprehensive evaluation metrics in exploration, navigation efficiency, and safety. The video can be found at this https URL."
    }
  ],
  "Synteraction Lab": [
    {
      "title": "FARTrack: Fast Autoregressive Visual Tracking with High Performance",
      "url": "https://arxiv.org/abs/2602.03214",
      "date": "2026-02-03",
      "authors_text": "Guijie Wang, Tong Lin, Yifan Bai, Anjia Cao, Shiyi Liang",
      "is_highlight": false,
      "score": 79.0,
      "summary": "FARTrack is a fast autoregressive visual tracking framework that enhances inference speed and performance through task-specific self-distillation and inter-frame autoregressive sparsification, achieving 343 FPS on GPU.",
      "teaser_image": "https://arxiv.org/html/2602.03214/x1.png",
      "debug_abstract": "Inference speed and tracking performance are two critical evaluation metrics in the field of visual tracking. However, high-performance trackers often suffer from slow processing speeds, making them impractical for deployment on resource-constrained devices. To alleviate this issue, we propose FARTrack, a Fast Auto-Regressive Tracking framework. Since autoregression emphasizes the temporal nature of the trajectory sequence, it can maintain high performance while achieving efficient execution across various devices. FARTrack introduces Task-Specific Self-Distillation and Inter-frame Autoregressive Sparsification, designed from the perspectives of shallow-yet-accurate distillation and redundant-to-essential token optimization, respectively. Task-Specific Self-Distillation achieves model compression by distilling task-specific tokens layer by layer, enhancing the model&#39;s inference speed while avoiding suboptimal manual teacher-student layer pairs assignments. Meanwhile, Inter-frame Autoregressive Sparsification sequentially condenses multiple templates, avoiding additional runtime overhead while learning a temporally-global optimal sparsification strategy. FARTrack demonstrates outstanding speed and competitive performance. It delivers an AO of 70.6% on GOT-10k in real-time. Beyond, our fastest model achieves a speed of 343 FPS on the GPU and 121 FPS on the CPU."
    },
    {
      "title": "PWAVEP: Purifying Imperceptible Adversarial Perturbations in 3D Point Clouds via Spectral Graph Wavelets",
      "url": "https://arxiv.org/abs/2602.03333",
      "date": "2026-02-03",
      "authors_text": "Haoran Li, Renyang Liu, Hongjia Liu, Chen Wang, Long Yin",
      "is_highlight": false,
      "score": 59.0,
      "summary": "PWAVEP introduces a non-invasive defense mechanism using spectral graph wavelets to purify adversarial perturbations in 3D point clouds, enhancing accuracy and robustness.",
      "teaser_image": "https://arxiv.org/html/2602.03333/x1.png",
      "debug_abstract": "Recent progress in adversarial attacks on 3D point clouds, particularly in achieving spatial imperceptibility and high attack performance, presents significant challenges for defenders. Current defensive approaches remain cumbersome, often requiring invasive model modifications, expensive training procedures or auxiliary data access. To address these threats, in this paper, we propose a plug-and-play and non-invasive defense mechanism in the spectral domain, grounded in a theoretical and empirical analysis of the relationship between imperceptible perturbations and high-frequency spectral components. Building upon these insights, we introduce a novel purification framework, termed PWAVEP, which begins by computing a spectral graph wavelet domain saliency score and local sparsity score for each point. Guided by these values, PWAVEP adopts a hierarchical strategy, it eliminates the most salient points, which are identified as hardly recoverable adversarial outliers. Simultaneously, it applies a spectral filtering process to a broader set of moderately salient points. This process leverages a graph wavelet transform to attenuate high-frequency coefficients associated with the targeted points, thereby effectively suppressing adversarial noise. Extensive evaluations demonstrate that the proposed PWAVEP achieves superior accuracy and robustness compared to existing approaches, advancing the state-of-the-art in 3D point cloud purification. Code and datasets are available at this https URL"
    },
    {
      "title": "Inject Once Survive Later: Backdooring Vision-Language-Action Models to Persist Through Downstream Fine-tuning",
      "url": "https://arxiv.org/abs/2602.00500",
      "date": "2026-01-31",
      "authors_text": "Jianyi Zhou, Yujie Wei, Ruichen Zhen, Bo Zhao, Xiaobo Xia",
      "is_highlight": true,
      "score": 60.0,
      "summary": "The paper introduces INFUSE, a robust backdoor attack framework for Vision-Language-Action models that survives user fine-tuning, maintaining high attack success rates while preserving performance.",
      "teaser_image": "https://arxiv.org/html/2602.00500/x2.png",
      "debug_abstract": "Vision-Language-Action (VLA) models have become foundational to modern embodied AI systems. By integrating visual perception, language understanding, and action planning, they enable general-purpose task execution across diverse environments. Despite their importance, the security of VLA models remains underexplored -- particularly in the context of backdoor attacks, which pose realistic threats in physical-world deployments. While recent methods attempt to inject backdoors into VLA models, these backdoors are easily erased during downstream adaptation, as user-side fine-tuning with clean data significantly alters model parameters, rendering them impractical for real-world applications. To address these challenges, we propose INFUSE (INjection into Fine-tUne-inSensitive modulEs), the first backdoor attack framework for VLA base models that remains effective even with arbitrary user fine-tuning. INFUSE begins by analyzing parameter sensitivity across diverse fine-tuning scenarios to identify modules that remain largely unchanged -- the fine-tune-insensitive modules. It then injects backdoors into these stable modules while freezing the rest, ensuring malicious behavior persists after extensive user fine-tuning. Comprehensive experiments across multiple VLA architectures demonstrate INFUSE&#39;s effectiveness. After user-side fine-tuning, INFUSE maintains mean attack success rates of 91.0% on simulation environments and 79.8% on real-world robot tasks, substantially surpassing BadVLA (38.8% and 36.6%, respectively), while preserving clean-task performance comparable to standard models. These results uncover a critical threat: backdoors implanted before distribution can persist through fine-tuning and remain effective at deployment."
    },
    {
      "title": "APEX: A Decoupled Memory-based Explorer for Asynchronous Aerial Object Goal Navigation",
      "url": "https://arxiv.org/abs/2602.00551",
      "date": "2026-01-31",
      "authors_text": "Daoxuan Zhang, Ping Chen, Xiaobo Xia, Xiu Su, Ruichen Zhen",
      "is_highlight": false,
      "score": 88.0,
      "summary": "APEX is a novel hierarchical agent that enhances UAV navigation through efficient exploration and target acquisition using dynamic memory, reinforcement learning, and open-vocabulary detection.",
      "teaser_image": "https://arxiv.org/html/2602.00551/x1.png",
      "debug_abstract": "Aerial Object Goal Navigation, a challenging frontier in Embodied AI, requires an Unmanned Aerial Vehicle (UAV) agent to autonomously explore, reason, and identify a specific target using only visual perception and language description. However, existing methods struggle with the memorization of complex spatial representations in aerial environments, reliable and interpretable action decision-making, and inefficient exploration and information gathering. To address these challenges, we introduce \\textbf{APEX} (Aerial Parallel Explorer), a novel hierarchical agent designed for efficient exploration and target acquisition in complex aerial settings. APEX is built upon a modular, three-part architecture: 1) Dynamic Spatio-Semantic Mapping Memory, which leverages the zero-shot capability of a Vision-Language Model (VLM) to dynamically construct high-resolution 3D Attraction, Exploration, and Obstacle maps, serving as an interpretable memory mechanism. 2) Action Decision Module, trained with reinforcement learning, which translates this rich spatial understanding into a fine-grained and robust control policy. 3) Target Grounding Module, which employs an open-vocabulary detector to achieve definitive and generalizable target identification. All these components are integrated into a hierarchical, asynchronous, and parallel framework, effectively bypassing the VLM&#39;s inference latency and boosting the agent&#39;s proactivity in exploration. Extensive experiments show that APEX outperforms the previous state of the art by +4.2\\% SR and +2.8\\% SPL on challenging UAV-ON benchmarks, demonstrating its superior efficiency and the effectiveness of its hierarchical asynchronous design. Our source code is provided in \\href{this https URL}{GitHub}"
    },
    {
      "title": "Self-Guard: Defending Large Reasoning Models via enhanced self-reflection",
      "url": "https://arxiv.org/abs/2602.00707",
      "date": "2026-01-31",
      "authors_text": "Jingnan Zheng, Jingjun Xu, Yanzhen Luo, Chenhang Cui, Gelei Deng",
      "is_highlight": false,
      "score": 50.0,
      "summary": "Self-Guard is a lightweight framework enhancing safety compliance in Large Reasoning Models by promoting self-reflection and mitigating risks without sacrificing model utility.",
      "teaser_image": "https://arxiv.org/html/2602.00707/materials/workflow.png",
      "debug_abstract": "The emergence of Large Reasoning Models (LRMs) introduces a new paradigm of explicit reasoning, enabling remarkable advances yet posing unique risks such as reasoning manipulation and information leakage. To mitigate these risks, current alignment strategies predominantly rely on heavy post-training paradigms or external interventions. However, these approaches are often computationally intensive and fail to address the inherent awareness-compliance gap, a critical misalignment where models recognize potential risks yet prioritize following user instructions due to their sycophantic tendencies. To address these limitations, we propose Self-Guard, a lightweight safety defense framework that reinforces safety compliance at the representational level. Self-Guard operates through two principal stages: (1) safety-oriented prompting, which activates the model&#39;s latent safety awareness to evoke spontaneous reflection, and (2) safety activation steering, which extracts the resulting directional shift in the hidden state space and amplifies it to ensure that safety compliance prevails over sycophancy during inference. Experiments demonstrate that Self-Guard effectively bridges the awareness-compliance gap, achieving robust safety performance without compromising model utility. Furthermore, Self-Guard exhibits strong generalization across diverse unseen risks and varying model scales, offering a cost-efficient solution for LRM safety alignment."
    },
    {
      "title": "Unveiling the Cognitive Compass: Theory-of-Mind-Guided Multimodal Emotion Reasoning",
      "url": "https://arxiv.org/abs/2602.00971",
      "date": "2026-02-01",
      "authors_text": "Meng Luo, Bobo Li, Shanqing Xu, Shize Zhang, Qiuchan Chen",
      "is_highlight": false,
      "score": 62.0,
      "summary": "The paper presents HitEmotion, a benchmark for assessing and enhancing multimodal large language models' emotional reasoning through Theory of Mind-guided methods and reinforcement learning.",
      "teaser_image": "https://arxiv.org/html/2602.00971/figures/teaser.png",
      "debug_abstract": "Despite rapid progress in multimodal large language models (MLLMs), their capability for deep emotional understanding remains limited. We argue that genuine affective intelligence requires explicit modeling of Theory of Mind (ToM), the cognitive substrate from which emotions arise. To this end, we introduce HitEmotion, a ToM-grounded hierarchical benchmark that diagnoses capability breakpoints across increasing levels of cognitive depth. Second, we propose a ToM-guided reasoning chain that tracks mental states and calibrates cross-modal evidence to achieve faithful emotional reasoning. We further introduce TMPO, a reinforcement learning method that uses intermediate mental states as process-level supervision to guide and strengthen model reasoning. Extensive experiments show that HitEmotion exposes deep emotional reasoning deficits in state-of-the-art models, especially on cognitively demanding tasks. In evaluation, the ToM-guided reasoning chain and TMPO improve end-task accuracy and yield more faithful, more coherent rationales. In conclusion, our work provides the research community with a practical toolkit for evaluating and enhancing the cognition-based emotional understanding capabilities of MLLMs. Our dataset and code are available at: this https URL."
    },
    {
      "title": "Do All Individual Layers Help? An Empirical Study of Task-Interfering Layers in Vision-Language Models",
      "url": "https://arxiv.org/abs/2602.01167",
      "date": "2026-02-01",
      "authors_text": "Zhiming Liu, Yujie Wei, Lei Feng, Xiu Su, Xiaobo Xia",
      "is_highlight": false,
      "score": 52.0,
      "summary": "This study identifies Task-Interfering Layers in vision-language models, demonstrating that selectively bypassing certain layers can enhance task performance without retraining.",
      "teaser_image": "https://arxiv.org/html/2602.01167/x1.png",
      "debug_abstract": "Current VLMs have demonstrated capabilities across a wide range of multimodal tasks. Typically, in a pretrained VLM, all layers are engaged by default to make predictions on downstream tasks. We find that intervening on a single layer, such as by zeroing its parameters, can improve the performance on certain tasks, indicating that some layers hinder rather than help downstream tasks. We systematically investigate how individual layers influence different tasks via layer intervention. Specifically, we measure the change in performance relative to the base model after intervening on each layer and observe improvements when bypassing specific layers. This improvement can be generalizable across models and datasets, indicating the presence of Task-Interfering Layers that harm downstream tasks&#39; performance. We introduce Task-Layer Interaction Vector, which quantifies the effect of intervening on each layer of a VLM given a task. These task-interfering layers exhibit task-specific sensitivity patterns: tasks requiring similar capabilities show consistent response trends under layer interventions, as evidenced by the high similarity in their task-layer interaction vectors. Inspired by these findings, we propose TaLo (Task-Adaptive Layer Knockout), a training-free, test-time adaptation method that dynamically identifies and bypasses the most interfering layer for a given task. Without parameter updates, TaLo improves performance across various models and datasets, including boosting Qwen-VL&#39;s accuracy on the Maps task in ScienceQA by up to 16.6%. Our work reveals an unexpected form of modularity in pretrained VLMs and provides a plug-and-play, training-free mechanism to unlock hidden capabilities at inference time. The source code will be publicly available."
    },
    {
      "title": "PRISM: Festina Lente Proactivity -- Risk-Sensitive, Uncertainty-Aware Deliberation for Proactive Agents",
      "url": "https://arxiv.org/abs/2602.01532",
      "date": "2026-02-02",
      "authors_text": "Yuxuan Fu, Xiaoyu Tan, Teqi Hao, Chen Zhan, Xihe Qiu",
      "is_highlight": false,
      "score": 45.0,
      "summary": "PRISM introduces a decision-theoretic framework for proactive agents that optimizes intervention timing and accuracy, significantly reducing false alarms and enhancing performance on ProactiveBench.",
      "teaser_image": "https://arxiv.org/html/2602.01532/fig2.png",
      "debug_abstract": "Proactive agents must decide not only what to say but also whether and when to intervene. Many current systems rely on brittle heuristics or indiscriminate long reasoning, which offers little control over the benefit-burden tradeoff. We formulate the problem as cost-sensitive selective intervention and present PRISM, a novel framework that couples a decision-theoretic gate with a dual-process reasoning architecture. At inference time, the agent intervenes only when a calibrated probability of user acceptance exceeds a threshold derived from asymmetric costs of missed help and false alarms. Inspired by festina lente (Latin: &#34;make haste slowly&#34;), we gate by an acceptance-calibrated, cost-derived threshold and invoke a resource-intensive Slow mode with counterfactual checks only near the decision boundary, concentrating computation on ambiguous and high-stakes cases. Training uses gate-aligned, schema-locked distillation: a teacher running the full PRISM pipeline provides dense, executable supervision on unlabeled interaction traces, while the student learns a response policy that is explicitly decoupled from the intervention gate to enable tunable and auditable control. On ProactiveBench, PRISM reduces false alarms by 22.78% and improves F1 by 20.14% over strong baselines. These results show that principled decision-theoretic gating, paired with selective slow reasoning and aligned distillation, yields proactive agents that are precise, computationally efficient, and controllable. To facilitate reproducibility, we release our code, models, and resources at this https URL all experiments use the open-source ProactiveBench benchmark."
    },
    {
      "title": "ReCALL: Recalibrating Capability Degradation for MLLM-based Composed Image Retrieval",
      "url": "https://arxiv.org/abs/2602.01639",
      "date": "2026-02-02",
      "authors_text": "Tianyu Yang, ChenWei He, Xiangzhao Hao, Tianyue Wang, Jiarui Guo",
      "is_highlight": false,
      "score": 42.0,
      "summary": "ReCALL addresses capability degradation in MLLM-based Composed Image Retrieval by diagnosing issues, generating corrective instructions, and refining the retriever through continual training.",
      "teaser_image": "https://arxiv.org/html/2602.01639/x1.png",
      "debug_abstract": "Composed Image Retrieval (CIR) aims to retrieve target images based on a hybrid query comprising a reference image and a modification text. Early dual-tower Vision-Language Models (VLMs) struggle with cross-modality compositional reasoning required for this task. Recently, adapting generative Multimodal Large Language Models (MLLMs) for retrieval offers a promising direction. However, we identify that this adaptation strategy overlooks a fundamental issue: adapting a generative MLLM into a single-embedding discriminative retriever triggers a paradigm conflict, which leads to Capability Degradation - the deterioration of native fine-grained reasoning after retrieval adaptation. To address this challenge, we propose ReCALL (Recalibrating Capability Degradation), a model-agnostic framework that follows a diagnose-generate-refine pipeline: Firstly, we diagnose cognitive blind spots of the retriever via self-guided informative instance mining. Next, we generate corrective instructions and triplets by CoT prompting the foundation MLLM and conduct quality control with VQA-based consistency filtering. Finally, we refine the retriever through continual training on these triplets with a grouped contrastive scheme, thereby internalizing fine-grained visual-semantic distinctions and realigning the discriminative embedding space of retriever with intrinsic compositional reasoning within the MLLM. Extensive experiments on CIRR and FashionIQ show that ReCALL consistently recalibrates degraded capabilities and achieves state-of-the-art performance. Code will be released soon."
    },
    {
      "title": "Beyond Precision: Training-Inference Mismatch is an Optimization Problem and Simple LR Scheduling Fixes It",
      "url": "https://arxiv.org/abs/2602.01826",
      "date": "2026-02-02",
      "authors_text": "Yaxiang Zhang, Yingru Li, Jiacai Liu, Jiawei Xu, Ziniu Li",
      "is_highlight": false,
      "score": 35.0,
      "summary": "This paper presents a dynamic learning rate scheduling method to mitigate training-inference mismatch and stabilize reinforcement learning in large language models by addressing gradient noise.",
      "teaser_image": "https://arxiv.org/html/2602.01826/Qwen3-4B_baseline_vs_importance_sampling_patches.png",
      "debug_abstract": "Reinforcement Learning (RL) for training Large Language Models is notoriously unstable. While recent studies attribute this to &#34;training inference mismatch stemming&#34; from inconsistent hybrid engines, standard remedies, such as Importance Sampling, might fail during extended training runs. In this work, we analyze this instability through the lens of optimization, demonstrating that gradient noise and training-inference mismatch escalate in tandem as training progresses. Meanwhile, we find that the mismatch can be effectively suppressed by shrinking the update size. Taken together, we deduce that the mismatch is not merely a static numerical discrepancy, but a dynamic failure coupled with the model&#39;s optimization. Based on this insight, we propose a simple yet effective solution: a specialized Learning Rate (LR) scheduler. Instead of pre-defined decay schedule in traditional LR scheduler, our method dynamically triggers LR decay based on response length, which we identify as a reliable early-warning signal for impending instability. Empirical evidence suggests that by reducing the learning rate as gradient noise rises, we can consistently stabilize RL training and keep the training-inference mismatch at a safe level."
    },
    {
      "title": "UrbanGS: A Scalable and Efficient Architecture for Geometrically Accurate Large-Scene Reconstruction",
      "url": "https://arxiv.org/abs/2602.02089",
      "date": "2026-02-02",
      "authors_text": "Changbai Li, Haodong Zhu, Hanlin Chen, Xiuping Liang, Tongfei Chen",
      "is_highlight": false,
      "score": 22.0,
      "summary": "UrbanGS is a scalable framework that enhances geometric accuracy and memory efficiency in large-scale urban scene reconstruction through innovative regularization and adaptive Gaussian pruning techniques.",
      "teaser_image": "https://arxiv.org/html/2602.02089/figure/main.png",
      "debug_abstract": "While 3D Gaussian Splatting (3DGS) enables high-quality, real-time rendering for bounded scenes, its extension to large-scale urban environments gives rise to critical challenges in terms of geometric consistency, memory efficiency, and computational scalability. To address these issues, we present UrbanGS, a scalable reconstruction framework that effectively tackles these challenges for city-scale applications. First, we propose a Depth-Consistent D-Normal Regularization module. Unlike existing approaches that rely solely on monocular normal estimators, which can effectively update rotation parameters yet struggle to update position parameters, our method integrates D-Normal constraints with external depth supervision. This allows for comprehensive updates of all geometric parameters. By further incorporating an adaptive confidence weighting mechanism based on gradient consistency and inverse depth deviation, our approach significantly enhances multi-view depth alignment and geometric coherence, which effectively resolves the issue of geometric accuracy in complex large-scale scenes. To improve scalability, we introduce a Spatially Adaptive Gaussian Pruning (SAGP) strategy, which dynamically adjusts Gaussian density based on local geometric complexity and visibility to reduce redundancy. Additionally, a unified partitioning and view assignment scheme is designed to eliminate boundary artifacts and optimize computational load. Extensive experiments on multiple urban datasets demonstrate that UrbanGS achieves superior performance in rendering quality, geometric accuracy, and memory efficiency, providing a systematic solution for high-fidelity large-scale scene reconstruction."
    },
    {
      "title": "TIDE: Trajectory-based Diagnostic Evaluation of Test-Time Improvement in LLM Agents",
      "url": "https://arxiv.org/abs/2602.02196",
      "date": "2026-02-02",
      "authors_text": "Hang Yan, Xinyu Che, Fangzhi Xu, Qiushi Sun, Zichen Ding",
      "is_highlight": false,
      "score": 20.0,
      "summary": "The TIDE framework evaluates Test-Time Improvement in LLM agents by analyzing task completion dynamics, recursive behaviors, and memory constraints to enhance performance optimization.",
      "teaser_image": "https://arxiv.org/html/2602.02196/x1.png",
      "debug_abstract": "Recent advances in autonomous LLM agents demonstrate their ability to improve performance through iterative interaction with the environment. We define this paradigm as Test-Time Improvement (TTI). However, the mechanisms under how and why TTI succeed or fail remain poorly understood, and existing evaluation metrics fail to capture their task optimization efficiency, behavior adaptation after erroneous actions, and the specific utility of working memory for task completion. To address these gaps, we propose Test-time Improvement Diagnostic Evaluation (TIDE), an agent-agnostic and environment-agnostic framework that decomposes TTI into three comprehensive and interconnected dimensions. The framework measures (1) the overall temporal dynamics of task completion and (2) identifies whether performance is primarily constrained by recursive looping behaviors or (3) by burdensome accumulated memory. Through extensive experiments across diverse agents and environments, TIDE highlights that improving agent performance requires more than scaling internal reasoning, calling for explicitly optimizing the interaction dynamics between the agent and the environment."
    },
    {
      "title": "A Low-Cost Vision-Based Tactile Gripper with Pretraining Learning for Contact-Rich Manipulation",
      "url": "https://arxiv.org/abs/2602.00514",
      "date": "2026-01-31",
      "authors_text": "Yaohua Liu, Binkai Ou, Zicheng Qiu, Ce Hao, Yemin Wang",
      "is_highlight": false,
      "score": 11.0,
      "summary": "The LVTG gripper enhances contact-rich robotic manipulation through low-cost, durable visuo-tactile integration and improved policy learning via contrastive pretraining, achieving higher success rates.",
      "teaser_image": "https://arxiv.org/html/2602.00514/x1.png",
      "debug_abstract": "Robotic manipulation in contact-rich environments remains challenging, particularly when relying on conventional tactile sensors that suffer from limited sensing range, reliability, and cost-effectiveness. In this work, we present LVTG, a low-cost visuo-tactile gripper designed for stable, robust, and efficient physical interaction. Unlike existing visuo-tactile sensors, LVTG enables more effective and stable grasping of larger and heavier everyday objects, thanks to its enhanced tactile sensing area and greater opening angle. Its surface skin is made of highly wear-resistant material, significantly improving durability and extending operational lifespan. The integration of vision and tactile feedback allows LVTG to provide rich, high-fidelity sensory data, facilitating reliable perception during complex manipulation tasks. Furthermore, LVTG features a modular design that supports rapid maintenance and replacement. To effectively fuse vision and touch, We adopt a CLIP-inspired contrastive learning objective to align tactile embeddings with their corresponding visual observations, enabling a shared cross-modal representation space for visuo-tactile perception. This alignment improves the performance of an Action Chunking Transformer (ACT) policy in contact-rich manipulation, leading to more efficient data collection and more effective policy learning. Compared to the original ACT method, the proposed LVTG with pretraining achieves significantly higher success rates in manipulation tasks."
    },
    {
      "title": "FD-VLA: Force-Distilled Vision-Language-Action Model for Contact-Rich Manipulation",
      "url": "https://arxiv.org/abs/2602.02142",
      "date": "2026-02-02",
      "authors_text": "Ruiteng Zhao, Wenshuo Wang, Yicheng Ma, Xiaocong Li, Francis E. H. Tay",
      "is_highlight": false,
      "score": 0,
      "summary": "The FD-VLA framework enhances contact-rich manipulation by integrating force awareness through a novel distillation method, improving performance without physical force sensors.",
      "teaser_image": "https://arxiv.org/html/2602.02142/x1.png",
      "debug_abstract": "Force sensing is a crucial modality for Vision-Language-Action (VLA) frameworks, as it enables fine-grained perception and dexterous manipulation in contact-rich tasks. We present Force-Distilled VLA (FD-VLA), a novel framework that integrates force awareness into contact-rich manipulation without relying on physical force sensors. The core of our approach is a Force Distillation Module (FDM), which distills force by mapping a learnable query token, conditioned on visual observations and robot states, into a predicted force token aligned with the latent representation of actual force signals. During inference, this distilled force token is injected into the pretrained VLM, enabling force-aware reasoning while preserving the integrity of its vision-language semantics. This design provides two key benefits: first, it allows practical deployment across a wide range of robots that lack expensive or fragile force-torque sensors, thereby reducing hardware cost and complexity; second, the FDM introduces an additional force-vision-state fusion prior to the VLM, which improves cross-modal alignment and enhances perception-action robustness in contact-rich scenarios. Surprisingly, our physical experiments show that the distilled force token outperforms direct sensor force measurements as well as other baselines, which highlights the effectiveness of this force-distilled VLA approach."
    },
    {
      "title": "Abstracting Robot Manipulation Skills via Mixture-of-Experts Diffusion Policies",
      "url": "https://arxiv.org/abs/2601.21251",
      "date": "2026-01-29",
      "authors_text": "Ce Hao, Xuanran Zhai, Yaohua Liu, Harold Soh",
      "is_highlight": false,
      "score": 90.0,
      "summary": "The Skill Mixture-of-Experts Policy (SMP) enhances robot manipulation by efficiently utilizing a compact skill basis and adaptive expert activation for improved multi-task performance.",
      "teaser_image": "https://arxiv.org/html/2601.21251/x1.png",
      "debug_abstract": "Diffusion-based policies have recently shown strong results in robot manipulation, but their extension to multi-task scenarios is hindered by the high cost of scaling model size and demonstrations. We introduce Skill Mixture-of-Experts Policy (SMP), a diffusion-based mixture-of-experts policy that learns a compact orthogonal skill basis and uses sticky routing to compose actions from a small, task-relevant subset of experts at each step. A variational training objective supports this design, and adaptive expert activation at inference yields fast sampling without oversized backbones. We validate SMP in simulation and on a real dual-arm platform with multi-task learning and transfer learning tasks, where SMP achieves higher success rates and markedly lower inference cost than large diffusion baselines. These results indicate a practical path toward scalable, transferable multi-task manipulation: learn reusable skills once, activate only what is needed, and adapt quickly when tasks change."
    },
    {
      "title": "TraceRouter: Robust Safety for Large Foundation Models via Path-Level Intervention",
      "url": "https://arxiv.org/abs/2601.21900",
      "date": "2026-01-29",
      "authors_text": "Chuancheng Shi, Shangze Li, Wenjun Lu, Wenhua Wu, Cong Wang",
      "is_highlight": false,
      "score": 73.0,
      "summary": "TraceRouter enhances the safety of large foundation models by disconnecting harmful semantic pathways through a novel path-level intervention framework, outperforming existing defenses.",
      "teaser_image": "https://arxiv.org/html/2601.21900/x8.png",
      "debug_abstract": "Despite their capabilities, large foundation models (LFMs) remain susceptible to adversarial manipulation. Current defenses predominantly rely on the &#34;locality hypothesis&#34;, suppressing isolated neurons or features. However, harmful semantics act as distributed, cross-layer circuits, rendering such localized interventions brittle and detrimental to utility. To bridge this gap, we propose \\textbf{TraceRouter}, a path-level framework that traces and disconnects the causal propagation circuits of illicit semantics. TraceRouter operates in three stages: (1) it pinpoints a sensitive onset layer by analyzing attention divergence; (2) it leverages sparse autoencoders (SAEs) and differential activation analysis to disentangle and isolate malicious features; and (3) it maps these features to downstream causal pathways via feature influence scores (FIS) derived from zero-out interventions. By selectively suppressing these causal chains, TraceRouter physically severs the flow of harmful information while leaving orthogonal computation routes intact. Extensive experiments demonstrate that TraceRouter significantly outperforms state-of-the-art baselines, achieving a superior trade-off between adversarial robustness and general utility. Our code will be publicly released. WARNING: This paper contains unsafe model responses."
    },
    {
      "title": "GPO: Growing Policy Optimization for Legged Robot Locomotion and Whole-Body Control",
      "url": "https://arxiv.org/abs/2601.20668",
      "date": "2026-01-28",
      "authors_text": "Shuhao Liao, Peizhuo Li, Xinrong Yang, Linnan Chang, Zhaoxin Fan",
      "is_highlight": false,
      "score": 92.0,
      "summary": "The paper presents Growing Policy Optimization (GPO), a framework that enhances reinforcement learning for legged robots by progressively expanding the action space to improve data collection and policy performance.",
      "teaser_image": "https://arxiv.org/html/2601.20668/pic/framework.png",
      "debug_abstract": "Training reinforcement learning (RL) policies for legged robots remains challenging due to high-dimensional continuous actions, hardware constraints, and limited exploration. Existing methods for locomotion and whole-body control work well for position-based control with environment-specific heuristics (e.g., reward shaping, curriculum design, and manual initialization), but are less effective for torque-based control, where sufficiently exploring the action space and obtaining informative gradient signals for training is significantly more difficult. We introduce Growing Policy Optimization (GPO), a training framework that applies a time-varying action transformation to restrict the effective action space in the early stage, thereby encouraging more effective data collection and policy learning, and then progressively expands it to enhance exploration and achieve higher expected return. We prove that this transformation preserves the PPO update rule and introduces only bounded, vanishing gradient distortion, thereby ensuring stable training. We evaluate GPO on both quadruped and hexapod robots, including zero-shot deployment of simulation-trained policies on hardware. Policies trained with GPO consistently achieve better performance. These results suggest that GPO provides a general, environment-agnostic optimization framework for learning legged locomotion."
    },
    {
      "title": "Curiosity Driven Knowledge Retrieval for Mobile Agents",
      "url": "https://arxiv.org/abs/2601.19306",
      "date": "2026-01-27",
      "authors_text": "Sijia Li, Xiaoyu Tan, Shahir Ali, Niels Schmidt, Gengchen Ma",
      "is_highlight": false,
      "score": 80.0,
      "summary": "The paper presents a curiosity-driven framework for mobile agents that enhances performance by retrieving and integrating structured knowledge (AppCards) when faced with uncertainty, achieving improved planning reliability.",
      "debug_abstract": "Mobile agents have made progress toward reliable smartphone automation, yet performance in complex applications remains limited by incomplete knowledge and weak generalization to unseen environments. We introduce a curiosity driven knowledge retrieval framework that formalizes uncertainty during execution as a curiosity score. When this score exceeds a threshold, the system retrieves external information from documentation, code repositories, and historical trajectories. Retrieved content is organized into structured AppCards, which encode functional semantics, parameter conventions, interface mappings, and interaction patterns. During execution, an enhanced agent selectively integrates relevant AppCards into its reasoning process, thereby compensating for knowledge blind spots and improving planning reliability. Evaluation on the AndroidWorld benchmark shows consistent improvements across backbones, with an average gain of six percentage points and a new state of the art success rate of 88.8\\% when combined with GPT-5. Analysis indicates that AppCards are particularly effective for multi step and cross application tasks, while improvements depend on the backbone model. Case studies further confirm that AppCards reduce ambiguity, shorten exploration, and support stable execution trajectories. Task trajectories are publicly available at this https URL."
    },
    {
      "title": "Video-KTR: Reinforcing Video Reasoning via Key Token Attribution",
      "url": "https://arxiv.org/abs/2601.19686",
      "date": "2026-01-27",
      "authors_text": "Ziyue Wang, Sheng Jin, Zhongrong Zuo, Jiawei Wu, Han Qiu",
      "is_highlight": false,
      "score": 66.0,
      "summary": "Video-KTR enhances video reasoning in multimodal models by selectively reinforcing key tokens through a novel attribution framework, achieving state-of-the-art performance across multiple benchmarks.",
      "debug_abstract": "Reinforcement learning (RL) has shown strong potential for enhancing reasoning in multimodal large language models, yet existing video reasoning methods often rely on coarse sequence-level rewards or single-factor token selection, neglecting fine-grained links among visual inputs, temporal dynamics, and linguistic outputs, limiting both accuracy and interpretability. We propose Video-KTR, a modality-aware policy shaping framework that performs selective, token-level RL by combining three attribution signals: (1) visual-aware tokens identified via counterfactual masking to reveal perceptual dependence; (2) temporal-aware tokens detected through frame shuffling to expose temporal sensitivity; and (3) high-entropy tokens signaling predictive uncertainty. By reinforcing only these key tokens, Video-KTR focuses learning on semantically informative, modality-sensitive content while filtering out low-value tokens. Across five challenging benchmarks, Video-KTR achieves state-of-the-art or highly competitive results, achieving 42.7\\% on Video-Holmes (surpassing GPT-4o) with consistent gains on both reasoning and general video understanding tasks. Ablation studies verify the complementary roles of the attribution signals and the robustness of targeted token-level updates. Overall, Video-KTR improves accuracy and interpretability, offering a simple, drop-in extension to RL for complex video reasoning. Our code and models are available at this https URL."
    },
    {
      "title": "ORION: Option-Regularized Deep Reinforcement Learning for Cooperative Multi-Agent Online Navigation",
      "url": "https://arxiv.org/abs/2601.01155",
      "date": "2026-01-26",
      "authors_text": "Shizhe Zhang, Jingsong Liang, Zhitao Zhou, Shuhan Ye, Yizhuo Wang",
      "is_highlight": false,
      "score": 88.0,
      "summary": "ORION is a deep reinforcement learning framework that enables decentralized, cooperative navigation for multi-agent systems in partially known environments by integrating prior maps and online observations.",
      "debug_abstract": "Existing methods for multi-agent navigation typically assume fully known environments, offering limited support for partially known scenarios such as warehouses or factory floors. There, agents may need to plan trajectories that balance their own path optimality with their ability to collect and share information about the environment that can help their teammates reach their own goals. To these ends, we propose ORION, a novel deep reinforcement learning framework for cooperative multi-agent online navigation in partially known environments. Starting from an imperfect prior map, ORION trains agents to make decentralized decisions, coordinate to reach their individual targets, and actively reduce map uncertainty by sharing online observations in a closed perception-action loop. We first design a shared graph encoder that fuses prior map with online perception into a unified representation, providing robust state embeddings under dynamic map discrepancies. At the core of ORION is an option-critic framework that learns to reason about a set of high-level cooperative modes that translate into sequences of low-level actions, allowing agents to switch between individual navigation and team-level exploration adaptively. We further introduce a dual-stage cooperation strategy that enables agents to assist teammates under map uncertainty, thereby reducing the overall makespan. Across extensive maze-like maps and large-scale warehouse environments, our simulation results show that ORION achieves high-quality, real-time decentralized cooperation over varying team sizes, outperforming state-of-the-art classical and learning-based baselines. Finally, we validate ORION on physical robot teams, demonstrating its robustness and practicality for real-world cooperative navigation."
    },
    {
      "title": "Fast and Safe Trajectory Optimization for Mobile Manipulators With Neural Configuration Space Distance Field",
      "url": "https://arxiv.org/abs/2601.18548",
      "date": "2026-01-26",
      "authors_text": "Yulin Li, Zhiyuan Song, Yiming Li, Zhicheng Song, Kai Chen",
      "is_highlight": false,
      "score": 86.0,
      "summary": "This paper introduces Generalized Configuration Space Distance Fields (GCDF) for efficient trajectory optimization in mobile manipulators, enhancing collision reasoning and enabling agile motion in complex environments.",
      "debug_abstract": "Mobile manipulators promise agile, long-horizon behavior by coordinating base and arm motion, yet whole-body trajectory optimization in cluttered, confined spaces remains difficult due to high-dimensional nonconvexity and the need for fast, accurate collision reasoning. Configuration Space Distance Fields (CDF) enable fixed-base manipulators to model collisions directly in configuration space via smooth, implicit distances. This representation holds strong potential to bypass the nonlinear configuration-to-workspace mapping while preserving accurate whole-body geometry and providing optimization-friendly collision costs. Yet, extending this capability to mobile manipulators is hindered by unbounded workspaces and tighter base-arm coupling. We lift this promise to mobile manipulation with Generalized Configuration Space Distance Fields (GCDF), extending CDF to robots with both translational and rotational joints in unbounded workspaces with tighter base-arm coupling. We prove that GCDF preserves Euclidean-like local distance structure and accurately encodes whole-body geometry in configuration space, and develop a data generation and training pipeline that yields continuous neural GCDFs with accurate values and gradients, supporting efficient GPU-batched queries. Building on this representation, we develop a high-performance sequential convex optimization framework centered on GCDF-based collision reasoning. The solver scales to large numbers of implicit constraints through (i) online specification of neural constraints, (ii) sparsity-aware active-set detection with parallel batched evaluation across thousands of constraints, and (iii) incremental constraint management for rapid replanning under scene changes."
    },
    {
      "title": "AdaReasoner: Dynamic Tool Orchestration for Iterative Visual Reasoning",
      "url": "https://arxiv.org/abs/2601.18631",
      "date": "2026-01-26",
      "authors_text": "Mingyang Song, Haoyu Sun, Jiawei Gu, Linjie Li, Luxin Xu",
      "is_highlight": false,
      "score": 61.0,
      "summary": "AdaReasoner is a multimodal model that autonomously optimizes tool selection and usage for iterative visual reasoning, achieving state-of-the-art performance on various benchmarks.",
      "debug_abstract": "When humans face problems beyond their immediate capabilities, they rely on tools, providing a promising paradigm for improving visual reasoning in multimodal large language models (MLLMs). Effective reasoning, therefore, hinges on knowing which tools to use, when to invoke them, and how to compose them over multiple steps, even when faced with new tools or new tasks. We introduce \\textbf{AdaReasoner}, a family of multimodal models that learn tool use as a general reasoning skill rather than as tool-specific or explicitly supervised behavior. AdaReasoner is enabled by (i) a scalable data curation pipeline exposing models to long-horizon, multi-step tool interactions; (ii) Tool-GRPO, a reinforcement learning algorithm that optimizes tool selection and sequencing based on end-task success; and (iii) an adaptive learning mechanism that dynamically regulates tool usage. Together, these components allow models to infer tool utility from task context and intermediate outcomes, enabling coordination of multiple tools and generalization to unseen tools. Empirically, AdaReasoner exhibits strong tool-adaptive and generalization behaviors: it autonomously adopts beneficial tools, suppresses irrelevant ones, and adjusts tool usage frequency based on task demands, despite never being explicitly trained to do so. These capabilities translate into state-of-the-art performance across challenging benchmarks, improving the 7B base model by +24.9\\% on average and surpassing strong proprietary systems such as GPT-5 on multiple tasks, including VSP and Jigsaw."
    },
    {
      "title": "Emotion-LLaMAv2 and MMEVerse: A New Framework and Benchmark for Multimodal Emotion Understanding",
      "url": "https://arxiv.org/abs/2601.16449",
      "date": "2026-01-23",
      "authors_text": "Xiaojiang Peng, Jingyi Chen, Zebang Cheng, Bao Peng, Fengyi Wu",
      "is_highlight": false,
      "score": 58.0,
      "summary": "Emotion-LLaMAv2 and MMEVerse introduce a comprehensive framework and benchmark for enhanced multimodal emotion understanding through advanced encoding, feature interaction, and large-scale dataset integration.",
      "debug_abstract": "Understanding human emotions from multimodal signals poses a significant challenge in affective computing and human-robot interaction. While multimodal large language models (MLLMs) have excelled in general vision-language tasks, their capabilities in emotional reasoning remain limited. The field currently suffers from a scarcity of large-scale datasets with high-quality, descriptive emotion annotations and lacks standardized benchmarks for evaluation. Our preliminary framework, Emotion-LLaMA, pioneered instruction-tuned multimodal learning for emotion reasoning but was restricted by explicit face detectors, implicit fusion strategies, and low-quality training data with limited scale. To address these limitations, we present Emotion-LLaMAv2 and the MMEVerse benchmark, establishing an end-to-end pipeline together with a standardized evaluation setting for emotion recognition and reasoning. Emotion-LLaMAv2 introduces three key advances. First, an end-to-end multiview encoder eliminates external face detection and captures nuanced emotional cues via richer spatial and temporal multiview tokens. Second, a Conv Attention pre-fusion module is designed to enable simultaneous local and global multimodal feature interactions external to the LLM backbone. Third, a perception-to-cognition curriculum instruction tuning scheme within the LLaMA2 backbone unifies emotion recognition and free-form emotion reasoning. To support large-scale training and reproducible evaluation, MMEVerse aggregates twelve publicly available emotion datasets, including IEMOCAP, MELD, DFEW, and MAFW, into a unified multimodal instruction format. The data are re-annotated via a multi-agent pipeline involving Qwen2 Audio, Qwen2.5 VL, and GPT 4o, producing 130k training clips and 36k testing clips across 18 evaluation benchmarks."
    },
    {
      "title": "Virtual Traffic Police: Large Language Model-Augmented Traffic Signal Control for Unforeseen Incidents",
      "url": "https://arxiv.org/abs/2601.15816",
      "date": "2026-01-22",
      "authors_text": "Shiqi Wei, Qiqing Wang, Kaidi Yang",
      "is_highlight": false,
      "score": 84.0,
      "summary": "The paper presents a hierarchical framework that enhances traditional traffic signal control with Large Language Models to efficiently manage unforeseen traffic incidents.",
      "debug_abstract": "Adaptive traffic signal control (TSC) has demonstrated strong effectiveness in managing dynamic traffic flows. However, conventional methods often struggle when unforeseen traffic incidents occur (e.g., accidents and road maintenance), which typically require labor-intensive and inefficient manual interventions by traffic police officers. Large Language Models (LLMs) appear to be a promising solution thanks to their remarkable reasoning and generalization capabilities. Nevertheless, existing works often propose to replace existing TSC systems with LLM-based systems, which can be (i) unreliable due to the inherent hallucinations of LLMs and (ii) costly due to the need for system replacement. To address the issues of existing works, we propose a hierarchical framework that augments existing TSC systems with LLMs, whereby a virtual traffic police agent at the upper level dynamically fine-tunes selected parameters of signal controllers at the lower level in response to real-time traffic incidents. To enhance domain-specific reliability in response to unforeseen traffic incidents, we devise a self-refined traffic language retrieval system (TLRS), whereby retrieval-augmented generation is employed to draw knowledge from a tailored traffic language database that encompasses traffic conditions and controller operation principles. Moreover, we devise an LLM-based verifier to update the TLRS continuously over the reasoning process. Our results show that LLMs can serve as trustworthy virtual traffic police officers that can adapt conventional TSC methods to unforeseen traffic incidents with significantly improved operational efficiency and reliability."
    },
    {
      "title": "PhysicsMind: Sim and Real Mechanics Benchmarking for Physical Reasoning and Prediction in Foundational VLMs and World Models",
      "url": "https://arxiv.org/abs/2601.16007",
      "date": "2026-01-22",
      "authors_text": "Chak-Wing Mak, Guanyu Zhu, Boyi Zhang, Hongji Li, Xiaowei Chi",
      "is_highlight": false,
      "score": 94.1,
      "summary": "PhysicsMind introduces a benchmark for evaluating physical reasoning in multimodal models through real and simulated tasks focused on fundamental physics principles, revealing significant gaps in current models' understanding.",
      "debug_abstract": "Modern foundational Multimodal Large Language Models (MLLMs) and video world models have advanced significantly in mathematical, common-sense, and visual reasoning, but their grasp of the underlying physics remains underexplored. Existing benchmarks attempting to measure this matter rely on synthetic, Visual Question Answer templates or focus on perceptual video quality that is tangential to measuring how well the video abides by physical laws. To address this fragmentation, we introduce PhysicsMind, a unified benchmark with both real and simulation environments that evaluates law-consistent reasoning and generation over three canonical principles: Center of Mass, Lever Equilibrium, and Newton&#39;s First Law. PhysicsMind comprises two main tasks: i) VQA tasks, testing whether models can reason and determine physical quantities and values from images or short videos, and ii) Video Generation(VG) tasks, evaluating if predicted motion trajectories obey the same center-of-mass, torque, and inertial constraints as the ground truth. A broad range of recent models and video generation models is evaluated on PhysicsMind and found to rely on appearance heuristics while often violating basic mechanics. These gaps indicate that current scaling and training are still insufficient for robust physical understanding, underscoring PhysicsMind as a focused testbed for physics-aware multimodal models. Our data will be released upon acceptance."
    },
    {
      "title": "AION: Aerial Indoor Object-Goal Navigation Using Dual-Policy Reinforcement Learning",
      "url": "https://arxiv.org/abs/2601.15614",
      "date": "2026-01-22",
      "authors_text": "Zichen Yan, Yuchen Hou, Shenao Wang, Yichao Gao, Rui Huang",
      "is_highlight": false,
      "score": 85.6,
      "summary": "AION introduces a dual-policy reinforcement learning framework for aerial object-goal navigation, enhancing exploration and navigation efficiency without external localization or global maps.",
      "debug_abstract": "Object-Goal Navigation (ObjectNav) requires an agent to autonomously explore an unknown environment and navigate toward target objects specified by a semantic label. While prior work has primarily studied zero-shot ObjectNav under 2D locomotion, extending it to aerial platforms with 3D locomotion capability remains underexplored. Aerial robots offer superior maneuverability and search efficiency, but they also introduce new challenges in spatial perception, dynamic control, and safety assurance. In this paper, we propose AION for vision-based aerial ObjectNav without relying on external localization or global maps. AION is an end-to-end dual-policy reinforcement learning (RL) framework that decouples exploration and goal-reaching behaviors into two specialized policies. We evaluate AION on the AI2-THOR benchmark and further assess its real-time performance in IsaacSim using high-fidelity drone models. Experimental results show that AION achieves superior performance across comprehensive evaluation metrics in exploration, navigation efficiency, and safety. The video can be found at this https URL."
    }
  ],
  "Microsystem Engineering and Robotics": [
    {
      "title": "FARTrack: Fast Autoregressive Visual Tracking with High Performance",
      "url": "https://arxiv.org/abs/2602.03214",
      "date": "2026-02-03",
      "authors_text": "Guijie Wang, Tong Lin, Yifan Bai, Anjia Cao, Shiyi Liang",
      "is_highlight": false,
      "score": 79.0,
      "summary": "FARTrack is a fast autoregressive visual tracking framework that enhances inference speed and performance through task-specific self-distillation and inter-frame autoregressive sparsification, achieving 343 FPS on GPU.",
      "teaser_image": "https://arxiv.org/html/2602.03214/x1.png",
      "debug_abstract": "Inference speed and tracking performance are two critical evaluation metrics in the field of visual tracking. However, high-performance trackers often suffer from slow processing speeds, making them impractical for deployment on resource-constrained devices. To alleviate this issue, we propose FARTrack, a Fast Auto-Regressive Tracking framework. Since autoregression emphasizes the temporal nature of the trajectory sequence, it can maintain high performance while achieving efficient execution across various devices. FARTrack introduces Task-Specific Self-Distillation and Inter-frame Autoregressive Sparsification, designed from the perspectives of shallow-yet-accurate distillation and redundant-to-essential token optimization, respectively. Task-Specific Self-Distillation achieves model compression by distilling task-specific tokens layer by layer, enhancing the model&#39;s inference speed while avoiding suboptimal manual teacher-student layer pairs assignments. Meanwhile, Inter-frame Autoregressive Sparsification sequentially condenses multiple templates, avoiding additional runtime overhead while learning a temporally-global optimal sparsification strategy. FARTrack demonstrates outstanding speed and competitive performance. It delivers an AO of 70.6% on GOT-10k in real-time. Beyond, our fastest model achieves a speed of 343 FPS on the GPU and 121 FPS on the CPU."
    },
    {
      "title": "PWAVEP: Purifying Imperceptible Adversarial Perturbations in 3D Point Clouds via Spectral Graph Wavelets",
      "url": "https://arxiv.org/abs/2602.03333",
      "date": "2026-02-03",
      "authors_text": "Haoran Li, Renyang Liu, Hongjia Liu, Chen Wang, Long Yin",
      "is_highlight": false,
      "score": 59.0,
      "summary": "PWAVEP introduces a non-invasive defense mechanism using spectral graph wavelets to purify adversarial perturbations in 3D point clouds, enhancing accuracy and robustness.",
      "teaser_image": "https://arxiv.org/html/2602.03333/x1.png",
      "debug_abstract": "Recent progress in adversarial attacks on 3D point clouds, particularly in achieving spatial imperceptibility and high attack performance, presents significant challenges for defenders. Current defensive approaches remain cumbersome, often requiring invasive model modifications, expensive training procedures or auxiliary data access. To address these threats, in this paper, we propose a plug-and-play and non-invasive defense mechanism in the spectral domain, grounded in a theoretical and empirical analysis of the relationship between imperceptible perturbations and high-frequency spectral components. Building upon these insights, we introduce a novel purification framework, termed PWAVEP, which begins by computing a spectral graph wavelet domain saliency score and local sparsity score for each point. Guided by these values, PWAVEP adopts a hierarchical strategy, it eliminates the most salient points, which are identified as hardly recoverable adversarial outliers. Simultaneously, it applies a spectral filtering process to a broader set of moderately salient points. This process leverages a graph wavelet transform to attenuate high-frequency coefficients associated with the targeted points, thereby effectively suppressing adversarial noise. Extensive evaluations demonstrate that the proposed PWAVEP achieves superior accuracy and robustness compared to existing approaches, advancing the state-of-the-art in 3D point cloud purification. Code and datasets are available at this https URL"
    },
    {
      "title": "Inject Once Survive Later: Backdooring Vision-Language-Action Models to Persist Through Downstream Fine-tuning",
      "url": "https://arxiv.org/abs/2602.00500",
      "date": "2026-01-31",
      "authors_text": "Jianyi Zhou, Yujie Wei, Ruichen Zhen, Bo Zhao, Xiaobo Xia",
      "is_highlight": true,
      "score": 60.0,
      "summary": "The paper introduces INFUSE, a robust backdoor attack framework for Vision-Language-Action models that survives user fine-tuning, maintaining high attack success rates while preserving performance.",
      "teaser_image": "https://arxiv.org/html/2602.00500/x2.png",
      "debug_abstract": "Vision-Language-Action (VLA) models have become foundational to modern embodied AI systems. By integrating visual perception, language understanding, and action planning, they enable general-purpose task execution across diverse environments. Despite their importance, the security of VLA models remains underexplored -- particularly in the context of backdoor attacks, which pose realistic threats in physical-world deployments. While recent methods attempt to inject backdoors into VLA models, these backdoors are easily erased during downstream adaptation, as user-side fine-tuning with clean data significantly alters model parameters, rendering them impractical for real-world applications. To address these challenges, we propose INFUSE (INjection into Fine-tUne-inSensitive modulEs), the first backdoor attack framework for VLA base models that remains effective even with arbitrary user fine-tuning. INFUSE begins by analyzing parameter sensitivity across diverse fine-tuning scenarios to identify modules that remain largely unchanged -- the fine-tune-insensitive modules. It then injects backdoors into these stable modules while freezing the rest, ensuring malicious behavior persists after extensive user fine-tuning. Comprehensive experiments across multiple VLA architectures demonstrate INFUSE&#39;s effectiveness. After user-side fine-tuning, INFUSE maintains mean attack success rates of 91.0% on simulation environments and 79.8% on real-world robot tasks, substantially surpassing BadVLA (38.8% and 36.6%, respectively), while preserving clean-task performance comparable to standard models. These results uncover a critical threat: backdoors implanted before distribution can persist through fine-tuning and remain effective at deployment."
    },
    {
      "title": "APEX: A Decoupled Memory-based Explorer for Asynchronous Aerial Object Goal Navigation",
      "url": "https://arxiv.org/abs/2602.00551",
      "date": "2026-01-31",
      "authors_text": "Daoxuan Zhang, Ping Chen, Xiaobo Xia, Xiu Su, Ruichen Zhen",
      "is_highlight": false,
      "score": 88.0,
      "summary": "APEX is a novel hierarchical agent that enhances UAV navigation through efficient exploration and target acquisition using dynamic memory, reinforcement learning, and open-vocabulary detection.",
      "teaser_image": "https://arxiv.org/html/2602.00551/x1.png",
      "debug_abstract": "Aerial Object Goal Navigation, a challenging frontier in Embodied AI, requires an Unmanned Aerial Vehicle (UAV) agent to autonomously explore, reason, and identify a specific target using only visual perception and language description. However, existing methods struggle with the memorization of complex spatial representations in aerial environments, reliable and interpretable action decision-making, and inefficient exploration and information gathering. To address these challenges, we introduce \\textbf{APEX} (Aerial Parallel Explorer), a novel hierarchical agent designed for efficient exploration and target acquisition in complex aerial settings. APEX is built upon a modular, three-part architecture: 1) Dynamic Spatio-Semantic Mapping Memory, which leverages the zero-shot capability of a Vision-Language Model (VLM) to dynamically construct high-resolution 3D Attraction, Exploration, and Obstacle maps, serving as an interpretable memory mechanism. 2) Action Decision Module, trained with reinforcement learning, which translates this rich spatial understanding into a fine-grained and robust control policy. 3) Target Grounding Module, which employs an open-vocabulary detector to achieve definitive and generalizable target identification. All these components are integrated into a hierarchical, asynchronous, and parallel framework, effectively bypassing the VLM&#39;s inference latency and boosting the agent&#39;s proactivity in exploration. Extensive experiments show that APEX outperforms the previous state of the art by +4.2\\% SR and +2.8\\% SPL on challenging UAV-ON benchmarks, demonstrating its superior efficiency and the effectiveness of its hierarchical asynchronous design. Our source code is provided in \\href{this https URL}{GitHub}"
    },
    {
      "title": "Self-Guard: Defending Large Reasoning Models via enhanced self-reflection",
      "url": "https://arxiv.org/abs/2602.00707",
      "date": "2026-01-31",
      "authors_text": "Jingnan Zheng, Jingjun Xu, Yanzhen Luo, Chenhang Cui, Gelei Deng",
      "is_highlight": false,
      "score": 50.0,
      "summary": "Self-Guard is a lightweight framework enhancing safety compliance in Large Reasoning Models by promoting self-reflection and mitigating risks without sacrificing model utility.",
      "teaser_image": "https://arxiv.org/html/2602.00707/materials/workflow.png",
      "debug_abstract": "The emergence of Large Reasoning Models (LRMs) introduces a new paradigm of explicit reasoning, enabling remarkable advances yet posing unique risks such as reasoning manipulation and information leakage. To mitigate these risks, current alignment strategies predominantly rely on heavy post-training paradigms or external interventions. However, these approaches are often computationally intensive and fail to address the inherent awareness-compliance gap, a critical misalignment where models recognize potential risks yet prioritize following user instructions due to their sycophantic tendencies. To address these limitations, we propose Self-Guard, a lightweight safety defense framework that reinforces safety compliance at the representational level. Self-Guard operates through two principal stages: (1) safety-oriented prompting, which activates the model&#39;s latent safety awareness to evoke spontaneous reflection, and (2) safety activation steering, which extracts the resulting directional shift in the hidden state space and amplifies it to ensure that safety compliance prevails over sycophancy during inference. Experiments demonstrate that Self-Guard effectively bridges the awareness-compliance gap, achieving robust safety performance without compromising model utility. Furthermore, Self-Guard exhibits strong generalization across diverse unseen risks and varying model scales, offering a cost-efficient solution for LRM safety alignment."
    },
    {
      "title": "Unveiling the Cognitive Compass: Theory-of-Mind-Guided Multimodal Emotion Reasoning",
      "url": "https://arxiv.org/abs/2602.00971",
      "date": "2026-02-01",
      "authors_text": "Meng Luo, Bobo Li, Shanqing Xu, Shize Zhang, Qiuchan Chen",
      "is_highlight": false,
      "score": 62.0,
      "summary": "The paper presents HitEmotion, a benchmark for assessing and enhancing multimodal large language models' emotional reasoning through Theory of Mind-guided methods and reinforcement learning.",
      "teaser_image": "https://arxiv.org/html/2602.00971/figures/teaser.png",
      "debug_abstract": "Despite rapid progress in multimodal large language models (MLLMs), their capability for deep emotional understanding remains limited. We argue that genuine affective intelligence requires explicit modeling of Theory of Mind (ToM), the cognitive substrate from which emotions arise. To this end, we introduce HitEmotion, a ToM-grounded hierarchical benchmark that diagnoses capability breakpoints across increasing levels of cognitive depth. Second, we propose a ToM-guided reasoning chain that tracks mental states and calibrates cross-modal evidence to achieve faithful emotional reasoning. We further introduce TMPO, a reinforcement learning method that uses intermediate mental states as process-level supervision to guide and strengthen model reasoning. Extensive experiments show that HitEmotion exposes deep emotional reasoning deficits in state-of-the-art models, especially on cognitively demanding tasks. In evaluation, the ToM-guided reasoning chain and TMPO improve end-task accuracy and yield more faithful, more coherent rationales. In conclusion, our work provides the research community with a practical toolkit for evaluating and enhancing the cognition-based emotional understanding capabilities of MLLMs. Our dataset and code are available at: this https URL."
    },
    {
      "title": "Do All Individual Layers Help? An Empirical Study of Task-Interfering Layers in Vision-Language Models",
      "url": "https://arxiv.org/abs/2602.01167",
      "date": "2026-02-01",
      "authors_text": "Zhiming Liu, Yujie Wei, Lei Feng, Xiu Su, Xiaobo Xia",
      "is_highlight": false,
      "score": 52.0,
      "summary": "This study identifies Task-Interfering Layers in vision-language models, demonstrating that selectively bypassing certain layers can enhance task performance without retraining.",
      "teaser_image": "https://arxiv.org/html/2602.01167/x1.png",
      "debug_abstract": "Current VLMs have demonstrated capabilities across a wide range of multimodal tasks. Typically, in a pretrained VLM, all layers are engaged by default to make predictions on downstream tasks. We find that intervening on a single layer, such as by zeroing its parameters, can improve the performance on certain tasks, indicating that some layers hinder rather than help downstream tasks. We systematically investigate how individual layers influence different tasks via layer intervention. Specifically, we measure the change in performance relative to the base model after intervening on each layer and observe improvements when bypassing specific layers. This improvement can be generalizable across models and datasets, indicating the presence of Task-Interfering Layers that harm downstream tasks&#39; performance. We introduce Task-Layer Interaction Vector, which quantifies the effect of intervening on each layer of a VLM given a task. These task-interfering layers exhibit task-specific sensitivity patterns: tasks requiring similar capabilities show consistent response trends under layer interventions, as evidenced by the high similarity in their task-layer interaction vectors. Inspired by these findings, we propose TaLo (Task-Adaptive Layer Knockout), a training-free, test-time adaptation method that dynamically identifies and bypasses the most interfering layer for a given task. Without parameter updates, TaLo improves performance across various models and datasets, including boosting Qwen-VL&#39;s accuracy on the Maps task in ScienceQA by up to 16.6%. Our work reveals an unexpected form of modularity in pretrained VLMs and provides a plug-and-play, training-free mechanism to unlock hidden capabilities at inference time. The source code will be publicly available."
    },
    {
      "title": "PRISM: Festina Lente Proactivity -- Risk-Sensitive, Uncertainty-Aware Deliberation for Proactive Agents",
      "url": "https://arxiv.org/abs/2602.01532",
      "date": "2026-02-02",
      "authors_text": "Yuxuan Fu, Xiaoyu Tan, Teqi Hao, Chen Zhan, Xihe Qiu",
      "is_highlight": false,
      "score": 45.0,
      "summary": "PRISM introduces a decision-theoretic framework for proactive agents that optimizes intervention timing and accuracy, significantly reducing false alarms and enhancing performance on ProactiveBench.",
      "teaser_image": "https://arxiv.org/html/2602.01532/fig2.png",
      "debug_abstract": "Proactive agents must decide not only what to say but also whether and when to intervene. Many current systems rely on brittle heuristics or indiscriminate long reasoning, which offers little control over the benefit-burden tradeoff. We formulate the problem as cost-sensitive selective intervention and present PRISM, a novel framework that couples a decision-theoretic gate with a dual-process reasoning architecture. At inference time, the agent intervenes only when a calibrated probability of user acceptance exceeds a threshold derived from asymmetric costs of missed help and false alarms. Inspired by festina lente (Latin: &#34;make haste slowly&#34;), we gate by an acceptance-calibrated, cost-derived threshold and invoke a resource-intensive Slow mode with counterfactual checks only near the decision boundary, concentrating computation on ambiguous and high-stakes cases. Training uses gate-aligned, schema-locked distillation: a teacher running the full PRISM pipeline provides dense, executable supervision on unlabeled interaction traces, while the student learns a response policy that is explicitly decoupled from the intervention gate to enable tunable and auditable control. On ProactiveBench, PRISM reduces false alarms by 22.78% and improves F1 by 20.14% over strong baselines. These results show that principled decision-theoretic gating, paired with selective slow reasoning and aligned distillation, yields proactive agents that are precise, computationally efficient, and controllable. To facilitate reproducibility, we release our code, models, and resources at this https URL all experiments use the open-source ProactiveBench benchmark."
    },
    {
      "title": "ReCALL: Recalibrating Capability Degradation for MLLM-based Composed Image Retrieval",
      "url": "https://arxiv.org/abs/2602.01639",
      "date": "2026-02-02",
      "authors_text": "Tianyu Yang, ChenWei He, Xiangzhao Hao, Tianyue Wang, Jiarui Guo",
      "is_highlight": false,
      "score": 42.0,
      "summary": "ReCALL addresses capability degradation in MLLM-based Composed Image Retrieval by diagnosing issues, generating corrective instructions, and refining the retriever through continual training.",
      "teaser_image": "https://arxiv.org/html/2602.01639/x1.png",
      "debug_abstract": "Composed Image Retrieval (CIR) aims to retrieve target images based on a hybrid query comprising a reference image and a modification text. Early dual-tower Vision-Language Models (VLMs) struggle with cross-modality compositional reasoning required for this task. Recently, adapting generative Multimodal Large Language Models (MLLMs) for retrieval offers a promising direction. However, we identify that this adaptation strategy overlooks a fundamental issue: adapting a generative MLLM into a single-embedding discriminative retriever triggers a paradigm conflict, which leads to Capability Degradation - the deterioration of native fine-grained reasoning after retrieval adaptation. To address this challenge, we propose ReCALL (Recalibrating Capability Degradation), a model-agnostic framework that follows a diagnose-generate-refine pipeline: Firstly, we diagnose cognitive blind spots of the retriever via self-guided informative instance mining. Next, we generate corrective instructions and triplets by CoT prompting the foundation MLLM and conduct quality control with VQA-based consistency filtering. Finally, we refine the retriever through continual training on these triplets with a grouped contrastive scheme, thereby internalizing fine-grained visual-semantic distinctions and realigning the discriminative embedding space of retriever with intrinsic compositional reasoning within the MLLM. Extensive experiments on CIRR and FashionIQ show that ReCALL consistently recalibrates degraded capabilities and achieves state-of-the-art performance. Code will be released soon."
    },
    {
      "title": "Beyond Precision: Training-Inference Mismatch is an Optimization Problem and Simple LR Scheduling Fixes It",
      "url": "https://arxiv.org/abs/2602.01826",
      "date": "2026-02-02",
      "authors_text": "Yaxiang Zhang, Yingru Li, Jiacai Liu, Jiawei Xu, Ziniu Li",
      "is_highlight": false,
      "score": 35.0,
      "summary": "This paper presents a dynamic learning rate scheduling method to mitigate training-inference mismatch and stabilize reinforcement learning in large language models by addressing gradient noise.",
      "teaser_image": "https://arxiv.org/html/2602.01826/Qwen3-4B_baseline_vs_importance_sampling_patches.png",
      "debug_abstract": "Reinforcement Learning (RL) for training Large Language Models is notoriously unstable. While recent studies attribute this to &#34;training inference mismatch stemming&#34; from inconsistent hybrid engines, standard remedies, such as Importance Sampling, might fail during extended training runs. In this work, we analyze this instability through the lens of optimization, demonstrating that gradient noise and training-inference mismatch escalate in tandem as training progresses. Meanwhile, we find that the mismatch can be effectively suppressed by shrinking the update size. Taken together, we deduce that the mismatch is not merely a static numerical discrepancy, but a dynamic failure coupled with the model&#39;s optimization. Based on this insight, we propose a simple yet effective solution: a specialized Learning Rate (LR) scheduler. Instead of pre-defined decay schedule in traditional LR scheduler, our method dynamically triggers LR decay based on response length, which we identify as a reliable early-warning signal for impending instability. Empirical evidence suggests that by reducing the learning rate as gradient noise rises, we can consistently stabilize RL training and keep the training-inference mismatch at a safe level."
    },
    {
      "title": "UrbanGS: A Scalable and Efficient Architecture for Geometrically Accurate Large-Scene Reconstruction",
      "url": "https://arxiv.org/abs/2602.02089",
      "date": "2026-02-02",
      "authors_text": "Changbai Li, Haodong Zhu, Hanlin Chen, Xiuping Liang, Tongfei Chen",
      "is_highlight": false,
      "score": 22.0,
      "summary": "UrbanGS is a scalable framework that enhances geometric accuracy and memory efficiency in large-scale urban scene reconstruction through innovative regularization and adaptive Gaussian pruning techniques.",
      "teaser_image": "https://arxiv.org/html/2602.02089/figure/main.png",
      "debug_abstract": "While 3D Gaussian Splatting (3DGS) enables high-quality, real-time rendering for bounded scenes, its extension to large-scale urban environments gives rise to critical challenges in terms of geometric consistency, memory efficiency, and computational scalability. To address these issues, we present UrbanGS, a scalable reconstruction framework that effectively tackles these challenges for city-scale applications. First, we propose a Depth-Consistent D-Normal Regularization module. Unlike existing approaches that rely solely on monocular normal estimators, which can effectively update rotation parameters yet struggle to update position parameters, our method integrates D-Normal constraints with external depth supervision. This allows for comprehensive updates of all geometric parameters. By further incorporating an adaptive confidence weighting mechanism based on gradient consistency and inverse depth deviation, our approach significantly enhances multi-view depth alignment and geometric coherence, which effectively resolves the issue of geometric accuracy in complex large-scale scenes. To improve scalability, we introduce a Spatially Adaptive Gaussian Pruning (SAGP) strategy, which dynamically adjusts Gaussian density based on local geometric complexity and visibility to reduce redundancy. Additionally, a unified partitioning and view assignment scheme is designed to eliminate boundary artifacts and optimize computational load. Extensive experiments on multiple urban datasets demonstrate that UrbanGS achieves superior performance in rendering quality, geometric accuracy, and memory efficiency, providing a systematic solution for high-fidelity large-scale scene reconstruction."
    },
    {
      "title": "TIDE: Trajectory-based Diagnostic Evaluation of Test-Time Improvement in LLM Agents",
      "url": "https://arxiv.org/abs/2602.02196",
      "date": "2026-02-02",
      "authors_text": "Hang Yan, Xinyu Che, Fangzhi Xu, Qiushi Sun, Zichen Ding",
      "is_highlight": false,
      "score": 20.0,
      "summary": "The TIDE framework evaluates Test-Time Improvement in LLM agents by analyzing task completion dynamics, recursive behaviors, and memory constraints to enhance performance optimization.",
      "teaser_image": "https://arxiv.org/html/2602.02196/x1.png",
      "debug_abstract": "Recent advances in autonomous LLM agents demonstrate their ability to improve performance through iterative interaction with the environment. We define this paradigm as Test-Time Improvement (TTI). However, the mechanisms under how and why TTI succeed or fail remain poorly understood, and existing evaluation metrics fail to capture their task optimization efficiency, behavior adaptation after erroneous actions, and the specific utility of working memory for task completion. To address these gaps, we propose Test-time Improvement Diagnostic Evaluation (TIDE), an agent-agnostic and environment-agnostic framework that decomposes TTI into three comprehensive and interconnected dimensions. The framework measures (1) the overall temporal dynamics of task completion and (2) identifies whether performance is primarily constrained by recursive looping behaviors or (3) by burdensome accumulated memory. Through extensive experiments across diverse agents and environments, TIDE highlights that improving agent performance requires more than scaling internal reasoning, calling for explicitly optimizing the interaction dynamics between the agent and the environment."
    },
    {
      "title": "A Low-Cost Vision-Based Tactile Gripper with Pretraining Learning for Contact-Rich Manipulation",
      "url": "https://arxiv.org/abs/2602.00514",
      "date": "2026-01-31",
      "authors_text": "Yaohua Liu, Binkai Ou, Zicheng Qiu, Ce Hao, Yemin Wang",
      "is_highlight": false,
      "score": 11.0,
      "summary": "The LVTG gripper enhances contact-rich robotic manipulation through low-cost, durable visuo-tactile integration and improved policy learning via contrastive pretraining, achieving higher success rates.",
      "teaser_image": "https://arxiv.org/html/2602.00514/x1.png",
      "debug_abstract": "Robotic manipulation in contact-rich environments remains challenging, particularly when relying on conventional tactile sensors that suffer from limited sensing range, reliability, and cost-effectiveness. In this work, we present LVTG, a low-cost visuo-tactile gripper designed for stable, robust, and efficient physical interaction. Unlike existing visuo-tactile sensors, LVTG enables more effective and stable grasping of larger and heavier everyday objects, thanks to its enhanced tactile sensing area and greater opening angle. Its surface skin is made of highly wear-resistant material, significantly improving durability and extending operational lifespan. The integration of vision and tactile feedback allows LVTG to provide rich, high-fidelity sensory data, facilitating reliable perception during complex manipulation tasks. Furthermore, LVTG features a modular design that supports rapid maintenance and replacement. To effectively fuse vision and touch, We adopt a CLIP-inspired contrastive learning objective to align tactile embeddings with their corresponding visual observations, enabling a shared cross-modal representation space for visuo-tactile perception. This alignment improves the performance of an Action Chunking Transformer (ACT) policy in contact-rich manipulation, leading to more efficient data collection and more effective policy learning. Compared to the original ACT method, the proposed LVTG with pretraining achieves significantly higher success rates in manipulation tasks."
    },
    {
      "title": "FD-VLA: Force-Distilled Vision-Language-Action Model for Contact-Rich Manipulation",
      "url": "https://arxiv.org/abs/2602.02142",
      "date": "2026-02-02",
      "authors_text": "Ruiteng Zhao, Wenshuo Wang, Yicheng Ma, Xiaocong Li, Francis E. H. Tay",
      "is_highlight": false,
      "score": 0,
      "summary": "The FD-VLA framework enhances contact-rich manipulation by integrating force awareness through a novel distillation method, improving performance without physical force sensors.",
      "teaser_image": "https://arxiv.org/html/2602.02142/x1.png",
      "debug_abstract": "Force sensing is a crucial modality for Vision-Language-Action (VLA) frameworks, as it enables fine-grained perception and dexterous manipulation in contact-rich tasks. We present Force-Distilled VLA (FD-VLA), a novel framework that integrates force awareness into contact-rich manipulation without relying on physical force sensors. The core of our approach is a Force Distillation Module (FDM), which distills force by mapping a learnable query token, conditioned on visual observations and robot states, into a predicted force token aligned with the latent representation of actual force signals. During inference, this distilled force token is injected into the pretrained VLM, enabling force-aware reasoning while preserving the integrity of its vision-language semantics. This design provides two key benefits: first, it allows practical deployment across a wide range of robots that lack expensive or fragile force-torque sensors, thereby reducing hardware cost and complexity; second, the FDM introduces an additional force-vision-state fusion prior to the VLM, which improves cross-modal alignment and enhances perception-action robustness in contact-rich scenarios. Surprisingly, our physical experiments show that the distilled force token outperforms direct sensor force measurements as well as other baselines, which highlights the effectiveness of this force-distilled VLA approach."
    },
    {
      "title": "Abstracting Robot Manipulation Skills via Mixture-of-Experts Diffusion Policies",
      "url": "https://arxiv.org/abs/2601.21251",
      "date": "2026-01-29",
      "authors_text": "Ce Hao, Xuanran Zhai, Yaohua Liu, Harold Soh",
      "is_highlight": false,
      "score": 90.0,
      "summary": "The Skill Mixture-of-Experts Policy (SMP) enhances robot manipulation by efficiently utilizing a compact skill basis and adaptive expert activation for improved multi-task performance.",
      "teaser_image": "https://arxiv.org/html/2601.21251/x1.png",
      "debug_abstract": "Diffusion-based policies have recently shown strong results in robot manipulation, but their extension to multi-task scenarios is hindered by the high cost of scaling model size and demonstrations. We introduce Skill Mixture-of-Experts Policy (SMP), a diffusion-based mixture-of-experts policy that learns a compact orthogonal skill basis and uses sticky routing to compose actions from a small, task-relevant subset of experts at each step. A variational training objective supports this design, and adaptive expert activation at inference yields fast sampling without oversized backbones. We validate SMP in simulation and on a real dual-arm platform with multi-task learning and transfer learning tasks, where SMP achieves higher success rates and markedly lower inference cost than large diffusion baselines. These results indicate a practical path toward scalable, transferable multi-task manipulation: learn reusable skills once, activate only what is needed, and adapt quickly when tasks change."
    },
    {
      "title": "TraceRouter: Robust Safety for Large Foundation Models via Path-Level Intervention",
      "url": "https://arxiv.org/abs/2601.21900",
      "date": "2026-01-29",
      "authors_text": "Chuancheng Shi, Shangze Li, Wenjun Lu, Wenhua Wu, Cong Wang",
      "is_highlight": false,
      "score": 73.0,
      "summary": "TraceRouter enhances the safety of large foundation models by disconnecting harmful semantic pathways through a novel path-level intervention framework, outperforming existing defenses.",
      "teaser_image": "https://arxiv.org/html/2601.21900/x8.png",
      "debug_abstract": "Despite their capabilities, large foundation models (LFMs) remain susceptible to adversarial manipulation. Current defenses predominantly rely on the &#34;locality hypothesis&#34;, suppressing isolated neurons or features. However, harmful semantics act as distributed, cross-layer circuits, rendering such localized interventions brittle and detrimental to utility. To bridge this gap, we propose \\textbf{TraceRouter}, a path-level framework that traces and disconnects the causal propagation circuits of illicit semantics. TraceRouter operates in three stages: (1) it pinpoints a sensitive onset layer by analyzing attention divergence; (2) it leverages sparse autoencoders (SAEs) and differential activation analysis to disentangle and isolate malicious features; and (3) it maps these features to downstream causal pathways via feature influence scores (FIS) derived from zero-out interventions. By selectively suppressing these causal chains, TraceRouter physically severs the flow of harmful information while leaving orthogonal computation routes intact. Extensive experiments demonstrate that TraceRouter significantly outperforms state-of-the-art baselines, achieving a superior trade-off between adversarial robustness and general utility. Our code will be publicly released. WARNING: This paper contains unsafe model responses."
    },
    {
      "title": "GPO: Growing Policy Optimization for Legged Robot Locomotion and Whole-Body Control",
      "url": "https://arxiv.org/abs/2601.20668",
      "date": "2026-01-28",
      "authors_text": "Shuhao Liao, Peizhuo Li, Xinrong Yang, Linnan Chang, Zhaoxin Fan",
      "is_highlight": false,
      "score": 92.0,
      "summary": "The paper presents Growing Policy Optimization (GPO), a framework that enhances reinforcement learning for legged robots by progressively expanding the action space to improve data collection and policy performance.",
      "teaser_image": "https://arxiv.org/html/2601.20668/pic/framework.png",
      "debug_abstract": "Training reinforcement learning (RL) policies for legged robots remains challenging due to high-dimensional continuous actions, hardware constraints, and limited exploration. Existing methods for locomotion and whole-body control work well for position-based control with environment-specific heuristics (e.g., reward shaping, curriculum design, and manual initialization), but are less effective for torque-based control, where sufficiently exploring the action space and obtaining informative gradient signals for training is significantly more difficult. We introduce Growing Policy Optimization (GPO), a training framework that applies a time-varying action transformation to restrict the effective action space in the early stage, thereby encouraging more effective data collection and policy learning, and then progressively expands it to enhance exploration and achieve higher expected return. We prove that this transformation preserves the PPO update rule and introduces only bounded, vanishing gradient distortion, thereby ensuring stable training. We evaluate GPO on both quadruped and hexapod robots, including zero-shot deployment of simulation-trained policies on hardware. Policies trained with GPO consistently achieve better performance. These results suggest that GPO provides a general, environment-agnostic optimization framework for learning legged locomotion."
    },
    {
      "title": "Curiosity Driven Knowledge Retrieval for Mobile Agents",
      "url": "https://arxiv.org/abs/2601.19306",
      "date": "2026-01-27",
      "authors_text": "Sijia Li, Xiaoyu Tan, Shahir Ali, Niels Schmidt, Gengchen Ma",
      "is_highlight": false,
      "score": 80.0,
      "summary": "The paper presents a curiosity-driven framework for mobile agents that enhances performance by retrieving and integrating structured knowledge (AppCards) when faced with uncertainty, achieving improved planning reliability.",
      "debug_abstract": "Mobile agents have made progress toward reliable smartphone automation, yet performance in complex applications remains limited by incomplete knowledge and weak generalization to unseen environments. We introduce a curiosity driven knowledge retrieval framework that formalizes uncertainty during execution as a curiosity score. When this score exceeds a threshold, the system retrieves external information from documentation, code repositories, and historical trajectories. Retrieved content is organized into structured AppCards, which encode functional semantics, parameter conventions, interface mappings, and interaction patterns. During execution, an enhanced agent selectively integrates relevant AppCards into its reasoning process, thereby compensating for knowledge blind spots and improving planning reliability. Evaluation on the AndroidWorld benchmark shows consistent improvements across backbones, with an average gain of six percentage points and a new state of the art success rate of 88.8\\% when combined with GPT-5. Analysis indicates that AppCards are particularly effective for multi step and cross application tasks, while improvements depend on the backbone model. Case studies further confirm that AppCards reduce ambiguity, shorten exploration, and support stable execution trajectories. Task trajectories are publicly available at this https URL."
    },
    {
      "title": "Video-KTR: Reinforcing Video Reasoning via Key Token Attribution",
      "url": "https://arxiv.org/abs/2601.19686",
      "date": "2026-01-27",
      "authors_text": "Ziyue Wang, Sheng Jin, Zhongrong Zuo, Jiawei Wu, Han Qiu",
      "is_highlight": false,
      "score": 66.0,
      "summary": "Video-KTR enhances video reasoning in multimodal models by selectively reinforcing key tokens through a novel attribution framework, achieving state-of-the-art performance across multiple benchmarks.",
      "debug_abstract": "Reinforcement learning (RL) has shown strong potential for enhancing reasoning in multimodal large language models, yet existing video reasoning methods often rely on coarse sequence-level rewards or single-factor token selection, neglecting fine-grained links among visual inputs, temporal dynamics, and linguistic outputs, limiting both accuracy and interpretability. We propose Video-KTR, a modality-aware policy shaping framework that performs selective, token-level RL by combining three attribution signals: (1) visual-aware tokens identified via counterfactual masking to reveal perceptual dependence; (2) temporal-aware tokens detected through frame shuffling to expose temporal sensitivity; and (3) high-entropy tokens signaling predictive uncertainty. By reinforcing only these key tokens, Video-KTR focuses learning on semantically informative, modality-sensitive content while filtering out low-value tokens. Across five challenging benchmarks, Video-KTR achieves state-of-the-art or highly competitive results, achieving 42.7\\% on Video-Holmes (surpassing GPT-4o) with consistent gains on both reasoning and general video understanding tasks. Ablation studies verify the complementary roles of the attribution signals and the robustness of targeted token-level updates. Overall, Video-KTR improves accuracy and interpretability, offering a simple, drop-in extension to RL for complex video reasoning. Our code and models are available at this https URL."
    },
    {
      "title": "ORION: Option-Regularized Deep Reinforcement Learning for Cooperative Multi-Agent Online Navigation",
      "url": "https://arxiv.org/abs/2601.01155",
      "date": "2026-01-26",
      "authors_text": "Shizhe Zhang, Jingsong Liang, Zhitao Zhou, Shuhan Ye, Yizhuo Wang",
      "is_highlight": false,
      "score": 88.0,
      "summary": "ORION is a deep reinforcement learning framework that enables decentralized, cooperative navigation for multi-agent systems in partially known environments by integrating prior maps and online observations.",
      "debug_abstract": "Existing methods for multi-agent navigation typically assume fully known environments, offering limited support for partially known scenarios such as warehouses or factory floors. There, agents may need to plan trajectories that balance their own path optimality with their ability to collect and share information about the environment that can help their teammates reach their own goals. To these ends, we propose ORION, a novel deep reinforcement learning framework for cooperative multi-agent online navigation in partially known environments. Starting from an imperfect prior map, ORION trains agents to make decentralized decisions, coordinate to reach their individual targets, and actively reduce map uncertainty by sharing online observations in a closed perception-action loop. We first design a shared graph encoder that fuses prior map with online perception into a unified representation, providing robust state embeddings under dynamic map discrepancies. At the core of ORION is an option-critic framework that learns to reason about a set of high-level cooperative modes that translate into sequences of low-level actions, allowing agents to switch between individual navigation and team-level exploration adaptively. We further introduce a dual-stage cooperation strategy that enables agents to assist teammates under map uncertainty, thereby reducing the overall makespan. Across extensive maze-like maps and large-scale warehouse environments, our simulation results show that ORION achieves high-quality, real-time decentralized cooperation over varying team sizes, outperforming state-of-the-art classical and learning-based baselines. Finally, we validate ORION on physical robot teams, demonstrating its robustness and practicality for real-world cooperative navigation."
    },
    {
      "title": "Fast and Safe Trajectory Optimization for Mobile Manipulators With Neural Configuration Space Distance Field",
      "url": "https://arxiv.org/abs/2601.18548",
      "date": "2026-01-26",
      "authors_text": "Yulin Li, Zhiyuan Song, Yiming Li, Zhicheng Song, Kai Chen",
      "is_highlight": false,
      "score": 86.0,
      "summary": "This paper introduces Generalized Configuration Space Distance Fields (GCDF) for efficient trajectory optimization in mobile manipulators, enhancing collision reasoning and enabling agile motion in complex environments.",
      "debug_abstract": "Mobile manipulators promise agile, long-horizon behavior by coordinating base and arm motion, yet whole-body trajectory optimization in cluttered, confined spaces remains difficult due to high-dimensional nonconvexity and the need for fast, accurate collision reasoning. Configuration Space Distance Fields (CDF) enable fixed-base manipulators to model collisions directly in configuration space via smooth, implicit distances. This representation holds strong potential to bypass the nonlinear configuration-to-workspace mapping while preserving accurate whole-body geometry and providing optimization-friendly collision costs. Yet, extending this capability to mobile manipulators is hindered by unbounded workspaces and tighter base-arm coupling. We lift this promise to mobile manipulation with Generalized Configuration Space Distance Fields (GCDF), extending CDF to robots with both translational and rotational joints in unbounded workspaces with tighter base-arm coupling. We prove that GCDF preserves Euclidean-like local distance structure and accurately encodes whole-body geometry in configuration space, and develop a data generation and training pipeline that yields continuous neural GCDFs with accurate values and gradients, supporting efficient GPU-batched queries. Building on this representation, we develop a high-performance sequential convex optimization framework centered on GCDF-based collision reasoning. The solver scales to large numbers of implicit constraints through (i) online specification of neural constraints, (ii) sparsity-aware active-set detection with parallel batched evaluation across thousands of constraints, and (iii) incremental constraint management for rapid replanning under scene changes."
    },
    {
      "title": "AdaReasoner: Dynamic Tool Orchestration for Iterative Visual Reasoning",
      "url": "https://arxiv.org/abs/2601.18631",
      "date": "2026-01-26",
      "authors_text": "Mingyang Song, Haoyu Sun, Jiawei Gu, Linjie Li, Luxin Xu",
      "is_highlight": false,
      "score": 61.0,
      "summary": "AdaReasoner is a multimodal model that autonomously optimizes tool selection and usage for iterative visual reasoning, achieving state-of-the-art performance on various benchmarks.",
      "debug_abstract": "When humans face problems beyond their immediate capabilities, they rely on tools, providing a promising paradigm for improving visual reasoning in multimodal large language models (MLLMs). Effective reasoning, therefore, hinges on knowing which tools to use, when to invoke them, and how to compose them over multiple steps, even when faced with new tools or new tasks. We introduce \\textbf{AdaReasoner}, a family of multimodal models that learn tool use as a general reasoning skill rather than as tool-specific or explicitly supervised behavior. AdaReasoner is enabled by (i) a scalable data curation pipeline exposing models to long-horizon, multi-step tool interactions; (ii) Tool-GRPO, a reinforcement learning algorithm that optimizes tool selection and sequencing based on end-task success; and (iii) an adaptive learning mechanism that dynamically regulates tool usage. Together, these components allow models to infer tool utility from task context and intermediate outcomes, enabling coordination of multiple tools and generalization to unseen tools. Empirically, AdaReasoner exhibits strong tool-adaptive and generalization behaviors: it autonomously adopts beneficial tools, suppresses irrelevant ones, and adjusts tool usage frequency based on task demands, despite never being explicitly trained to do so. These capabilities translate into state-of-the-art performance across challenging benchmarks, improving the 7B base model by +24.9\\% on average and surpassing strong proprietary systems such as GPT-5 on multiple tasks, including VSP and Jigsaw."
    },
    {
      "title": "Emotion-LLaMAv2 and MMEVerse: A New Framework and Benchmark for Multimodal Emotion Understanding",
      "url": "https://arxiv.org/abs/2601.16449",
      "date": "2026-01-23",
      "authors_text": "Xiaojiang Peng, Jingyi Chen, Zebang Cheng, Bao Peng, Fengyi Wu",
      "is_highlight": false,
      "score": 58.0,
      "summary": "Emotion-LLaMAv2 and MMEVerse introduce a comprehensive framework and benchmark for enhanced multimodal emotion understanding through advanced encoding, feature interaction, and large-scale dataset integration.",
      "debug_abstract": "Understanding human emotions from multimodal signals poses a significant challenge in affective computing and human-robot interaction. While multimodal large language models (MLLMs) have excelled in general vision-language tasks, their capabilities in emotional reasoning remain limited. The field currently suffers from a scarcity of large-scale datasets with high-quality, descriptive emotion annotations and lacks standardized benchmarks for evaluation. Our preliminary framework, Emotion-LLaMA, pioneered instruction-tuned multimodal learning for emotion reasoning but was restricted by explicit face detectors, implicit fusion strategies, and low-quality training data with limited scale. To address these limitations, we present Emotion-LLaMAv2 and the MMEVerse benchmark, establishing an end-to-end pipeline together with a standardized evaluation setting for emotion recognition and reasoning. Emotion-LLaMAv2 introduces three key advances. First, an end-to-end multiview encoder eliminates external face detection and captures nuanced emotional cues via richer spatial and temporal multiview tokens. Second, a Conv Attention pre-fusion module is designed to enable simultaneous local and global multimodal feature interactions external to the LLM backbone. Third, a perception-to-cognition curriculum instruction tuning scheme within the LLaMA2 backbone unifies emotion recognition and free-form emotion reasoning. To support large-scale training and reproducible evaluation, MMEVerse aggregates twelve publicly available emotion datasets, including IEMOCAP, MELD, DFEW, and MAFW, into a unified multimodal instruction format. The data are re-annotated via a multi-agent pipeline involving Qwen2 Audio, Qwen2.5 VL, and GPT 4o, producing 130k training clips and 36k testing clips across 18 evaluation benchmarks."
    },
    {
      "title": "Virtual Traffic Police: Large Language Model-Augmented Traffic Signal Control for Unforeseen Incidents",
      "url": "https://arxiv.org/abs/2601.15816",
      "date": "2026-01-22",
      "authors_text": "Shiqi Wei, Qiqing Wang, Kaidi Yang",
      "is_highlight": false,
      "score": 84.0,
      "summary": "The paper presents a hierarchical framework that enhances traditional traffic signal control with Large Language Models to efficiently manage unforeseen traffic incidents.",
      "debug_abstract": "Adaptive traffic signal control (TSC) has demonstrated strong effectiveness in managing dynamic traffic flows. However, conventional methods often struggle when unforeseen traffic incidents occur (e.g., accidents and road maintenance), which typically require labor-intensive and inefficient manual interventions by traffic police officers. Large Language Models (LLMs) appear to be a promising solution thanks to their remarkable reasoning and generalization capabilities. Nevertheless, existing works often propose to replace existing TSC systems with LLM-based systems, which can be (i) unreliable due to the inherent hallucinations of LLMs and (ii) costly due to the need for system replacement. To address the issues of existing works, we propose a hierarchical framework that augments existing TSC systems with LLMs, whereby a virtual traffic police agent at the upper level dynamically fine-tunes selected parameters of signal controllers at the lower level in response to real-time traffic incidents. To enhance domain-specific reliability in response to unforeseen traffic incidents, we devise a self-refined traffic language retrieval system (TLRS), whereby retrieval-augmented generation is employed to draw knowledge from a tailored traffic language database that encompasses traffic conditions and controller operation principles. Moreover, we devise an LLM-based verifier to update the TLRS continuously over the reasoning process. Our results show that LLMs can serve as trustworthy virtual traffic police officers that can adapt conventional TSC methods to unforeseen traffic incidents with significantly improved operational efficiency and reliability."
    },
    {
      "title": "PhysicsMind: Sim and Real Mechanics Benchmarking for Physical Reasoning and Prediction in Foundational VLMs and World Models",
      "url": "https://arxiv.org/abs/2601.16007",
      "date": "2026-01-22",
      "authors_text": "Chak-Wing Mak, Guanyu Zhu, Boyi Zhang, Hongji Li, Xiaowei Chi",
      "is_highlight": false,
      "score": 94.1,
      "summary": "PhysicsMind introduces a benchmark for evaluating physical reasoning in multimodal models through real and simulated tasks focused on fundamental physics principles, revealing significant gaps in current models' understanding.",
      "debug_abstract": "Modern foundational Multimodal Large Language Models (MLLMs) and video world models have advanced significantly in mathematical, common-sense, and visual reasoning, but their grasp of the underlying physics remains underexplored. Existing benchmarks attempting to measure this matter rely on synthetic, Visual Question Answer templates or focus on perceptual video quality that is tangential to measuring how well the video abides by physical laws. To address this fragmentation, we introduce PhysicsMind, a unified benchmark with both real and simulation environments that evaluates law-consistent reasoning and generation over three canonical principles: Center of Mass, Lever Equilibrium, and Newton&#39;s First Law. PhysicsMind comprises two main tasks: i) VQA tasks, testing whether models can reason and determine physical quantities and values from images or short videos, and ii) Video Generation(VG) tasks, evaluating if predicted motion trajectories obey the same center-of-mass, torque, and inertial constraints as the ground truth. A broad range of recent models and video generation models is evaluated on PhysicsMind and found to rely on appearance heuristics while often violating basic mechanics. These gaps indicate that current scaling and training are still insufficient for robust physical understanding, underscoring PhysicsMind as a focused testbed for physics-aware multimodal models. Our data will be released upon acceptance."
    },
    {
      "title": "AION: Aerial Indoor Object-Goal Navigation Using Dual-Policy Reinforcement Learning",
      "url": "https://arxiv.org/abs/2601.15614",
      "date": "2026-01-22",
      "authors_text": "Zichen Yan, Yuchen Hou, Shenao Wang, Yichao Gao, Rui Huang",
      "is_highlight": false,
      "score": 85.6,
      "summary": "AION introduces a dual-policy reinforcement learning framework for aerial object-goal navigation, enhancing exploration and navigation efficiency without external localization or global maps.",
      "debug_abstract": "Object-Goal Navigation (ObjectNav) requires an agent to autonomously explore an unknown environment and navigate toward target objects specified by a semantic label. While prior work has primarily studied zero-shot ObjectNav under 2D locomotion, extending it to aerial platforms with 3D locomotion capability remains underexplored. Aerial robots offer superior maneuverability and search efficiency, but they also introduce new challenges in spatial perception, dynamic control, and safety assurance. In this paper, we propose AION for vision-based aerial ObjectNav without relying on external localization or global maps. AION is an end-to-end dual-policy reinforcement learning (RL) framework that decouples exploration and goal-reaching behaviors into two specialized policies. We evaluate AION on the AI2-THOR benchmark and further assess its real-time performance in IsaacSim using high-fidelity drone models. Experimental results show that AION achieves superior performance across comprehensive evaluation metrics in exploration, navigation efficiency, and safety. The video can be found at this https URL."
    }
  ],
  "范明明老师实验室": [
    {
      "title": "Visual Reasoning over Time Series via Multi-Agent System",
      "url": "https://arxiv.org/abs/2602.03026",
      "date": "2026-02-03",
      "authors_text": "Weilin Ruan, Yuxuan Liang",
      "is_highlight": false,
      "score": 66.0,
      "summary": "MAS4TS is a multi-agent system that enhances time series analysis through visual reasoning and adaptive tool usage, achieving state-of-the-art performance across diverse tasks.",
      "teaser_image": "https://arxiv.org/html/2602.03026/x2.png",
      "debug_abstract": "Time series analysis underpins many real-world applications, yet existing time-series-specific methods and pretrained large-model-based approaches remain limited in integrating intuitive visual reasoning and generalizing across tasks with adaptive tool usage. To address these limitations, we propose MAS4TS, a tool-driven multi-agent system for general time series tasks, built upon an Analyzer-Reasoner-Executor paradigm that integrates agent communication, visual reasoning, and latent reconstruction within a unified framework. MAS4TS first performs visual reasoning over time series plots with structured priors using a Vision-Language Model to extract temporal structures, and subsequently reconstructs predictive trajectories in latent space. Three specialized agents coordinate via shared memory and gated communication, while a router selects task-specific tool chains for execution. Extensive experiments on multiple benchmarks demonstrate that MAS4TS achieves state-of-the-art performance across a wide range of time series tasks, while exhibiting strong generalization and efficient inference."
    },
    {
      "title": "Neural Predictor-Corrector: Solving Homotopy Problems with Reinforcement Learning",
      "url": "https://arxiv.org/abs/2602.03086",
      "date": "2026-02-03",
      "authors_text": "Jiayao Mai, Bangyan Liao, Zhenjun Zhao, Yingping Zeng, Haoang Li",
      "is_highlight": false,
      "score": 64.0,
      "summary": "The Neural Predictor-Corrector (NPC) framework uses reinforcement learning to automate step size and termination decisions in solving diverse homotopy problems, outperforming traditional methods.",
      "teaser_image": "https://arxiv.org/html/2602.03086/x2.png",
      "debug_abstract": "The Homotopy paradigm, a general principle for solving challenging problems, appears across diverse domains such as robust optimization, global optimization, polynomial root-finding, and sampling. Practical solvers for these problems typically follow a predictor-corrector (PC) structure, but rely on hand-crafted heuristics for step sizes and iteration termination, which are often suboptimal and task-specific. To address this, we unify these problems under a single framework, which enables the design of a general neural solver. Building on this unified view, we propose Neural Predictor-Corrector (NPC), which replaces hand-crafted heuristics with automatically learned policies. NPC formulates policy selection as a sequential decision-making problem and leverages reinforcement learning to automatically discover efficient strategies. To further enhance generalization, we introduce an amortized training mechanism, enabling one-time offline training for a class of problems and efficient online inference on new instances. Experiments on four representative homotopy problems demonstrate that our method generalizes effectively to unseen instances. It consistently outperforms classical and specialized baselines in efficiency while demonstrating superior stability across tasks, highlighting the value of unifying homotopy methods into a single neural framework."
    },
    {
      "title": "Diffusion-Driven Inter-Outer Surface Separation for Point Clouds with Open Boundaries",
      "url": "https://arxiv.org/abs/2602.00739",
      "date": "2026-01-31",
      "authors_text": "Zhengyan Qin, Liyuan Qiu",
      "is_highlight": false,
      "score": 72.0,
      "summary": "The paper presents a diffusion-based algorithm for effectively separating inter and outer surfaces in double-layered point clouds with open boundaries, addressing artifacts from TSDF fusion.",
      "teaser_image": "https://arxiv.org/html/2602.00739/figures/pipeline5.png",
      "debug_abstract": "We propose a diffusion-based algorithm for separating the inter and outer layer surfaces from double-layered point clouds, particularly those exhibiting the &#34;double surface artifact&#34; caused by truncation in Truncated Signed Distance Function (TSDF) fusion during indoor or medical 3D reconstruction. This artifact arises from asymmetric truncation thresholds, leading to erroneous inter and outer shells in the fused volume, which our method addresses by extracting the true inter layer to mitigate challenges like overlapping surfaces and disordered normals. We focus on point clouds with \\emph{open boundaries} (i.e., sampled surfaces with topological openings/holes through which particles may escape), rather than point clouds with \\emph{missing surface regions} where no samples exist. Our approach enables robust processing of both watertight and open-boundary models, achieving extraction of the inter layer from 20,000 inter and 20,000 outer points in approximately 10 seconds. This solution is particularly effective for applications requiring accurate surface representations, such as indoor scene modeling and medical imaging, where double-layered point clouds are prevalent, and it accommodates both closed (watertight) and open-boundary surface geometries. Our goal is \\emph{post-hoc} inter/outer shell separation as a lightweight module after TSDF fusion; we do not aim to replace full variational or learning-based reconstruction pipelines."
    },
    {
      "title": "Physics-informed Diffusion Mamba Transformer for Real-world Driving",
      "url": "https://arxiv.org/abs/2602.00808",
      "date": "2026-01-31",
      "authors_text": "Hang Zhou, Qiang Zhang, Peiran Liu, Yihao Qin, Zhaoxu Yan",
      "is_highlight": false,
      "score": 74.0,
      "summary": "The paper presents a Diffusion Mamba Transformer that integrates sequential context and physical constraints for improved trajectory planning in autonomous driving, outperforming existing models.",
      "teaser_image": "https://arxiv.org/html/2602.00808/difference_pipeline3.png",
      "debug_abstract": "Autonomous driving systems demand trajectory planners that not only model the inherent uncertainty of future motions but also respect complex temporal dependencies and underlying physical laws. While diffusion-based generative models excel at capturing multi-modal distributions, they often fail to incorporate long-term sequential contexts and domain-specific physical priors. In this work, we bridge these gaps with two key innovations. First, we introduce a Diffusion Mamba Transformer architecture that embeds mamba and attention into the diffusion process, enabling more effective aggregation of sequential input contexts from sensor streams and past motion histories. Second, we design a Port-Hamiltonian Neural Network module that seamlessly integrates energy-based physical constraints into the diffusion model, thereby enhancing trajectory predictions with both consistency and interpretability. Extensive evaluations on standard autonomous driving benchmarks demonstrate that our unified framework significantly outperforms state-of-the-art baselines in predictive accuracy, physical plausibility, and robustness, thereby advancing safe and reliable motion planning."
    },
    {
      "title": "Invariance on Manifolds: Understanding Robust Visual Representations for Place Recognition",
      "url": "https://arxiv.org/abs/2602.00841",
      "date": "2026-01-31",
      "authors_text": "Jintao Cheng, Weibin Li, Zhijian He, Jin Wu, Chi Man Vong",
      "is_highlight": false,
      "score": 66.0,
      "summary": "This paper presents a training-free Second-Order Geometric Statistics framework for robust visual place recognition, utilizing covariance descriptors on the SPD manifold for effective zero-shot generalization.",
      "teaser_image": "https://arxiv.org/html/2602.00841/x2.png",
      "debug_abstract": "Visual Place Recognition (VPR) demands representations robust to drastic environmental and viewpoint shifts. Current aggregation paradigms, however, either rely on data-hungry supervision or simplistic first-order statistics, often neglecting intrinsic structural correlations. In this work, we propose a Second-Order Geometric Statistics framework that inherently captures geometric stability without training. We conceptualize scenes as covariance descriptors on the Symmetric Positive Definite (SPD) manifold, where perturbations manifest as tractable congruence transformations. By leveraging geometry-aware Riemannian mappings, we project these descriptors into a linearized Euclidean embedding, effectively decoupling signal structure from noise. Our approach introduces a training-free framework built upon fixed, pre-trained backbones, achieving strong zero-shot generalization without parameter updates. Extensive experiments confirm that our method achieves highly competitive performance against state-of-the-art baselines, particularly excelling in challenging zero-shot scenarios."
    },
    {
      "title": "PolicyFlow: Policy Optimization with Continuous Normalizing Flow in Reinforcement Learning",
      "url": "https://arxiv.org/abs/2602.01156",
      "date": "2026-02-01",
      "authors_text": "Shunpeng Yang, Ben Liu, Hua Chen",
      "is_highlight": false,
      "score": 71.0,
      "summary": "PolicyFlow is a novel reinforcement learning algorithm that integrates continuous normalizing flows with PPO, enhancing performance and stability while reducing computational costs and encouraging diverse behaviors.",
      "teaser_image": "https://arxiv.org/html/2602.01156/x1.png",
      "debug_abstract": "Among on-policy reinforcement learning algorithms, Proximal Policy Optimization (PPO) demonstrates is widely favored for its simplicity, numerical stability, and strong empirical performance. Standard PPO relies on surrogate objectives defined via importance ratios, which require evaluating policy likelihood that is typically straightforward when the policy is modeled as a Gaussian distribution. However, extending PPO to more expressive, high-capacity policy models such as continuous normalizing flows (CNFs), also known as flow-matching models, is challenging because likelihood evaluation along the full flow trajectory is computationally expensive and often numerically unstable. To resolve this issue, we propose PolicyFlow, a novel on-policy CNF-based reinforcement learning algorithm that integrates expressive CNF policies with PPO-style objectives without requiring likelihood evaluation along the full flow path. PolicyFlow approximates importance ratios using velocity field variations along a simple interpolation path, reducing computational overhead without compromising training stability. To further prevent mode collapse and further encourage diverse behaviors, we propose the Brownian Regularizer, an implicit policy entropy regularizer inspired by Brownian motion, which is conceptually elegant and computationally lightweight. Experiments on diverse tasks across various environments including MultiGoal, PointMaze, IsaacLab and MuJoCo Playground show that PolicyFlow achieves competitive or superior performance compared to PPO using Gaussian policies and flow-based baselines including FPO and DPPO. Notably, results on MultiGoal highlight PolicyFlow&#39;s ability to capture richer multimodal action distributions."
    },
    {
      "title": "Do All Individual Layers Help? An Empirical Study of Task-Interfering Layers in Vision-Language Models",
      "url": "https://arxiv.org/abs/2602.01167",
      "date": "2026-02-01",
      "authors_text": "Zhiming Liu, Yujie Wei, Lei Feng, Xiu Su, Xiaobo Xia",
      "is_highlight": false,
      "score": 52.0,
      "summary": "This study identifies Task-Interfering Layers in vision-language models, demonstrating that selectively bypassing certain layers can enhance task performance without retraining.",
      "teaser_image": "https://arxiv.org/html/2602.01167/x1.png",
      "debug_abstract": "Current VLMs have demonstrated capabilities across a wide range of multimodal tasks. Typically, in a pretrained VLM, all layers are engaged by default to make predictions on downstream tasks. We find that intervening on a single layer, such as by zeroing its parameters, can improve the performance on certain tasks, indicating that some layers hinder rather than help downstream tasks. We systematically investigate how individual layers influence different tasks via layer intervention. Specifically, we measure the change in performance relative to the base model after intervening on each layer and observe improvements when bypassing specific layers. This improvement can be generalizable across models and datasets, indicating the presence of Task-Interfering Layers that harm downstream tasks&#39; performance. We introduce Task-Layer Interaction Vector, which quantifies the effect of intervening on each layer of a VLM given a task. These task-interfering layers exhibit task-specific sensitivity patterns: tasks requiring similar capabilities show consistent response trends under layer interventions, as evidenced by the high similarity in their task-layer interaction vectors. Inspired by these findings, we propose TaLo (Task-Adaptive Layer Knockout), a training-free, test-time adaptation method that dynamically identifies and bypasses the most interfering layer for a given task. Without parameter updates, TaLo improves performance across various models and datasets, including boosting Qwen-VL&#39;s accuracy on the Maps task in ScienceQA by up to 16.6%. Our work reveals an unexpected form of modularity in pretrained VLMs and provides a plug-and-play, training-free mechanism to unlock hidden capabilities at inference time. The source code will be publicly available."
    },
    {
      "title": "Towards Exploratory and Focused Manipulation with Bimanual Active Perception: A New Problem, Benchmark and Strategy",
      "url": "https://arxiv.org/abs/2602.01939",
      "date": "2026-02-02",
      "authors_text": "Yuxin He, Ruihao Zhang, Tianao Shen, Cheng Liu, Qiang Nie",
      "is_highlight": false,
      "score": 28.0,
      "summary": "The paper introduces the Exploratory and Focused Manipulation (EFM) problem, establishes the EFM-10 benchmark, and proposes a Bimanual Active Perception strategy to enhance manipulation tasks.",
      "teaser_image": "https://arxiv.org/html/2602.01939/x2.png",
      "debug_abstract": "Recently, active vision has reemerged as an important concept for manipulation, since visual occlusion occurs more frequently when main cameras are mounted on the robot heads. We reflect on the visual occlusion issue and identify its essence as the absence of information useful for task completion. Inspired by this, we come up with the more fundamental problem of Exploratory and Focused Manipulation (EFM). The proposed problem is about actively collecting information to complete challenging manipulation tasks that require exploration or focus. As an initial attempt to address this problem, we establish the EFM-10 benchmark that consists of 4 categories of tasks that align with our definition (10 tasks in total). We further come up with a Bimanual Active Perception (BAP) strategy, which leverages one arm to provide active vision and another arm to provide force sensing while manipulating. Based on this idea, we collect a dataset named BAPData for the tasks in EFM-10. With the dataset, we successfully verify the effectiveness of the BAP strategy in an imitation learning manner. We hope that the EFM-10 benchmark along with the BAP strategy can become a cornerstone that facilitates future research towards this direction. Project website: this http URL."
    },
    {
      "title": "Breaking the Static Graph: Context-Aware Traversal for Robust Retrieval-Augmented Generation",
      "url": "https://arxiv.org/abs/2602.01965",
      "date": "2026-02-02",
      "authors_text": "Kwun Hang Lau, Fangyuan Zhang, Boyu Ruan, Yingli Zhou, Qintian Guo",
      "is_highlight": false,
      "score": 27.0,
      "summary": "CatRAG enhances Retrieval-Augmented Generation by transforming static Knowledge Graphs into query-adaptive structures, improving multi-hop evidence retrieval and reasoning completeness.",
      "teaser_image": "https://arxiv.org/html/2602.01965/image/HubBiasShift.png",
      "debug_abstract": "Recent advances in Retrieval-Augmented Generation (RAG) have shifted from simple vector similarity to structure-aware approaches like HippoRAG, which leverage Knowledge Graphs (KGs) and Personalized PageRank (PPR) to capture multi-hop dependencies. However, these methods suffer from a &#34;Static Graph Fallacy&#34;: they rely on fixed transition probabilities determined during indexing. This rigidity ignores the query-dependent nature of edge relevance, causing semantic drift where random walks are diverted into high-degree &#34;hub&#34; nodes before reaching critical downstream evidence. Consequently, models often achieve high partial recall but fail to retrieve the complete evidence chain required for multi-hop queries. To address this, we propose CatRAG, Context-Aware Traversal for robust RAG, a framework that builds on the HippoRAG 2 architecture and transforms the static KG into a query-adaptive navigation structure. We introduce a multi-faceted framework to steer the random walk: (1) Symbolic Anchoring, which injects weak entity constraints to regularize the random walk; (2) Query-Aware Dynamic Edge Weighting, which dynamically modulates graph structure, to prune irrelevant paths while amplifying those aligned with the query&#39;s intent; and (3) Key-Fact Passage Weight Enhancement, a cost-efficient bias that structurally anchors the random walk to likely evidence. Experiments across four multi-hop benchmarks demonstrate that CatRAG consistently outperforms state of the art baselines. Our analysis reveals that while standard Recall metrics show modest gains, CatRAG achieves substantial improvements in reasoning completeness, the capacity to recover the entire evidence path without gaps. These results reveal that our approach effectively bridges the gap between retrieving partial context and enabling fully grounded reasoning. Resources are available at this https URL."
    },
    {
      "title": "Towards Agentic Intelligence for Materials Science",
      "url": "https://arxiv.org/abs/2602.00169",
      "date": "2026-01-29",
      "authors_text": "Huan Zhang, Yizhan Li, Wenhao Huang, Ziyu Hou, Yu Song",
      "is_highlight": false,
      "score": 12.0,
      "summary": "This paper advocates for developing agentic AI systems in materials science that autonomously plan, act, and learn to enhance discovery processes beyond traditional models.",
      "teaser_image": "https://arxiv.org/html/2602.00169/x1.png",
      "debug_abstract": "The convergence of artificial intelligence and materials science presents a transformative opportunity, but achieving true acceleration in discovery requires moving beyond task-isolated, fine-tuned models toward agentic systems that plan, act, and learn across the full discovery loop. This survey advances a unique pipeline-centric view that spans from corpus curation and pretraining, through domain adaptation and instruction tuning, to goal-conditioned agents interfacing with simulation and experimental platforms. Unlike prior reviews, we treat the entire process as an end-to-end system to be optimized for tangible discovery outcomes rather than proxy benchmarks. This perspective allows us to trace how upstream design choices-such as data curation and training objectives-can be aligned with downstream experimental success through effective credit assignment. To bridge communities and establish a shared frame of reference, we first present an integrated lens that aligns terminology, evaluation, and workflow stages across AI and materials science. We then analyze the field through two focused lenses: From the AI perspective, the survey details LLM strengths in pattern recognition, predictive analytics, and natural language processing for literature mining, materials characterization, and property prediction; from the materials science perspective, it highlights applications in materials design, process optimization, and the acceleration of computational workflows via integration with external tools (e.g., DFT, robotic labs). Finally, we contrast passive, reactive approaches with agentic design, cataloging current contributions while motivating systems that pursue long-horizon goals with autonomy, memory, and tool use. This survey charts a practical roadmap towards autonomous, safety-aware LLM agents aimed at discovering novel and useful materials."
    },
    {
      "title": "Advances and Innovations in the Multi-Agent Robotic System (MARS) Challenge",
      "url": "https://arxiv.org/abs/2601.18733",
      "date": "2026-01-26",
      "authors_text": "Li Kang, Heng Zhou, Xiufeng Song, Rui Li, Bruno N. Y. Chen",
      "is_highlight": false,
      "score": 78.0,
      "summary": "The MARS Challenge promotes advancements in multi-agent robotic systems by exploring vision-language models for collaborative task execution and planning in dynamic environments.",
      "teaser_image": null,
      "debug_abstract": "Recent advancements in multimodal large language models and vision-languageaction models have significantly driven progress in Embodied AI. As the field transitions toward more complex task scenarios, multi-agent system frameworks are becoming essential for achieving scalable, efficient, and collaborative solutions. This shift is fueled by three primary factors: increasing agent capabilities, enhancing system efficiency through task delegation, and enabling advanced human-agent interactions. To address the challenges posed by multi-agent collaboration, we propose the Multi-Agent Robotic System (MARS) Challenge, held at the NeurIPS 2025 Workshop on SpaVLE. The competition focuses on two critical areas: planning and control, where participants explore multi-agent embodied planning using vision-language models (VLMs) to coordinate tasks and policy execution to perform robotic manipulation in dynamic environments. By evaluating solutions submitted by participants, the challenge provides valuable insights into the design and coordination of embodied multi-agent systems, contributing to the future development of advanced collaborative AI systems."
    },
    {
      "title": "Gaussian Belief Propagation Network for Depth Completion",
      "url": "https://arxiv.org/abs/2601.21291",
      "date": "2026-01-29",
      "authors_text": "Jie Tang, Pingping Xie, Jian Li, Ping Tan",
      "is_highlight": false,
      "score": 68.0,
      "summary": "The Gaussian Belief Propagation Network (GBPN) integrates deep learning with probabilistic models for effective depth completion from sparse measurements, achieving state-of-the-art results across various benchmarks.",
      "teaser_image": "https://arxiv.org/html/2601.21291/x1.png",
      "debug_abstract": "Depth completion aims to predict a dense depth map from a color image with sparse depth measurements. Although deep learning methods have achieved state-of-the-art (SOTA), effectively handling the sparse and irregular nature of input depth data in deep networks remains a significant challenge, often limiting performance, especially under high sparsity. To overcome this limitation, we introduce the Gaussian Belief Propagation Network (GBPN), a novel hybrid framework synergistically integrating deep learning with probabilistic graphical models for end-to-end depth completion. Specifically, a scene-specific Markov Random Field (MRF) is dynamically constructed by the Graphical Model Construction Network (GMCN), and then inferred via Gaussian Belief Propagation (GBP) to yield the dense depth distribution. Crucially, the GMCN learns to construct not only the data-dependent potentials of MRF but also its structure by predicting adaptive non-local edges, enabling the capture of complex, long-range spatial dependencies. Furthermore, we enhance GBP with a serial \\&amp; parallel message passing scheme, designed for effective information propagation, particularly from sparse measurements. Extensive experiments demonstrate that GBPN achieves SOTA performance on the NYUv2 and KITTI benchmarks. Evaluations across varying sparsity levels, sparsity patterns, and datasets highlight GBPN&#39;s superior performance, notable robustness, and generalizable capability."
    },
    {
      "title": "4D-CAAL: 4D Radar-Camera Calibration and Auto-Labeling for Autonomous Driving",
      "url": "https://arxiv.org/abs/2601.21454",
      "date": "2026-01-29",
      "authors_text": "Shanliang Yao, Zhuoxiao Li, Runwei Guan, Kebin Cao, Meng Xia",
      "is_highlight": false,
      "score": 89.0,
      "summary": "The 4D-CAAL framework integrates 4D radar-camera calibration and auto-labeling using a dual-purpose target and robust correspondence matching, enhancing autonomous driving perception efficiency.",
      "teaser_image": "https://arxiv.org/html/2601.21454/images/4DRC-fixed.png",
      "debug_abstract": "4D radar has emerged as a critical sensor for autonomous driving, primarily due to its enhanced capabilities in elevation measurement and higher resolution compared to traditional 3D radar. Effective integration of 4D radar with cameras requires accurate extrinsic calibration, and the development of radar-based perception algorithms demands large-scale annotated datasets. However, existing calibration methods often employ separate targets optimized for either visual or radar modalities, complicating correspondence establishment. Furthermore, manually labeling sparse radar data is labor-intensive and unreliable. To address these challenges, we propose 4D-CAAL, a unified framework for 4D radar-camera calibration and auto-labeling. Our approach introduces a novel dual-purpose calibration target design, integrating a checkerboard pattern on the front surface for camera detection and a corner reflector at the center of the back surface for radar detection. We develop a robust correspondence matching algorithm that aligns the checkerboard center with the strongest radar reflection point, enabling accurate extrinsic calibration. Subsequently, we present an auto-labeling pipeline that leverages the calibrated sensor relationship to transfer annotations from camera-based segmentations to radar point clouds through geometric projection and multi-feature optimization. Extensive experiments demonstrate that our method achieves high calibration accuracy while significantly reducing manual annotation effort, thereby accelerating the development of robust multi-modal perception systems for autonomous driving."
    },
    {
      "title": "DynaWeb: Model-Based Reinforcement Learning of Web Agents",
      "url": "https://arxiv.org/abs/2601.22149",
      "date": "2026-01-29",
      "authors_text": "Hang Ding, Peidong Liu, Junqiao Wang, Ziwei Ji, Meng Cao",
      "is_highlight": false,
      "score": 77.0,
      "summary": "DynaWeb introduces a model-based reinforcement learning framework that enhances web agent training by simulating interactions with a web world model, improving efficiency and performance.",
      "teaser_image": "https://arxiv.org/html/2601.22149/figures/dynaweb.png",
      "debug_abstract": "The development of autonomous web agents, powered by Large Language Models (LLMs) and reinforcement learning (RL), represents a significant step towards general-purpose AI assistants. However, training these agents is severely hampered by the challenges of interacting with the live internet, which is inefficient, costly, and fraught with risks. Model-based reinforcement learning (MBRL) offers a promising solution by learning a world model of the environment to enable simulated interaction. This paper introduces DynaWeb, a novel MBRL framework that trains web agents through interacting with a web world model trained to predict naturalistic web page representations given agent actions. This model serves as a synthetic web environment where an agent policy can dream by generating vast quantities of rollout action trajectories for efficient online reinforcement learning. Beyond free policy rollouts, DynaWeb incorporates real expert trajectories from training data, which are randomly interleaved with on-policy rollouts during training to improve stability and sample efficiency. Experiments conducted on the challenging WebArena and WebVoyager benchmarks demonstrate that DynaWeb consistently and significantly improves the performance of state-of-the-art open-source web agent models. Our findings establish the viability of training web agents through imagination, offering a scalable and efficient way to scale up online agentic RL."
    },
    {
      "title": "Endogenous Reprompting: Self-Evolving Cognitive Alignment for Unified Multimodal Models",
      "url": "https://arxiv.org/abs/2601.20305",
      "date": "2026-01-28",
      "authors_text": "Zhenchen Tang, Songlin Yang, Zichuan Wang, Bo Peng, Yang Li",
      "is_highlight": false,
      "score": 69.0,
      "summary": "The paper introduces Endogenous Reprompting and the SEER framework, enhancing Unified Multimodal Models' generative reasoning through self-aligned descriptors and reinforcement learning, outperforming existing methods.",
      "teaser_image": "https://arxiv.org/html/2601.20305/x2.png",
      "debug_abstract": "Unified Multimodal Models (UMMs) exhibit strong understanding, yet this capability often fails to effectively guide generation. We identify this as a Cognitive Gap: the model lacks the understanding of how to enhance its own generation process. To bridge this gap, we propose Endogenous Reprompting, a mechanism that transforms the model&#39;s understanding from a passive encoding process into an explicit generative reasoning step by generating self-aligned descriptors during generation. To achieve this, we introduce SEER (Self-Evolving Evaluator and Reprompter), a training framework that establishes a two-stage endogenous loop using only 300 samples from a compact proxy task, Visual Instruction Elaboration. First, Reinforcement Learning with Verifiable Rewards (RLVR) activates the model&#39;s latent evaluation ability via curriculum learning, producing a high-fidelity endogenous reward signal. Second, Reinforcement Learning with Model-rewarded Thinking (RLMT) leverages this signal to optimize the generative reasoning policy. Experiments show that SEER consistently outperforms state-of-the-art baselines in evaluation accuracy, reprompting efficiency, and generation quality, without sacrificing general multimodal capabilities."
    },
    {
      "title": "Glance and Focus Reinforcement for Pan-cancer Screening",
      "url": "https://arxiv.org/abs/2601.19103",
      "date": "2026-01-27",
      "authors_text": "Linshan Wu, Jiaxin Zhuang, Hao Chen",
      "is_highlight": false,
      "score": 65.0,
      "summary": "GF-Screen introduces a novel reinforcement learning framework that enhances pan-cancer screening by effectively localizing and segmenting lesions in CT scans, outperforming existing methods.",
      "debug_abstract": "Pan-cancer screening in large-scale CT scans remains challenging for existing AI methods, primarily due to the difficulty of localizing diverse types of tiny lesions in large CT volumes. The extreme foreground-background imbalance significantly hinders models from focusing on diseased regions, while redundant focus on healthy regions not only decreases the efficiency but also increases false positives. Inspired by radiologists&#39; glance and focus diagnostic strategy, we introduce GF-Screen, a Glance and Focus reinforcement learning framework for pan-cancer screening. GF-Screen employs a Glance model to localize the diseased regions and a Focus model to precisely segment the lesions, where segmentation results of the Focus model are leveraged to reward the Glance model via Reinforcement Learning (RL). Specifically, the Glance model crops a group of sub-volumes from the entire CT volume and learns to select the sub-volumes with lesions for the Focus model to segment. Given that the selecting operation is non-differentiable for segmentation training, we propose to employ the segmentation results to reward the Glance model. To optimize the Glance model, we introduce a novel group relative learning paradigm, which employs group relative comparison to prioritize high-advantage predictions and discard low-advantage predictions within sub-volume groups, not only improving efficiency but also reducing false positives. In this way, for the first time, we effectively extend cutting-edge RL techniques to tackle the specific challenges in pan-cancer screening. Extensive experiments on 16 internal and 7 external datasets across 9 lesion types demonstrated the effectiveness of GF-Screen. Notably, GF-Screen leads the public validation leaderboard of MICCAI FLARE25 pan-cancer challenge, surpassing the FLARE24 champion solution by a large margin (+25.6% DSC and +28.2% NSD)."
    },
    {
      "title": "Enhancing Inverse Perspective Mapping for Automatic Vectorized Road Map Generation",
      "url": "https://arxiv.org/abs/2601.19536",
      "date": "2026-01-27",
      "authors_text": "Hongji Liu, Linwei Zheng, Yongjian Li, Mingkai Tang, Xiaoyang Yan",
      "is_highlight": false,
      "score": 70.0,
      "summary": "This study introduces a cost-effective framework using enhanced inverse perspective mapping and Catmull-Rom splines for high-precision, automatic vectorized road map generation.",
      "debug_abstract": "In this study, we present a low-cost and unified framework for vectorized road mapping leveraging enhanced inverse perspective mapping (IPM). In this framework, Catmull-Rom splines are utilized to characterize lane lines, and all the other ground markings are depicted using polygons uniformly. The results from instance segmentation serve as references to refine the three-dimensional position of spline control points and polygon corner points. In conjunction with this process, the homography matrix of IPM and vehicle poses are optimized simultaneously. Our proposed framework significantly reduces the mapping errors associated with IPM. It also improves the accuracy of the initial IPM homography matrix and the predicted vehicle poses. Furthermore, it addresses the limitations imposed by the coplanarity assumption in IPM. These enhancements enable IPM to be effectively applied to vectorized road mapping, which serves a cost-effective solution with enhanced accuracy. In addition, our framework generalizes road map elements to include all common ground markings and lane lines. The proposed framework is evaluated in two different practical scenarios, and the test results show that our method can automatically generate high-precision maps with near-centimeter-level accuracy. Importantly, the optimized IPM matrix achieves an accuracy comparable to that of manual calibration, while the accuracy of vehicle poses is also significantly improved."
    },
    {
      "title": "Geometry-Grounded Gaussian Splatting",
      "url": "https://arxiv.org/abs/2601.17835",
      "date": "2026-01-25",
      "authors_text": "Baowen Zhang, Chenxing Jiang, Heng Li, Shaojie Shen, Ping Tan",
      "is_highlight": true,
      "score": 53.0,
      "summary": "This paper introduces Geometry-Grounded Gaussian Splatting, enhancing shape extraction from Gaussian primitives by treating them as stochastic solids for improved depth map rendering and reconstruction quality.",
      "debug_abstract": "Gaussian Splatting (GS) has demonstrated impressive quality and efficiency in novel view synthesis. However, shape extraction from Gaussian primitives remains an open problem. Due to inadequate geometry parameterization and approximation, existing shape reconstruction methods suffer from poor multi-view consistency and are sensitive to floaters. In this paper, we present a rigorous theoretical derivation that establishes Gaussian primitives as a specific type of stochastic solids. This theoretical framework provides a principled foundation for Geometry-Grounded Gaussian Splatting by enabling the direct treatment of Gaussian primitives as explicit geometric representations. Using the volumetric nature of stochastic solids, our method efficiently renders high-quality depth maps for fine-grained geometry extraction. Experiments show that our method achieves the best shape reconstruction results among all Gaussian Splatting-based methods on public datasets."
    },
    {
      "title": "CoT-Seg: Rethinking Segmentation with Chain-of-Thought Reasoning and Self-Correction",
      "url": "https://arxiv.org/abs/2601.17420",
      "date": "2026-01-24",
      "authors_text": "Shiu-hong Kao, Chak Ho Huang, Huaiqian Liu, Yu-Wing Tai, Chi-Keung Tang",
      "is_highlight": false,
      "score": 68.0,
      "summary": "CoT-Seg introduces a training-free framework that enhances segmentation through chain-of-thought reasoning and self-correction, improving performance on complex queries and ambiguous images.",
      "debug_abstract": "Existing works of reasoning segmentation often fall short in complex cases, particularly when addressing complicated queries and out-of-domain images. Inspired by the chain-of-thought reasoning, where harder problems require longer thinking steps/time, this paper aims to explore a system that can think step-by-step, look up information if needed, generate results, self-evaluate its own results, and refine the results, in the same way humans approach harder questions. We introduce CoT-Seg, a training-free framework that rethinks reasoning segmentation by combining chain-of-thought reasoning with self-correction. Instead of fine-tuning, CoT-Seg leverages the inherent reasoning ability of pre-trained MLLMs (GPT-4o) to decompose queries into meta-instructions, extract fine-grained semantics from images, and identify target objects even under implicit or complex prompts. Moreover, CoT-Seg incorporates a self-correction stage: the model evaluates its own segmentation against the original query and reasoning trace, identifies mismatches, and iteratively refines the mask. This tight integration of reasoning and correction significantly improves reliability and robustness, especially in ambiguous or error-prone cases. Furthermore, our CoT-Seg framework allows easy incorporation of retrieval-augmented reasoning, enabling the system to access external knowledge when the input lacks sufficient information. To showcase CoT-Seg&#39;s ability to handle very challenging cases ,we introduce a new dataset ReasonSeg-Hard. Our results highlight that combining chain-of-thought reasoning, self-correction, offers a powerful paradigm for vision-language integration driven segmentation."
    },
    {
      "title": "Uni-RS: A Spatially Faithful Unified Understanding and Generation Model for Remote Sensing",
      "url": "https://arxiv.org/abs/2601.17673",
      "date": "2026-01-25",
      "authors_text": "Weiyu Zhang, Yuan Hu, Yong Li, Yu Liu",
      "is_highlight": false,
      "score": 58.0,
      "summary": "Uni-RS is a novel unified multimodal model for remote sensing that enhances spatial faithfulness in text-to-image generation by integrating spatial layout planning and supervision techniques.",
      "debug_abstract": "Unified remote sensing multimodal models exhibit a pronounced spatial reversal curse: Although they can accurately recognize and describe object locations in images, they often fail to faithfully execute the same spatial relations during text-to-image generation, where such relations constitute core semantic information in remote sensing. Motivated by this observation, we propose Uni-RS, the first unified multimodal model tailored for remote sensing, to explicitly address the spatial asymmetry between understanding and generation. Specifically, we first introduce explicit Spatial-Layout Planning to transform textual instructions into spatial layout plans, decoupling geometric planning from visual synthesis. We then impose Spatial-Aware Query Supervision to bias learnable queries toward spatial relations explicitly specified in the instruction. Finally, we develop Image-Caption Spatial Layout Variation to expose the model to systematic geometry-consistent spatial transformations. Extensive experiments across multiple benchmarks show that our approach substantially improves spatial faithfulness in text-to-image generation, while maintaining strong performance on multimodal understanding tasks like image captioning, visual grounding, and VQA tasks."
    },
    {
      "title": "TC-IDM: Grounding Video Generation for Executable Zero-shot Robot Motion",
      "url": "https://arxiv.org/abs/2601.18323",
      "date": "2026-01-26",
      "authors_text": "Weishi Mi, Yong Bao, Xiaowei Chi, Xiaozhu Ju, Zhiyuan Qin",
      "is_highlight": false,
      "score": 89.0,
      "summary": "The TC-IDM model enhances robot motion execution by bridging visual planning and physical control through tool-centric trajectory synthesis, achieving superior generalization and performance in diverse tasks.",
      "debug_abstract": "The vision-language-action (VLA) paradigm has enabled powerful robotic control by leveraging vision-language models, but its reliance on large-scale, high-quality robot data limits its generalization. Generative world models offer a promising alternative for general-purpose embodied AI, yet a critical gap remains between their pixel-level plans and physically executable actions. To this end, we propose the Tool-Centric Inverse Dynamics Model (TC-IDM). By focusing on the tool&#39;s imagined trajectory as synthesized by the world model, TC-IDM establishes a robust intermediate representation that bridges the gap between visual planning and physical control. TC-IDM extracts the tool&#39;s point cloud trajectories via segmentation and 3D motion estimation from generated videos. Considering diverse tool attributes, our architecture employs decoupled action heads to project these planned trajectories into 6-DoF end-effector motions and corresponding control signals. This plan-and-translate paradigm not only supports a wide range of end-effectors but also significantly improves viewpoint invariance. Furthermore, it exhibits strong generalization capabilities across long-horizon and out-of-distribution tasks, including interacting with deformable objects. In real-world evaluations, the world model with TC-IDM achieves an average success rate of 61.11 percent, with 77.7 percent on simple tasks and 38.46 percent on zero-shot deformable object tasks. It substantially outperforms end-to-end VLA-style baselines and other inverse dynamics models."
    },
    {
      "title": "Fast and Safe Trajectory Optimization for Mobile Manipulators With Neural Configuration Space Distance Field",
      "url": "https://arxiv.org/abs/2601.18548",
      "date": "2026-01-26",
      "authors_text": "Yulin Li, Zhiyuan Song, Yiming Li, Zhicheng Song, Kai Chen",
      "is_highlight": false,
      "score": 86.0,
      "summary": "This paper introduces Generalized Configuration Space Distance Fields (GCDF) for efficient trajectory optimization in mobile manipulators, enhancing collision reasoning and enabling agile motion in complex environments.",
      "debug_abstract": "Mobile manipulators promise agile, long-horizon behavior by coordinating base and arm motion, yet whole-body trajectory optimization in cluttered, confined spaces remains difficult due to high-dimensional nonconvexity and the need for fast, accurate collision reasoning. Configuration Space Distance Fields (CDF) enable fixed-base manipulators to model collisions directly in configuration space via smooth, implicit distances. This representation holds strong potential to bypass the nonlinear configuration-to-workspace mapping while preserving accurate whole-body geometry and providing optimization-friendly collision costs. Yet, extending this capability to mobile manipulators is hindered by unbounded workspaces and tighter base-arm coupling. We lift this promise to mobile manipulation with Generalized Configuration Space Distance Fields (GCDF), extending CDF to robots with both translational and rotational joints in unbounded workspaces with tighter base-arm coupling. We prove that GCDF preserves Euclidean-like local distance structure and accurately encodes whole-body geometry in configuration space, and develop a data generation and training pipeline that yields continuous neural GCDFs with accurate values and gradients, supporting efficient GPU-batched queries. Building on this representation, we develop a high-performance sequential convex optimization framework centered on GCDF-based collision reasoning. The solver scales to large numbers of implicit constraints through (i) online specification of neural constraints, (ii) sparsity-aware active-set detection with parallel batched evaluation across thousands of constraints, and (iii) incremental constraint management for rapid replanning under scene changes."
    },
    {
      "title": "EvoCUA: Evolving Computer Use Agents via Learning from Scalable Synthetic Experience",
      "url": "https://arxiv.org/abs/2601.15876",
      "date": "2026-01-22",
      "authors_text": "Taofeng Xue, Chong Peng, Mianqiu Huang, Linsen Guo, Tiancheng Han",
      "is_highlight": false,
      "score": 82.6,
      "summary": "EvoCUA introduces a self-sustaining evolutionary model for computer-use agents, leveraging autonomous task generation and iterative learning to significantly outperform existing benchmarks in multimodal AI.",
      "debug_abstract": "The development of native computer-use agents (CUA) represents a significant leap in multimodal AI. However, their potential is currently bottlenecked by the constraints of static data scaling. Existing paradigms relying primarily on passive imitation of static datasets struggle to capture the intricate causal dynamics inherent in long-horizon computer tasks. In this work, we introduce EvoCUA, a native computer use agentic model. Unlike static imitation, EvoCUA integrates data generation and policy optimization into a self-sustaining evolutionary cycle. To mitigate data scarcity, we develop a verifiable synthesis engine that autonomously generates diverse tasks coupled with executable validators. To enable large-scale experience acquisition, we design a scalable infrastructure orchestrating tens of thousands of asynchronous sandbox rollouts. Building on these massive trajectories, we propose an iterative evolving learning strategy to efficiently internalize this experience. This mechanism dynamically regulates policy updates by identifying capability boundaries -- reinforcing successful routines while transforming failure trajectories into rich supervision through error analysis and self-correction. Empirical evaluations on the OSWorld benchmark demonstrate that EvoCUA achieves a success rate of 56.7%, establishing a new open-source state-of-the-art. Notably, EvoCUA significantly outperforms the previous best open-source model, OpenCUA-72B (45.0%), and surpasses leading closed-weights models such as UI-TARS-2 (53.1%). Crucially, our results underscore the generalizability of this approach: the evolving paradigm driven by learning from experience yields consistent performance gains across foundation models of varying scales, establishing a robust and scalable path for advancing native agent capabilities."
    },
    {
      "title": "DualShield: Safe Model Predictive Diffusion via Reachability Analysis for Interactive Autonomous Driving",
      "url": "https://arxiv.org/abs/2601.15729",
      "date": "2026-01-22",
      "authors_text": "Rui Yang, Lei Zheng, Ruoyu Yao, Jun Ma",
      "is_highlight": true,
      "score": 92.7,
      "summary": "DualShield enhances autonomous driving safety by integrating Hamilton-Jacobi reachability analysis into diffusion models for proactive guidance and reactive safety assurance under uncertain interactions.",
      "debug_abstract": "Diffusion models have emerged as a powerful approach for multimodal motion planning in autonomous driving. However, their practical deployment is typically hindered by the inherent difficulty in enforcing vehicle dynamics and a critical reliance on accurate predictions of other agents, making them prone to safety issues under uncertain interactions. To address these limitations, we introduce DualShield, a planning and control framework that leverages Hamilton-Jacobi (HJ) reachability value functions in a dual capacity. First, the value functions act as proactive guidance, steering the diffusion denoising process towards safe and dynamically feasible regions. Second, they form a reactive safety shield using control barrier-value functions (CBVFs) to modify the executed actions and ensure safety. This dual mechanism preserves the rich exploration capabilities of diffusion models while providing principled safety assurance under uncertain and even adversarial interactions. Simulations in challenging unprotected U-turn scenarios demonstrate that DualShield significantly improves both safety and task efficiency compared to leading methods from different planning paradigms under uncertainty."
    }
  ],
  "机器人研究所 (CKSRI)": [
    {
      "title": "Visual Reasoning over Time Series via Multi-Agent System",
      "url": "https://arxiv.org/abs/2602.03026",
      "date": "2026-02-03",
      "authors_text": "Weilin Ruan, Yuxuan Liang",
      "is_highlight": false,
      "score": 66.0,
      "summary": "MAS4TS is a multi-agent system that enhances time series analysis through visual reasoning and adaptive tool usage, achieving state-of-the-art performance across diverse tasks.",
      "teaser_image": "https://arxiv.org/html/2602.03026/x2.png",
      "debug_abstract": "Time series analysis underpins many real-world applications, yet existing time-series-specific methods and pretrained large-model-based approaches remain limited in integrating intuitive visual reasoning and generalizing across tasks with adaptive tool usage. To address these limitations, we propose MAS4TS, a tool-driven multi-agent system for general time series tasks, built upon an Analyzer-Reasoner-Executor paradigm that integrates agent communication, visual reasoning, and latent reconstruction within a unified framework. MAS4TS first performs visual reasoning over time series plots with structured priors using a Vision-Language Model to extract temporal structures, and subsequently reconstructs predictive trajectories in latent space. Three specialized agents coordinate via shared memory and gated communication, while a router selects task-specific tool chains for execution. Extensive experiments on multiple benchmarks demonstrate that MAS4TS achieves state-of-the-art performance across a wide range of time series tasks, while exhibiting strong generalization and efficient inference."
    },
    {
      "title": "Neural Predictor-Corrector: Solving Homotopy Problems with Reinforcement Learning",
      "url": "https://arxiv.org/abs/2602.03086",
      "date": "2026-02-03",
      "authors_text": "Jiayao Mai, Bangyan Liao, Zhenjun Zhao, Yingping Zeng, Haoang Li",
      "is_highlight": false,
      "score": 64.0,
      "summary": "The Neural Predictor-Corrector (NPC) framework uses reinforcement learning to automate step size and termination decisions in solving diverse homotopy problems, outperforming traditional methods.",
      "teaser_image": "https://arxiv.org/html/2602.03086/x2.png",
      "debug_abstract": "The Homotopy paradigm, a general principle for solving challenging problems, appears across diverse domains such as robust optimization, global optimization, polynomial root-finding, and sampling. Practical solvers for these problems typically follow a predictor-corrector (PC) structure, but rely on hand-crafted heuristics for step sizes and iteration termination, which are often suboptimal and task-specific. To address this, we unify these problems under a single framework, which enables the design of a general neural solver. Building on this unified view, we propose Neural Predictor-Corrector (NPC), which replaces hand-crafted heuristics with automatically learned policies. NPC formulates policy selection as a sequential decision-making problem and leverages reinforcement learning to automatically discover efficient strategies. To further enhance generalization, we introduce an amortized training mechanism, enabling one-time offline training for a class of problems and efficient online inference on new instances. Experiments on four representative homotopy problems demonstrate that our method generalizes effectively to unseen instances. It consistently outperforms classical and specialized baselines in efficiency while demonstrating superior stability across tasks, highlighting the value of unifying homotopy methods into a single neural framework."
    },
    {
      "title": "Diffusion-Driven Inter-Outer Surface Separation for Point Clouds with Open Boundaries",
      "url": "https://arxiv.org/abs/2602.00739",
      "date": "2026-01-31",
      "authors_text": "Zhengyan Qin, Liyuan Qiu",
      "is_highlight": false,
      "score": 72.0,
      "summary": "The paper presents a diffusion-based algorithm for effectively separating inter and outer surfaces in double-layered point clouds with open boundaries, addressing artifacts from TSDF fusion.",
      "teaser_image": "https://arxiv.org/html/2602.00739/figures/pipeline5.png",
      "debug_abstract": "We propose a diffusion-based algorithm for separating the inter and outer layer surfaces from double-layered point clouds, particularly those exhibiting the &#34;double surface artifact&#34; caused by truncation in Truncated Signed Distance Function (TSDF) fusion during indoor or medical 3D reconstruction. This artifact arises from asymmetric truncation thresholds, leading to erroneous inter and outer shells in the fused volume, which our method addresses by extracting the true inter layer to mitigate challenges like overlapping surfaces and disordered normals. We focus on point clouds with \\emph{open boundaries} (i.e., sampled surfaces with topological openings/holes through which particles may escape), rather than point clouds with \\emph{missing surface regions} where no samples exist. Our approach enables robust processing of both watertight and open-boundary models, achieving extraction of the inter layer from 20,000 inter and 20,000 outer points in approximately 10 seconds. This solution is particularly effective for applications requiring accurate surface representations, such as indoor scene modeling and medical imaging, where double-layered point clouds are prevalent, and it accommodates both closed (watertight) and open-boundary surface geometries. Our goal is \\emph{post-hoc} inter/outer shell separation as a lightweight module after TSDF fusion; we do not aim to replace full variational or learning-based reconstruction pipelines."
    },
    {
      "title": "Physics-informed Diffusion Mamba Transformer for Real-world Driving",
      "url": "https://arxiv.org/abs/2602.00808",
      "date": "2026-01-31",
      "authors_text": "Hang Zhou, Qiang Zhang, Peiran Liu, Yihao Qin, Zhaoxu Yan",
      "is_highlight": false,
      "score": 74.0,
      "summary": "The paper presents a Diffusion Mamba Transformer that integrates sequential context and physical constraints for improved trajectory planning in autonomous driving, outperforming existing models.",
      "teaser_image": "https://arxiv.org/html/2602.00808/difference_pipeline3.png",
      "debug_abstract": "Autonomous driving systems demand trajectory planners that not only model the inherent uncertainty of future motions but also respect complex temporal dependencies and underlying physical laws. While diffusion-based generative models excel at capturing multi-modal distributions, they often fail to incorporate long-term sequential contexts and domain-specific physical priors. In this work, we bridge these gaps with two key innovations. First, we introduce a Diffusion Mamba Transformer architecture that embeds mamba and attention into the diffusion process, enabling more effective aggregation of sequential input contexts from sensor streams and past motion histories. Second, we design a Port-Hamiltonian Neural Network module that seamlessly integrates energy-based physical constraints into the diffusion model, thereby enhancing trajectory predictions with both consistency and interpretability. Extensive evaluations on standard autonomous driving benchmarks demonstrate that our unified framework significantly outperforms state-of-the-art baselines in predictive accuracy, physical plausibility, and robustness, thereby advancing safe and reliable motion planning."
    },
    {
      "title": "Invariance on Manifolds: Understanding Robust Visual Representations for Place Recognition",
      "url": "https://arxiv.org/abs/2602.00841",
      "date": "2026-01-31",
      "authors_text": "Jintao Cheng, Weibin Li, Zhijian He, Jin Wu, Chi Man Vong",
      "is_highlight": false,
      "score": 66.0,
      "summary": "This paper presents a training-free Second-Order Geometric Statistics framework for robust visual place recognition, utilizing covariance descriptors on the SPD manifold for effective zero-shot generalization.",
      "teaser_image": "https://arxiv.org/html/2602.00841/x2.png",
      "debug_abstract": "Visual Place Recognition (VPR) demands representations robust to drastic environmental and viewpoint shifts. Current aggregation paradigms, however, either rely on data-hungry supervision or simplistic first-order statistics, often neglecting intrinsic structural correlations. In this work, we propose a Second-Order Geometric Statistics framework that inherently captures geometric stability without training. We conceptualize scenes as covariance descriptors on the Symmetric Positive Definite (SPD) manifold, where perturbations manifest as tractable congruence transformations. By leveraging geometry-aware Riemannian mappings, we project these descriptors into a linearized Euclidean embedding, effectively decoupling signal structure from noise. Our approach introduces a training-free framework built upon fixed, pre-trained backbones, achieving strong zero-shot generalization without parameter updates. Extensive experiments confirm that our method achieves highly competitive performance against state-of-the-art baselines, particularly excelling in challenging zero-shot scenarios."
    },
    {
      "title": "PolicyFlow: Policy Optimization with Continuous Normalizing Flow in Reinforcement Learning",
      "url": "https://arxiv.org/abs/2602.01156",
      "date": "2026-02-01",
      "authors_text": "Shunpeng Yang, Ben Liu, Hua Chen",
      "is_highlight": false,
      "score": 71.0,
      "summary": "PolicyFlow is a novel reinforcement learning algorithm that integrates continuous normalizing flows with PPO, enhancing performance and stability while reducing computational costs and encouraging diverse behaviors.",
      "teaser_image": "https://arxiv.org/html/2602.01156/x1.png",
      "debug_abstract": "Among on-policy reinforcement learning algorithms, Proximal Policy Optimization (PPO) demonstrates is widely favored for its simplicity, numerical stability, and strong empirical performance. Standard PPO relies on surrogate objectives defined via importance ratios, which require evaluating policy likelihood that is typically straightforward when the policy is modeled as a Gaussian distribution. However, extending PPO to more expressive, high-capacity policy models such as continuous normalizing flows (CNFs), also known as flow-matching models, is challenging because likelihood evaluation along the full flow trajectory is computationally expensive and often numerically unstable. To resolve this issue, we propose PolicyFlow, a novel on-policy CNF-based reinforcement learning algorithm that integrates expressive CNF policies with PPO-style objectives without requiring likelihood evaluation along the full flow path. PolicyFlow approximates importance ratios using velocity field variations along a simple interpolation path, reducing computational overhead without compromising training stability. To further prevent mode collapse and further encourage diverse behaviors, we propose the Brownian Regularizer, an implicit policy entropy regularizer inspired by Brownian motion, which is conceptually elegant and computationally lightweight. Experiments on diverse tasks across various environments including MultiGoal, PointMaze, IsaacLab and MuJoCo Playground show that PolicyFlow achieves competitive or superior performance compared to PPO using Gaussian policies and flow-based baselines including FPO and DPPO. Notably, results on MultiGoal highlight PolicyFlow&#39;s ability to capture richer multimodal action distributions."
    },
    {
      "title": "Do All Individual Layers Help? An Empirical Study of Task-Interfering Layers in Vision-Language Models",
      "url": "https://arxiv.org/abs/2602.01167",
      "date": "2026-02-01",
      "authors_text": "Zhiming Liu, Yujie Wei, Lei Feng, Xiu Su, Xiaobo Xia",
      "is_highlight": false,
      "score": 52.0,
      "summary": "This study identifies Task-Interfering Layers in vision-language models, demonstrating that selectively bypassing certain layers can enhance task performance without retraining.",
      "teaser_image": "https://arxiv.org/html/2602.01167/x1.png",
      "debug_abstract": "Current VLMs have demonstrated capabilities across a wide range of multimodal tasks. Typically, in a pretrained VLM, all layers are engaged by default to make predictions on downstream tasks. We find that intervening on a single layer, such as by zeroing its parameters, can improve the performance on certain tasks, indicating that some layers hinder rather than help downstream tasks. We systematically investigate how individual layers influence different tasks via layer intervention. Specifically, we measure the change in performance relative to the base model after intervening on each layer and observe improvements when bypassing specific layers. This improvement can be generalizable across models and datasets, indicating the presence of Task-Interfering Layers that harm downstream tasks&#39; performance. We introduce Task-Layer Interaction Vector, which quantifies the effect of intervening on each layer of a VLM given a task. These task-interfering layers exhibit task-specific sensitivity patterns: tasks requiring similar capabilities show consistent response trends under layer interventions, as evidenced by the high similarity in their task-layer interaction vectors. Inspired by these findings, we propose TaLo (Task-Adaptive Layer Knockout), a training-free, test-time adaptation method that dynamically identifies and bypasses the most interfering layer for a given task. Without parameter updates, TaLo improves performance across various models and datasets, including boosting Qwen-VL&#39;s accuracy on the Maps task in ScienceQA by up to 16.6%. Our work reveals an unexpected form of modularity in pretrained VLMs and provides a plug-and-play, training-free mechanism to unlock hidden capabilities at inference time. The source code will be publicly available."
    },
    {
      "title": "Towards Exploratory and Focused Manipulation with Bimanual Active Perception: A New Problem, Benchmark and Strategy",
      "url": "https://arxiv.org/abs/2602.01939",
      "date": "2026-02-02",
      "authors_text": "Yuxin He, Ruihao Zhang, Tianao Shen, Cheng Liu, Qiang Nie",
      "is_highlight": false,
      "score": 28.0,
      "summary": "The paper introduces the Exploratory and Focused Manipulation (EFM) problem, establishes the EFM-10 benchmark, and proposes a Bimanual Active Perception strategy to enhance manipulation tasks.",
      "teaser_image": "https://arxiv.org/html/2602.01939/x2.png",
      "debug_abstract": "Recently, active vision has reemerged as an important concept for manipulation, since visual occlusion occurs more frequently when main cameras are mounted on the robot heads. We reflect on the visual occlusion issue and identify its essence as the absence of information useful for task completion. Inspired by this, we come up with the more fundamental problem of Exploratory and Focused Manipulation (EFM). The proposed problem is about actively collecting information to complete challenging manipulation tasks that require exploration or focus. As an initial attempt to address this problem, we establish the EFM-10 benchmark that consists of 4 categories of tasks that align with our definition (10 tasks in total). We further come up with a Bimanual Active Perception (BAP) strategy, which leverages one arm to provide active vision and another arm to provide force sensing while manipulating. Based on this idea, we collect a dataset named BAPData for the tasks in EFM-10. With the dataset, we successfully verify the effectiveness of the BAP strategy in an imitation learning manner. We hope that the EFM-10 benchmark along with the BAP strategy can become a cornerstone that facilitates future research towards this direction. Project website: this http URL."
    },
    {
      "title": "Breaking the Static Graph: Context-Aware Traversal for Robust Retrieval-Augmented Generation",
      "url": "https://arxiv.org/abs/2602.01965",
      "date": "2026-02-02",
      "authors_text": "Kwun Hang Lau, Fangyuan Zhang, Boyu Ruan, Yingli Zhou, Qintian Guo",
      "is_highlight": false,
      "score": 27.0,
      "summary": "CatRAG enhances Retrieval-Augmented Generation by transforming static Knowledge Graphs into query-adaptive structures, improving multi-hop evidence retrieval and reasoning completeness.",
      "teaser_image": "https://arxiv.org/html/2602.01965/image/HubBiasShift.png",
      "debug_abstract": "Recent advances in Retrieval-Augmented Generation (RAG) have shifted from simple vector similarity to structure-aware approaches like HippoRAG, which leverage Knowledge Graphs (KGs) and Personalized PageRank (PPR) to capture multi-hop dependencies. However, these methods suffer from a &#34;Static Graph Fallacy&#34;: they rely on fixed transition probabilities determined during indexing. This rigidity ignores the query-dependent nature of edge relevance, causing semantic drift where random walks are diverted into high-degree &#34;hub&#34; nodes before reaching critical downstream evidence. Consequently, models often achieve high partial recall but fail to retrieve the complete evidence chain required for multi-hop queries. To address this, we propose CatRAG, Context-Aware Traversal for robust RAG, a framework that builds on the HippoRAG 2 architecture and transforms the static KG into a query-adaptive navigation structure. We introduce a multi-faceted framework to steer the random walk: (1) Symbolic Anchoring, which injects weak entity constraints to regularize the random walk; (2) Query-Aware Dynamic Edge Weighting, which dynamically modulates graph structure, to prune irrelevant paths while amplifying those aligned with the query&#39;s intent; and (3) Key-Fact Passage Weight Enhancement, a cost-efficient bias that structurally anchors the random walk to likely evidence. Experiments across four multi-hop benchmarks demonstrate that CatRAG consistently outperforms state of the art baselines. Our analysis reveals that while standard Recall metrics show modest gains, CatRAG achieves substantial improvements in reasoning completeness, the capacity to recover the entire evidence path without gaps. These results reveal that our approach effectively bridges the gap between retrieving partial context and enabling fully grounded reasoning. Resources are available at this https URL."
    },
    {
      "title": "Towards Agentic Intelligence for Materials Science",
      "url": "https://arxiv.org/abs/2602.00169",
      "date": "2026-01-29",
      "authors_text": "Huan Zhang, Yizhan Li, Wenhao Huang, Ziyu Hou, Yu Song",
      "is_highlight": false,
      "score": 12.0,
      "summary": "This paper advocates for developing agentic AI systems in materials science that autonomously plan, act, and learn to enhance discovery processes beyond traditional models.",
      "teaser_image": "https://arxiv.org/html/2602.00169/x1.png",
      "debug_abstract": "The convergence of artificial intelligence and materials science presents a transformative opportunity, but achieving true acceleration in discovery requires moving beyond task-isolated, fine-tuned models toward agentic systems that plan, act, and learn across the full discovery loop. This survey advances a unique pipeline-centric view that spans from corpus curation and pretraining, through domain adaptation and instruction tuning, to goal-conditioned agents interfacing with simulation and experimental platforms. Unlike prior reviews, we treat the entire process as an end-to-end system to be optimized for tangible discovery outcomes rather than proxy benchmarks. This perspective allows us to trace how upstream design choices-such as data curation and training objectives-can be aligned with downstream experimental success through effective credit assignment. To bridge communities and establish a shared frame of reference, we first present an integrated lens that aligns terminology, evaluation, and workflow stages across AI and materials science. We then analyze the field through two focused lenses: From the AI perspective, the survey details LLM strengths in pattern recognition, predictive analytics, and natural language processing for literature mining, materials characterization, and property prediction; from the materials science perspective, it highlights applications in materials design, process optimization, and the acceleration of computational workflows via integration with external tools (e.g., DFT, robotic labs). Finally, we contrast passive, reactive approaches with agentic design, cataloging current contributions while motivating systems that pursue long-horizon goals with autonomy, memory, and tool use. This survey charts a practical roadmap towards autonomous, safety-aware LLM agents aimed at discovering novel and useful materials."
    },
    {
      "title": "Advances and Innovations in the Multi-Agent Robotic System (MARS) Challenge",
      "url": "https://arxiv.org/abs/2601.18733",
      "date": "2026-01-26",
      "authors_text": "Li Kang, Heng Zhou, Xiufeng Song, Rui Li, Bruno N. Y. Chen",
      "is_highlight": false,
      "score": 78.0,
      "summary": "The MARS Challenge promotes advancements in multi-agent robotic systems by exploring vision-language models for collaborative task execution and planning in dynamic environments.",
      "teaser_image": null,
      "debug_abstract": "Recent advancements in multimodal large language models and vision-languageaction models have significantly driven progress in Embodied AI. As the field transitions toward more complex task scenarios, multi-agent system frameworks are becoming essential for achieving scalable, efficient, and collaborative solutions. This shift is fueled by three primary factors: increasing agent capabilities, enhancing system efficiency through task delegation, and enabling advanced human-agent interactions. To address the challenges posed by multi-agent collaboration, we propose the Multi-Agent Robotic System (MARS) Challenge, held at the NeurIPS 2025 Workshop on SpaVLE. The competition focuses on two critical areas: planning and control, where participants explore multi-agent embodied planning using vision-language models (VLMs) to coordinate tasks and policy execution to perform robotic manipulation in dynamic environments. By evaluating solutions submitted by participants, the challenge provides valuable insights into the design and coordination of embodied multi-agent systems, contributing to the future development of advanced collaborative AI systems."
    },
    {
      "title": "Gaussian Belief Propagation Network for Depth Completion",
      "url": "https://arxiv.org/abs/2601.21291",
      "date": "2026-01-29",
      "authors_text": "Jie Tang, Pingping Xie, Jian Li, Ping Tan",
      "is_highlight": false,
      "score": 68.0,
      "summary": "The Gaussian Belief Propagation Network (GBPN) integrates deep learning with probabilistic models for effective depth completion from sparse measurements, achieving state-of-the-art results across various benchmarks.",
      "teaser_image": "https://arxiv.org/html/2601.21291/x1.png",
      "debug_abstract": "Depth completion aims to predict a dense depth map from a color image with sparse depth measurements. Although deep learning methods have achieved state-of-the-art (SOTA), effectively handling the sparse and irregular nature of input depth data in deep networks remains a significant challenge, often limiting performance, especially under high sparsity. To overcome this limitation, we introduce the Gaussian Belief Propagation Network (GBPN), a novel hybrid framework synergistically integrating deep learning with probabilistic graphical models for end-to-end depth completion. Specifically, a scene-specific Markov Random Field (MRF) is dynamically constructed by the Graphical Model Construction Network (GMCN), and then inferred via Gaussian Belief Propagation (GBP) to yield the dense depth distribution. Crucially, the GMCN learns to construct not only the data-dependent potentials of MRF but also its structure by predicting adaptive non-local edges, enabling the capture of complex, long-range spatial dependencies. Furthermore, we enhance GBP with a serial \\&amp; parallel message passing scheme, designed for effective information propagation, particularly from sparse measurements. Extensive experiments demonstrate that GBPN achieves SOTA performance on the NYUv2 and KITTI benchmarks. Evaluations across varying sparsity levels, sparsity patterns, and datasets highlight GBPN&#39;s superior performance, notable robustness, and generalizable capability."
    },
    {
      "title": "4D-CAAL: 4D Radar-Camera Calibration and Auto-Labeling for Autonomous Driving",
      "url": "https://arxiv.org/abs/2601.21454",
      "date": "2026-01-29",
      "authors_text": "Shanliang Yao, Zhuoxiao Li, Runwei Guan, Kebin Cao, Meng Xia",
      "is_highlight": false,
      "score": 89.0,
      "summary": "The 4D-CAAL framework integrates 4D radar-camera calibration and auto-labeling using a dual-purpose target and robust correspondence matching, enhancing autonomous driving perception efficiency.",
      "teaser_image": "https://arxiv.org/html/2601.21454/images/4DRC-fixed.png",
      "debug_abstract": "4D radar has emerged as a critical sensor for autonomous driving, primarily due to its enhanced capabilities in elevation measurement and higher resolution compared to traditional 3D radar. Effective integration of 4D radar with cameras requires accurate extrinsic calibration, and the development of radar-based perception algorithms demands large-scale annotated datasets. However, existing calibration methods often employ separate targets optimized for either visual or radar modalities, complicating correspondence establishment. Furthermore, manually labeling sparse radar data is labor-intensive and unreliable. To address these challenges, we propose 4D-CAAL, a unified framework for 4D radar-camera calibration and auto-labeling. Our approach introduces a novel dual-purpose calibration target design, integrating a checkerboard pattern on the front surface for camera detection and a corner reflector at the center of the back surface for radar detection. We develop a robust correspondence matching algorithm that aligns the checkerboard center with the strongest radar reflection point, enabling accurate extrinsic calibration. Subsequently, we present an auto-labeling pipeline that leverages the calibrated sensor relationship to transfer annotations from camera-based segmentations to radar point clouds through geometric projection and multi-feature optimization. Extensive experiments demonstrate that our method achieves high calibration accuracy while significantly reducing manual annotation effort, thereby accelerating the development of robust multi-modal perception systems for autonomous driving."
    },
    {
      "title": "DynaWeb: Model-Based Reinforcement Learning of Web Agents",
      "url": "https://arxiv.org/abs/2601.22149",
      "date": "2026-01-29",
      "authors_text": "Hang Ding, Peidong Liu, Junqiao Wang, Ziwei Ji, Meng Cao",
      "is_highlight": false,
      "score": 77.0,
      "summary": "DynaWeb introduces a model-based reinforcement learning framework that enhances web agent training by simulating interactions with a web world model, improving efficiency and performance.",
      "teaser_image": "https://arxiv.org/html/2601.22149/figures/dynaweb.png",
      "debug_abstract": "The development of autonomous web agents, powered by Large Language Models (LLMs) and reinforcement learning (RL), represents a significant step towards general-purpose AI assistants. However, training these agents is severely hampered by the challenges of interacting with the live internet, which is inefficient, costly, and fraught with risks. Model-based reinforcement learning (MBRL) offers a promising solution by learning a world model of the environment to enable simulated interaction. This paper introduces DynaWeb, a novel MBRL framework that trains web agents through interacting with a web world model trained to predict naturalistic web page representations given agent actions. This model serves as a synthetic web environment where an agent policy can dream by generating vast quantities of rollout action trajectories for efficient online reinforcement learning. Beyond free policy rollouts, DynaWeb incorporates real expert trajectories from training data, which are randomly interleaved with on-policy rollouts during training to improve stability and sample efficiency. Experiments conducted on the challenging WebArena and WebVoyager benchmarks demonstrate that DynaWeb consistently and significantly improves the performance of state-of-the-art open-source web agent models. Our findings establish the viability of training web agents through imagination, offering a scalable and efficient way to scale up online agentic RL."
    },
    {
      "title": "Endogenous Reprompting: Self-Evolving Cognitive Alignment for Unified Multimodal Models",
      "url": "https://arxiv.org/abs/2601.20305",
      "date": "2026-01-28",
      "authors_text": "Zhenchen Tang, Songlin Yang, Zichuan Wang, Bo Peng, Yang Li",
      "is_highlight": false,
      "score": 69.0,
      "summary": "The paper introduces Endogenous Reprompting and the SEER framework, enhancing Unified Multimodal Models' generative reasoning through self-aligned descriptors and reinforcement learning, outperforming existing methods.",
      "teaser_image": "https://arxiv.org/html/2601.20305/x2.png",
      "debug_abstract": "Unified Multimodal Models (UMMs) exhibit strong understanding, yet this capability often fails to effectively guide generation. We identify this as a Cognitive Gap: the model lacks the understanding of how to enhance its own generation process. To bridge this gap, we propose Endogenous Reprompting, a mechanism that transforms the model&#39;s understanding from a passive encoding process into an explicit generative reasoning step by generating self-aligned descriptors during generation. To achieve this, we introduce SEER (Self-Evolving Evaluator and Reprompter), a training framework that establishes a two-stage endogenous loop using only 300 samples from a compact proxy task, Visual Instruction Elaboration. First, Reinforcement Learning with Verifiable Rewards (RLVR) activates the model&#39;s latent evaluation ability via curriculum learning, producing a high-fidelity endogenous reward signal. Second, Reinforcement Learning with Model-rewarded Thinking (RLMT) leverages this signal to optimize the generative reasoning policy. Experiments show that SEER consistently outperforms state-of-the-art baselines in evaluation accuracy, reprompting efficiency, and generation quality, without sacrificing general multimodal capabilities."
    },
    {
      "title": "Glance and Focus Reinforcement for Pan-cancer Screening",
      "url": "https://arxiv.org/abs/2601.19103",
      "date": "2026-01-27",
      "authors_text": "Linshan Wu, Jiaxin Zhuang, Hao Chen",
      "is_highlight": false,
      "score": 65.0,
      "summary": "GF-Screen introduces a novel reinforcement learning framework that enhances pan-cancer screening by effectively localizing and segmenting lesions in CT scans, outperforming existing methods.",
      "debug_abstract": "Pan-cancer screening in large-scale CT scans remains challenging for existing AI methods, primarily due to the difficulty of localizing diverse types of tiny lesions in large CT volumes. The extreme foreground-background imbalance significantly hinders models from focusing on diseased regions, while redundant focus on healthy regions not only decreases the efficiency but also increases false positives. Inspired by radiologists&#39; glance and focus diagnostic strategy, we introduce GF-Screen, a Glance and Focus reinforcement learning framework for pan-cancer screening. GF-Screen employs a Glance model to localize the diseased regions and a Focus model to precisely segment the lesions, where segmentation results of the Focus model are leveraged to reward the Glance model via Reinforcement Learning (RL). Specifically, the Glance model crops a group of sub-volumes from the entire CT volume and learns to select the sub-volumes with lesions for the Focus model to segment. Given that the selecting operation is non-differentiable for segmentation training, we propose to employ the segmentation results to reward the Glance model. To optimize the Glance model, we introduce a novel group relative learning paradigm, which employs group relative comparison to prioritize high-advantage predictions and discard low-advantage predictions within sub-volume groups, not only improving efficiency but also reducing false positives. In this way, for the first time, we effectively extend cutting-edge RL techniques to tackle the specific challenges in pan-cancer screening. Extensive experiments on 16 internal and 7 external datasets across 9 lesion types demonstrated the effectiveness of GF-Screen. Notably, GF-Screen leads the public validation leaderboard of MICCAI FLARE25 pan-cancer challenge, surpassing the FLARE24 champion solution by a large margin (+25.6% DSC and +28.2% NSD)."
    },
    {
      "title": "Enhancing Inverse Perspective Mapping for Automatic Vectorized Road Map Generation",
      "url": "https://arxiv.org/abs/2601.19536",
      "date": "2026-01-27",
      "authors_text": "Hongji Liu, Linwei Zheng, Yongjian Li, Mingkai Tang, Xiaoyang Yan",
      "is_highlight": false,
      "score": 70.0,
      "summary": "This study introduces a cost-effective framework using enhanced inverse perspective mapping and Catmull-Rom splines for high-precision, automatic vectorized road map generation.",
      "debug_abstract": "In this study, we present a low-cost and unified framework for vectorized road mapping leveraging enhanced inverse perspective mapping (IPM). In this framework, Catmull-Rom splines are utilized to characterize lane lines, and all the other ground markings are depicted using polygons uniformly. The results from instance segmentation serve as references to refine the three-dimensional position of spline control points and polygon corner points. In conjunction with this process, the homography matrix of IPM and vehicle poses are optimized simultaneously. Our proposed framework significantly reduces the mapping errors associated with IPM. It also improves the accuracy of the initial IPM homography matrix and the predicted vehicle poses. Furthermore, it addresses the limitations imposed by the coplanarity assumption in IPM. These enhancements enable IPM to be effectively applied to vectorized road mapping, which serves a cost-effective solution with enhanced accuracy. In addition, our framework generalizes road map elements to include all common ground markings and lane lines. The proposed framework is evaluated in two different practical scenarios, and the test results show that our method can automatically generate high-precision maps with near-centimeter-level accuracy. Importantly, the optimized IPM matrix achieves an accuracy comparable to that of manual calibration, while the accuracy of vehicle poses is also significantly improved."
    },
    {
      "title": "Geometry-Grounded Gaussian Splatting",
      "url": "https://arxiv.org/abs/2601.17835",
      "date": "2026-01-25",
      "authors_text": "Baowen Zhang, Chenxing Jiang, Heng Li, Shaojie Shen, Ping Tan",
      "is_highlight": true,
      "score": 53.0,
      "summary": "This paper introduces Geometry-Grounded Gaussian Splatting, enhancing shape extraction from Gaussian primitives by treating them as stochastic solids for improved depth map rendering and reconstruction quality.",
      "debug_abstract": "Gaussian Splatting (GS) has demonstrated impressive quality and efficiency in novel view synthesis. However, shape extraction from Gaussian primitives remains an open problem. Due to inadequate geometry parameterization and approximation, existing shape reconstruction methods suffer from poor multi-view consistency and are sensitive to floaters. In this paper, we present a rigorous theoretical derivation that establishes Gaussian primitives as a specific type of stochastic solids. This theoretical framework provides a principled foundation for Geometry-Grounded Gaussian Splatting by enabling the direct treatment of Gaussian primitives as explicit geometric representations. Using the volumetric nature of stochastic solids, our method efficiently renders high-quality depth maps for fine-grained geometry extraction. Experiments show that our method achieves the best shape reconstruction results among all Gaussian Splatting-based methods on public datasets."
    },
    {
      "title": "CoT-Seg: Rethinking Segmentation with Chain-of-Thought Reasoning and Self-Correction",
      "url": "https://arxiv.org/abs/2601.17420",
      "date": "2026-01-24",
      "authors_text": "Shiu-hong Kao, Chak Ho Huang, Huaiqian Liu, Yu-Wing Tai, Chi-Keung Tang",
      "is_highlight": false,
      "score": 68.0,
      "summary": "CoT-Seg introduces a training-free framework that enhances segmentation through chain-of-thought reasoning and self-correction, improving performance on complex queries and ambiguous images.",
      "debug_abstract": "Existing works of reasoning segmentation often fall short in complex cases, particularly when addressing complicated queries and out-of-domain images. Inspired by the chain-of-thought reasoning, where harder problems require longer thinking steps/time, this paper aims to explore a system that can think step-by-step, look up information if needed, generate results, self-evaluate its own results, and refine the results, in the same way humans approach harder questions. We introduce CoT-Seg, a training-free framework that rethinks reasoning segmentation by combining chain-of-thought reasoning with self-correction. Instead of fine-tuning, CoT-Seg leverages the inherent reasoning ability of pre-trained MLLMs (GPT-4o) to decompose queries into meta-instructions, extract fine-grained semantics from images, and identify target objects even under implicit or complex prompts. Moreover, CoT-Seg incorporates a self-correction stage: the model evaluates its own segmentation against the original query and reasoning trace, identifies mismatches, and iteratively refines the mask. This tight integration of reasoning and correction significantly improves reliability and robustness, especially in ambiguous or error-prone cases. Furthermore, our CoT-Seg framework allows easy incorporation of retrieval-augmented reasoning, enabling the system to access external knowledge when the input lacks sufficient information. To showcase CoT-Seg&#39;s ability to handle very challenging cases ,we introduce a new dataset ReasonSeg-Hard. Our results highlight that combining chain-of-thought reasoning, self-correction, offers a powerful paradigm for vision-language integration driven segmentation."
    },
    {
      "title": "Uni-RS: A Spatially Faithful Unified Understanding and Generation Model for Remote Sensing",
      "url": "https://arxiv.org/abs/2601.17673",
      "date": "2026-01-25",
      "authors_text": "Weiyu Zhang, Yuan Hu, Yong Li, Yu Liu",
      "is_highlight": false,
      "score": 58.0,
      "summary": "Uni-RS is a novel unified multimodal model for remote sensing that enhances spatial faithfulness in text-to-image generation by integrating spatial layout planning and supervision techniques.",
      "debug_abstract": "Unified remote sensing multimodal models exhibit a pronounced spatial reversal curse: Although they can accurately recognize and describe object locations in images, they often fail to faithfully execute the same spatial relations during text-to-image generation, where such relations constitute core semantic information in remote sensing. Motivated by this observation, we propose Uni-RS, the first unified multimodal model tailored for remote sensing, to explicitly address the spatial asymmetry between understanding and generation. Specifically, we first introduce explicit Spatial-Layout Planning to transform textual instructions into spatial layout plans, decoupling geometric planning from visual synthesis. We then impose Spatial-Aware Query Supervision to bias learnable queries toward spatial relations explicitly specified in the instruction. Finally, we develop Image-Caption Spatial Layout Variation to expose the model to systematic geometry-consistent spatial transformations. Extensive experiments across multiple benchmarks show that our approach substantially improves spatial faithfulness in text-to-image generation, while maintaining strong performance on multimodal understanding tasks like image captioning, visual grounding, and VQA tasks."
    },
    {
      "title": "TC-IDM: Grounding Video Generation for Executable Zero-shot Robot Motion",
      "url": "https://arxiv.org/abs/2601.18323",
      "date": "2026-01-26",
      "authors_text": "Weishi Mi, Yong Bao, Xiaowei Chi, Xiaozhu Ju, Zhiyuan Qin",
      "is_highlight": false,
      "score": 89.0,
      "summary": "The TC-IDM model enhances robot motion execution by bridging visual planning and physical control through tool-centric trajectory synthesis, achieving superior generalization and performance in diverse tasks.",
      "debug_abstract": "The vision-language-action (VLA) paradigm has enabled powerful robotic control by leveraging vision-language models, but its reliance on large-scale, high-quality robot data limits its generalization. Generative world models offer a promising alternative for general-purpose embodied AI, yet a critical gap remains between their pixel-level plans and physically executable actions. To this end, we propose the Tool-Centric Inverse Dynamics Model (TC-IDM). By focusing on the tool&#39;s imagined trajectory as synthesized by the world model, TC-IDM establishes a robust intermediate representation that bridges the gap between visual planning and physical control. TC-IDM extracts the tool&#39;s point cloud trajectories via segmentation and 3D motion estimation from generated videos. Considering diverse tool attributes, our architecture employs decoupled action heads to project these planned trajectories into 6-DoF end-effector motions and corresponding control signals. This plan-and-translate paradigm not only supports a wide range of end-effectors but also significantly improves viewpoint invariance. Furthermore, it exhibits strong generalization capabilities across long-horizon and out-of-distribution tasks, including interacting with deformable objects. In real-world evaluations, the world model with TC-IDM achieves an average success rate of 61.11 percent, with 77.7 percent on simple tasks and 38.46 percent on zero-shot deformable object tasks. It substantially outperforms end-to-end VLA-style baselines and other inverse dynamics models."
    },
    {
      "title": "Fast and Safe Trajectory Optimization for Mobile Manipulators With Neural Configuration Space Distance Field",
      "url": "https://arxiv.org/abs/2601.18548",
      "date": "2026-01-26",
      "authors_text": "Yulin Li, Zhiyuan Song, Yiming Li, Zhicheng Song, Kai Chen",
      "is_highlight": false,
      "score": 86.0,
      "summary": "This paper introduces Generalized Configuration Space Distance Fields (GCDF) for efficient trajectory optimization in mobile manipulators, enhancing collision reasoning and enabling agile motion in complex environments.",
      "debug_abstract": "Mobile manipulators promise agile, long-horizon behavior by coordinating base and arm motion, yet whole-body trajectory optimization in cluttered, confined spaces remains difficult due to high-dimensional nonconvexity and the need for fast, accurate collision reasoning. Configuration Space Distance Fields (CDF) enable fixed-base manipulators to model collisions directly in configuration space via smooth, implicit distances. This representation holds strong potential to bypass the nonlinear configuration-to-workspace mapping while preserving accurate whole-body geometry and providing optimization-friendly collision costs. Yet, extending this capability to mobile manipulators is hindered by unbounded workspaces and tighter base-arm coupling. We lift this promise to mobile manipulation with Generalized Configuration Space Distance Fields (GCDF), extending CDF to robots with both translational and rotational joints in unbounded workspaces with tighter base-arm coupling. We prove that GCDF preserves Euclidean-like local distance structure and accurately encodes whole-body geometry in configuration space, and develop a data generation and training pipeline that yields continuous neural GCDFs with accurate values and gradients, supporting efficient GPU-batched queries. Building on this representation, we develop a high-performance sequential convex optimization framework centered on GCDF-based collision reasoning. The solver scales to large numbers of implicit constraints through (i) online specification of neural constraints, (ii) sparsity-aware active-set detection with parallel batched evaluation across thousands of constraints, and (iii) incremental constraint management for rapid replanning under scene changes."
    },
    {
      "title": "EvoCUA: Evolving Computer Use Agents via Learning from Scalable Synthetic Experience",
      "url": "https://arxiv.org/abs/2601.15876",
      "date": "2026-01-22",
      "authors_text": "Taofeng Xue, Chong Peng, Mianqiu Huang, Linsen Guo, Tiancheng Han",
      "is_highlight": false,
      "score": 82.6,
      "summary": "EvoCUA introduces a self-sustaining evolutionary model for computer-use agents, leveraging autonomous task generation and iterative learning to significantly outperform existing benchmarks in multimodal AI.",
      "debug_abstract": "The development of native computer-use agents (CUA) represents a significant leap in multimodal AI. However, their potential is currently bottlenecked by the constraints of static data scaling. Existing paradigms relying primarily on passive imitation of static datasets struggle to capture the intricate causal dynamics inherent in long-horizon computer tasks. In this work, we introduce EvoCUA, a native computer use agentic model. Unlike static imitation, EvoCUA integrates data generation and policy optimization into a self-sustaining evolutionary cycle. To mitigate data scarcity, we develop a verifiable synthesis engine that autonomously generates diverse tasks coupled with executable validators. To enable large-scale experience acquisition, we design a scalable infrastructure orchestrating tens of thousands of asynchronous sandbox rollouts. Building on these massive trajectories, we propose an iterative evolving learning strategy to efficiently internalize this experience. This mechanism dynamically regulates policy updates by identifying capability boundaries -- reinforcing successful routines while transforming failure trajectories into rich supervision through error analysis and self-correction. Empirical evaluations on the OSWorld benchmark demonstrate that EvoCUA achieves a success rate of 56.7%, establishing a new open-source state-of-the-art. Notably, EvoCUA significantly outperforms the previous best open-source model, OpenCUA-72B (45.0%), and surpasses leading closed-weights models such as UI-TARS-2 (53.1%). Crucially, our results underscore the generalizability of this approach: the evolving paradigm driven by learning from experience yields consistent performance gains across foundation models of varying scales, establishing a robust and scalable path for advancing native agent capabilities."
    },
    {
      "title": "DualShield: Safe Model Predictive Diffusion via Reachability Analysis for Interactive Autonomous Driving",
      "url": "https://arxiv.org/abs/2601.15729",
      "date": "2026-01-22",
      "authors_text": "Rui Yang, Lei Zheng, Ruoyu Yao, Jun Ma",
      "is_highlight": true,
      "score": 92.7,
      "summary": "DualShield enhances autonomous driving safety by integrating Hamilton-Jacobi reachability analysis into diffusion models for proactive guidance and reactive safety assurance under uncertain interactions.",
      "debug_abstract": "Diffusion models have emerged as a powerful approach for multimodal motion planning in autonomous driving. However, their practical deployment is typically hindered by the inherent difficulty in enforcing vehicle dynamics and a critical reliance on accurate predictions of other agents, making them prone to safety issues under uncertain interactions. To address these limitations, we introduce DualShield, a planning and control framework that leverages Hamilton-Jacobi (HJ) reachability value functions in a dual capacity. First, the value functions act as proactive guidance, steering the diffusion denoising process towards safe and dynamically feasible regions. Second, they form a reactive safety shield using control barrier-value functions (CBVFs) to modify the executed actions and ensure safety. This dual mechanism preserves the rich exploration capabilities of diffusion models while providing principled safety assurance under uncertain and even adversarial interactions. Simulations in challenging unprotected U-turn scenarios demonstrate that DualShield significantly improves both safety and task efficiency compared to leading methods from different planning paradigms under uncertainty."
    }
  ],
  "Precognition Lab": [
    {
      "title": "Visual Reasoning over Time Series via Multi-Agent System",
      "url": "https://arxiv.org/abs/2602.03026",
      "date": "2026-02-03",
      "authors_text": "Weilin Ruan, Yuxuan Liang",
      "is_highlight": false,
      "score": 66.0,
      "summary": "MAS4TS is a multi-agent system that enhances time series analysis through visual reasoning and adaptive tool usage, achieving state-of-the-art performance across diverse tasks.",
      "teaser_image": "https://arxiv.org/html/2602.03026/x2.png",
      "debug_abstract": "Time series analysis underpins many real-world applications, yet existing time-series-specific methods and pretrained large-model-based approaches remain limited in integrating intuitive visual reasoning and generalizing across tasks with adaptive tool usage. To address these limitations, we propose MAS4TS, a tool-driven multi-agent system for general time series tasks, built upon an Analyzer-Reasoner-Executor paradigm that integrates agent communication, visual reasoning, and latent reconstruction within a unified framework. MAS4TS first performs visual reasoning over time series plots with structured priors using a Vision-Language Model to extract temporal structures, and subsequently reconstructs predictive trajectories in latent space. Three specialized agents coordinate via shared memory and gated communication, while a router selects task-specific tool chains for execution. Extensive experiments on multiple benchmarks demonstrate that MAS4TS achieves state-of-the-art performance across a wide range of time series tasks, while exhibiting strong generalization and efficient inference."
    },
    {
      "title": "Neural Predictor-Corrector: Solving Homotopy Problems with Reinforcement Learning",
      "url": "https://arxiv.org/abs/2602.03086",
      "date": "2026-02-03",
      "authors_text": "Jiayao Mai, Bangyan Liao, Zhenjun Zhao, Yingping Zeng, Haoang Li",
      "is_highlight": false,
      "score": 64.0,
      "summary": "The Neural Predictor-Corrector (NPC) framework uses reinforcement learning to automate step size and termination decisions in solving diverse homotopy problems, outperforming traditional methods.",
      "teaser_image": "https://arxiv.org/html/2602.03086/x2.png",
      "debug_abstract": "The Homotopy paradigm, a general principle for solving challenging problems, appears across diverse domains such as robust optimization, global optimization, polynomial root-finding, and sampling. Practical solvers for these problems typically follow a predictor-corrector (PC) structure, but rely on hand-crafted heuristics for step sizes and iteration termination, which are often suboptimal and task-specific. To address this, we unify these problems under a single framework, which enables the design of a general neural solver. Building on this unified view, we propose Neural Predictor-Corrector (NPC), which replaces hand-crafted heuristics with automatically learned policies. NPC formulates policy selection as a sequential decision-making problem and leverages reinforcement learning to automatically discover efficient strategies. To further enhance generalization, we introduce an amortized training mechanism, enabling one-time offline training for a class of problems and efficient online inference on new instances. Experiments on four representative homotopy problems demonstrate that our method generalizes effectively to unseen instances. It consistently outperforms classical and specialized baselines in efficiency while demonstrating superior stability across tasks, highlighting the value of unifying homotopy methods into a single neural framework."
    },
    {
      "title": "Diffusion-Driven Inter-Outer Surface Separation for Point Clouds with Open Boundaries",
      "url": "https://arxiv.org/abs/2602.00739",
      "date": "2026-01-31",
      "authors_text": "Zhengyan Qin, Liyuan Qiu",
      "is_highlight": false,
      "score": 72.0,
      "summary": "The paper presents a diffusion-based algorithm for effectively separating inter and outer surfaces in double-layered point clouds with open boundaries, addressing artifacts from TSDF fusion.",
      "teaser_image": "https://arxiv.org/html/2602.00739/figures/pipeline5.png",
      "debug_abstract": "We propose a diffusion-based algorithm for separating the inter and outer layer surfaces from double-layered point clouds, particularly those exhibiting the &#34;double surface artifact&#34; caused by truncation in Truncated Signed Distance Function (TSDF) fusion during indoor or medical 3D reconstruction. This artifact arises from asymmetric truncation thresholds, leading to erroneous inter and outer shells in the fused volume, which our method addresses by extracting the true inter layer to mitigate challenges like overlapping surfaces and disordered normals. We focus on point clouds with \\emph{open boundaries} (i.e., sampled surfaces with topological openings/holes through which particles may escape), rather than point clouds with \\emph{missing surface regions} where no samples exist. Our approach enables robust processing of both watertight and open-boundary models, achieving extraction of the inter layer from 20,000 inter and 20,000 outer points in approximately 10 seconds. This solution is particularly effective for applications requiring accurate surface representations, such as indoor scene modeling and medical imaging, where double-layered point clouds are prevalent, and it accommodates both closed (watertight) and open-boundary surface geometries. Our goal is \\emph{post-hoc} inter/outer shell separation as a lightweight module after TSDF fusion; we do not aim to replace full variational or learning-based reconstruction pipelines."
    },
    {
      "title": "Physics-informed Diffusion Mamba Transformer for Real-world Driving",
      "url": "https://arxiv.org/abs/2602.00808",
      "date": "2026-01-31",
      "authors_text": "Hang Zhou, Qiang Zhang, Peiran Liu, Yihao Qin, Zhaoxu Yan",
      "is_highlight": false,
      "score": 74.0,
      "summary": "The paper presents a Diffusion Mamba Transformer that integrates sequential context and physical constraints for improved trajectory planning in autonomous driving, outperforming existing models.",
      "teaser_image": "https://arxiv.org/html/2602.00808/difference_pipeline3.png",
      "debug_abstract": "Autonomous driving systems demand trajectory planners that not only model the inherent uncertainty of future motions but also respect complex temporal dependencies and underlying physical laws. While diffusion-based generative models excel at capturing multi-modal distributions, they often fail to incorporate long-term sequential contexts and domain-specific physical priors. In this work, we bridge these gaps with two key innovations. First, we introduce a Diffusion Mamba Transformer architecture that embeds mamba and attention into the diffusion process, enabling more effective aggregation of sequential input contexts from sensor streams and past motion histories. Second, we design a Port-Hamiltonian Neural Network module that seamlessly integrates energy-based physical constraints into the diffusion model, thereby enhancing trajectory predictions with both consistency and interpretability. Extensive evaluations on standard autonomous driving benchmarks demonstrate that our unified framework significantly outperforms state-of-the-art baselines in predictive accuracy, physical plausibility, and robustness, thereby advancing safe and reliable motion planning."
    },
    {
      "title": "Invariance on Manifolds: Understanding Robust Visual Representations for Place Recognition",
      "url": "https://arxiv.org/abs/2602.00841",
      "date": "2026-01-31",
      "authors_text": "Jintao Cheng, Weibin Li, Zhijian He, Jin Wu, Chi Man Vong",
      "is_highlight": false,
      "score": 66.0,
      "summary": "This paper presents a training-free Second-Order Geometric Statistics framework for robust visual place recognition, utilizing covariance descriptors on the SPD manifold for effective zero-shot generalization.",
      "teaser_image": "https://arxiv.org/html/2602.00841/x2.png",
      "debug_abstract": "Visual Place Recognition (VPR) demands representations robust to drastic environmental and viewpoint shifts. Current aggregation paradigms, however, either rely on data-hungry supervision or simplistic first-order statistics, often neglecting intrinsic structural correlations. In this work, we propose a Second-Order Geometric Statistics framework that inherently captures geometric stability without training. We conceptualize scenes as covariance descriptors on the Symmetric Positive Definite (SPD) manifold, where perturbations manifest as tractable congruence transformations. By leveraging geometry-aware Riemannian mappings, we project these descriptors into a linearized Euclidean embedding, effectively decoupling signal structure from noise. Our approach introduces a training-free framework built upon fixed, pre-trained backbones, achieving strong zero-shot generalization without parameter updates. Extensive experiments confirm that our method achieves highly competitive performance against state-of-the-art baselines, particularly excelling in challenging zero-shot scenarios."
    },
    {
      "title": "PolicyFlow: Policy Optimization with Continuous Normalizing Flow in Reinforcement Learning",
      "url": "https://arxiv.org/abs/2602.01156",
      "date": "2026-02-01",
      "authors_text": "Shunpeng Yang, Ben Liu, Hua Chen",
      "is_highlight": false,
      "score": 71.0,
      "summary": "PolicyFlow is a novel reinforcement learning algorithm that integrates continuous normalizing flows with PPO, enhancing performance and stability while reducing computational costs and encouraging diverse behaviors.",
      "teaser_image": "https://arxiv.org/html/2602.01156/x1.png",
      "debug_abstract": "Among on-policy reinforcement learning algorithms, Proximal Policy Optimization (PPO) demonstrates is widely favored for its simplicity, numerical stability, and strong empirical performance. Standard PPO relies on surrogate objectives defined via importance ratios, which require evaluating policy likelihood that is typically straightforward when the policy is modeled as a Gaussian distribution. However, extending PPO to more expressive, high-capacity policy models such as continuous normalizing flows (CNFs), also known as flow-matching models, is challenging because likelihood evaluation along the full flow trajectory is computationally expensive and often numerically unstable. To resolve this issue, we propose PolicyFlow, a novel on-policy CNF-based reinforcement learning algorithm that integrates expressive CNF policies with PPO-style objectives without requiring likelihood evaluation along the full flow path. PolicyFlow approximates importance ratios using velocity field variations along a simple interpolation path, reducing computational overhead without compromising training stability. To further prevent mode collapse and further encourage diverse behaviors, we propose the Brownian Regularizer, an implicit policy entropy regularizer inspired by Brownian motion, which is conceptually elegant and computationally lightweight. Experiments on diverse tasks across various environments including MultiGoal, PointMaze, IsaacLab and MuJoCo Playground show that PolicyFlow achieves competitive or superior performance compared to PPO using Gaussian policies and flow-based baselines including FPO and DPPO. Notably, results on MultiGoal highlight PolicyFlow&#39;s ability to capture richer multimodal action distributions."
    },
    {
      "title": "Do All Individual Layers Help? An Empirical Study of Task-Interfering Layers in Vision-Language Models",
      "url": "https://arxiv.org/abs/2602.01167",
      "date": "2026-02-01",
      "authors_text": "Zhiming Liu, Yujie Wei, Lei Feng, Xiu Su, Xiaobo Xia",
      "is_highlight": false,
      "score": 52.0,
      "summary": "This study identifies Task-Interfering Layers in vision-language models, demonstrating that selectively bypassing certain layers can enhance task performance without retraining.",
      "teaser_image": "https://arxiv.org/html/2602.01167/x1.png",
      "debug_abstract": "Current VLMs have demonstrated capabilities across a wide range of multimodal tasks. Typically, in a pretrained VLM, all layers are engaged by default to make predictions on downstream tasks. We find that intervening on a single layer, such as by zeroing its parameters, can improve the performance on certain tasks, indicating that some layers hinder rather than help downstream tasks. We systematically investigate how individual layers influence different tasks via layer intervention. Specifically, we measure the change in performance relative to the base model after intervening on each layer and observe improvements when bypassing specific layers. This improvement can be generalizable across models and datasets, indicating the presence of Task-Interfering Layers that harm downstream tasks&#39; performance. We introduce Task-Layer Interaction Vector, which quantifies the effect of intervening on each layer of a VLM given a task. These task-interfering layers exhibit task-specific sensitivity patterns: tasks requiring similar capabilities show consistent response trends under layer interventions, as evidenced by the high similarity in their task-layer interaction vectors. Inspired by these findings, we propose TaLo (Task-Adaptive Layer Knockout), a training-free, test-time adaptation method that dynamically identifies and bypasses the most interfering layer for a given task. Without parameter updates, TaLo improves performance across various models and datasets, including boosting Qwen-VL&#39;s accuracy on the Maps task in ScienceQA by up to 16.6%. Our work reveals an unexpected form of modularity in pretrained VLMs and provides a plug-and-play, training-free mechanism to unlock hidden capabilities at inference time. The source code will be publicly available."
    },
    {
      "title": "Towards Exploratory and Focused Manipulation with Bimanual Active Perception: A New Problem, Benchmark and Strategy",
      "url": "https://arxiv.org/abs/2602.01939",
      "date": "2026-02-02",
      "authors_text": "Yuxin He, Ruihao Zhang, Tianao Shen, Cheng Liu, Qiang Nie",
      "is_highlight": false,
      "score": 28.0,
      "summary": "The paper introduces the Exploratory and Focused Manipulation (EFM) problem, establishes the EFM-10 benchmark, and proposes a Bimanual Active Perception strategy to enhance manipulation tasks.",
      "teaser_image": "https://arxiv.org/html/2602.01939/x2.png",
      "debug_abstract": "Recently, active vision has reemerged as an important concept for manipulation, since visual occlusion occurs more frequently when main cameras are mounted on the robot heads. We reflect on the visual occlusion issue and identify its essence as the absence of information useful for task completion. Inspired by this, we come up with the more fundamental problem of Exploratory and Focused Manipulation (EFM). The proposed problem is about actively collecting information to complete challenging manipulation tasks that require exploration or focus. As an initial attempt to address this problem, we establish the EFM-10 benchmark that consists of 4 categories of tasks that align with our definition (10 tasks in total). We further come up with a Bimanual Active Perception (BAP) strategy, which leverages one arm to provide active vision and another arm to provide force sensing while manipulating. Based on this idea, we collect a dataset named BAPData for the tasks in EFM-10. With the dataset, we successfully verify the effectiveness of the BAP strategy in an imitation learning manner. We hope that the EFM-10 benchmark along with the BAP strategy can become a cornerstone that facilitates future research towards this direction. Project website: this http URL."
    },
    {
      "title": "Breaking the Static Graph: Context-Aware Traversal for Robust Retrieval-Augmented Generation",
      "url": "https://arxiv.org/abs/2602.01965",
      "date": "2026-02-02",
      "authors_text": "Kwun Hang Lau, Fangyuan Zhang, Boyu Ruan, Yingli Zhou, Qintian Guo",
      "is_highlight": false,
      "score": 27.0,
      "summary": "CatRAG enhances Retrieval-Augmented Generation by transforming static Knowledge Graphs into query-adaptive structures, improving multi-hop evidence retrieval and reasoning completeness.",
      "teaser_image": "https://arxiv.org/html/2602.01965/image/HubBiasShift.png",
      "debug_abstract": "Recent advances in Retrieval-Augmented Generation (RAG) have shifted from simple vector similarity to structure-aware approaches like HippoRAG, which leverage Knowledge Graphs (KGs) and Personalized PageRank (PPR) to capture multi-hop dependencies. However, these methods suffer from a &#34;Static Graph Fallacy&#34;: they rely on fixed transition probabilities determined during indexing. This rigidity ignores the query-dependent nature of edge relevance, causing semantic drift where random walks are diverted into high-degree &#34;hub&#34; nodes before reaching critical downstream evidence. Consequently, models often achieve high partial recall but fail to retrieve the complete evidence chain required for multi-hop queries. To address this, we propose CatRAG, Context-Aware Traversal for robust RAG, a framework that builds on the HippoRAG 2 architecture and transforms the static KG into a query-adaptive navigation structure. We introduce a multi-faceted framework to steer the random walk: (1) Symbolic Anchoring, which injects weak entity constraints to regularize the random walk; (2) Query-Aware Dynamic Edge Weighting, which dynamically modulates graph structure, to prune irrelevant paths while amplifying those aligned with the query&#39;s intent; and (3) Key-Fact Passage Weight Enhancement, a cost-efficient bias that structurally anchors the random walk to likely evidence. Experiments across four multi-hop benchmarks demonstrate that CatRAG consistently outperforms state of the art baselines. Our analysis reveals that while standard Recall metrics show modest gains, CatRAG achieves substantial improvements in reasoning completeness, the capacity to recover the entire evidence path without gaps. These results reveal that our approach effectively bridges the gap between retrieving partial context and enabling fully grounded reasoning. Resources are available at this https URL."
    },
    {
      "title": "Towards Agentic Intelligence for Materials Science",
      "url": "https://arxiv.org/abs/2602.00169",
      "date": "2026-01-29",
      "authors_text": "Huan Zhang, Yizhan Li, Wenhao Huang, Ziyu Hou, Yu Song",
      "is_highlight": false,
      "score": 12.0,
      "summary": "This paper advocates for developing agentic AI systems in materials science that autonomously plan, act, and learn to enhance discovery processes beyond traditional models.",
      "teaser_image": "https://arxiv.org/html/2602.00169/x1.png",
      "debug_abstract": "The convergence of artificial intelligence and materials science presents a transformative opportunity, but achieving true acceleration in discovery requires moving beyond task-isolated, fine-tuned models toward agentic systems that plan, act, and learn across the full discovery loop. This survey advances a unique pipeline-centric view that spans from corpus curation and pretraining, through domain adaptation and instruction tuning, to goal-conditioned agents interfacing with simulation and experimental platforms. Unlike prior reviews, we treat the entire process as an end-to-end system to be optimized for tangible discovery outcomes rather than proxy benchmarks. This perspective allows us to trace how upstream design choices-such as data curation and training objectives-can be aligned with downstream experimental success through effective credit assignment. To bridge communities and establish a shared frame of reference, we first present an integrated lens that aligns terminology, evaluation, and workflow stages across AI and materials science. We then analyze the field through two focused lenses: From the AI perspective, the survey details LLM strengths in pattern recognition, predictive analytics, and natural language processing for literature mining, materials characterization, and property prediction; from the materials science perspective, it highlights applications in materials design, process optimization, and the acceleration of computational workflows via integration with external tools (e.g., DFT, robotic labs). Finally, we contrast passive, reactive approaches with agentic design, cataloging current contributions while motivating systems that pursue long-horizon goals with autonomy, memory, and tool use. This survey charts a practical roadmap towards autonomous, safety-aware LLM agents aimed at discovering novel and useful materials."
    },
    {
      "title": "Advances and Innovations in the Multi-Agent Robotic System (MARS) Challenge",
      "url": "https://arxiv.org/abs/2601.18733",
      "date": "2026-01-26",
      "authors_text": "Li Kang, Heng Zhou, Xiufeng Song, Rui Li, Bruno N. Y. Chen",
      "is_highlight": false,
      "score": 78.0,
      "summary": "The MARS Challenge promotes advancements in multi-agent robotic systems by exploring vision-language models for collaborative task execution and planning in dynamic environments.",
      "teaser_image": null,
      "debug_abstract": "Recent advancements in multimodal large language models and vision-languageaction models have significantly driven progress in Embodied AI. As the field transitions toward more complex task scenarios, multi-agent system frameworks are becoming essential for achieving scalable, efficient, and collaborative solutions. This shift is fueled by three primary factors: increasing agent capabilities, enhancing system efficiency through task delegation, and enabling advanced human-agent interactions. To address the challenges posed by multi-agent collaboration, we propose the Multi-Agent Robotic System (MARS) Challenge, held at the NeurIPS 2025 Workshop on SpaVLE. The competition focuses on two critical areas: planning and control, where participants explore multi-agent embodied planning using vision-language models (VLMs) to coordinate tasks and policy execution to perform robotic manipulation in dynamic environments. By evaluating solutions submitted by participants, the challenge provides valuable insights into the design and coordination of embodied multi-agent systems, contributing to the future development of advanced collaborative AI systems."
    },
    {
      "title": "Gaussian Belief Propagation Network for Depth Completion",
      "url": "https://arxiv.org/abs/2601.21291",
      "date": "2026-01-29",
      "authors_text": "Jie Tang, Pingping Xie, Jian Li, Ping Tan",
      "is_highlight": false,
      "score": 68.0,
      "summary": "The Gaussian Belief Propagation Network (GBPN) integrates deep learning with probabilistic models for effective depth completion from sparse measurements, achieving state-of-the-art results across various benchmarks.",
      "teaser_image": "https://arxiv.org/html/2601.21291/x1.png",
      "debug_abstract": "Depth completion aims to predict a dense depth map from a color image with sparse depth measurements. Although deep learning methods have achieved state-of-the-art (SOTA), effectively handling the sparse and irregular nature of input depth data in deep networks remains a significant challenge, often limiting performance, especially under high sparsity. To overcome this limitation, we introduce the Gaussian Belief Propagation Network (GBPN), a novel hybrid framework synergistically integrating deep learning with probabilistic graphical models for end-to-end depth completion. Specifically, a scene-specific Markov Random Field (MRF) is dynamically constructed by the Graphical Model Construction Network (GMCN), and then inferred via Gaussian Belief Propagation (GBP) to yield the dense depth distribution. Crucially, the GMCN learns to construct not only the data-dependent potentials of MRF but also its structure by predicting adaptive non-local edges, enabling the capture of complex, long-range spatial dependencies. Furthermore, we enhance GBP with a serial \\&amp; parallel message passing scheme, designed for effective information propagation, particularly from sparse measurements. Extensive experiments demonstrate that GBPN achieves SOTA performance on the NYUv2 and KITTI benchmarks. Evaluations across varying sparsity levels, sparsity patterns, and datasets highlight GBPN&#39;s superior performance, notable robustness, and generalizable capability."
    },
    {
      "title": "4D-CAAL: 4D Radar-Camera Calibration and Auto-Labeling for Autonomous Driving",
      "url": "https://arxiv.org/abs/2601.21454",
      "date": "2026-01-29",
      "authors_text": "Shanliang Yao, Zhuoxiao Li, Runwei Guan, Kebin Cao, Meng Xia",
      "is_highlight": false,
      "score": 89.0,
      "summary": "The 4D-CAAL framework integrates 4D radar-camera calibration and auto-labeling using a dual-purpose target and robust correspondence matching, enhancing autonomous driving perception efficiency.",
      "teaser_image": "https://arxiv.org/html/2601.21454/images/4DRC-fixed.png",
      "debug_abstract": "4D radar has emerged as a critical sensor for autonomous driving, primarily due to its enhanced capabilities in elevation measurement and higher resolution compared to traditional 3D radar. Effective integration of 4D radar with cameras requires accurate extrinsic calibration, and the development of radar-based perception algorithms demands large-scale annotated datasets. However, existing calibration methods often employ separate targets optimized for either visual or radar modalities, complicating correspondence establishment. Furthermore, manually labeling sparse radar data is labor-intensive and unreliable. To address these challenges, we propose 4D-CAAL, a unified framework for 4D radar-camera calibration and auto-labeling. Our approach introduces a novel dual-purpose calibration target design, integrating a checkerboard pattern on the front surface for camera detection and a corner reflector at the center of the back surface for radar detection. We develop a robust correspondence matching algorithm that aligns the checkerboard center with the strongest radar reflection point, enabling accurate extrinsic calibration. Subsequently, we present an auto-labeling pipeline that leverages the calibrated sensor relationship to transfer annotations from camera-based segmentations to radar point clouds through geometric projection and multi-feature optimization. Extensive experiments demonstrate that our method achieves high calibration accuracy while significantly reducing manual annotation effort, thereby accelerating the development of robust multi-modal perception systems for autonomous driving."
    },
    {
      "title": "DynaWeb: Model-Based Reinforcement Learning of Web Agents",
      "url": "https://arxiv.org/abs/2601.22149",
      "date": "2026-01-29",
      "authors_text": "Hang Ding, Peidong Liu, Junqiao Wang, Ziwei Ji, Meng Cao",
      "is_highlight": false,
      "score": 77.0,
      "summary": "DynaWeb introduces a model-based reinforcement learning framework that enhances web agent training by simulating interactions with a web world model, improving efficiency and performance.",
      "teaser_image": "https://arxiv.org/html/2601.22149/figures/dynaweb.png",
      "debug_abstract": "The development of autonomous web agents, powered by Large Language Models (LLMs) and reinforcement learning (RL), represents a significant step towards general-purpose AI assistants. However, training these agents is severely hampered by the challenges of interacting with the live internet, which is inefficient, costly, and fraught with risks. Model-based reinforcement learning (MBRL) offers a promising solution by learning a world model of the environment to enable simulated interaction. This paper introduces DynaWeb, a novel MBRL framework that trains web agents through interacting with a web world model trained to predict naturalistic web page representations given agent actions. This model serves as a synthetic web environment where an agent policy can dream by generating vast quantities of rollout action trajectories for efficient online reinforcement learning. Beyond free policy rollouts, DynaWeb incorporates real expert trajectories from training data, which are randomly interleaved with on-policy rollouts during training to improve stability and sample efficiency. Experiments conducted on the challenging WebArena and WebVoyager benchmarks demonstrate that DynaWeb consistently and significantly improves the performance of state-of-the-art open-source web agent models. Our findings establish the viability of training web agents through imagination, offering a scalable and efficient way to scale up online agentic RL."
    },
    {
      "title": "Endogenous Reprompting: Self-Evolving Cognitive Alignment for Unified Multimodal Models",
      "url": "https://arxiv.org/abs/2601.20305",
      "date": "2026-01-28",
      "authors_text": "Zhenchen Tang, Songlin Yang, Zichuan Wang, Bo Peng, Yang Li",
      "is_highlight": false,
      "score": 69.0,
      "summary": "The paper introduces Endogenous Reprompting and the SEER framework, enhancing Unified Multimodal Models' generative reasoning through self-aligned descriptors and reinforcement learning, outperforming existing methods.",
      "teaser_image": "https://arxiv.org/html/2601.20305/x2.png",
      "debug_abstract": "Unified Multimodal Models (UMMs) exhibit strong understanding, yet this capability often fails to effectively guide generation. We identify this as a Cognitive Gap: the model lacks the understanding of how to enhance its own generation process. To bridge this gap, we propose Endogenous Reprompting, a mechanism that transforms the model&#39;s understanding from a passive encoding process into an explicit generative reasoning step by generating self-aligned descriptors during generation. To achieve this, we introduce SEER (Self-Evolving Evaluator and Reprompter), a training framework that establishes a two-stage endogenous loop using only 300 samples from a compact proxy task, Visual Instruction Elaboration. First, Reinforcement Learning with Verifiable Rewards (RLVR) activates the model&#39;s latent evaluation ability via curriculum learning, producing a high-fidelity endogenous reward signal. Second, Reinforcement Learning with Model-rewarded Thinking (RLMT) leverages this signal to optimize the generative reasoning policy. Experiments show that SEER consistently outperforms state-of-the-art baselines in evaluation accuracy, reprompting efficiency, and generation quality, without sacrificing general multimodal capabilities."
    },
    {
      "title": "Glance and Focus Reinforcement for Pan-cancer Screening",
      "url": "https://arxiv.org/abs/2601.19103",
      "date": "2026-01-27",
      "authors_text": "Linshan Wu, Jiaxin Zhuang, Hao Chen",
      "is_highlight": false,
      "score": 65.0,
      "summary": "GF-Screen introduces a novel reinforcement learning framework that enhances pan-cancer screening by effectively localizing and segmenting lesions in CT scans, outperforming existing methods.",
      "debug_abstract": "Pan-cancer screening in large-scale CT scans remains challenging for existing AI methods, primarily due to the difficulty of localizing diverse types of tiny lesions in large CT volumes. The extreme foreground-background imbalance significantly hinders models from focusing on diseased regions, while redundant focus on healthy regions not only decreases the efficiency but also increases false positives. Inspired by radiologists&#39; glance and focus diagnostic strategy, we introduce GF-Screen, a Glance and Focus reinforcement learning framework for pan-cancer screening. GF-Screen employs a Glance model to localize the diseased regions and a Focus model to precisely segment the lesions, where segmentation results of the Focus model are leveraged to reward the Glance model via Reinforcement Learning (RL). Specifically, the Glance model crops a group of sub-volumes from the entire CT volume and learns to select the sub-volumes with lesions for the Focus model to segment. Given that the selecting operation is non-differentiable for segmentation training, we propose to employ the segmentation results to reward the Glance model. To optimize the Glance model, we introduce a novel group relative learning paradigm, which employs group relative comparison to prioritize high-advantage predictions and discard low-advantage predictions within sub-volume groups, not only improving efficiency but also reducing false positives. In this way, for the first time, we effectively extend cutting-edge RL techniques to tackle the specific challenges in pan-cancer screening. Extensive experiments on 16 internal and 7 external datasets across 9 lesion types demonstrated the effectiveness of GF-Screen. Notably, GF-Screen leads the public validation leaderboard of MICCAI FLARE25 pan-cancer challenge, surpassing the FLARE24 champion solution by a large margin (+25.6% DSC and +28.2% NSD)."
    },
    {
      "title": "Enhancing Inverse Perspective Mapping for Automatic Vectorized Road Map Generation",
      "url": "https://arxiv.org/abs/2601.19536",
      "date": "2026-01-27",
      "authors_text": "Hongji Liu, Linwei Zheng, Yongjian Li, Mingkai Tang, Xiaoyang Yan",
      "is_highlight": false,
      "score": 70.0,
      "summary": "This study introduces a cost-effective framework using enhanced inverse perspective mapping and Catmull-Rom splines for high-precision, automatic vectorized road map generation.",
      "debug_abstract": "In this study, we present a low-cost and unified framework for vectorized road mapping leveraging enhanced inverse perspective mapping (IPM). In this framework, Catmull-Rom splines are utilized to characterize lane lines, and all the other ground markings are depicted using polygons uniformly. The results from instance segmentation serve as references to refine the three-dimensional position of spline control points and polygon corner points. In conjunction with this process, the homography matrix of IPM and vehicle poses are optimized simultaneously. Our proposed framework significantly reduces the mapping errors associated with IPM. It also improves the accuracy of the initial IPM homography matrix and the predicted vehicle poses. Furthermore, it addresses the limitations imposed by the coplanarity assumption in IPM. These enhancements enable IPM to be effectively applied to vectorized road mapping, which serves a cost-effective solution with enhanced accuracy. In addition, our framework generalizes road map elements to include all common ground markings and lane lines. The proposed framework is evaluated in two different practical scenarios, and the test results show that our method can automatically generate high-precision maps with near-centimeter-level accuracy. Importantly, the optimized IPM matrix achieves an accuracy comparable to that of manual calibration, while the accuracy of vehicle poses is also significantly improved."
    },
    {
      "title": "Geometry-Grounded Gaussian Splatting",
      "url": "https://arxiv.org/abs/2601.17835",
      "date": "2026-01-25",
      "authors_text": "Baowen Zhang, Chenxing Jiang, Heng Li, Shaojie Shen, Ping Tan",
      "is_highlight": true,
      "score": 53.0,
      "summary": "This paper introduces Geometry-Grounded Gaussian Splatting, enhancing shape extraction from Gaussian primitives by treating them as stochastic solids for improved depth map rendering and reconstruction quality.",
      "debug_abstract": "Gaussian Splatting (GS) has demonstrated impressive quality and efficiency in novel view synthesis. However, shape extraction from Gaussian primitives remains an open problem. Due to inadequate geometry parameterization and approximation, existing shape reconstruction methods suffer from poor multi-view consistency and are sensitive to floaters. In this paper, we present a rigorous theoretical derivation that establishes Gaussian primitives as a specific type of stochastic solids. This theoretical framework provides a principled foundation for Geometry-Grounded Gaussian Splatting by enabling the direct treatment of Gaussian primitives as explicit geometric representations. Using the volumetric nature of stochastic solids, our method efficiently renders high-quality depth maps for fine-grained geometry extraction. Experiments show that our method achieves the best shape reconstruction results among all Gaussian Splatting-based methods on public datasets."
    },
    {
      "title": "CoT-Seg: Rethinking Segmentation with Chain-of-Thought Reasoning and Self-Correction",
      "url": "https://arxiv.org/abs/2601.17420",
      "date": "2026-01-24",
      "authors_text": "Shiu-hong Kao, Chak Ho Huang, Huaiqian Liu, Yu-Wing Tai, Chi-Keung Tang",
      "is_highlight": false,
      "score": 68.0,
      "summary": "CoT-Seg introduces a training-free framework that enhances segmentation through chain-of-thought reasoning and self-correction, improving performance on complex queries and ambiguous images.",
      "debug_abstract": "Existing works of reasoning segmentation often fall short in complex cases, particularly when addressing complicated queries and out-of-domain images. Inspired by the chain-of-thought reasoning, where harder problems require longer thinking steps/time, this paper aims to explore a system that can think step-by-step, look up information if needed, generate results, self-evaluate its own results, and refine the results, in the same way humans approach harder questions. We introduce CoT-Seg, a training-free framework that rethinks reasoning segmentation by combining chain-of-thought reasoning with self-correction. Instead of fine-tuning, CoT-Seg leverages the inherent reasoning ability of pre-trained MLLMs (GPT-4o) to decompose queries into meta-instructions, extract fine-grained semantics from images, and identify target objects even under implicit or complex prompts. Moreover, CoT-Seg incorporates a self-correction stage: the model evaluates its own segmentation against the original query and reasoning trace, identifies mismatches, and iteratively refines the mask. This tight integration of reasoning and correction significantly improves reliability and robustness, especially in ambiguous or error-prone cases. Furthermore, our CoT-Seg framework allows easy incorporation of retrieval-augmented reasoning, enabling the system to access external knowledge when the input lacks sufficient information. To showcase CoT-Seg&#39;s ability to handle very challenging cases ,we introduce a new dataset ReasonSeg-Hard. Our results highlight that combining chain-of-thought reasoning, self-correction, offers a powerful paradigm for vision-language integration driven segmentation."
    },
    {
      "title": "Uni-RS: A Spatially Faithful Unified Understanding and Generation Model for Remote Sensing",
      "url": "https://arxiv.org/abs/2601.17673",
      "date": "2026-01-25",
      "authors_text": "Weiyu Zhang, Yuan Hu, Yong Li, Yu Liu",
      "is_highlight": false,
      "score": 58.0,
      "summary": "Uni-RS is a novel unified multimodal model for remote sensing that enhances spatial faithfulness in text-to-image generation by integrating spatial layout planning and supervision techniques.",
      "debug_abstract": "Unified remote sensing multimodal models exhibit a pronounced spatial reversal curse: Although they can accurately recognize and describe object locations in images, they often fail to faithfully execute the same spatial relations during text-to-image generation, where such relations constitute core semantic information in remote sensing. Motivated by this observation, we propose Uni-RS, the first unified multimodal model tailored for remote sensing, to explicitly address the spatial asymmetry between understanding and generation. Specifically, we first introduce explicit Spatial-Layout Planning to transform textual instructions into spatial layout plans, decoupling geometric planning from visual synthesis. We then impose Spatial-Aware Query Supervision to bias learnable queries toward spatial relations explicitly specified in the instruction. Finally, we develop Image-Caption Spatial Layout Variation to expose the model to systematic geometry-consistent spatial transformations. Extensive experiments across multiple benchmarks show that our approach substantially improves spatial faithfulness in text-to-image generation, while maintaining strong performance on multimodal understanding tasks like image captioning, visual grounding, and VQA tasks."
    },
    {
      "title": "TC-IDM: Grounding Video Generation for Executable Zero-shot Robot Motion",
      "url": "https://arxiv.org/abs/2601.18323",
      "date": "2026-01-26",
      "authors_text": "Weishi Mi, Yong Bao, Xiaowei Chi, Xiaozhu Ju, Zhiyuan Qin",
      "is_highlight": false,
      "score": 89.0,
      "summary": "The TC-IDM model enhances robot motion execution by bridging visual planning and physical control through tool-centric trajectory synthesis, achieving superior generalization and performance in diverse tasks.",
      "debug_abstract": "The vision-language-action (VLA) paradigm has enabled powerful robotic control by leveraging vision-language models, but its reliance on large-scale, high-quality robot data limits its generalization. Generative world models offer a promising alternative for general-purpose embodied AI, yet a critical gap remains between their pixel-level plans and physically executable actions. To this end, we propose the Tool-Centric Inverse Dynamics Model (TC-IDM). By focusing on the tool&#39;s imagined trajectory as synthesized by the world model, TC-IDM establishes a robust intermediate representation that bridges the gap between visual planning and physical control. TC-IDM extracts the tool&#39;s point cloud trajectories via segmentation and 3D motion estimation from generated videos. Considering diverse tool attributes, our architecture employs decoupled action heads to project these planned trajectories into 6-DoF end-effector motions and corresponding control signals. This plan-and-translate paradigm not only supports a wide range of end-effectors but also significantly improves viewpoint invariance. Furthermore, it exhibits strong generalization capabilities across long-horizon and out-of-distribution tasks, including interacting with deformable objects. In real-world evaluations, the world model with TC-IDM achieves an average success rate of 61.11 percent, with 77.7 percent on simple tasks and 38.46 percent on zero-shot deformable object tasks. It substantially outperforms end-to-end VLA-style baselines and other inverse dynamics models."
    },
    {
      "title": "Fast and Safe Trajectory Optimization for Mobile Manipulators With Neural Configuration Space Distance Field",
      "url": "https://arxiv.org/abs/2601.18548",
      "date": "2026-01-26",
      "authors_text": "Yulin Li, Zhiyuan Song, Yiming Li, Zhicheng Song, Kai Chen",
      "is_highlight": false,
      "score": 86.0,
      "summary": "This paper introduces Generalized Configuration Space Distance Fields (GCDF) for efficient trajectory optimization in mobile manipulators, enhancing collision reasoning and enabling agile motion in complex environments.",
      "debug_abstract": "Mobile manipulators promise agile, long-horizon behavior by coordinating base and arm motion, yet whole-body trajectory optimization in cluttered, confined spaces remains difficult due to high-dimensional nonconvexity and the need for fast, accurate collision reasoning. Configuration Space Distance Fields (CDF) enable fixed-base manipulators to model collisions directly in configuration space via smooth, implicit distances. This representation holds strong potential to bypass the nonlinear configuration-to-workspace mapping while preserving accurate whole-body geometry and providing optimization-friendly collision costs. Yet, extending this capability to mobile manipulators is hindered by unbounded workspaces and tighter base-arm coupling. We lift this promise to mobile manipulation with Generalized Configuration Space Distance Fields (GCDF), extending CDF to robots with both translational and rotational joints in unbounded workspaces with tighter base-arm coupling. We prove that GCDF preserves Euclidean-like local distance structure and accurately encodes whole-body geometry in configuration space, and develop a data generation and training pipeline that yields continuous neural GCDFs with accurate values and gradients, supporting efficient GPU-batched queries. Building on this representation, we develop a high-performance sequential convex optimization framework centered on GCDF-based collision reasoning. The solver scales to large numbers of implicit constraints through (i) online specification of neural constraints, (ii) sparsity-aware active-set detection with parallel batched evaluation across thousands of constraints, and (iii) incremental constraint management for rapid replanning under scene changes."
    },
    {
      "title": "EvoCUA: Evolving Computer Use Agents via Learning from Scalable Synthetic Experience",
      "url": "https://arxiv.org/abs/2601.15876",
      "date": "2026-01-22",
      "authors_text": "Taofeng Xue, Chong Peng, Mianqiu Huang, Linsen Guo, Tiancheng Han",
      "is_highlight": false,
      "score": 82.6,
      "summary": "EvoCUA introduces a self-sustaining evolutionary model for computer-use agents, leveraging autonomous task generation and iterative learning to significantly outperform existing benchmarks in multimodal AI.",
      "debug_abstract": "The development of native computer-use agents (CUA) represents a significant leap in multimodal AI. However, their potential is currently bottlenecked by the constraints of static data scaling. Existing paradigms relying primarily on passive imitation of static datasets struggle to capture the intricate causal dynamics inherent in long-horizon computer tasks. In this work, we introduce EvoCUA, a native computer use agentic model. Unlike static imitation, EvoCUA integrates data generation and policy optimization into a self-sustaining evolutionary cycle. To mitigate data scarcity, we develop a verifiable synthesis engine that autonomously generates diverse tasks coupled with executable validators. To enable large-scale experience acquisition, we design a scalable infrastructure orchestrating tens of thousands of asynchronous sandbox rollouts. Building on these massive trajectories, we propose an iterative evolving learning strategy to efficiently internalize this experience. This mechanism dynamically regulates policy updates by identifying capability boundaries -- reinforcing successful routines while transforming failure trajectories into rich supervision through error analysis and self-correction. Empirical evaluations on the OSWorld benchmark demonstrate that EvoCUA achieves a success rate of 56.7%, establishing a new open-source state-of-the-art. Notably, EvoCUA significantly outperforms the previous best open-source model, OpenCUA-72B (45.0%), and surpasses leading closed-weights models such as UI-TARS-2 (53.1%). Crucially, our results underscore the generalizability of this approach: the evolving paradigm driven by learning from experience yields consistent performance gains across foundation models of varying scales, establishing a robust and scalable path for advancing native agent capabilities."
    },
    {
      "title": "DualShield: Safe Model Predictive Diffusion via Reachability Analysis for Interactive Autonomous Driving",
      "url": "https://arxiv.org/abs/2601.15729",
      "date": "2026-01-22",
      "authors_text": "Rui Yang, Lei Zheng, Ruoyu Yao, Jun Ma",
      "is_highlight": true,
      "score": 92.7,
      "summary": "DualShield enhances autonomous driving safety by integrating Hamilton-Jacobi reachability analysis into diffusion models for proactive guidance and reactive safety assurance under uncertain interactions.",
      "debug_abstract": "Diffusion models have emerged as a powerful approach for multimodal motion planning in autonomous driving. However, their practical deployment is typically hindered by the inherent difficulty in enforcing vehicle dynamics and a critical reliance on accurate predictions of other agents, making them prone to safety issues under uncertain interactions. To address these limitations, we introduce DualShield, a planning and control framework that leverages Hamilton-Jacobi (HJ) reachability value functions in a dual capacity. First, the value functions act as proactive guidance, steering the diffusion denoising process towards safe and dynamically feasible regions. Second, they form a reactive safety shield using control barrier-value functions (CBVFs) to modify the executed actions and ensure safety. This dual mechanism preserves the rich exploration capabilities of diffusion models while providing principled safety assurance under uncertain and even adversarial interactions. Simulations in challenging unprotected U-turn scenarios demonstrate that DualShield significantly improves both safety and task efficiency compared to leading methods from different planning paradigms under uncertainty."
    }
  ],
  "机器人研究所": [
    {
      "title": "Visual Reasoning over Time Series via Multi-Agent System",
      "url": "https://arxiv.org/abs/2602.03026",
      "date": "2026-02-03",
      "authors_text": "Weilin Ruan, Yuxuan Liang",
      "is_highlight": false,
      "score": 66.0,
      "summary": "MAS4TS is a multi-agent system that enhances time series analysis through visual reasoning and adaptive tool usage, achieving state-of-the-art performance across diverse tasks.",
      "teaser_image": "https://arxiv.org/html/2602.03026/x2.png",
      "debug_abstract": "Time series analysis underpins many real-world applications, yet existing time-series-specific methods and pretrained large-model-based approaches remain limited in integrating intuitive visual reasoning and generalizing across tasks with adaptive tool usage. To address these limitations, we propose MAS4TS, a tool-driven multi-agent system for general time series tasks, built upon an Analyzer-Reasoner-Executor paradigm that integrates agent communication, visual reasoning, and latent reconstruction within a unified framework. MAS4TS first performs visual reasoning over time series plots with structured priors using a Vision-Language Model to extract temporal structures, and subsequently reconstructs predictive trajectories in latent space. Three specialized agents coordinate via shared memory and gated communication, while a router selects task-specific tool chains for execution. Extensive experiments on multiple benchmarks demonstrate that MAS4TS achieves state-of-the-art performance across a wide range of time series tasks, while exhibiting strong generalization and efficient inference."
    },
    {
      "title": "Neural Predictor-Corrector: Solving Homotopy Problems with Reinforcement Learning",
      "url": "https://arxiv.org/abs/2602.03086",
      "date": "2026-02-03",
      "authors_text": "Jiayao Mai, Bangyan Liao, Zhenjun Zhao, Yingping Zeng, Haoang Li",
      "is_highlight": false,
      "score": 64.0,
      "summary": "The Neural Predictor-Corrector (NPC) framework uses reinforcement learning to automate step size and termination decisions in solving diverse homotopy problems, outperforming traditional methods.",
      "teaser_image": "https://arxiv.org/html/2602.03086/x2.png",
      "debug_abstract": "The Homotopy paradigm, a general principle for solving challenging problems, appears across diverse domains such as robust optimization, global optimization, polynomial root-finding, and sampling. Practical solvers for these problems typically follow a predictor-corrector (PC) structure, but rely on hand-crafted heuristics for step sizes and iteration termination, which are often suboptimal and task-specific. To address this, we unify these problems under a single framework, which enables the design of a general neural solver. Building on this unified view, we propose Neural Predictor-Corrector (NPC), which replaces hand-crafted heuristics with automatically learned policies. NPC formulates policy selection as a sequential decision-making problem and leverages reinforcement learning to automatically discover efficient strategies. To further enhance generalization, we introduce an amortized training mechanism, enabling one-time offline training for a class of problems and efficient online inference on new instances. Experiments on four representative homotopy problems demonstrate that our method generalizes effectively to unseen instances. It consistently outperforms classical and specialized baselines in efficiency while demonstrating superior stability across tasks, highlighting the value of unifying homotopy methods into a single neural framework."
    },
    {
      "title": "Diffusion-Driven Inter-Outer Surface Separation for Point Clouds with Open Boundaries",
      "url": "https://arxiv.org/abs/2602.00739",
      "date": "2026-01-31",
      "authors_text": "Zhengyan Qin, Liyuan Qiu",
      "is_highlight": false,
      "score": 72.0,
      "summary": "The paper presents a diffusion-based algorithm for effectively separating inter and outer surfaces in double-layered point clouds with open boundaries, addressing artifacts from TSDF fusion.",
      "teaser_image": "https://arxiv.org/html/2602.00739/figures/pipeline5.png",
      "debug_abstract": "We propose a diffusion-based algorithm for separating the inter and outer layer surfaces from double-layered point clouds, particularly those exhibiting the &#34;double surface artifact&#34; caused by truncation in Truncated Signed Distance Function (TSDF) fusion during indoor or medical 3D reconstruction. This artifact arises from asymmetric truncation thresholds, leading to erroneous inter and outer shells in the fused volume, which our method addresses by extracting the true inter layer to mitigate challenges like overlapping surfaces and disordered normals. We focus on point clouds with \\emph{open boundaries} (i.e., sampled surfaces with topological openings/holes through which particles may escape), rather than point clouds with \\emph{missing surface regions} where no samples exist. Our approach enables robust processing of both watertight and open-boundary models, achieving extraction of the inter layer from 20,000 inter and 20,000 outer points in approximately 10 seconds. This solution is particularly effective for applications requiring accurate surface representations, such as indoor scene modeling and medical imaging, where double-layered point clouds are prevalent, and it accommodates both closed (watertight) and open-boundary surface geometries. Our goal is \\emph{post-hoc} inter/outer shell separation as a lightweight module after TSDF fusion; we do not aim to replace full variational or learning-based reconstruction pipelines."
    },
    {
      "title": "Physics-informed Diffusion Mamba Transformer for Real-world Driving",
      "url": "https://arxiv.org/abs/2602.00808",
      "date": "2026-01-31",
      "authors_text": "Hang Zhou, Qiang Zhang, Peiran Liu, Yihao Qin, Zhaoxu Yan",
      "is_highlight": false,
      "score": 74.0,
      "summary": "The paper presents a Diffusion Mamba Transformer that integrates sequential context and physical constraints for improved trajectory planning in autonomous driving, outperforming existing models.",
      "teaser_image": "https://arxiv.org/html/2602.00808/difference_pipeline3.png",
      "debug_abstract": "Autonomous driving systems demand trajectory planners that not only model the inherent uncertainty of future motions but also respect complex temporal dependencies and underlying physical laws. While diffusion-based generative models excel at capturing multi-modal distributions, they often fail to incorporate long-term sequential contexts and domain-specific physical priors. In this work, we bridge these gaps with two key innovations. First, we introduce a Diffusion Mamba Transformer architecture that embeds mamba and attention into the diffusion process, enabling more effective aggregation of sequential input contexts from sensor streams and past motion histories. Second, we design a Port-Hamiltonian Neural Network module that seamlessly integrates energy-based physical constraints into the diffusion model, thereby enhancing trajectory predictions with both consistency and interpretability. Extensive evaluations on standard autonomous driving benchmarks demonstrate that our unified framework significantly outperforms state-of-the-art baselines in predictive accuracy, physical plausibility, and robustness, thereby advancing safe and reliable motion planning."
    },
    {
      "title": "Invariance on Manifolds: Understanding Robust Visual Representations for Place Recognition",
      "url": "https://arxiv.org/abs/2602.00841",
      "date": "2026-01-31",
      "authors_text": "Jintao Cheng, Weibin Li, Zhijian He, Jin Wu, Chi Man Vong",
      "is_highlight": false,
      "score": 66.0,
      "summary": "This paper presents a training-free Second-Order Geometric Statistics framework for robust visual place recognition, utilizing covariance descriptors on the SPD manifold for effective zero-shot generalization.",
      "teaser_image": "https://arxiv.org/html/2602.00841/x2.png",
      "debug_abstract": "Visual Place Recognition (VPR) demands representations robust to drastic environmental and viewpoint shifts. Current aggregation paradigms, however, either rely on data-hungry supervision or simplistic first-order statistics, often neglecting intrinsic structural correlations. In this work, we propose a Second-Order Geometric Statistics framework that inherently captures geometric stability without training. We conceptualize scenes as covariance descriptors on the Symmetric Positive Definite (SPD) manifold, where perturbations manifest as tractable congruence transformations. By leveraging geometry-aware Riemannian mappings, we project these descriptors into a linearized Euclidean embedding, effectively decoupling signal structure from noise. Our approach introduces a training-free framework built upon fixed, pre-trained backbones, achieving strong zero-shot generalization without parameter updates. Extensive experiments confirm that our method achieves highly competitive performance against state-of-the-art baselines, particularly excelling in challenging zero-shot scenarios."
    },
    {
      "title": "PolicyFlow: Policy Optimization with Continuous Normalizing Flow in Reinforcement Learning",
      "url": "https://arxiv.org/abs/2602.01156",
      "date": "2026-02-01",
      "authors_text": "Shunpeng Yang, Ben Liu, Hua Chen",
      "is_highlight": false,
      "score": 71.0,
      "summary": "PolicyFlow is a novel reinforcement learning algorithm that integrates continuous normalizing flows with PPO, enhancing performance and stability while reducing computational costs and encouraging diverse behaviors.",
      "teaser_image": "https://arxiv.org/html/2602.01156/x1.png",
      "debug_abstract": "Among on-policy reinforcement learning algorithms, Proximal Policy Optimization (PPO) demonstrates is widely favored for its simplicity, numerical stability, and strong empirical performance. Standard PPO relies on surrogate objectives defined via importance ratios, which require evaluating policy likelihood that is typically straightforward when the policy is modeled as a Gaussian distribution. However, extending PPO to more expressive, high-capacity policy models such as continuous normalizing flows (CNFs), also known as flow-matching models, is challenging because likelihood evaluation along the full flow trajectory is computationally expensive and often numerically unstable. To resolve this issue, we propose PolicyFlow, a novel on-policy CNF-based reinforcement learning algorithm that integrates expressive CNF policies with PPO-style objectives without requiring likelihood evaluation along the full flow path. PolicyFlow approximates importance ratios using velocity field variations along a simple interpolation path, reducing computational overhead without compromising training stability. To further prevent mode collapse and further encourage diverse behaviors, we propose the Brownian Regularizer, an implicit policy entropy regularizer inspired by Brownian motion, which is conceptually elegant and computationally lightweight. Experiments on diverse tasks across various environments including MultiGoal, PointMaze, IsaacLab and MuJoCo Playground show that PolicyFlow achieves competitive or superior performance compared to PPO using Gaussian policies and flow-based baselines including FPO and DPPO. Notably, results on MultiGoal highlight PolicyFlow&#39;s ability to capture richer multimodal action distributions."
    },
    {
      "title": "Do All Individual Layers Help? An Empirical Study of Task-Interfering Layers in Vision-Language Models",
      "url": "https://arxiv.org/abs/2602.01167",
      "date": "2026-02-01",
      "authors_text": "Zhiming Liu, Yujie Wei, Lei Feng, Xiu Su, Xiaobo Xia",
      "is_highlight": false,
      "score": 52.0,
      "summary": "This study identifies Task-Interfering Layers in vision-language models, demonstrating that selectively bypassing certain layers can enhance task performance without retraining.",
      "teaser_image": "https://arxiv.org/html/2602.01167/x1.png",
      "debug_abstract": "Current VLMs have demonstrated capabilities across a wide range of multimodal tasks. Typically, in a pretrained VLM, all layers are engaged by default to make predictions on downstream tasks. We find that intervening on a single layer, such as by zeroing its parameters, can improve the performance on certain tasks, indicating that some layers hinder rather than help downstream tasks. We systematically investigate how individual layers influence different tasks via layer intervention. Specifically, we measure the change in performance relative to the base model after intervening on each layer and observe improvements when bypassing specific layers. This improvement can be generalizable across models and datasets, indicating the presence of Task-Interfering Layers that harm downstream tasks&#39; performance. We introduce Task-Layer Interaction Vector, which quantifies the effect of intervening on each layer of a VLM given a task. These task-interfering layers exhibit task-specific sensitivity patterns: tasks requiring similar capabilities show consistent response trends under layer interventions, as evidenced by the high similarity in their task-layer interaction vectors. Inspired by these findings, we propose TaLo (Task-Adaptive Layer Knockout), a training-free, test-time adaptation method that dynamically identifies and bypasses the most interfering layer for a given task. Without parameter updates, TaLo improves performance across various models and datasets, including boosting Qwen-VL&#39;s accuracy on the Maps task in ScienceQA by up to 16.6%. Our work reveals an unexpected form of modularity in pretrained VLMs and provides a plug-and-play, training-free mechanism to unlock hidden capabilities at inference time. The source code will be publicly available."
    },
    {
      "title": "Towards Exploratory and Focused Manipulation with Bimanual Active Perception: A New Problem, Benchmark and Strategy",
      "url": "https://arxiv.org/abs/2602.01939",
      "date": "2026-02-02",
      "authors_text": "Yuxin He, Ruihao Zhang, Tianao Shen, Cheng Liu, Qiang Nie",
      "is_highlight": false,
      "score": 28.0,
      "summary": "The paper introduces the Exploratory and Focused Manipulation (EFM) problem, establishes the EFM-10 benchmark, and proposes a Bimanual Active Perception strategy to enhance manipulation tasks.",
      "teaser_image": "https://arxiv.org/html/2602.01939/x2.png",
      "debug_abstract": "Recently, active vision has reemerged as an important concept for manipulation, since visual occlusion occurs more frequently when main cameras are mounted on the robot heads. We reflect on the visual occlusion issue and identify its essence as the absence of information useful for task completion. Inspired by this, we come up with the more fundamental problem of Exploratory and Focused Manipulation (EFM). The proposed problem is about actively collecting information to complete challenging manipulation tasks that require exploration or focus. As an initial attempt to address this problem, we establish the EFM-10 benchmark that consists of 4 categories of tasks that align with our definition (10 tasks in total). We further come up with a Bimanual Active Perception (BAP) strategy, which leverages one arm to provide active vision and another arm to provide force sensing while manipulating. Based on this idea, we collect a dataset named BAPData for the tasks in EFM-10. With the dataset, we successfully verify the effectiveness of the BAP strategy in an imitation learning manner. We hope that the EFM-10 benchmark along with the BAP strategy can become a cornerstone that facilitates future research towards this direction. Project website: this http URL."
    },
    {
      "title": "Breaking the Static Graph: Context-Aware Traversal for Robust Retrieval-Augmented Generation",
      "url": "https://arxiv.org/abs/2602.01965",
      "date": "2026-02-02",
      "authors_text": "Kwun Hang Lau, Fangyuan Zhang, Boyu Ruan, Yingli Zhou, Qintian Guo",
      "is_highlight": false,
      "score": 27.0,
      "summary": "CatRAG enhances Retrieval-Augmented Generation by transforming static Knowledge Graphs into query-adaptive structures, improving multi-hop evidence retrieval and reasoning completeness.",
      "teaser_image": "https://arxiv.org/html/2602.01965/image/HubBiasShift.png",
      "debug_abstract": "Recent advances in Retrieval-Augmented Generation (RAG) have shifted from simple vector similarity to structure-aware approaches like HippoRAG, which leverage Knowledge Graphs (KGs) and Personalized PageRank (PPR) to capture multi-hop dependencies. However, these methods suffer from a &#34;Static Graph Fallacy&#34;: they rely on fixed transition probabilities determined during indexing. This rigidity ignores the query-dependent nature of edge relevance, causing semantic drift where random walks are diverted into high-degree &#34;hub&#34; nodes before reaching critical downstream evidence. Consequently, models often achieve high partial recall but fail to retrieve the complete evidence chain required for multi-hop queries. To address this, we propose CatRAG, Context-Aware Traversal for robust RAG, a framework that builds on the HippoRAG 2 architecture and transforms the static KG into a query-adaptive navigation structure. We introduce a multi-faceted framework to steer the random walk: (1) Symbolic Anchoring, which injects weak entity constraints to regularize the random walk; (2) Query-Aware Dynamic Edge Weighting, which dynamically modulates graph structure, to prune irrelevant paths while amplifying those aligned with the query&#39;s intent; and (3) Key-Fact Passage Weight Enhancement, a cost-efficient bias that structurally anchors the random walk to likely evidence. Experiments across four multi-hop benchmarks demonstrate that CatRAG consistently outperforms state of the art baselines. Our analysis reveals that while standard Recall metrics show modest gains, CatRAG achieves substantial improvements in reasoning completeness, the capacity to recover the entire evidence path without gaps. These results reveal that our approach effectively bridges the gap between retrieving partial context and enabling fully grounded reasoning. Resources are available at this https URL."
    },
    {
      "title": "Towards Agentic Intelligence for Materials Science",
      "url": "https://arxiv.org/abs/2602.00169",
      "date": "2026-01-29",
      "authors_text": "Huan Zhang, Yizhan Li, Wenhao Huang, Ziyu Hou, Yu Song",
      "is_highlight": false,
      "score": 12.0,
      "summary": "This paper advocates for developing agentic AI systems in materials science that autonomously plan, act, and learn to enhance discovery processes beyond traditional models.",
      "teaser_image": "https://arxiv.org/html/2602.00169/x1.png",
      "debug_abstract": "The convergence of artificial intelligence and materials science presents a transformative opportunity, but achieving true acceleration in discovery requires moving beyond task-isolated, fine-tuned models toward agentic systems that plan, act, and learn across the full discovery loop. This survey advances a unique pipeline-centric view that spans from corpus curation and pretraining, through domain adaptation and instruction tuning, to goal-conditioned agents interfacing with simulation and experimental platforms. Unlike prior reviews, we treat the entire process as an end-to-end system to be optimized for tangible discovery outcomes rather than proxy benchmarks. This perspective allows us to trace how upstream design choices-such as data curation and training objectives-can be aligned with downstream experimental success through effective credit assignment. To bridge communities and establish a shared frame of reference, we first present an integrated lens that aligns terminology, evaluation, and workflow stages across AI and materials science. We then analyze the field through two focused lenses: From the AI perspective, the survey details LLM strengths in pattern recognition, predictive analytics, and natural language processing for literature mining, materials characterization, and property prediction; from the materials science perspective, it highlights applications in materials design, process optimization, and the acceleration of computational workflows via integration with external tools (e.g., DFT, robotic labs). Finally, we contrast passive, reactive approaches with agentic design, cataloging current contributions while motivating systems that pursue long-horizon goals with autonomy, memory, and tool use. This survey charts a practical roadmap towards autonomous, safety-aware LLM agents aimed at discovering novel and useful materials."
    },
    {
      "title": "Advances and Innovations in the Multi-Agent Robotic System (MARS) Challenge",
      "url": "https://arxiv.org/abs/2601.18733",
      "date": "2026-01-26",
      "authors_text": "Li Kang, Heng Zhou, Xiufeng Song, Rui Li, Bruno N. Y. Chen",
      "is_highlight": false,
      "score": 78.0,
      "summary": "The MARS Challenge promotes advancements in multi-agent robotic systems by exploring vision-language models for collaborative task execution and planning in dynamic environments.",
      "teaser_image": null,
      "debug_abstract": "Recent advancements in multimodal large language models and vision-languageaction models have significantly driven progress in Embodied AI. As the field transitions toward more complex task scenarios, multi-agent system frameworks are becoming essential for achieving scalable, efficient, and collaborative solutions. This shift is fueled by three primary factors: increasing agent capabilities, enhancing system efficiency through task delegation, and enabling advanced human-agent interactions. To address the challenges posed by multi-agent collaboration, we propose the Multi-Agent Robotic System (MARS) Challenge, held at the NeurIPS 2025 Workshop on SpaVLE. The competition focuses on two critical areas: planning and control, where participants explore multi-agent embodied planning using vision-language models (VLMs) to coordinate tasks and policy execution to perform robotic manipulation in dynamic environments. By evaluating solutions submitted by participants, the challenge provides valuable insights into the design and coordination of embodied multi-agent systems, contributing to the future development of advanced collaborative AI systems."
    },
    {
      "title": "Gaussian Belief Propagation Network for Depth Completion",
      "url": "https://arxiv.org/abs/2601.21291",
      "date": "2026-01-29",
      "authors_text": "Jie Tang, Pingping Xie, Jian Li, Ping Tan",
      "is_highlight": false,
      "score": 68.0,
      "summary": "The Gaussian Belief Propagation Network (GBPN) integrates deep learning with probabilistic models for effective depth completion from sparse measurements, achieving state-of-the-art results across various benchmarks.",
      "teaser_image": "https://arxiv.org/html/2601.21291/x1.png",
      "debug_abstract": "Depth completion aims to predict a dense depth map from a color image with sparse depth measurements. Although deep learning methods have achieved state-of-the-art (SOTA), effectively handling the sparse and irregular nature of input depth data in deep networks remains a significant challenge, often limiting performance, especially under high sparsity. To overcome this limitation, we introduce the Gaussian Belief Propagation Network (GBPN), a novel hybrid framework synergistically integrating deep learning with probabilistic graphical models for end-to-end depth completion. Specifically, a scene-specific Markov Random Field (MRF) is dynamically constructed by the Graphical Model Construction Network (GMCN), and then inferred via Gaussian Belief Propagation (GBP) to yield the dense depth distribution. Crucially, the GMCN learns to construct not only the data-dependent potentials of MRF but also its structure by predicting adaptive non-local edges, enabling the capture of complex, long-range spatial dependencies. Furthermore, we enhance GBP with a serial \\&amp; parallel message passing scheme, designed for effective information propagation, particularly from sparse measurements. Extensive experiments demonstrate that GBPN achieves SOTA performance on the NYUv2 and KITTI benchmarks. Evaluations across varying sparsity levels, sparsity patterns, and datasets highlight GBPN&#39;s superior performance, notable robustness, and generalizable capability."
    },
    {
      "title": "4D-CAAL: 4D Radar-Camera Calibration and Auto-Labeling for Autonomous Driving",
      "url": "https://arxiv.org/abs/2601.21454",
      "date": "2026-01-29",
      "authors_text": "Shanliang Yao, Zhuoxiao Li, Runwei Guan, Kebin Cao, Meng Xia",
      "is_highlight": false,
      "score": 89.0,
      "summary": "The 4D-CAAL framework integrates 4D radar-camera calibration and auto-labeling using a dual-purpose target and robust correspondence matching, enhancing autonomous driving perception efficiency.",
      "teaser_image": "https://arxiv.org/html/2601.21454/images/4DRC-fixed.png",
      "debug_abstract": "4D radar has emerged as a critical sensor for autonomous driving, primarily due to its enhanced capabilities in elevation measurement and higher resolution compared to traditional 3D radar. Effective integration of 4D radar with cameras requires accurate extrinsic calibration, and the development of radar-based perception algorithms demands large-scale annotated datasets. However, existing calibration methods often employ separate targets optimized for either visual or radar modalities, complicating correspondence establishment. Furthermore, manually labeling sparse radar data is labor-intensive and unreliable. To address these challenges, we propose 4D-CAAL, a unified framework for 4D radar-camera calibration and auto-labeling. Our approach introduces a novel dual-purpose calibration target design, integrating a checkerboard pattern on the front surface for camera detection and a corner reflector at the center of the back surface for radar detection. We develop a robust correspondence matching algorithm that aligns the checkerboard center with the strongest radar reflection point, enabling accurate extrinsic calibration. Subsequently, we present an auto-labeling pipeline that leverages the calibrated sensor relationship to transfer annotations from camera-based segmentations to radar point clouds through geometric projection and multi-feature optimization. Extensive experiments demonstrate that our method achieves high calibration accuracy while significantly reducing manual annotation effort, thereby accelerating the development of robust multi-modal perception systems for autonomous driving."
    },
    {
      "title": "DynaWeb: Model-Based Reinforcement Learning of Web Agents",
      "url": "https://arxiv.org/abs/2601.22149",
      "date": "2026-01-29",
      "authors_text": "Hang Ding, Peidong Liu, Junqiao Wang, Ziwei Ji, Meng Cao",
      "is_highlight": false,
      "score": 77.0,
      "summary": "DynaWeb introduces a model-based reinforcement learning framework that enhances web agent training by simulating interactions with a web world model, improving efficiency and performance.",
      "teaser_image": "https://arxiv.org/html/2601.22149/figures/dynaweb.png",
      "debug_abstract": "The development of autonomous web agents, powered by Large Language Models (LLMs) and reinforcement learning (RL), represents a significant step towards general-purpose AI assistants. However, training these agents is severely hampered by the challenges of interacting with the live internet, which is inefficient, costly, and fraught with risks. Model-based reinforcement learning (MBRL) offers a promising solution by learning a world model of the environment to enable simulated interaction. This paper introduces DynaWeb, a novel MBRL framework that trains web agents through interacting with a web world model trained to predict naturalistic web page representations given agent actions. This model serves as a synthetic web environment where an agent policy can dream by generating vast quantities of rollout action trajectories for efficient online reinforcement learning. Beyond free policy rollouts, DynaWeb incorporates real expert trajectories from training data, which are randomly interleaved with on-policy rollouts during training to improve stability and sample efficiency. Experiments conducted on the challenging WebArena and WebVoyager benchmarks demonstrate that DynaWeb consistently and significantly improves the performance of state-of-the-art open-source web agent models. Our findings establish the viability of training web agents through imagination, offering a scalable and efficient way to scale up online agentic RL."
    },
    {
      "title": "Endogenous Reprompting: Self-Evolving Cognitive Alignment for Unified Multimodal Models",
      "url": "https://arxiv.org/abs/2601.20305",
      "date": "2026-01-28",
      "authors_text": "Zhenchen Tang, Songlin Yang, Zichuan Wang, Bo Peng, Yang Li",
      "is_highlight": false,
      "score": 69.0,
      "summary": "The paper introduces Endogenous Reprompting and the SEER framework, enhancing Unified Multimodal Models' generative reasoning through self-aligned descriptors and reinforcement learning, outperforming existing methods.",
      "teaser_image": "https://arxiv.org/html/2601.20305/x2.png",
      "debug_abstract": "Unified Multimodal Models (UMMs) exhibit strong understanding, yet this capability often fails to effectively guide generation. We identify this as a Cognitive Gap: the model lacks the understanding of how to enhance its own generation process. To bridge this gap, we propose Endogenous Reprompting, a mechanism that transforms the model&#39;s understanding from a passive encoding process into an explicit generative reasoning step by generating self-aligned descriptors during generation. To achieve this, we introduce SEER (Self-Evolving Evaluator and Reprompter), a training framework that establishes a two-stage endogenous loop using only 300 samples from a compact proxy task, Visual Instruction Elaboration. First, Reinforcement Learning with Verifiable Rewards (RLVR) activates the model&#39;s latent evaluation ability via curriculum learning, producing a high-fidelity endogenous reward signal. Second, Reinforcement Learning with Model-rewarded Thinking (RLMT) leverages this signal to optimize the generative reasoning policy. Experiments show that SEER consistently outperforms state-of-the-art baselines in evaluation accuracy, reprompting efficiency, and generation quality, without sacrificing general multimodal capabilities."
    },
    {
      "title": "Glance and Focus Reinforcement for Pan-cancer Screening",
      "url": "https://arxiv.org/abs/2601.19103",
      "date": "2026-01-27",
      "authors_text": "Linshan Wu, Jiaxin Zhuang, Hao Chen",
      "is_highlight": false,
      "score": 65.0,
      "summary": "GF-Screen introduces a novel reinforcement learning framework that enhances pan-cancer screening by effectively localizing and segmenting lesions in CT scans, outperforming existing methods.",
      "debug_abstract": "Pan-cancer screening in large-scale CT scans remains challenging for existing AI methods, primarily due to the difficulty of localizing diverse types of tiny lesions in large CT volumes. The extreme foreground-background imbalance significantly hinders models from focusing on diseased regions, while redundant focus on healthy regions not only decreases the efficiency but also increases false positives. Inspired by radiologists&#39; glance and focus diagnostic strategy, we introduce GF-Screen, a Glance and Focus reinforcement learning framework for pan-cancer screening. GF-Screen employs a Glance model to localize the diseased regions and a Focus model to precisely segment the lesions, where segmentation results of the Focus model are leveraged to reward the Glance model via Reinforcement Learning (RL). Specifically, the Glance model crops a group of sub-volumes from the entire CT volume and learns to select the sub-volumes with lesions for the Focus model to segment. Given that the selecting operation is non-differentiable for segmentation training, we propose to employ the segmentation results to reward the Glance model. To optimize the Glance model, we introduce a novel group relative learning paradigm, which employs group relative comparison to prioritize high-advantage predictions and discard low-advantage predictions within sub-volume groups, not only improving efficiency but also reducing false positives. In this way, for the first time, we effectively extend cutting-edge RL techniques to tackle the specific challenges in pan-cancer screening. Extensive experiments on 16 internal and 7 external datasets across 9 lesion types demonstrated the effectiveness of GF-Screen. Notably, GF-Screen leads the public validation leaderboard of MICCAI FLARE25 pan-cancer challenge, surpassing the FLARE24 champion solution by a large margin (+25.6% DSC and +28.2% NSD)."
    },
    {
      "title": "Enhancing Inverse Perspective Mapping for Automatic Vectorized Road Map Generation",
      "url": "https://arxiv.org/abs/2601.19536",
      "date": "2026-01-27",
      "authors_text": "Hongji Liu, Linwei Zheng, Yongjian Li, Mingkai Tang, Xiaoyang Yan",
      "is_highlight": false,
      "score": 70.0,
      "summary": "This study introduces a cost-effective framework using enhanced inverse perspective mapping and Catmull-Rom splines for high-precision, automatic vectorized road map generation.",
      "debug_abstract": "In this study, we present a low-cost and unified framework for vectorized road mapping leveraging enhanced inverse perspective mapping (IPM). In this framework, Catmull-Rom splines are utilized to characterize lane lines, and all the other ground markings are depicted using polygons uniformly. The results from instance segmentation serve as references to refine the three-dimensional position of spline control points and polygon corner points. In conjunction with this process, the homography matrix of IPM and vehicle poses are optimized simultaneously. Our proposed framework significantly reduces the mapping errors associated with IPM. It also improves the accuracy of the initial IPM homography matrix and the predicted vehicle poses. Furthermore, it addresses the limitations imposed by the coplanarity assumption in IPM. These enhancements enable IPM to be effectively applied to vectorized road mapping, which serves a cost-effective solution with enhanced accuracy. In addition, our framework generalizes road map elements to include all common ground markings and lane lines. The proposed framework is evaluated in two different practical scenarios, and the test results show that our method can automatically generate high-precision maps with near-centimeter-level accuracy. Importantly, the optimized IPM matrix achieves an accuracy comparable to that of manual calibration, while the accuracy of vehicle poses is also significantly improved."
    },
    {
      "title": "Geometry-Grounded Gaussian Splatting",
      "url": "https://arxiv.org/abs/2601.17835",
      "date": "2026-01-25",
      "authors_text": "Baowen Zhang, Chenxing Jiang, Heng Li, Shaojie Shen, Ping Tan",
      "is_highlight": true,
      "score": 53.0,
      "summary": "This paper introduces Geometry-Grounded Gaussian Splatting, enhancing shape extraction from Gaussian primitives by treating them as stochastic solids for improved depth map rendering and reconstruction quality.",
      "debug_abstract": "Gaussian Splatting (GS) has demonstrated impressive quality and efficiency in novel view synthesis. However, shape extraction from Gaussian primitives remains an open problem. Due to inadequate geometry parameterization and approximation, existing shape reconstruction methods suffer from poor multi-view consistency and are sensitive to floaters. In this paper, we present a rigorous theoretical derivation that establishes Gaussian primitives as a specific type of stochastic solids. This theoretical framework provides a principled foundation for Geometry-Grounded Gaussian Splatting by enabling the direct treatment of Gaussian primitives as explicit geometric representations. Using the volumetric nature of stochastic solids, our method efficiently renders high-quality depth maps for fine-grained geometry extraction. Experiments show that our method achieves the best shape reconstruction results among all Gaussian Splatting-based methods on public datasets."
    },
    {
      "title": "CoT-Seg: Rethinking Segmentation with Chain-of-Thought Reasoning and Self-Correction",
      "url": "https://arxiv.org/abs/2601.17420",
      "date": "2026-01-24",
      "authors_text": "Shiu-hong Kao, Chak Ho Huang, Huaiqian Liu, Yu-Wing Tai, Chi-Keung Tang",
      "is_highlight": false,
      "score": 68.0,
      "summary": "CoT-Seg introduces a training-free framework that enhances segmentation through chain-of-thought reasoning and self-correction, improving performance on complex queries and ambiguous images.",
      "debug_abstract": "Existing works of reasoning segmentation often fall short in complex cases, particularly when addressing complicated queries and out-of-domain images. Inspired by the chain-of-thought reasoning, where harder problems require longer thinking steps/time, this paper aims to explore a system that can think step-by-step, look up information if needed, generate results, self-evaluate its own results, and refine the results, in the same way humans approach harder questions. We introduce CoT-Seg, a training-free framework that rethinks reasoning segmentation by combining chain-of-thought reasoning with self-correction. Instead of fine-tuning, CoT-Seg leverages the inherent reasoning ability of pre-trained MLLMs (GPT-4o) to decompose queries into meta-instructions, extract fine-grained semantics from images, and identify target objects even under implicit or complex prompts. Moreover, CoT-Seg incorporates a self-correction stage: the model evaluates its own segmentation against the original query and reasoning trace, identifies mismatches, and iteratively refines the mask. This tight integration of reasoning and correction significantly improves reliability and robustness, especially in ambiguous or error-prone cases. Furthermore, our CoT-Seg framework allows easy incorporation of retrieval-augmented reasoning, enabling the system to access external knowledge when the input lacks sufficient information. To showcase CoT-Seg&#39;s ability to handle very challenging cases ,we introduce a new dataset ReasonSeg-Hard. Our results highlight that combining chain-of-thought reasoning, self-correction, offers a powerful paradigm for vision-language integration driven segmentation."
    },
    {
      "title": "Uni-RS: A Spatially Faithful Unified Understanding and Generation Model for Remote Sensing",
      "url": "https://arxiv.org/abs/2601.17673",
      "date": "2026-01-25",
      "authors_text": "Weiyu Zhang, Yuan Hu, Yong Li, Yu Liu",
      "is_highlight": false,
      "score": 58.0,
      "summary": "Uni-RS is a novel unified multimodal model for remote sensing that enhances spatial faithfulness in text-to-image generation by integrating spatial layout planning and supervision techniques.",
      "debug_abstract": "Unified remote sensing multimodal models exhibit a pronounced spatial reversal curse: Although they can accurately recognize and describe object locations in images, they often fail to faithfully execute the same spatial relations during text-to-image generation, where such relations constitute core semantic information in remote sensing. Motivated by this observation, we propose Uni-RS, the first unified multimodal model tailored for remote sensing, to explicitly address the spatial asymmetry between understanding and generation. Specifically, we first introduce explicit Spatial-Layout Planning to transform textual instructions into spatial layout plans, decoupling geometric planning from visual synthesis. We then impose Spatial-Aware Query Supervision to bias learnable queries toward spatial relations explicitly specified in the instruction. Finally, we develop Image-Caption Spatial Layout Variation to expose the model to systematic geometry-consistent spatial transformations. Extensive experiments across multiple benchmarks show that our approach substantially improves spatial faithfulness in text-to-image generation, while maintaining strong performance on multimodal understanding tasks like image captioning, visual grounding, and VQA tasks."
    },
    {
      "title": "TC-IDM: Grounding Video Generation for Executable Zero-shot Robot Motion",
      "url": "https://arxiv.org/abs/2601.18323",
      "date": "2026-01-26",
      "authors_text": "Weishi Mi, Yong Bao, Xiaowei Chi, Xiaozhu Ju, Zhiyuan Qin",
      "is_highlight": false,
      "score": 89.0,
      "summary": "The TC-IDM model enhances robot motion execution by bridging visual planning and physical control through tool-centric trajectory synthesis, achieving superior generalization and performance in diverse tasks.",
      "debug_abstract": "The vision-language-action (VLA) paradigm has enabled powerful robotic control by leveraging vision-language models, but its reliance on large-scale, high-quality robot data limits its generalization. Generative world models offer a promising alternative for general-purpose embodied AI, yet a critical gap remains between their pixel-level plans and physically executable actions. To this end, we propose the Tool-Centric Inverse Dynamics Model (TC-IDM). By focusing on the tool&#39;s imagined trajectory as synthesized by the world model, TC-IDM establishes a robust intermediate representation that bridges the gap between visual planning and physical control. TC-IDM extracts the tool&#39;s point cloud trajectories via segmentation and 3D motion estimation from generated videos. Considering diverse tool attributes, our architecture employs decoupled action heads to project these planned trajectories into 6-DoF end-effector motions and corresponding control signals. This plan-and-translate paradigm not only supports a wide range of end-effectors but also significantly improves viewpoint invariance. Furthermore, it exhibits strong generalization capabilities across long-horizon and out-of-distribution tasks, including interacting with deformable objects. In real-world evaluations, the world model with TC-IDM achieves an average success rate of 61.11 percent, with 77.7 percent on simple tasks and 38.46 percent on zero-shot deformable object tasks. It substantially outperforms end-to-end VLA-style baselines and other inverse dynamics models."
    },
    {
      "title": "Fast and Safe Trajectory Optimization for Mobile Manipulators With Neural Configuration Space Distance Field",
      "url": "https://arxiv.org/abs/2601.18548",
      "date": "2026-01-26",
      "authors_text": "Yulin Li, Zhiyuan Song, Yiming Li, Zhicheng Song, Kai Chen",
      "is_highlight": false,
      "score": 86.0,
      "summary": "This paper introduces Generalized Configuration Space Distance Fields (GCDF) for efficient trajectory optimization in mobile manipulators, enhancing collision reasoning and enabling agile motion in complex environments.",
      "debug_abstract": "Mobile manipulators promise agile, long-horizon behavior by coordinating base and arm motion, yet whole-body trajectory optimization in cluttered, confined spaces remains difficult due to high-dimensional nonconvexity and the need for fast, accurate collision reasoning. Configuration Space Distance Fields (CDF) enable fixed-base manipulators to model collisions directly in configuration space via smooth, implicit distances. This representation holds strong potential to bypass the nonlinear configuration-to-workspace mapping while preserving accurate whole-body geometry and providing optimization-friendly collision costs. Yet, extending this capability to mobile manipulators is hindered by unbounded workspaces and tighter base-arm coupling. We lift this promise to mobile manipulation with Generalized Configuration Space Distance Fields (GCDF), extending CDF to robots with both translational and rotational joints in unbounded workspaces with tighter base-arm coupling. We prove that GCDF preserves Euclidean-like local distance structure and accurately encodes whole-body geometry in configuration space, and develop a data generation and training pipeline that yields continuous neural GCDFs with accurate values and gradients, supporting efficient GPU-batched queries. Building on this representation, we develop a high-performance sequential convex optimization framework centered on GCDF-based collision reasoning. The solver scales to large numbers of implicit constraints through (i) online specification of neural constraints, (ii) sparsity-aware active-set detection with parallel batched evaluation across thousands of constraints, and (iii) incremental constraint management for rapid replanning under scene changes."
    },
    {
      "title": "EvoCUA: Evolving Computer Use Agents via Learning from Scalable Synthetic Experience",
      "url": "https://arxiv.org/abs/2601.15876",
      "date": "2026-01-22",
      "authors_text": "Taofeng Xue, Chong Peng, Mianqiu Huang, Linsen Guo, Tiancheng Han",
      "is_highlight": false,
      "score": 82.6,
      "summary": "EvoCUA introduces a self-sustaining evolutionary model for computer-use agents, leveraging autonomous task generation and iterative learning to significantly outperform existing benchmarks in multimodal AI.",
      "debug_abstract": "The development of native computer-use agents (CUA) represents a significant leap in multimodal AI. However, their potential is currently bottlenecked by the constraints of static data scaling. Existing paradigms relying primarily on passive imitation of static datasets struggle to capture the intricate causal dynamics inherent in long-horizon computer tasks. In this work, we introduce EvoCUA, a native computer use agentic model. Unlike static imitation, EvoCUA integrates data generation and policy optimization into a self-sustaining evolutionary cycle. To mitigate data scarcity, we develop a verifiable synthesis engine that autonomously generates diverse tasks coupled with executable validators. To enable large-scale experience acquisition, we design a scalable infrastructure orchestrating tens of thousands of asynchronous sandbox rollouts. Building on these massive trajectories, we propose an iterative evolving learning strategy to efficiently internalize this experience. This mechanism dynamically regulates policy updates by identifying capability boundaries -- reinforcing successful routines while transforming failure trajectories into rich supervision through error analysis and self-correction. Empirical evaluations on the OSWorld benchmark demonstrate that EvoCUA achieves a success rate of 56.7%, establishing a new open-source state-of-the-art. Notably, EvoCUA significantly outperforms the previous best open-source model, OpenCUA-72B (45.0%), and surpasses leading closed-weights models such as UI-TARS-2 (53.1%). Crucially, our results underscore the generalizability of this approach: the evolving paradigm driven by learning from experience yields consistent performance gains across foundation models of varying scales, establishing a robust and scalable path for advancing native agent capabilities."
    },
    {
      "title": "DualShield: Safe Model Predictive Diffusion via Reachability Analysis for Interactive Autonomous Driving",
      "url": "https://arxiv.org/abs/2601.15729",
      "date": "2026-01-22",
      "authors_text": "Rui Yang, Lei Zheng, Ruoyu Yao, Jun Ma",
      "is_highlight": true,
      "score": 92.7,
      "summary": "DualShield enhances autonomous driving safety by integrating Hamilton-Jacobi reachability analysis into diffusion models for proactive guidance and reactive safety assurance under uncertain interactions.",
      "debug_abstract": "Diffusion models have emerged as a powerful approach for multimodal motion planning in autonomous driving. However, their practical deployment is typically hindered by the inherent difficulty in enforcing vehicle dynamics and a critical reliance on accurate predictions of other agents, making them prone to safety issues under uncertain interactions. To address these limitations, we introduce DualShield, a planning and control framework that leverages Hamilton-Jacobi (HJ) reachability value functions in a dual capacity. First, the value functions act as proactive guidance, steering the diffusion denoising process towards safe and dynamically feasible regions. Second, they form a reactive safety shield using control barrier-value functions (CBVFs) to modify the executed actions and ensure safety. This dual mechanism preserves the rich exploration capabilities of diffusion models while providing principled safety assurance under uncertain and even adversarial interactions. Simulations in challenging unprotected U-turn scenarios demonstrate that DualShield significantly improves both safety and task efficiency compared to leading methods from different planning paradigms under uncertainty."
    }
  ],
  "Jun MA老师实验室": [
    {
      "title": "Visual Reasoning over Time Series via Multi-Agent System",
      "url": "https://arxiv.org/abs/2602.03026",
      "date": "2026-02-03",
      "authors_text": "Weilin Ruan, Yuxuan Liang",
      "is_highlight": false,
      "score": 66.0,
      "summary": "MAS4TS is a multi-agent system that enhances time series analysis through visual reasoning and adaptive tool usage, achieving state-of-the-art performance across diverse tasks.",
      "teaser_image": "https://arxiv.org/html/2602.03026/x2.png",
      "debug_abstract": "Time series analysis underpins many real-world applications, yet existing time-series-specific methods and pretrained large-model-based approaches remain limited in integrating intuitive visual reasoning and generalizing across tasks with adaptive tool usage. To address these limitations, we propose MAS4TS, a tool-driven multi-agent system for general time series tasks, built upon an Analyzer-Reasoner-Executor paradigm that integrates agent communication, visual reasoning, and latent reconstruction within a unified framework. MAS4TS first performs visual reasoning over time series plots with structured priors using a Vision-Language Model to extract temporal structures, and subsequently reconstructs predictive trajectories in latent space. Three specialized agents coordinate via shared memory and gated communication, while a router selects task-specific tool chains for execution. Extensive experiments on multiple benchmarks demonstrate that MAS4TS achieves state-of-the-art performance across a wide range of time series tasks, while exhibiting strong generalization and efficient inference."
    },
    {
      "title": "Neural Predictor-Corrector: Solving Homotopy Problems with Reinforcement Learning",
      "url": "https://arxiv.org/abs/2602.03086",
      "date": "2026-02-03",
      "authors_text": "Jiayao Mai, Bangyan Liao, Zhenjun Zhao, Yingping Zeng, Haoang Li",
      "is_highlight": false,
      "score": 64.0,
      "summary": "The Neural Predictor-Corrector (NPC) framework uses reinforcement learning to automate step size and termination decisions in solving diverse homotopy problems, outperforming traditional methods.",
      "teaser_image": "https://arxiv.org/html/2602.03086/x2.png",
      "debug_abstract": "The Homotopy paradigm, a general principle for solving challenging problems, appears across diverse domains such as robust optimization, global optimization, polynomial root-finding, and sampling. Practical solvers for these problems typically follow a predictor-corrector (PC) structure, but rely on hand-crafted heuristics for step sizes and iteration termination, which are often suboptimal and task-specific. To address this, we unify these problems under a single framework, which enables the design of a general neural solver. Building on this unified view, we propose Neural Predictor-Corrector (NPC), which replaces hand-crafted heuristics with automatically learned policies. NPC formulates policy selection as a sequential decision-making problem and leverages reinforcement learning to automatically discover efficient strategies. To further enhance generalization, we introduce an amortized training mechanism, enabling one-time offline training for a class of problems and efficient online inference on new instances. Experiments on four representative homotopy problems demonstrate that our method generalizes effectively to unseen instances. It consistently outperforms classical and specialized baselines in efficiency while demonstrating superior stability across tasks, highlighting the value of unifying homotopy methods into a single neural framework."
    },
    {
      "title": "Diffusion-Driven Inter-Outer Surface Separation for Point Clouds with Open Boundaries",
      "url": "https://arxiv.org/abs/2602.00739",
      "date": "2026-01-31",
      "authors_text": "Zhengyan Qin, Liyuan Qiu",
      "is_highlight": false,
      "score": 72.0,
      "summary": "The paper presents a diffusion-based algorithm for effectively separating inter and outer surfaces in double-layered point clouds with open boundaries, addressing artifacts from TSDF fusion.",
      "teaser_image": "https://arxiv.org/html/2602.00739/figures/pipeline5.png",
      "debug_abstract": "We propose a diffusion-based algorithm for separating the inter and outer layer surfaces from double-layered point clouds, particularly those exhibiting the &#34;double surface artifact&#34; caused by truncation in Truncated Signed Distance Function (TSDF) fusion during indoor or medical 3D reconstruction. This artifact arises from asymmetric truncation thresholds, leading to erroneous inter and outer shells in the fused volume, which our method addresses by extracting the true inter layer to mitigate challenges like overlapping surfaces and disordered normals. We focus on point clouds with \\emph{open boundaries} (i.e., sampled surfaces with topological openings/holes through which particles may escape), rather than point clouds with \\emph{missing surface regions} where no samples exist. Our approach enables robust processing of both watertight and open-boundary models, achieving extraction of the inter layer from 20,000 inter and 20,000 outer points in approximately 10 seconds. This solution is particularly effective for applications requiring accurate surface representations, such as indoor scene modeling and medical imaging, where double-layered point clouds are prevalent, and it accommodates both closed (watertight) and open-boundary surface geometries. Our goal is \\emph{post-hoc} inter/outer shell separation as a lightweight module after TSDF fusion; we do not aim to replace full variational or learning-based reconstruction pipelines."
    },
    {
      "title": "Physics-informed Diffusion Mamba Transformer for Real-world Driving",
      "url": "https://arxiv.org/abs/2602.00808",
      "date": "2026-01-31",
      "authors_text": "Hang Zhou, Qiang Zhang, Peiran Liu, Yihao Qin, Zhaoxu Yan",
      "is_highlight": false,
      "score": 74.0,
      "summary": "The paper presents a Diffusion Mamba Transformer that integrates sequential context and physical constraints for improved trajectory planning in autonomous driving, outperforming existing models.",
      "teaser_image": "https://arxiv.org/html/2602.00808/difference_pipeline3.png",
      "debug_abstract": "Autonomous driving systems demand trajectory planners that not only model the inherent uncertainty of future motions but also respect complex temporal dependencies and underlying physical laws. While diffusion-based generative models excel at capturing multi-modal distributions, they often fail to incorporate long-term sequential contexts and domain-specific physical priors. In this work, we bridge these gaps with two key innovations. First, we introduce a Diffusion Mamba Transformer architecture that embeds mamba and attention into the diffusion process, enabling more effective aggregation of sequential input contexts from sensor streams and past motion histories. Second, we design a Port-Hamiltonian Neural Network module that seamlessly integrates energy-based physical constraints into the diffusion model, thereby enhancing trajectory predictions with both consistency and interpretability. Extensive evaluations on standard autonomous driving benchmarks demonstrate that our unified framework significantly outperforms state-of-the-art baselines in predictive accuracy, physical plausibility, and robustness, thereby advancing safe and reliable motion planning."
    },
    {
      "title": "Invariance on Manifolds: Understanding Robust Visual Representations for Place Recognition",
      "url": "https://arxiv.org/abs/2602.00841",
      "date": "2026-01-31",
      "authors_text": "Jintao Cheng, Weibin Li, Zhijian He, Jin Wu, Chi Man Vong",
      "is_highlight": false,
      "score": 66.0,
      "summary": "This paper presents a training-free Second-Order Geometric Statistics framework for robust visual place recognition, utilizing covariance descriptors on the SPD manifold for effective zero-shot generalization.",
      "teaser_image": "https://arxiv.org/html/2602.00841/x2.png",
      "debug_abstract": "Visual Place Recognition (VPR) demands representations robust to drastic environmental and viewpoint shifts. Current aggregation paradigms, however, either rely on data-hungry supervision or simplistic first-order statistics, often neglecting intrinsic structural correlations. In this work, we propose a Second-Order Geometric Statistics framework that inherently captures geometric stability without training. We conceptualize scenes as covariance descriptors on the Symmetric Positive Definite (SPD) manifold, where perturbations manifest as tractable congruence transformations. By leveraging geometry-aware Riemannian mappings, we project these descriptors into a linearized Euclidean embedding, effectively decoupling signal structure from noise. Our approach introduces a training-free framework built upon fixed, pre-trained backbones, achieving strong zero-shot generalization without parameter updates. Extensive experiments confirm that our method achieves highly competitive performance against state-of-the-art baselines, particularly excelling in challenging zero-shot scenarios."
    },
    {
      "title": "PolicyFlow: Policy Optimization with Continuous Normalizing Flow in Reinforcement Learning",
      "url": "https://arxiv.org/abs/2602.01156",
      "date": "2026-02-01",
      "authors_text": "Shunpeng Yang, Ben Liu, Hua Chen",
      "is_highlight": false,
      "score": 71.0,
      "summary": "PolicyFlow is a novel reinforcement learning algorithm that integrates continuous normalizing flows with PPO, enhancing performance and stability while reducing computational costs and encouraging diverse behaviors.",
      "teaser_image": "https://arxiv.org/html/2602.01156/x1.png",
      "debug_abstract": "Among on-policy reinforcement learning algorithms, Proximal Policy Optimization (PPO) demonstrates is widely favored for its simplicity, numerical stability, and strong empirical performance. Standard PPO relies on surrogate objectives defined via importance ratios, which require evaluating policy likelihood that is typically straightforward when the policy is modeled as a Gaussian distribution. However, extending PPO to more expressive, high-capacity policy models such as continuous normalizing flows (CNFs), also known as flow-matching models, is challenging because likelihood evaluation along the full flow trajectory is computationally expensive and often numerically unstable. To resolve this issue, we propose PolicyFlow, a novel on-policy CNF-based reinforcement learning algorithm that integrates expressive CNF policies with PPO-style objectives without requiring likelihood evaluation along the full flow path. PolicyFlow approximates importance ratios using velocity field variations along a simple interpolation path, reducing computational overhead without compromising training stability. To further prevent mode collapse and further encourage diverse behaviors, we propose the Brownian Regularizer, an implicit policy entropy regularizer inspired by Brownian motion, which is conceptually elegant and computationally lightweight. Experiments on diverse tasks across various environments including MultiGoal, PointMaze, IsaacLab and MuJoCo Playground show that PolicyFlow achieves competitive or superior performance compared to PPO using Gaussian policies and flow-based baselines including FPO and DPPO. Notably, results on MultiGoal highlight PolicyFlow&#39;s ability to capture richer multimodal action distributions."
    },
    {
      "title": "Do All Individual Layers Help? An Empirical Study of Task-Interfering Layers in Vision-Language Models",
      "url": "https://arxiv.org/abs/2602.01167",
      "date": "2026-02-01",
      "authors_text": "Zhiming Liu, Yujie Wei, Lei Feng, Xiu Su, Xiaobo Xia",
      "is_highlight": false,
      "score": 52.0,
      "summary": "This study identifies Task-Interfering Layers in vision-language models, demonstrating that selectively bypassing certain layers can enhance task performance without retraining.",
      "teaser_image": "https://arxiv.org/html/2602.01167/x1.png",
      "debug_abstract": "Current VLMs have demonstrated capabilities across a wide range of multimodal tasks. Typically, in a pretrained VLM, all layers are engaged by default to make predictions on downstream tasks. We find that intervening on a single layer, such as by zeroing its parameters, can improve the performance on certain tasks, indicating that some layers hinder rather than help downstream tasks. We systematically investigate how individual layers influence different tasks via layer intervention. Specifically, we measure the change in performance relative to the base model after intervening on each layer and observe improvements when bypassing specific layers. This improvement can be generalizable across models and datasets, indicating the presence of Task-Interfering Layers that harm downstream tasks&#39; performance. We introduce Task-Layer Interaction Vector, which quantifies the effect of intervening on each layer of a VLM given a task. These task-interfering layers exhibit task-specific sensitivity patterns: tasks requiring similar capabilities show consistent response trends under layer interventions, as evidenced by the high similarity in their task-layer interaction vectors. Inspired by these findings, we propose TaLo (Task-Adaptive Layer Knockout), a training-free, test-time adaptation method that dynamically identifies and bypasses the most interfering layer for a given task. Without parameter updates, TaLo improves performance across various models and datasets, including boosting Qwen-VL&#39;s accuracy on the Maps task in ScienceQA by up to 16.6%. Our work reveals an unexpected form of modularity in pretrained VLMs and provides a plug-and-play, training-free mechanism to unlock hidden capabilities at inference time. The source code will be publicly available."
    },
    {
      "title": "Towards Exploratory and Focused Manipulation with Bimanual Active Perception: A New Problem, Benchmark and Strategy",
      "url": "https://arxiv.org/abs/2602.01939",
      "date": "2026-02-02",
      "authors_text": "Yuxin He, Ruihao Zhang, Tianao Shen, Cheng Liu, Qiang Nie",
      "is_highlight": false,
      "score": 28.0,
      "summary": "The paper introduces the Exploratory and Focused Manipulation (EFM) problem, establishes the EFM-10 benchmark, and proposes a Bimanual Active Perception strategy to enhance manipulation tasks.",
      "teaser_image": "https://arxiv.org/html/2602.01939/x2.png",
      "debug_abstract": "Recently, active vision has reemerged as an important concept for manipulation, since visual occlusion occurs more frequently when main cameras are mounted on the robot heads. We reflect on the visual occlusion issue and identify its essence as the absence of information useful for task completion. Inspired by this, we come up with the more fundamental problem of Exploratory and Focused Manipulation (EFM). The proposed problem is about actively collecting information to complete challenging manipulation tasks that require exploration or focus. As an initial attempt to address this problem, we establish the EFM-10 benchmark that consists of 4 categories of tasks that align with our definition (10 tasks in total). We further come up with a Bimanual Active Perception (BAP) strategy, which leverages one arm to provide active vision and another arm to provide force sensing while manipulating. Based on this idea, we collect a dataset named BAPData for the tasks in EFM-10. With the dataset, we successfully verify the effectiveness of the BAP strategy in an imitation learning manner. We hope that the EFM-10 benchmark along with the BAP strategy can become a cornerstone that facilitates future research towards this direction. Project website: this http URL."
    },
    {
      "title": "Breaking the Static Graph: Context-Aware Traversal for Robust Retrieval-Augmented Generation",
      "url": "https://arxiv.org/abs/2602.01965",
      "date": "2026-02-02",
      "authors_text": "Kwun Hang Lau, Fangyuan Zhang, Boyu Ruan, Yingli Zhou, Qintian Guo",
      "is_highlight": false,
      "score": 27.0,
      "summary": "CatRAG enhances Retrieval-Augmented Generation by transforming static Knowledge Graphs into query-adaptive structures, improving multi-hop evidence retrieval and reasoning completeness.",
      "teaser_image": "https://arxiv.org/html/2602.01965/image/HubBiasShift.png",
      "debug_abstract": "Recent advances in Retrieval-Augmented Generation (RAG) have shifted from simple vector similarity to structure-aware approaches like HippoRAG, which leverage Knowledge Graphs (KGs) and Personalized PageRank (PPR) to capture multi-hop dependencies. However, these methods suffer from a &#34;Static Graph Fallacy&#34;: they rely on fixed transition probabilities determined during indexing. This rigidity ignores the query-dependent nature of edge relevance, causing semantic drift where random walks are diverted into high-degree &#34;hub&#34; nodes before reaching critical downstream evidence. Consequently, models often achieve high partial recall but fail to retrieve the complete evidence chain required for multi-hop queries. To address this, we propose CatRAG, Context-Aware Traversal for robust RAG, a framework that builds on the HippoRAG 2 architecture and transforms the static KG into a query-adaptive navigation structure. We introduce a multi-faceted framework to steer the random walk: (1) Symbolic Anchoring, which injects weak entity constraints to regularize the random walk; (2) Query-Aware Dynamic Edge Weighting, which dynamically modulates graph structure, to prune irrelevant paths while amplifying those aligned with the query&#39;s intent; and (3) Key-Fact Passage Weight Enhancement, a cost-efficient bias that structurally anchors the random walk to likely evidence. Experiments across four multi-hop benchmarks demonstrate that CatRAG consistently outperforms state of the art baselines. Our analysis reveals that while standard Recall metrics show modest gains, CatRAG achieves substantial improvements in reasoning completeness, the capacity to recover the entire evidence path without gaps. These results reveal that our approach effectively bridges the gap between retrieving partial context and enabling fully grounded reasoning. Resources are available at this https URL."
    },
    {
      "title": "Towards Agentic Intelligence for Materials Science",
      "url": "https://arxiv.org/abs/2602.00169",
      "date": "2026-01-29",
      "authors_text": "Huan Zhang, Yizhan Li, Wenhao Huang, Ziyu Hou, Yu Song",
      "is_highlight": false,
      "score": 12.0,
      "summary": "This paper advocates for developing agentic AI systems in materials science that autonomously plan, act, and learn to enhance discovery processes beyond traditional models.",
      "teaser_image": "https://arxiv.org/html/2602.00169/x1.png",
      "debug_abstract": "The convergence of artificial intelligence and materials science presents a transformative opportunity, but achieving true acceleration in discovery requires moving beyond task-isolated, fine-tuned models toward agentic systems that plan, act, and learn across the full discovery loop. This survey advances a unique pipeline-centric view that spans from corpus curation and pretraining, through domain adaptation and instruction tuning, to goal-conditioned agents interfacing with simulation and experimental platforms. Unlike prior reviews, we treat the entire process as an end-to-end system to be optimized for tangible discovery outcomes rather than proxy benchmarks. This perspective allows us to trace how upstream design choices-such as data curation and training objectives-can be aligned with downstream experimental success through effective credit assignment. To bridge communities and establish a shared frame of reference, we first present an integrated lens that aligns terminology, evaluation, and workflow stages across AI and materials science. We then analyze the field through two focused lenses: From the AI perspective, the survey details LLM strengths in pattern recognition, predictive analytics, and natural language processing for literature mining, materials characterization, and property prediction; from the materials science perspective, it highlights applications in materials design, process optimization, and the acceleration of computational workflows via integration with external tools (e.g., DFT, robotic labs). Finally, we contrast passive, reactive approaches with agentic design, cataloging current contributions while motivating systems that pursue long-horizon goals with autonomy, memory, and tool use. This survey charts a practical roadmap towards autonomous, safety-aware LLM agents aimed at discovering novel and useful materials."
    },
    {
      "title": "Advances and Innovations in the Multi-Agent Robotic System (MARS) Challenge",
      "url": "https://arxiv.org/abs/2601.18733",
      "date": "2026-01-26",
      "authors_text": "Li Kang, Heng Zhou, Xiufeng Song, Rui Li, Bruno N. Y. Chen",
      "is_highlight": false,
      "score": 78.0,
      "summary": "The MARS Challenge promotes advancements in multi-agent robotic systems by exploring vision-language models for collaborative task execution and planning in dynamic environments.",
      "teaser_image": null,
      "debug_abstract": "Recent advancements in multimodal large language models and vision-languageaction models have significantly driven progress in Embodied AI. As the field transitions toward more complex task scenarios, multi-agent system frameworks are becoming essential for achieving scalable, efficient, and collaborative solutions. This shift is fueled by three primary factors: increasing agent capabilities, enhancing system efficiency through task delegation, and enabling advanced human-agent interactions. To address the challenges posed by multi-agent collaboration, we propose the Multi-Agent Robotic System (MARS) Challenge, held at the NeurIPS 2025 Workshop on SpaVLE. The competition focuses on two critical areas: planning and control, where participants explore multi-agent embodied planning using vision-language models (VLMs) to coordinate tasks and policy execution to perform robotic manipulation in dynamic environments. By evaluating solutions submitted by participants, the challenge provides valuable insights into the design and coordination of embodied multi-agent systems, contributing to the future development of advanced collaborative AI systems."
    },
    {
      "title": "Gaussian Belief Propagation Network for Depth Completion",
      "url": "https://arxiv.org/abs/2601.21291",
      "date": "2026-01-29",
      "authors_text": "Jie Tang, Pingping Xie, Jian Li, Ping Tan",
      "is_highlight": false,
      "score": 68.0,
      "summary": "The Gaussian Belief Propagation Network (GBPN) integrates deep learning with probabilistic models for effective depth completion from sparse measurements, achieving state-of-the-art results across various benchmarks.",
      "teaser_image": "https://arxiv.org/html/2601.21291/x1.png",
      "debug_abstract": "Depth completion aims to predict a dense depth map from a color image with sparse depth measurements. Although deep learning methods have achieved state-of-the-art (SOTA), effectively handling the sparse and irregular nature of input depth data in deep networks remains a significant challenge, often limiting performance, especially under high sparsity. To overcome this limitation, we introduce the Gaussian Belief Propagation Network (GBPN), a novel hybrid framework synergistically integrating deep learning with probabilistic graphical models for end-to-end depth completion. Specifically, a scene-specific Markov Random Field (MRF) is dynamically constructed by the Graphical Model Construction Network (GMCN), and then inferred via Gaussian Belief Propagation (GBP) to yield the dense depth distribution. Crucially, the GMCN learns to construct not only the data-dependent potentials of MRF but also its structure by predicting adaptive non-local edges, enabling the capture of complex, long-range spatial dependencies. Furthermore, we enhance GBP with a serial \\&amp; parallel message passing scheme, designed for effective information propagation, particularly from sparse measurements. Extensive experiments demonstrate that GBPN achieves SOTA performance on the NYUv2 and KITTI benchmarks. Evaluations across varying sparsity levels, sparsity patterns, and datasets highlight GBPN&#39;s superior performance, notable robustness, and generalizable capability."
    },
    {
      "title": "4D-CAAL: 4D Radar-Camera Calibration and Auto-Labeling for Autonomous Driving",
      "url": "https://arxiv.org/abs/2601.21454",
      "date": "2026-01-29",
      "authors_text": "Shanliang Yao, Zhuoxiao Li, Runwei Guan, Kebin Cao, Meng Xia",
      "is_highlight": false,
      "score": 89.0,
      "summary": "The 4D-CAAL framework integrates 4D radar-camera calibration and auto-labeling using a dual-purpose target and robust correspondence matching, enhancing autonomous driving perception efficiency.",
      "teaser_image": "https://arxiv.org/html/2601.21454/images/4DRC-fixed.png",
      "debug_abstract": "4D radar has emerged as a critical sensor for autonomous driving, primarily due to its enhanced capabilities in elevation measurement and higher resolution compared to traditional 3D radar. Effective integration of 4D radar with cameras requires accurate extrinsic calibration, and the development of radar-based perception algorithms demands large-scale annotated datasets. However, existing calibration methods often employ separate targets optimized for either visual or radar modalities, complicating correspondence establishment. Furthermore, manually labeling sparse radar data is labor-intensive and unreliable. To address these challenges, we propose 4D-CAAL, a unified framework for 4D radar-camera calibration and auto-labeling. Our approach introduces a novel dual-purpose calibration target design, integrating a checkerboard pattern on the front surface for camera detection and a corner reflector at the center of the back surface for radar detection. We develop a robust correspondence matching algorithm that aligns the checkerboard center with the strongest radar reflection point, enabling accurate extrinsic calibration. Subsequently, we present an auto-labeling pipeline that leverages the calibrated sensor relationship to transfer annotations from camera-based segmentations to radar point clouds through geometric projection and multi-feature optimization. Extensive experiments demonstrate that our method achieves high calibration accuracy while significantly reducing manual annotation effort, thereby accelerating the development of robust multi-modal perception systems for autonomous driving."
    },
    {
      "title": "DynaWeb: Model-Based Reinforcement Learning of Web Agents",
      "url": "https://arxiv.org/abs/2601.22149",
      "date": "2026-01-29",
      "authors_text": "Hang Ding, Peidong Liu, Junqiao Wang, Ziwei Ji, Meng Cao",
      "is_highlight": false,
      "score": 77.0,
      "summary": "DynaWeb introduces a model-based reinforcement learning framework that enhances web agent training by simulating interactions with a web world model, improving efficiency and performance.",
      "teaser_image": "https://arxiv.org/html/2601.22149/figures/dynaweb.png",
      "debug_abstract": "The development of autonomous web agents, powered by Large Language Models (LLMs) and reinforcement learning (RL), represents a significant step towards general-purpose AI assistants. However, training these agents is severely hampered by the challenges of interacting with the live internet, which is inefficient, costly, and fraught with risks. Model-based reinforcement learning (MBRL) offers a promising solution by learning a world model of the environment to enable simulated interaction. This paper introduces DynaWeb, a novel MBRL framework that trains web agents through interacting with a web world model trained to predict naturalistic web page representations given agent actions. This model serves as a synthetic web environment where an agent policy can dream by generating vast quantities of rollout action trajectories for efficient online reinforcement learning. Beyond free policy rollouts, DynaWeb incorporates real expert trajectories from training data, which are randomly interleaved with on-policy rollouts during training to improve stability and sample efficiency. Experiments conducted on the challenging WebArena and WebVoyager benchmarks demonstrate that DynaWeb consistently and significantly improves the performance of state-of-the-art open-source web agent models. Our findings establish the viability of training web agents through imagination, offering a scalable and efficient way to scale up online agentic RL."
    },
    {
      "title": "Endogenous Reprompting: Self-Evolving Cognitive Alignment for Unified Multimodal Models",
      "url": "https://arxiv.org/abs/2601.20305",
      "date": "2026-01-28",
      "authors_text": "Zhenchen Tang, Songlin Yang, Zichuan Wang, Bo Peng, Yang Li",
      "is_highlight": false,
      "score": 69.0,
      "summary": "The paper introduces Endogenous Reprompting and the SEER framework, enhancing Unified Multimodal Models' generative reasoning through self-aligned descriptors and reinforcement learning, outperforming existing methods.",
      "teaser_image": "https://arxiv.org/html/2601.20305/x2.png",
      "debug_abstract": "Unified Multimodal Models (UMMs) exhibit strong understanding, yet this capability often fails to effectively guide generation. We identify this as a Cognitive Gap: the model lacks the understanding of how to enhance its own generation process. To bridge this gap, we propose Endogenous Reprompting, a mechanism that transforms the model&#39;s understanding from a passive encoding process into an explicit generative reasoning step by generating self-aligned descriptors during generation. To achieve this, we introduce SEER (Self-Evolving Evaluator and Reprompter), a training framework that establishes a two-stage endogenous loop using only 300 samples from a compact proxy task, Visual Instruction Elaboration. First, Reinforcement Learning with Verifiable Rewards (RLVR) activates the model&#39;s latent evaluation ability via curriculum learning, producing a high-fidelity endogenous reward signal. Second, Reinforcement Learning with Model-rewarded Thinking (RLMT) leverages this signal to optimize the generative reasoning policy. Experiments show that SEER consistently outperforms state-of-the-art baselines in evaluation accuracy, reprompting efficiency, and generation quality, without sacrificing general multimodal capabilities."
    },
    {
      "title": "Glance and Focus Reinforcement for Pan-cancer Screening",
      "url": "https://arxiv.org/abs/2601.19103",
      "date": "2026-01-27",
      "authors_text": "Linshan Wu, Jiaxin Zhuang, Hao Chen",
      "is_highlight": false,
      "score": 65.0,
      "summary": "GF-Screen introduces a novel reinforcement learning framework that enhances pan-cancer screening by effectively localizing and segmenting lesions in CT scans, outperforming existing methods.",
      "debug_abstract": "Pan-cancer screening in large-scale CT scans remains challenging for existing AI methods, primarily due to the difficulty of localizing diverse types of tiny lesions in large CT volumes. The extreme foreground-background imbalance significantly hinders models from focusing on diseased regions, while redundant focus on healthy regions not only decreases the efficiency but also increases false positives. Inspired by radiologists&#39; glance and focus diagnostic strategy, we introduce GF-Screen, a Glance and Focus reinforcement learning framework for pan-cancer screening. GF-Screen employs a Glance model to localize the diseased regions and a Focus model to precisely segment the lesions, where segmentation results of the Focus model are leveraged to reward the Glance model via Reinforcement Learning (RL). Specifically, the Glance model crops a group of sub-volumes from the entire CT volume and learns to select the sub-volumes with lesions for the Focus model to segment. Given that the selecting operation is non-differentiable for segmentation training, we propose to employ the segmentation results to reward the Glance model. To optimize the Glance model, we introduce a novel group relative learning paradigm, which employs group relative comparison to prioritize high-advantage predictions and discard low-advantage predictions within sub-volume groups, not only improving efficiency but also reducing false positives. In this way, for the first time, we effectively extend cutting-edge RL techniques to tackle the specific challenges in pan-cancer screening. Extensive experiments on 16 internal and 7 external datasets across 9 lesion types demonstrated the effectiveness of GF-Screen. Notably, GF-Screen leads the public validation leaderboard of MICCAI FLARE25 pan-cancer challenge, surpassing the FLARE24 champion solution by a large margin (+25.6% DSC and +28.2% NSD)."
    },
    {
      "title": "Enhancing Inverse Perspective Mapping for Automatic Vectorized Road Map Generation",
      "url": "https://arxiv.org/abs/2601.19536",
      "date": "2026-01-27",
      "authors_text": "Hongji Liu, Linwei Zheng, Yongjian Li, Mingkai Tang, Xiaoyang Yan",
      "is_highlight": false,
      "score": 70.0,
      "summary": "This study introduces a cost-effective framework using enhanced inverse perspective mapping and Catmull-Rom splines for high-precision, automatic vectorized road map generation.",
      "debug_abstract": "In this study, we present a low-cost and unified framework for vectorized road mapping leveraging enhanced inverse perspective mapping (IPM). In this framework, Catmull-Rom splines are utilized to characterize lane lines, and all the other ground markings are depicted using polygons uniformly. The results from instance segmentation serve as references to refine the three-dimensional position of spline control points and polygon corner points. In conjunction with this process, the homography matrix of IPM and vehicle poses are optimized simultaneously. Our proposed framework significantly reduces the mapping errors associated with IPM. It also improves the accuracy of the initial IPM homography matrix and the predicted vehicle poses. Furthermore, it addresses the limitations imposed by the coplanarity assumption in IPM. These enhancements enable IPM to be effectively applied to vectorized road mapping, which serves a cost-effective solution with enhanced accuracy. In addition, our framework generalizes road map elements to include all common ground markings and lane lines. The proposed framework is evaluated in two different practical scenarios, and the test results show that our method can automatically generate high-precision maps with near-centimeter-level accuracy. Importantly, the optimized IPM matrix achieves an accuracy comparable to that of manual calibration, while the accuracy of vehicle poses is also significantly improved."
    },
    {
      "title": "Geometry-Grounded Gaussian Splatting",
      "url": "https://arxiv.org/abs/2601.17835",
      "date": "2026-01-25",
      "authors_text": "Baowen Zhang, Chenxing Jiang, Heng Li, Shaojie Shen, Ping Tan",
      "is_highlight": true,
      "score": 53.0,
      "summary": "This paper introduces Geometry-Grounded Gaussian Splatting, enhancing shape extraction from Gaussian primitives by treating them as stochastic solids for improved depth map rendering and reconstruction quality.",
      "debug_abstract": "Gaussian Splatting (GS) has demonstrated impressive quality and efficiency in novel view synthesis. However, shape extraction from Gaussian primitives remains an open problem. Due to inadequate geometry parameterization and approximation, existing shape reconstruction methods suffer from poor multi-view consistency and are sensitive to floaters. In this paper, we present a rigorous theoretical derivation that establishes Gaussian primitives as a specific type of stochastic solids. This theoretical framework provides a principled foundation for Geometry-Grounded Gaussian Splatting by enabling the direct treatment of Gaussian primitives as explicit geometric representations. Using the volumetric nature of stochastic solids, our method efficiently renders high-quality depth maps for fine-grained geometry extraction. Experiments show that our method achieves the best shape reconstruction results among all Gaussian Splatting-based methods on public datasets."
    },
    {
      "title": "CoT-Seg: Rethinking Segmentation with Chain-of-Thought Reasoning and Self-Correction",
      "url": "https://arxiv.org/abs/2601.17420",
      "date": "2026-01-24",
      "authors_text": "Shiu-hong Kao, Chak Ho Huang, Huaiqian Liu, Yu-Wing Tai, Chi-Keung Tang",
      "is_highlight": false,
      "score": 68.0,
      "summary": "CoT-Seg introduces a training-free framework that enhances segmentation through chain-of-thought reasoning and self-correction, improving performance on complex queries and ambiguous images.",
      "debug_abstract": "Existing works of reasoning segmentation often fall short in complex cases, particularly when addressing complicated queries and out-of-domain images. Inspired by the chain-of-thought reasoning, where harder problems require longer thinking steps/time, this paper aims to explore a system that can think step-by-step, look up information if needed, generate results, self-evaluate its own results, and refine the results, in the same way humans approach harder questions. We introduce CoT-Seg, a training-free framework that rethinks reasoning segmentation by combining chain-of-thought reasoning with self-correction. Instead of fine-tuning, CoT-Seg leverages the inherent reasoning ability of pre-trained MLLMs (GPT-4o) to decompose queries into meta-instructions, extract fine-grained semantics from images, and identify target objects even under implicit or complex prompts. Moreover, CoT-Seg incorporates a self-correction stage: the model evaluates its own segmentation against the original query and reasoning trace, identifies mismatches, and iteratively refines the mask. This tight integration of reasoning and correction significantly improves reliability and robustness, especially in ambiguous or error-prone cases. Furthermore, our CoT-Seg framework allows easy incorporation of retrieval-augmented reasoning, enabling the system to access external knowledge when the input lacks sufficient information. To showcase CoT-Seg&#39;s ability to handle very challenging cases ,we introduce a new dataset ReasonSeg-Hard. Our results highlight that combining chain-of-thought reasoning, self-correction, offers a powerful paradigm for vision-language integration driven segmentation."
    },
    {
      "title": "Uni-RS: A Spatially Faithful Unified Understanding and Generation Model for Remote Sensing",
      "url": "https://arxiv.org/abs/2601.17673",
      "date": "2026-01-25",
      "authors_text": "Weiyu Zhang, Yuan Hu, Yong Li, Yu Liu",
      "is_highlight": false,
      "score": 58.0,
      "summary": "Uni-RS is a novel unified multimodal model for remote sensing that enhances spatial faithfulness in text-to-image generation by integrating spatial layout planning and supervision techniques.",
      "debug_abstract": "Unified remote sensing multimodal models exhibit a pronounced spatial reversal curse: Although they can accurately recognize and describe object locations in images, they often fail to faithfully execute the same spatial relations during text-to-image generation, where such relations constitute core semantic information in remote sensing. Motivated by this observation, we propose Uni-RS, the first unified multimodal model tailored for remote sensing, to explicitly address the spatial asymmetry between understanding and generation. Specifically, we first introduce explicit Spatial-Layout Planning to transform textual instructions into spatial layout plans, decoupling geometric planning from visual synthesis. We then impose Spatial-Aware Query Supervision to bias learnable queries toward spatial relations explicitly specified in the instruction. Finally, we develop Image-Caption Spatial Layout Variation to expose the model to systematic geometry-consistent spatial transformations. Extensive experiments across multiple benchmarks show that our approach substantially improves spatial faithfulness in text-to-image generation, while maintaining strong performance on multimodal understanding tasks like image captioning, visual grounding, and VQA tasks."
    },
    {
      "title": "TC-IDM: Grounding Video Generation for Executable Zero-shot Robot Motion",
      "url": "https://arxiv.org/abs/2601.18323",
      "date": "2026-01-26",
      "authors_text": "Weishi Mi, Yong Bao, Xiaowei Chi, Xiaozhu Ju, Zhiyuan Qin",
      "is_highlight": false,
      "score": 89.0,
      "summary": "The TC-IDM model enhances robot motion execution by bridging visual planning and physical control through tool-centric trajectory synthesis, achieving superior generalization and performance in diverse tasks.",
      "debug_abstract": "The vision-language-action (VLA) paradigm has enabled powerful robotic control by leveraging vision-language models, but its reliance on large-scale, high-quality robot data limits its generalization. Generative world models offer a promising alternative for general-purpose embodied AI, yet a critical gap remains between their pixel-level plans and physically executable actions. To this end, we propose the Tool-Centric Inverse Dynamics Model (TC-IDM). By focusing on the tool&#39;s imagined trajectory as synthesized by the world model, TC-IDM establishes a robust intermediate representation that bridges the gap between visual planning and physical control. TC-IDM extracts the tool&#39;s point cloud trajectories via segmentation and 3D motion estimation from generated videos. Considering diverse tool attributes, our architecture employs decoupled action heads to project these planned trajectories into 6-DoF end-effector motions and corresponding control signals. This plan-and-translate paradigm not only supports a wide range of end-effectors but also significantly improves viewpoint invariance. Furthermore, it exhibits strong generalization capabilities across long-horizon and out-of-distribution tasks, including interacting with deformable objects. In real-world evaluations, the world model with TC-IDM achieves an average success rate of 61.11 percent, with 77.7 percent on simple tasks and 38.46 percent on zero-shot deformable object tasks. It substantially outperforms end-to-end VLA-style baselines and other inverse dynamics models."
    },
    {
      "title": "Fast and Safe Trajectory Optimization for Mobile Manipulators With Neural Configuration Space Distance Field",
      "url": "https://arxiv.org/abs/2601.18548",
      "date": "2026-01-26",
      "authors_text": "Yulin Li, Zhiyuan Song, Yiming Li, Zhicheng Song, Kai Chen",
      "is_highlight": false,
      "score": 86.0,
      "summary": "This paper introduces Generalized Configuration Space Distance Fields (GCDF) for efficient trajectory optimization in mobile manipulators, enhancing collision reasoning and enabling agile motion in complex environments.",
      "debug_abstract": "Mobile manipulators promise agile, long-horizon behavior by coordinating base and arm motion, yet whole-body trajectory optimization in cluttered, confined spaces remains difficult due to high-dimensional nonconvexity and the need for fast, accurate collision reasoning. Configuration Space Distance Fields (CDF) enable fixed-base manipulators to model collisions directly in configuration space via smooth, implicit distances. This representation holds strong potential to bypass the nonlinear configuration-to-workspace mapping while preserving accurate whole-body geometry and providing optimization-friendly collision costs. Yet, extending this capability to mobile manipulators is hindered by unbounded workspaces and tighter base-arm coupling. We lift this promise to mobile manipulation with Generalized Configuration Space Distance Fields (GCDF), extending CDF to robots with both translational and rotational joints in unbounded workspaces with tighter base-arm coupling. We prove that GCDF preserves Euclidean-like local distance structure and accurately encodes whole-body geometry in configuration space, and develop a data generation and training pipeline that yields continuous neural GCDFs with accurate values and gradients, supporting efficient GPU-batched queries. Building on this representation, we develop a high-performance sequential convex optimization framework centered on GCDF-based collision reasoning. The solver scales to large numbers of implicit constraints through (i) online specification of neural constraints, (ii) sparsity-aware active-set detection with parallel batched evaluation across thousands of constraints, and (iii) incremental constraint management for rapid replanning under scene changes."
    },
    {
      "title": "EvoCUA: Evolving Computer Use Agents via Learning from Scalable Synthetic Experience",
      "url": "https://arxiv.org/abs/2601.15876",
      "date": "2026-01-22",
      "authors_text": "Taofeng Xue, Chong Peng, Mianqiu Huang, Linsen Guo, Tiancheng Han",
      "is_highlight": false,
      "score": 82.6,
      "summary": "EvoCUA introduces a self-sustaining evolutionary model for computer-use agents, leveraging autonomous task generation and iterative learning to significantly outperform existing benchmarks in multimodal AI.",
      "debug_abstract": "The development of native computer-use agents (CUA) represents a significant leap in multimodal AI. However, their potential is currently bottlenecked by the constraints of static data scaling. Existing paradigms relying primarily on passive imitation of static datasets struggle to capture the intricate causal dynamics inherent in long-horizon computer tasks. In this work, we introduce EvoCUA, a native computer use agentic model. Unlike static imitation, EvoCUA integrates data generation and policy optimization into a self-sustaining evolutionary cycle. To mitigate data scarcity, we develop a verifiable synthesis engine that autonomously generates diverse tasks coupled with executable validators. To enable large-scale experience acquisition, we design a scalable infrastructure orchestrating tens of thousands of asynchronous sandbox rollouts. Building on these massive trajectories, we propose an iterative evolving learning strategy to efficiently internalize this experience. This mechanism dynamically regulates policy updates by identifying capability boundaries -- reinforcing successful routines while transforming failure trajectories into rich supervision through error analysis and self-correction. Empirical evaluations on the OSWorld benchmark demonstrate that EvoCUA achieves a success rate of 56.7%, establishing a new open-source state-of-the-art. Notably, EvoCUA significantly outperforms the previous best open-source model, OpenCUA-72B (45.0%), and surpasses leading closed-weights models such as UI-TARS-2 (53.1%). Crucially, our results underscore the generalizability of this approach: the evolving paradigm driven by learning from experience yields consistent performance gains across foundation models of varying scales, establishing a robust and scalable path for advancing native agent capabilities."
    },
    {
      "title": "DualShield: Safe Model Predictive Diffusion via Reachability Analysis for Interactive Autonomous Driving",
      "url": "https://arxiv.org/abs/2601.15729",
      "date": "2026-01-22",
      "authors_text": "Rui Yang, Lei Zheng, Ruoyu Yao, Jun Ma",
      "is_highlight": true,
      "score": 92.7,
      "summary": "DualShield enhances autonomous driving safety by integrating Hamilton-Jacobi reachability analysis into diffusion models for proactive guidance and reactive safety assurance under uncertain interactions.",
      "debug_abstract": "Diffusion models have emerged as a powerful approach for multimodal motion planning in autonomous driving. However, their practical deployment is typically hindered by the inherent difficulty in enforcing vehicle dynamics and a critical reliance on accurate predictions of other agents, making them prone to safety issues under uncertain interactions. To address these limitations, we introduce DualShield, a planning and control framework that leverages Hamilton-Jacobi (HJ) reachability value functions in a dual capacity. First, the value functions act as proactive guidance, steering the diffusion denoising process towards safe and dynamically feasible regions. Second, they form a reactive safety shield using control barrier-value functions (CBVFs) to modify the executed actions and ensure safety. This dual mechanism preserves the rich exploration capabilities of diffusion models while providing principled safety assurance under uncertain and even adversarial interactions. Simulations in challenging unprotected U-turn scenarios demonstrate that DualShield significantly improves both safety and task efficiency compared to leading methods from different planning paradigms under uncertainty."
    }
  ],
  "Cheng Kar-Shun Robotics Institute (CKSRI)": [
    {
      "title": "Visual Reasoning over Time Series via Multi-Agent System",
      "url": "https://arxiv.org/abs/2602.03026",
      "date": "2026-02-03",
      "authors_text": "Weilin Ruan, Yuxuan Liang",
      "is_highlight": false,
      "score": 66.0,
      "summary": "MAS4TS is a multi-agent system that enhances time series analysis through visual reasoning and adaptive tool usage, achieving state-of-the-art performance across diverse tasks.",
      "teaser_image": "https://arxiv.org/html/2602.03026/x2.png",
      "debug_abstract": "Time series analysis underpins many real-world applications, yet existing time-series-specific methods and pretrained large-model-based approaches remain limited in integrating intuitive visual reasoning and generalizing across tasks with adaptive tool usage. To address these limitations, we propose MAS4TS, a tool-driven multi-agent system for general time series tasks, built upon an Analyzer-Reasoner-Executor paradigm that integrates agent communication, visual reasoning, and latent reconstruction within a unified framework. MAS4TS first performs visual reasoning over time series plots with structured priors using a Vision-Language Model to extract temporal structures, and subsequently reconstructs predictive trajectories in latent space. Three specialized agents coordinate via shared memory and gated communication, while a router selects task-specific tool chains for execution. Extensive experiments on multiple benchmarks demonstrate that MAS4TS achieves state-of-the-art performance across a wide range of time series tasks, while exhibiting strong generalization and efficient inference."
    },
    {
      "title": "Neural Predictor-Corrector: Solving Homotopy Problems with Reinforcement Learning",
      "url": "https://arxiv.org/abs/2602.03086",
      "date": "2026-02-03",
      "authors_text": "Jiayao Mai, Bangyan Liao, Zhenjun Zhao, Yingping Zeng, Haoang Li",
      "is_highlight": false,
      "score": 64.0,
      "summary": "The Neural Predictor-Corrector (NPC) framework uses reinforcement learning to automate step size and termination decisions in solving diverse homotopy problems, outperforming traditional methods.",
      "teaser_image": "https://arxiv.org/html/2602.03086/x2.png",
      "debug_abstract": "The Homotopy paradigm, a general principle for solving challenging problems, appears across diverse domains such as robust optimization, global optimization, polynomial root-finding, and sampling. Practical solvers for these problems typically follow a predictor-corrector (PC) structure, but rely on hand-crafted heuristics for step sizes and iteration termination, which are often suboptimal and task-specific. To address this, we unify these problems under a single framework, which enables the design of a general neural solver. Building on this unified view, we propose Neural Predictor-Corrector (NPC), which replaces hand-crafted heuristics with automatically learned policies. NPC formulates policy selection as a sequential decision-making problem and leverages reinforcement learning to automatically discover efficient strategies. To further enhance generalization, we introduce an amortized training mechanism, enabling one-time offline training for a class of problems and efficient online inference on new instances. Experiments on four representative homotopy problems demonstrate that our method generalizes effectively to unseen instances. It consistently outperforms classical and specialized baselines in efficiency while demonstrating superior stability across tasks, highlighting the value of unifying homotopy methods into a single neural framework."
    },
    {
      "title": "Diffusion-Driven Inter-Outer Surface Separation for Point Clouds with Open Boundaries",
      "url": "https://arxiv.org/abs/2602.00739",
      "date": "2026-01-31",
      "authors_text": "Zhengyan Qin, Liyuan Qiu",
      "is_highlight": false,
      "score": 72.0,
      "summary": "The paper presents a diffusion-based algorithm for effectively separating inter and outer surfaces in double-layered point clouds with open boundaries, addressing artifacts from TSDF fusion.",
      "teaser_image": "https://arxiv.org/html/2602.00739/figures/pipeline5.png",
      "debug_abstract": "We propose a diffusion-based algorithm for separating the inter and outer layer surfaces from double-layered point clouds, particularly those exhibiting the &#34;double surface artifact&#34; caused by truncation in Truncated Signed Distance Function (TSDF) fusion during indoor or medical 3D reconstruction. This artifact arises from asymmetric truncation thresholds, leading to erroneous inter and outer shells in the fused volume, which our method addresses by extracting the true inter layer to mitigate challenges like overlapping surfaces and disordered normals. We focus on point clouds with \\emph{open boundaries} (i.e., sampled surfaces with topological openings/holes through which particles may escape), rather than point clouds with \\emph{missing surface regions} where no samples exist. Our approach enables robust processing of both watertight and open-boundary models, achieving extraction of the inter layer from 20,000 inter and 20,000 outer points in approximately 10 seconds. This solution is particularly effective for applications requiring accurate surface representations, such as indoor scene modeling and medical imaging, where double-layered point clouds are prevalent, and it accommodates both closed (watertight) and open-boundary surface geometries. Our goal is \\emph{post-hoc} inter/outer shell separation as a lightweight module after TSDF fusion; we do not aim to replace full variational or learning-based reconstruction pipelines."
    },
    {
      "title": "Physics-informed Diffusion Mamba Transformer for Real-world Driving",
      "url": "https://arxiv.org/abs/2602.00808",
      "date": "2026-01-31",
      "authors_text": "Hang Zhou, Qiang Zhang, Peiran Liu, Yihao Qin, Zhaoxu Yan",
      "is_highlight": false,
      "score": 74.0,
      "summary": "The paper presents a Diffusion Mamba Transformer that integrates sequential context and physical constraints for improved trajectory planning in autonomous driving, outperforming existing models.",
      "teaser_image": "https://arxiv.org/html/2602.00808/difference_pipeline3.png",
      "debug_abstract": "Autonomous driving systems demand trajectory planners that not only model the inherent uncertainty of future motions but also respect complex temporal dependencies and underlying physical laws. While diffusion-based generative models excel at capturing multi-modal distributions, they often fail to incorporate long-term sequential contexts and domain-specific physical priors. In this work, we bridge these gaps with two key innovations. First, we introduce a Diffusion Mamba Transformer architecture that embeds mamba and attention into the diffusion process, enabling more effective aggregation of sequential input contexts from sensor streams and past motion histories. Second, we design a Port-Hamiltonian Neural Network module that seamlessly integrates energy-based physical constraints into the diffusion model, thereby enhancing trajectory predictions with both consistency and interpretability. Extensive evaluations on standard autonomous driving benchmarks demonstrate that our unified framework significantly outperforms state-of-the-art baselines in predictive accuracy, physical plausibility, and robustness, thereby advancing safe and reliable motion planning."
    },
    {
      "title": "Invariance on Manifolds: Understanding Robust Visual Representations for Place Recognition",
      "url": "https://arxiv.org/abs/2602.00841",
      "date": "2026-01-31",
      "authors_text": "Jintao Cheng, Weibin Li, Zhijian He, Jin Wu, Chi Man Vong",
      "is_highlight": false,
      "score": 66.0,
      "summary": "This paper presents a training-free Second-Order Geometric Statistics framework for robust visual place recognition, utilizing covariance descriptors on the SPD manifold for effective zero-shot generalization.",
      "teaser_image": "https://arxiv.org/html/2602.00841/x2.png",
      "debug_abstract": "Visual Place Recognition (VPR) demands representations robust to drastic environmental and viewpoint shifts. Current aggregation paradigms, however, either rely on data-hungry supervision or simplistic first-order statistics, often neglecting intrinsic structural correlations. In this work, we propose a Second-Order Geometric Statistics framework that inherently captures geometric stability without training. We conceptualize scenes as covariance descriptors on the Symmetric Positive Definite (SPD) manifold, where perturbations manifest as tractable congruence transformations. By leveraging geometry-aware Riemannian mappings, we project these descriptors into a linearized Euclidean embedding, effectively decoupling signal structure from noise. Our approach introduces a training-free framework built upon fixed, pre-trained backbones, achieving strong zero-shot generalization without parameter updates. Extensive experiments confirm that our method achieves highly competitive performance against state-of-the-art baselines, particularly excelling in challenging zero-shot scenarios."
    },
    {
      "title": "PolicyFlow: Policy Optimization with Continuous Normalizing Flow in Reinforcement Learning",
      "url": "https://arxiv.org/abs/2602.01156",
      "date": "2026-02-01",
      "authors_text": "Shunpeng Yang, Ben Liu, Hua Chen",
      "is_highlight": false,
      "score": 71.0,
      "summary": "PolicyFlow is a novel reinforcement learning algorithm that integrates continuous normalizing flows with PPO, enhancing performance and stability while reducing computational costs and encouraging diverse behaviors.",
      "teaser_image": "https://arxiv.org/html/2602.01156/x1.png",
      "debug_abstract": "Among on-policy reinforcement learning algorithms, Proximal Policy Optimization (PPO) demonstrates is widely favored for its simplicity, numerical stability, and strong empirical performance. Standard PPO relies on surrogate objectives defined via importance ratios, which require evaluating policy likelihood that is typically straightforward when the policy is modeled as a Gaussian distribution. However, extending PPO to more expressive, high-capacity policy models such as continuous normalizing flows (CNFs), also known as flow-matching models, is challenging because likelihood evaluation along the full flow trajectory is computationally expensive and often numerically unstable. To resolve this issue, we propose PolicyFlow, a novel on-policy CNF-based reinforcement learning algorithm that integrates expressive CNF policies with PPO-style objectives without requiring likelihood evaluation along the full flow path. PolicyFlow approximates importance ratios using velocity field variations along a simple interpolation path, reducing computational overhead without compromising training stability. To further prevent mode collapse and further encourage diverse behaviors, we propose the Brownian Regularizer, an implicit policy entropy regularizer inspired by Brownian motion, which is conceptually elegant and computationally lightweight. Experiments on diverse tasks across various environments including MultiGoal, PointMaze, IsaacLab and MuJoCo Playground show that PolicyFlow achieves competitive or superior performance compared to PPO using Gaussian policies and flow-based baselines including FPO and DPPO. Notably, results on MultiGoal highlight PolicyFlow&#39;s ability to capture richer multimodal action distributions."
    },
    {
      "title": "Do All Individual Layers Help? An Empirical Study of Task-Interfering Layers in Vision-Language Models",
      "url": "https://arxiv.org/abs/2602.01167",
      "date": "2026-02-01",
      "authors_text": "Zhiming Liu, Yujie Wei, Lei Feng, Xiu Su, Xiaobo Xia",
      "is_highlight": false,
      "score": 52.0,
      "summary": "This study identifies Task-Interfering Layers in vision-language models, demonstrating that selectively bypassing certain layers can enhance task performance without retraining.",
      "teaser_image": "https://arxiv.org/html/2602.01167/x1.png",
      "debug_abstract": "Current VLMs have demonstrated capabilities across a wide range of multimodal tasks. Typically, in a pretrained VLM, all layers are engaged by default to make predictions on downstream tasks. We find that intervening on a single layer, such as by zeroing its parameters, can improve the performance on certain tasks, indicating that some layers hinder rather than help downstream tasks. We systematically investigate how individual layers influence different tasks via layer intervention. Specifically, we measure the change in performance relative to the base model after intervening on each layer and observe improvements when bypassing specific layers. This improvement can be generalizable across models and datasets, indicating the presence of Task-Interfering Layers that harm downstream tasks&#39; performance. We introduce Task-Layer Interaction Vector, which quantifies the effect of intervening on each layer of a VLM given a task. These task-interfering layers exhibit task-specific sensitivity patterns: tasks requiring similar capabilities show consistent response trends under layer interventions, as evidenced by the high similarity in their task-layer interaction vectors. Inspired by these findings, we propose TaLo (Task-Adaptive Layer Knockout), a training-free, test-time adaptation method that dynamically identifies and bypasses the most interfering layer for a given task. Without parameter updates, TaLo improves performance across various models and datasets, including boosting Qwen-VL&#39;s accuracy on the Maps task in ScienceQA by up to 16.6%. Our work reveals an unexpected form of modularity in pretrained VLMs and provides a plug-and-play, training-free mechanism to unlock hidden capabilities at inference time. The source code will be publicly available."
    },
    {
      "title": "Towards Exploratory and Focused Manipulation with Bimanual Active Perception: A New Problem, Benchmark and Strategy",
      "url": "https://arxiv.org/abs/2602.01939",
      "date": "2026-02-02",
      "authors_text": "Yuxin He, Ruihao Zhang, Tianao Shen, Cheng Liu, Qiang Nie",
      "is_highlight": false,
      "score": 28.0,
      "summary": "The paper introduces the Exploratory and Focused Manipulation (EFM) problem, establishes the EFM-10 benchmark, and proposes a Bimanual Active Perception strategy to enhance manipulation tasks.",
      "teaser_image": "https://arxiv.org/html/2602.01939/x2.png",
      "debug_abstract": "Recently, active vision has reemerged as an important concept for manipulation, since visual occlusion occurs more frequently when main cameras are mounted on the robot heads. We reflect on the visual occlusion issue and identify its essence as the absence of information useful for task completion. Inspired by this, we come up with the more fundamental problem of Exploratory and Focused Manipulation (EFM). The proposed problem is about actively collecting information to complete challenging manipulation tasks that require exploration or focus. As an initial attempt to address this problem, we establish the EFM-10 benchmark that consists of 4 categories of tasks that align with our definition (10 tasks in total). We further come up with a Bimanual Active Perception (BAP) strategy, which leverages one arm to provide active vision and another arm to provide force sensing while manipulating. Based on this idea, we collect a dataset named BAPData for the tasks in EFM-10. With the dataset, we successfully verify the effectiveness of the BAP strategy in an imitation learning manner. We hope that the EFM-10 benchmark along with the BAP strategy can become a cornerstone that facilitates future research towards this direction. Project website: this http URL."
    },
    {
      "title": "Breaking the Static Graph: Context-Aware Traversal for Robust Retrieval-Augmented Generation",
      "url": "https://arxiv.org/abs/2602.01965",
      "date": "2026-02-02",
      "authors_text": "Kwun Hang Lau, Fangyuan Zhang, Boyu Ruan, Yingli Zhou, Qintian Guo",
      "is_highlight": false,
      "score": 27.0,
      "summary": "CatRAG enhances Retrieval-Augmented Generation by transforming static Knowledge Graphs into query-adaptive structures, improving multi-hop evidence retrieval and reasoning completeness.",
      "teaser_image": "https://arxiv.org/html/2602.01965/image/HubBiasShift.png",
      "debug_abstract": "Recent advances in Retrieval-Augmented Generation (RAG) have shifted from simple vector similarity to structure-aware approaches like HippoRAG, which leverage Knowledge Graphs (KGs) and Personalized PageRank (PPR) to capture multi-hop dependencies. However, these methods suffer from a &#34;Static Graph Fallacy&#34;: they rely on fixed transition probabilities determined during indexing. This rigidity ignores the query-dependent nature of edge relevance, causing semantic drift where random walks are diverted into high-degree &#34;hub&#34; nodes before reaching critical downstream evidence. Consequently, models often achieve high partial recall but fail to retrieve the complete evidence chain required for multi-hop queries. To address this, we propose CatRAG, Context-Aware Traversal for robust RAG, a framework that builds on the HippoRAG 2 architecture and transforms the static KG into a query-adaptive navigation structure. We introduce a multi-faceted framework to steer the random walk: (1) Symbolic Anchoring, which injects weak entity constraints to regularize the random walk; (2) Query-Aware Dynamic Edge Weighting, which dynamically modulates graph structure, to prune irrelevant paths while amplifying those aligned with the query&#39;s intent; and (3) Key-Fact Passage Weight Enhancement, a cost-efficient bias that structurally anchors the random walk to likely evidence. Experiments across four multi-hop benchmarks demonstrate that CatRAG consistently outperforms state of the art baselines. Our analysis reveals that while standard Recall metrics show modest gains, CatRAG achieves substantial improvements in reasoning completeness, the capacity to recover the entire evidence path without gaps. These results reveal that our approach effectively bridges the gap between retrieving partial context and enabling fully grounded reasoning. Resources are available at this https URL."
    },
    {
      "title": "Towards Agentic Intelligence for Materials Science",
      "url": "https://arxiv.org/abs/2602.00169",
      "date": "2026-01-29",
      "authors_text": "Huan Zhang, Yizhan Li, Wenhao Huang, Ziyu Hou, Yu Song",
      "is_highlight": false,
      "score": 12.0,
      "summary": "This paper advocates for developing agentic AI systems in materials science that autonomously plan, act, and learn to enhance discovery processes beyond traditional models.",
      "teaser_image": "https://arxiv.org/html/2602.00169/x1.png",
      "debug_abstract": "The convergence of artificial intelligence and materials science presents a transformative opportunity, but achieving true acceleration in discovery requires moving beyond task-isolated, fine-tuned models toward agentic systems that plan, act, and learn across the full discovery loop. This survey advances a unique pipeline-centric view that spans from corpus curation and pretraining, through domain adaptation and instruction tuning, to goal-conditioned agents interfacing with simulation and experimental platforms. Unlike prior reviews, we treat the entire process as an end-to-end system to be optimized for tangible discovery outcomes rather than proxy benchmarks. This perspective allows us to trace how upstream design choices-such as data curation and training objectives-can be aligned with downstream experimental success through effective credit assignment. To bridge communities and establish a shared frame of reference, we first present an integrated lens that aligns terminology, evaluation, and workflow stages across AI and materials science. We then analyze the field through two focused lenses: From the AI perspective, the survey details LLM strengths in pattern recognition, predictive analytics, and natural language processing for literature mining, materials characterization, and property prediction; from the materials science perspective, it highlights applications in materials design, process optimization, and the acceleration of computational workflows via integration with external tools (e.g., DFT, robotic labs). Finally, we contrast passive, reactive approaches with agentic design, cataloging current contributions while motivating systems that pursue long-horizon goals with autonomy, memory, and tool use. This survey charts a practical roadmap towards autonomous, safety-aware LLM agents aimed at discovering novel and useful materials."
    },
    {
      "title": "Advances and Innovations in the Multi-Agent Robotic System (MARS) Challenge",
      "url": "https://arxiv.org/abs/2601.18733",
      "date": "2026-01-26",
      "authors_text": "Li Kang, Heng Zhou, Xiufeng Song, Rui Li, Bruno N. Y. Chen",
      "is_highlight": false,
      "score": 78.0,
      "summary": "The MARS Challenge promotes advancements in multi-agent robotic systems by exploring vision-language models for collaborative task execution and planning in dynamic environments.",
      "teaser_image": null,
      "debug_abstract": "Recent advancements in multimodal large language models and vision-languageaction models have significantly driven progress in Embodied AI. As the field transitions toward more complex task scenarios, multi-agent system frameworks are becoming essential for achieving scalable, efficient, and collaborative solutions. This shift is fueled by three primary factors: increasing agent capabilities, enhancing system efficiency through task delegation, and enabling advanced human-agent interactions. To address the challenges posed by multi-agent collaboration, we propose the Multi-Agent Robotic System (MARS) Challenge, held at the NeurIPS 2025 Workshop on SpaVLE. The competition focuses on two critical areas: planning and control, where participants explore multi-agent embodied planning using vision-language models (VLMs) to coordinate tasks and policy execution to perform robotic manipulation in dynamic environments. By evaluating solutions submitted by participants, the challenge provides valuable insights into the design and coordination of embodied multi-agent systems, contributing to the future development of advanced collaborative AI systems."
    },
    {
      "title": "Gaussian Belief Propagation Network for Depth Completion",
      "url": "https://arxiv.org/abs/2601.21291",
      "date": "2026-01-29",
      "authors_text": "Jie Tang, Pingping Xie, Jian Li, Ping Tan",
      "is_highlight": false,
      "score": 68.0,
      "summary": "The Gaussian Belief Propagation Network (GBPN) integrates deep learning with probabilistic models for effective depth completion from sparse measurements, achieving state-of-the-art results across various benchmarks.",
      "teaser_image": "https://arxiv.org/html/2601.21291/x1.png",
      "debug_abstract": "Depth completion aims to predict a dense depth map from a color image with sparse depth measurements. Although deep learning methods have achieved state-of-the-art (SOTA), effectively handling the sparse and irregular nature of input depth data in deep networks remains a significant challenge, often limiting performance, especially under high sparsity. To overcome this limitation, we introduce the Gaussian Belief Propagation Network (GBPN), a novel hybrid framework synergistically integrating deep learning with probabilistic graphical models for end-to-end depth completion. Specifically, a scene-specific Markov Random Field (MRF) is dynamically constructed by the Graphical Model Construction Network (GMCN), and then inferred via Gaussian Belief Propagation (GBP) to yield the dense depth distribution. Crucially, the GMCN learns to construct not only the data-dependent potentials of MRF but also its structure by predicting adaptive non-local edges, enabling the capture of complex, long-range spatial dependencies. Furthermore, we enhance GBP with a serial \\&amp; parallel message passing scheme, designed for effective information propagation, particularly from sparse measurements. Extensive experiments demonstrate that GBPN achieves SOTA performance on the NYUv2 and KITTI benchmarks. Evaluations across varying sparsity levels, sparsity patterns, and datasets highlight GBPN&#39;s superior performance, notable robustness, and generalizable capability."
    },
    {
      "title": "4D-CAAL: 4D Radar-Camera Calibration and Auto-Labeling for Autonomous Driving",
      "url": "https://arxiv.org/abs/2601.21454",
      "date": "2026-01-29",
      "authors_text": "Shanliang Yao, Zhuoxiao Li, Runwei Guan, Kebin Cao, Meng Xia",
      "is_highlight": false,
      "score": 89.0,
      "summary": "The 4D-CAAL framework integrates 4D radar-camera calibration and auto-labeling using a dual-purpose target and robust correspondence matching, enhancing autonomous driving perception efficiency.",
      "teaser_image": "https://arxiv.org/html/2601.21454/images/4DRC-fixed.png",
      "debug_abstract": "4D radar has emerged as a critical sensor for autonomous driving, primarily due to its enhanced capabilities in elevation measurement and higher resolution compared to traditional 3D radar. Effective integration of 4D radar with cameras requires accurate extrinsic calibration, and the development of radar-based perception algorithms demands large-scale annotated datasets. However, existing calibration methods often employ separate targets optimized for either visual or radar modalities, complicating correspondence establishment. Furthermore, manually labeling sparse radar data is labor-intensive and unreliable. To address these challenges, we propose 4D-CAAL, a unified framework for 4D radar-camera calibration and auto-labeling. Our approach introduces a novel dual-purpose calibration target design, integrating a checkerboard pattern on the front surface for camera detection and a corner reflector at the center of the back surface for radar detection. We develop a robust correspondence matching algorithm that aligns the checkerboard center with the strongest radar reflection point, enabling accurate extrinsic calibration. Subsequently, we present an auto-labeling pipeline that leverages the calibrated sensor relationship to transfer annotations from camera-based segmentations to radar point clouds through geometric projection and multi-feature optimization. Extensive experiments demonstrate that our method achieves high calibration accuracy while significantly reducing manual annotation effort, thereby accelerating the development of robust multi-modal perception systems for autonomous driving."
    },
    {
      "title": "DynaWeb: Model-Based Reinforcement Learning of Web Agents",
      "url": "https://arxiv.org/abs/2601.22149",
      "date": "2026-01-29",
      "authors_text": "Hang Ding, Peidong Liu, Junqiao Wang, Ziwei Ji, Meng Cao",
      "is_highlight": false,
      "score": 77.0,
      "summary": "DynaWeb introduces a model-based reinforcement learning framework that enhances web agent training by simulating interactions with a web world model, improving efficiency and performance.",
      "teaser_image": "https://arxiv.org/html/2601.22149/figures/dynaweb.png",
      "debug_abstract": "The development of autonomous web agents, powered by Large Language Models (LLMs) and reinforcement learning (RL), represents a significant step towards general-purpose AI assistants. However, training these agents is severely hampered by the challenges of interacting with the live internet, which is inefficient, costly, and fraught with risks. Model-based reinforcement learning (MBRL) offers a promising solution by learning a world model of the environment to enable simulated interaction. This paper introduces DynaWeb, a novel MBRL framework that trains web agents through interacting with a web world model trained to predict naturalistic web page representations given agent actions. This model serves as a synthetic web environment where an agent policy can dream by generating vast quantities of rollout action trajectories for efficient online reinforcement learning. Beyond free policy rollouts, DynaWeb incorporates real expert trajectories from training data, which are randomly interleaved with on-policy rollouts during training to improve stability and sample efficiency. Experiments conducted on the challenging WebArena and WebVoyager benchmarks demonstrate that DynaWeb consistently and significantly improves the performance of state-of-the-art open-source web agent models. Our findings establish the viability of training web agents through imagination, offering a scalable and efficient way to scale up online agentic RL."
    },
    {
      "title": "Endogenous Reprompting: Self-Evolving Cognitive Alignment for Unified Multimodal Models",
      "url": "https://arxiv.org/abs/2601.20305",
      "date": "2026-01-28",
      "authors_text": "Zhenchen Tang, Songlin Yang, Zichuan Wang, Bo Peng, Yang Li",
      "is_highlight": false,
      "score": 69.0,
      "summary": "The paper introduces Endogenous Reprompting and the SEER framework, enhancing Unified Multimodal Models' generative reasoning through self-aligned descriptors and reinforcement learning, outperforming existing methods.",
      "teaser_image": "https://arxiv.org/html/2601.20305/x2.png",
      "debug_abstract": "Unified Multimodal Models (UMMs) exhibit strong understanding, yet this capability often fails to effectively guide generation. We identify this as a Cognitive Gap: the model lacks the understanding of how to enhance its own generation process. To bridge this gap, we propose Endogenous Reprompting, a mechanism that transforms the model&#39;s understanding from a passive encoding process into an explicit generative reasoning step by generating self-aligned descriptors during generation. To achieve this, we introduce SEER (Self-Evolving Evaluator and Reprompter), a training framework that establishes a two-stage endogenous loop using only 300 samples from a compact proxy task, Visual Instruction Elaboration. First, Reinforcement Learning with Verifiable Rewards (RLVR) activates the model&#39;s latent evaluation ability via curriculum learning, producing a high-fidelity endogenous reward signal. Second, Reinforcement Learning with Model-rewarded Thinking (RLMT) leverages this signal to optimize the generative reasoning policy. Experiments show that SEER consistently outperforms state-of-the-art baselines in evaluation accuracy, reprompting efficiency, and generation quality, without sacrificing general multimodal capabilities."
    },
    {
      "title": "Glance and Focus Reinforcement for Pan-cancer Screening",
      "url": "https://arxiv.org/abs/2601.19103",
      "date": "2026-01-27",
      "authors_text": "Linshan Wu, Jiaxin Zhuang, Hao Chen",
      "is_highlight": false,
      "score": 65.0,
      "summary": "GF-Screen introduces a novel reinforcement learning framework that enhances pan-cancer screening by effectively localizing and segmenting lesions in CT scans, outperforming existing methods.",
      "debug_abstract": "Pan-cancer screening in large-scale CT scans remains challenging for existing AI methods, primarily due to the difficulty of localizing diverse types of tiny lesions in large CT volumes. The extreme foreground-background imbalance significantly hinders models from focusing on diseased regions, while redundant focus on healthy regions not only decreases the efficiency but also increases false positives. Inspired by radiologists&#39; glance and focus diagnostic strategy, we introduce GF-Screen, a Glance and Focus reinforcement learning framework for pan-cancer screening. GF-Screen employs a Glance model to localize the diseased regions and a Focus model to precisely segment the lesions, where segmentation results of the Focus model are leveraged to reward the Glance model via Reinforcement Learning (RL). Specifically, the Glance model crops a group of sub-volumes from the entire CT volume and learns to select the sub-volumes with lesions for the Focus model to segment. Given that the selecting operation is non-differentiable for segmentation training, we propose to employ the segmentation results to reward the Glance model. To optimize the Glance model, we introduce a novel group relative learning paradigm, which employs group relative comparison to prioritize high-advantage predictions and discard low-advantage predictions within sub-volume groups, not only improving efficiency but also reducing false positives. In this way, for the first time, we effectively extend cutting-edge RL techniques to tackle the specific challenges in pan-cancer screening. Extensive experiments on 16 internal and 7 external datasets across 9 lesion types demonstrated the effectiveness of GF-Screen. Notably, GF-Screen leads the public validation leaderboard of MICCAI FLARE25 pan-cancer challenge, surpassing the FLARE24 champion solution by a large margin (+25.6% DSC and +28.2% NSD)."
    },
    {
      "title": "Enhancing Inverse Perspective Mapping for Automatic Vectorized Road Map Generation",
      "url": "https://arxiv.org/abs/2601.19536",
      "date": "2026-01-27",
      "authors_text": "Hongji Liu, Linwei Zheng, Yongjian Li, Mingkai Tang, Xiaoyang Yan",
      "is_highlight": false,
      "score": 70.0,
      "summary": "This study introduces a cost-effective framework using enhanced inverse perspective mapping and Catmull-Rom splines for high-precision, automatic vectorized road map generation.",
      "debug_abstract": "In this study, we present a low-cost and unified framework for vectorized road mapping leveraging enhanced inverse perspective mapping (IPM). In this framework, Catmull-Rom splines are utilized to characterize lane lines, and all the other ground markings are depicted using polygons uniformly. The results from instance segmentation serve as references to refine the three-dimensional position of spline control points and polygon corner points. In conjunction with this process, the homography matrix of IPM and vehicle poses are optimized simultaneously. Our proposed framework significantly reduces the mapping errors associated with IPM. It also improves the accuracy of the initial IPM homography matrix and the predicted vehicle poses. Furthermore, it addresses the limitations imposed by the coplanarity assumption in IPM. These enhancements enable IPM to be effectively applied to vectorized road mapping, which serves a cost-effective solution with enhanced accuracy. In addition, our framework generalizes road map elements to include all common ground markings and lane lines. The proposed framework is evaluated in two different practical scenarios, and the test results show that our method can automatically generate high-precision maps with near-centimeter-level accuracy. Importantly, the optimized IPM matrix achieves an accuracy comparable to that of manual calibration, while the accuracy of vehicle poses is also significantly improved."
    },
    {
      "title": "Geometry-Grounded Gaussian Splatting",
      "url": "https://arxiv.org/abs/2601.17835",
      "date": "2026-01-25",
      "authors_text": "Baowen Zhang, Chenxing Jiang, Heng Li, Shaojie Shen, Ping Tan",
      "is_highlight": true,
      "score": 53.0,
      "summary": "This paper introduces Geometry-Grounded Gaussian Splatting, enhancing shape extraction from Gaussian primitives by treating them as stochastic solids for improved depth map rendering and reconstruction quality.",
      "debug_abstract": "Gaussian Splatting (GS) has demonstrated impressive quality and efficiency in novel view synthesis. However, shape extraction from Gaussian primitives remains an open problem. Due to inadequate geometry parameterization and approximation, existing shape reconstruction methods suffer from poor multi-view consistency and are sensitive to floaters. In this paper, we present a rigorous theoretical derivation that establishes Gaussian primitives as a specific type of stochastic solids. This theoretical framework provides a principled foundation for Geometry-Grounded Gaussian Splatting by enabling the direct treatment of Gaussian primitives as explicit geometric representations. Using the volumetric nature of stochastic solids, our method efficiently renders high-quality depth maps for fine-grained geometry extraction. Experiments show that our method achieves the best shape reconstruction results among all Gaussian Splatting-based methods on public datasets."
    },
    {
      "title": "CoT-Seg: Rethinking Segmentation with Chain-of-Thought Reasoning and Self-Correction",
      "url": "https://arxiv.org/abs/2601.17420",
      "date": "2026-01-24",
      "authors_text": "Shiu-hong Kao, Chak Ho Huang, Huaiqian Liu, Yu-Wing Tai, Chi-Keung Tang",
      "is_highlight": false,
      "score": 68.0,
      "summary": "CoT-Seg introduces a training-free framework that enhances segmentation through chain-of-thought reasoning and self-correction, improving performance on complex queries and ambiguous images.",
      "debug_abstract": "Existing works of reasoning segmentation often fall short in complex cases, particularly when addressing complicated queries and out-of-domain images. Inspired by the chain-of-thought reasoning, where harder problems require longer thinking steps/time, this paper aims to explore a system that can think step-by-step, look up information if needed, generate results, self-evaluate its own results, and refine the results, in the same way humans approach harder questions. We introduce CoT-Seg, a training-free framework that rethinks reasoning segmentation by combining chain-of-thought reasoning with self-correction. Instead of fine-tuning, CoT-Seg leverages the inherent reasoning ability of pre-trained MLLMs (GPT-4o) to decompose queries into meta-instructions, extract fine-grained semantics from images, and identify target objects even under implicit or complex prompts. Moreover, CoT-Seg incorporates a self-correction stage: the model evaluates its own segmentation against the original query and reasoning trace, identifies mismatches, and iteratively refines the mask. This tight integration of reasoning and correction significantly improves reliability and robustness, especially in ambiguous or error-prone cases. Furthermore, our CoT-Seg framework allows easy incorporation of retrieval-augmented reasoning, enabling the system to access external knowledge when the input lacks sufficient information. To showcase CoT-Seg&#39;s ability to handle very challenging cases ,we introduce a new dataset ReasonSeg-Hard. Our results highlight that combining chain-of-thought reasoning, self-correction, offers a powerful paradigm for vision-language integration driven segmentation."
    },
    {
      "title": "Uni-RS: A Spatially Faithful Unified Understanding and Generation Model for Remote Sensing",
      "url": "https://arxiv.org/abs/2601.17673",
      "date": "2026-01-25",
      "authors_text": "Weiyu Zhang, Yuan Hu, Yong Li, Yu Liu",
      "is_highlight": false,
      "score": 58.0,
      "summary": "Uni-RS is a novel unified multimodal model for remote sensing that enhances spatial faithfulness in text-to-image generation by integrating spatial layout planning and supervision techniques.",
      "debug_abstract": "Unified remote sensing multimodal models exhibit a pronounced spatial reversal curse: Although they can accurately recognize and describe object locations in images, they often fail to faithfully execute the same spatial relations during text-to-image generation, where such relations constitute core semantic information in remote sensing. Motivated by this observation, we propose Uni-RS, the first unified multimodal model tailored for remote sensing, to explicitly address the spatial asymmetry between understanding and generation. Specifically, we first introduce explicit Spatial-Layout Planning to transform textual instructions into spatial layout plans, decoupling geometric planning from visual synthesis. We then impose Spatial-Aware Query Supervision to bias learnable queries toward spatial relations explicitly specified in the instruction. Finally, we develop Image-Caption Spatial Layout Variation to expose the model to systematic geometry-consistent spatial transformations. Extensive experiments across multiple benchmarks show that our approach substantially improves spatial faithfulness in text-to-image generation, while maintaining strong performance on multimodal understanding tasks like image captioning, visual grounding, and VQA tasks."
    },
    {
      "title": "TC-IDM: Grounding Video Generation for Executable Zero-shot Robot Motion",
      "url": "https://arxiv.org/abs/2601.18323",
      "date": "2026-01-26",
      "authors_text": "Weishi Mi, Yong Bao, Xiaowei Chi, Xiaozhu Ju, Zhiyuan Qin",
      "is_highlight": false,
      "score": 89.0,
      "summary": "The TC-IDM model enhances robot motion execution by bridging visual planning and physical control through tool-centric trajectory synthesis, achieving superior generalization and performance in diverse tasks.",
      "debug_abstract": "The vision-language-action (VLA) paradigm has enabled powerful robotic control by leveraging vision-language models, but its reliance on large-scale, high-quality robot data limits its generalization. Generative world models offer a promising alternative for general-purpose embodied AI, yet a critical gap remains between their pixel-level plans and physically executable actions. To this end, we propose the Tool-Centric Inverse Dynamics Model (TC-IDM). By focusing on the tool&#39;s imagined trajectory as synthesized by the world model, TC-IDM establishes a robust intermediate representation that bridges the gap between visual planning and physical control. TC-IDM extracts the tool&#39;s point cloud trajectories via segmentation and 3D motion estimation from generated videos. Considering diverse tool attributes, our architecture employs decoupled action heads to project these planned trajectories into 6-DoF end-effector motions and corresponding control signals. This plan-and-translate paradigm not only supports a wide range of end-effectors but also significantly improves viewpoint invariance. Furthermore, it exhibits strong generalization capabilities across long-horizon and out-of-distribution tasks, including interacting with deformable objects. In real-world evaluations, the world model with TC-IDM achieves an average success rate of 61.11 percent, with 77.7 percent on simple tasks and 38.46 percent on zero-shot deformable object tasks. It substantially outperforms end-to-end VLA-style baselines and other inverse dynamics models."
    },
    {
      "title": "Fast and Safe Trajectory Optimization for Mobile Manipulators With Neural Configuration Space Distance Field",
      "url": "https://arxiv.org/abs/2601.18548",
      "date": "2026-01-26",
      "authors_text": "Yulin Li, Zhiyuan Song, Yiming Li, Zhicheng Song, Kai Chen",
      "is_highlight": false,
      "score": 86.0,
      "summary": "This paper introduces Generalized Configuration Space Distance Fields (GCDF) for efficient trajectory optimization in mobile manipulators, enhancing collision reasoning and enabling agile motion in complex environments.",
      "debug_abstract": "Mobile manipulators promise agile, long-horizon behavior by coordinating base and arm motion, yet whole-body trajectory optimization in cluttered, confined spaces remains difficult due to high-dimensional nonconvexity and the need for fast, accurate collision reasoning. Configuration Space Distance Fields (CDF) enable fixed-base manipulators to model collisions directly in configuration space via smooth, implicit distances. This representation holds strong potential to bypass the nonlinear configuration-to-workspace mapping while preserving accurate whole-body geometry and providing optimization-friendly collision costs. Yet, extending this capability to mobile manipulators is hindered by unbounded workspaces and tighter base-arm coupling. We lift this promise to mobile manipulation with Generalized Configuration Space Distance Fields (GCDF), extending CDF to robots with both translational and rotational joints in unbounded workspaces with tighter base-arm coupling. We prove that GCDF preserves Euclidean-like local distance structure and accurately encodes whole-body geometry in configuration space, and develop a data generation and training pipeline that yields continuous neural GCDFs with accurate values and gradients, supporting efficient GPU-batched queries. Building on this representation, we develop a high-performance sequential convex optimization framework centered on GCDF-based collision reasoning. The solver scales to large numbers of implicit constraints through (i) online specification of neural constraints, (ii) sparsity-aware active-set detection with parallel batched evaluation across thousands of constraints, and (iii) incremental constraint management for rapid replanning under scene changes."
    },
    {
      "title": "EvoCUA: Evolving Computer Use Agents via Learning from Scalable Synthetic Experience",
      "url": "https://arxiv.org/abs/2601.15876",
      "date": "2026-01-22",
      "authors_text": "Taofeng Xue, Chong Peng, Mianqiu Huang, Linsen Guo, Tiancheng Han",
      "is_highlight": false,
      "score": 82.6,
      "summary": "EvoCUA introduces a self-sustaining evolutionary model for computer-use agents, leveraging autonomous task generation and iterative learning to significantly outperform existing benchmarks in multimodal AI.",
      "debug_abstract": "The development of native computer-use agents (CUA) represents a significant leap in multimodal AI. However, their potential is currently bottlenecked by the constraints of static data scaling. Existing paradigms relying primarily on passive imitation of static datasets struggle to capture the intricate causal dynamics inherent in long-horizon computer tasks. In this work, we introduce EvoCUA, a native computer use agentic model. Unlike static imitation, EvoCUA integrates data generation and policy optimization into a self-sustaining evolutionary cycle. To mitigate data scarcity, we develop a verifiable synthesis engine that autonomously generates diverse tasks coupled with executable validators. To enable large-scale experience acquisition, we design a scalable infrastructure orchestrating tens of thousands of asynchronous sandbox rollouts. Building on these massive trajectories, we propose an iterative evolving learning strategy to efficiently internalize this experience. This mechanism dynamically regulates policy updates by identifying capability boundaries -- reinforcing successful routines while transforming failure trajectories into rich supervision through error analysis and self-correction. Empirical evaluations on the OSWorld benchmark demonstrate that EvoCUA achieves a success rate of 56.7%, establishing a new open-source state-of-the-art. Notably, EvoCUA significantly outperforms the previous best open-source model, OpenCUA-72B (45.0%), and surpasses leading closed-weights models such as UI-TARS-2 (53.1%). Crucially, our results underscore the generalizability of this approach: the evolving paradigm driven by learning from experience yields consistent performance gains across foundation models of varying scales, establishing a robust and scalable path for advancing native agent capabilities."
    },
    {
      "title": "DualShield: Safe Model Predictive Diffusion via Reachability Analysis for Interactive Autonomous Driving",
      "url": "https://arxiv.org/abs/2601.15729",
      "date": "2026-01-22",
      "authors_text": "Rui Yang, Lei Zheng, Ruoyu Yao, Jun Ma",
      "is_highlight": true,
      "score": 92.7,
      "summary": "DualShield enhances autonomous driving safety by integrating Hamilton-Jacobi reachability analysis into diffusion models for proactive guidance and reactive safety assurance under uncertain interactions.",
      "debug_abstract": "Diffusion models have emerged as a powerful approach for multimodal motion planning in autonomous driving. However, their practical deployment is typically hindered by the inherent difficulty in enforcing vehicle dynamics and a critical reliance on accurate predictions of other agents, making them prone to safety issues under uncertain interactions. To address these limitations, we introduce DualShield, a planning and control framework that leverages Hamilton-Jacobi (HJ) reachability value functions in a dual capacity. First, the value functions act as proactive guidance, steering the diffusion denoising process towards safe and dynamically feasible regions. Second, they form a reactive safety shield using control barrier-value functions (CBVFs) to modify the executed actions and ensure safety. This dual mechanism preserves the rich exploration capabilities of diffusion models while providing principled safety assurance under uncertain and even adversarial interactions. Simulations in challenging unprotected U-turn scenarios demonstrate that DualShield significantly improves both safety and task efficiency compared to leading methods from different planning paradigms under uncertainty."
    }
  ],
  "武汉大学": [
    {
      "title": "IVC-Prune: Revealing the Implicit Visual Coordinates in LVLMs for Vision Token Pruning",
      "url": "https://arxiv.org/abs/2602.03060",
      "date": "2026-02-03",
      "authors_text": "Zhichao Sun, Yidong Ma, Gang Liu, Yibo Chen, Xu Tang",
      "is_highlight": true,
      "score": 62.0,
      "summary": "IVC-Prune enhances visual token pruning in LVLMs by preserving essential implicit visual coordinates and semantically relevant tokens, achieving significant reductions in token count without performance loss.",
      "teaser_image": "https://arxiv.org/html/2602.03060/x2.png",
      "debug_abstract": "Large Vision-Language Models (LVLMs) achieve impressive performance across multiple tasks. A significant challenge, however, is their prohibitive inference cost when processing high-resolution visual inputs. While visual token pruning has emerged as a promising solution, existing methods that primarily focus on semantic relevance often discard tokens that are crucial for spatial reasoning. We address this gap through a novel insight into \\emph{how LVLMs process spatial reasoning}. Specifically, we reveal that LVLMs implicitly establish visual coordinate systems through Rotary Position Embeddings (RoPE), where specific token positions serve as \\textbf{implicit visual coordinates} (IVC tokens) that are essential for spatial reasoning. Based on this insight, we propose \\textbf{IVC-Prune}, a training-free, prompt-aware pruning strategy that retains both IVC tokens and semantically relevant foreground tokens. IVC tokens are identified by theoretically analyzing the mathematical properties of RoPE, targeting positions at which its rotation matrices approximate identity matrix or the $90^\\circ$ rotation matrix. Foreground tokens are identified through a robust two-stage process: semantic seed discovery followed by contextual refinement via value-vector similarity. Extensive evaluations across four representative LVLMs and twenty diverse benchmarks show that IVC-Prune reduces visual tokens by approximately 50\\% while maintaining $\\geq$ 99\\% of the original performance and even achieving improvements on several benchmarks. Source codes are available at this https URL."
    },
    {
      "title": "When Attention Betrays: Erasing Backdoor Attacks in Robotic Policies by Reconstructing Visual Tokens",
      "url": "https://arxiv.org/abs/2602.03153",
      "date": "2026-02-03",
      "authors_text": "Xuetao Li, Pinhan Fu, Wenke Huang, Nengyuan Pan, Songhua Yang",
      "is_highlight": false,
      "score": 67.0,
      "summary": "The paper presents Bera, a novel framework that detects and erases backdoor attacks in vision-language-action models without retraining, ensuring robust robotic behavior.",
      "teaser_image": "https://arxiv.org/html/2602.03153/x4.png",
      "debug_abstract": "Downstream fine-tuning of vision-language-action (VLA) models enhances robotics, yet exposes the pipeline to backdoor risks. Attackers can pretrain VLAs on poisoned data to implant backdoors that remain stealthy but can trigger harmful behavior during inference. However, existing defenses either lack mechanistic insight into multimodal backdoors or impose prohibitive computational costs via full-model retraining. To this end, we uncover a deep-layer attention grabbing mechanism: backdoors redirect late-stage attention and form compact embedding clusters near the clean manifold. Leveraging this insight, we introduce Bera, a test-time backdoor erasure framework that detects tokens with anomalous attention via latent-space localization, masks suspicious regions using deep-layer cues, and reconstructs a trigger-free image to break the trigger-unsafe-action mapping while restoring correct behavior. Unlike prior defenses, Bera requires neither retraining of VLAs nor any changes to the training pipeline. Extensive experiments across multiple embodied platforms and tasks show that Bera effectively maintains nominal performance, significantly reduces attack success rates, and consistently restores benign behavior from backdoored outputs, thereby offering a robust and practical defense mechanism for securing robotic systems."
    },
    {
      "title": "Agentic Proposing: Enhancing Large Language Model Reasoning via Compositional Skill Synthesis",
      "url": "https://arxiv.org/abs/2602.03279",
      "date": "2026-02-03",
      "authors_text": "Zhengbo Jiao, Shaobo Wang, Zifan Zhang, Xuan Ren, Wei Wang",
      "is_highlight": false,
      "score": 71.0,
      "summary": "The paper introduces Agentic Proposing, a framework for generating high-quality synthetic training data that enhances reasoning in large language models, achieving superior performance with minimal human input.",
      "teaser_image": "https://arxiv.org/html/2602.03279/x2.png",
      "debug_abstract": "Advancing complex reasoning in large language models relies on high-quality, verifiable datasets, yet human annotation remains cost-prohibitive and difficult to scale. Current synthesis paradigms often face a recurring trade-off: maintaining structural validity typically restricts problem complexity, while relaxing constraints to increase difficulty frequently leads to inconsistent or unsolvable instances. To address this, we propose Agentic Proposing, a framework that models problem synthesis as a goal-driven sequential decision process where a specialized agent dynamically selects and composes modular reasoning skills. Through an iterative workflow of internal reflection and tool-use, we develop the Agentic-Proposer-4B using Multi-Granularity Policy Optimization (MGPO) to generate high-precision, verifiable training trajectories across mathematics, coding, and science. Empirical results demonstrate that downstream solvers trained on agent-synthesized data significantly outperform leading baselines and exhibit robust cross-domain generalization. Notably, a 30B solver trained on only 11,000 synthesized trajectories achieves a state-of-the-art 91.6% accuracy on AIME25, rivaling frontier-scale proprietary models such as GPT-5 and proving that a small volume of high-quality synthetic signals can effectively substitute for massive human-curated datasets."
    },
    {
      "title": "Socratic-Geo: Synthetic Data Generation and Geometric Reasoning via Multi-Agent Interaction",
      "url": "https://arxiv.org/abs/2602.03414",
      "date": "2026-02-03",
      "authors_text": "Zhengbo Jiao, Shaobo Wang, Zifan Zhang, Wei Wang, Bing Zhao",
      "is_highlight": false,
      "score": 58.0,
      "summary": "Socratic-Geo introduces a multi-agent framework for autonomous data synthesis and geometric reasoning, significantly improving model performance with minimal seed problems and surpassing existing benchmarks.",
      "teaser_image": "https://arxiv.org/html/2602.03414/x1.png",
      "debug_abstract": "Multimodal Large Language Models (MLLMs) have significantly advanced vision-language understanding. However, even state-of-the-art models struggle with geometric reasoning, revealing a critical bottleneck: the extreme scarcity of high-quality image-text pairs. Human annotation is prohibitively expensive, while automated methods fail to ensure fidelity and training effectiveness. Existing approaches either passively adapt to available images or employ inefficient random exploration with filtering, decoupling generation from learning needs. We propose Socratic-Geo, a fully autonomous framework that dynamically couples data synthesis with model learning through multi-agent interaction. The Teacher agent generates parameterized Python scripts with reflective feedback (Reflect for solvability, RePI for visual validity), ensuring image-text pair purity. The Solver agent optimizes reasoning through preference learning, with failure paths guiding Teacher&#39;s targeted augmentation. Independently, the Generator learns image generation capabilities on accumulated &#34;image-code-instruction&#34; triplets, distilling programmatic drawing intelligence into visual generation. Starting from only 108 seed problems, Socratic-Solver achieves 49.11 on six benchmarks using one-quarter of baseline data, surpassing strong baselines by 2.43 points. Socratic-Generator achieves 42.4% on GenExam, establishing new state-of-the-art for open-source models, surpassing Seedream-4.0 (39.8%) and approaching Gemini-2.5-Flash-Image (43.1%)."
    },
    {
      "title": "Interacted Planes Reveal 3D Line Mapping",
      "url": "https://arxiv.org/abs/2602.01296",
      "date": "2026-02-01",
      "authors_text": "Zeran Ke, Bin Tan, Gui-Song Xia, Yujun Shen, Nan Xue",
      "is_highlight": false,
      "score": 59.0,
      "summary": "LiP-Map introduces a joint optimization framework for 3D line mapping that integrates line and planar primitives, enhancing accuracy and efficiency in scene reconstruction.",
      "teaser_image": "https://arxiv.org/html/2602.01296/x2.png",
      "debug_abstract": "3D line mapping from multi-view RGB images provides a compact and structured visual representation of scenes. We study the problem from a physical and topological perspective: a 3D line most naturally emerges as the edge of a finite 3D planar patch. We present LiP-Map, a line-plane joint optimization framework that explicitly models learnable line and planar primitives. This coupling enables accurate and detailed 3D line mapping while maintaining strong efficiency (typically completing a reconstruction in 3 to 5 minutes per scene). LiP-Map pioneers the integration of planar topology into 3D line mapping, not by imposing pairwise coplanarity constraints but by explicitly constructing interactions between plane and line primitives, thus offering a principled route toward structured reconstruction in man-made environments. On more than 100 scenes from ScanNetV2, ScanNet++, Hypersim, 7Scenes, and Tanks\\&amp;Temple, LiP-Map improves both accuracy and completeness over state-of-the-art methods. Beyond line mapping quality, LiP-Map significantly advances line-assisted visual localization, establishing strong performance on 7Scenes. Our code is released at this https URL for reproducible research."
    },
    {
      "title": "MagicFuse: Single Image Fusion for Visual and Semantic Reinforcement",
      "url": "https://arxiv.org/abs/2602.01760",
      "date": "2026-02-02",
      "authors_text": "Hao Zhang, Yanping Zha, Zizhuo Li, Meiqi Gong, Jiayi Ma",
      "is_highlight": true,
      "score": 36.0,
      "summary": "MagicFuse is a novel framework that enables effective cross-spectral scene representation from a single low-quality visible image using knowledge-level fusion techniques.",
      "teaser_image": "https://arxiv.org/html/2602.01760/x2.png",
      "debug_abstract": "This paper focuses on a highly practical scenario: how to continue benefiting from the advantages of multi-modal image fusion under harsh conditions when only visible imaging sensors are available. To achieve this goal, we propose a novel concept of single-image fusion, which extends conventional data-level fusion to the knowledge level. Specifically, we develop MagicFuse, a novel single image fusion framework capable of deriving a comprehensive cross-spectral scene representation from a single low-quality visible image. MagicFuse first introduces an intra-spectral knowledge reinforcement branch and a cross-spectral knowledge generation branch based on the diffusion models. They mine scene information obscured in the visible spectrum and learn thermal radiation distribution patterns transferred to the infrared spectrum, respectively. Building on them, we design a multi-domain knowledge fusion branch that integrates the probabilistic noise from the diffusion streams of these two branches, from which a cross-spectral scene representation can be obtained through successive sampling. Then, we impose both visual and semantic constraints to ensure that this scene representation can satisfy human observation while supporting downstream semantic decision-making. Extensive experiments show that our MagicFuse achieves visual and semantic representation performance comparable to or even better than state-of-the-art fusion methods with multi-modal inputs, despite relying solely on a single degraded visible image."
    },
    {
      "title": "4D-CAAL: 4D Radar-Camera Calibration and Auto-Labeling for Autonomous Driving",
      "url": "https://arxiv.org/abs/2601.21454",
      "date": "2026-01-29",
      "authors_text": "Shanliang Yao, Zhuoxiao Li, Runwei Guan, Kebin Cao, Meng Xia",
      "is_highlight": false,
      "score": 89.0,
      "summary": "The 4D-CAAL framework integrates 4D radar-camera calibration and auto-labeling using a dual-purpose target and robust correspondence matching, enhancing autonomous driving perception efficiency.",
      "teaser_image": "https://arxiv.org/html/2601.21454/images/4DRC-fixed.png",
      "debug_abstract": "4D radar has emerged as a critical sensor for autonomous driving, primarily due to its enhanced capabilities in elevation measurement and higher resolution compared to traditional 3D radar. Effective integration of 4D radar with cameras requires accurate extrinsic calibration, and the development of radar-based perception algorithms demands large-scale annotated datasets. However, existing calibration methods often employ separate targets optimized for either visual or radar modalities, complicating correspondence establishment. Furthermore, manually labeling sparse radar data is labor-intensive and unreliable. To address these challenges, we propose 4D-CAAL, a unified framework for 4D radar-camera calibration and auto-labeling. Our approach introduces a novel dual-purpose calibration target design, integrating a checkerboard pattern on the front surface for camera detection and a corner reflector at the center of the back surface for radar detection. We develop a robust correspondence matching algorithm that aligns the checkerboard center with the strongest radar reflection point, enabling accurate extrinsic calibration. Subsequently, we present an auto-labeling pipeline that leverages the calibrated sensor relationship to transfer annotations from camera-based segmentations to radar point clouds through geometric projection and multi-feature optimization. Extensive experiments demonstrate that our method achieves high calibration accuracy while significantly reducing manual annotation effort, thereby accelerating the development of robust multi-modal perception systems for autonomous driving."
    },
    {
      "title": "Policy of Thoughts: Scaling LLM Reasoning via Test-time Policy Evolution",
      "url": "https://arxiv.org/abs/2601.20379",
      "date": "2026-01-28",
      "authors_text": "Zhengbo Jiao, Hongyu Xian, Qinglong Wang, Yunpu Ma, Zhebo Wang",
      "is_highlight": false,
      "score": 74.0,
      "summary": "The Policy of Thoughts framework enhances LLM reasoning by enabling real-time policy evolution through execution feedback, significantly improving performance on complex tasks.",
      "teaser_image": "https://arxiv.org/html/2601.20379/x1.png",
      "debug_abstract": "Large language models (LLMs) struggle with complex, long-horizon reasoning due to instability caused by their frozen policy assumption. Current test-time scaling methods treat execution feedback merely as an external signal for filtering or rewriting trajectories, without internalizing it to improve the underlying reasoning strategy. Inspired by Popper&#39;s epistemology of &#34;conjectures and refutations,&#34; we argue that intelligence requires real-time evolution of the model&#39;s policy through learning from failed attempts. We introduce Policy of Thoughts (PoT), a framework that recasts reasoning as a within-instance online optimization process. PoT first generates diverse candidate solutions via an efficient exploration mechanism, then uses Group Relative Policy Optimization (GRPO) to update a transient LoRA adapter based on execution feedback. This closed-loop design enables dynamic, instance-specific refinement of the model&#39;s reasoning priors. Experiments show that PoT dramatically boosts performance: a 4B model achieves 49.71% accuracy on LiveCodeBench, outperforming GPT-4o and DeepSeek-V3 despite being over 50 smaller."
    },
    {
      "title": "Fast Converging 3D Gaussian Splatting for 1-Minute Reconstruction",
      "url": "https://arxiv.org/abs/2601.19489",
      "date": "2026-01-27",
      "authors_text": "Ziyu Zhang, Tianle Liu, Diantao Tu, Shuhan Shen",
      "is_highlight": false,
      "score": 60.0,
      "summary": "The paper presents a rapid 3D Gaussian Splatting reconstruction method that achieves high-fidelity results within one minute, winning the SIGGRAPH Asia 3DGS Fast Reconstruction Challenge.",
      "debug_abstract": "We present a fast 3DGS reconstruction pipeline designed to converge within one minute, developed for the SIGGRAPH Asia 3DGS Fast Reconstruction Challenge. The challenge consists of an initial round using SLAM-generated camera poses (with noisy trajectories) and a final round using COLMAP poses (highly accurate). To robustly handle these heterogeneous settings, we develop a two-stage solution. In the first round, we use reverse per-Gaussian parallel optimization and compact forward splatting based on Taming-GS and Speedy-splat, load-balanced tiling, an anchor-based Neural-Gaussian representation enabling rapid convergence with fewer learnable parameters, initialization from monocular depth and partially from feed-forward 3DGS models, and a global pose refinement module for noisy SLAM trajectories. In the final round, the accurate COLMAP poses change the optimization landscape; we disable pose refinement, revert from Neural-Gaussians back to standard 3DGS to eliminate MLP inference overhead, introduce multi-view consistency-guided Gaussian splitting inspired by Fast-GS, and introduce a depth estimator to supervise the rendered depth. Together, these techniques enable high-fidelity reconstruction under a strict one-minute budget. Our method achieved the top performance with a PSNR of 28.43 and ranked first in the competition."
    },
    {
      "title": "Physical Prompt Injection Attacks on Large Vision-Language Models",
      "url": "https://arxiv.org/abs/2601.17383",
      "date": "2026-01-24",
      "authors_text": "Chen Ling, Kai Hu, Hangcheng Liu, Xingshuo Han, Tianwei Zhang",
      "is_highlight": false,
      "score": 55.0,
      "summary": "The paper introduces Physical Prompt Injection Attacks (PPIA), a novel, highly effective method for manipulating Large Vision-Language Models using visually embedded prompts in physical environments.",
      "debug_abstract": "Large Vision-Language Models (LVLMs) are increasingly deployed in real-world intelligent systems for perception and reasoning in open physical environments. While LVLMs are known to be vulnerable to prompt injection attacks, existing methods either require access to input channels or depend on knowledge of user queries, assumptions that rarely hold in practical deployments. We propose the first Physical Prompt Injection Attack (PPIA), a black-box, query-agnostic attack that embeds malicious typographic instructions into physical objects perceivable by the LVLM. PPIA requires no access to the model, its inputs, or internal pipeline, and operates solely through visual observation. It combines offline selection of highly recognizable and semantically effective visual prompts with strategic environment-aware placement guided by spatiotemporal attention, ensuring that the injected prompts are both perceivable and influential on model behavior. We evaluate PPIA across 10 state-of-the-art LVLMs in both simulated and real-world settings on tasks including visual question answering, planning, and navigation, PPIA achieves attack success rates up to 98%, with strong robustness under varying physical conditions such as distance, viewpoint, and illumination. Our code is publicly available at this https URL."
    },
    {
      "title": "EFSI-DETR: Efficient Frequency-Semantic Integration for Real-Time Small Object Detection in UAV Imagery",
      "url": "https://arxiv.org/abs/2601.18597",
      "date": "2026-01-26",
      "authors_text": "Yu Xia, Chang Liu, Tianqi Xiang, Zhigang Tu",
      "is_highlight": false,
      "score": 52.0,
      "summary": "EFSI-DETR enhances real-time small object detection in UAV imagery by integrating dynamic frequency-spatial cues and efficient semantic feature extraction, achieving state-of-the-art performance.",
      "debug_abstract": "Real-time small object detection in Unmanned Aerial Vehicle (UAV) imagery remains challenging due to limited feature representation and ineffective multi-scale fusion. Existing methods underutilize frequency information and rely on static convolutional operations, which constrain the capacity to obtain rich feature representations and hinder the effective exploitation of deep semantic features. To address these issues, we propose EFSI-DETR, a novel detection framework that integrates efficient semantic feature enhancement with dynamic frequency-spatial guidance. EFSI-DETR comprises two main components: (1) a Dynamic Frequency-Spatial Unified Synergy Network (DyFusNet) that jointly exploits frequency and spatial cues for robust multi-scale feature fusion, (2) an Efficient Semantic Feature Concentrator (ESFC) that enables deep semantic extraction with minimal computational cost. Furthermore, a Fine-grained Feature Retention (FFR) strategy is adopted to incorporate spatially rich shallow features during fusion to preserve fine-grained details, crucial for small object detection in UAV imagery. Extensive experiments on VisDrone and CODrone benchmarks demonstrate that our EFSI-DETR achieves the state-of-the-art performance with real-time efficiency, yielding improvement of \\textbf{1.6}\\% and \\textbf{5.8}\\% in AP and AP$_{s}$ on VisDrone, while obtaining \\textbf{188} FPS inference speed on a single RTX 4090 GPU."
    },
    {
      "title": "AnchoredDream: Zero-Shot 360° Indoor Scene Generation from a Single View via Geometric Grounding",
      "url": "https://arxiv.org/abs/2601.16532",
      "date": "2026-01-23",
      "authors_text": "Runmao Yao, Junsheng Zhou, Zhen Dong, Yu-Shen Liu",
      "is_highlight": false,
      "score": 72.0,
      "summary": "AnchoredDream introduces a zero-shot method for generating 360° indoor scenes from a single image, enhancing appearance consistency and geometric plausibility through a mutual boosting mechanism.",
      "debug_abstract": "Single-view indoor scene generation plays a crucial role in a range of real-world applications. However, generating a complete 360° scene from a single image remains a highly ill-posed and challenging problem. Recent approaches have made progress by leveraging diffusion models and depth estimation networks, yet they still struggle to maintain appearance consistency and geometric plausibility under large viewpoint changes, limiting their effectiveness in full-scene generation. To address this, we propose AnchoredDream, a novel zero-shot pipeline that anchors 360° scene generation on high-fidelity geometry via an appearance-geometry mutual boosting mechanism. Given a single-view image, our method first performs appearance-guided geometry generation to construct a reliable 3D scene layout. Then, we progressively generate the complete scene through a series of modules: warp-and-inpaint, warp-and-refine, post-optimization, and a novel Grouting Block, which ensures seamless transitions between the input view and generated regions. Extensive experiments demonstrate that AnchoredDream outperforms existing methods by a large margin in both appearance consistency and geometric plausibility--all in a zero-shot manner. Our results highlight the potential of geometric grounding for high-quality, zero-shot single-view scene generation."
    },
    {
      "title": "VideoThinker: Building Agentic VideoLLMs with LLM-Guided Tool Reasoning",
      "url": "https://arxiv.org/abs/2601.15724",
      "date": "2026-01-22",
      "authors_text": "Chenglin Li, Qianglong Chen, Feng Han, Yikun Wang, Xingxi Yin",
      "is_highlight": false,
      "score": 89.6,
      "summary": "VideoThinker enhances long-form video understanding by utilizing synthetic tool interaction trajectories for adaptive reasoning, outperforming existing models through dynamic exploration and multi-step tool use.",
      "debug_abstract": "Long-form video understanding remains a fundamental challenge for current Video Large Language Models. Most existing models rely on static reasoning over uniformly sampled frames, which weakens temporal localization and leads to substantial information loss in long videos. Agentic tools such as temporal retrieval, spatial zoom, and temporal zoom offer a natural way to overcome these limitations by enabling adaptive exploration of key moments. However, constructing agentic video understanding data requires models that already possess strong long-form video comprehension, creating a circular dependency. We address this challenge with VideoThinker, an agentic Video Large Language Model trained entirely on synthetic tool interaction trajectories. Our key idea is to convert videos into rich captions and employ a powerful agentic language model to generate multi-step tool use sequences in caption space. These trajectories are subsequently grounded back to video by replacing captions with the corresponding frames, yielding a large-scale interleaved video and tool reasoning dataset without requiring any long-form understanding from the underlying model. Training on this synthetic agentic dataset equips VideoThinker with dynamic reasoning capabilities, adaptive temporal exploration, and multi-step tool use. Remarkably, VideoThinker significantly outperforms both caption-only language model agents and strong video model baselines across long-video benchmarks, demonstrating the effectiveness of tool augmented synthetic data and adaptive retrieval and zoom reasoning for long-form video understanding."
    },
    {
      "title": "SAMTok: Representing Any Mask with Two Words",
      "url": "https://arxiv.org/abs/2601.16093",
      "date": "2026-01-22",
      "authors_text": "Yikang Zhou, Tao Zhang, Dengxian Gong, Yuanzheng Wu, Ye Tian",
      "is_highlight": false,
      "score": 74.5,
      "summary": "SAMTok introduces a novel mask tokenizer that converts region masks into two tokens, enhancing multi-modal LLMs' pixel-wise capabilities through efficient training and reinforcement learning.",
      "debug_abstract": "Pixel-wise capabilities are essential for building interactive intelligent systems. However, pixel-wise multi-modal LLMs (MLLMs) remain difficult to scale due to complex region-level encoders, specialized segmentation decoders, and incompatible training objectives. To address these challenges, we present SAMTok, a discrete mask tokenizer that converts any region mask into two special tokens and reconstructs the mask using these tokens with high fidelity. By treating masks as new language tokens, SAMTok enables base MLLMs (such as the QwenVL series) to learn pixel-wise capabilities through standard next-token prediction and simple reinforcement learning, without architectural modifications and specialized loss design. SAMTok builds on SAM2 and is trained on 209M diverse masks using a mask encoder and residual vector quantizer to produce discrete, compact, and information-rich tokens. With 5M SAMTok-formatted mask understanding and generation data samples, QwenVL-SAMTok attains state-of-the-art or comparable results on region captioning, region VQA, grounded conversation, referring segmentation, scene graph parsing, and multi-round interactive segmentation. We further introduce a textual answer-matching reward that enables efficient reinforcement learning for mask generation, delivering substantial improvements on GRES and GCG benchmarks. Our results demonstrate a scalable and straightforward paradigm for equipping MLLMs with strong pixel-wise capabilities. Our code and models are available."
    },
    {
      "title": "Off-Policy Actor-Critic with Sigmoid-Bounded Entropy for Real-World Robot Learning",
      "url": "https://arxiv.org/abs/2601.15761",
      "date": "2026-01-22",
      "authors_text": "Xiefeng Wu, Mingyu Hu, Shu Zhang",
      "is_highlight": false,
      "score": 78.4,
      "summary": "The paper presents SigEnt-SAC, an efficient off-policy actor-critic method that utilizes a sigmoid-bounded entropy term to enhance real-world robot learning from minimal data.",
      "debug_abstract": "Deploying reinforcement learning in the real world remains challenging due to sample inefficiency, sparse rewards, and noisy visual observations. Prior work leverages demonstrations and human feedback to improve learning efficiency and robustness. However, offline-to-online methods need large datasets and can be unstable, while VLA-assisted RL relies on large-scale pretraining and fine-tuning. As a result, a low-cost real-world RL method with minimal data requirements has yet to emerge. We introduce \\textbf{SigEnt-SAC}, an off-policy actor-critic method that learns from scratch using a single expert trajectory. Our key design is a sigmoid-bounded entropy term that prevents negative-entropy-driven optimization toward out-of-distribution actions and reduces Q-function oscillations. We benchmark SigEnt-SAC on D4RL tasks against representative baselines. Experiments show that SigEnt-SAC substantially alleviates Q-function oscillations and reaches a 100\\% success rate faster than prior methods. Finally, we validate SigEnt-SAC on four real-world robotic tasks across multiple embodiments, where agents learn from raw images and sparse rewards; results demonstrate that SigEnt-SAC can learn successful policies with only a small number of real-world interactions, suggesting a low-cost and practical pathway for real-world RL deployment."
    }
  ],
  "S-Lab for Advanced Intelligence": [
    {
      "title": "Bongards at the Boundary of Perception and Reasoning: Programs or Language?",
      "url": "https://arxiv.org/abs/2602.03038",
      "date": "2026-02-03",
      "authors_text": "Cassidy Langenfeld, Claas Beger, Gloria Geng, Wasu Top Piriyakulkij, Keya Hu",
      "is_highlight": false,
      "score": 60.0,
      "summary": "This paper introduces a neurosymbolic method utilizing LLMs and Bayesian optimization to tackle Bongard problems, enhancing visual reasoning beyond conventional VLM capabilities.",
      "teaser_image": "https://arxiv.org/html/2602.03038/x3.png",
      "debug_abstract": "Vision-Language Models (VLMs) have made great strides in everyday visual tasks, such as captioning a natural image, or answering commonsense questions about such images. But humans possess the puzzling ability to deploy their visual reasoning abilities in radically new situations, a skill rigorously tested by the classic set of visual reasoning challenges known as the Bongard problems. We present a neurosymbolic approach to solving these problems: given a hypothesized solution rule for a Bongard problem, we leverage LLMs to generate parameterized programmatic representations for the rule and perform parameter fitting using Bayesian optimization. We evaluate our method on classifying Bongard problem images given the ground truth rule, as well as on solving the problems from scratch."
    },
    {
      "title": "Estimation of Ground Reaction Forces from Kinematic Data during Locomotion",
      "url": "https://arxiv.org/abs/2602.03177",
      "date": "2026-02-03",
      "authors_text": "Gautami Golani, Dong Anh Khoa To, Ananda Sidarta, Arun-Kumar Kaliya-Perumal, Oliver Roberts",
      "is_highlight": false,
      "score": 44.0,
      "summary": "This study introduces a force-plate-free method for estimating ground reaction forces from kinematic data, enhancing clinical gait analysis without the need for specialized equipment.",
      "teaser_image": "https://arxiv.org/html/2602.03177/x2.png",
      "debug_abstract": "Ground reaction forces (GRFs) provide fundamental insight into human gait mechanics and are widely used to assess joint loading, limb symmetry, balance control, and motor function. Despite their clinical relevance, the use of GRF remains underutilised in clinical workflows due to the practical limitations of force plate systems. In this work, we present a force-plate-free approach for estimating GRFs using only marker-based motion capture data. This kinematics only method to estimate and decompose GRF makes it well suited for widespread clinical depolyment. By using kinematics from sixteen body segments, we estimate the centre of mass (CoM) and compute GRFs, which are subsequently decomposed into individual components through a minimization-based approach. Through this framework, we can identify gait stance phases and provide access to clinically meaningful kinetic measures without a dedicated force plate system. Experimental results demonstrate the viability of CoM and GRF estimation based solely on kinematic data, supporting force-plate-free gait analysis."
    },
    {
      "title": "Self-Guard: Defending Large Reasoning Models via enhanced self-reflection",
      "url": "https://arxiv.org/abs/2602.00707",
      "date": "2026-01-31",
      "authors_text": "Jingnan Zheng, Jingjun Xu, Yanzhen Luo, Chenhang Cui, Gelei Deng",
      "is_highlight": false,
      "score": 50.0,
      "summary": "Self-Guard is a lightweight framework enhancing safety compliance in Large Reasoning Models by promoting self-reflection and mitigating risks without sacrificing model utility.",
      "teaser_image": "https://arxiv.org/html/2602.00707/materials/workflow.png",
      "debug_abstract": "The emergence of Large Reasoning Models (LRMs) introduces a new paradigm of explicit reasoning, enabling remarkable advances yet posing unique risks such as reasoning manipulation and information leakage. To mitigate these risks, current alignment strategies predominantly rely on heavy post-training paradigms or external interventions. However, these approaches are often computationally intensive and fail to address the inherent awareness-compliance gap, a critical misalignment where models recognize potential risks yet prioritize following user instructions due to their sycophantic tendencies. To address these limitations, we propose Self-Guard, a lightweight safety defense framework that reinforces safety compliance at the representational level. Self-Guard operates through two principal stages: (1) safety-oriented prompting, which activates the model&#39;s latent safety awareness to evoke spontaneous reflection, and (2) safety activation steering, which extracts the resulting directional shift in the hidden state space and amplifies it to ensure that safety compliance prevails over sycophancy during inference. Experiments demonstrate that Self-Guard effectively bridges the awareness-compliance gap, achieving robust safety performance without compromising model utility. Furthermore, Self-Guard exhibits strong generalization across diverse unseen risks and varying model scales, offering a cost-efficient solution for LRM safety alignment."
    },
    {
      "title": "Learning Adaptive Cross-Embodiment Visuomotor Policy with Contrastive Prompt Orchestration",
      "url": "https://arxiv.org/abs/2602.01040",
      "date": "2026-02-01",
      "authors_text": "Yuhang Zhang, Chao Yan, Jiaxi Yu, Jiaping Xiao, Mir Feroskhan",
      "is_highlight": false,
      "score": 92.0,
      "summary": "The paper presents ContrAstive Prompt Orchestration (CAPO), a method for improving adaptive visuomotor policies in diverse environments through hybrid contrastive learning and dynamic prompt orchestration.",
      "teaser_image": "https://arxiv.org/html/2602.01040/x4.png",
      "debug_abstract": "Learning adaptive visuomotor policies for embodied agents remains a formidable challenge, particularly when facing cross-embodiment variations such as diverse sensor configurations and dynamic properties. Conventional learning approaches often struggle to separate task-relevant features from domain-specific variations (e.g., lighting, field-of-view, and rotation), leading to poor sample efficiency and catastrophic failure in unseen environments. To bridge this gap, we propose ContrAstive Prompt Orchestration (CAPO), a novel approach for learning visuomotor policies that integrates contrastive prompt learning and adaptive prompt orchestration. For prompt learning, we devise a hybrid contrastive learning strategy that integrates visual, temporal action, and text objectives to establish a pool of learnable prompts, where each prompt induces a visual representation encapsulating fine-grained domain factors. Based on these learned prompts, we introduce an adaptive prompt orchestration mechanism that dynamically aggregates these prompts conditioned on current observations. This enables the agent to adaptively construct optimal state representations by identifying dominant domain factors instantaneously. Consequently, the policy optimization is effectively shielded from irrelevant interference, preventing the common issue of overfitting to source domains. Extensive experiments demonstrate that CAPO significantly outperforms state-of-the-art baselines in sample efficiency and asymptotic performance. Crucially, it exhibits superior zero-shot adaptation across unseen target domains characterized by drastic environmental (e.g., illumination) and physical shifts (e.g., field-of-view and rotation), validating its effectiveness as a viable solution for cross-embodiment visuomotor policy adaptation."
    },
    {
      "title": "Estimating Force Interactions of Deformable Linear Objects from their Shapes",
      "url": "https://arxiv.org/abs/2602.01085",
      "date": "2026-02-01",
      "authors_text": "Qi Jing Chen, Shilin Shan, Timothy Bretl, Quang-Cuong Pham",
      "is_highlight": false,
      "score": 83.0,
      "summary": "This paper presents a novel method for estimating external forces on deformable linear objects using shape data, enhancing robot interaction safety and efficiency without requiring force sensors.",
      "teaser_image": "https://arxiv.org/html/2602.01085/x2.png",
      "debug_abstract": "This work introduces an analytical approach for detecting and estimating external forces acting on deformable linear objects (DLOs) using only their observed shapes. In many robot-wire interaction tasks, contact occurs not at the end-effector but at other points along the robot&#39;s body. Such scenarios arise when robots manipulate wires indirectly (e.g., by nudging) or when wires act as passive obstacles in the environment. Accurately identifying these interactions is crucial for safe and efficient trajectory planning, helping to prevent wire damage, avoid restricted robot motions, and mitigate potential hazards. Existing approaches often rely on expensive external force-torque sensor or that contacts occur at the end-effector for accurate force estimation. Using wire shape information acquired from a depth camera and under the assumption that the wire is in or near its static equilibrium, our method estimates both the location and magnitude of external forces without additional prior knowledge. This is achieved by exploiting derived consistency conditions and solving a system of linear equations based on force-torque balance along the wire. The approach was validated through simulation, where it achieved high accuracy, and through real-world experiments, where accurate estimation was demonstrated in selected interaction scenarios."
    },
    {
      "title": "SGHA-Attack: Semantic-Guided Hierarchical Alignment for Transferable Targeted Attacks on Vision-Language Models",
      "url": "https://arxiv.org/abs/2602.01574",
      "date": "2026-02-02",
      "authors_text": "Haobo Wang, Weiqi Luo, Xiaojun Jia, Xiaochun Cao",
      "is_highlight": false,
      "score": 44.0,
      "summary": "SGHA-Attack enhances targeted adversarial attacks on vision-language models by leveraging multiple semantic references and aligning intermediate features for improved transferability and robustness.",
      "teaser_image": "https://arxiv.org/html/2602.01574/x1.png",
      "debug_abstract": "Large vision-language models (VLMs) are vulnerable to transfer-based adversarial perturbations, enabling attackers to optimize on surrogate models and manipulate black-box VLM outputs. Prior targeted transfer attacks often overfit surrogate-specific embedding space by relying on a single reference and emphasizing final-layer alignment, which underutilizes intermediate semantics and degrades transfer across heterogeneous VLMs. To address this, we propose SGHA-Attack, a Semantic-Guided Hierarchical Alignment framework that adopts multiple target references and enforces intermediate-layer consistency. Concretely, we generate a visually grounded reference pool by sampling a frozen text-to-image model conditioned on the target prompt, and then carefully select the Top-K most semantically relevant anchors under the surrogate to form a weighted mixture for stable optimization guidance. Building on these anchors, SGHA-Attack injects target semantics throughout the feature hierarchy by aligning intermediate visual representations at both global and spatial granularities across multiple depths, and by synchronizing intermediate visual and textual features in a shared latent subspace to provide early cross-modal supervision before the final projection. Extensive experiments on open-source and commercial black-box VLMs show that SGHA-Attack achieves stronger targeted transferability than prior methods and remains robust under preprocessing and purification defenses."
    },
    {
      "title": "ClueTracer: Question-to-Vision Clue Tracing for Training-Free Hallucination Suppression in Multimodal Reasoning",
      "url": "https://arxiv.org/abs/2602.02004",
      "date": "2026-02-02",
      "authors_text": "Gongli Xi, Kun Wang, Zeming Gao, Huahui Yi, Haolang Lu",
      "is_highlight": false,
      "score": 24.0,
      "summary": "ClueTracer is a training-free plugin that suppresses hallucinations in multimodal reasoning by tracing visual clue propagation, enhancing model performance across various architectures.",
      "teaser_image": "https://arxiv.org/html/2602.02004/x1.png",
      "debug_abstract": "Large multimodal reasoning models solve challenging visual problems via explicit long-chain inference: they gather visual clues from images and decode clues into textual tokens. Yet this capability also increases hallucinations, where the model generates content that is not supported by the input image or the question. To understand this failure mode, we identify \\emph{reasoning drift}: during clue gathering, the model over-focuses on question-irrelevant entities, diluting focus on task-relevant cues and gradually decoupling the reasoning trace from visual grounding. As a consequence, many inference-time localization or intervention methods developed for non-reasoning models fail to pinpoint the true clues in reasoning settings. Motivated by these insights, we introduce ClueRecall, a metric for assessing visual clue retrieval, and present ClueTracer, a training-free, parameter-free, and architecture-agnostic plugin for hallucination suppression. ClueTracer starts from the question and traces how key clues propagate along the model&#39;s reasoning pathway (question $\\rightarrow$ outputs $\\rightarrow$ visual tokens), thereby localizing task-relevant patches while suppressing spurious attention to irrelevant regions. Remarkably, \\textbf{without any additional training}, ClueTracer improves all \\textbf{reasoning} architectures (including \\texttt{R1-OneVision}, \\texttt{Ocean-R1}, \\texttt{MM-Eureka}, \\emph{etc}.) by $\\mathbf{1.21\\times}$ on reasoning benchmarks. When transferred to \\textbf{non-reasoning} settings, it yields a $\\mathbf{1.14\\times}$ gain."
    },
    {
      "title": "Superman: Unifying Skeleton and Vision for Human Motion Perception and Generation",
      "url": "https://arxiv.org/abs/2602.02401",
      "date": "2026-02-02",
      "authors_text": "Xinshun Wang, Peiming Li, Ziyi Wang, Zhongbin Fang, Zhichao Deng",
      "is_highlight": false,
      "score": 15.0,
      "summary": "Superman unifies visual perception and skeleton-based motion generation through a Vision-Guided Motion Tokenizer, enabling robust cross-modal learning and superior performance in human motion tasks.",
      "teaser_image": "https://arxiv.org/html/2602.02401/x2.png",
      "debug_abstract": "Human motion analysis tasks, such as temporal 3D pose estimation, motion prediction, and motion in-betweening, play an essential role in computer vision. However, current paradigms suffer from severe fragmentation. First, the field is split between ``perception&#39;&#39; models that understand motion from video but only output text, and ``generation&#39;&#39; models that cannot perceive from raw visual input. Second, generative MLLMs are often limited to single-frame, static poses using dense, parametric SMPL models, failing to handle temporal motion. Third, existing motion vocabularies are built from skeleton data alone, severing the link to the visual domain. To address these challenges, we introduce Superman, a unified framework that bridges visual perception with temporal, skeleton-based motion generation. Our solution is twofold. First, to overcome the modality disconnect, we propose a Vision-Guided Motion Tokenizer. Leveraging the natural geometric alignment between 3D skeletons and visual data, this module pioneers robust joint learning from both modalities, creating a unified, cross-modal motion vocabulary. Second, grounded in this motion language, a single, unified MLLM architecture is trained to handle all tasks. This module flexibly processes diverse, temporal inputs, unifying 3D skeleton pose estimation from video (perception) with skeleton-based motion prediction and in-betweening (generation). Extensive experiments on standard benchmarks, including Human3.6M, demonstrate that our unified method achieves state-of-the-art or competitive performance across all motion tasks. This showcases a more efficient and scalable path for generative motion analysis using skeletons."
    },
    {
      "title": "Failure-Aware Bimanual Teleoperation via Conservative Value Guided Assistance",
      "url": "https://arxiv.org/abs/2602.01092",
      "date": "2026-02-01",
      "authors_text": "Peng Zhou, Zhongxuan Li, Jinsong Wu, Jiaming Qi, Jun Hu",
      "is_highlight": false,
      "score": 9.0,
      "summary": "This paper presents a failure-aware bimanual teleoperation framework that uses conservative value learning to enhance task success and reduce operator workload through compliant haptic assistance.",
      "teaser_image": "https://arxiv.org/html/2602.01092/x1.png",
      "debug_abstract": "Teleoperation of high-precision manipulation is con-strained by tight success tolerances and complex contact dy-namics, which make impending failures difficult for human operators to anticipate under partial observability. This paper proposes a value-guided, failure-aware framework for bimanual teleoperation that provides compliant haptic assistance while pre-serving continuous human authority. The framework is trained entirely from heterogeneous offline teleoperation data containing both successful and failed executions. Task feasibility is mod-eled as a conservative success score learned via Conservative Value Learning, yielding a risk-sensitive estimate that remains reliable under distribution shift. During online operation, the learned success score regulates the level of assistance, while a learned actor provides a corrective motion direction. Both are integrated through a joint-space impedance interface on the master side, yielding continuous guidance that steers the operator away from failure-prone actions without overriding intent. Experimental results on contact-rich manipulation tasks demonstrate improved task success rates and reduced operator workload compared to conventional teleoperation and shared-autonomy baselines, indicating that conservative value learning provides an effective mechanism for embedding failure awareness into bilateral teleoperation. Experimental videos are available at this https URL"
    },
    {
      "title": "UV-M3TL: A Unified and Versatile Multimodal Multi-Task Learning Framework for Assistive Driving Perception",
      "url": "https://arxiv.org/abs/2602.01594",
      "date": "2026-02-02",
      "authors_text": "Wenzhuo Liu, Qiannan Guo, Zhen Wang, Wenshuo Wang, Lei Yang",
      "is_highlight": false,
      "score": 8.0,
      "summary": "The UV-M3TL framework enhances ADAS by effectively recognizing driver behavior and context through dual-branch multimodal embedding and adaptive loss, achieving state-of-the-art performance across multiple tasks.",
      "teaser_image": "https://arxiv.org/html/2602.01594/x2.png",
      "debug_abstract": "Advanced Driver Assistance Systems (ADAS) need to understand human driver behavior while perceiving their navigation context, but jointly learning these heterogeneous tasks would cause inter-task negative transfer and impair system performance. Here, we propose a Unified and Versatile Multimodal Multi-Task Learning (UV-M3TL) framework to simultaneously recognize driver behavior, driver emotion, vehicle behavior, and traffic context, while mitigating inter-task negative transfer. Our framework incorporates two core components: dual-branch spatial channel multimodal embedding (DB-SCME) and adaptive feature-decoupled multi-task loss (AFD-Loss). DB-SCME enhances cross-task knowledge transfer while mitigating task conflicts by employing a dual-branch structure to explicitly model salient task-shared and task-specific features. AFD-Loss improves the stability of joint optimization while guiding the model to learn diverse multi-task representations by introducing an adaptive weighting mechanism based on learning dynamics and feature decoupling constraints. We evaluate our method on the AIDE dataset, and the experimental results demonstrate that UV-M3TL achieves state-of-the-art performance across all four tasks. To further prove the versatility, we evaluate UV-M3TL on additional public multi-task perception benchmarks (BDD100K, CityScapes, NYUD-v2, and PASCAL-Context), where it consistently delivers strong performance across diverse task combinations, attaining state-of-the-art results on most tasks."
    },
    {
      "title": "FlowSteer: Interactive Agentic Workflow Orchestration via End-to-End Reinforcement Learning",
      "url": "https://arxiv.org/abs/2602.01664",
      "date": "2026-02-02",
      "authors_text": "Mingda Zhang, Haoran Luo, Tiesunlong Shen, Qika Lin, Xiaoying Tang",
      "is_highlight": false,
      "score": 7.0,
      "summary": "FlowSteer is an end-to-end reinforcement learning framework that automates workflow orchestration through interactive learning, addressing manual costs and improving performance across diverse tasks.",
      "teaser_image": "https://arxiv.org/html/2602.01664/x1.png",
      "debug_abstract": "In recent years, a variety of powerful agentic workflows have been applied to solve a wide range of human problems. However, existing workflow orchestration still faces key challenges, including high manual cost, reliance on specific operators/large language models (LLMs), and sparse reward signals. To address these challenges, we propose FlowSteer, an end-to-end reinforcement learning framework that takes a lightweight policy model as the agent and an executable canvas environment, automating workflow orchestration through multi-turn interaction. In this process, the policy model analyzes execution states and selects editing actions, while the canvas executes operators and returns feedback for iterative refinement. Moreover, FlowSteer provides a plug-and-play framework that supports diverse operator libraries and interchangeable LLM backends. To effectively train this interaction paradigm, we propose Canvas Workflow Relative Policy Optimization (CWRPO), which introduces diversity-constrained rewards with conditional release to stabilize learning and suppress shortcut behaviors. Experimental results on twelve datasets show that FlowSteer significantly outperforms baselines across various tasks."
    },
    {
      "title": "FD-VLA: Force-Distilled Vision-Language-Action Model for Contact-Rich Manipulation",
      "url": "https://arxiv.org/abs/2602.02142",
      "date": "2026-02-02",
      "authors_text": "Ruiteng Zhao, Wenshuo Wang, Yicheng Ma, Xiaocong Li, Francis E. H. Tay",
      "is_highlight": false,
      "score": 0,
      "summary": "The FD-VLA framework enhances contact-rich manipulation by integrating force awareness through a novel distillation method, improving performance without physical force sensors.",
      "teaser_image": "https://arxiv.org/html/2602.02142/x1.png",
      "debug_abstract": "Force sensing is a crucial modality for Vision-Language-Action (VLA) frameworks, as it enables fine-grained perception and dexterous manipulation in contact-rich tasks. We present Force-Distilled VLA (FD-VLA), a novel framework that integrates force awareness into contact-rich manipulation without relying on physical force sensors. The core of our approach is a Force Distillation Module (FDM), which distills force by mapping a learnable query token, conditioned on visual observations and robot states, into a predicted force token aligned with the latent representation of actual force signals. During inference, this distilled force token is injected into the pretrained VLM, enabling force-aware reasoning while preserving the integrity of its vision-language semantics. This design provides two key benefits: first, it allows practical deployment across a wide range of robots that lack expensive or fragile force-torque sensors, thereby reducing hardware cost and complexity; second, the FDM introduces an additional force-vision-state fusion prior to the VLM, which improves cross-modal alignment and enhances perception-action robustness in contact-rich scenarios. Surprisingly, our physical experiments show that the distilled force token outperforms direct sensor force measurements as well as other baselines, which highlights the effectiveness of this force-distilled VLA approach."
    },
    {
      "title": "DynamicVLA: A Vision-Language-Action Model for Dynamic Object Manipulation",
      "url": "https://arxiv.org/abs/2601.22153",
      "date": "2026-01-29",
      "authors_text": "Haozhe Xie, Beichen Wen, Jiarui Zheng, Zhaoxi Chen, Fangzhou Hong",
      "is_highlight": false,
      "score": 93.0,
      "summary": "DynamicVLA is a novel Vision-Language-Action framework that enhances dynamic object manipulation through efficient encoding, continuous inference, and a new benchmark for data collection.",
      "teaser_image": "https://arxiv.org/html/2601.22153/x1.png",
      "debug_abstract": "Manipulating dynamic objects remains an open challenge for Vision-Language-Action (VLA) models, which, despite strong generalization in static manipulation, struggle in dynamic scenarios requiring rapid perception, temporal anticipation, and continuous control. We present DynamicVLA, a framework for dynamic object manipulation that integrates temporal reasoning and closed-loop adaptation through three key designs: 1) a compact 0.4B VLA using a convolutional vision encoder for spatially efficient, structurally faithful encoding, enabling fast multimodal inference; 2) Continuous Inference, enabling overlapping reasoning and execution for lower latency and timely adaptation to object motion; and 3) Latent-aware Action Streaming, which bridges the perception-execution gap by enforcing temporally aligned action execution. To fill the missing foundation of dynamic manipulation data, we introduce the Dynamic Object Manipulation (DOM) benchmark, built from scratch with an auto data collection pipeline that efficiently gathers 200K synthetic episodes across 2.8K scenes and 206 objects, and enables fast collection of 2K real-world episodes without teleoperation. Extensive evaluations demonstrate remarkable improvements in response speed, perception, and generalization, positioning DynamicVLA as a unified framework for general dynamic object manipulation across embodiments."
    },
    {
      "title": "E2HiL: Entropy-Guided Sample Selection for Efficient Real-World Human-in-the-Loop Reinforcement Learning",
      "url": "https://arxiv.org/abs/2601.19969",
      "date": "2026-01-27",
      "authors_text": "Haoyuan Deng, Yuanjiang Xue, Haoyang Du, Boyang Zhou, Zhenyu Wu",
      "is_highlight": false,
      "score": 80.0,
      "summary": "E2HiL enhances human-in-the-loop reinforcement learning by efficiently selecting informative samples, achieving 42.1% higher success rates with 10.1% fewer human interventions.",
      "teaser_image": "https://arxiv.org/html/2601.19969/x2.png",
      "debug_abstract": "Human-in-the-loop guidance has emerged as an effective approach for enabling faster convergence in online reinforcement learning (RL) of complex real-world manipulation tasks. However, existing human-in-the-loop RL (HiL-RL) frameworks often suffer from low sample efficiency, requiring substantial human interventions to achieve convergence and thereby leading to high labor costs. To address this, we propose a sample-efficient real-world human-in-the-loop RL framework named \\method, which requires fewer human intervention by actively selecting informative samples. Specifically, stable reduction of policy entropy enables improved trade-off between exploration and exploitation with higher sample efficiency. We first build influence functions of different samples on the policy entropy, which is efficiently estimated by the covariance of action probabilities and soft advantages of policies. Then we select samples with moderate values of influence functions, where shortcut samples that induce sharp entropy drops and noisy samples with negligible effect are pruned. Extensive experiments on four real-world manipulation tasks demonstrate that \\method achieves a 42.1\\% higher success rate while requiring 10.1\\% fewer human interventions compared to the state-of-the-art HiL-RL method, validating its effectiveness. The project page providing code, videos, and mathematical formulations can be found at this https URL."
    },
    {
      "title": "RF-MatID: Dataset and Benchmark for Radio Frequency Material Identification",
      "url": "https://arxiv.org/abs/2601.20377",
      "date": "2026-01-28",
      "authors_text": "Xinyan Chen, Qinchun Li, Ruiqin Ma, Jiaqi Bai, Li Yi",
      "is_highlight": false,
      "score": 58.0,
      "summary": "RF-MatID introduces a comprehensive open-source RF dataset and benchmark for fine-grained material identification, addressing limitations of existing optical methods and enhancing algorithmic development.",
      "teaser_image": "https://arxiv.org/html/2601.20377/figures/materials.png",
      "debug_abstract": "Accurate material identification plays a crucial role in embodied AI systems, enabling a wide range of applications. However, current vision-based solutions are limited by the inherent constraints of optical sensors, while radio-frequency (RF) approaches, which can reveal intrinsic material properties, have received growing attention. Despite this progress, RF-based material identification remains hindered by the lack of large-scale public datasets and the limited benchmarking of learning-based approaches. In this work, we present RF-MatID, the first open-source, large-scale, wide-band, and geometry-diverse RF dataset for fine-grained material identification. RF-MatID includes 16 fine-grained categories grouped into 5 superclasses, spanning a broad frequency range from 4 to 43.5 GHz, and comprises 142k samples in both frequency- and time-domain representations. The dataset systematically incorporates controlled geometry perturbations, including variations in incidence angle and stand-off distance. We further establish a multi-setting, multi-protocol benchmark by evaluating state-of-the-art deep learning models, assessing both in-distribution performance and out-of-distribution robustness under cross-angle and cross-distance shifts. The 5 frequency-allocation protocols enable systematic frequency- and region-level analysis, thereby facilitating real-world deployment. RF-MatID aims to enable reproducible research, accelerate algorithmic advancement, foster cross-domain robustness, and support the development of real-world application in RF-based material identification."
    },
    {
      "title": "Video-KTR: Reinforcing Video Reasoning via Key Token Attribution",
      "url": "https://arxiv.org/abs/2601.19686",
      "date": "2026-01-27",
      "authors_text": "Ziyue Wang, Sheng Jin, Zhongrong Zuo, Jiawei Wu, Han Qiu",
      "is_highlight": false,
      "score": 66.0,
      "summary": "Video-KTR enhances video reasoning in multimodal models by selectively reinforcing key tokens through a novel attribution framework, achieving state-of-the-art performance across multiple benchmarks.",
      "debug_abstract": "Reinforcement learning (RL) has shown strong potential for enhancing reasoning in multimodal large language models, yet existing video reasoning methods often rely on coarse sequence-level rewards or single-factor token selection, neglecting fine-grained links among visual inputs, temporal dynamics, and linguistic outputs, limiting both accuracy and interpretability. We propose Video-KTR, a modality-aware policy shaping framework that performs selective, token-level RL by combining three attribution signals: (1) visual-aware tokens identified via counterfactual masking to reveal perceptual dependence; (2) temporal-aware tokens detected through frame shuffling to expose temporal sensitivity; and (3) high-entropy tokens signaling predictive uncertainty. By reinforcing only these key tokens, Video-KTR focuses learning on semantically informative, modality-sensitive content while filtering out low-value tokens. Across five challenging benchmarks, Video-KTR achieves state-of-the-art or highly competitive results, achieving 42.7\\% on Video-Holmes (surpassing GPT-4o) with consistent gains on both reasoning and general video understanding tasks. Ablation studies verify the complementary roles of the attribution signals and the robustness of targeted token-level updates. Overall, Video-KTR improves accuracy and interpretability, offering a simple, drop-in extension to RL for complex video reasoning. Our code and models are available at this https URL."
    },
    {
      "title": "RoamScene3D: Immersive Text-to-3D Scene Generation via Adaptive Object-aware Roaming",
      "url": "https://arxiv.org/abs/2601.19433",
      "date": "2026-01-27",
      "authors_text": "Jisheng Chu, Wenrui Li, Rui Zhao, Wangmeng Zuo, Shifeng Chen",
      "is_highlight": false,
      "score": 67.0,
      "summary": "RoamScene3D introduces a novel framework for immersive text-to-3D scene generation that enhances spatial awareness and object relations, outperforming existing methods in realism and consistency.",
      "debug_abstract": "Generating immersive 3D scenes from texts is a core task in computer vision, crucial for applications in virtual reality and game development. Despite the promise of leveraging 2D diffusion priors, existing methods suffer from spatial blindness and rely on predefined trajectories that fail to exploit the inner relationships among salient objects. Consequently, these approaches are unable to comprehend the semantic layout, preventing them from exploring the scene adaptively to infer occluded content. Moreover, current inpainting models operate in 2D image space, struggling to plausibly fill holes caused by camera motion. To address these limitations, we propose RoamScene3D, a novel framework that bridges the gap between semantic guidance and spatial generation. Our method reasons about the semantic relations among objects and produces consistent and photorealistic scenes. Specifically, we employ a vision-language model (VLM) to construct a scene graph that encodes object relations, guiding the camera to perceive salient object boundaries and plan an adaptive roaming trajectory. Furthermore, to mitigate the limitations of static 2D priors, we introduce a Motion-Injected Inpainting model that is fine-tuned on a synthetic panoramic dataset integrating authentic camera trajectories, making it adaptive to camera motion. Extensive experiments demonstrate that with semantic reasoning and geometric constraints, our method significantly outperforms state-of-the-art approaches in producing consistent and photorealistic scenes. Our code is available at this https URL."
    },
    {
      "title": "Physical Prompt Injection Attacks on Large Vision-Language Models",
      "url": "https://arxiv.org/abs/2601.17383",
      "date": "2026-01-24",
      "authors_text": "Chen Ling, Kai Hu, Hangcheng Liu, Xingshuo Han, Tianwei Zhang",
      "is_highlight": false,
      "score": 55.0,
      "summary": "The paper introduces Physical Prompt Injection Attacks (PPIA), a novel, highly effective method for manipulating Large Vision-Language Models using visually embedded prompts in physical environments.",
      "debug_abstract": "Large Vision-Language Models (LVLMs) are increasingly deployed in real-world intelligent systems for perception and reasoning in open physical environments. While LVLMs are known to be vulnerable to prompt injection attacks, existing methods either require access to input channels or depend on knowledge of user queries, assumptions that rarely hold in practical deployments. We propose the first Physical Prompt Injection Attack (PPIA), a black-box, query-agnostic attack that embeds malicious typographic instructions into physical objects perceivable by the LVLM. PPIA requires no access to the model, its inputs, or internal pipeline, and operates solely through visual observation. It combines offline selection of highly recognizable and semantically effective visual prompts with strategic environment-aware placement guided by spatiotemporal attention, ensuring that the injected prompts are both perceivable and influential on model behavior. We evaluate PPIA across 10 state-of-the-art LVLMs in both simulated and real-world settings on tasks including visual question answering, planning, and navigation, PPIA achieves attack success rates up to 98%, with strong robustness under varying physical conditions such as distance, viewpoint, and illumination. Our code is publicly available at this https URL."
    },
    {
      "title": "Delay-Compensated Stiffness Estimation for Robot-Mediated Dyadic Interaction",
      "url": "https://arxiv.org/abs/2601.17812",
      "date": "2026-01-25",
      "authors_text": "Mingtian Du, Suhas Raghavendra Kulkarni, Bernardo Noronha, Domenico Campolo",
      "is_highlight": false,
      "score": 80.0,
      "summary": "The paper presents a delay-compensated stiffness estimation framework for robot-mediated dyadic interactions, improving accuracy in remote physical therapy despite network-induced haptic delays.",
      "debug_abstract": "Robot-mediated human-human (dyadic) interactions enable therapists to provide physical therapy remotely, yet an accurate perception of patient stiffness remains challenging due to network-induced haptic delays. Conventional stiffness estimation methods, which neglect delay, suffer from temporal misalignment between force and position signals, leading to significant estimation errors as delays increase. To address this, we propose a robust, delay-compensated stiffness estimation framework by deriving an algebraic estimator based on quasi-static equilibrium that explicitly accounts for temporally aligning the expert&#39;s input with the novice&#39;s response. A Normalised Weighted Least Squares (NWLS) implementation is then introduced to robustly filter dynamic bias resulting from the algebraic derivation. Experiments using commercial rehabilitation robots (H-MAN) as the platform demonstrate that the proposed method significantly outperforms the standard estimator, maintaining consistent tracking accuracy under multiple introduced delays. These findings offer a promising solution for achieving high-fidelity haptic perception in remote dyadic interaction, potentially facilitating reliable stiffness assessment in therapeutic settings across networks."
    },
    {
      "title": "ORION: Option-Regularized Deep Reinforcement Learning for Cooperative Multi-Agent Online Navigation",
      "url": "https://arxiv.org/abs/2601.01155",
      "date": "2026-01-26",
      "authors_text": "Shizhe Zhang, Jingsong Liang, Zhitao Zhou, Shuhan Ye, Yizhuo Wang",
      "is_highlight": false,
      "score": 88.0,
      "summary": "ORION is a deep reinforcement learning framework that enables decentralized, cooperative navigation for multi-agent systems in partially known environments by integrating prior maps and online observations.",
      "debug_abstract": "Existing methods for multi-agent navigation typically assume fully known environments, offering limited support for partially known scenarios such as warehouses or factory floors. There, agents may need to plan trajectories that balance their own path optimality with their ability to collect and share information about the environment that can help their teammates reach their own goals. To these ends, we propose ORION, a novel deep reinforcement learning framework for cooperative multi-agent online navigation in partially known environments. Starting from an imperfect prior map, ORION trains agents to make decentralized decisions, coordinate to reach their individual targets, and actively reduce map uncertainty by sharing online observations in a closed perception-action loop. We first design a shared graph encoder that fuses prior map with online perception into a unified representation, providing robust state embeddings under dynamic map discrepancies. At the core of ORION is an option-critic framework that learns to reason about a set of high-level cooperative modes that translate into sequences of low-level actions, allowing agents to switch between individual navigation and team-level exploration adaptively. We further introduce a dual-stage cooperation strategy that enables agents to assist teammates under map uncertainty, thereby reducing the overall makespan. Across extensive maze-like maps and large-scale warehouse environments, our simulation results show that ORION achieves high-quality, real-time decentralized cooperation over varying team sizes, outperforming state-of-the-art classical and learning-based baselines. Finally, we validate ORION on physical robot teams, demonstrating its robustness and practicality for real-world cooperative navigation."
    },
    {
      "title": "Resisting Manipulative Bots in Meme Coin Copy Trading: A Multi-Agent Approach with Chain-of-Thought Reasoning",
      "url": "https://arxiv.org/abs/2601.08641",
      "date": "2026-01-26",
      "authors_text": "Yichen Luo, Yebo Feng, Jiahua Xu, Yang Liu",
      "is_highlight": false,
      "score": 67.0,
      "summary": "This paper presents a multi-agent system utilizing explainable LLMs to defend against manipulative bots in meme coin copy trading, enhancing prediction accuracy and economic performance.",
      "debug_abstract": "Copy trading has become the dominant entry strategy in meme coin markets. However, due to the market&#39;s extreme illiquid and volatile nature, the strategy exposes an exploitable attack surface: adversaries deploy manipulative bots to front-run trades, conceal positions, and fabricate sentiment, systematically extracting value from naïve copiers at scale. Despite its prevalence, bot-driven manipulation remains largely unexplored, and no robust defensive framework exists. We propose a manipulation-resistant copy-trading system based on a multi-agent architecture powered by a multi-modal, explainable large language model (LLM). Our system decomposes copy trading into three specialized agents for coin evaluation, wallet selection, and timing assessment. Evaluated on historical data from over 6,000 meme coins, our approach outperforms zero-shot and most statistic-driven baselines in prediction accuracy as well as all baselines in economic performance, achieving an average return of 14% for identified smart-money trades and an estimated copier return of 3% per trade under realistic market frictions. Overall, our results demonstrate the effectiveness of agent-based defenses and predictability of trader profitability in adversarial meme coin markets, providing a practical foundation for robust copy trading."
    },
    {
      "title": "OnlineSI: Taming Large Language Model for Online 3D Understanding and Grounding",
      "url": "https://arxiv.org/abs/2601.16538",
      "date": "2026-01-23",
      "authors_text": "Zixian Liu, Zhaoxi Chen, Liang Pan, Ziwei Liu",
      "is_highlight": false,
      "score": 88.0,
      "summary": "OnlineSI is a framework that enhances Multimodal Large Language Models' spatial understanding in dynamic environments by integrating continuous learning from video streams and 3D point cloud data.",
      "debug_abstract": "In recent years, researchers have increasingly been interested in how to enable Multimodal Large Language Models (MLLM) to possess spatial understanding and reasoning capabilities. However, most existing methods overlook the importance of the ability to continuously work in an ever-changing world, and lack the possibility of deployment on embodied systems in real-world environments. In this work, we introduce OnlineSI, a framework that can continuously improve its spatial understanding of its surroundings given a video stream. Our core idea is to maintain a finite spatial memory to retain past observations, ensuring the computation required for each inference does not increase as the input accumulates. We further integrate 3D point cloud information with semantic information, helping MLLM to better locate and identify objects in the scene. To evaluate our method, we introduce the Fuzzy $F_1$-Score to mitigate ambiguity, and test our method on two representative datasets. Experiments demonstrate the effectiveness of our method, paving the way towards real-world embodied systems."
    },
    {
      "title": "Accurate Calibration and Robust LiDAR-Inertial Odometry for Spinning Actuated LiDAR Systems",
      "url": "https://arxiv.org/abs/2601.15946",
      "date": "2026-01-22",
      "authors_text": "Zijie Chen, Xiaowei Liu, Yong Xu, Shenghai Yuan, Jianping Li",
      "is_highlight": false,
      "score": 74.5,
      "summary": "This paper introduces LM-Calibr for targetless LiDAR-motor calibration and EVA-LIO for adaptive LiDAR-inertial odometry, enhancing localization in spinning actuated LiDAR systems.",
      "debug_abstract": "Accurate calibration and robust localization are fundamental for downstream tasks in spinning actuated LiDAR applications. Existing methods, however, require parameterizing extrinsic parameters based on different mounting configurations, limiting their generalizability. Additionally, spinning actuated LiDAR inevitably scans featureless regions, which complicates the balance between scanning coverage and localization robustness. To address these challenges, this letter presents a targetless LiDAR-motor calibration (LM-Calibr) on the basis of the Denavit-Hartenberg convention and an environmental adaptive LiDAR-inertial odometry (EVA-LIO). LM-Calibr supports calibration of LiDAR-motor systems with various mounting configurations. Extensive experiments demonstrate its accuracy and convergence across different scenarios, mounting angles, and initial values. Additionally, EVA-LIO adaptively selects downsample rates and map resolutions according to spatial scale. This adaptivity enables the actuator to operate at maximum speed, thereby enhancing scanning completeness while ensuring robust localization, even when LiDAR briefly scans featureless areas. The source code and hardware design are available on GitHub: \\textcolor{blue}{\\href{this https URL}{this http URL\\_calibr}}. The video is available at \\textcolor{blue}{\\href{this https URL}{this http URL}}"
    }
  ],
  "ROSE Lab": [
    {
      "title": "Bongards at the Boundary of Perception and Reasoning: Programs or Language?",
      "url": "https://arxiv.org/abs/2602.03038",
      "date": "2026-02-03",
      "authors_text": "Cassidy Langenfeld, Claas Beger, Gloria Geng, Wasu Top Piriyakulkij, Keya Hu",
      "is_highlight": false,
      "score": 60.0,
      "summary": "This paper introduces a neurosymbolic method utilizing LLMs and Bayesian optimization to tackle Bongard problems, enhancing visual reasoning beyond conventional VLM capabilities.",
      "teaser_image": "https://arxiv.org/html/2602.03038/x3.png",
      "debug_abstract": "Vision-Language Models (VLMs) have made great strides in everyday visual tasks, such as captioning a natural image, or answering commonsense questions about such images. But humans possess the puzzling ability to deploy their visual reasoning abilities in radically new situations, a skill rigorously tested by the classic set of visual reasoning challenges known as the Bongard problems. We present a neurosymbolic approach to solving these problems: given a hypothesized solution rule for a Bongard problem, we leverage LLMs to generate parameterized programmatic representations for the rule and perform parameter fitting using Bayesian optimization. We evaluate our method on classifying Bongard problem images given the ground truth rule, as well as on solving the problems from scratch."
    },
    {
      "title": "Estimation of Ground Reaction Forces from Kinematic Data during Locomotion",
      "url": "https://arxiv.org/abs/2602.03177",
      "date": "2026-02-03",
      "authors_text": "Gautami Golani, Dong Anh Khoa To, Ananda Sidarta, Arun-Kumar Kaliya-Perumal, Oliver Roberts",
      "is_highlight": false,
      "score": 44.0,
      "summary": "This study introduces a force-plate-free method for estimating ground reaction forces from kinematic data, enhancing clinical gait analysis without the need for specialized equipment.",
      "teaser_image": "https://arxiv.org/html/2602.03177/x2.png",
      "debug_abstract": "Ground reaction forces (GRFs) provide fundamental insight into human gait mechanics and are widely used to assess joint loading, limb symmetry, balance control, and motor function. Despite their clinical relevance, the use of GRF remains underutilised in clinical workflows due to the practical limitations of force plate systems. In this work, we present a force-plate-free approach for estimating GRFs using only marker-based motion capture data. This kinematics only method to estimate and decompose GRF makes it well suited for widespread clinical depolyment. By using kinematics from sixteen body segments, we estimate the centre of mass (CoM) and compute GRFs, which are subsequently decomposed into individual components through a minimization-based approach. Through this framework, we can identify gait stance phases and provide access to clinically meaningful kinetic measures without a dedicated force plate system. Experimental results demonstrate the viability of CoM and GRF estimation based solely on kinematic data, supporting force-plate-free gait analysis."
    },
    {
      "title": "Self-Guard: Defending Large Reasoning Models via enhanced self-reflection",
      "url": "https://arxiv.org/abs/2602.00707",
      "date": "2026-01-31",
      "authors_text": "Jingnan Zheng, Jingjun Xu, Yanzhen Luo, Chenhang Cui, Gelei Deng",
      "is_highlight": false,
      "score": 50.0,
      "summary": "Self-Guard is a lightweight framework enhancing safety compliance in Large Reasoning Models by promoting self-reflection and mitigating risks without sacrificing model utility.",
      "teaser_image": "https://arxiv.org/html/2602.00707/materials/workflow.png",
      "debug_abstract": "The emergence of Large Reasoning Models (LRMs) introduces a new paradigm of explicit reasoning, enabling remarkable advances yet posing unique risks such as reasoning manipulation and information leakage. To mitigate these risks, current alignment strategies predominantly rely on heavy post-training paradigms or external interventions. However, these approaches are often computationally intensive and fail to address the inherent awareness-compliance gap, a critical misalignment where models recognize potential risks yet prioritize following user instructions due to their sycophantic tendencies. To address these limitations, we propose Self-Guard, a lightweight safety defense framework that reinforces safety compliance at the representational level. Self-Guard operates through two principal stages: (1) safety-oriented prompting, which activates the model&#39;s latent safety awareness to evoke spontaneous reflection, and (2) safety activation steering, which extracts the resulting directional shift in the hidden state space and amplifies it to ensure that safety compliance prevails over sycophancy during inference. Experiments demonstrate that Self-Guard effectively bridges the awareness-compliance gap, achieving robust safety performance without compromising model utility. Furthermore, Self-Guard exhibits strong generalization across diverse unseen risks and varying model scales, offering a cost-efficient solution for LRM safety alignment."
    },
    {
      "title": "Learning Adaptive Cross-Embodiment Visuomotor Policy with Contrastive Prompt Orchestration",
      "url": "https://arxiv.org/abs/2602.01040",
      "date": "2026-02-01",
      "authors_text": "Yuhang Zhang, Chao Yan, Jiaxi Yu, Jiaping Xiao, Mir Feroskhan",
      "is_highlight": false,
      "score": 92.0,
      "summary": "The paper presents ContrAstive Prompt Orchestration (CAPO), a method for improving adaptive visuomotor policies in diverse environments through hybrid contrastive learning and dynamic prompt orchestration.",
      "teaser_image": "https://arxiv.org/html/2602.01040/x4.png",
      "debug_abstract": "Learning adaptive visuomotor policies for embodied agents remains a formidable challenge, particularly when facing cross-embodiment variations such as diverse sensor configurations and dynamic properties. Conventional learning approaches often struggle to separate task-relevant features from domain-specific variations (e.g., lighting, field-of-view, and rotation), leading to poor sample efficiency and catastrophic failure in unseen environments. To bridge this gap, we propose ContrAstive Prompt Orchestration (CAPO), a novel approach for learning visuomotor policies that integrates contrastive prompt learning and adaptive prompt orchestration. For prompt learning, we devise a hybrid contrastive learning strategy that integrates visual, temporal action, and text objectives to establish a pool of learnable prompts, where each prompt induces a visual representation encapsulating fine-grained domain factors. Based on these learned prompts, we introduce an adaptive prompt orchestration mechanism that dynamically aggregates these prompts conditioned on current observations. This enables the agent to adaptively construct optimal state representations by identifying dominant domain factors instantaneously. Consequently, the policy optimization is effectively shielded from irrelevant interference, preventing the common issue of overfitting to source domains. Extensive experiments demonstrate that CAPO significantly outperforms state-of-the-art baselines in sample efficiency and asymptotic performance. Crucially, it exhibits superior zero-shot adaptation across unseen target domains characterized by drastic environmental (e.g., illumination) and physical shifts (e.g., field-of-view and rotation), validating its effectiveness as a viable solution for cross-embodiment visuomotor policy adaptation."
    },
    {
      "title": "Estimating Force Interactions of Deformable Linear Objects from their Shapes",
      "url": "https://arxiv.org/abs/2602.01085",
      "date": "2026-02-01",
      "authors_text": "Qi Jing Chen, Shilin Shan, Timothy Bretl, Quang-Cuong Pham",
      "is_highlight": false,
      "score": 83.0,
      "summary": "This paper presents a novel method for estimating external forces on deformable linear objects using shape data, enhancing robot interaction safety and efficiency without requiring force sensors.",
      "teaser_image": "https://arxiv.org/html/2602.01085/x2.png",
      "debug_abstract": "This work introduces an analytical approach for detecting and estimating external forces acting on deformable linear objects (DLOs) using only their observed shapes. In many robot-wire interaction tasks, contact occurs not at the end-effector but at other points along the robot&#39;s body. Such scenarios arise when robots manipulate wires indirectly (e.g., by nudging) or when wires act as passive obstacles in the environment. Accurately identifying these interactions is crucial for safe and efficient trajectory planning, helping to prevent wire damage, avoid restricted robot motions, and mitigate potential hazards. Existing approaches often rely on expensive external force-torque sensor or that contacts occur at the end-effector for accurate force estimation. Using wire shape information acquired from a depth camera and under the assumption that the wire is in or near its static equilibrium, our method estimates both the location and magnitude of external forces without additional prior knowledge. This is achieved by exploiting derived consistency conditions and solving a system of linear equations based on force-torque balance along the wire. The approach was validated through simulation, where it achieved high accuracy, and through real-world experiments, where accurate estimation was demonstrated in selected interaction scenarios."
    },
    {
      "title": "SGHA-Attack: Semantic-Guided Hierarchical Alignment for Transferable Targeted Attacks on Vision-Language Models",
      "url": "https://arxiv.org/abs/2602.01574",
      "date": "2026-02-02",
      "authors_text": "Haobo Wang, Weiqi Luo, Xiaojun Jia, Xiaochun Cao",
      "is_highlight": false,
      "score": 44.0,
      "summary": "SGHA-Attack enhances targeted adversarial attacks on vision-language models by leveraging multiple semantic references and aligning intermediate features for improved transferability and robustness.",
      "teaser_image": "https://arxiv.org/html/2602.01574/x1.png",
      "debug_abstract": "Large vision-language models (VLMs) are vulnerable to transfer-based adversarial perturbations, enabling attackers to optimize on surrogate models and manipulate black-box VLM outputs. Prior targeted transfer attacks often overfit surrogate-specific embedding space by relying on a single reference and emphasizing final-layer alignment, which underutilizes intermediate semantics and degrades transfer across heterogeneous VLMs. To address this, we propose SGHA-Attack, a Semantic-Guided Hierarchical Alignment framework that adopts multiple target references and enforces intermediate-layer consistency. Concretely, we generate a visually grounded reference pool by sampling a frozen text-to-image model conditioned on the target prompt, and then carefully select the Top-K most semantically relevant anchors under the surrogate to form a weighted mixture for stable optimization guidance. Building on these anchors, SGHA-Attack injects target semantics throughout the feature hierarchy by aligning intermediate visual representations at both global and spatial granularities across multiple depths, and by synchronizing intermediate visual and textual features in a shared latent subspace to provide early cross-modal supervision before the final projection. Extensive experiments on open-source and commercial black-box VLMs show that SGHA-Attack achieves stronger targeted transferability than prior methods and remains robust under preprocessing and purification defenses."
    },
    {
      "title": "ClueTracer: Question-to-Vision Clue Tracing for Training-Free Hallucination Suppression in Multimodal Reasoning",
      "url": "https://arxiv.org/abs/2602.02004",
      "date": "2026-02-02",
      "authors_text": "Gongli Xi, Kun Wang, Zeming Gao, Huahui Yi, Haolang Lu",
      "is_highlight": false,
      "score": 24.0,
      "summary": "ClueTracer is a training-free plugin that suppresses hallucinations in multimodal reasoning by tracing visual clue propagation, enhancing model performance across various architectures.",
      "teaser_image": "https://arxiv.org/html/2602.02004/x1.png",
      "debug_abstract": "Large multimodal reasoning models solve challenging visual problems via explicit long-chain inference: they gather visual clues from images and decode clues into textual tokens. Yet this capability also increases hallucinations, where the model generates content that is not supported by the input image or the question. To understand this failure mode, we identify \\emph{reasoning drift}: during clue gathering, the model over-focuses on question-irrelevant entities, diluting focus on task-relevant cues and gradually decoupling the reasoning trace from visual grounding. As a consequence, many inference-time localization or intervention methods developed for non-reasoning models fail to pinpoint the true clues in reasoning settings. Motivated by these insights, we introduce ClueRecall, a metric for assessing visual clue retrieval, and present ClueTracer, a training-free, parameter-free, and architecture-agnostic plugin for hallucination suppression. ClueTracer starts from the question and traces how key clues propagate along the model&#39;s reasoning pathway (question $\\rightarrow$ outputs $\\rightarrow$ visual tokens), thereby localizing task-relevant patches while suppressing spurious attention to irrelevant regions. Remarkably, \\textbf{without any additional training}, ClueTracer improves all \\textbf{reasoning} architectures (including \\texttt{R1-OneVision}, \\texttt{Ocean-R1}, \\texttt{MM-Eureka}, \\emph{etc}.) by $\\mathbf{1.21\\times}$ on reasoning benchmarks. When transferred to \\textbf{non-reasoning} settings, it yields a $\\mathbf{1.14\\times}$ gain."
    },
    {
      "title": "Superman: Unifying Skeleton and Vision for Human Motion Perception and Generation",
      "url": "https://arxiv.org/abs/2602.02401",
      "date": "2026-02-02",
      "authors_text": "Xinshun Wang, Peiming Li, Ziyi Wang, Zhongbin Fang, Zhichao Deng",
      "is_highlight": false,
      "score": 15.0,
      "summary": "Superman unifies visual perception and skeleton-based motion generation through a Vision-Guided Motion Tokenizer, enabling robust cross-modal learning and superior performance in human motion tasks.",
      "teaser_image": "https://arxiv.org/html/2602.02401/x2.png",
      "debug_abstract": "Human motion analysis tasks, such as temporal 3D pose estimation, motion prediction, and motion in-betweening, play an essential role in computer vision. However, current paradigms suffer from severe fragmentation. First, the field is split between ``perception&#39;&#39; models that understand motion from video but only output text, and ``generation&#39;&#39; models that cannot perceive from raw visual input. Second, generative MLLMs are often limited to single-frame, static poses using dense, parametric SMPL models, failing to handle temporal motion. Third, existing motion vocabularies are built from skeleton data alone, severing the link to the visual domain. To address these challenges, we introduce Superman, a unified framework that bridges visual perception with temporal, skeleton-based motion generation. Our solution is twofold. First, to overcome the modality disconnect, we propose a Vision-Guided Motion Tokenizer. Leveraging the natural geometric alignment between 3D skeletons and visual data, this module pioneers robust joint learning from both modalities, creating a unified, cross-modal motion vocabulary. Second, grounded in this motion language, a single, unified MLLM architecture is trained to handle all tasks. This module flexibly processes diverse, temporal inputs, unifying 3D skeleton pose estimation from video (perception) with skeleton-based motion prediction and in-betweening (generation). Extensive experiments on standard benchmarks, including Human3.6M, demonstrate that our unified method achieves state-of-the-art or competitive performance across all motion tasks. This showcases a more efficient and scalable path for generative motion analysis using skeletons."
    },
    {
      "title": "Failure-Aware Bimanual Teleoperation via Conservative Value Guided Assistance",
      "url": "https://arxiv.org/abs/2602.01092",
      "date": "2026-02-01",
      "authors_text": "Peng Zhou, Zhongxuan Li, Jinsong Wu, Jiaming Qi, Jun Hu",
      "is_highlight": false,
      "score": 9.0,
      "summary": "This paper presents a failure-aware bimanual teleoperation framework that uses conservative value learning to enhance task success and reduce operator workload through compliant haptic assistance.",
      "teaser_image": "https://arxiv.org/html/2602.01092/x1.png",
      "debug_abstract": "Teleoperation of high-precision manipulation is con-strained by tight success tolerances and complex contact dy-namics, which make impending failures difficult for human operators to anticipate under partial observability. This paper proposes a value-guided, failure-aware framework for bimanual teleoperation that provides compliant haptic assistance while pre-serving continuous human authority. The framework is trained entirely from heterogeneous offline teleoperation data containing both successful and failed executions. Task feasibility is mod-eled as a conservative success score learned via Conservative Value Learning, yielding a risk-sensitive estimate that remains reliable under distribution shift. During online operation, the learned success score regulates the level of assistance, while a learned actor provides a corrective motion direction. Both are integrated through a joint-space impedance interface on the master side, yielding continuous guidance that steers the operator away from failure-prone actions without overriding intent. Experimental results on contact-rich manipulation tasks demonstrate improved task success rates and reduced operator workload compared to conventional teleoperation and shared-autonomy baselines, indicating that conservative value learning provides an effective mechanism for embedding failure awareness into bilateral teleoperation. Experimental videos are available at this https URL"
    },
    {
      "title": "UV-M3TL: A Unified and Versatile Multimodal Multi-Task Learning Framework for Assistive Driving Perception",
      "url": "https://arxiv.org/abs/2602.01594",
      "date": "2026-02-02",
      "authors_text": "Wenzhuo Liu, Qiannan Guo, Zhen Wang, Wenshuo Wang, Lei Yang",
      "is_highlight": false,
      "score": 8.0,
      "summary": "The UV-M3TL framework enhances ADAS by effectively recognizing driver behavior and context through dual-branch multimodal embedding and adaptive loss, achieving state-of-the-art performance across multiple tasks.",
      "teaser_image": "https://arxiv.org/html/2602.01594/x2.png",
      "debug_abstract": "Advanced Driver Assistance Systems (ADAS) need to understand human driver behavior while perceiving their navigation context, but jointly learning these heterogeneous tasks would cause inter-task negative transfer and impair system performance. Here, we propose a Unified and Versatile Multimodal Multi-Task Learning (UV-M3TL) framework to simultaneously recognize driver behavior, driver emotion, vehicle behavior, and traffic context, while mitigating inter-task negative transfer. Our framework incorporates two core components: dual-branch spatial channel multimodal embedding (DB-SCME) and adaptive feature-decoupled multi-task loss (AFD-Loss). DB-SCME enhances cross-task knowledge transfer while mitigating task conflicts by employing a dual-branch structure to explicitly model salient task-shared and task-specific features. AFD-Loss improves the stability of joint optimization while guiding the model to learn diverse multi-task representations by introducing an adaptive weighting mechanism based on learning dynamics and feature decoupling constraints. We evaluate our method on the AIDE dataset, and the experimental results demonstrate that UV-M3TL achieves state-of-the-art performance across all four tasks. To further prove the versatility, we evaluate UV-M3TL on additional public multi-task perception benchmarks (BDD100K, CityScapes, NYUD-v2, and PASCAL-Context), where it consistently delivers strong performance across diverse task combinations, attaining state-of-the-art results on most tasks."
    },
    {
      "title": "FlowSteer: Interactive Agentic Workflow Orchestration via End-to-End Reinforcement Learning",
      "url": "https://arxiv.org/abs/2602.01664",
      "date": "2026-02-02",
      "authors_text": "Mingda Zhang, Haoran Luo, Tiesunlong Shen, Qika Lin, Xiaoying Tang",
      "is_highlight": false,
      "score": 7.0,
      "summary": "FlowSteer is an end-to-end reinforcement learning framework that automates workflow orchestration through interactive learning, addressing manual costs and improving performance across diverse tasks.",
      "teaser_image": "https://arxiv.org/html/2602.01664/x1.png",
      "debug_abstract": "In recent years, a variety of powerful agentic workflows have been applied to solve a wide range of human problems. However, existing workflow orchestration still faces key challenges, including high manual cost, reliance on specific operators/large language models (LLMs), and sparse reward signals. To address these challenges, we propose FlowSteer, an end-to-end reinforcement learning framework that takes a lightweight policy model as the agent and an executable canvas environment, automating workflow orchestration through multi-turn interaction. In this process, the policy model analyzes execution states and selects editing actions, while the canvas executes operators and returns feedback for iterative refinement. Moreover, FlowSteer provides a plug-and-play framework that supports diverse operator libraries and interchangeable LLM backends. To effectively train this interaction paradigm, we propose Canvas Workflow Relative Policy Optimization (CWRPO), which introduces diversity-constrained rewards with conditional release to stabilize learning and suppress shortcut behaviors. Experimental results on twelve datasets show that FlowSteer significantly outperforms baselines across various tasks."
    },
    {
      "title": "FD-VLA: Force-Distilled Vision-Language-Action Model for Contact-Rich Manipulation",
      "url": "https://arxiv.org/abs/2602.02142",
      "date": "2026-02-02",
      "authors_text": "Ruiteng Zhao, Wenshuo Wang, Yicheng Ma, Xiaocong Li, Francis E. H. Tay",
      "is_highlight": false,
      "score": 0,
      "summary": "The FD-VLA framework enhances contact-rich manipulation by integrating force awareness through a novel distillation method, improving performance without physical force sensors.",
      "teaser_image": "https://arxiv.org/html/2602.02142/x1.png",
      "debug_abstract": "Force sensing is a crucial modality for Vision-Language-Action (VLA) frameworks, as it enables fine-grained perception and dexterous manipulation in contact-rich tasks. We present Force-Distilled VLA (FD-VLA), a novel framework that integrates force awareness into contact-rich manipulation without relying on physical force sensors. The core of our approach is a Force Distillation Module (FDM), which distills force by mapping a learnable query token, conditioned on visual observations and robot states, into a predicted force token aligned with the latent representation of actual force signals. During inference, this distilled force token is injected into the pretrained VLM, enabling force-aware reasoning while preserving the integrity of its vision-language semantics. This design provides two key benefits: first, it allows practical deployment across a wide range of robots that lack expensive or fragile force-torque sensors, thereby reducing hardware cost and complexity; second, the FDM introduces an additional force-vision-state fusion prior to the VLM, which improves cross-modal alignment and enhances perception-action robustness in contact-rich scenarios. Surprisingly, our physical experiments show that the distilled force token outperforms direct sensor force measurements as well as other baselines, which highlights the effectiveness of this force-distilled VLA approach."
    },
    {
      "title": "DynamicVLA: A Vision-Language-Action Model for Dynamic Object Manipulation",
      "url": "https://arxiv.org/abs/2601.22153",
      "date": "2026-01-29",
      "authors_text": "Haozhe Xie, Beichen Wen, Jiarui Zheng, Zhaoxi Chen, Fangzhou Hong",
      "is_highlight": false,
      "score": 93.0,
      "summary": "DynamicVLA is a novel Vision-Language-Action framework that enhances dynamic object manipulation through efficient encoding, continuous inference, and a new benchmark for data collection.",
      "teaser_image": "https://arxiv.org/html/2601.22153/x1.png",
      "debug_abstract": "Manipulating dynamic objects remains an open challenge for Vision-Language-Action (VLA) models, which, despite strong generalization in static manipulation, struggle in dynamic scenarios requiring rapid perception, temporal anticipation, and continuous control. We present DynamicVLA, a framework for dynamic object manipulation that integrates temporal reasoning and closed-loop adaptation through three key designs: 1) a compact 0.4B VLA using a convolutional vision encoder for spatially efficient, structurally faithful encoding, enabling fast multimodal inference; 2) Continuous Inference, enabling overlapping reasoning and execution for lower latency and timely adaptation to object motion; and 3) Latent-aware Action Streaming, which bridges the perception-execution gap by enforcing temporally aligned action execution. To fill the missing foundation of dynamic manipulation data, we introduce the Dynamic Object Manipulation (DOM) benchmark, built from scratch with an auto data collection pipeline that efficiently gathers 200K synthetic episodes across 2.8K scenes and 206 objects, and enables fast collection of 2K real-world episodes without teleoperation. Extensive evaluations demonstrate remarkable improvements in response speed, perception, and generalization, positioning DynamicVLA as a unified framework for general dynamic object manipulation across embodiments."
    },
    {
      "title": "E2HiL: Entropy-Guided Sample Selection for Efficient Real-World Human-in-the-Loop Reinforcement Learning",
      "url": "https://arxiv.org/abs/2601.19969",
      "date": "2026-01-27",
      "authors_text": "Haoyuan Deng, Yuanjiang Xue, Haoyang Du, Boyang Zhou, Zhenyu Wu",
      "is_highlight": false,
      "score": 80.0,
      "summary": "E2HiL enhances human-in-the-loop reinforcement learning by efficiently selecting informative samples, achieving 42.1% higher success rates with 10.1% fewer human interventions.",
      "teaser_image": "https://arxiv.org/html/2601.19969/x2.png",
      "debug_abstract": "Human-in-the-loop guidance has emerged as an effective approach for enabling faster convergence in online reinforcement learning (RL) of complex real-world manipulation tasks. However, existing human-in-the-loop RL (HiL-RL) frameworks often suffer from low sample efficiency, requiring substantial human interventions to achieve convergence and thereby leading to high labor costs. To address this, we propose a sample-efficient real-world human-in-the-loop RL framework named \\method, which requires fewer human intervention by actively selecting informative samples. Specifically, stable reduction of policy entropy enables improved trade-off between exploration and exploitation with higher sample efficiency. We first build influence functions of different samples on the policy entropy, which is efficiently estimated by the covariance of action probabilities and soft advantages of policies. Then we select samples with moderate values of influence functions, where shortcut samples that induce sharp entropy drops and noisy samples with negligible effect are pruned. Extensive experiments on four real-world manipulation tasks demonstrate that \\method achieves a 42.1\\% higher success rate while requiring 10.1\\% fewer human interventions compared to the state-of-the-art HiL-RL method, validating its effectiveness. The project page providing code, videos, and mathematical formulations can be found at this https URL."
    },
    {
      "title": "RF-MatID: Dataset and Benchmark for Radio Frequency Material Identification",
      "url": "https://arxiv.org/abs/2601.20377",
      "date": "2026-01-28",
      "authors_text": "Xinyan Chen, Qinchun Li, Ruiqin Ma, Jiaqi Bai, Li Yi",
      "is_highlight": false,
      "score": 58.0,
      "summary": "RF-MatID introduces a comprehensive open-source RF dataset and benchmark for fine-grained material identification, addressing limitations of existing optical methods and enhancing algorithmic development.",
      "teaser_image": "https://arxiv.org/html/2601.20377/figures/materials.png",
      "debug_abstract": "Accurate material identification plays a crucial role in embodied AI systems, enabling a wide range of applications. However, current vision-based solutions are limited by the inherent constraints of optical sensors, while radio-frequency (RF) approaches, which can reveal intrinsic material properties, have received growing attention. Despite this progress, RF-based material identification remains hindered by the lack of large-scale public datasets and the limited benchmarking of learning-based approaches. In this work, we present RF-MatID, the first open-source, large-scale, wide-band, and geometry-diverse RF dataset for fine-grained material identification. RF-MatID includes 16 fine-grained categories grouped into 5 superclasses, spanning a broad frequency range from 4 to 43.5 GHz, and comprises 142k samples in both frequency- and time-domain representations. The dataset systematically incorporates controlled geometry perturbations, including variations in incidence angle and stand-off distance. We further establish a multi-setting, multi-protocol benchmark by evaluating state-of-the-art deep learning models, assessing both in-distribution performance and out-of-distribution robustness under cross-angle and cross-distance shifts. The 5 frequency-allocation protocols enable systematic frequency- and region-level analysis, thereby facilitating real-world deployment. RF-MatID aims to enable reproducible research, accelerate algorithmic advancement, foster cross-domain robustness, and support the development of real-world application in RF-based material identification."
    },
    {
      "title": "Video-KTR: Reinforcing Video Reasoning via Key Token Attribution",
      "url": "https://arxiv.org/abs/2601.19686",
      "date": "2026-01-27",
      "authors_text": "Ziyue Wang, Sheng Jin, Zhongrong Zuo, Jiawei Wu, Han Qiu",
      "is_highlight": false,
      "score": 66.0,
      "summary": "Video-KTR enhances video reasoning in multimodal models by selectively reinforcing key tokens through a novel attribution framework, achieving state-of-the-art performance across multiple benchmarks.",
      "debug_abstract": "Reinforcement learning (RL) has shown strong potential for enhancing reasoning in multimodal large language models, yet existing video reasoning methods often rely on coarse sequence-level rewards or single-factor token selection, neglecting fine-grained links among visual inputs, temporal dynamics, and linguistic outputs, limiting both accuracy and interpretability. We propose Video-KTR, a modality-aware policy shaping framework that performs selective, token-level RL by combining three attribution signals: (1) visual-aware tokens identified via counterfactual masking to reveal perceptual dependence; (2) temporal-aware tokens detected through frame shuffling to expose temporal sensitivity; and (3) high-entropy tokens signaling predictive uncertainty. By reinforcing only these key tokens, Video-KTR focuses learning on semantically informative, modality-sensitive content while filtering out low-value tokens. Across five challenging benchmarks, Video-KTR achieves state-of-the-art or highly competitive results, achieving 42.7\\% on Video-Holmes (surpassing GPT-4o) with consistent gains on both reasoning and general video understanding tasks. Ablation studies verify the complementary roles of the attribution signals and the robustness of targeted token-level updates. Overall, Video-KTR improves accuracy and interpretability, offering a simple, drop-in extension to RL for complex video reasoning. Our code and models are available at this https URL."
    },
    {
      "title": "RoamScene3D: Immersive Text-to-3D Scene Generation via Adaptive Object-aware Roaming",
      "url": "https://arxiv.org/abs/2601.19433",
      "date": "2026-01-27",
      "authors_text": "Jisheng Chu, Wenrui Li, Rui Zhao, Wangmeng Zuo, Shifeng Chen",
      "is_highlight": false,
      "score": 67.0,
      "summary": "RoamScene3D introduces a novel framework for immersive text-to-3D scene generation that enhances spatial awareness and object relations, outperforming existing methods in realism and consistency.",
      "debug_abstract": "Generating immersive 3D scenes from texts is a core task in computer vision, crucial for applications in virtual reality and game development. Despite the promise of leveraging 2D diffusion priors, existing methods suffer from spatial blindness and rely on predefined trajectories that fail to exploit the inner relationships among salient objects. Consequently, these approaches are unable to comprehend the semantic layout, preventing them from exploring the scene adaptively to infer occluded content. Moreover, current inpainting models operate in 2D image space, struggling to plausibly fill holes caused by camera motion. To address these limitations, we propose RoamScene3D, a novel framework that bridges the gap between semantic guidance and spatial generation. Our method reasons about the semantic relations among objects and produces consistent and photorealistic scenes. Specifically, we employ a vision-language model (VLM) to construct a scene graph that encodes object relations, guiding the camera to perceive salient object boundaries and plan an adaptive roaming trajectory. Furthermore, to mitigate the limitations of static 2D priors, we introduce a Motion-Injected Inpainting model that is fine-tuned on a synthetic panoramic dataset integrating authentic camera trajectories, making it adaptive to camera motion. Extensive experiments demonstrate that with semantic reasoning and geometric constraints, our method significantly outperforms state-of-the-art approaches in producing consistent and photorealistic scenes. Our code is available at this https URL."
    },
    {
      "title": "Physical Prompt Injection Attacks on Large Vision-Language Models",
      "url": "https://arxiv.org/abs/2601.17383",
      "date": "2026-01-24",
      "authors_text": "Chen Ling, Kai Hu, Hangcheng Liu, Xingshuo Han, Tianwei Zhang",
      "is_highlight": false,
      "score": 55.0,
      "summary": "The paper introduces Physical Prompt Injection Attacks (PPIA), a novel, highly effective method for manipulating Large Vision-Language Models using visually embedded prompts in physical environments.",
      "debug_abstract": "Large Vision-Language Models (LVLMs) are increasingly deployed in real-world intelligent systems for perception and reasoning in open physical environments. While LVLMs are known to be vulnerable to prompt injection attacks, existing methods either require access to input channels or depend on knowledge of user queries, assumptions that rarely hold in practical deployments. We propose the first Physical Prompt Injection Attack (PPIA), a black-box, query-agnostic attack that embeds malicious typographic instructions into physical objects perceivable by the LVLM. PPIA requires no access to the model, its inputs, or internal pipeline, and operates solely through visual observation. It combines offline selection of highly recognizable and semantically effective visual prompts with strategic environment-aware placement guided by spatiotemporal attention, ensuring that the injected prompts are both perceivable and influential on model behavior. We evaluate PPIA across 10 state-of-the-art LVLMs in both simulated and real-world settings on tasks including visual question answering, planning, and navigation, PPIA achieves attack success rates up to 98%, with strong robustness under varying physical conditions such as distance, viewpoint, and illumination. Our code is publicly available at this https URL."
    },
    {
      "title": "Delay-Compensated Stiffness Estimation for Robot-Mediated Dyadic Interaction",
      "url": "https://arxiv.org/abs/2601.17812",
      "date": "2026-01-25",
      "authors_text": "Mingtian Du, Suhas Raghavendra Kulkarni, Bernardo Noronha, Domenico Campolo",
      "is_highlight": false,
      "score": 80.0,
      "summary": "The paper presents a delay-compensated stiffness estimation framework for robot-mediated dyadic interactions, improving accuracy in remote physical therapy despite network-induced haptic delays.",
      "debug_abstract": "Robot-mediated human-human (dyadic) interactions enable therapists to provide physical therapy remotely, yet an accurate perception of patient stiffness remains challenging due to network-induced haptic delays. Conventional stiffness estimation methods, which neglect delay, suffer from temporal misalignment between force and position signals, leading to significant estimation errors as delays increase. To address this, we propose a robust, delay-compensated stiffness estimation framework by deriving an algebraic estimator based on quasi-static equilibrium that explicitly accounts for temporally aligning the expert&#39;s input with the novice&#39;s response. A Normalised Weighted Least Squares (NWLS) implementation is then introduced to robustly filter dynamic bias resulting from the algebraic derivation. Experiments using commercial rehabilitation robots (H-MAN) as the platform demonstrate that the proposed method significantly outperforms the standard estimator, maintaining consistent tracking accuracy under multiple introduced delays. These findings offer a promising solution for achieving high-fidelity haptic perception in remote dyadic interaction, potentially facilitating reliable stiffness assessment in therapeutic settings across networks."
    },
    {
      "title": "ORION: Option-Regularized Deep Reinforcement Learning for Cooperative Multi-Agent Online Navigation",
      "url": "https://arxiv.org/abs/2601.01155",
      "date": "2026-01-26",
      "authors_text": "Shizhe Zhang, Jingsong Liang, Zhitao Zhou, Shuhan Ye, Yizhuo Wang",
      "is_highlight": false,
      "score": 88.0,
      "summary": "ORION is a deep reinforcement learning framework that enables decentralized, cooperative navigation for multi-agent systems in partially known environments by integrating prior maps and online observations.",
      "debug_abstract": "Existing methods for multi-agent navigation typically assume fully known environments, offering limited support for partially known scenarios such as warehouses or factory floors. There, agents may need to plan trajectories that balance their own path optimality with their ability to collect and share information about the environment that can help their teammates reach their own goals. To these ends, we propose ORION, a novel deep reinforcement learning framework for cooperative multi-agent online navigation in partially known environments. Starting from an imperfect prior map, ORION trains agents to make decentralized decisions, coordinate to reach their individual targets, and actively reduce map uncertainty by sharing online observations in a closed perception-action loop. We first design a shared graph encoder that fuses prior map with online perception into a unified representation, providing robust state embeddings under dynamic map discrepancies. At the core of ORION is an option-critic framework that learns to reason about a set of high-level cooperative modes that translate into sequences of low-level actions, allowing agents to switch between individual navigation and team-level exploration adaptively. We further introduce a dual-stage cooperation strategy that enables agents to assist teammates under map uncertainty, thereby reducing the overall makespan. Across extensive maze-like maps and large-scale warehouse environments, our simulation results show that ORION achieves high-quality, real-time decentralized cooperation over varying team sizes, outperforming state-of-the-art classical and learning-based baselines. Finally, we validate ORION on physical robot teams, demonstrating its robustness and practicality for real-world cooperative navigation."
    },
    {
      "title": "Resisting Manipulative Bots in Meme Coin Copy Trading: A Multi-Agent Approach with Chain-of-Thought Reasoning",
      "url": "https://arxiv.org/abs/2601.08641",
      "date": "2026-01-26",
      "authors_text": "Yichen Luo, Yebo Feng, Jiahua Xu, Yang Liu",
      "is_highlight": false,
      "score": 67.0,
      "summary": "This paper presents a multi-agent system utilizing explainable LLMs to defend against manipulative bots in meme coin copy trading, enhancing prediction accuracy and economic performance.",
      "debug_abstract": "Copy trading has become the dominant entry strategy in meme coin markets. However, due to the market&#39;s extreme illiquid and volatile nature, the strategy exposes an exploitable attack surface: adversaries deploy manipulative bots to front-run trades, conceal positions, and fabricate sentiment, systematically extracting value from naïve copiers at scale. Despite its prevalence, bot-driven manipulation remains largely unexplored, and no robust defensive framework exists. We propose a manipulation-resistant copy-trading system based on a multi-agent architecture powered by a multi-modal, explainable large language model (LLM). Our system decomposes copy trading into three specialized agents for coin evaluation, wallet selection, and timing assessment. Evaluated on historical data from over 6,000 meme coins, our approach outperforms zero-shot and most statistic-driven baselines in prediction accuracy as well as all baselines in economic performance, achieving an average return of 14% for identified smart-money trades and an estimated copier return of 3% per trade under realistic market frictions. Overall, our results demonstrate the effectiveness of agent-based defenses and predictability of trader profitability in adversarial meme coin markets, providing a practical foundation for robust copy trading."
    },
    {
      "title": "OnlineSI: Taming Large Language Model for Online 3D Understanding and Grounding",
      "url": "https://arxiv.org/abs/2601.16538",
      "date": "2026-01-23",
      "authors_text": "Zixian Liu, Zhaoxi Chen, Liang Pan, Ziwei Liu",
      "is_highlight": false,
      "score": 88.0,
      "summary": "OnlineSI is a framework that enhances Multimodal Large Language Models' spatial understanding in dynamic environments by integrating continuous learning from video streams and 3D point cloud data.",
      "debug_abstract": "In recent years, researchers have increasingly been interested in how to enable Multimodal Large Language Models (MLLM) to possess spatial understanding and reasoning capabilities. However, most existing methods overlook the importance of the ability to continuously work in an ever-changing world, and lack the possibility of deployment on embodied systems in real-world environments. In this work, we introduce OnlineSI, a framework that can continuously improve its spatial understanding of its surroundings given a video stream. Our core idea is to maintain a finite spatial memory to retain past observations, ensuring the computation required for each inference does not increase as the input accumulates. We further integrate 3D point cloud information with semantic information, helping MLLM to better locate and identify objects in the scene. To evaluate our method, we introduce the Fuzzy $F_1$-Score to mitigate ambiguity, and test our method on two representative datasets. Experiments demonstrate the effectiveness of our method, paving the way towards real-world embodied systems."
    },
    {
      "title": "Accurate Calibration and Robust LiDAR-Inertial Odometry for Spinning Actuated LiDAR Systems",
      "url": "https://arxiv.org/abs/2601.15946",
      "date": "2026-01-22",
      "authors_text": "Zijie Chen, Xiaowei Liu, Yong Xu, Shenghai Yuan, Jianping Li",
      "is_highlight": false,
      "score": 74.5,
      "summary": "This paper introduces LM-Calibr for targetless LiDAR-motor calibration and EVA-LIO for adaptive LiDAR-inertial odometry, enhancing localization in spinning actuated LiDAR systems.",
      "debug_abstract": "Accurate calibration and robust localization are fundamental for downstream tasks in spinning actuated LiDAR applications. Existing methods, however, require parameterizing extrinsic parameters based on different mounting configurations, limiting their generalizability. Additionally, spinning actuated LiDAR inevitably scans featureless regions, which complicates the balance between scanning coverage and localization robustness. To address these challenges, this letter presents a targetless LiDAR-motor calibration (LM-Calibr) on the basis of the Denavit-Hartenberg convention and an environmental adaptive LiDAR-inertial odometry (EVA-LIO). LM-Calibr supports calibration of LiDAR-motor systems with various mounting configurations. Extensive experiments demonstrate its accuracy and convergence across different scenarios, mounting angles, and initial values. Additionally, EVA-LIO adaptively selects downsample rates and map resolutions according to spatial scale. This adaptivity enables the actuator to operate at maximum speed, thereby enhancing scanning completeness while ensuring robust localization, even when LiDAR briefly scans featureless areas. The source code and hardware design are available on GitHub: \\textcolor{blue}{\\href{this https URL}{this http URL\\_calibr}}. The video is available at \\textcolor{blue}{\\href{this https URL}{this http URL}}"
    }
  ],
  "PINE Lab": [
    {
      "title": "Bongards at the Boundary of Perception and Reasoning: Programs or Language?",
      "url": "https://arxiv.org/abs/2602.03038",
      "date": "2026-02-03",
      "authors_text": "Cassidy Langenfeld, Claas Beger, Gloria Geng, Wasu Top Piriyakulkij, Keya Hu",
      "is_highlight": false,
      "score": 60.0,
      "summary": "This paper introduces a neurosymbolic method utilizing LLMs and Bayesian optimization to tackle Bongard problems, enhancing visual reasoning beyond conventional VLM capabilities.",
      "teaser_image": "https://arxiv.org/html/2602.03038/x3.png",
      "debug_abstract": "Vision-Language Models (VLMs) have made great strides in everyday visual tasks, such as captioning a natural image, or answering commonsense questions about such images. But humans possess the puzzling ability to deploy their visual reasoning abilities in radically new situations, a skill rigorously tested by the classic set of visual reasoning challenges known as the Bongard problems. We present a neurosymbolic approach to solving these problems: given a hypothesized solution rule for a Bongard problem, we leverage LLMs to generate parameterized programmatic representations for the rule and perform parameter fitting using Bayesian optimization. We evaluate our method on classifying Bongard problem images given the ground truth rule, as well as on solving the problems from scratch."
    },
    {
      "title": "Estimation of Ground Reaction Forces from Kinematic Data during Locomotion",
      "url": "https://arxiv.org/abs/2602.03177",
      "date": "2026-02-03",
      "authors_text": "Gautami Golani, Dong Anh Khoa To, Ananda Sidarta, Arun-Kumar Kaliya-Perumal, Oliver Roberts",
      "is_highlight": false,
      "score": 44.0,
      "summary": "This study introduces a force-plate-free method for estimating ground reaction forces from kinematic data, enhancing clinical gait analysis without the need for specialized equipment.",
      "teaser_image": "https://arxiv.org/html/2602.03177/x2.png",
      "debug_abstract": "Ground reaction forces (GRFs) provide fundamental insight into human gait mechanics and are widely used to assess joint loading, limb symmetry, balance control, and motor function. Despite their clinical relevance, the use of GRF remains underutilised in clinical workflows due to the practical limitations of force plate systems. In this work, we present a force-plate-free approach for estimating GRFs using only marker-based motion capture data. This kinematics only method to estimate and decompose GRF makes it well suited for widespread clinical depolyment. By using kinematics from sixteen body segments, we estimate the centre of mass (CoM) and compute GRFs, which are subsequently decomposed into individual components through a minimization-based approach. Through this framework, we can identify gait stance phases and provide access to clinically meaningful kinetic measures without a dedicated force plate system. Experimental results demonstrate the viability of CoM and GRF estimation based solely on kinematic data, supporting force-plate-free gait analysis."
    },
    {
      "title": "Self-Guard: Defending Large Reasoning Models via enhanced self-reflection",
      "url": "https://arxiv.org/abs/2602.00707",
      "date": "2026-01-31",
      "authors_text": "Jingnan Zheng, Jingjun Xu, Yanzhen Luo, Chenhang Cui, Gelei Deng",
      "is_highlight": false,
      "score": 50.0,
      "summary": "Self-Guard is a lightweight framework enhancing safety compliance in Large Reasoning Models by promoting self-reflection and mitigating risks without sacrificing model utility.",
      "teaser_image": "https://arxiv.org/html/2602.00707/materials/workflow.png",
      "debug_abstract": "The emergence of Large Reasoning Models (LRMs) introduces a new paradigm of explicit reasoning, enabling remarkable advances yet posing unique risks such as reasoning manipulation and information leakage. To mitigate these risks, current alignment strategies predominantly rely on heavy post-training paradigms or external interventions. However, these approaches are often computationally intensive and fail to address the inherent awareness-compliance gap, a critical misalignment where models recognize potential risks yet prioritize following user instructions due to their sycophantic tendencies. To address these limitations, we propose Self-Guard, a lightweight safety defense framework that reinforces safety compliance at the representational level. Self-Guard operates through two principal stages: (1) safety-oriented prompting, which activates the model&#39;s latent safety awareness to evoke spontaneous reflection, and (2) safety activation steering, which extracts the resulting directional shift in the hidden state space and amplifies it to ensure that safety compliance prevails over sycophancy during inference. Experiments demonstrate that Self-Guard effectively bridges the awareness-compliance gap, achieving robust safety performance without compromising model utility. Furthermore, Self-Guard exhibits strong generalization across diverse unseen risks and varying model scales, offering a cost-efficient solution for LRM safety alignment."
    },
    {
      "title": "Learning Adaptive Cross-Embodiment Visuomotor Policy with Contrastive Prompt Orchestration",
      "url": "https://arxiv.org/abs/2602.01040",
      "date": "2026-02-01",
      "authors_text": "Yuhang Zhang, Chao Yan, Jiaxi Yu, Jiaping Xiao, Mir Feroskhan",
      "is_highlight": false,
      "score": 92.0,
      "summary": "The paper presents ContrAstive Prompt Orchestration (CAPO), a method for improving adaptive visuomotor policies in diverse environments through hybrid contrastive learning and dynamic prompt orchestration.",
      "teaser_image": "https://arxiv.org/html/2602.01040/x4.png",
      "debug_abstract": "Learning adaptive visuomotor policies for embodied agents remains a formidable challenge, particularly when facing cross-embodiment variations such as diverse sensor configurations and dynamic properties. Conventional learning approaches often struggle to separate task-relevant features from domain-specific variations (e.g., lighting, field-of-view, and rotation), leading to poor sample efficiency and catastrophic failure in unseen environments. To bridge this gap, we propose ContrAstive Prompt Orchestration (CAPO), a novel approach for learning visuomotor policies that integrates contrastive prompt learning and adaptive prompt orchestration. For prompt learning, we devise a hybrid contrastive learning strategy that integrates visual, temporal action, and text objectives to establish a pool of learnable prompts, where each prompt induces a visual representation encapsulating fine-grained domain factors. Based on these learned prompts, we introduce an adaptive prompt orchestration mechanism that dynamically aggregates these prompts conditioned on current observations. This enables the agent to adaptively construct optimal state representations by identifying dominant domain factors instantaneously. Consequently, the policy optimization is effectively shielded from irrelevant interference, preventing the common issue of overfitting to source domains. Extensive experiments demonstrate that CAPO significantly outperforms state-of-the-art baselines in sample efficiency and asymptotic performance. Crucially, it exhibits superior zero-shot adaptation across unseen target domains characterized by drastic environmental (e.g., illumination) and physical shifts (e.g., field-of-view and rotation), validating its effectiveness as a viable solution for cross-embodiment visuomotor policy adaptation."
    },
    {
      "title": "Estimating Force Interactions of Deformable Linear Objects from their Shapes",
      "url": "https://arxiv.org/abs/2602.01085",
      "date": "2026-02-01",
      "authors_text": "Qi Jing Chen, Shilin Shan, Timothy Bretl, Quang-Cuong Pham",
      "is_highlight": false,
      "score": 83.0,
      "summary": "This paper presents a novel method for estimating external forces on deformable linear objects using shape data, enhancing robot interaction safety and efficiency without requiring force sensors.",
      "teaser_image": "https://arxiv.org/html/2602.01085/x2.png",
      "debug_abstract": "This work introduces an analytical approach for detecting and estimating external forces acting on deformable linear objects (DLOs) using only their observed shapes. In many robot-wire interaction tasks, contact occurs not at the end-effector but at other points along the robot&#39;s body. Such scenarios arise when robots manipulate wires indirectly (e.g., by nudging) or when wires act as passive obstacles in the environment. Accurately identifying these interactions is crucial for safe and efficient trajectory planning, helping to prevent wire damage, avoid restricted robot motions, and mitigate potential hazards. Existing approaches often rely on expensive external force-torque sensor or that contacts occur at the end-effector for accurate force estimation. Using wire shape information acquired from a depth camera and under the assumption that the wire is in or near its static equilibrium, our method estimates both the location and magnitude of external forces without additional prior knowledge. This is achieved by exploiting derived consistency conditions and solving a system of linear equations based on force-torque balance along the wire. The approach was validated through simulation, where it achieved high accuracy, and through real-world experiments, where accurate estimation was demonstrated in selected interaction scenarios."
    },
    {
      "title": "SGHA-Attack: Semantic-Guided Hierarchical Alignment for Transferable Targeted Attacks on Vision-Language Models",
      "url": "https://arxiv.org/abs/2602.01574",
      "date": "2026-02-02",
      "authors_text": "Haobo Wang, Weiqi Luo, Xiaojun Jia, Xiaochun Cao",
      "is_highlight": false,
      "score": 44.0,
      "summary": "SGHA-Attack enhances targeted adversarial attacks on vision-language models by leveraging multiple semantic references and aligning intermediate features for improved transferability and robustness.",
      "teaser_image": "https://arxiv.org/html/2602.01574/x1.png",
      "debug_abstract": "Large vision-language models (VLMs) are vulnerable to transfer-based adversarial perturbations, enabling attackers to optimize on surrogate models and manipulate black-box VLM outputs. Prior targeted transfer attacks often overfit surrogate-specific embedding space by relying on a single reference and emphasizing final-layer alignment, which underutilizes intermediate semantics and degrades transfer across heterogeneous VLMs. To address this, we propose SGHA-Attack, a Semantic-Guided Hierarchical Alignment framework that adopts multiple target references and enforces intermediate-layer consistency. Concretely, we generate a visually grounded reference pool by sampling a frozen text-to-image model conditioned on the target prompt, and then carefully select the Top-K most semantically relevant anchors under the surrogate to form a weighted mixture for stable optimization guidance. Building on these anchors, SGHA-Attack injects target semantics throughout the feature hierarchy by aligning intermediate visual representations at both global and spatial granularities across multiple depths, and by synchronizing intermediate visual and textual features in a shared latent subspace to provide early cross-modal supervision before the final projection. Extensive experiments on open-source and commercial black-box VLMs show that SGHA-Attack achieves stronger targeted transferability than prior methods and remains robust under preprocessing and purification defenses."
    },
    {
      "title": "ClueTracer: Question-to-Vision Clue Tracing for Training-Free Hallucination Suppression in Multimodal Reasoning",
      "url": "https://arxiv.org/abs/2602.02004",
      "date": "2026-02-02",
      "authors_text": "Gongli Xi, Kun Wang, Zeming Gao, Huahui Yi, Haolang Lu",
      "is_highlight": false,
      "score": 24.0,
      "summary": "ClueTracer is a training-free plugin that suppresses hallucinations in multimodal reasoning by tracing visual clue propagation, enhancing model performance across various architectures.",
      "teaser_image": "https://arxiv.org/html/2602.02004/x1.png",
      "debug_abstract": "Large multimodal reasoning models solve challenging visual problems via explicit long-chain inference: they gather visual clues from images and decode clues into textual tokens. Yet this capability also increases hallucinations, where the model generates content that is not supported by the input image or the question. To understand this failure mode, we identify \\emph{reasoning drift}: during clue gathering, the model over-focuses on question-irrelevant entities, diluting focus on task-relevant cues and gradually decoupling the reasoning trace from visual grounding. As a consequence, many inference-time localization or intervention methods developed for non-reasoning models fail to pinpoint the true clues in reasoning settings. Motivated by these insights, we introduce ClueRecall, a metric for assessing visual clue retrieval, and present ClueTracer, a training-free, parameter-free, and architecture-agnostic plugin for hallucination suppression. ClueTracer starts from the question and traces how key clues propagate along the model&#39;s reasoning pathway (question $\\rightarrow$ outputs $\\rightarrow$ visual tokens), thereby localizing task-relevant patches while suppressing spurious attention to irrelevant regions. Remarkably, \\textbf{without any additional training}, ClueTracer improves all \\textbf{reasoning} architectures (including \\texttt{R1-OneVision}, \\texttt{Ocean-R1}, \\texttt{MM-Eureka}, \\emph{etc}.) by $\\mathbf{1.21\\times}$ on reasoning benchmarks. When transferred to \\textbf{non-reasoning} settings, it yields a $\\mathbf{1.14\\times}$ gain."
    },
    {
      "title": "Superman: Unifying Skeleton and Vision for Human Motion Perception and Generation",
      "url": "https://arxiv.org/abs/2602.02401",
      "date": "2026-02-02",
      "authors_text": "Xinshun Wang, Peiming Li, Ziyi Wang, Zhongbin Fang, Zhichao Deng",
      "is_highlight": false,
      "score": 15.0,
      "summary": "Superman unifies visual perception and skeleton-based motion generation through a Vision-Guided Motion Tokenizer, enabling robust cross-modal learning and superior performance in human motion tasks.",
      "teaser_image": "https://arxiv.org/html/2602.02401/x2.png",
      "debug_abstract": "Human motion analysis tasks, such as temporal 3D pose estimation, motion prediction, and motion in-betweening, play an essential role in computer vision. However, current paradigms suffer from severe fragmentation. First, the field is split between ``perception&#39;&#39; models that understand motion from video but only output text, and ``generation&#39;&#39; models that cannot perceive from raw visual input. Second, generative MLLMs are often limited to single-frame, static poses using dense, parametric SMPL models, failing to handle temporal motion. Third, existing motion vocabularies are built from skeleton data alone, severing the link to the visual domain. To address these challenges, we introduce Superman, a unified framework that bridges visual perception with temporal, skeleton-based motion generation. Our solution is twofold. First, to overcome the modality disconnect, we propose a Vision-Guided Motion Tokenizer. Leveraging the natural geometric alignment between 3D skeletons and visual data, this module pioneers robust joint learning from both modalities, creating a unified, cross-modal motion vocabulary. Second, grounded in this motion language, a single, unified MLLM architecture is trained to handle all tasks. This module flexibly processes diverse, temporal inputs, unifying 3D skeleton pose estimation from video (perception) with skeleton-based motion prediction and in-betweening (generation). Extensive experiments on standard benchmarks, including Human3.6M, demonstrate that our unified method achieves state-of-the-art or competitive performance across all motion tasks. This showcases a more efficient and scalable path for generative motion analysis using skeletons."
    },
    {
      "title": "Failure-Aware Bimanual Teleoperation via Conservative Value Guided Assistance",
      "url": "https://arxiv.org/abs/2602.01092",
      "date": "2026-02-01",
      "authors_text": "Peng Zhou, Zhongxuan Li, Jinsong Wu, Jiaming Qi, Jun Hu",
      "is_highlight": false,
      "score": 9.0,
      "summary": "This paper presents a failure-aware bimanual teleoperation framework that uses conservative value learning to enhance task success and reduce operator workload through compliant haptic assistance.",
      "teaser_image": "https://arxiv.org/html/2602.01092/x1.png",
      "debug_abstract": "Teleoperation of high-precision manipulation is con-strained by tight success tolerances and complex contact dy-namics, which make impending failures difficult for human operators to anticipate under partial observability. This paper proposes a value-guided, failure-aware framework for bimanual teleoperation that provides compliant haptic assistance while pre-serving continuous human authority. The framework is trained entirely from heterogeneous offline teleoperation data containing both successful and failed executions. Task feasibility is mod-eled as a conservative success score learned via Conservative Value Learning, yielding a risk-sensitive estimate that remains reliable under distribution shift. During online operation, the learned success score regulates the level of assistance, while a learned actor provides a corrective motion direction. Both are integrated through a joint-space impedance interface on the master side, yielding continuous guidance that steers the operator away from failure-prone actions without overriding intent. Experimental results on contact-rich manipulation tasks demonstrate improved task success rates and reduced operator workload compared to conventional teleoperation and shared-autonomy baselines, indicating that conservative value learning provides an effective mechanism for embedding failure awareness into bilateral teleoperation. Experimental videos are available at this https URL"
    },
    {
      "title": "UV-M3TL: A Unified and Versatile Multimodal Multi-Task Learning Framework for Assistive Driving Perception",
      "url": "https://arxiv.org/abs/2602.01594",
      "date": "2026-02-02",
      "authors_text": "Wenzhuo Liu, Qiannan Guo, Zhen Wang, Wenshuo Wang, Lei Yang",
      "is_highlight": false,
      "score": 8.0,
      "summary": "The UV-M3TL framework enhances ADAS by effectively recognizing driver behavior and context through dual-branch multimodal embedding and adaptive loss, achieving state-of-the-art performance across multiple tasks.",
      "teaser_image": "https://arxiv.org/html/2602.01594/x2.png",
      "debug_abstract": "Advanced Driver Assistance Systems (ADAS) need to understand human driver behavior while perceiving their navigation context, but jointly learning these heterogeneous tasks would cause inter-task negative transfer and impair system performance. Here, we propose a Unified and Versatile Multimodal Multi-Task Learning (UV-M3TL) framework to simultaneously recognize driver behavior, driver emotion, vehicle behavior, and traffic context, while mitigating inter-task negative transfer. Our framework incorporates two core components: dual-branch spatial channel multimodal embedding (DB-SCME) and adaptive feature-decoupled multi-task loss (AFD-Loss). DB-SCME enhances cross-task knowledge transfer while mitigating task conflicts by employing a dual-branch structure to explicitly model salient task-shared and task-specific features. AFD-Loss improves the stability of joint optimization while guiding the model to learn diverse multi-task representations by introducing an adaptive weighting mechanism based on learning dynamics and feature decoupling constraints. We evaluate our method on the AIDE dataset, and the experimental results demonstrate that UV-M3TL achieves state-of-the-art performance across all four tasks. To further prove the versatility, we evaluate UV-M3TL on additional public multi-task perception benchmarks (BDD100K, CityScapes, NYUD-v2, and PASCAL-Context), where it consistently delivers strong performance across diverse task combinations, attaining state-of-the-art results on most tasks."
    },
    {
      "title": "FlowSteer: Interactive Agentic Workflow Orchestration via End-to-End Reinforcement Learning",
      "url": "https://arxiv.org/abs/2602.01664",
      "date": "2026-02-02",
      "authors_text": "Mingda Zhang, Haoran Luo, Tiesunlong Shen, Qika Lin, Xiaoying Tang",
      "is_highlight": false,
      "score": 7.0,
      "summary": "FlowSteer is an end-to-end reinforcement learning framework that automates workflow orchestration through interactive learning, addressing manual costs and improving performance across diverse tasks.",
      "teaser_image": "https://arxiv.org/html/2602.01664/x1.png",
      "debug_abstract": "In recent years, a variety of powerful agentic workflows have been applied to solve a wide range of human problems. However, existing workflow orchestration still faces key challenges, including high manual cost, reliance on specific operators/large language models (LLMs), and sparse reward signals. To address these challenges, we propose FlowSteer, an end-to-end reinforcement learning framework that takes a lightweight policy model as the agent and an executable canvas environment, automating workflow orchestration through multi-turn interaction. In this process, the policy model analyzes execution states and selects editing actions, while the canvas executes operators and returns feedback for iterative refinement. Moreover, FlowSteer provides a plug-and-play framework that supports diverse operator libraries and interchangeable LLM backends. To effectively train this interaction paradigm, we propose Canvas Workflow Relative Policy Optimization (CWRPO), which introduces diversity-constrained rewards with conditional release to stabilize learning and suppress shortcut behaviors. Experimental results on twelve datasets show that FlowSteer significantly outperforms baselines across various tasks."
    },
    {
      "title": "FD-VLA: Force-Distilled Vision-Language-Action Model for Contact-Rich Manipulation",
      "url": "https://arxiv.org/abs/2602.02142",
      "date": "2026-02-02",
      "authors_text": "Ruiteng Zhao, Wenshuo Wang, Yicheng Ma, Xiaocong Li, Francis E. H. Tay",
      "is_highlight": false,
      "score": 0,
      "summary": "The FD-VLA framework enhances contact-rich manipulation by integrating force awareness through a novel distillation method, improving performance without physical force sensors.",
      "teaser_image": "https://arxiv.org/html/2602.02142/x1.png",
      "debug_abstract": "Force sensing is a crucial modality for Vision-Language-Action (VLA) frameworks, as it enables fine-grained perception and dexterous manipulation in contact-rich tasks. We present Force-Distilled VLA (FD-VLA), a novel framework that integrates force awareness into contact-rich manipulation without relying on physical force sensors. The core of our approach is a Force Distillation Module (FDM), which distills force by mapping a learnable query token, conditioned on visual observations and robot states, into a predicted force token aligned with the latent representation of actual force signals. During inference, this distilled force token is injected into the pretrained VLM, enabling force-aware reasoning while preserving the integrity of its vision-language semantics. This design provides two key benefits: first, it allows practical deployment across a wide range of robots that lack expensive or fragile force-torque sensors, thereby reducing hardware cost and complexity; second, the FDM introduces an additional force-vision-state fusion prior to the VLM, which improves cross-modal alignment and enhances perception-action robustness in contact-rich scenarios. Surprisingly, our physical experiments show that the distilled force token outperforms direct sensor force measurements as well as other baselines, which highlights the effectiveness of this force-distilled VLA approach."
    },
    {
      "title": "DynamicVLA: A Vision-Language-Action Model for Dynamic Object Manipulation",
      "url": "https://arxiv.org/abs/2601.22153",
      "date": "2026-01-29",
      "authors_text": "Haozhe Xie, Beichen Wen, Jiarui Zheng, Zhaoxi Chen, Fangzhou Hong",
      "is_highlight": false,
      "score": 93.0,
      "summary": "DynamicVLA is a novel Vision-Language-Action framework that enhances dynamic object manipulation through efficient encoding, continuous inference, and a new benchmark for data collection.",
      "teaser_image": "https://arxiv.org/html/2601.22153/x1.png",
      "debug_abstract": "Manipulating dynamic objects remains an open challenge for Vision-Language-Action (VLA) models, which, despite strong generalization in static manipulation, struggle in dynamic scenarios requiring rapid perception, temporal anticipation, and continuous control. We present DynamicVLA, a framework for dynamic object manipulation that integrates temporal reasoning and closed-loop adaptation through three key designs: 1) a compact 0.4B VLA using a convolutional vision encoder for spatially efficient, structurally faithful encoding, enabling fast multimodal inference; 2) Continuous Inference, enabling overlapping reasoning and execution for lower latency and timely adaptation to object motion; and 3) Latent-aware Action Streaming, which bridges the perception-execution gap by enforcing temporally aligned action execution. To fill the missing foundation of dynamic manipulation data, we introduce the Dynamic Object Manipulation (DOM) benchmark, built from scratch with an auto data collection pipeline that efficiently gathers 200K synthetic episodes across 2.8K scenes and 206 objects, and enables fast collection of 2K real-world episodes without teleoperation. Extensive evaluations demonstrate remarkable improvements in response speed, perception, and generalization, positioning DynamicVLA as a unified framework for general dynamic object manipulation across embodiments."
    },
    {
      "title": "E2HiL: Entropy-Guided Sample Selection for Efficient Real-World Human-in-the-Loop Reinforcement Learning",
      "url": "https://arxiv.org/abs/2601.19969",
      "date": "2026-01-27",
      "authors_text": "Haoyuan Deng, Yuanjiang Xue, Haoyang Du, Boyang Zhou, Zhenyu Wu",
      "is_highlight": false,
      "score": 80.0,
      "summary": "E2HiL enhances human-in-the-loop reinforcement learning by efficiently selecting informative samples, achieving 42.1% higher success rates with 10.1% fewer human interventions.",
      "teaser_image": "https://arxiv.org/html/2601.19969/x2.png",
      "debug_abstract": "Human-in-the-loop guidance has emerged as an effective approach for enabling faster convergence in online reinforcement learning (RL) of complex real-world manipulation tasks. However, existing human-in-the-loop RL (HiL-RL) frameworks often suffer from low sample efficiency, requiring substantial human interventions to achieve convergence and thereby leading to high labor costs. To address this, we propose a sample-efficient real-world human-in-the-loop RL framework named \\method, which requires fewer human intervention by actively selecting informative samples. Specifically, stable reduction of policy entropy enables improved trade-off between exploration and exploitation with higher sample efficiency. We first build influence functions of different samples on the policy entropy, which is efficiently estimated by the covariance of action probabilities and soft advantages of policies. Then we select samples with moderate values of influence functions, where shortcut samples that induce sharp entropy drops and noisy samples with negligible effect are pruned. Extensive experiments on four real-world manipulation tasks demonstrate that \\method achieves a 42.1\\% higher success rate while requiring 10.1\\% fewer human interventions compared to the state-of-the-art HiL-RL method, validating its effectiveness. The project page providing code, videos, and mathematical formulations can be found at this https URL."
    },
    {
      "title": "RF-MatID: Dataset and Benchmark for Radio Frequency Material Identification",
      "url": "https://arxiv.org/abs/2601.20377",
      "date": "2026-01-28",
      "authors_text": "Xinyan Chen, Qinchun Li, Ruiqin Ma, Jiaqi Bai, Li Yi",
      "is_highlight": false,
      "score": 58.0,
      "summary": "RF-MatID introduces a comprehensive open-source RF dataset and benchmark for fine-grained material identification, addressing limitations of existing optical methods and enhancing algorithmic development.",
      "teaser_image": "https://arxiv.org/html/2601.20377/figures/materials.png",
      "debug_abstract": "Accurate material identification plays a crucial role in embodied AI systems, enabling a wide range of applications. However, current vision-based solutions are limited by the inherent constraints of optical sensors, while radio-frequency (RF) approaches, which can reveal intrinsic material properties, have received growing attention. Despite this progress, RF-based material identification remains hindered by the lack of large-scale public datasets and the limited benchmarking of learning-based approaches. In this work, we present RF-MatID, the first open-source, large-scale, wide-band, and geometry-diverse RF dataset for fine-grained material identification. RF-MatID includes 16 fine-grained categories grouped into 5 superclasses, spanning a broad frequency range from 4 to 43.5 GHz, and comprises 142k samples in both frequency- and time-domain representations. The dataset systematically incorporates controlled geometry perturbations, including variations in incidence angle and stand-off distance. We further establish a multi-setting, multi-protocol benchmark by evaluating state-of-the-art deep learning models, assessing both in-distribution performance and out-of-distribution robustness under cross-angle and cross-distance shifts. The 5 frequency-allocation protocols enable systematic frequency- and region-level analysis, thereby facilitating real-world deployment. RF-MatID aims to enable reproducible research, accelerate algorithmic advancement, foster cross-domain robustness, and support the development of real-world application in RF-based material identification."
    },
    {
      "title": "Video-KTR: Reinforcing Video Reasoning via Key Token Attribution",
      "url": "https://arxiv.org/abs/2601.19686",
      "date": "2026-01-27",
      "authors_text": "Ziyue Wang, Sheng Jin, Zhongrong Zuo, Jiawei Wu, Han Qiu",
      "is_highlight": false,
      "score": 66.0,
      "summary": "Video-KTR enhances video reasoning in multimodal models by selectively reinforcing key tokens through a novel attribution framework, achieving state-of-the-art performance across multiple benchmarks.",
      "debug_abstract": "Reinforcement learning (RL) has shown strong potential for enhancing reasoning in multimodal large language models, yet existing video reasoning methods often rely on coarse sequence-level rewards or single-factor token selection, neglecting fine-grained links among visual inputs, temporal dynamics, and linguistic outputs, limiting both accuracy and interpretability. We propose Video-KTR, a modality-aware policy shaping framework that performs selective, token-level RL by combining three attribution signals: (1) visual-aware tokens identified via counterfactual masking to reveal perceptual dependence; (2) temporal-aware tokens detected through frame shuffling to expose temporal sensitivity; and (3) high-entropy tokens signaling predictive uncertainty. By reinforcing only these key tokens, Video-KTR focuses learning on semantically informative, modality-sensitive content while filtering out low-value tokens. Across five challenging benchmarks, Video-KTR achieves state-of-the-art or highly competitive results, achieving 42.7\\% on Video-Holmes (surpassing GPT-4o) with consistent gains on both reasoning and general video understanding tasks. Ablation studies verify the complementary roles of the attribution signals and the robustness of targeted token-level updates. Overall, Video-KTR improves accuracy and interpretability, offering a simple, drop-in extension to RL for complex video reasoning. Our code and models are available at this https URL."
    },
    {
      "title": "RoamScene3D: Immersive Text-to-3D Scene Generation via Adaptive Object-aware Roaming",
      "url": "https://arxiv.org/abs/2601.19433",
      "date": "2026-01-27",
      "authors_text": "Jisheng Chu, Wenrui Li, Rui Zhao, Wangmeng Zuo, Shifeng Chen",
      "is_highlight": false,
      "score": 67.0,
      "summary": "RoamScene3D introduces a novel framework for immersive text-to-3D scene generation that enhances spatial awareness and object relations, outperforming existing methods in realism and consistency.",
      "debug_abstract": "Generating immersive 3D scenes from texts is a core task in computer vision, crucial for applications in virtual reality and game development. Despite the promise of leveraging 2D diffusion priors, existing methods suffer from spatial blindness and rely on predefined trajectories that fail to exploit the inner relationships among salient objects. Consequently, these approaches are unable to comprehend the semantic layout, preventing them from exploring the scene adaptively to infer occluded content. Moreover, current inpainting models operate in 2D image space, struggling to plausibly fill holes caused by camera motion. To address these limitations, we propose RoamScene3D, a novel framework that bridges the gap between semantic guidance and spatial generation. Our method reasons about the semantic relations among objects and produces consistent and photorealistic scenes. Specifically, we employ a vision-language model (VLM) to construct a scene graph that encodes object relations, guiding the camera to perceive salient object boundaries and plan an adaptive roaming trajectory. Furthermore, to mitigate the limitations of static 2D priors, we introduce a Motion-Injected Inpainting model that is fine-tuned on a synthetic panoramic dataset integrating authentic camera trajectories, making it adaptive to camera motion. Extensive experiments demonstrate that with semantic reasoning and geometric constraints, our method significantly outperforms state-of-the-art approaches in producing consistent and photorealistic scenes. Our code is available at this https URL."
    },
    {
      "title": "Physical Prompt Injection Attacks on Large Vision-Language Models",
      "url": "https://arxiv.org/abs/2601.17383",
      "date": "2026-01-24",
      "authors_text": "Chen Ling, Kai Hu, Hangcheng Liu, Xingshuo Han, Tianwei Zhang",
      "is_highlight": false,
      "score": 55.0,
      "summary": "The paper introduces Physical Prompt Injection Attacks (PPIA), a novel, highly effective method for manipulating Large Vision-Language Models using visually embedded prompts in physical environments.",
      "debug_abstract": "Large Vision-Language Models (LVLMs) are increasingly deployed in real-world intelligent systems for perception and reasoning in open physical environments. While LVLMs are known to be vulnerable to prompt injection attacks, existing methods either require access to input channels or depend on knowledge of user queries, assumptions that rarely hold in practical deployments. We propose the first Physical Prompt Injection Attack (PPIA), a black-box, query-agnostic attack that embeds malicious typographic instructions into physical objects perceivable by the LVLM. PPIA requires no access to the model, its inputs, or internal pipeline, and operates solely through visual observation. It combines offline selection of highly recognizable and semantically effective visual prompts with strategic environment-aware placement guided by spatiotemporal attention, ensuring that the injected prompts are both perceivable and influential on model behavior. We evaluate PPIA across 10 state-of-the-art LVLMs in both simulated and real-world settings on tasks including visual question answering, planning, and navigation, PPIA achieves attack success rates up to 98%, with strong robustness under varying physical conditions such as distance, viewpoint, and illumination. Our code is publicly available at this https URL."
    },
    {
      "title": "Delay-Compensated Stiffness Estimation for Robot-Mediated Dyadic Interaction",
      "url": "https://arxiv.org/abs/2601.17812",
      "date": "2026-01-25",
      "authors_text": "Mingtian Du, Suhas Raghavendra Kulkarni, Bernardo Noronha, Domenico Campolo",
      "is_highlight": false,
      "score": 80.0,
      "summary": "The paper presents a delay-compensated stiffness estimation framework for robot-mediated dyadic interactions, improving accuracy in remote physical therapy despite network-induced haptic delays.",
      "debug_abstract": "Robot-mediated human-human (dyadic) interactions enable therapists to provide physical therapy remotely, yet an accurate perception of patient stiffness remains challenging due to network-induced haptic delays. Conventional stiffness estimation methods, which neglect delay, suffer from temporal misalignment between force and position signals, leading to significant estimation errors as delays increase. To address this, we propose a robust, delay-compensated stiffness estimation framework by deriving an algebraic estimator based on quasi-static equilibrium that explicitly accounts for temporally aligning the expert&#39;s input with the novice&#39;s response. A Normalised Weighted Least Squares (NWLS) implementation is then introduced to robustly filter dynamic bias resulting from the algebraic derivation. Experiments using commercial rehabilitation robots (H-MAN) as the platform demonstrate that the proposed method significantly outperforms the standard estimator, maintaining consistent tracking accuracy under multiple introduced delays. These findings offer a promising solution for achieving high-fidelity haptic perception in remote dyadic interaction, potentially facilitating reliable stiffness assessment in therapeutic settings across networks."
    },
    {
      "title": "ORION: Option-Regularized Deep Reinforcement Learning for Cooperative Multi-Agent Online Navigation",
      "url": "https://arxiv.org/abs/2601.01155",
      "date": "2026-01-26",
      "authors_text": "Shizhe Zhang, Jingsong Liang, Zhitao Zhou, Shuhan Ye, Yizhuo Wang",
      "is_highlight": false,
      "score": 88.0,
      "summary": "ORION is a deep reinforcement learning framework that enables decentralized, cooperative navigation for multi-agent systems in partially known environments by integrating prior maps and online observations.",
      "debug_abstract": "Existing methods for multi-agent navigation typically assume fully known environments, offering limited support for partially known scenarios such as warehouses or factory floors. There, agents may need to plan trajectories that balance their own path optimality with their ability to collect and share information about the environment that can help their teammates reach their own goals. To these ends, we propose ORION, a novel deep reinforcement learning framework for cooperative multi-agent online navigation in partially known environments. Starting from an imperfect prior map, ORION trains agents to make decentralized decisions, coordinate to reach their individual targets, and actively reduce map uncertainty by sharing online observations in a closed perception-action loop. We first design a shared graph encoder that fuses prior map with online perception into a unified representation, providing robust state embeddings under dynamic map discrepancies. At the core of ORION is an option-critic framework that learns to reason about a set of high-level cooperative modes that translate into sequences of low-level actions, allowing agents to switch between individual navigation and team-level exploration adaptively. We further introduce a dual-stage cooperation strategy that enables agents to assist teammates under map uncertainty, thereby reducing the overall makespan. Across extensive maze-like maps and large-scale warehouse environments, our simulation results show that ORION achieves high-quality, real-time decentralized cooperation over varying team sizes, outperforming state-of-the-art classical and learning-based baselines. Finally, we validate ORION on physical robot teams, demonstrating its robustness and practicality for real-world cooperative navigation."
    },
    {
      "title": "Resisting Manipulative Bots in Meme Coin Copy Trading: A Multi-Agent Approach with Chain-of-Thought Reasoning",
      "url": "https://arxiv.org/abs/2601.08641",
      "date": "2026-01-26",
      "authors_text": "Yichen Luo, Yebo Feng, Jiahua Xu, Yang Liu",
      "is_highlight": false,
      "score": 67.0,
      "summary": "This paper presents a multi-agent system utilizing explainable LLMs to defend against manipulative bots in meme coin copy trading, enhancing prediction accuracy and economic performance.",
      "debug_abstract": "Copy trading has become the dominant entry strategy in meme coin markets. However, due to the market&#39;s extreme illiquid and volatile nature, the strategy exposes an exploitable attack surface: adversaries deploy manipulative bots to front-run trades, conceal positions, and fabricate sentiment, systematically extracting value from naïve copiers at scale. Despite its prevalence, bot-driven manipulation remains largely unexplored, and no robust defensive framework exists. We propose a manipulation-resistant copy-trading system based on a multi-agent architecture powered by a multi-modal, explainable large language model (LLM). Our system decomposes copy trading into three specialized agents for coin evaluation, wallet selection, and timing assessment. Evaluated on historical data from over 6,000 meme coins, our approach outperforms zero-shot and most statistic-driven baselines in prediction accuracy as well as all baselines in economic performance, achieving an average return of 14% for identified smart-money trades and an estimated copier return of 3% per trade under realistic market frictions. Overall, our results demonstrate the effectiveness of agent-based defenses and predictability of trader profitability in adversarial meme coin markets, providing a practical foundation for robust copy trading."
    },
    {
      "title": "OnlineSI: Taming Large Language Model for Online 3D Understanding and Grounding",
      "url": "https://arxiv.org/abs/2601.16538",
      "date": "2026-01-23",
      "authors_text": "Zixian Liu, Zhaoxi Chen, Liang Pan, Ziwei Liu",
      "is_highlight": false,
      "score": 88.0,
      "summary": "OnlineSI is a framework that enhances Multimodal Large Language Models' spatial understanding in dynamic environments by integrating continuous learning from video streams and 3D point cloud data.",
      "debug_abstract": "In recent years, researchers have increasingly been interested in how to enable Multimodal Large Language Models (MLLM) to possess spatial understanding and reasoning capabilities. However, most existing methods overlook the importance of the ability to continuously work in an ever-changing world, and lack the possibility of deployment on embodied systems in real-world environments. In this work, we introduce OnlineSI, a framework that can continuously improve its spatial understanding of its surroundings given a video stream. Our core idea is to maintain a finite spatial memory to retain past observations, ensuring the computation required for each inference does not increase as the input accumulates. We further integrate 3D point cloud information with semantic information, helping MLLM to better locate and identify objects in the scene. To evaluate our method, we introduce the Fuzzy $F_1$-Score to mitigate ambiguity, and test our method on two representative datasets. Experiments demonstrate the effectiveness of our method, paving the way towards real-world embodied systems."
    },
    {
      "title": "Accurate Calibration and Robust LiDAR-Inertial Odometry for Spinning Actuated LiDAR Systems",
      "url": "https://arxiv.org/abs/2601.15946",
      "date": "2026-01-22",
      "authors_text": "Zijie Chen, Xiaowei Liu, Yong Xu, Shenghai Yuan, Jianping Li",
      "is_highlight": false,
      "score": 74.5,
      "summary": "This paper introduces LM-Calibr for targetless LiDAR-motor calibration and EVA-LIO for adaptive LiDAR-inertial odometry, enhancing localization in spinning actuated LiDAR systems.",
      "debug_abstract": "Accurate calibration and robust localization are fundamental for downstream tasks in spinning actuated LiDAR applications. Existing methods, however, require parameterizing extrinsic parameters based on different mounting configurations, limiting their generalizability. Additionally, spinning actuated LiDAR inevitably scans featureless regions, which complicates the balance between scanning coverage and localization robustness. To address these challenges, this letter presents a targetless LiDAR-motor calibration (LM-Calibr) on the basis of the Denavit-Hartenberg convention and an environmental adaptive LiDAR-inertial odometry (EVA-LIO). LM-Calibr supports calibration of LiDAR-motor systems with various mounting configurations. Extensive experiments demonstrate its accuracy and convergence across different scenarios, mounting angles, and initial values. Additionally, EVA-LIO adaptively selects downsample rates and map resolutions according to spatial scale. This adaptivity enables the actuator to operate at maximum speed, thereby enhancing scanning completeness while ensuring robust localization, even when LiDAR briefly scans featureless areas. The source code and hardware design are available on GitHub: \\textcolor{blue}{\\href{this https URL}{this http URL\\_calibr}}. The video is available at \\textcolor{blue}{\\href{this https URL}{this http URL}}"
    }
  ],
  "MARS Lab": [
    {
      "title": "Bongards at the Boundary of Perception and Reasoning: Programs or Language?",
      "url": "https://arxiv.org/abs/2602.03038",
      "date": "2026-02-03",
      "authors_text": "Cassidy Langenfeld, Claas Beger, Gloria Geng, Wasu Top Piriyakulkij, Keya Hu",
      "is_highlight": false,
      "score": 60.0,
      "summary": "This paper introduces a neurosymbolic method utilizing LLMs and Bayesian optimization to tackle Bongard problems, enhancing visual reasoning beyond conventional VLM capabilities.",
      "teaser_image": "https://arxiv.org/html/2602.03038/x3.png",
      "debug_abstract": "Vision-Language Models (VLMs) have made great strides in everyday visual tasks, such as captioning a natural image, or answering commonsense questions about such images. But humans possess the puzzling ability to deploy their visual reasoning abilities in radically new situations, a skill rigorously tested by the classic set of visual reasoning challenges known as the Bongard problems. We present a neurosymbolic approach to solving these problems: given a hypothesized solution rule for a Bongard problem, we leverage LLMs to generate parameterized programmatic representations for the rule and perform parameter fitting using Bayesian optimization. We evaluate our method on classifying Bongard problem images given the ground truth rule, as well as on solving the problems from scratch."
    },
    {
      "title": "Estimation of Ground Reaction Forces from Kinematic Data during Locomotion",
      "url": "https://arxiv.org/abs/2602.03177",
      "date": "2026-02-03",
      "authors_text": "Gautami Golani, Dong Anh Khoa To, Ananda Sidarta, Arun-Kumar Kaliya-Perumal, Oliver Roberts",
      "is_highlight": false,
      "score": 44.0,
      "summary": "This study introduces a force-plate-free method for estimating ground reaction forces from kinematic data, enhancing clinical gait analysis without the need for specialized equipment.",
      "teaser_image": "https://arxiv.org/html/2602.03177/x2.png",
      "debug_abstract": "Ground reaction forces (GRFs) provide fundamental insight into human gait mechanics and are widely used to assess joint loading, limb symmetry, balance control, and motor function. Despite their clinical relevance, the use of GRF remains underutilised in clinical workflows due to the practical limitations of force plate systems. In this work, we present a force-plate-free approach for estimating GRFs using only marker-based motion capture data. This kinematics only method to estimate and decompose GRF makes it well suited for widespread clinical depolyment. By using kinematics from sixteen body segments, we estimate the centre of mass (CoM) and compute GRFs, which are subsequently decomposed into individual components through a minimization-based approach. Through this framework, we can identify gait stance phases and provide access to clinically meaningful kinetic measures without a dedicated force plate system. Experimental results demonstrate the viability of CoM and GRF estimation based solely on kinematic data, supporting force-plate-free gait analysis."
    },
    {
      "title": "Self-Guard: Defending Large Reasoning Models via enhanced self-reflection",
      "url": "https://arxiv.org/abs/2602.00707",
      "date": "2026-01-31",
      "authors_text": "Jingnan Zheng, Jingjun Xu, Yanzhen Luo, Chenhang Cui, Gelei Deng",
      "is_highlight": false,
      "score": 50.0,
      "summary": "Self-Guard is a lightweight framework enhancing safety compliance in Large Reasoning Models by promoting self-reflection and mitigating risks without sacrificing model utility.",
      "teaser_image": "https://arxiv.org/html/2602.00707/materials/workflow.png",
      "debug_abstract": "The emergence of Large Reasoning Models (LRMs) introduces a new paradigm of explicit reasoning, enabling remarkable advances yet posing unique risks such as reasoning manipulation and information leakage. To mitigate these risks, current alignment strategies predominantly rely on heavy post-training paradigms or external interventions. However, these approaches are often computationally intensive and fail to address the inherent awareness-compliance gap, a critical misalignment where models recognize potential risks yet prioritize following user instructions due to their sycophantic tendencies. To address these limitations, we propose Self-Guard, a lightweight safety defense framework that reinforces safety compliance at the representational level. Self-Guard operates through two principal stages: (1) safety-oriented prompting, which activates the model&#39;s latent safety awareness to evoke spontaneous reflection, and (2) safety activation steering, which extracts the resulting directional shift in the hidden state space and amplifies it to ensure that safety compliance prevails over sycophancy during inference. Experiments demonstrate that Self-Guard effectively bridges the awareness-compliance gap, achieving robust safety performance without compromising model utility. Furthermore, Self-Guard exhibits strong generalization across diverse unseen risks and varying model scales, offering a cost-efficient solution for LRM safety alignment."
    },
    {
      "title": "Learning Adaptive Cross-Embodiment Visuomotor Policy with Contrastive Prompt Orchestration",
      "url": "https://arxiv.org/abs/2602.01040",
      "date": "2026-02-01",
      "authors_text": "Yuhang Zhang, Chao Yan, Jiaxi Yu, Jiaping Xiao, Mir Feroskhan",
      "is_highlight": false,
      "score": 92.0,
      "summary": "The paper presents ContrAstive Prompt Orchestration (CAPO), a method for improving adaptive visuomotor policies in diverse environments through hybrid contrastive learning and dynamic prompt orchestration.",
      "teaser_image": "https://arxiv.org/html/2602.01040/x4.png",
      "debug_abstract": "Learning adaptive visuomotor policies for embodied agents remains a formidable challenge, particularly when facing cross-embodiment variations such as diverse sensor configurations and dynamic properties. Conventional learning approaches often struggle to separate task-relevant features from domain-specific variations (e.g., lighting, field-of-view, and rotation), leading to poor sample efficiency and catastrophic failure in unseen environments. To bridge this gap, we propose ContrAstive Prompt Orchestration (CAPO), a novel approach for learning visuomotor policies that integrates contrastive prompt learning and adaptive prompt orchestration. For prompt learning, we devise a hybrid contrastive learning strategy that integrates visual, temporal action, and text objectives to establish a pool of learnable prompts, where each prompt induces a visual representation encapsulating fine-grained domain factors. Based on these learned prompts, we introduce an adaptive prompt orchestration mechanism that dynamically aggregates these prompts conditioned on current observations. This enables the agent to adaptively construct optimal state representations by identifying dominant domain factors instantaneously. Consequently, the policy optimization is effectively shielded from irrelevant interference, preventing the common issue of overfitting to source domains. Extensive experiments demonstrate that CAPO significantly outperforms state-of-the-art baselines in sample efficiency and asymptotic performance. Crucially, it exhibits superior zero-shot adaptation across unseen target domains characterized by drastic environmental (e.g., illumination) and physical shifts (e.g., field-of-view and rotation), validating its effectiveness as a viable solution for cross-embodiment visuomotor policy adaptation."
    },
    {
      "title": "Estimating Force Interactions of Deformable Linear Objects from their Shapes",
      "url": "https://arxiv.org/abs/2602.01085",
      "date": "2026-02-01",
      "authors_text": "Qi Jing Chen, Shilin Shan, Timothy Bretl, Quang-Cuong Pham",
      "is_highlight": false,
      "score": 83.0,
      "summary": "This paper presents a novel method for estimating external forces on deformable linear objects using shape data, enhancing robot interaction safety and efficiency without requiring force sensors.",
      "teaser_image": "https://arxiv.org/html/2602.01085/x2.png",
      "debug_abstract": "This work introduces an analytical approach for detecting and estimating external forces acting on deformable linear objects (DLOs) using only their observed shapes. In many robot-wire interaction tasks, contact occurs not at the end-effector but at other points along the robot&#39;s body. Such scenarios arise when robots manipulate wires indirectly (e.g., by nudging) or when wires act as passive obstacles in the environment. Accurately identifying these interactions is crucial for safe and efficient trajectory planning, helping to prevent wire damage, avoid restricted robot motions, and mitigate potential hazards. Existing approaches often rely on expensive external force-torque sensor or that contacts occur at the end-effector for accurate force estimation. Using wire shape information acquired from a depth camera and under the assumption that the wire is in or near its static equilibrium, our method estimates both the location and magnitude of external forces without additional prior knowledge. This is achieved by exploiting derived consistency conditions and solving a system of linear equations based on force-torque balance along the wire. The approach was validated through simulation, where it achieved high accuracy, and through real-world experiments, where accurate estimation was demonstrated in selected interaction scenarios."
    },
    {
      "title": "SGHA-Attack: Semantic-Guided Hierarchical Alignment for Transferable Targeted Attacks on Vision-Language Models",
      "url": "https://arxiv.org/abs/2602.01574",
      "date": "2026-02-02",
      "authors_text": "Haobo Wang, Weiqi Luo, Xiaojun Jia, Xiaochun Cao",
      "is_highlight": false,
      "score": 44.0,
      "summary": "SGHA-Attack enhances targeted adversarial attacks on vision-language models by leveraging multiple semantic references and aligning intermediate features for improved transferability and robustness.",
      "teaser_image": "https://arxiv.org/html/2602.01574/x1.png",
      "debug_abstract": "Large vision-language models (VLMs) are vulnerable to transfer-based adversarial perturbations, enabling attackers to optimize on surrogate models and manipulate black-box VLM outputs. Prior targeted transfer attacks often overfit surrogate-specific embedding space by relying on a single reference and emphasizing final-layer alignment, which underutilizes intermediate semantics and degrades transfer across heterogeneous VLMs. To address this, we propose SGHA-Attack, a Semantic-Guided Hierarchical Alignment framework that adopts multiple target references and enforces intermediate-layer consistency. Concretely, we generate a visually grounded reference pool by sampling a frozen text-to-image model conditioned on the target prompt, and then carefully select the Top-K most semantically relevant anchors under the surrogate to form a weighted mixture for stable optimization guidance. Building on these anchors, SGHA-Attack injects target semantics throughout the feature hierarchy by aligning intermediate visual representations at both global and spatial granularities across multiple depths, and by synchronizing intermediate visual and textual features in a shared latent subspace to provide early cross-modal supervision before the final projection. Extensive experiments on open-source and commercial black-box VLMs show that SGHA-Attack achieves stronger targeted transferability than prior methods and remains robust under preprocessing and purification defenses."
    },
    {
      "title": "ClueTracer: Question-to-Vision Clue Tracing for Training-Free Hallucination Suppression in Multimodal Reasoning",
      "url": "https://arxiv.org/abs/2602.02004",
      "date": "2026-02-02",
      "authors_text": "Gongli Xi, Kun Wang, Zeming Gao, Huahui Yi, Haolang Lu",
      "is_highlight": false,
      "score": 24.0,
      "summary": "ClueTracer is a training-free plugin that suppresses hallucinations in multimodal reasoning by tracing visual clue propagation, enhancing model performance across various architectures.",
      "teaser_image": "https://arxiv.org/html/2602.02004/x1.png",
      "debug_abstract": "Large multimodal reasoning models solve challenging visual problems via explicit long-chain inference: they gather visual clues from images and decode clues into textual tokens. Yet this capability also increases hallucinations, where the model generates content that is not supported by the input image or the question. To understand this failure mode, we identify \\emph{reasoning drift}: during clue gathering, the model over-focuses on question-irrelevant entities, diluting focus on task-relevant cues and gradually decoupling the reasoning trace from visual grounding. As a consequence, many inference-time localization or intervention methods developed for non-reasoning models fail to pinpoint the true clues in reasoning settings. Motivated by these insights, we introduce ClueRecall, a metric for assessing visual clue retrieval, and present ClueTracer, a training-free, parameter-free, and architecture-agnostic plugin for hallucination suppression. ClueTracer starts from the question and traces how key clues propagate along the model&#39;s reasoning pathway (question $\\rightarrow$ outputs $\\rightarrow$ visual tokens), thereby localizing task-relevant patches while suppressing spurious attention to irrelevant regions. Remarkably, \\textbf{without any additional training}, ClueTracer improves all \\textbf{reasoning} architectures (including \\texttt{R1-OneVision}, \\texttt{Ocean-R1}, \\texttt{MM-Eureka}, \\emph{etc}.) by $\\mathbf{1.21\\times}$ on reasoning benchmarks. When transferred to \\textbf{non-reasoning} settings, it yields a $\\mathbf{1.14\\times}$ gain."
    },
    {
      "title": "Superman: Unifying Skeleton and Vision for Human Motion Perception and Generation",
      "url": "https://arxiv.org/abs/2602.02401",
      "date": "2026-02-02",
      "authors_text": "Xinshun Wang, Peiming Li, Ziyi Wang, Zhongbin Fang, Zhichao Deng",
      "is_highlight": false,
      "score": 15.0,
      "summary": "Superman unifies visual perception and skeleton-based motion generation through a Vision-Guided Motion Tokenizer, enabling robust cross-modal learning and superior performance in human motion tasks.",
      "teaser_image": "https://arxiv.org/html/2602.02401/x2.png",
      "debug_abstract": "Human motion analysis tasks, such as temporal 3D pose estimation, motion prediction, and motion in-betweening, play an essential role in computer vision. However, current paradigms suffer from severe fragmentation. First, the field is split between ``perception&#39;&#39; models that understand motion from video but only output text, and ``generation&#39;&#39; models that cannot perceive from raw visual input. Second, generative MLLMs are often limited to single-frame, static poses using dense, parametric SMPL models, failing to handle temporal motion. Third, existing motion vocabularies are built from skeleton data alone, severing the link to the visual domain. To address these challenges, we introduce Superman, a unified framework that bridges visual perception with temporal, skeleton-based motion generation. Our solution is twofold. First, to overcome the modality disconnect, we propose a Vision-Guided Motion Tokenizer. Leveraging the natural geometric alignment between 3D skeletons and visual data, this module pioneers robust joint learning from both modalities, creating a unified, cross-modal motion vocabulary. Second, grounded in this motion language, a single, unified MLLM architecture is trained to handle all tasks. This module flexibly processes diverse, temporal inputs, unifying 3D skeleton pose estimation from video (perception) with skeleton-based motion prediction and in-betweening (generation). Extensive experiments on standard benchmarks, including Human3.6M, demonstrate that our unified method achieves state-of-the-art or competitive performance across all motion tasks. This showcases a more efficient and scalable path for generative motion analysis using skeletons."
    },
    {
      "title": "Failure-Aware Bimanual Teleoperation via Conservative Value Guided Assistance",
      "url": "https://arxiv.org/abs/2602.01092",
      "date": "2026-02-01",
      "authors_text": "Peng Zhou, Zhongxuan Li, Jinsong Wu, Jiaming Qi, Jun Hu",
      "is_highlight": false,
      "score": 9.0,
      "summary": "This paper presents a failure-aware bimanual teleoperation framework that uses conservative value learning to enhance task success and reduce operator workload through compliant haptic assistance.",
      "teaser_image": "https://arxiv.org/html/2602.01092/x1.png",
      "debug_abstract": "Teleoperation of high-precision manipulation is con-strained by tight success tolerances and complex contact dy-namics, which make impending failures difficult for human operators to anticipate under partial observability. This paper proposes a value-guided, failure-aware framework for bimanual teleoperation that provides compliant haptic assistance while pre-serving continuous human authority. The framework is trained entirely from heterogeneous offline teleoperation data containing both successful and failed executions. Task feasibility is mod-eled as a conservative success score learned via Conservative Value Learning, yielding a risk-sensitive estimate that remains reliable under distribution shift. During online operation, the learned success score regulates the level of assistance, while a learned actor provides a corrective motion direction. Both are integrated through a joint-space impedance interface on the master side, yielding continuous guidance that steers the operator away from failure-prone actions without overriding intent. Experimental results on contact-rich manipulation tasks demonstrate improved task success rates and reduced operator workload compared to conventional teleoperation and shared-autonomy baselines, indicating that conservative value learning provides an effective mechanism for embedding failure awareness into bilateral teleoperation. Experimental videos are available at this https URL"
    },
    {
      "title": "UV-M3TL: A Unified and Versatile Multimodal Multi-Task Learning Framework for Assistive Driving Perception",
      "url": "https://arxiv.org/abs/2602.01594",
      "date": "2026-02-02",
      "authors_text": "Wenzhuo Liu, Qiannan Guo, Zhen Wang, Wenshuo Wang, Lei Yang",
      "is_highlight": false,
      "score": 8.0,
      "summary": "The UV-M3TL framework enhances ADAS by effectively recognizing driver behavior and context through dual-branch multimodal embedding and adaptive loss, achieving state-of-the-art performance across multiple tasks.",
      "teaser_image": "https://arxiv.org/html/2602.01594/x2.png",
      "debug_abstract": "Advanced Driver Assistance Systems (ADAS) need to understand human driver behavior while perceiving their navigation context, but jointly learning these heterogeneous tasks would cause inter-task negative transfer and impair system performance. Here, we propose a Unified and Versatile Multimodal Multi-Task Learning (UV-M3TL) framework to simultaneously recognize driver behavior, driver emotion, vehicle behavior, and traffic context, while mitigating inter-task negative transfer. Our framework incorporates two core components: dual-branch spatial channel multimodal embedding (DB-SCME) and adaptive feature-decoupled multi-task loss (AFD-Loss). DB-SCME enhances cross-task knowledge transfer while mitigating task conflicts by employing a dual-branch structure to explicitly model salient task-shared and task-specific features. AFD-Loss improves the stability of joint optimization while guiding the model to learn diverse multi-task representations by introducing an adaptive weighting mechanism based on learning dynamics and feature decoupling constraints. We evaluate our method on the AIDE dataset, and the experimental results demonstrate that UV-M3TL achieves state-of-the-art performance across all four tasks. To further prove the versatility, we evaluate UV-M3TL on additional public multi-task perception benchmarks (BDD100K, CityScapes, NYUD-v2, and PASCAL-Context), where it consistently delivers strong performance across diverse task combinations, attaining state-of-the-art results on most tasks."
    },
    {
      "title": "FlowSteer: Interactive Agentic Workflow Orchestration via End-to-End Reinforcement Learning",
      "url": "https://arxiv.org/abs/2602.01664",
      "date": "2026-02-02",
      "authors_text": "Mingda Zhang, Haoran Luo, Tiesunlong Shen, Qika Lin, Xiaoying Tang",
      "is_highlight": false,
      "score": 7.0,
      "summary": "FlowSteer is an end-to-end reinforcement learning framework that automates workflow orchestration through interactive learning, addressing manual costs and improving performance across diverse tasks.",
      "teaser_image": "https://arxiv.org/html/2602.01664/x1.png",
      "debug_abstract": "In recent years, a variety of powerful agentic workflows have been applied to solve a wide range of human problems. However, existing workflow orchestration still faces key challenges, including high manual cost, reliance on specific operators/large language models (LLMs), and sparse reward signals. To address these challenges, we propose FlowSteer, an end-to-end reinforcement learning framework that takes a lightweight policy model as the agent and an executable canvas environment, automating workflow orchestration through multi-turn interaction. In this process, the policy model analyzes execution states and selects editing actions, while the canvas executes operators and returns feedback for iterative refinement. Moreover, FlowSteer provides a plug-and-play framework that supports diverse operator libraries and interchangeable LLM backends. To effectively train this interaction paradigm, we propose Canvas Workflow Relative Policy Optimization (CWRPO), which introduces diversity-constrained rewards with conditional release to stabilize learning and suppress shortcut behaviors. Experimental results on twelve datasets show that FlowSteer significantly outperforms baselines across various tasks."
    },
    {
      "title": "FD-VLA: Force-Distilled Vision-Language-Action Model for Contact-Rich Manipulation",
      "url": "https://arxiv.org/abs/2602.02142",
      "date": "2026-02-02",
      "authors_text": "Ruiteng Zhao, Wenshuo Wang, Yicheng Ma, Xiaocong Li, Francis E. H. Tay",
      "is_highlight": false,
      "score": 0,
      "summary": "The FD-VLA framework enhances contact-rich manipulation by integrating force awareness through a novel distillation method, improving performance without physical force sensors.",
      "teaser_image": "https://arxiv.org/html/2602.02142/x1.png",
      "debug_abstract": "Force sensing is a crucial modality for Vision-Language-Action (VLA) frameworks, as it enables fine-grained perception and dexterous manipulation in contact-rich tasks. We present Force-Distilled VLA (FD-VLA), a novel framework that integrates force awareness into contact-rich manipulation without relying on physical force sensors. The core of our approach is a Force Distillation Module (FDM), which distills force by mapping a learnable query token, conditioned on visual observations and robot states, into a predicted force token aligned with the latent representation of actual force signals. During inference, this distilled force token is injected into the pretrained VLM, enabling force-aware reasoning while preserving the integrity of its vision-language semantics. This design provides two key benefits: first, it allows practical deployment across a wide range of robots that lack expensive or fragile force-torque sensors, thereby reducing hardware cost and complexity; second, the FDM introduces an additional force-vision-state fusion prior to the VLM, which improves cross-modal alignment and enhances perception-action robustness in contact-rich scenarios. Surprisingly, our physical experiments show that the distilled force token outperforms direct sensor force measurements as well as other baselines, which highlights the effectiveness of this force-distilled VLA approach."
    },
    {
      "title": "DynamicVLA: A Vision-Language-Action Model for Dynamic Object Manipulation",
      "url": "https://arxiv.org/abs/2601.22153",
      "date": "2026-01-29",
      "authors_text": "Haozhe Xie, Beichen Wen, Jiarui Zheng, Zhaoxi Chen, Fangzhou Hong",
      "is_highlight": false,
      "score": 93.0,
      "summary": "DynamicVLA is a novel Vision-Language-Action framework that enhances dynamic object manipulation through efficient encoding, continuous inference, and a new benchmark for data collection.",
      "teaser_image": "https://arxiv.org/html/2601.22153/x1.png",
      "debug_abstract": "Manipulating dynamic objects remains an open challenge for Vision-Language-Action (VLA) models, which, despite strong generalization in static manipulation, struggle in dynamic scenarios requiring rapid perception, temporal anticipation, and continuous control. We present DynamicVLA, a framework for dynamic object manipulation that integrates temporal reasoning and closed-loop adaptation through three key designs: 1) a compact 0.4B VLA using a convolutional vision encoder for spatially efficient, structurally faithful encoding, enabling fast multimodal inference; 2) Continuous Inference, enabling overlapping reasoning and execution for lower latency and timely adaptation to object motion; and 3) Latent-aware Action Streaming, which bridges the perception-execution gap by enforcing temporally aligned action execution. To fill the missing foundation of dynamic manipulation data, we introduce the Dynamic Object Manipulation (DOM) benchmark, built from scratch with an auto data collection pipeline that efficiently gathers 200K synthetic episodes across 2.8K scenes and 206 objects, and enables fast collection of 2K real-world episodes without teleoperation. Extensive evaluations demonstrate remarkable improvements in response speed, perception, and generalization, positioning DynamicVLA as a unified framework for general dynamic object manipulation across embodiments."
    },
    {
      "title": "E2HiL: Entropy-Guided Sample Selection for Efficient Real-World Human-in-the-Loop Reinforcement Learning",
      "url": "https://arxiv.org/abs/2601.19969",
      "date": "2026-01-27",
      "authors_text": "Haoyuan Deng, Yuanjiang Xue, Haoyang Du, Boyang Zhou, Zhenyu Wu",
      "is_highlight": false,
      "score": 80.0,
      "summary": "E2HiL enhances human-in-the-loop reinforcement learning by efficiently selecting informative samples, achieving 42.1% higher success rates with 10.1% fewer human interventions.",
      "teaser_image": "https://arxiv.org/html/2601.19969/x2.png",
      "debug_abstract": "Human-in-the-loop guidance has emerged as an effective approach for enabling faster convergence in online reinforcement learning (RL) of complex real-world manipulation tasks. However, existing human-in-the-loop RL (HiL-RL) frameworks often suffer from low sample efficiency, requiring substantial human interventions to achieve convergence and thereby leading to high labor costs. To address this, we propose a sample-efficient real-world human-in-the-loop RL framework named \\method, which requires fewer human intervention by actively selecting informative samples. Specifically, stable reduction of policy entropy enables improved trade-off between exploration and exploitation with higher sample efficiency. We first build influence functions of different samples on the policy entropy, which is efficiently estimated by the covariance of action probabilities and soft advantages of policies. Then we select samples with moderate values of influence functions, where shortcut samples that induce sharp entropy drops and noisy samples with negligible effect are pruned. Extensive experiments on four real-world manipulation tasks demonstrate that \\method achieves a 42.1\\% higher success rate while requiring 10.1\\% fewer human interventions compared to the state-of-the-art HiL-RL method, validating its effectiveness. The project page providing code, videos, and mathematical formulations can be found at this https URL."
    },
    {
      "title": "RF-MatID: Dataset and Benchmark for Radio Frequency Material Identification",
      "url": "https://arxiv.org/abs/2601.20377",
      "date": "2026-01-28",
      "authors_text": "Xinyan Chen, Qinchun Li, Ruiqin Ma, Jiaqi Bai, Li Yi",
      "is_highlight": false,
      "score": 58.0,
      "summary": "RF-MatID introduces a comprehensive open-source RF dataset and benchmark for fine-grained material identification, addressing limitations of existing optical methods and enhancing algorithmic development.",
      "teaser_image": "https://arxiv.org/html/2601.20377/figures/materials.png",
      "debug_abstract": "Accurate material identification plays a crucial role in embodied AI systems, enabling a wide range of applications. However, current vision-based solutions are limited by the inherent constraints of optical sensors, while radio-frequency (RF) approaches, which can reveal intrinsic material properties, have received growing attention. Despite this progress, RF-based material identification remains hindered by the lack of large-scale public datasets and the limited benchmarking of learning-based approaches. In this work, we present RF-MatID, the first open-source, large-scale, wide-band, and geometry-diverse RF dataset for fine-grained material identification. RF-MatID includes 16 fine-grained categories grouped into 5 superclasses, spanning a broad frequency range from 4 to 43.5 GHz, and comprises 142k samples in both frequency- and time-domain representations. The dataset systematically incorporates controlled geometry perturbations, including variations in incidence angle and stand-off distance. We further establish a multi-setting, multi-protocol benchmark by evaluating state-of-the-art deep learning models, assessing both in-distribution performance and out-of-distribution robustness under cross-angle and cross-distance shifts. The 5 frequency-allocation protocols enable systematic frequency- and region-level analysis, thereby facilitating real-world deployment. RF-MatID aims to enable reproducible research, accelerate algorithmic advancement, foster cross-domain robustness, and support the development of real-world application in RF-based material identification."
    },
    {
      "title": "Video-KTR: Reinforcing Video Reasoning via Key Token Attribution",
      "url": "https://arxiv.org/abs/2601.19686",
      "date": "2026-01-27",
      "authors_text": "Ziyue Wang, Sheng Jin, Zhongrong Zuo, Jiawei Wu, Han Qiu",
      "is_highlight": false,
      "score": 66.0,
      "summary": "Video-KTR enhances video reasoning in multimodal models by selectively reinforcing key tokens through a novel attribution framework, achieving state-of-the-art performance across multiple benchmarks.",
      "debug_abstract": "Reinforcement learning (RL) has shown strong potential for enhancing reasoning in multimodal large language models, yet existing video reasoning methods often rely on coarse sequence-level rewards or single-factor token selection, neglecting fine-grained links among visual inputs, temporal dynamics, and linguistic outputs, limiting both accuracy and interpretability. We propose Video-KTR, a modality-aware policy shaping framework that performs selective, token-level RL by combining three attribution signals: (1) visual-aware tokens identified via counterfactual masking to reveal perceptual dependence; (2) temporal-aware tokens detected through frame shuffling to expose temporal sensitivity; and (3) high-entropy tokens signaling predictive uncertainty. By reinforcing only these key tokens, Video-KTR focuses learning on semantically informative, modality-sensitive content while filtering out low-value tokens. Across five challenging benchmarks, Video-KTR achieves state-of-the-art or highly competitive results, achieving 42.7\\% on Video-Holmes (surpassing GPT-4o) with consistent gains on both reasoning and general video understanding tasks. Ablation studies verify the complementary roles of the attribution signals and the robustness of targeted token-level updates. Overall, Video-KTR improves accuracy and interpretability, offering a simple, drop-in extension to RL for complex video reasoning. Our code and models are available at this https URL."
    },
    {
      "title": "RoamScene3D: Immersive Text-to-3D Scene Generation via Adaptive Object-aware Roaming",
      "url": "https://arxiv.org/abs/2601.19433",
      "date": "2026-01-27",
      "authors_text": "Jisheng Chu, Wenrui Li, Rui Zhao, Wangmeng Zuo, Shifeng Chen",
      "is_highlight": false,
      "score": 67.0,
      "summary": "RoamScene3D introduces a novel framework for immersive text-to-3D scene generation that enhances spatial awareness and object relations, outperforming existing methods in realism and consistency.",
      "debug_abstract": "Generating immersive 3D scenes from texts is a core task in computer vision, crucial for applications in virtual reality and game development. Despite the promise of leveraging 2D diffusion priors, existing methods suffer from spatial blindness and rely on predefined trajectories that fail to exploit the inner relationships among salient objects. Consequently, these approaches are unable to comprehend the semantic layout, preventing them from exploring the scene adaptively to infer occluded content. Moreover, current inpainting models operate in 2D image space, struggling to plausibly fill holes caused by camera motion. To address these limitations, we propose RoamScene3D, a novel framework that bridges the gap between semantic guidance and spatial generation. Our method reasons about the semantic relations among objects and produces consistent and photorealistic scenes. Specifically, we employ a vision-language model (VLM) to construct a scene graph that encodes object relations, guiding the camera to perceive salient object boundaries and plan an adaptive roaming trajectory. Furthermore, to mitigate the limitations of static 2D priors, we introduce a Motion-Injected Inpainting model that is fine-tuned on a synthetic panoramic dataset integrating authentic camera trajectories, making it adaptive to camera motion. Extensive experiments demonstrate that with semantic reasoning and geometric constraints, our method significantly outperforms state-of-the-art approaches in producing consistent and photorealistic scenes. Our code is available at this https URL."
    },
    {
      "title": "Physical Prompt Injection Attacks on Large Vision-Language Models",
      "url": "https://arxiv.org/abs/2601.17383",
      "date": "2026-01-24",
      "authors_text": "Chen Ling, Kai Hu, Hangcheng Liu, Xingshuo Han, Tianwei Zhang",
      "is_highlight": false,
      "score": 55.0,
      "summary": "The paper introduces Physical Prompt Injection Attacks (PPIA), a novel, highly effective method for manipulating Large Vision-Language Models using visually embedded prompts in physical environments.",
      "debug_abstract": "Large Vision-Language Models (LVLMs) are increasingly deployed in real-world intelligent systems for perception and reasoning in open physical environments. While LVLMs are known to be vulnerable to prompt injection attacks, existing methods either require access to input channels or depend on knowledge of user queries, assumptions that rarely hold in practical deployments. We propose the first Physical Prompt Injection Attack (PPIA), a black-box, query-agnostic attack that embeds malicious typographic instructions into physical objects perceivable by the LVLM. PPIA requires no access to the model, its inputs, or internal pipeline, and operates solely through visual observation. It combines offline selection of highly recognizable and semantically effective visual prompts with strategic environment-aware placement guided by spatiotemporal attention, ensuring that the injected prompts are both perceivable and influential on model behavior. We evaluate PPIA across 10 state-of-the-art LVLMs in both simulated and real-world settings on tasks including visual question answering, planning, and navigation, PPIA achieves attack success rates up to 98%, with strong robustness under varying physical conditions such as distance, viewpoint, and illumination. Our code is publicly available at this https URL."
    },
    {
      "title": "Delay-Compensated Stiffness Estimation for Robot-Mediated Dyadic Interaction",
      "url": "https://arxiv.org/abs/2601.17812",
      "date": "2026-01-25",
      "authors_text": "Mingtian Du, Suhas Raghavendra Kulkarni, Bernardo Noronha, Domenico Campolo",
      "is_highlight": false,
      "score": 80.0,
      "summary": "The paper presents a delay-compensated stiffness estimation framework for robot-mediated dyadic interactions, improving accuracy in remote physical therapy despite network-induced haptic delays.",
      "debug_abstract": "Robot-mediated human-human (dyadic) interactions enable therapists to provide physical therapy remotely, yet an accurate perception of patient stiffness remains challenging due to network-induced haptic delays. Conventional stiffness estimation methods, which neglect delay, suffer from temporal misalignment between force and position signals, leading to significant estimation errors as delays increase. To address this, we propose a robust, delay-compensated stiffness estimation framework by deriving an algebraic estimator based on quasi-static equilibrium that explicitly accounts for temporally aligning the expert&#39;s input with the novice&#39;s response. A Normalised Weighted Least Squares (NWLS) implementation is then introduced to robustly filter dynamic bias resulting from the algebraic derivation. Experiments using commercial rehabilitation robots (H-MAN) as the platform demonstrate that the proposed method significantly outperforms the standard estimator, maintaining consistent tracking accuracy under multiple introduced delays. These findings offer a promising solution for achieving high-fidelity haptic perception in remote dyadic interaction, potentially facilitating reliable stiffness assessment in therapeutic settings across networks."
    },
    {
      "title": "ORION: Option-Regularized Deep Reinforcement Learning for Cooperative Multi-Agent Online Navigation",
      "url": "https://arxiv.org/abs/2601.01155",
      "date": "2026-01-26",
      "authors_text": "Shizhe Zhang, Jingsong Liang, Zhitao Zhou, Shuhan Ye, Yizhuo Wang",
      "is_highlight": false,
      "score": 88.0,
      "summary": "ORION is a deep reinforcement learning framework that enables decentralized, cooperative navigation for multi-agent systems in partially known environments by integrating prior maps and online observations.",
      "debug_abstract": "Existing methods for multi-agent navigation typically assume fully known environments, offering limited support for partially known scenarios such as warehouses or factory floors. There, agents may need to plan trajectories that balance their own path optimality with their ability to collect and share information about the environment that can help their teammates reach their own goals. To these ends, we propose ORION, a novel deep reinforcement learning framework for cooperative multi-agent online navigation in partially known environments. Starting from an imperfect prior map, ORION trains agents to make decentralized decisions, coordinate to reach their individual targets, and actively reduce map uncertainty by sharing online observations in a closed perception-action loop. We first design a shared graph encoder that fuses prior map with online perception into a unified representation, providing robust state embeddings under dynamic map discrepancies. At the core of ORION is an option-critic framework that learns to reason about a set of high-level cooperative modes that translate into sequences of low-level actions, allowing agents to switch between individual navigation and team-level exploration adaptively. We further introduce a dual-stage cooperation strategy that enables agents to assist teammates under map uncertainty, thereby reducing the overall makespan. Across extensive maze-like maps and large-scale warehouse environments, our simulation results show that ORION achieves high-quality, real-time decentralized cooperation over varying team sizes, outperforming state-of-the-art classical and learning-based baselines. Finally, we validate ORION on physical robot teams, demonstrating its robustness and practicality for real-world cooperative navigation."
    },
    {
      "title": "Resisting Manipulative Bots in Meme Coin Copy Trading: A Multi-Agent Approach with Chain-of-Thought Reasoning",
      "url": "https://arxiv.org/abs/2601.08641",
      "date": "2026-01-26",
      "authors_text": "Yichen Luo, Yebo Feng, Jiahua Xu, Yang Liu",
      "is_highlight": false,
      "score": 67.0,
      "summary": "This paper presents a multi-agent system utilizing explainable LLMs to defend against manipulative bots in meme coin copy trading, enhancing prediction accuracy and economic performance.",
      "debug_abstract": "Copy trading has become the dominant entry strategy in meme coin markets. However, due to the market&#39;s extreme illiquid and volatile nature, the strategy exposes an exploitable attack surface: adversaries deploy manipulative bots to front-run trades, conceal positions, and fabricate sentiment, systematically extracting value from naïve copiers at scale. Despite its prevalence, bot-driven manipulation remains largely unexplored, and no robust defensive framework exists. We propose a manipulation-resistant copy-trading system based on a multi-agent architecture powered by a multi-modal, explainable large language model (LLM). Our system decomposes copy trading into three specialized agents for coin evaluation, wallet selection, and timing assessment. Evaluated on historical data from over 6,000 meme coins, our approach outperforms zero-shot and most statistic-driven baselines in prediction accuracy as well as all baselines in economic performance, achieving an average return of 14% for identified smart-money trades and an estimated copier return of 3% per trade under realistic market frictions. Overall, our results demonstrate the effectiveness of agent-based defenses and predictability of trader profitability in adversarial meme coin markets, providing a practical foundation for robust copy trading."
    },
    {
      "title": "OnlineSI: Taming Large Language Model for Online 3D Understanding and Grounding",
      "url": "https://arxiv.org/abs/2601.16538",
      "date": "2026-01-23",
      "authors_text": "Zixian Liu, Zhaoxi Chen, Liang Pan, Ziwei Liu",
      "is_highlight": false,
      "score": 88.0,
      "summary": "OnlineSI is a framework that enhances Multimodal Large Language Models' spatial understanding in dynamic environments by integrating continuous learning from video streams and 3D point cloud data.",
      "debug_abstract": "In recent years, researchers have increasingly been interested in how to enable Multimodal Large Language Models (MLLM) to possess spatial understanding and reasoning capabilities. However, most existing methods overlook the importance of the ability to continuously work in an ever-changing world, and lack the possibility of deployment on embodied systems in real-world environments. In this work, we introduce OnlineSI, a framework that can continuously improve its spatial understanding of its surroundings given a video stream. Our core idea is to maintain a finite spatial memory to retain past observations, ensuring the computation required for each inference does not increase as the input accumulates. We further integrate 3D point cloud information with semantic information, helping MLLM to better locate and identify objects in the scene. To evaluate our method, we introduce the Fuzzy $F_1$-Score to mitigate ambiguity, and test our method on two representative datasets. Experiments demonstrate the effectiveness of our method, paving the way towards real-world embodied systems."
    },
    {
      "title": "Accurate Calibration and Robust LiDAR-Inertial Odometry for Spinning Actuated LiDAR Systems",
      "url": "https://arxiv.org/abs/2601.15946",
      "date": "2026-01-22",
      "authors_text": "Zijie Chen, Xiaowei Liu, Yong Xu, Shenghai Yuan, Jianping Li",
      "is_highlight": false,
      "score": 74.5,
      "summary": "This paper introduces LM-Calibr for targetless LiDAR-motor calibration and EVA-LIO for adaptive LiDAR-inertial odometry, enhancing localization in spinning actuated LiDAR systems.",
      "debug_abstract": "Accurate calibration and robust localization are fundamental for downstream tasks in spinning actuated LiDAR applications. Existing methods, however, require parameterizing extrinsic parameters based on different mounting configurations, limiting their generalizability. Additionally, spinning actuated LiDAR inevitably scans featureless regions, which complicates the balance between scanning coverage and localization robustness. To address these challenges, this letter presents a targetless LiDAR-motor calibration (LM-Calibr) on the basis of the Denavit-Hartenberg convention and an environmental adaptive LiDAR-inertial odometry (EVA-LIO). LM-Calibr supports calibration of LiDAR-motor systems with various mounting configurations. Extensive experiments demonstrate its accuracy and convergence across different scenarios, mounting angles, and initial values. Additionally, EVA-LIO adaptively selects downsample rates and map resolutions according to spatial scale. This adaptivity enables the actuator to operate at maximum speed, thereby enhancing scanning completeness while ensuring robust localization, even when LiDAR briefly scans featureless areas. The source code and hardware design are available on GitHub: \\textcolor{blue}{\\href{this https URL}{this http URL\\_calibr}}. The video is available at \\textcolor{blue}{\\href{this https URL}{this http URL}}"
    }
  ],
  "MReaLLab": [
    {
      "title": "Bongards at the Boundary of Perception and Reasoning: Programs or Language?",
      "url": "https://arxiv.org/abs/2602.03038",
      "date": "2026-02-03",
      "authors_text": "Cassidy Langenfeld, Claas Beger, Gloria Geng, Wasu Top Piriyakulkij, Keya Hu",
      "is_highlight": false,
      "score": 60.0,
      "summary": "This paper introduces a neurosymbolic method utilizing LLMs and Bayesian optimization to tackle Bongard problems, enhancing visual reasoning beyond conventional VLM capabilities.",
      "teaser_image": "https://arxiv.org/html/2602.03038/x3.png",
      "debug_abstract": "Vision-Language Models (VLMs) have made great strides in everyday visual tasks, such as captioning a natural image, or answering commonsense questions about such images. But humans possess the puzzling ability to deploy their visual reasoning abilities in radically new situations, a skill rigorously tested by the classic set of visual reasoning challenges known as the Bongard problems. We present a neurosymbolic approach to solving these problems: given a hypothesized solution rule for a Bongard problem, we leverage LLMs to generate parameterized programmatic representations for the rule and perform parameter fitting using Bayesian optimization. We evaluate our method on classifying Bongard problem images given the ground truth rule, as well as on solving the problems from scratch."
    },
    {
      "title": "Estimation of Ground Reaction Forces from Kinematic Data during Locomotion",
      "url": "https://arxiv.org/abs/2602.03177",
      "date": "2026-02-03",
      "authors_text": "Gautami Golani, Dong Anh Khoa To, Ananda Sidarta, Arun-Kumar Kaliya-Perumal, Oliver Roberts",
      "is_highlight": false,
      "score": 44.0,
      "summary": "This study introduces a force-plate-free method for estimating ground reaction forces from kinematic data, enhancing clinical gait analysis without the need for specialized equipment.",
      "teaser_image": "https://arxiv.org/html/2602.03177/x2.png",
      "debug_abstract": "Ground reaction forces (GRFs) provide fundamental insight into human gait mechanics and are widely used to assess joint loading, limb symmetry, balance control, and motor function. Despite their clinical relevance, the use of GRF remains underutilised in clinical workflows due to the practical limitations of force plate systems. In this work, we present a force-plate-free approach for estimating GRFs using only marker-based motion capture data. This kinematics only method to estimate and decompose GRF makes it well suited for widespread clinical depolyment. By using kinematics from sixteen body segments, we estimate the centre of mass (CoM) and compute GRFs, which are subsequently decomposed into individual components through a minimization-based approach. Through this framework, we can identify gait stance phases and provide access to clinically meaningful kinetic measures without a dedicated force plate system. Experimental results demonstrate the viability of CoM and GRF estimation based solely on kinematic data, supporting force-plate-free gait analysis."
    },
    {
      "title": "Self-Guard: Defending Large Reasoning Models via enhanced self-reflection",
      "url": "https://arxiv.org/abs/2602.00707",
      "date": "2026-01-31",
      "authors_text": "Jingnan Zheng, Jingjun Xu, Yanzhen Luo, Chenhang Cui, Gelei Deng",
      "is_highlight": false,
      "score": 50.0,
      "summary": "Self-Guard is a lightweight framework enhancing safety compliance in Large Reasoning Models by promoting self-reflection and mitigating risks without sacrificing model utility.",
      "teaser_image": "https://arxiv.org/html/2602.00707/materials/workflow.png",
      "debug_abstract": "The emergence of Large Reasoning Models (LRMs) introduces a new paradigm of explicit reasoning, enabling remarkable advances yet posing unique risks such as reasoning manipulation and information leakage. To mitigate these risks, current alignment strategies predominantly rely on heavy post-training paradigms or external interventions. However, these approaches are often computationally intensive and fail to address the inherent awareness-compliance gap, a critical misalignment where models recognize potential risks yet prioritize following user instructions due to their sycophantic tendencies. To address these limitations, we propose Self-Guard, a lightweight safety defense framework that reinforces safety compliance at the representational level. Self-Guard operates through two principal stages: (1) safety-oriented prompting, which activates the model&#39;s latent safety awareness to evoke spontaneous reflection, and (2) safety activation steering, which extracts the resulting directional shift in the hidden state space and amplifies it to ensure that safety compliance prevails over sycophancy during inference. Experiments demonstrate that Self-Guard effectively bridges the awareness-compliance gap, achieving robust safety performance without compromising model utility. Furthermore, Self-Guard exhibits strong generalization across diverse unseen risks and varying model scales, offering a cost-efficient solution for LRM safety alignment."
    },
    {
      "title": "Learning Adaptive Cross-Embodiment Visuomotor Policy with Contrastive Prompt Orchestration",
      "url": "https://arxiv.org/abs/2602.01040",
      "date": "2026-02-01",
      "authors_text": "Yuhang Zhang, Chao Yan, Jiaxi Yu, Jiaping Xiao, Mir Feroskhan",
      "is_highlight": false,
      "score": 92.0,
      "summary": "The paper presents ContrAstive Prompt Orchestration (CAPO), a method for improving adaptive visuomotor policies in diverse environments through hybrid contrastive learning and dynamic prompt orchestration.",
      "teaser_image": "https://arxiv.org/html/2602.01040/x4.png",
      "debug_abstract": "Learning adaptive visuomotor policies for embodied agents remains a formidable challenge, particularly when facing cross-embodiment variations such as diverse sensor configurations and dynamic properties. Conventional learning approaches often struggle to separate task-relevant features from domain-specific variations (e.g., lighting, field-of-view, and rotation), leading to poor sample efficiency and catastrophic failure in unseen environments. To bridge this gap, we propose ContrAstive Prompt Orchestration (CAPO), a novel approach for learning visuomotor policies that integrates contrastive prompt learning and adaptive prompt orchestration. For prompt learning, we devise a hybrid contrastive learning strategy that integrates visual, temporal action, and text objectives to establish a pool of learnable prompts, where each prompt induces a visual representation encapsulating fine-grained domain factors. Based on these learned prompts, we introduce an adaptive prompt orchestration mechanism that dynamically aggregates these prompts conditioned on current observations. This enables the agent to adaptively construct optimal state representations by identifying dominant domain factors instantaneously. Consequently, the policy optimization is effectively shielded from irrelevant interference, preventing the common issue of overfitting to source domains. Extensive experiments demonstrate that CAPO significantly outperforms state-of-the-art baselines in sample efficiency and asymptotic performance. Crucially, it exhibits superior zero-shot adaptation across unseen target domains characterized by drastic environmental (e.g., illumination) and physical shifts (e.g., field-of-view and rotation), validating its effectiveness as a viable solution for cross-embodiment visuomotor policy adaptation."
    },
    {
      "title": "Estimating Force Interactions of Deformable Linear Objects from their Shapes",
      "url": "https://arxiv.org/abs/2602.01085",
      "date": "2026-02-01",
      "authors_text": "Qi Jing Chen, Shilin Shan, Timothy Bretl, Quang-Cuong Pham",
      "is_highlight": false,
      "score": 83.0,
      "summary": "This paper presents a novel method for estimating external forces on deformable linear objects using shape data, enhancing robot interaction safety and efficiency without requiring force sensors.",
      "teaser_image": "https://arxiv.org/html/2602.01085/x2.png",
      "debug_abstract": "This work introduces an analytical approach for detecting and estimating external forces acting on deformable linear objects (DLOs) using only their observed shapes. In many robot-wire interaction tasks, contact occurs not at the end-effector but at other points along the robot&#39;s body. Such scenarios arise when robots manipulate wires indirectly (e.g., by nudging) or when wires act as passive obstacles in the environment. Accurately identifying these interactions is crucial for safe and efficient trajectory planning, helping to prevent wire damage, avoid restricted robot motions, and mitigate potential hazards. Existing approaches often rely on expensive external force-torque sensor or that contacts occur at the end-effector for accurate force estimation. Using wire shape information acquired from a depth camera and under the assumption that the wire is in or near its static equilibrium, our method estimates both the location and magnitude of external forces without additional prior knowledge. This is achieved by exploiting derived consistency conditions and solving a system of linear equations based on force-torque balance along the wire. The approach was validated through simulation, where it achieved high accuracy, and through real-world experiments, where accurate estimation was demonstrated in selected interaction scenarios."
    },
    {
      "title": "SGHA-Attack: Semantic-Guided Hierarchical Alignment for Transferable Targeted Attacks on Vision-Language Models",
      "url": "https://arxiv.org/abs/2602.01574",
      "date": "2026-02-02",
      "authors_text": "Haobo Wang, Weiqi Luo, Xiaojun Jia, Xiaochun Cao",
      "is_highlight": false,
      "score": 44.0,
      "summary": "SGHA-Attack enhances targeted adversarial attacks on vision-language models by leveraging multiple semantic references and aligning intermediate features for improved transferability and robustness.",
      "teaser_image": "https://arxiv.org/html/2602.01574/x1.png",
      "debug_abstract": "Large vision-language models (VLMs) are vulnerable to transfer-based adversarial perturbations, enabling attackers to optimize on surrogate models and manipulate black-box VLM outputs. Prior targeted transfer attacks often overfit surrogate-specific embedding space by relying on a single reference and emphasizing final-layer alignment, which underutilizes intermediate semantics and degrades transfer across heterogeneous VLMs. To address this, we propose SGHA-Attack, a Semantic-Guided Hierarchical Alignment framework that adopts multiple target references and enforces intermediate-layer consistency. Concretely, we generate a visually grounded reference pool by sampling a frozen text-to-image model conditioned on the target prompt, and then carefully select the Top-K most semantically relevant anchors under the surrogate to form a weighted mixture for stable optimization guidance. Building on these anchors, SGHA-Attack injects target semantics throughout the feature hierarchy by aligning intermediate visual representations at both global and spatial granularities across multiple depths, and by synchronizing intermediate visual and textual features in a shared latent subspace to provide early cross-modal supervision before the final projection. Extensive experiments on open-source and commercial black-box VLMs show that SGHA-Attack achieves stronger targeted transferability than prior methods and remains robust under preprocessing and purification defenses."
    },
    {
      "title": "ClueTracer: Question-to-Vision Clue Tracing for Training-Free Hallucination Suppression in Multimodal Reasoning",
      "url": "https://arxiv.org/abs/2602.02004",
      "date": "2026-02-02",
      "authors_text": "Gongli Xi, Kun Wang, Zeming Gao, Huahui Yi, Haolang Lu",
      "is_highlight": false,
      "score": 24.0,
      "summary": "ClueTracer is a training-free plugin that suppresses hallucinations in multimodal reasoning by tracing visual clue propagation, enhancing model performance across various architectures.",
      "teaser_image": "https://arxiv.org/html/2602.02004/x1.png",
      "debug_abstract": "Large multimodal reasoning models solve challenging visual problems via explicit long-chain inference: they gather visual clues from images and decode clues into textual tokens. Yet this capability also increases hallucinations, where the model generates content that is not supported by the input image or the question. To understand this failure mode, we identify \\emph{reasoning drift}: during clue gathering, the model over-focuses on question-irrelevant entities, diluting focus on task-relevant cues and gradually decoupling the reasoning trace from visual grounding. As a consequence, many inference-time localization or intervention methods developed for non-reasoning models fail to pinpoint the true clues in reasoning settings. Motivated by these insights, we introduce ClueRecall, a metric for assessing visual clue retrieval, and present ClueTracer, a training-free, parameter-free, and architecture-agnostic plugin for hallucination suppression. ClueTracer starts from the question and traces how key clues propagate along the model&#39;s reasoning pathway (question $\\rightarrow$ outputs $\\rightarrow$ visual tokens), thereby localizing task-relevant patches while suppressing spurious attention to irrelevant regions. Remarkably, \\textbf{without any additional training}, ClueTracer improves all \\textbf{reasoning} architectures (including \\texttt{R1-OneVision}, \\texttt{Ocean-R1}, \\texttt{MM-Eureka}, \\emph{etc}.) by $\\mathbf{1.21\\times}$ on reasoning benchmarks. When transferred to \\textbf{non-reasoning} settings, it yields a $\\mathbf{1.14\\times}$ gain."
    },
    {
      "title": "Superman: Unifying Skeleton and Vision for Human Motion Perception and Generation",
      "url": "https://arxiv.org/abs/2602.02401",
      "date": "2026-02-02",
      "authors_text": "Xinshun Wang, Peiming Li, Ziyi Wang, Zhongbin Fang, Zhichao Deng",
      "is_highlight": false,
      "score": 15.0,
      "summary": "Superman unifies visual perception and skeleton-based motion generation through a Vision-Guided Motion Tokenizer, enabling robust cross-modal learning and superior performance in human motion tasks.",
      "teaser_image": "https://arxiv.org/html/2602.02401/x2.png",
      "debug_abstract": "Human motion analysis tasks, such as temporal 3D pose estimation, motion prediction, and motion in-betweening, play an essential role in computer vision. However, current paradigms suffer from severe fragmentation. First, the field is split between ``perception&#39;&#39; models that understand motion from video but only output text, and ``generation&#39;&#39; models that cannot perceive from raw visual input. Second, generative MLLMs are often limited to single-frame, static poses using dense, parametric SMPL models, failing to handle temporal motion. Third, existing motion vocabularies are built from skeleton data alone, severing the link to the visual domain. To address these challenges, we introduce Superman, a unified framework that bridges visual perception with temporal, skeleton-based motion generation. Our solution is twofold. First, to overcome the modality disconnect, we propose a Vision-Guided Motion Tokenizer. Leveraging the natural geometric alignment between 3D skeletons and visual data, this module pioneers robust joint learning from both modalities, creating a unified, cross-modal motion vocabulary. Second, grounded in this motion language, a single, unified MLLM architecture is trained to handle all tasks. This module flexibly processes diverse, temporal inputs, unifying 3D skeleton pose estimation from video (perception) with skeleton-based motion prediction and in-betweening (generation). Extensive experiments on standard benchmarks, including Human3.6M, demonstrate that our unified method achieves state-of-the-art or competitive performance across all motion tasks. This showcases a more efficient and scalable path for generative motion analysis using skeletons."
    },
    {
      "title": "Failure-Aware Bimanual Teleoperation via Conservative Value Guided Assistance",
      "url": "https://arxiv.org/abs/2602.01092",
      "date": "2026-02-01",
      "authors_text": "Peng Zhou, Zhongxuan Li, Jinsong Wu, Jiaming Qi, Jun Hu",
      "is_highlight": false,
      "score": 9.0,
      "summary": "This paper presents a failure-aware bimanual teleoperation framework that uses conservative value learning to enhance task success and reduce operator workload through compliant haptic assistance.",
      "teaser_image": "https://arxiv.org/html/2602.01092/x1.png",
      "debug_abstract": "Teleoperation of high-precision manipulation is con-strained by tight success tolerances and complex contact dy-namics, which make impending failures difficult for human operators to anticipate under partial observability. This paper proposes a value-guided, failure-aware framework for bimanual teleoperation that provides compliant haptic assistance while pre-serving continuous human authority. The framework is trained entirely from heterogeneous offline teleoperation data containing both successful and failed executions. Task feasibility is mod-eled as a conservative success score learned via Conservative Value Learning, yielding a risk-sensitive estimate that remains reliable under distribution shift. During online operation, the learned success score regulates the level of assistance, while a learned actor provides a corrective motion direction. Both are integrated through a joint-space impedance interface on the master side, yielding continuous guidance that steers the operator away from failure-prone actions without overriding intent. Experimental results on contact-rich manipulation tasks demonstrate improved task success rates and reduced operator workload compared to conventional teleoperation and shared-autonomy baselines, indicating that conservative value learning provides an effective mechanism for embedding failure awareness into bilateral teleoperation. Experimental videos are available at this https URL"
    },
    {
      "title": "UV-M3TL: A Unified and Versatile Multimodal Multi-Task Learning Framework for Assistive Driving Perception",
      "url": "https://arxiv.org/abs/2602.01594",
      "date": "2026-02-02",
      "authors_text": "Wenzhuo Liu, Qiannan Guo, Zhen Wang, Wenshuo Wang, Lei Yang",
      "is_highlight": false,
      "score": 8.0,
      "summary": "The UV-M3TL framework enhances ADAS by effectively recognizing driver behavior and context through dual-branch multimodal embedding and adaptive loss, achieving state-of-the-art performance across multiple tasks.",
      "teaser_image": "https://arxiv.org/html/2602.01594/x2.png",
      "debug_abstract": "Advanced Driver Assistance Systems (ADAS) need to understand human driver behavior while perceiving their navigation context, but jointly learning these heterogeneous tasks would cause inter-task negative transfer and impair system performance. Here, we propose a Unified and Versatile Multimodal Multi-Task Learning (UV-M3TL) framework to simultaneously recognize driver behavior, driver emotion, vehicle behavior, and traffic context, while mitigating inter-task negative transfer. Our framework incorporates two core components: dual-branch spatial channel multimodal embedding (DB-SCME) and adaptive feature-decoupled multi-task loss (AFD-Loss). DB-SCME enhances cross-task knowledge transfer while mitigating task conflicts by employing a dual-branch structure to explicitly model salient task-shared and task-specific features. AFD-Loss improves the stability of joint optimization while guiding the model to learn diverse multi-task representations by introducing an adaptive weighting mechanism based on learning dynamics and feature decoupling constraints. We evaluate our method on the AIDE dataset, and the experimental results demonstrate that UV-M3TL achieves state-of-the-art performance across all four tasks. To further prove the versatility, we evaluate UV-M3TL on additional public multi-task perception benchmarks (BDD100K, CityScapes, NYUD-v2, and PASCAL-Context), where it consistently delivers strong performance across diverse task combinations, attaining state-of-the-art results on most tasks."
    },
    {
      "title": "FlowSteer: Interactive Agentic Workflow Orchestration via End-to-End Reinforcement Learning",
      "url": "https://arxiv.org/abs/2602.01664",
      "date": "2026-02-02",
      "authors_text": "Mingda Zhang, Haoran Luo, Tiesunlong Shen, Qika Lin, Xiaoying Tang",
      "is_highlight": false,
      "score": 7.0,
      "summary": "FlowSteer is an end-to-end reinforcement learning framework that automates workflow orchestration through interactive learning, addressing manual costs and improving performance across diverse tasks.",
      "teaser_image": "https://arxiv.org/html/2602.01664/x1.png",
      "debug_abstract": "In recent years, a variety of powerful agentic workflows have been applied to solve a wide range of human problems. However, existing workflow orchestration still faces key challenges, including high manual cost, reliance on specific operators/large language models (LLMs), and sparse reward signals. To address these challenges, we propose FlowSteer, an end-to-end reinforcement learning framework that takes a lightweight policy model as the agent and an executable canvas environment, automating workflow orchestration through multi-turn interaction. In this process, the policy model analyzes execution states and selects editing actions, while the canvas executes operators and returns feedback for iterative refinement. Moreover, FlowSteer provides a plug-and-play framework that supports diverse operator libraries and interchangeable LLM backends. To effectively train this interaction paradigm, we propose Canvas Workflow Relative Policy Optimization (CWRPO), which introduces diversity-constrained rewards with conditional release to stabilize learning and suppress shortcut behaviors. Experimental results on twelve datasets show that FlowSteer significantly outperforms baselines across various tasks."
    },
    {
      "title": "FD-VLA: Force-Distilled Vision-Language-Action Model for Contact-Rich Manipulation",
      "url": "https://arxiv.org/abs/2602.02142",
      "date": "2026-02-02",
      "authors_text": "Ruiteng Zhao, Wenshuo Wang, Yicheng Ma, Xiaocong Li, Francis E. H. Tay",
      "is_highlight": false,
      "score": 0,
      "summary": "The FD-VLA framework enhances contact-rich manipulation by integrating force awareness through a novel distillation method, improving performance without physical force sensors.",
      "teaser_image": "https://arxiv.org/html/2602.02142/x1.png",
      "debug_abstract": "Force sensing is a crucial modality for Vision-Language-Action (VLA) frameworks, as it enables fine-grained perception and dexterous manipulation in contact-rich tasks. We present Force-Distilled VLA (FD-VLA), a novel framework that integrates force awareness into contact-rich manipulation without relying on physical force sensors. The core of our approach is a Force Distillation Module (FDM), which distills force by mapping a learnable query token, conditioned on visual observations and robot states, into a predicted force token aligned with the latent representation of actual force signals. During inference, this distilled force token is injected into the pretrained VLM, enabling force-aware reasoning while preserving the integrity of its vision-language semantics. This design provides two key benefits: first, it allows practical deployment across a wide range of robots that lack expensive or fragile force-torque sensors, thereby reducing hardware cost and complexity; second, the FDM introduces an additional force-vision-state fusion prior to the VLM, which improves cross-modal alignment and enhances perception-action robustness in contact-rich scenarios. Surprisingly, our physical experiments show that the distilled force token outperforms direct sensor force measurements as well as other baselines, which highlights the effectiveness of this force-distilled VLA approach."
    },
    {
      "title": "DynamicVLA: A Vision-Language-Action Model for Dynamic Object Manipulation",
      "url": "https://arxiv.org/abs/2601.22153",
      "date": "2026-01-29",
      "authors_text": "Haozhe Xie, Beichen Wen, Jiarui Zheng, Zhaoxi Chen, Fangzhou Hong",
      "is_highlight": false,
      "score": 93.0,
      "summary": "DynamicVLA is a novel Vision-Language-Action framework that enhances dynamic object manipulation through efficient encoding, continuous inference, and a new benchmark for data collection.",
      "teaser_image": "https://arxiv.org/html/2601.22153/x1.png",
      "debug_abstract": "Manipulating dynamic objects remains an open challenge for Vision-Language-Action (VLA) models, which, despite strong generalization in static manipulation, struggle in dynamic scenarios requiring rapid perception, temporal anticipation, and continuous control. We present DynamicVLA, a framework for dynamic object manipulation that integrates temporal reasoning and closed-loop adaptation through three key designs: 1) a compact 0.4B VLA using a convolutional vision encoder for spatially efficient, structurally faithful encoding, enabling fast multimodal inference; 2) Continuous Inference, enabling overlapping reasoning and execution for lower latency and timely adaptation to object motion; and 3) Latent-aware Action Streaming, which bridges the perception-execution gap by enforcing temporally aligned action execution. To fill the missing foundation of dynamic manipulation data, we introduce the Dynamic Object Manipulation (DOM) benchmark, built from scratch with an auto data collection pipeline that efficiently gathers 200K synthetic episodes across 2.8K scenes and 206 objects, and enables fast collection of 2K real-world episodes without teleoperation. Extensive evaluations demonstrate remarkable improvements in response speed, perception, and generalization, positioning DynamicVLA as a unified framework for general dynamic object manipulation across embodiments."
    },
    {
      "title": "E2HiL: Entropy-Guided Sample Selection for Efficient Real-World Human-in-the-Loop Reinforcement Learning",
      "url": "https://arxiv.org/abs/2601.19969",
      "date": "2026-01-27",
      "authors_text": "Haoyuan Deng, Yuanjiang Xue, Haoyang Du, Boyang Zhou, Zhenyu Wu",
      "is_highlight": false,
      "score": 80.0,
      "summary": "E2HiL enhances human-in-the-loop reinforcement learning by efficiently selecting informative samples, achieving 42.1% higher success rates with 10.1% fewer human interventions.",
      "teaser_image": "https://arxiv.org/html/2601.19969/x2.png",
      "debug_abstract": "Human-in-the-loop guidance has emerged as an effective approach for enabling faster convergence in online reinforcement learning (RL) of complex real-world manipulation tasks. However, existing human-in-the-loop RL (HiL-RL) frameworks often suffer from low sample efficiency, requiring substantial human interventions to achieve convergence and thereby leading to high labor costs. To address this, we propose a sample-efficient real-world human-in-the-loop RL framework named \\method, which requires fewer human intervention by actively selecting informative samples. Specifically, stable reduction of policy entropy enables improved trade-off between exploration and exploitation with higher sample efficiency. We first build influence functions of different samples on the policy entropy, which is efficiently estimated by the covariance of action probabilities and soft advantages of policies. Then we select samples with moderate values of influence functions, where shortcut samples that induce sharp entropy drops and noisy samples with negligible effect are pruned. Extensive experiments on four real-world manipulation tasks demonstrate that \\method achieves a 42.1\\% higher success rate while requiring 10.1\\% fewer human interventions compared to the state-of-the-art HiL-RL method, validating its effectiveness. The project page providing code, videos, and mathematical formulations can be found at this https URL."
    },
    {
      "title": "RF-MatID: Dataset and Benchmark for Radio Frequency Material Identification",
      "url": "https://arxiv.org/abs/2601.20377",
      "date": "2026-01-28",
      "authors_text": "Xinyan Chen, Qinchun Li, Ruiqin Ma, Jiaqi Bai, Li Yi",
      "is_highlight": false,
      "score": 58.0,
      "summary": "RF-MatID introduces a comprehensive open-source RF dataset and benchmark for fine-grained material identification, addressing limitations of existing optical methods and enhancing algorithmic development.",
      "teaser_image": "https://arxiv.org/html/2601.20377/figures/materials.png",
      "debug_abstract": "Accurate material identification plays a crucial role in embodied AI systems, enabling a wide range of applications. However, current vision-based solutions are limited by the inherent constraints of optical sensors, while radio-frequency (RF) approaches, which can reveal intrinsic material properties, have received growing attention. Despite this progress, RF-based material identification remains hindered by the lack of large-scale public datasets and the limited benchmarking of learning-based approaches. In this work, we present RF-MatID, the first open-source, large-scale, wide-band, and geometry-diverse RF dataset for fine-grained material identification. RF-MatID includes 16 fine-grained categories grouped into 5 superclasses, spanning a broad frequency range from 4 to 43.5 GHz, and comprises 142k samples in both frequency- and time-domain representations. The dataset systematically incorporates controlled geometry perturbations, including variations in incidence angle and stand-off distance. We further establish a multi-setting, multi-protocol benchmark by evaluating state-of-the-art deep learning models, assessing both in-distribution performance and out-of-distribution robustness under cross-angle and cross-distance shifts. The 5 frequency-allocation protocols enable systematic frequency- and region-level analysis, thereby facilitating real-world deployment. RF-MatID aims to enable reproducible research, accelerate algorithmic advancement, foster cross-domain robustness, and support the development of real-world application in RF-based material identification."
    },
    {
      "title": "Video-KTR: Reinforcing Video Reasoning via Key Token Attribution",
      "url": "https://arxiv.org/abs/2601.19686",
      "date": "2026-01-27",
      "authors_text": "Ziyue Wang, Sheng Jin, Zhongrong Zuo, Jiawei Wu, Han Qiu",
      "is_highlight": false,
      "score": 66.0,
      "summary": "Video-KTR enhances video reasoning in multimodal models by selectively reinforcing key tokens through a novel attribution framework, achieving state-of-the-art performance across multiple benchmarks.",
      "debug_abstract": "Reinforcement learning (RL) has shown strong potential for enhancing reasoning in multimodal large language models, yet existing video reasoning methods often rely on coarse sequence-level rewards or single-factor token selection, neglecting fine-grained links among visual inputs, temporal dynamics, and linguistic outputs, limiting both accuracy and interpretability. We propose Video-KTR, a modality-aware policy shaping framework that performs selective, token-level RL by combining three attribution signals: (1) visual-aware tokens identified via counterfactual masking to reveal perceptual dependence; (2) temporal-aware tokens detected through frame shuffling to expose temporal sensitivity; and (3) high-entropy tokens signaling predictive uncertainty. By reinforcing only these key tokens, Video-KTR focuses learning on semantically informative, modality-sensitive content while filtering out low-value tokens. Across five challenging benchmarks, Video-KTR achieves state-of-the-art or highly competitive results, achieving 42.7\\% on Video-Holmes (surpassing GPT-4o) with consistent gains on both reasoning and general video understanding tasks. Ablation studies verify the complementary roles of the attribution signals and the robustness of targeted token-level updates. Overall, Video-KTR improves accuracy and interpretability, offering a simple, drop-in extension to RL for complex video reasoning. Our code and models are available at this https URL."
    },
    {
      "title": "RoamScene3D: Immersive Text-to-3D Scene Generation via Adaptive Object-aware Roaming",
      "url": "https://arxiv.org/abs/2601.19433",
      "date": "2026-01-27",
      "authors_text": "Jisheng Chu, Wenrui Li, Rui Zhao, Wangmeng Zuo, Shifeng Chen",
      "is_highlight": false,
      "score": 67.0,
      "summary": "RoamScene3D introduces a novel framework for immersive text-to-3D scene generation that enhances spatial awareness and object relations, outperforming existing methods in realism and consistency.",
      "debug_abstract": "Generating immersive 3D scenes from texts is a core task in computer vision, crucial for applications in virtual reality and game development. Despite the promise of leveraging 2D diffusion priors, existing methods suffer from spatial blindness and rely on predefined trajectories that fail to exploit the inner relationships among salient objects. Consequently, these approaches are unable to comprehend the semantic layout, preventing them from exploring the scene adaptively to infer occluded content. Moreover, current inpainting models operate in 2D image space, struggling to plausibly fill holes caused by camera motion. To address these limitations, we propose RoamScene3D, a novel framework that bridges the gap between semantic guidance and spatial generation. Our method reasons about the semantic relations among objects and produces consistent and photorealistic scenes. Specifically, we employ a vision-language model (VLM) to construct a scene graph that encodes object relations, guiding the camera to perceive salient object boundaries and plan an adaptive roaming trajectory. Furthermore, to mitigate the limitations of static 2D priors, we introduce a Motion-Injected Inpainting model that is fine-tuned on a synthetic panoramic dataset integrating authentic camera trajectories, making it adaptive to camera motion. Extensive experiments demonstrate that with semantic reasoning and geometric constraints, our method significantly outperforms state-of-the-art approaches in producing consistent and photorealistic scenes. Our code is available at this https URL."
    },
    {
      "title": "Physical Prompt Injection Attacks on Large Vision-Language Models",
      "url": "https://arxiv.org/abs/2601.17383",
      "date": "2026-01-24",
      "authors_text": "Chen Ling, Kai Hu, Hangcheng Liu, Xingshuo Han, Tianwei Zhang",
      "is_highlight": false,
      "score": 55.0,
      "summary": "The paper introduces Physical Prompt Injection Attacks (PPIA), a novel, highly effective method for manipulating Large Vision-Language Models using visually embedded prompts in physical environments.",
      "debug_abstract": "Large Vision-Language Models (LVLMs) are increasingly deployed in real-world intelligent systems for perception and reasoning in open physical environments. While LVLMs are known to be vulnerable to prompt injection attacks, existing methods either require access to input channels or depend on knowledge of user queries, assumptions that rarely hold in practical deployments. We propose the first Physical Prompt Injection Attack (PPIA), a black-box, query-agnostic attack that embeds malicious typographic instructions into physical objects perceivable by the LVLM. PPIA requires no access to the model, its inputs, or internal pipeline, and operates solely through visual observation. It combines offline selection of highly recognizable and semantically effective visual prompts with strategic environment-aware placement guided by spatiotemporal attention, ensuring that the injected prompts are both perceivable and influential on model behavior. We evaluate PPIA across 10 state-of-the-art LVLMs in both simulated and real-world settings on tasks including visual question answering, planning, and navigation, PPIA achieves attack success rates up to 98%, with strong robustness under varying physical conditions such as distance, viewpoint, and illumination. Our code is publicly available at this https URL."
    },
    {
      "title": "Delay-Compensated Stiffness Estimation for Robot-Mediated Dyadic Interaction",
      "url": "https://arxiv.org/abs/2601.17812",
      "date": "2026-01-25",
      "authors_text": "Mingtian Du, Suhas Raghavendra Kulkarni, Bernardo Noronha, Domenico Campolo",
      "is_highlight": false,
      "score": 80.0,
      "summary": "The paper presents a delay-compensated stiffness estimation framework for robot-mediated dyadic interactions, improving accuracy in remote physical therapy despite network-induced haptic delays.",
      "debug_abstract": "Robot-mediated human-human (dyadic) interactions enable therapists to provide physical therapy remotely, yet an accurate perception of patient stiffness remains challenging due to network-induced haptic delays. Conventional stiffness estimation methods, which neglect delay, suffer from temporal misalignment between force and position signals, leading to significant estimation errors as delays increase. To address this, we propose a robust, delay-compensated stiffness estimation framework by deriving an algebraic estimator based on quasi-static equilibrium that explicitly accounts for temporally aligning the expert&#39;s input with the novice&#39;s response. A Normalised Weighted Least Squares (NWLS) implementation is then introduced to robustly filter dynamic bias resulting from the algebraic derivation. Experiments using commercial rehabilitation robots (H-MAN) as the platform demonstrate that the proposed method significantly outperforms the standard estimator, maintaining consistent tracking accuracy under multiple introduced delays. These findings offer a promising solution for achieving high-fidelity haptic perception in remote dyadic interaction, potentially facilitating reliable stiffness assessment in therapeutic settings across networks."
    },
    {
      "title": "ORION: Option-Regularized Deep Reinforcement Learning for Cooperative Multi-Agent Online Navigation",
      "url": "https://arxiv.org/abs/2601.01155",
      "date": "2026-01-26",
      "authors_text": "Shizhe Zhang, Jingsong Liang, Zhitao Zhou, Shuhan Ye, Yizhuo Wang",
      "is_highlight": false,
      "score": 88.0,
      "summary": "ORION is a deep reinforcement learning framework that enables decentralized, cooperative navigation for multi-agent systems in partially known environments by integrating prior maps and online observations.",
      "debug_abstract": "Existing methods for multi-agent navigation typically assume fully known environments, offering limited support for partially known scenarios such as warehouses or factory floors. There, agents may need to plan trajectories that balance their own path optimality with their ability to collect and share information about the environment that can help their teammates reach their own goals. To these ends, we propose ORION, a novel deep reinforcement learning framework for cooperative multi-agent online navigation in partially known environments. Starting from an imperfect prior map, ORION trains agents to make decentralized decisions, coordinate to reach their individual targets, and actively reduce map uncertainty by sharing online observations in a closed perception-action loop. We first design a shared graph encoder that fuses prior map with online perception into a unified representation, providing robust state embeddings under dynamic map discrepancies. At the core of ORION is an option-critic framework that learns to reason about a set of high-level cooperative modes that translate into sequences of low-level actions, allowing agents to switch between individual navigation and team-level exploration adaptively. We further introduce a dual-stage cooperation strategy that enables agents to assist teammates under map uncertainty, thereby reducing the overall makespan. Across extensive maze-like maps and large-scale warehouse environments, our simulation results show that ORION achieves high-quality, real-time decentralized cooperation over varying team sizes, outperforming state-of-the-art classical and learning-based baselines. Finally, we validate ORION on physical robot teams, demonstrating its robustness and practicality for real-world cooperative navigation."
    },
    {
      "title": "Resisting Manipulative Bots in Meme Coin Copy Trading: A Multi-Agent Approach with Chain-of-Thought Reasoning",
      "url": "https://arxiv.org/abs/2601.08641",
      "date": "2026-01-26",
      "authors_text": "Yichen Luo, Yebo Feng, Jiahua Xu, Yang Liu",
      "is_highlight": false,
      "score": 67.0,
      "summary": "This paper presents a multi-agent system utilizing explainable LLMs to defend against manipulative bots in meme coin copy trading, enhancing prediction accuracy and economic performance.",
      "debug_abstract": "Copy trading has become the dominant entry strategy in meme coin markets. However, due to the market&#39;s extreme illiquid and volatile nature, the strategy exposes an exploitable attack surface: adversaries deploy manipulative bots to front-run trades, conceal positions, and fabricate sentiment, systematically extracting value from naïve copiers at scale. Despite its prevalence, bot-driven manipulation remains largely unexplored, and no robust defensive framework exists. We propose a manipulation-resistant copy-trading system based on a multi-agent architecture powered by a multi-modal, explainable large language model (LLM). Our system decomposes copy trading into three specialized agents for coin evaluation, wallet selection, and timing assessment. Evaluated on historical data from over 6,000 meme coins, our approach outperforms zero-shot and most statistic-driven baselines in prediction accuracy as well as all baselines in economic performance, achieving an average return of 14% for identified smart-money trades and an estimated copier return of 3% per trade under realistic market frictions. Overall, our results demonstrate the effectiveness of agent-based defenses and predictability of trader profitability in adversarial meme coin markets, providing a practical foundation for robust copy trading."
    },
    {
      "title": "OnlineSI: Taming Large Language Model for Online 3D Understanding and Grounding",
      "url": "https://arxiv.org/abs/2601.16538",
      "date": "2026-01-23",
      "authors_text": "Zixian Liu, Zhaoxi Chen, Liang Pan, Ziwei Liu",
      "is_highlight": false,
      "score": 88.0,
      "summary": "OnlineSI is a framework that enhances Multimodal Large Language Models' spatial understanding in dynamic environments by integrating continuous learning from video streams and 3D point cloud data.",
      "debug_abstract": "In recent years, researchers have increasingly been interested in how to enable Multimodal Large Language Models (MLLM) to possess spatial understanding and reasoning capabilities. However, most existing methods overlook the importance of the ability to continuously work in an ever-changing world, and lack the possibility of deployment on embodied systems in real-world environments. In this work, we introduce OnlineSI, a framework that can continuously improve its spatial understanding of its surroundings given a video stream. Our core idea is to maintain a finite spatial memory to retain past observations, ensuring the computation required for each inference does not increase as the input accumulates. We further integrate 3D point cloud information with semantic information, helping MLLM to better locate and identify objects in the scene. To evaluate our method, we introduce the Fuzzy $F_1$-Score to mitigate ambiguity, and test our method on two representative datasets. Experiments demonstrate the effectiveness of our method, paving the way towards real-world embodied systems."
    },
    {
      "title": "Accurate Calibration and Robust LiDAR-Inertial Odometry for Spinning Actuated LiDAR Systems",
      "url": "https://arxiv.org/abs/2601.15946",
      "date": "2026-01-22",
      "authors_text": "Zijie Chen, Xiaowei Liu, Yong Xu, Shenghai Yuan, Jianping Li",
      "is_highlight": false,
      "score": 74.5,
      "summary": "This paper introduces LM-Calibr for targetless LiDAR-motor calibration and EVA-LIO for adaptive LiDAR-inertial odometry, enhancing localization in spinning actuated LiDAR systems.",
      "debug_abstract": "Accurate calibration and robust localization are fundamental for downstream tasks in spinning actuated LiDAR applications. Existing methods, however, require parameterizing extrinsic parameters based on different mounting configurations, limiting their generalizability. Additionally, spinning actuated LiDAR inevitably scans featureless regions, which complicates the balance between scanning coverage and localization robustness. To address these challenges, this letter presents a targetless LiDAR-motor calibration (LM-Calibr) on the basis of the Denavit-Hartenberg convention and an environmental adaptive LiDAR-inertial odometry (EVA-LIO). LM-Calibr supports calibration of LiDAR-motor systems with various mounting configurations. Extensive experiments demonstrate its accuracy and convergence across different scenarios, mounting angles, and initial values. Additionally, EVA-LIO adaptively selects downsample rates and map resolutions according to spatial scale. This adaptivity enables the actuator to operate at maximum speed, thereby enhancing scanning completeness while ensuring robust localization, even when LiDAR briefly scans featureless areas. The source code and hardware design are available on GitHub: \\textcolor{blue}{\\href{this https URL}{this http URL\\_calibr}}. The video is available at \\textcolor{blue}{\\href{this https URL}{this http URL}}"
    }
  ],
  "MMLab@NTU": [
    {
      "title": "Bongards at the Boundary of Perception and Reasoning: Programs or Language?",
      "url": "https://arxiv.org/abs/2602.03038",
      "date": "2026-02-03",
      "authors_text": "Cassidy Langenfeld, Claas Beger, Gloria Geng, Wasu Top Piriyakulkij, Keya Hu",
      "is_highlight": false,
      "score": 60.0,
      "summary": "This paper introduces a neurosymbolic method utilizing LLMs and Bayesian optimization to tackle Bongard problems, enhancing visual reasoning beyond conventional VLM capabilities.",
      "teaser_image": "https://arxiv.org/html/2602.03038/x3.png",
      "debug_abstract": "Vision-Language Models (VLMs) have made great strides in everyday visual tasks, such as captioning a natural image, or answering commonsense questions about such images. But humans possess the puzzling ability to deploy their visual reasoning abilities in radically new situations, a skill rigorously tested by the classic set of visual reasoning challenges known as the Bongard problems. We present a neurosymbolic approach to solving these problems: given a hypothesized solution rule for a Bongard problem, we leverage LLMs to generate parameterized programmatic representations for the rule and perform parameter fitting using Bayesian optimization. We evaluate our method on classifying Bongard problem images given the ground truth rule, as well as on solving the problems from scratch."
    },
    {
      "title": "Estimation of Ground Reaction Forces from Kinematic Data during Locomotion",
      "url": "https://arxiv.org/abs/2602.03177",
      "date": "2026-02-03",
      "authors_text": "Gautami Golani, Dong Anh Khoa To, Ananda Sidarta, Arun-Kumar Kaliya-Perumal, Oliver Roberts",
      "is_highlight": false,
      "score": 44.0,
      "summary": "This study introduces a force-plate-free method for estimating ground reaction forces from kinematic data, enhancing clinical gait analysis without the need for specialized equipment.",
      "teaser_image": "https://arxiv.org/html/2602.03177/x2.png",
      "debug_abstract": "Ground reaction forces (GRFs) provide fundamental insight into human gait mechanics and are widely used to assess joint loading, limb symmetry, balance control, and motor function. Despite their clinical relevance, the use of GRF remains underutilised in clinical workflows due to the practical limitations of force plate systems. In this work, we present a force-plate-free approach for estimating GRFs using only marker-based motion capture data. This kinematics only method to estimate and decompose GRF makes it well suited for widespread clinical depolyment. By using kinematics from sixteen body segments, we estimate the centre of mass (CoM) and compute GRFs, which are subsequently decomposed into individual components through a minimization-based approach. Through this framework, we can identify gait stance phases and provide access to clinically meaningful kinetic measures without a dedicated force plate system. Experimental results demonstrate the viability of CoM and GRF estimation based solely on kinematic data, supporting force-plate-free gait analysis."
    },
    {
      "title": "Self-Guard: Defending Large Reasoning Models via enhanced self-reflection",
      "url": "https://arxiv.org/abs/2602.00707",
      "date": "2026-01-31",
      "authors_text": "Jingnan Zheng, Jingjun Xu, Yanzhen Luo, Chenhang Cui, Gelei Deng",
      "is_highlight": false,
      "score": 50.0,
      "summary": "Self-Guard is a lightweight framework enhancing safety compliance in Large Reasoning Models by promoting self-reflection and mitigating risks without sacrificing model utility.",
      "teaser_image": "https://arxiv.org/html/2602.00707/materials/workflow.png",
      "debug_abstract": "The emergence of Large Reasoning Models (LRMs) introduces a new paradigm of explicit reasoning, enabling remarkable advances yet posing unique risks such as reasoning manipulation and information leakage. To mitigate these risks, current alignment strategies predominantly rely on heavy post-training paradigms or external interventions. However, these approaches are often computationally intensive and fail to address the inherent awareness-compliance gap, a critical misalignment where models recognize potential risks yet prioritize following user instructions due to their sycophantic tendencies. To address these limitations, we propose Self-Guard, a lightweight safety defense framework that reinforces safety compliance at the representational level. Self-Guard operates through two principal stages: (1) safety-oriented prompting, which activates the model&#39;s latent safety awareness to evoke spontaneous reflection, and (2) safety activation steering, which extracts the resulting directional shift in the hidden state space and amplifies it to ensure that safety compliance prevails over sycophancy during inference. Experiments demonstrate that Self-Guard effectively bridges the awareness-compliance gap, achieving robust safety performance without compromising model utility. Furthermore, Self-Guard exhibits strong generalization across diverse unseen risks and varying model scales, offering a cost-efficient solution for LRM safety alignment."
    },
    {
      "title": "Learning Adaptive Cross-Embodiment Visuomotor Policy with Contrastive Prompt Orchestration",
      "url": "https://arxiv.org/abs/2602.01040",
      "date": "2026-02-01",
      "authors_text": "Yuhang Zhang, Chao Yan, Jiaxi Yu, Jiaping Xiao, Mir Feroskhan",
      "is_highlight": false,
      "score": 92.0,
      "summary": "The paper presents ContrAstive Prompt Orchestration (CAPO), a method for improving adaptive visuomotor policies in diverse environments through hybrid contrastive learning and dynamic prompt orchestration.",
      "teaser_image": "https://arxiv.org/html/2602.01040/x4.png",
      "debug_abstract": "Learning adaptive visuomotor policies for embodied agents remains a formidable challenge, particularly when facing cross-embodiment variations such as diverse sensor configurations and dynamic properties. Conventional learning approaches often struggle to separate task-relevant features from domain-specific variations (e.g., lighting, field-of-view, and rotation), leading to poor sample efficiency and catastrophic failure in unseen environments. To bridge this gap, we propose ContrAstive Prompt Orchestration (CAPO), a novel approach for learning visuomotor policies that integrates contrastive prompt learning and adaptive prompt orchestration. For prompt learning, we devise a hybrid contrastive learning strategy that integrates visual, temporal action, and text objectives to establish a pool of learnable prompts, where each prompt induces a visual representation encapsulating fine-grained domain factors. Based on these learned prompts, we introduce an adaptive prompt orchestration mechanism that dynamically aggregates these prompts conditioned on current observations. This enables the agent to adaptively construct optimal state representations by identifying dominant domain factors instantaneously. Consequently, the policy optimization is effectively shielded from irrelevant interference, preventing the common issue of overfitting to source domains. Extensive experiments demonstrate that CAPO significantly outperforms state-of-the-art baselines in sample efficiency and asymptotic performance. Crucially, it exhibits superior zero-shot adaptation across unseen target domains characterized by drastic environmental (e.g., illumination) and physical shifts (e.g., field-of-view and rotation), validating its effectiveness as a viable solution for cross-embodiment visuomotor policy adaptation."
    },
    {
      "title": "Estimating Force Interactions of Deformable Linear Objects from their Shapes",
      "url": "https://arxiv.org/abs/2602.01085",
      "date": "2026-02-01",
      "authors_text": "Qi Jing Chen, Shilin Shan, Timothy Bretl, Quang-Cuong Pham",
      "is_highlight": false,
      "score": 83.0,
      "summary": "This paper presents a novel method for estimating external forces on deformable linear objects using shape data, enhancing robot interaction safety and efficiency without requiring force sensors.",
      "teaser_image": "https://arxiv.org/html/2602.01085/x2.png",
      "debug_abstract": "This work introduces an analytical approach for detecting and estimating external forces acting on deformable linear objects (DLOs) using only their observed shapes. In many robot-wire interaction tasks, contact occurs not at the end-effector but at other points along the robot&#39;s body. Such scenarios arise when robots manipulate wires indirectly (e.g., by nudging) or when wires act as passive obstacles in the environment. Accurately identifying these interactions is crucial for safe and efficient trajectory planning, helping to prevent wire damage, avoid restricted robot motions, and mitigate potential hazards. Existing approaches often rely on expensive external force-torque sensor or that contacts occur at the end-effector for accurate force estimation. Using wire shape information acquired from a depth camera and under the assumption that the wire is in or near its static equilibrium, our method estimates both the location and magnitude of external forces without additional prior knowledge. This is achieved by exploiting derived consistency conditions and solving a system of linear equations based on force-torque balance along the wire. The approach was validated through simulation, where it achieved high accuracy, and through real-world experiments, where accurate estimation was demonstrated in selected interaction scenarios."
    },
    {
      "title": "SGHA-Attack: Semantic-Guided Hierarchical Alignment for Transferable Targeted Attacks on Vision-Language Models",
      "url": "https://arxiv.org/abs/2602.01574",
      "date": "2026-02-02",
      "authors_text": "Haobo Wang, Weiqi Luo, Xiaojun Jia, Xiaochun Cao",
      "is_highlight": false,
      "score": 44.0,
      "summary": "SGHA-Attack enhances targeted adversarial attacks on vision-language models by leveraging multiple semantic references and aligning intermediate features for improved transferability and robustness.",
      "teaser_image": "https://arxiv.org/html/2602.01574/x1.png",
      "debug_abstract": "Large vision-language models (VLMs) are vulnerable to transfer-based adversarial perturbations, enabling attackers to optimize on surrogate models and manipulate black-box VLM outputs. Prior targeted transfer attacks often overfit surrogate-specific embedding space by relying on a single reference and emphasizing final-layer alignment, which underutilizes intermediate semantics and degrades transfer across heterogeneous VLMs. To address this, we propose SGHA-Attack, a Semantic-Guided Hierarchical Alignment framework that adopts multiple target references and enforces intermediate-layer consistency. Concretely, we generate a visually grounded reference pool by sampling a frozen text-to-image model conditioned on the target prompt, and then carefully select the Top-K most semantically relevant anchors under the surrogate to form a weighted mixture for stable optimization guidance. Building on these anchors, SGHA-Attack injects target semantics throughout the feature hierarchy by aligning intermediate visual representations at both global and spatial granularities across multiple depths, and by synchronizing intermediate visual and textual features in a shared latent subspace to provide early cross-modal supervision before the final projection. Extensive experiments on open-source and commercial black-box VLMs show that SGHA-Attack achieves stronger targeted transferability than prior methods and remains robust under preprocessing and purification defenses."
    },
    {
      "title": "ClueTracer: Question-to-Vision Clue Tracing for Training-Free Hallucination Suppression in Multimodal Reasoning",
      "url": "https://arxiv.org/abs/2602.02004",
      "date": "2026-02-02",
      "authors_text": "Gongli Xi, Kun Wang, Zeming Gao, Huahui Yi, Haolang Lu",
      "is_highlight": false,
      "score": 24.0,
      "summary": "ClueTracer is a training-free plugin that suppresses hallucinations in multimodal reasoning by tracing visual clue propagation, enhancing model performance across various architectures.",
      "teaser_image": "https://arxiv.org/html/2602.02004/x1.png",
      "debug_abstract": "Large multimodal reasoning models solve challenging visual problems via explicit long-chain inference: they gather visual clues from images and decode clues into textual tokens. Yet this capability also increases hallucinations, where the model generates content that is not supported by the input image or the question. To understand this failure mode, we identify \\emph{reasoning drift}: during clue gathering, the model over-focuses on question-irrelevant entities, diluting focus on task-relevant cues and gradually decoupling the reasoning trace from visual grounding. As a consequence, many inference-time localization or intervention methods developed for non-reasoning models fail to pinpoint the true clues in reasoning settings. Motivated by these insights, we introduce ClueRecall, a metric for assessing visual clue retrieval, and present ClueTracer, a training-free, parameter-free, and architecture-agnostic plugin for hallucination suppression. ClueTracer starts from the question and traces how key clues propagate along the model&#39;s reasoning pathway (question $\\rightarrow$ outputs $\\rightarrow$ visual tokens), thereby localizing task-relevant patches while suppressing spurious attention to irrelevant regions. Remarkably, \\textbf{without any additional training}, ClueTracer improves all \\textbf{reasoning} architectures (including \\texttt{R1-OneVision}, \\texttt{Ocean-R1}, \\texttt{MM-Eureka}, \\emph{etc}.) by $\\mathbf{1.21\\times}$ on reasoning benchmarks. When transferred to \\textbf{non-reasoning} settings, it yields a $\\mathbf{1.14\\times}$ gain."
    },
    {
      "title": "Superman: Unifying Skeleton and Vision for Human Motion Perception and Generation",
      "url": "https://arxiv.org/abs/2602.02401",
      "date": "2026-02-02",
      "authors_text": "Xinshun Wang, Peiming Li, Ziyi Wang, Zhongbin Fang, Zhichao Deng",
      "is_highlight": false,
      "score": 15.0,
      "summary": "Superman unifies visual perception and skeleton-based motion generation through a Vision-Guided Motion Tokenizer, enabling robust cross-modal learning and superior performance in human motion tasks.",
      "teaser_image": "https://arxiv.org/html/2602.02401/x2.png",
      "debug_abstract": "Human motion analysis tasks, such as temporal 3D pose estimation, motion prediction, and motion in-betweening, play an essential role in computer vision. However, current paradigms suffer from severe fragmentation. First, the field is split between ``perception&#39;&#39; models that understand motion from video but only output text, and ``generation&#39;&#39; models that cannot perceive from raw visual input. Second, generative MLLMs are often limited to single-frame, static poses using dense, parametric SMPL models, failing to handle temporal motion. Third, existing motion vocabularies are built from skeleton data alone, severing the link to the visual domain. To address these challenges, we introduce Superman, a unified framework that bridges visual perception with temporal, skeleton-based motion generation. Our solution is twofold. First, to overcome the modality disconnect, we propose a Vision-Guided Motion Tokenizer. Leveraging the natural geometric alignment between 3D skeletons and visual data, this module pioneers robust joint learning from both modalities, creating a unified, cross-modal motion vocabulary. Second, grounded in this motion language, a single, unified MLLM architecture is trained to handle all tasks. This module flexibly processes diverse, temporal inputs, unifying 3D skeleton pose estimation from video (perception) with skeleton-based motion prediction and in-betweening (generation). Extensive experiments on standard benchmarks, including Human3.6M, demonstrate that our unified method achieves state-of-the-art or competitive performance across all motion tasks. This showcases a more efficient and scalable path for generative motion analysis using skeletons."
    },
    {
      "title": "Failure-Aware Bimanual Teleoperation via Conservative Value Guided Assistance",
      "url": "https://arxiv.org/abs/2602.01092",
      "date": "2026-02-01",
      "authors_text": "Peng Zhou, Zhongxuan Li, Jinsong Wu, Jiaming Qi, Jun Hu",
      "is_highlight": false,
      "score": 9.0,
      "summary": "This paper presents a failure-aware bimanual teleoperation framework that uses conservative value learning to enhance task success and reduce operator workload through compliant haptic assistance.",
      "teaser_image": "https://arxiv.org/html/2602.01092/x1.png",
      "debug_abstract": "Teleoperation of high-precision manipulation is con-strained by tight success tolerances and complex contact dy-namics, which make impending failures difficult for human operators to anticipate under partial observability. This paper proposes a value-guided, failure-aware framework for bimanual teleoperation that provides compliant haptic assistance while pre-serving continuous human authority. The framework is trained entirely from heterogeneous offline teleoperation data containing both successful and failed executions. Task feasibility is mod-eled as a conservative success score learned via Conservative Value Learning, yielding a risk-sensitive estimate that remains reliable under distribution shift. During online operation, the learned success score regulates the level of assistance, while a learned actor provides a corrective motion direction. Both are integrated through a joint-space impedance interface on the master side, yielding continuous guidance that steers the operator away from failure-prone actions without overriding intent. Experimental results on contact-rich manipulation tasks demonstrate improved task success rates and reduced operator workload compared to conventional teleoperation and shared-autonomy baselines, indicating that conservative value learning provides an effective mechanism for embedding failure awareness into bilateral teleoperation. Experimental videos are available at this https URL"
    },
    {
      "title": "UV-M3TL: A Unified and Versatile Multimodal Multi-Task Learning Framework for Assistive Driving Perception",
      "url": "https://arxiv.org/abs/2602.01594",
      "date": "2026-02-02",
      "authors_text": "Wenzhuo Liu, Qiannan Guo, Zhen Wang, Wenshuo Wang, Lei Yang",
      "is_highlight": false,
      "score": 8.0,
      "summary": "The UV-M3TL framework enhances ADAS by effectively recognizing driver behavior and context through dual-branch multimodal embedding and adaptive loss, achieving state-of-the-art performance across multiple tasks.",
      "teaser_image": "https://arxiv.org/html/2602.01594/x2.png",
      "debug_abstract": "Advanced Driver Assistance Systems (ADAS) need to understand human driver behavior while perceiving their navigation context, but jointly learning these heterogeneous tasks would cause inter-task negative transfer and impair system performance. Here, we propose a Unified and Versatile Multimodal Multi-Task Learning (UV-M3TL) framework to simultaneously recognize driver behavior, driver emotion, vehicle behavior, and traffic context, while mitigating inter-task negative transfer. Our framework incorporates two core components: dual-branch spatial channel multimodal embedding (DB-SCME) and adaptive feature-decoupled multi-task loss (AFD-Loss). DB-SCME enhances cross-task knowledge transfer while mitigating task conflicts by employing a dual-branch structure to explicitly model salient task-shared and task-specific features. AFD-Loss improves the stability of joint optimization while guiding the model to learn diverse multi-task representations by introducing an adaptive weighting mechanism based on learning dynamics and feature decoupling constraints. We evaluate our method on the AIDE dataset, and the experimental results demonstrate that UV-M3TL achieves state-of-the-art performance across all four tasks. To further prove the versatility, we evaluate UV-M3TL on additional public multi-task perception benchmarks (BDD100K, CityScapes, NYUD-v2, and PASCAL-Context), where it consistently delivers strong performance across diverse task combinations, attaining state-of-the-art results on most tasks."
    },
    {
      "title": "FlowSteer: Interactive Agentic Workflow Orchestration via End-to-End Reinforcement Learning",
      "url": "https://arxiv.org/abs/2602.01664",
      "date": "2026-02-02",
      "authors_text": "Mingda Zhang, Haoran Luo, Tiesunlong Shen, Qika Lin, Xiaoying Tang",
      "is_highlight": false,
      "score": 7.0,
      "summary": "FlowSteer is an end-to-end reinforcement learning framework that automates workflow orchestration through interactive learning, addressing manual costs and improving performance across diverse tasks.",
      "teaser_image": "https://arxiv.org/html/2602.01664/x1.png",
      "debug_abstract": "In recent years, a variety of powerful agentic workflows have been applied to solve a wide range of human problems. However, existing workflow orchestration still faces key challenges, including high manual cost, reliance on specific operators/large language models (LLMs), and sparse reward signals. To address these challenges, we propose FlowSteer, an end-to-end reinforcement learning framework that takes a lightweight policy model as the agent and an executable canvas environment, automating workflow orchestration through multi-turn interaction. In this process, the policy model analyzes execution states and selects editing actions, while the canvas executes operators and returns feedback for iterative refinement. Moreover, FlowSteer provides a plug-and-play framework that supports diverse operator libraries and interchangeable LLM backends. To effectively train this interaction paradigm, we propose Canvas Workflow Relative Policy Optimization (CWRPO), which introduces diversity-constrained rewards with conditional release to stabilize learning and suppress shortcut behaviors. Experimental results on twelve datasets show that FlowSteer significantly outperforms baselines across various tasks."
    },
    {
      "title": "FD-VLA: Force-Distilled Vision-Language-Action Model for Contact-Rich Manipulation",
      "url": "https://arxiv.org/abs/2602.02142",
      "date": "2026-02-02",
      "authors_text": "Ruiteng Zhao, Wenshuo Wang, Yicheng Ma, Xiaocong Li, Francis E. H. Tay",
      "is_highlight": false,
      "score": 0,
      "summary": "The FD-VLA framework enhances contact-rich manipulation by integrating force awareness through a novel distillation method, improving performance without physical force sensors.",
      "teaser_image": "https://arxiv.org/html/2602.02142/x1.png",
      "debug_abstract": "Force sensing is a crucial modality for Vision-Language-Action (VLA) frameworks, as it enables fine-grained perception and dexterous manipulation in contact-rich tasks. We present Force-Distilled VLA (FD-VLA), a novel framework that integrates force awareness into contact-rich manipulation without relying on physical force sensors. The core of our approach is a Force Distillation Module (FDM), which distills force by mapping a learnable query token, conditioned on visual observations and robot states, into a predicted force token aligned with the latent representation of actual force signals. During inference, this distilled force token is injected into the pretrained VLM, enabling force-aware reasoning while preserving the integrity of its vision-language semantics. This design provides two key benefits: first, it allows practical deployment across a wide range of robots that lack expensive or fragile force-torque sensors, thereby reducing hardware cost and complexity; second, the FDM introduces an additional force-vision-state fusion prior to the VLM, which improves cross-modal alignment and enhances perception-action robustness in contact-rich scenarios. Surprisingly, our physical experiments show that the distilled force token outperforms direct sensor force measurements as well as other baselines, which highlights the effectiveness of this force-distilled VLA approach."
    },
    {
      "title": "DynamicVLA: A Vision-Language-Action Model for Dynamic Object Manipulation",
      "url": "https://arxiv.org/abs/2601.22153",
      "date": "2026-01-29",
      "authors_text": "Haozhe Xie, Beichen Wen, Jiarui Zheng, Zhaoxi Chen, Fangzhou Hong",
      "is_highlight": false,
      "score": 93.0,
      "summary": "DynamicVLA is a novel Vision-Language-Action framework that enhances dynamic object manipulation through efficient encoding, continuous inference, and a new benchmark for data collection.",
      "teaser_image": "https://arxiv.org/html/2601.22153/x1.png",
      "debug_abstract": "Manipulating dynamic objects remains an open challenge for Vision-Language-Action (VLA) models, which, despite strong generalization in static manipulation, struggle in dynamic scenarios requiring rapid perception, temporal anticipation, and continuous control. We present DynamicVLA, a framework for dynamic object manipulation that integrates temporal reasoning and closed-loop adaptation through three key designs: 1) a compact 0.4B VLA using a convolutional vision encoder for spatially efficient, structurally faithful encoding, enabling fast multimodal inference; 2) Continuous Inference, enabling overlapping reasoning and execution for lower latency and timely adaptation to object motion; and 3) Latent-aware Action Streaming, which bridges the perception-execution gap by enforcing temporally aligned action execution. To fill the missing foundation of dynamic manipulation data, we introduce the Dynamic Object Manipulation (DOM) benchmark, built from scratch with an auto data collection pipeline that efficiently gathers 200K synthetic episodes across 2.8K scenes and 206 objects, and enables fast collection of 2K real-world episodes without teleoperation. Extensive evaluations demonstrate remarkable improvements in response speed, perception, and generalization, positioning DynamicVLA as a unified framework for general dynamic object manipulation across embodiments."
    },
    {
      "title": "E2HiL: Entropy-Guided Sample Selection for Efficient Real-World Human-in-the-Loop Reinforcement Learning",
      "url": "https://arxiv.org/abs/2601.19969",
      "date": "2026-01-27",
      "authors_text": "Haoyuan Deng, Yuanjiang Xue, Haoyang Du, Boyang Zhou, Zhenyu Wu",
      "is_highlight": false,
      "score": 80.0,
      "summary": "E2HiL enhances human-in-the-loop reinforcement learning by efficiently selecting informative samples, achieving 42.1% higher success rates with 10.1% fewer human interventions.",
      "teaser_image": "https://arxiv.org/html/2601.19969/x2.png",
      "debug_abstract": "Human-in-the-loop guidance has emerged as an effective approach for enabling faster convergence in online reinforcement learning (RL) of complex real-world manipulation tasks. However, existing human-in-the-loop RL (HiL-RL) frameworks often suffer from low sample efficiency, requiring substantial human interventions to achieve convergence and thereby leading to high labor costs. To address this, we propose a sample-efficient real-world human-in-the-loop RL framework named \\method, which requires fewer human intervention by actively selecting informative samples. Specifically, stable reduction of policy entropy enables improved trade-off between exploration and exploitation with higher sample efficiency. We first build influence functions of different samples on the policy entropy, which is efficiently estimated by the covariance of action probabilities and soft advantages of policies. Then we select samples with moderate values of influence functions, where shortcut samples that induce sharp entropy drops and noisy samples with negligible effect are pruned. Extensive experiments on four real-world manipulation tasks demonstrate that \\method achieves a 42.1\\% higher success rate while requiring 10.1\\% fewer human interventions compared to the state-of-the-art HiL-RL method, validating its effectiveness. The project page providing code, videos, and mathematical formulations can be found at this https URL."
    },
    {
      "title": "RF-MatID: Dataset and Benchmark for Radio Frequency Material Identification",
      "url": "https://arxiv.org/abs/2601.20377",
      "date": "2026-01-28",
      "authors_text": "Xinyan Chen, Qinchun Li, Ruiqin Ma, Jiaqi Bai, Li Yi",
      "is_highlight": false,
      "score": 58.0,
      "summary": "RF-MatID introduces a comprehensive open-source RF dataset and benchmark for fine-grained material identification, addressing limitations of existing optical methods and enhancing algorithmic development.",
      "teaser_image": "https://arxiv.org/html/2601.20377/figures/materials.png",
      "debug_abstract": "Accurate material identification plays a crucial role in embodied AI systems, enabling a wide range of applications. However, current vision-based solutions are limited by the inherent constraints of optical sensors, while radio-frequency (RF) approaches, which can reveal intrinsic material properties, have received growing attention. Despite this progress, RF-based material identification remains hindered by the lack of large-scale public datasets and the limited benchmarking of learning-based approaches. In this work, we present RF-MatID, the first open-source, large-scale, wide-band, and geometry-diverse RF dataset for fine-grained material identification. RF-MatID includes 16 fine-grained categories grouped into 5 superclasses, spanning a broad frequency range from 4 to 43.5 GHz, and comprises 142k samples in both frequency- and time-domain representations. The dataset systematically incorporates controlled geometry perturbations, including variations in incidence angle and stand-off distance. We further establish a multi-setting, multi-protocol benchmark by evaluating state-of-the-art deep learning models, assessing both in-distribution performance and out-of-distribution robustness under cross-angle and cross-distance shifts. The 5 frequency-allocation protocols enable systematic frequency- and region-level analysis, thereby facilitating real-world deployment. RF-MatID aims to enable reproducible research, accelerate algorithmic advancement, foster cross-domain robustness, and support the development of real-world application in RF-based material identification."
    },
    {
      "title": "Video-KTR: Reinforcing Video Reasoning via Key Token Attribution",
      "url": "https://arxiv.org/abs/2601.19686",
      "date": "2026-01-27",
      "authors_text": "Ziyue Wang, Sheng Jin, Zhongrong Zuo, Jiawei Wu, Han Qiu",
      "is_highlight": false,
      "score": 66.0,
      "summary": "Video-KTR enhances video reasoning in multimodal models by selectively reinforcing key tokens through a novel attribution framework, achieving state-of-the-art performance across multiple benchmarks.",
      "debug_abstract": "Reinforcement learning (RL) has shown strong potential for enhancing reasoning in multimodal large language models, yet existing video reasoning methods often rely on coarse sequence-level rewards or single-factor token selection, neglecting fine-grained links among visual inputs, temporal dynamics, and linguistic outputs, limiting both accuracy and interpretability. We propose Video-KTR, a modality-aware policy shaping framework that performs selective, token-level RL by combining three attribution signals: (1) visual-aware tokens identified via counterfactual masking to reveal perceptual dependence; (2) temporal-aware tokens detected through frame shuffling to expose temporal sensitivity; and (3) high-entropy tokens signaling predictive uncertainty. By reinforcing only these key tokens, Video-KTR focuses learning on semantically informative, modality-sensitive content while filtering out low-value tokens. Across five challenging benchmarks, Video-KTR achieves state-of-the-art or highly competitive results, achieving 42.7\\% on Video-Holmes (surpassing GPT-4o) with consistent gains on both reasoning and general video understanding tasks. Ablation studies verify the complementary roles of the attribution signals and the robustness of targeted token-level updates. Overall, Video-KTR improves accuracy and interpretability, offering a simple, drop-in extension to RL for complex video reasoning. Our code and models are available at this https URL."
    },
    {
      "title": "RoamScene3D: Immersive Text-to-3D Scene Generation via Adaptive Object-aware Roaming",
      "url": "https://arxiv.org/abs/2601.19433",
      "date": "2026-01-27",
      "authors_text": "Jisheng Chu, Wenrui Li, Rui Zhao, Wangmeng Zuo, Shifeng Chen",
      "is_highlight": false,
      "score": 67.0,
      "summary": "RoamScene3D introduces a novel framework for immersive text-to-3D scene generation that enhances spatial awareness and object relations, outperforming existing methods in realism and consistency.",
      "debug_abstract": "Generating immersive 3D scenes from texts is a core task in computer vision, crucial for applications in virtual reality and game development. Despite the promise of leveraging 2D diffusion priors, existing methods suffer from spatial blindness and rely on predefined trajectories that fail to exploit the inner relationships among salient objects. Consequently, these approaches are unable to comprehend the semantic layout, preventing them from exploring the scene adaptively to infer occluded content. Moreover, current inpainting models operate in 2D image space, struggling to plausibly fill holes caused by camera motion. To address these limitations, we propose RoamScene3D, a novel framework that bridges the gap between semantic guidance and spatial generation. Our method reasons about the semantic relations among objects and produces consistent and photorealistic scenes. Specifically, we employ a vision-language model (VLM) to construct a scene graph that encodes object relations, guiding the camera to perceive salient object boundaries and plan an adaptive roaming trajectory. Furthermore, to mitigate the limitations of static 2D priors, we introduce a Motion-Injected Inpainting model that is fine-tuned on a synthetic panoramic dataset integrating authentic camera trajectories, making it adaptive to camera motion. Extensive experiments demonstrate that with semantic reasoning and geometric constraints, our method significantly outperforms state-of-the-art approaches in producing consistent and photorealistic scenes. Our code is available at this https URL."
    },
    {
      "title": "Physical Prompt Injection Attacks on Large Vision-Language Models",
      "url": "https://arxiv.org/abs/2601.17383",
      "date": "2026-01-24",
      "authors_text": "Chen Ling, Kai Hu, Hangcheng Liu, Xingshuo Han, Tianwei Zhang",
      "is_highlight": false,
      "score": 55.0,
      "summary": "The paper introduces Physical Prompt Injection Attacks (PPIA), a novel, highly effective method for manipulating Large Vision-Language Models using visually embedded prompts in physical environments.",
      "debug_abstract": "Large Vision-Language Models (LVLMs) are increasingly deployed in real-world intelligent systems for perception and reasoning in open physical environments. While LVLMs are known to be vulnerable to prompt injection attacks, existing methods either require access to input channels or depend on knowledge of user queries, assumptions that rarely hold in practical deployments. We propose the first Physical Prompt Injection Attack (PPIA), a black-box, query-agnostic attack that embeds malicious typographic instructions into physical objects perceivable by the LVLM. PPIA requires no access to the model, its inputs, or internal pipeline, and operates solely through visual observation. It combines offline selection of highly recognizable and semantically effective visual prompts with strategic environment-aware placement guided by spatiotemporal attention, ensuring that the injected prompts are both perceivable and influential on model behavior. We evaluate PPIA across 10 state-of-the-art LVLMs in both simulated and real-world settings on tasks including visual question answering, planning, and navigation, PPIA achieves attack success rates up to 98%, with strong robustness under varying physical conditions such as distance, viewpoint, and illumination. Our code is publicly available at this https URL."
    },
    {
      "title": "Delay-Compensated Stiffness Estimation for Robot-Mediated Dyadic Interaction",
      "url": "https://arxiv.org/abs/2601.17812",
      "date": "2026-01-25",
      "authors_text": "Mingtian Du, Suhas Raghavendra Kulkarni, Bernardo Noronha, Domenico Campolo",
      "is_highlight": false,
      "score": 80.0,
      "summary": "The paper presents a delay-compensated stiffness estimation framework for robot-mediated dyadic interactions, improving accuracy in remote physical therapy despite network-induced haptic delays.",
      "debug_abstract": "Robot-mediated human-human (dyadic) interactions enable therapists to provide physical therapy remotely, yet an accurate perception of patient stiffness remains challenging due to network-induced haptic delays. Conventional stiffness estimation methods, which neglect delay, suffer from temporal misalignment between force and position signals, leading to significant estimation errors as delays increase. To address this, we propose a robust, delay-compensated stiffness estimation framework by deriving an algebraic estimator based on quasi-static equilibrium that explicitly accounts for temporally aligning the expert&#39;s input with the novice&#39;s response. A Normalised Weighted Least Squares (NWLS) implementation is then introduced to robustly filter dynamic bias resulting from the algebraic derivation. Experiments using commercial rehabilitation robots (H-MAN) as the platform demonstrate that the proposed method significantly outperforms the standard estimator, maintaining consistent tracking accuracy under multiple introduced delays. These findings offer a promising solution for achieving high-fidelity haptic perception in remote dyadic interaction, potentially facilitating reliable stiffness assessment in therapeutic settings across networks."
    },
    {
      "title": "ORION: Option-Regularized Deep Reinforcement Learning for Cooperative Multi-Agent Online Navigation",
      "url": "https://arxiv.org/abs/2601.01155",
      "date": "2026-01-26",
      "authors_text": "Shizhe Zhang, Jingsong Liang, Zhitao Zhou, Shuhan Ye, Yizhuo Wang",
      "is_highlight": false,
      "score": 88.0,
      "summary": "ORION is a deep reinforcement learning framework that enables decentralized, cooperative navigation for multi-agent systems in partially known environments by integrating prior maps and online observations.",
      "debug_abstract": "Existing methods for multi-agent navigation typically assume fully known environments, offering limited support for partially known scenarios such as warehouses or factory floors. There, agents may need to plan trajectories that balance their own path optimality with their ability to collect and share information about the environment that can help their teammates reach their own goals. To these ends, we propose ORION, a novel deep reinforcement learning framework for cooperative multi-agent online navigation in partially known environments. Starting from an imperfect prior map, ORION trains agents to make decentralized decisions, coordinate to reach their individual targets, and actively reduce map uncertainty by sharing online observations in a closed perception-action loop. We first design a shared graph encoder that fuses prior map with online perception into a unified representation, providing robust state embeddings under dynamic map discrepancies. At the core of ORION is an option-critic framework that learns to reason about a set of high-level cooperative modes that translate into sequences of low-level actions, allowing agents to switch between individual navigation and team-level exploration adaptively. We further introduce a dual-stage cooperation strategy that enables agents to assist teammates under map uncertainty, thereby reducing the overall makespan. Across extensive maze-like maps and large-scale warehouse environments, our simulation results show that ORION achieves high-quality, real-time decentralized cooperation over varying team sizes, outperforming state-of-the-art classical and learning-based baselines. Finally, we validate ORION on physical robot teams, demonstrating its robustness and practicality for real-world cooperative navigation."
    },
    {
      "title": "Resisting Manipulative Bots in Meme Coin Copy Trading: A Multi-Agent Approach with Chain-of-Thought Reasoning",
      "url": "https://arxiv.org/abs/2601.08641",
      "date": "2026-01-26",
      "authors_text": "Yichen Luo, Yebo Feng, Jiahua Xu, Yang Liu",
      "is_highlight": false,
      "score": 67.0,
      "summary": "This paper presents a multi-agent system utilizing explainable LLMs to defend against manipulative bots in meme coin copy trading, enhancing prediction accuracy and economic performance.",
      "debug_abstract": "Copy trading has become the dominant entry strategy in meme coin markets. However, due to the market&#39;s extreme illiquid and volatile nature, the strategy exposes an exploitable attack surface: adversaries deploy manipulative bots to front-run trades, conceal positions, and fabricate sentiment, systematically extracting value from naïve copiers at scale. Despite its prevalence, bot-driven manipulation remains largely unexplored, and no robust defensive framework exists. We propose a manipulation-resistant copy-trading system based on a multi-agent architecture powered by a multi-modal, explainable large language model (LLM). Our system decomposes copy trading into three specialized agents for coin evaluation, wallet selection, and timing assessment. Evaluated on historical data from over 6,000 meme coins, our approach outperforms zero-shot and most statistic-driven baselines in prediction accuracy as well as all baselines in economic performance, achieving an average return of 14% for identified smart-money trades and an estimated copier return of 3% per trade under realistic market frictions. Overall, our results demonstrate the effectiveness of agent-based defenses and predictability of trader profitability in adversarial meme coin markets, providing a practical foundation for robust copy trading."
    },
    {
      "title": "OnlineSI: Taming Large Language Model for Online 3D Understanding and Grounding",
      "url": "https://arxiv.org/abs/2601.16538",
      "date": "2026-01-23",
      "authors_text": "Zixian Liu, Zhaoxi Chen, Liang Pan, Ziwei Liu",
      "is_highlight": false,
      "score": 88.0,
      "summary": "OnlineSI is a framework that enhances Multimodal Large Language Models' spatial understanding in dynamic environments by integrating continuous learning from video streams and 3D point cloud data.",
      "debug_abstract": "In recent years, researchers have increasingly been interested in how to enable Multimodal Large Language Models (MLLM) to possess spatial understanding and reasoning capabilities. However, most existing methods overlook the importance of the ability to continuously work in an ever-changing world, and lack the possibility of deployment on embodied systems in real-world environments. In this work, we introduce OnlineSI, a framework that can continuously improve its spatial understanding of its surroundings given a video stream. Our core idea is to maintain a finite spatial memory to retain past observations, ensuring the computation required for each inference does not increase as the input accumulates. We further integrate 3D point cloud information with semantic information, helping MLLM to better locate and identify objects in the scene. To evaluate our method, we introduce the Fuzzy $F_1$-Score to mitigate ambiguity, and test our method on two representative datasets. Experiments demonstrate the effectiveness of our method, paving the way towards real-world embodied systems."
    },
    {
      "title": "Accurate Calibration and Robust LiDAR-Inertial Odometry for Spinning Actuated LiDAR Systems",
      "url": "https://arxiv.org/abs/2601.15946",
      "date": "2026-01-22",
      "authors_text": "Zijie Chen, Xiaowei Liu, Yong Xu, Shenghai Yuan, Jianping Li",
      "is_highlight": false,
      "score": 74.5,
      "summary": "This paper introduces LM-Calibr for targetless LiDAR-motor calibration and EVA-LIO for adaptive LiDAR-inertial odometry, enhancing localization in spinning actuated LiDAR systems.",
      "debug_abstract": "Accurate calibration and robust localization are fundamental for downstream tasks in spinning actuated LiDAR applications. Existing methods, however, require parameterizing extrinsic parameters based on different mounting configurations, limiting their generalizability. Additionally, spinning actuated LiDAR inevitably scans featureless regions, which complicates the balance between scanning coverage and localization robustness. To address these challenges, this letter presents a targetless LiDAR-motor calibration (LM-Calibr) on the basis of the Denavit-Hartenberg convention and an environmental adaptive LiDAR-inertial odometry (EVA-LIO). LM-Calibr supports calibration of LiDAR-motor systems with various mounting configurations. Extensive experiments demonstrate its accuracy and convergence across different scenarios, mounting angles, and initial values. Additionally, EVA-LIO adaptively selects downsample rates and map resolutions according to spatial scale. This adaptivity enables the actuator to operate at maximum speed, thereby enhancing scanning completeness while ensuring robust localization, even when LiDAR briefly scans featureless areas. The source code and hardware design are available on GitHub: \\textcolor{blue}{\\href{this https URL}{this http URL\\_calibr}}. The video is available at \\textcolor{blue}{\\href{this https URL}{this http URL}}"
    }
  ],
  "浙江大学": [
    {
      "title": "Hand3R: Online 4D Hand-Scene Reconstruction in the Wild",
      "url": "https://arxiv.org/abs/2602.03200",
      "date": "2026-02-03",
      "authors_text": "Wendi Hu, Haonan Zhou, Wenhao Hu, Gaoang Wang",
      "is_highlight": false,
      "score": 91.0,
      "summary": "Hand3R is an innovative online framework for simultaneous 4D hand and scene reconstruction from monocular video, integrating hand priors with scene context for enhanced accuracy.",
      "teaser_image": "https://arxiv.org/html/2602.03200/x1.png",
      "debug_abstract": "For Embodied AI, jointly reconstructing dynamic hands and the dense scene context is crucial for understanding physical interaction. However, most existing methods recover isolated hands in local coordinates, overlooking the surrounding 3D environment. To address this, we present Hand3R, the first online framework for joint 4D hand-scene reconstruction from monocular video. Hand3R synergizes a pre-trained hand expert with a 4D scene foundation model via a scene-aware visual prompting mechanism. By injecting high-fidelity hand priors into a persistent scene memory, our approach enables simultaneous reconstruction of accurate hand meshes and dense metric-scale scene geometry in a single forward pass. Experiments demonstrate that Hand3R bypasses the reliance on offline optimization and delivers competitive performance in both local hand reconstruction and global positioning."
    },
    {
      "title": "V2X-DSC: Multi-Agent Collaborative Perception with Distributed Source Coding Guided Communication",
      "url": "https://arxiv.org/abs/2602.00687",
      "date": "2026-01-31",
      "authors_text": "Yuankun Zeng, Shaohui Li, Zhi Li, Shulan Ruan, Yu Liu",
      "is_highlight": false,
      "score": 76.0,
      "summary": "V2X-DSC introduces a Conditional Codec for efficient multi-agent collaborative perception by compressing and reconstructing BEV features, optimizing bandwidth usage while maintaining accuracy.",
      "teaser_image": "https://arxiv.org/html/2602.00687/figs/overview.jpg",
      "debug_abstract": "Collaborative perception improves 3D understanding by fusing multi-agent observations, yet intermediate-feature sharing faces strict bandwidth constraints as dense BEV features saturate V2X links. We observe that collaborators view the same physical world, making their features strongly correlated; thus receivers only need innovation beyond their local context. Revisiting this from a distributed source coding perspective, we propose V2X-DSC, a framework with a Conditional Codec (DCC) for bandwidth-constrained fusion. The sender compresses BEV features into compact codes, while the receiver performs conditional reconstruction using its local features as side information, allocating bits to complementary cues rather than redundant content. This conditional structure regularizes learning, encouraging incremental representation and yielding lower-noise features. Experiments on DAIR-V2X, OPV2V, and V2X-Real demonstrate state-of-the-art accuracy-bandwidth trade-offs under KB-level communication, and generalizes as a plug-and-play communication layer across multiple fusion backbones."
    },
    {
      "title": "USS-Nav: Unified Spatio-Semantic Scene Graph for Lightweight UAV Zero-Shot Object Navigation",
      "url": "https://arxiv.org/abs/2602.00708",
      "date": "2026-01-31",
      "authors_text": "Weiqi Gai, Yuman Gao, Yuan Zhou, Yufan Xie, Zhiyang Liu",
      "is_highlight": false,
      "score": 80.0,
      "summary": "USS-Nav introduces a lightweight framework for UAV zero-shot object navigation, utilizing a Unified Spatio-Semantic scene graph for efficient exploration and improved computational performance.",
      "teaser_image": "https://arxiv.org/html/2602.00708/figs/skeleton_gen.png",
      "debug_abstract": "Zero-Shot Object Navigation in unknown environments poses significant challenges for Unmanned Aerial Vehicles (UAVs) due to the conflict between high-level semantic reasoning requirements and limited onboard computational resources. To address this, we present USS-Nav, a lightweight framework that incrementally constructs a Unified Spatio-Semantic scene graph and enables efficient Large Language Model (LLM)-augmented Zero-Shot Object Navigation in unknown environments. Specifically, we introduce an incremental Spatial Connectivity Graph generation method utilizing polyhedral expansion to capture global geometric topology, which is dynamically partitioned into semantic regions via graph clustering. Concurrently, open-vocabulary object semantics are instantiated and anchored to this topology to form a hierarchical environmental representation. Leveraging this hierarchical structure, we present a coarse-to-fine exploration strategy: LLM grounded in the scene graph&#39;s semantics to determine global target regions, while a local planner optimizes frontier coverage based on information gain. Experimental results demonstrate that our framework outperforms state-of-the-art methods in terms of computational efficiency and real-time update frequency (15 Hz) on a resource-constrained platform. Furthermore, ablation studies confirm the effectiveness of our framework, showing substantial improvements in Success weighted by Path Length (SPL). The source code will be made publicly available to foster further research."
    },
    {
      "title": "Resource-Efficient Reinforcement for Reasoning Large Language Models via Dynamic One-Shot Policy Refinement",
      "url": "https://arxiv.org/abs/2602.00815",
      "date": "2026-01-31",
      "authors_text": "Yunjian Zhang, Sudong Wang, Yang Li, Peiran Xu, Conghao Zhou",
      "is_highlight": false,
      "score": 57.0,
      "summary": "The paper introduces Dynamic One-Shot Policy Refinement (DoPR), a resource-efficient reinforcement learning strategy that enhances reasoning in large language models while significantly reducing training costs.",
      "teaser_image": "https://arxiv.org/html/2602.00815/dopr.png",
      "debug_abstract": "Large language models (LLMs) have exhibited remarkable performance on complex reasoning tasks, with reinforcement learning under verifiable rewards (RLVR) emerging as a principled framework for aligning model behavior with reasoning chains. Despite its promise, RLVR remains prohibitively resource-intensive, requiring extensive reward signals and incurring substantial rollout costs during training. In this work, we revisit the fundamental question of data and compute efficiency in RLVR. We first establish a theoretical lower bound on the sample complexity required to unlock reasoning capabilities, and empirically validate that strong performance can be achieved with a surprisingly small number of training instances. To tackle the computational burden, we propose Dynamic One-Shot Policy Refinement (DoPR), an uncertainty-aware RL strategy that dynamically selects a single informative training sample per batch for policy updates, guided by reward volatility and exploration-driven acquisition. DoPR reduces rollout overhead by nearly an order of magnitude while preserving competitive reasoning accuracy, offering a scalable and resource-efficient solution for LLM post-training. This approach offers a practical path toward more efficient and accessible RL-based training for reasoning-intensive LLM applications."
    },
    {
      "title": "TransNormal: Dense Visual Semantics for Diffusion-based Transparent Object Normal Estimation",
      "url": "https://arxiv.org/abs/2602.00839",
      "date": "2026-01-31",
      "authors_text": "Mingwei Li, Hehe Fan, Yi Yang",
      "is_highlight": false,
      "score": 68.0,
      "summary": "TransNormal is a novel framework for monocular normal estimation of transparent objects, leveraging diffusion priors and dense visual semantics to enhance accuracy and detail preservation.",
      "teaser_image": "https://arxiv.org/html/2602.00839/sources/teasers/ablation_results_teaser_a/input.png",
      "debug_abstract": "Monocular normal estimation for transparent objects is critical for laboratory automation, yet it remains challenging due to complex light refraction and reflection. These optical properties often lead to catastrophic failures in conventional depth and normal sensors, hindering the deployment of embodied AI in scientific environments. We propose TransNormal, a novel framework that adapts pre-trained diffusion priors for single-step normal regression. To handle the lack of texture in transparent surfaces, TransNormal integrates dense visual semantics from DINOv3 via a cross-attention mechanism, providing strong geometric cues. Furthermore, we employ a multi-task learning objective and wavelet-based regularization to ensure the preservation of fine-grained structural details. To support this task, we introduce TransNormal-Synthetic, a physics-based dataset with high-fidelity normal maps for transparent labware. Extensive experiments demonstrate that TransNormal significantly outperforms state-of-the-art methods: on the ClearGrasp benchmark, it reduces mean error by 24.4% and improves 11.25° accuracy by 22.8%; on ClearPose, it achieves a 15.2% reduction in mean error. The code and dataset will be made publicly available at this https URL."
    },
    {
      "title": "LightCity: An Urban Dataset for Outdoor Inverse Rendering and Reconstruction under Multi-illumination Conditions",
      "url": "https://arxiv.org/abs/2602.01118",
      "date": "2026-02-01",
      "authors_text": "Jingjing Wang, Qirui Hu, Chong Bao, Yuke Zhu, Hujun Bao",
      "is_highlight": false,
      "score": 69.0,
      "summary": "LightCity is a comprehensive synthetic urban dataset designed for inverse rendering and 3D reconstruction, addressing challenges of complex multi-illumination conditions with over 50K images.",
      "teaser_image": "https://arxiv.org/html/2602.01118/x2.png",
      "debug_abstract": "Inverse rendering in urban scenes is pivotal for applications like autonomous driving and digital twins. Yet, it faces significant challenges due to complex illumination conditions, including multi-illumination and indirect light and shadow effects. However, the effects of these challenges on intrinsic decomposition and 3D reconstruction have not been explored due to the lack of appropriate datasets. In this paper, we present LightCity, a novel high-quality synthetic urban dataset featuring diverse illumination conditions with realistic indirect light and shadow effects. LightCity encompasses over 300 sky maps with highly controllable illumination, varying scales with street-level and aerial perspectives over 50K images, and rich properties such as depth, normal, material components, light and indirect light, etc. Besides, we leverage LightCity to benchmark three fundamental tasks in the urban environments and conduct a comprehensive analysis of these benchmarks, laying a robust foundation for advancing related research."
    },
    {
      "title": "PolicyFlow: Policy Optimization with Continuous Normalizing Flow in Reinforcement Learning",
      "url": "https://arxiv.org/abs/2602.01156",
      "date": "2026-02-01",
      "authors_text": "Shunpeng Yang, Ben Liu, Hua Chen",
      "is_highlight": false,
      "score": 71.0,
      "summary": "PolicyFlow is a novel reinforcement learning algorithm that integrates continuous normalizing flows with PPO, enhancing performance and stability while reducing computational costs and encouraging diverse behaviors.",
      "teaser_image": "https://arxiv.org/html/2602.01156/x1.png",
      "debug_abstract": "Among on-policy reinforcement learning algorithms, Proximal Policy Optimization (PPO) demonstrates is widely favored for its simplicity, numerical stability, and strong empirical performance. Standard PPO relies on surrogate objectives defined via importance ratios, which require evaluating policy likelihood that is typically straightforward when the policy is modeled as a Gaussian distribution. However, extending PPO to more expressive, high-capacity policy models such as continuous normalizing flows (CNFs), also known as flow-matching models, is challenging because likelihood evaluation along the full flow trajectory is computationally expensive and often numerically unstable. To resolve this issue, we propose PolicyFlow, a novel on-policy CNF-based reinforcement learning algorithm that integrates expressive CNF policies with PPO-style objectives without requiring likelihood evaluation along the full flow path. PolicyFlow approximates importance ratios using velocity field variations along a simple interpolation path, reducing computational overhead without compromising training stability. To further prevent mode collapse and further encourage diverse behaviors, we propose the Brownian Regularizer, an implicit policy entropy regularizer inspired by Brownian motion, which is conceptually elegant and computationally lightweight. Experiments on diverse tasks across various environments including MultiGoal, PointMaze, IsaacLab and MuJoCo Playground show that PolicyFlow achieves competitive or superior performance compared to PPO using Gaussian policies and flow-based baselines including FPO and DPPO. Notably, results on MultiGoal highlight PolicyFlow&#39;s ability to capture richer multimodal action distributions."
    },
    {
      "title": "TriphiBot: A Triphibious Robot Combining FOC-based Propulsion with Eccentric Design",
      "url": "https://arxiv.org/abs/2602.01385",
      "date": "2026-02-01",
      "authors_text": "Xiangyu Li, Mingwei Lai, Mengke Zhang, Junxiao Lin, Tiancheng Lai",
      "is_highlight": false,
      "score": 96.0,
      "summary": "The TriphiBot is a novel triphibious robot utilizing a minimalist design and FOC-based propulsion for efficient multi-domain motion and seamless transitions across air, land, and water.",
      "teaser_image": "https://arxiv.org/html/2602.01385/x6.png",
      "debug_abstract": "Triphibious robots capable of multi-domain motion and cross-domain transitions are promising to handle complex tasks across diverse environments. However, existing designs primarily focus on dual-mode platforms, and some designs suffer from high mechanical complexity or low propulsion efficiency, which limits their application. In this paper, we propose a novel triphibious robot capable of aerial, terrestrial, and aquatic motion, by a minimalist design combining a quadcopter structure with two passive wheels, without extra actuators. To address inefficiency of ground-support motion (moving on land/seabed) for quadcopter based designs, we introduce an eccentric Center of Gravity (CoG) design that inherently aligns thrust with motion, enhancing efficiency without specialized mechanical transformation designs. Furthermore, to address the drastic differences in motion control caused by different fluids (air and water), we develop a unified propulsion system based on Field-Oriented Control (FOC). This method resolves torque matching issues and enables precise, rapid bidirectional thrust across different mediums. Grounded in the perspective of living condition and ground support, we analyse the robot&#39;s dynamics and propose a Hybrid Nonlinear Model Predictive Control (HNMPC)-PID control system to ensure stable multi-domain motion and seamless transitions. Experimental results validate the robot&#39;s multi-domain motion and cross-mode transition capability, along with the efficiency and adaptability of the proposed propulsion system."
    },
    {
      "title": "Know Your Step: Faster and Better Alignment for Flow Matching Models via Step-aware Advantages",
      "url": "https://arxiv.org/abs/2602.01591",
      "date": "2026-02-02",
      "authors_text": "Zhixiong Yue, Zixuan Ni, Feiyang Ye, Jinshan Zhang, Sheng Shen",
      "is_highlight": false,
      "score": 39.0,
      "summary": "The paper presents TAFS GRPO, a novel framework enhancing flow matching models for text-to-image generation by improving alignment with human preferences through efficient few-step sampling and adaptive noise.",
      "teaser_image": "https://arxiv.org/html/2602.01591/x3.png",
      "debug_abstract": "Recent advances in flow matching models, particularly with reinforcement learning (RL), have significantly enhanced human preference alignment in few step text to image generators. However, existing RL based approaches for flow matching models typically rely on numerous denoising steps, while suffering from sparse and imprecise reward signals that often lead to suboptimal alignment. To address these limitations, we propose Temperature Annealed Few step Sampling with Group Relative Policy Optimization (TAFS GRPO), a novel framework for training flow matching text to image models into efficient few step generators well aligned with human preferences. Our method iteratively injects adaptive temporal noise onto the results of one step samples. By repeatedly annealing the model&#39;s sampled outputs, it introduces stochasticity into the sampling process while preserving the semantic integrity of each generated image. Moreover, its step aware advantage integration mechanism combines the GRPO to avoid the need for the differentiable of reward function and provide dense and step specific rewards for stable policy optimization. Extensive experiments demonstrate that TAFS GRPO achieves strong performance in few step text to image generation and significantly improves the alignment of generated images with human preferences. The code and models of this work will be available to facilitate further research."
    },
    {
      "title": "From Frames to Sequences: Temporally Consistent Human-Centric Dense Prediction",
      "url": "https://arxiv.org/abs/2602.01661",
      "date": "2026-02-02",
      "authors_text": "Xingyu Miao, Junting Dong, Qin Zhao, Yuhang Yang, Junhao Chen",
      "is_highlight": false,
      "score": 43.0,
      "summary": "This paper presents a novel synthetic data pipeline and a ViT-based model for achieving temporally consistent human-centric dense prediction in video sequences, outperforming existing methods.",
      "teaser_image": "https://arxiv.org/html/2602.01661/x2.png",
      "debug_abstract": "In this work, we focus on the challenge of temporally consistent human-centric dense prediction across video sequences. Existing models achieve strong per-frame accuracy but often flicker under motion, occlusion, and lighting changes, and they rarely have paired human video supervision for multiple dense tasks. We address this gap with a scalable synthetic data pipeline that generates photorealistic human frames and motion-aligned sequences with pixel-accurate depth, normals, and masks. Unlike prior static data synthetic pipelines, our pipeline provides both frame-level labels for spatial learning and sequence-level supervision for temporal learning. Building on this, we train a unified ViT-based dense predictor that (i) injects an explicit human geometric prior via CSE embeddings and (ii) improves geometry-feature reliability with a lightweight channel reweighting module after feature fusion. Our two-stage training strategy, combining static pretraining with dynamic sequence supervision, enables the model first to acquire robust spatial representations and then refine temporal consistency across motion-aligned sequences. Extensive experiments show that we achieve state-of-the-art performance on THuman2.1 and Hi4D and generalize effectively to in-the-wild videos."
    },
    {
      "title": "SWE-Universe: Scale Real-World Verifiable Environments to Millions",
      "url": "https://arxiv.org/abs/2602.02361",
      "date": "2026-02-02",
      "authors_text": "Mouxiang Chen, Lei Zhang, Yunlong Feng, Xuwu Wang, Wenting Zhao",
      "is_highlight": false,
      "score": 16.0,
      "summary": "SWE-Universe introduces a scalable framework for creating millions of verifiable software engineering environments from GitHub pull requests, enhancing agent training and performance.",
      "teaser_image": "https://arxiv.org/html/2602.02361/figures/env-build.png",
      "debug_abstract": "We propose SWE-Universe, a scalable and efficient framework for automatically constructing real-world software engineering (SWE) verifiable environments from GitHub pull requests (PRs). To overcome the prevalent challenges of automatic building, such as low production yield, weak verifiers, and prohibitive cost, our framework utilizes a building agent powered by an efficient custom-trained model. This agent employs iterative self-verification and in-loop hacking detection to ensure the reliable generation of high-fidelity, verifiable tasks. Using this method, we scale the number of real-world multilingual SWE environments to a million scale (807,693). We demonstrate the profound value of our environments through large-scale agentic mid-training and reinforcement learning. Finally, we applied this technique to Qwen3-Max-Thinking and achieved a score of 75.3% on SWE-Bench Verified. Our work provides both a critical resource and a robust methodology to advance the next generation of coding agents."
    },
    {
      "title": "PLANING: A Loosely Coupled Triangle-Gaussian Framework for Streaming 3D Reconstruction",
      "url": "https://arxiv.org/abs/2601.22046",
      "date": "2026-01-29",
      "authors_text": "Changjian Jiang, Kerui Ren, Xudong Li, Kaiwen Song, Linning Xu",
      "is_highlight": false,
      "score": 66.0,
      "summary": "PLANING is a novel streaming 3D reconstruction framework that efficiently combines geometric primitives and neural Gaussians, enhancing quality and speed while reducing redundancy.",
      "teaser_image": "https://arxiv.org/html/2601.22046/x3.png",
      "debug_abstract": "Streaming reconstruction from monocular image sequences remains challenging, as existing methods typically favor either high-quality rendering or accurate geometry, but rarely both. We present PLANING, an efficient on-the-fly reconstruction framework built on a hybrid representation that loosely couples explicit geometric primitives with neural Gaussians, enabling geometry and appearance to be modeled in a decoupled manner. This decoupling supports an online initialization and optimization strategy that separates geometry and appearance updates, yielding stable streaming reconstruction with substantially reduced structural redundancy. PLANING improves dense mesh Chamfer-L2 by 18.52% over PGSR, surpasses ARTDECO by 1.31 dB PSNR, and reconstructs ScanNetV2 scenes in under 100 seconds, over 5x faster than 2D Gaussian Splatting, while matching the quality of offline per-scene optimization. Beyond reconstruction quality, the structural clarity and computational efficiency of \\modelname~make it well suited for a broad range of downstream applications, such as enabling large-scale scene modeling and simulation-ready environments for embodied AI. Project page: this https URL ."
    },
    {
      "title": "Policy of Thoughts: Scaling LLM Reasoning via Test-time Policy Evolution",
      "url": "https://arxiv.org/abs/2601.20379",
      "date": "2026-01-28",
      "authors_text": "Zhengbo Jiao, Hongyu Xian, Qinglong Wang, Yunpu Ma, Zhebo Wang",
      "is_highlight": false,
      "score": 74.0,
      "summary": "The Policy of Thoughts framework enhances LLM reasoning by enabling real-time policy evolution through execution feedback, significantly improving performance on complex tasks.",
      "teaser_image": "https://arxiv.org/html/2601.20379/x1.png",
      "debug_abstract": "Large language models (LLMs) struggle with complex, long-horizon reasoning due to instability caused by their frozen policy assumption. Current test-time scaling methods treat execution feedback merely as an external signal for filtering or rewriting trajectories, without internalizing it to improve the underlying reasoning strategy. Inspired by Popper&#39;s epistemology of &#34;conjectures and refutations,&#34; we argue that intelligence requires real-time evolution of the model&#39;s policy through learning from failed attempts. We introduce Policy of Thoughts (PoT), a framework that recasts reasoning as a within-instance online optimization process. PoT first generates diverse candidate solutions via an efficient exploration mechanism, then uses Group Relative Policy Optimization (GRPO) to update a transient LoRA adapter based on execution feedback. This closed-loop design enables dynamic, instance-specific refinement of the model&#39;s reasoning priors. Experiments show that PoT dramatically boosts performance: a 4B model achieves 49.71% accuracy on LiveCodeBench, outperforming GPT-4o and DeepSeek-V3 despite being over 50 smaller."
    },
    {
      "title": "KeepLoRA: Continual Learning with Residual Gradient Adaptation",
      "url": "https://arxiv.org/abs/2601.19659",
      "date": "2026-01-27",
      "authors_text": "Mao-Lin Luo, Zi-Hao Zhou, Yi-Lin Zhang, Yuanyu Wan, Tong Wei",
      "is_highlight": false,
      "score": 64.0,
      "summary": "KeepLoRA is a novel approach for continual learning in vision-language models that effectively balances knowledge retention and task plasticity by restricting updates to a residual subspace.",
      "teaser_image": "https://arxiv.org/html/2601.19659/x2.png",
      "debug_abstract": "Continual learning for pre-trained vision-language models requires balancing three competing objectives: retaining pre-trained knowledge, preserving knowledge from a sequence of learned tasks, and maintaining the plasticity to acquire new knowledge. This paper presents a simple but effective approach called KeepLoRA to effectively balance these objectives. We first analyze the knowledge retention mechanism within the model parameter space and find that general knowledge is mainly encoded in the principal subspace, while task-specific knowledge is encoded in the residual subspace. Motivated by this finding, KeepLoRA learns new tasks by restricting LoRA parameter updates in the residual subspace to prevent interfering with previously learned capabilities. Specifically, we infuse knowledge for a new task by projecting its gradient onto a subspace orthogonal to both the principal subspace of pre-trained model and the dominant directions of previous task features. Our theoretical and empirical analyses confirm that KeepLoRA balances the three objectives and achieves state-of-the-art performance. The implementation code is available at this https URL."
    },
    {
      "title": "A DVL Aided Loosely Coupled Inertial Navigation Strategy for AUVs with Attitude Error Modeling and Variance Propagation",
      "url": "https://arxiv.org/abs/2601.19509",
      "date": "2026-01-27",
      "authors_text": "Jin Huang, Zichen Liu, Haoda Li, Zhikun Wang, Ying Chen",
      "is_highlight": false,
      "score": 77.0,
      "summary": "This paper presents a novel navigation strategy for AUVs that integrates attitude error modeling and variance propagation to significantly enhance long-term SINS/DVL navigation accuracy.",
      "debug_abstract": "In underwater navigation systems, strap-down inertial navigation system/Doppler velocity log (SINS/DVL)-based loosely coupled architectures are widely adopted. Conventional approaches project DVL velocities from the body coordinate system to the navigation coordinate system using SINS-derived attitude; however, accumulated attitude estimation errors introduce biases into velocity projection and degrade navigation performance during long-term operation. To address this issue, two complementary improvements are introduced. First, a vehicle attitude error-aware DVL velocity transformation model is formulated by incorporating attitude error terms into the observation equation to reduce projection-induced velocity bias. Second, a covariance matrix-based variance propagation method is developed to transform DVL measurement uncertainty across coordinate systems, introducing an expectation-based attitude error compensation term to achieve statistically consistent noise modeling. Simulation and field experiment results demonstrate that both improvements individually enhance navigation accuracy and confirm that accumulated attitude errors affect both projected velocity measurements and their associated uncertainty. When jointly applied, long-term error divergence is effectively suppressed. Field experimental results show that the proposed approach achieves a 78.3% improvement in 3D position RMSE and a 71.8% reduction in the maximum component-wise position error compared with the baseline IMU+DVL method, providing a robust solution for improving long-term SINS/DVL navigation performance."
    },
    {
      "title": "Temp-R1: A Unified Autonomous Agent for Complex Temporal KGQA via Reverse Curriculum Reinforcement Learning",
      "url": "https://arxiv.org/abs/2601.18296",
      "date": "2026-01-26",
      "authors_text": "Zhaoyan Gong, Zhiqiang Liu, Songze Li, Xiaoke Guo, Yuanxiang Liu",
      "is_highlight": false,
      "score": 77.0,
      "summary": "Temp-R1 is a novel autonomous agent for Temporal Knowledge Graph Question Answering, utilizing reverse curriculum reinforcement learning to enhance reasoning and outperform existing methods.",
      "debug_abstract": "Temporal Knowledge Graph Question Answering (TKGQA) is inherently challenging, as it requires sophisticated reasoning over dynamic facts with multi-hop dependencies and complex temporal constraints. Existing methods rely on fixed workflows and expensive closed-source APIs, limiting flexibility and scalability. We propose Temp-R1, the first autonomous end-to-end agent for TKGQA trained through reinforcement learning. To address cognitive overload in single-action reasoning, we expand the action space with specialized internal actions alongside external action. To prevent shortcut learning on simple questions, we introduce reverse curriculum learning that trains on difficult questions first, forcing the development of sophisticated reasoning before transferring to easier cases. Our 8B-parameter Temp-R1 achieves state-of-the-art performance on MultiTQ and TimelineKGQA, improving 19.8% over strong baselines on complex questions. Our work establishes a new paradigm for autonomous temporal reasoning agents. Our code will be publicly available soon at this https URL."
    },
    {
      "title": "Dep-Search: Learning Dependency-Aware Reasoning Traces with Persistent Memory",
      "url": "https://arxiv.org/abs/2601.18771",
      "date": "2026-01-26",
      "authors_text": "Yanming Liu, Xinyue Peng, Zixuan Yan, Yanxin Shen, Wenjie Xu",
      "is_highlight": false,
      "score": 46.0,
      "summary": "Dep-Search enhances large language models' multi-hop reasoning by integrating dependency-aware search, structured retrieval, and persistent memory, outperforming existing frameworks in complex tasks.",
      "debug_abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in complex reasoning tasks, particularly when augmented with search mechanisms that enable systematic exploration of external knowledge bases. The field has evolved from traditional retrieval-augmented generation (RAG) frameworks to more sophisticated search-based frameworks that orchestrate multi-step reasoning through explicit search strategies. However, existing search frameworks still rely heavily on implicit natural language reasoning to determine search strategies and how to leverage retrieved information across reasoning steps. This reliance on implicit reasoning creates fundamental challenges for managing dependencies between sub-questions, efficiently reusing previously retrieved knowledge, and learning optimal search strategies through reinforcement learning. To address these limitations, we propose Dep-Search, a dependency-aware search framework that advances beyond existing search frameworks by integrating structured reasoning, retrieval, and persistent memory through GRPO. Dep-Search introduces explicit control mechanisms that enable the model to decompose questions with dependency relationships, retrieve information when needed, access previously stored knowledge from memory, and summarize long reasoning contexts into reusable memory entries. Through extensive experiments on seven diverse question answering datasets, we demonstrate that Dep-Search significantly enhances LLMs&#39; ability to tackle complex multi-hop reasoning tasks, achieving substantial improvements over strong baselines across different model scales."
    },
    {
      "title": "IBISAgent: Reinforcing Pixel-Level Visual Reasoning in MLLMs for Universal Biomedical Object Referring and Segmentation",
      "url": "https://arxiv.org/abs/2601.03054",
      "date": "2026-01-23",
      "authors_text": "Yankai Jiang, Qiaoru Li, Binlu Xu, Haoran Sun, Chao Ding",
      "is_highlight": false,
      "score": 65.0,
      "summary": "IBISAgent enhances pixel-level visual reasoning in medical MLLMs through iterative decision-making and reinforcement learning, outperforming existing methods in segmentation tasks.",
      "debug_abstract": "Recent research on medical MLLMs has gradually shifted its focus from image-level understanding to fine-grained, pixel-level comprehension. Although segmentation serves as the foundation for pixel-level understanding, existing approaches face two major challenges. First, they introduce implicit segmentation tokens and require simultaneous fine-tuning of both the MLLM and external pixel decoders, which increases the risk of catastrophic forgetting and limits generalization to out-of-domain scenarios. Second, most methods rely on single-pass reasoning and lack the capability to iteratively refine segmentation results, leading to suboptimal performance. To overcome these limitations, we propose a novel agentic MLLM, named IBISAgent, that reformulates segmentation as a vision-centric, multi-step decision-making process. IBISAgent enables MLLMs to generate interleaved reasoning and text-based click actions, invoke segmentation tools, and produce high-quality masks without architectural modifications. By iteratively performing multi-step visual reasoning on masked image features, IBISAgent naturally supports mask refinement and promotes the development of pixel-level visual reasoning capabilities. We further design a two-stage training framework consisting of cold-start supervised fine-tuning and agentic reinforcement learning with tailored, fine-grained rewards, enhancing the model&#39;s robustness in complex medical referring and reasoning segmentation tasks. Extensive experiments demonstrate that IBISAgent consistently outperforms both closed-source and open-source SOTA methods. All datasets, code, and trained models will be released publicly."
    },
    {
      "title": "Collision-Free Humanoid Traversal in Cluttered Indoor Scenes",
      "url": "https://arxiv.org/abs/2601.16035",
      "date": "2026-01-23",
      "authors_text": "Han Xue, Sikai Liang, Zhikai Zhang, Zicheng Zeng, Yun Liu",
      "is_highlight": false,
      "score": 90.0,
      "summary": "The paper presents Humanoid Potential Field (HumanoidPF) for enabling collision-free traversal of humanoids in cluttered indoor environments, enhancing RL-based skill learning and sim-to-real transfer.",
      "debug_abstract": "We study the problem of collision-free humanoid traversal in cluttered indoor scenes, such as hurdling over objects scattered on the floor, crouching under low-hanging obstacles, or squeezing through narrow passages. To achieve this goal, the humanoid needs to map its perception of surrounding obstacles with diverse spatial layouts and geometries to the corresponding traversal skills. However, the lack of an effective representation that captures humanoid-obstacle relationships during collision avoidance makes directly learning such mappings difficult. We therefore propose Humanoid Potential Field (HumanoidPF), which encodes these relationships as collision-free motion directions, significantly facilitating RL-based traversal skill learning. We also find that HumanoidPF exhibits a surprisingly negligible sim-to-real gap as a perceptual representation. To further enable generalizable traversal skills through diverse and challenging cluttered indoor scenes, we further propose a hybrid scene generation method, incorporating crops of realistic 3D indoor scenes and procedurally synthesized obstacles. We successfully transfer our policy to the real world and develop a teleoperation system where users could command the humanoid to traverse in cluttered indoor scenes with just a single click. Extensive experiments are conducted in both simulation and the real world to validate the effectiveness of our method. Demos and code can be found in our website: this https URL."
    },
    {
      "title": "kNN-Graph: An adaptive graph model for $k$-nearest neighbors",
      "url": "https://arxiv.org/abs/2601.16509",
      "date": "2026-01-23",
      "authors_text": "Jiaye Li, Gang Chen, Hang Xu, Shichao Zhang",
      "is_highlight": false,
      "score": 50.0,
      "summary": "The kNN-Graph introduces an adaptive graph model that enhances k-nearest neighbors' inference speed and accuracy by decoupling computational complexity from neighbor selection during training.",
      "debug_abstract": "The k-nearest neighbors (kNN) algorithm is a cornerstone of non-parametric classification in artificial intelligence, yet its deployment in large-scale applications is persistently constrained by the computational trade-off between inference speed and accuracy. Existing approximate nearest neighbor solutions accelerate retrieval but often degrade classification precision and lack adaptability in selecting the optimal neighborhood size (k). Here, we present an adaptive graph model that decouples inference latency from computational complexity. By integrating a Hierarchical Navigable Small World (HNSW) graph with a pre-computed voting mechanism, our framework completely transfers the computational burden of neighbor selection and weighting to the training phase. Within this topological structure, higher graph layers enable rapid navigation, while lower layers encode precise, node-specific decision boundaries with adaptive neighbor counts. Benchmarking against eight state-of-the-art baselines across six diverse datasets, we demonstrate that this architecture significantly accelerates inference speeds, achieving real-time performance, without compromising classification accuracy. These findings offer a scalable, robust solution to the long-standing inference bottleneck of kNN, establishing a new structural paradigm for graph-based nonparametric learning."
    },
    {
      "title": "ReWeaver: Towards Simulation-Ready and Topology-Accurate Garment Reconstruction",
      "url": "https://arxiv.org/abs/2601.16672",
      "date": "2026-01-23",
      "authors_text": "Ming Li, Hui Shan, Kai Zheng, Chentao Shen, Siyu Liu",
      "is_highlight": false,
      "score": 76.0,
      "summary": "ReWeaver is a novel framework for accurate 3D garment reconstruction from sparse images, enhancing topology fidelity for simulations and robotic applications.",
      "debug_abstract": "High-quality 3D garment reconstruction plays a crucial role in mitigating the sim-to-real gap in applications such as digital avatars, virtual try-on and robotic manipulation. However, existing garment reconstruction methods typically rely on unstructured representations, such as 3D Gaussian Splats, struggling to provide accurate reconstructions of garment topology and sewing structures. As a result, the reconstructed outputs are often unsuitable for high-fidelity physical simulation. We propose ReWeaver, a novel framework for topology-accurate 3D garment and sewing pattern reconstruction from sparse multi-view RGB images. Given as few as four input views, ReWeaver predicts seams and panels as well as their connectivities in both the 2D UV space and the 3D space. The predicted seams and panels align precisely with the multi-view images, yielding structured 2D--3D garment representations suitable for 3D perception, high-fidelity physical simulation, and robotic manipulation. To enable effective training, we construct a large-scale dataset GCD-TS, comprising multi-view RGB images, 3D garment geometries, textured human body meshes and annotated sewing patterns. The dataset contains over 100,000 synthetic samples covering a wide range of complex geometries and topologies. Extensive experiments show that ReWeaver consistently outperforms existing methods in terms of topology accuracy, geometry alignment and seam-panel consistency."
    },
    {
      "title": "REL-SF4PASS: Panoramic Semantic Segmentation with REL Depth Representation and Spherical Fusion",
      "url": "https://arxiv.org/abs/2601.16788",
      "date": "2026-01-23",
      "authors_text": "Xuewei Li, Xinghan Bao, Zhimin Chen, Xi Li",
      "is_highlight": false,
      "score": 67.0,
      "summary": "The REL-SF4PASS method enhances panoramic semantic segmentation by utilizing a novel REL depth representation and Spherical-dynamic Multi-Modal Fusion, improving accuracy and robustness significantly.",
      "debug_abstract": "As an important and challenging problem in computer vision, Panoramic Semantic Segmentation (PASS) aims to give complete scene perception based on an ultra-wide angle of view. Most PASS methods often focus on spherical geometry with RGB input or using the depth information in original or HHA format, which does not make full use of panoramic image geometry. To address these shortcomings, we propose REL-SF4PASS with our REL depth representation based on cylindrical coordinate and Spherical-dynamic Multi-Modal Fusion SMMF. REL is made up of Rectified Depth, Elevation-Gained Vertical Inclination Angle, and Lateral Orientation Angle, which fully represents 3D space in cylindrical coordinate style and the surface normal direction. SMMF aims to ensure the diversity of fusion for different panoramic image regions and reduce the breakage of cylinder side surface expansion in ERP projection, which uses different fusion strategies to match the different regions in panoramic images. Experimental results show that REL-SF4PASS considerably improves performance and robustness on popular benchmark, Stanford2D3D Panoramic datasets. It gains 2.35% average mIoU improvement on all 3 folds and reduces the performance variance by approximately 70% when facing 3D disturbance."
    },
    {
      "title": "VideoThinker: Building Agentic VideoLLMs with LLM-Guided Tool Reasoning",
      "url": "https://arxiv.org/abs/2601.15724",
      "date": "2026-01-22",
      "authors_text": "Chenglin Li, Qianglong Chen, Feng Han, Yikun Wang, Xingxi Yin",
      "is_highlight": false,
      "score": 89.6,
      "summary": "VideoThinker enhances long-form video understanding by utilizing synthetic tool interaction trajectories for adaptive reasoning, outperforming existing models through dynamic exploration and multi-step tool use.",
      "debug_abstract": "Long-form video understanding remains a fundamental challenge for current Video Large Language Models. Most existing models rely on static reasoning over uniformly sampled frames, which weakens temporal localization and leads to substantial information loss in long videos. Agentic tools such as temporal retrieval, spatial zoom, and temporal zoom offer a natural way to overcome these limitations by enabling adaptive exploration of key moments. However, constructing agentic video understanding data requires models that already possess strong long-form video comprehension, creating a circular dependency. We address this challenge with VideoThinker, an agentic Video Large Language Model trained entirely on synthetic tool interaction trajectories. Our key idea is to convert videos into rich captions and employ a powerful agentic language model to generate multi-step tool use sequences in caption space. These trajectories are subsequently grounded back to video by replacing captions with the corresponding frames, yielding a large-scale interleaved video and tool reasoning dataset without requiring any long-form understanding from the underlying model. Training on this synthetic agentic dataset equips VideoThinker with dynamic reasoning capabilities, adaptive temporal exploration, and multi-step tool use. Remarkably, VideoThinker significantly outperforms both caption-only language model agents and strong video model baselines across long-video benchmarks, demonstrating the effectiveness of tool augmented synthetic data and adaptive retrieval and zoom reasoning for long-form video understanding."
    },
    {
      "title": "EVolSplat4D: Efficient Volume-based Gaussian Splatting for 4D Urban Scene Synthesis",
      "url": "https://arxiv.org/abs/2601.15951",
      "date": "2026-01-22",
      "authors_text": "Sheng Miao, Sijin Li, Pan Wang, Dongfeng Bai, Bingbing Liu",
      "is_highlight": false,
      "score": 76.8,
      "summary": "EVolSplat4D introduces a unified framework for efficient 4D urban scene synthesis, enhancing reconstruction accuracy and consistency for both static and dynamic environments.",
      "debug_abstract": "Novel view synthesis (NVS) of static and dynamic urban scenes is essential for autonomous driving simulation, yet existing methods often struggle to balance reconstruction time with quality. While state-of-the-art neural radiance fields and 3D Gaussian Splatting approaches achieve photorealism, they often rely on time-consuming per-scene optimization. Conversely, emerging feed-forward methods frequently adopt per-pixel Gaussian representations, which lead to 3D inconsistencies when aggregating multi-view predictions in complex, dynamic environments. We propose EvolSplat4D, a feed-forward framework that moves beyond existing per-pixel paradigms by unifying volume-based and pixel-based Gaussian prediction across three specialized branches. For close-range static regions, we predict consistent geometry of 3D Gaussians over multiple frames directly from a 3D feature volume, complemented by a semantically-enhanced image-based rendering module for predicting their appearance. For dynamic actors, we utilize object-centric canonical spaces and a motion-adjusted rendering module to aggregate temporal features, ensuring stable 4D reconstruction despite noisy motion priors. Far-Field scenery is handled by an efficient per-pixel Gaussian branch to ensure full-scene coverage. Experimental results on the KITTI-360, KITTI, Waymo, and PandaSet datasets show that EvolSplat4D reconstructs both static and dynamic environments with superior accuracy and consistency, outperforming both per-scene optimization and state-of-the-art feed-forward baselines."
    },
    {
      "title": "PUMA: Perception-driven Unified Foothold Prior for Mobility Augmented Quadruped Parkour",
      "url": "https://arxiv.org/abs/2601.15995",
      "date": "2026-01-22",
      "authors_text": "Liang Wang, Kanzhong Yao, Yang Liu, Weikai Qin, Jun Wu",
      "is_highlight": false,
      "score": 88.9,
      "summary": "PUMA is an end-to-end learning framework that enhances quadruped parkour by integrating visual perception and foothold priors for improved agility and adaptability.",
      "debug_abstract": "Parkour tasks for quadrupeds have emerged as a promising benchmark for agile locomotion. While human athletes can effectively perceive environmental characteristics to select appropriate footholds for obstacle traversal, endowing legged robots with similar perceptual reasoning remains a significant challenge. Existing methods often rely on hierarchical controllers that follow pre-computed footholds, thereby constraining the robot&#39;s real-time adaptability and the exploratory potential of reinforcement learning. To overcome these challenges, we present PUMA, an end-to-end learning framework that integrates visual perception and foothold priors into a single-stage training process. This approach leverages terrain features to estimate egocentric polar foothold priors, composed of relative distance and heading, guiding the robot in active posture adaptation for parkour tasks. Extensive experiments conducted in simulation and real-world environments across various discrete complex terrains, demonstrate PUMA&#39;s exceptional agility and robustness in challenging scenarios."
    }
  ],
  "MARS多模态学习实验室": [
    {
      "title": "EventFlash: Towards Efficient MLLMs for Event-Based Vision",
      "url": "https://arxiv.org/abs/2602.03230",
      "date": "2026-02-03",
      "authors_text": "Shaoyu Liu, Jianing Li, Guanghui Zhao, Yunjian Zhang, Wen Jiang",
      "is_highlight": false,
      "score": 63.0,
      "summary": "EventFlash introduces an efficient MLLM for event-based vision, leveraging spatiotemporal sparsification and adaptive sampling to enhance performance and reduce computational costs.",
      "teaser_image": "https://arxiv.org/html/2602.03230/x2.png",
      "debug_abstract": "Event-based multimodal large language models (MLLMs) enable robust perception in high-speed and low-light scenarios, addressing key limitations of frame-based MLLMs. However, current event-based MLLMs often rely on dense image-like processing paradigms, overlooking the spatiotemporal sparsity of event streams and resulting in high computational cost. In this paper, we propose EventFlash, a novel and efficient MLLM to explore spatiotemporal token sparsification for reducing data redundancy and accelerating inference. Technically, we build EventMind, a large-scale and scene-diverse dataset with over 500k instruction sets, providing both short and long event stream sequences to support our curriculum training strategy. We then present an adaptive temporal window aggregation module for efficient temporal sampling, which adaptively compresses temporal tokens while retaining key temporal cues. Finally, a sparse density-guided attention module is designed to improve spatial token efficiency by selecting informative regions and suppressing empty or sparse areas. Experimental results show that EventFlash achieves a $12.4\\times$ throughput improvement over the baseline (EventFlash-Zero) while maintaining comparable performance. It supports long-range event stream processing with up to 1,000 bins, significantly outperforming the 5-bin limit of EventGPT. We believe EventFlash serves as an efficient foundation model for event-based vision."
    },
    {
      "title": "InstaDrive: Instance-Aware Driving World Models for Realistic and Consistent Video Generation",
      "url": "https://arxiv.org/abs/2602.03242",
      "date": "2026-02-03",
      "authors_text": "Zhuoran Yang, Xi Guo, Chenjing Ding, Chiyu Wang, Wei Wu",
      "is_highlight": false,
      "score": 84.0,
      "summary": "InstaDrive introduces a framework that enhances realistic driving video generation by ensuring instance-level temporal consistency and spatial geometric fidelity through innovative feature extraction and alignment techniques.",
      "teaser_image": "https://arxiv.org/html/2602.03242/x2.png",
      "debug_abstract": "Autonomous driving relies on robust models trained on high-quality, large-scale multi-view driving videos. While world models offer a cost-effective solution for generating realistic driving videos, they struggle to maintain instance-level temporal consistency and spatial geometric fidelity. To address these challenges, we propose InstaDrive, a novel framework that enhances driving video realism through two key advancements: (1) Instance Flow Guider, which extracts and propagates instance features across frames to enforce temporal consistency, preserving instance identity over time. (2) Spatial Geometric Aligner, which improves spatial reasoning, ensures precise instance positioning, and explicitly models occlusion hierarchies. By incorporating these instance-aware mechanisms, InstaDrive achieves state-of-the-art video generation quality and enhances downstream autonomous driving tasks on the nuScenes dataset. Additionally, we utilize CARLA&#39;s autopilot to procedurally and stochastically simulate rare but safety-critical driving scenarios across diverse maps and regions, enabling rigorous safety evaluation for autonomous systems. Our project page is this https URL."
    },
    {
      "title": "Learning Physics-Grounded 4D Dynamics with Neural Gaussian Force Fields",
      "url": "https://arxiv.org/abs/2602.00148",
      "date": "2026-01-29",
      "authors_text": "Shiqian Li, Ruihong Shen, Junfeng Ni, Chang Pan, Chi Zhang",
      "is_highlight": false,
      "score": 82.0,
      "summary": "The Neural Gaussian Force Field (NGFF) framework enables rapid generation of interactive, physically realistic 4D videos from multi-view RGB inputs, enhancing robustness and generalization in physical dynamics.",
      "teaser_image": "https://arxiv.org/html/2602.00148/x1.png",
      "debug_abstract": "Predicting physical dynamics from raw visual data remains a major challenge in AI. While recent video generation models have achieved impressive visual quality, they still cannot consistently generate physically plausible videos due to a lack of modeling of physical laws. Recent approaches combining 3D Gaussian splatting and physics engines can produce physically plausible videos, but are hindered by high computational costs in both reconstruction and simulation, and often lack robustness in complex real-world scenarios. To address these issues, we introduce Neural Gaussian Force Field (NGFF), an end-to-end neural framework that integrates 3D Gaussian perception with physics-based dynamic modeling to generate interactive, physically realistic 4D videos from multi-view RGB inputs, achieving two orders of magnitude faster than prior Gaussian simulators. To support training, we also present GSCollision, a 4D Gaussian dataset featuring diverse materials, multi-object interactions, and complex scenes, totaling over 640k rendered physical videos (~4 TB). Evaluations on synthetic and real 3D scenarios show NGFF&#39;s strong generalization and robustness in physical reasoning, advancing video prediction towards physics-grounded world models."
    },
    {
      "title": "How Far Are LLMs from Professional Poker Players? Revisiting Game-Theoretic Reasoning with Agentic Tool Use",
      "url": "https://arxiv.org/abs/2602.00528",
      "date": "2026-01-31",
      "authors_text": "Minhua Lin, Enyan Dai, Hui Liu, Xianfeng Tang, Yuliang Yan",
      "is_highlight": false,
      "score": 55.0,
      "summary": "This paper evaluates LLMs in poker, revealing their strategic reasoning flaws and proposing ToolPoker, which enhances gameplay and reasoning through integrated external solvers.",
      "teaser_image": "https://arxiv.org/html/2602.00528/x2.png",
      "debug_abstract": "As Large Language Models (LLMs) are increasingly applied in high-stakes domains, their ability to reason strategically under uncertainty becomes critical. Poker provides a rigorous testbed, requiring not only strong actions but also principled, game-theoretic reasoning. In this paper, we conduct a systematic study of LLMs in multiple realistic poker tasks, evaluating both gameplay outcomes and reasoning traces. Our analysis reveals LLMs fail to compete against traditional algorithms and identifies three recurring flaws: reliance on heuristics, factual misunderstandings, and a &#34;knowing-doing&#34; gap where actions diverge from reasoning. An initial attempt with behavior cloning and step-level reinforcement learning improves reasoning style but remains insufficient for accurate game-theoretic play. Motivated by these limitations, we propose ToolPoker, a tool-integrated reasoning framework that combines external solvers for GTO-consistent actions with more precise professional-style explanations. Experiments demonstrate that ToolPoker achieves state-of-the-art gameplay while producing reasoning traces that closely reflect game-theoretic principles."
    },
    {
      "title": "V2X-DSC: Multi-Agent Collaborative Perception with Distributed Source Coding Guided Communication",
      "url": "https://arxiv.org/abs/2602.00687",
      "date": "2026-01-31",
      "authors_text": "Yuankun Zeng, Shaohui Li, Zhi Li, Shulan Ruan, Yu Liu",
      "is_highlight": false,
      "score": 76.0,
      "summary": "V2X-DSC introduces a Conditional Codec for efficient multi-agent collaborative perception by compressing and reconstructing BEV features, optimizing bandwidth usage while maintaining accuracy.",
      "teaser_image": "https://arxiv.org/html/2602.00687/figs/overview.jpg",
      "debug_abstract": "Collaborative perception improves 3D understanding by fusing multi-agent observations, yet intermediate-feature sharing faces strict bandwidth constraints as dense BEV features saturate V2X links. We observe that collaborators view the same physical world, making their features strongly correlated; thus receivers only need innovation beyond their local context. Revisiting this from a distributed source coding perspective, we propose V2X-DSC, a framework with a Conditional Codec (DCC) for bandwidth-constrained fusion. The sender compresses BEV features into compact codes, while the receiver performs conditional reconstruction using its local features as side information, allocating bits to complementary cues rather than redundant content. This conditional structure regularizes learning, encouraging incremental representation and yielding lower-noise features. Experiments on DAIR-V2X, OPV2V, and V2X-Real demonstrate state-of-the-art accuracy-bandwidth trade-offs under KB-level communication, and generalizes as a plug-and-play communication layer across multiple fusion backbones."
    },
    {
      "title": "Resource-Efficient Reinforcement for Reasoning Large Language Models via Dynamic One-Shot Policy Refinement",
      "url": "https://arxiv.org/abs/2602.00815",
      "date": "2026-01-31",
      "authors_text": "Yunjian Zhang, Sudong Wang, Yang Li, Peiran Xu, Conghao Zhou",
      "is_highlight": false,
      "score": 57.0,
      "summary": "The paper introduces Dynamic One-Shot Policy Refinement (DoPR), a resource-efficient reinforcement learning strategy that enhances reasoning in large language models while significantly reducing training costs.",
      "teaser_image": "https://arxiv.org/html/2602.00815/dopr.png",
      "debug_abstract": "Large language models (LLMs) have exhibited remarkable performance on complex reasoning tasks, with reinforcement learning under verifiable rewards (RLVR) emerging as a principled framework for aligning model behavior with reasoning chains. Despite its promise, RLVR remains prohibitively resource-intensive, requiring extensive reward signals and incurring substantial rollout costs during training. In this work, we revisit the fundamental question of data and compute efficiency in RLVR. We first establish a theoretical lower bound on the sample complexity required to unlock reasoning capabilities, and empirically validate that strong performance can be achieved with a surprisingly small number of training instances. To tackle the computational burden, we propose Dynamic One-Shot Policy Refinement (DoPR), an uncertainty-aware RL strategy that dynamically selects a single informative training sample per batch for policy updates, guided by reward volatility and exploration-driven acquisition. DoPR reduces rollout overhead by nearly an order of magnitude while preserving competitive reasoning accuracy, offering a scalable and resource-efficient solution for LLM post-training. This approach offers a practical path toward more efficient and accessible RL-based training for reasoning-intensive LLM applications."
    },
    {
      "title": "Making Avatars Interact: Towards Text-Driven Human-Object Interaction for Controllable Talking Avatars",
      "url": "https://arxiv.org/abs/2602.01538",
      "date": "2026-02-02",
      "authors_text": "Youliang Zhang, Zhengguang Zhou, Zhentao Yu, Ziyao Huang, Teng Hu",
      "is_highlight": false,
      "score": 46.0,
      "summary": "The paper presents InteractAvatar, a dual-stream framework for generating talking avatars that perform text-driven human-object interactions by decoupling perception and planning from video synthesis.",
      "teaser_image": "https://arxiv.org/html/2602.01538/x1.png",
      "debug_abstract": "Generating talking avatars is a fundamental task in video generation. Although existing methods can generate full-body talking avatars with simple human motion, extending this task to grounded human-object interaction (GHOI) remains an open challenge, requiring the avatar to perform text-aligned interactions with surrounding objects. This challenge stems from the need for environmental perception and the control-quality dilemma in GHOI generation. To address this, we propose a novel dual-stream framework, InteractAvatar, which decouples perception and planning from video synthesis for grounded human-object interaction. Leveraging detection to enhance environmental perception, we introduce a Perception and Interaction Module (PIM) to generate text-aligned interaction motions. Additionally, an Audio-Interaction Aware Generation Module (AIM) is proposed to synthesize vivid talking avatars performing object interactions. With a specially designed motion-to-video aligner, PIM and AIM share a similar network structure and enable parallel co-generation of motions and plausible videos, effectively mitigating the control-quality dilemma. Finally, we establish a benchmark, GroundedInter, for evaluating GHOI video generation. Extensive experiments and comparisons demonstrate the effectiveness of our method in generating grounded human-object interactions for talking avatars. Project page: this https URL"
    },
    {
      "title": "Reasoning with Autoregressive-Diffusion Collaborative Thoughts",
      "url": "https://arxiv.org/abs/2602.01608",
      "date": "2026-02-02",
      "authors_text": "Mu Yuan, Liekang Zeng, Guoliang Xing, Lan Zhang, Yunhao Liu",
      "is_highlight": false,
      "score": 41.0,
      "summary": "The paper presents Collaborative Thoughts, a framework integrating autoregressive and diffusion models for enhanced spatial reasoning and generation through iterative feedback and joint reasoning.",
      "teaser_image": "https://arxiv.org/html/2602.01608/x2.png",
      "debug_abstract": "Autoregressive and diffusion models represent two complementary generative paradigms. Autoregressive models excel at sequential planning and constraint composition, yet struggle with tasks that require explicit spatial or physical grounding. Diffusion models, in contrast, capture rich spatial structure through high-dimensional generation, but lack the stepwise logical control needed to satisfy complex, multi-stage constraints or to reliably identify and correct errors. We introduce Collaborative Thoughts, a unified collaborative framework that enables autoregressive and diffusion models to reason and generate jointly through a closed-loop interaction. In Collaborative Thoughts, autoregressive models perform structured planning and constraint management, diffusion models instantiate these constraints as intermediate visual thoughts, and a vision-based critic module evaluates whether the visual thoughts satisfy the intended structural and physical requirements. This feedback is then used to iteratively refine subsequent planning and generation steps, mitigating error propagation across modalities. Importantly, Collaborative Thoughts uses the same collaborative loop regardless of whether the task is autoregressive question answering or diffusion-based visual generation. Through representative examples, we illustrate how Collaborative Thoughts can improve the reliability of spatial reasoning and the controllability of generation."
    },
    {
      "title": "From Frames to Sequences: Temporally Consistent Human-Centric Dense Prediction",
      "url": "https://arxiv.org/abs/2602.01661",
      "date": "2026-02-02",
      "authors_text": "Xingyu Miao, Junting Dong, Qin Zhao, Yuhang Yang, Junhao Chen",
      "is_highlight": false,
      "score": 43.0,
      "summary": "This paper presents a novel synthetic data pipeline and a ViT-based model for achieving temporally consistent human-centric dense prediction in video sequences, outperforming existing methods.",
      "teaser_image": "https://arxiv.org/html/2602.01661/x2.png",
      "debug_abstract": "In this work, we focus on the challenge of temporally consistent human-centric dense prediction across video sequences. Existing models achieve strong per-frame accuracy but often flicker under motion, occlusion, and lighting changes, and they rarely have paired human video supervision for multiple dense tasks. We address this gap with a scalable synthetic data pipeline that generates photorealistic human frames and motion-aligned sequences with pixel-accurate depth, normals, and masks. Unlike prior static data synthetic pipelines, our pipeline provides both frame-level labels for spatial learning and sequence-level supervision for temporal learning. Building on this, we train a unified ViT-based dense predictor that (i) injects an explicit human geometric prior via CSE embeddings and (ii) improves geometry-feature reliability with a lightweight channel reweighting module after feature fusion. Our two-stage training strategy, combining static pretraining with dynamic sequence supervision, enables the model first to acquire robust spatial representations and then refine temporal consistency across motion-aligned sequences. Extensive experiments show that we achieve state-of-the-art performance on THuman2.1 and Hi4D and generalize effectively to in-the-wild videos."
    },
    {
      "title": "ForSim: Stepwise Forward Simulation for Traffic Policy Fine-Tuning",
      "url": "https://arxiv.org/abs/2602.01916",
      "date": "2026-02-02",
      "authors_text": "Keyu Chen, Wenchao Sun, Hao Cheng, Zheng Fu, Sifa Zheng",
      "is_highlight": true,
      "score": 29.0,
      "summary": "ForSim introduces a stepwise closed-loop simulation method that enhances traffic policy fine-tuning by improving agent interactions and preserving multimodal behaviors in autonomous driving.",
      "teaser_image": "https://arxiv.org/html/2602.01916/x2.png",
      "debug_abstract": "As the foundation of closed-loop training and evaluation in autonomous driving, traffic simulation still faces two fundamental challenges: covariate shift introduced by open-loop imitation learning and limited capacity to reflect the multimodal behaviors observed in real-world traffic. Although recent frameworks such as RIFT have partially addressed these issues through group-relative optimization, their forward simulation procedures remain largely non-reactive, leading to unrealistic agent interactions within the virtual domain and ultimately limiting simulation fidelity. To address these issues, we propose ForSim, a stepwise closed-loop forward simulation paradigm. At each virtual timestep, the traffic agent propagates the virtual candidate trajectory that best spatiotemporally matches the reference trajectory through physically grounded motion dynamics, thereby preserving multimodal behavioral diversity while ensuring intra-modality consistency. Other agents are updated with stepwise predictions, yielding coherent and interaction-aware evolution. When incorporated into the RIFT traffic simulation framework, ForSim operates in conjunction with group-relative optimization to fine-tune traffic policy. Extensive experiments confirm that this integration consistently improves safety while maintaining efficiency, realism, and comfort. These results underscore the importance of modeling closed-loop multimodal interactions within forward simulation and enhance the fidelity and reliability of traffic simulation for autonomous driving. Project Page: this https URL"
    },
    {
      "title": "Small Generalizable Prompt Predictive Models Can Steer Efficient RL Post-Training of Large Reasoning Models",
      "url": "https://arxiv.org/abs/2602.01970",
      "date": "2026-02-02",
      "authors_text": "Yun Qu, Qi Wang, Yixiu Mao, Heming Zou, Yuhang Jiang",
      "is_highlight": false,
      "score": 26.0,
      "summary": "The paper presents Generalizable Predictive Prompt Selection (GPS), a method that enhances reinforcement learning efficiency in large language models by utilizing a lightweight generative model for prompt selection.",
      "teaser_image": "https://arxiv.org/html/2602.01970/x1.png",
      "debug_abstract": "Reinforcement learning enhances the reasoning capabilities of large language models but often involves high computational costs due to rollout-intensive optimization. Online prompt selection presents a plausible solution by prioritizing informative prompts to improve training efficiency. However, current methods either depend on costly, exact evaluations or construct prompt-specific predictive models lacking generalization across prompts. This study introduces Generalizable Predictive Prompt Selection (GPS), which performs Bayesian inference towards prompt difficulty using a lightweight generative model trained on the shared optimization history. Intermediate-difficulty prioritization and history-anchored diversity are incorporated into the batch acquisition principle to select informative prompt batches. The small predictive model also generalizes at test-time for efficient computational allocation. Experiments across varied reasoning benchmarks indicate GPS&#39;s substantial improvements in training efficiency, final performance, and test-time efficiency over superior baseline methods."
    },
    {
      "title": "TTT-Parkour: Rapid Test-Time Training for Perceptive Robot Parkour",
      "url": "https://arxiv.org/abs/2602.02331",
      "date": "2026-02-02",
      "authors_text": "Shaoting Zhu, Baijun Ye, Jiaxuan Wang, Jiakang Chen, Ziwen Zhuang",
      "is_highlight": false,
      "score": 18.0,
      "summary": "TTT-Parkour introduces a rapid test-time training framework enabling humanoid robots to effectively navigate complex terrains through efficient real-to-sim-to-real learning and geometry reconstruction.",
      "teaser_image": "https://arxiv.org/html/2602.02331/x1.png",
      "debug_abstract": "Achieving highly dynamic humanoid parkour on unseen, complex terrains remains a challenge in robotics. Although general locomotion policies demonstrate capabilities across broad terrain distributions, they often struggle with arbitrary and highly challenging environments. To overcome this limitation, we propose a real-to-sim-to-real framework that leverages rapid test-time training (TTT) on novel terrains, significantly enhancing the robot&#39;s capability to traverse extremely difficult geometries. We adopt a two-stage end-to-end learning paradigm: a policy is first pre-trained on diverse procedurally generated terrains, followed by rapid fine-tuning on high-fidelity meshes reconstructed from real-world captures. Specifically, we develop a feed-forward, efficient, and high-fidelity geometry reconstruction pipeline using RGB-D inputs, ensuring both speed and quality during test-time training. We demonstrate that TTT-Parkour empowers humanoid robots to master complex obstacles, including wedges, stakes, boxes, trapezoids, and narrow beams. The whole pipeline of capturing, reconstructing, and test-time training requires less than 10 minutes on most tested terrains. Extensive experiments show that the policy after test-time training exhibits robust zero-shot sim-to-real transfer capability."
    },
    {
      "title": "Context Learning for Multi-Agent Discussion",
      "url": "https://arxiv.org/abs/2602.02350",
      "date": "2026-02-02",
      "authors_text": "Xingyuan Hua, Sheng Yue, Xinyi Li, Yizhe Zhao, Jinrui Zhang",
      "is_highlight": false,
      "score": 17.0,
      "summary": "The paper presents M2CL, a multi-LLM context learning method that enhances coherence in Multi-Agent Discussions, improving problem-solving performance by 20%-50% over existing approaches.",
      "teaser_image": "https://arxiv.org/html/2602.02350/x2.png",
      "debug_abstract": "Multi-Agent Discussion (MAD) has garnered increasing attention very recently, where multiple LLM instances collaboratively solve problems via structured discussion. However, we find that current MAD methods easily suffer from discussion inconsistency, LLMs fail to reach a coherent solution, due to the misalignment between their individual this http URL this paper, we introduce a multi-LLM context learning method (M2CL) that learns a context generator for each agent, capable of dynamically generating context instructions per discussion round via automatic information organization and refinement. Specifically, inspired by our theoretical insights on the context instruction, M2CL train the generators to control context coherence and output discrepancies via a carefully crafted self-adaptive this http URL enables LLMs to avoid premature convergence on majority noise and progressively reach the correct consensus. We evaluate M2CL on challenging tasks, including academic reasoning, embodied tasks, and mobile control. The results show that the performance of M2CL significantly surpasses existing methods by 20%--50%, while enjoying favorable transferability and computational efficiency."
    },
    {
      "title": "UV-M3TL: A Unified and Versatile Multimodal Multi-Task Learning Framework for Assistive Driving Perception",
      "url": "https://arxiv.org/abs/2602.01594",
      "date": "2026-02-02",
      "authors_text": "Wenzhuo Liu, Qiannan Guo, Zhen Wang, Wenshuo Wang, Lei Yang",
      "is_highlight": false,
      "score": 8.0,
      "summary": "The UV-M3TL framework enhances ADAS by effectively recognizing driver behavior and context through dual-branch multimodal embedding and adaptive loss, achieving state-of-the-art performance across multiple tasks.",
      "teaser_image": "https://arxiv.org/html/2602.01594/x2.png",
      "debug_abstract": "Advanced Driver Assistance Systems (ADAS) need to understand human driver behavior while perceiving their navigation context, but jointly learning these heterogeneous tasks would cause inter-task negative transfer and impair system performance. Here, we propose a Unified and Versatile Multimodal Multi-Task Learning (UV-M3TL) framework to simultaneously recognize driver behavior, driver emotion, vehicle behavior, and traffic context, while mitigating inter-task negative transfer. Our framework incorporates two core components: dual-branch spatial channel multimodal embedding (DB-SCME) and adaptive feature-decoupled multi-task loss (AFD-Loss). DB-SCME enhances cross-task knowledge transfer while mitigating task conflicts by employing a dual-branch structure to explicitly model salient task-shared and task-specific features. AFD-Loss improves the stability of joint optimization while guiding the model to learn diverse multi-task representations by introducing an adaptive weighting mechanism based on learning dynamics and feature decoupling constraints. We evaluate our method on the AIDE dataset, and the experimental results demonstrate that UV-M3TL achieves state-of-the-art performance across all four tasks. To further prove the versatility, we evaluate UV-M3TL on additional public multi-task perception benchmarks (BDD100K, CityScapes, NYUD-v2, and PASCAL-Context), where it consistently delivers strong performance across diverse task combinations, attaining state-of-the-art results on most tasks."
    },
    {
      "title": "Enhancing Indoor Occupancy Prediction via Sparse Query-Based Multi-Level Consistent Knowledge Distillation",
      "url": "https://arxiv.org/abs/2602.02318",
      "date": "2026-02-02",
      "authors_text": "Xiang Li, Yupeng Zheng, Pengfei Li, Yilun Chen, Ya-Qin Zhang",
      "is_highlight": false,
      "score": 0,
      "summary": "DiScene introduces a sparse query-based framework for efficient and robust indoor occupancy prediction through multi-level knowledge distillation, achieving state-of-the-art performance and faster inference.",
      "teaser_image": "https://arxiv.org/html/2602.02318/x2.png",
      "debug_abstract": "Occupancy prediction provides critical geometric and semantic understanding for robotics but faces efficiency-accuracy trade-offs. Current dense methods suffer computational waste on empty voxels, while sparse query-based approaches lack robustness in diverse and complex indoor scenes. In this paper, we propose DiScene, a novel sparse query-based framework that leverages multi-level distillation to achieve efficient and robust occupancy prediction. In particular, our method incorporates two key innovations: (1) a Multi-level Consistent Knowledge Distillation strategy, which transfers hierarchical representations from large teacher models to lightweight students through coordinated alignment across four levels, including encoder-level feature alignment, query-level feature matching, prior-level spatial guidance, and anchor-level high-confidence knowledge transfer and (2) a Teacher-Guided Initialization policy, employing optimized parameter warm-up to accelerate model convergence. Validated on the Occ-Scannet benchmark, DiScene achieves 23.2 FPS without depth priors while outperforming our baseline method, OPUS, by 36.1% and even better than the depth-enhanced version, OPUS†. With depth integration, DiScene† attains new SOTA performance, surpassing EmbodiedOcc by 3.7% with 1.62$\\times$ faster inference speed. Furthermore, experiments on the Occ3D-nuScenes benchmark and in-the-wild scenarios demonstrate the versatility of our approach in various environments. Code and models can be accessed at this https URL."
    },
    {
      "title": "InspecSafe-V1: A Multimodal Benchmark for Safety Assessment in Industrial Inspection Scenarios",
      "url": "https://arxiv.org/abs/2601.21173",
      "date": "2026-01-29",
      "authors_text": "Zeyi Liu, Shuang Liu, Jihai Min, Zhaoheng Zhang, Jun Cen",
      "is_highlight": false,
      "score": 82.0,
      "summary": "InspecSafe-V1 is a multimodal benchmark dataset for safety assessment in industrial inspections, featuring real-world data from diverse scenarios and multiple sensing modalities.",
      "teaser_image": "https://arxiv.org/html/2601.21173/x1.png",
      "debug_abstract": "With the rapid development of industrial intelligence and unmanned inspection, reliable perception and safety assessment for AI systems in complex and dynamic industrial sites has become a key bottleneck for deploying predictive maintenance and autonomous inspection. Most public datasets remain limited by simulated data sources, single-modality sensing, or the absence of fine-grained object-level annotations, which prevents robust scene understanding and multimodal safety reasoning for industrial foundation models. To address these limitations, InspecSafe-V1 is released as the first multimodal benchmark dataset for industrial inspection safety assessment that is collected from routine operations of real inspection robots in real-world environments. InspecSafe-V1 covers five representative industrial scenarios, including tunnels, power facilities, sintering equipment, oil and gas petrochemical plants, and coal conveyor trestles. The dataset is constructed from 41 wheeled and rail-mounted inspection robots operating at 2,239 valid inspection sites, yielding 5,013 inspection instances. For each instance, pixel-level segmentation annotations are provided for key objects in visible-spectrum images. In addition, a semantic scene description and a corresponding safety level label are provided according to practical inspection tasks. Seven synchronized sensing modalities are further included, including infrared video, audio, depth point clouds, radar point clouds, gas measurements, temperature, and humidity, to support multimodal anomaly recognition, cross-modal fusion, and comprehensive safety assessment in industrial environments."
    },
    {
      "title": "Listen, Look, Drive: Coupling Audio Instructions for User-aware VLA-based Autonomous Driving",
      "url": "https://arxiv.org/abs/2601.12142",
      "date": "2026-01-28",
      "authors_text": "Ziang Guo, Feng Yang, Xuefeng Zhang, Jiaqi Guo, Kun Zhao",
      "is_highlight": false,
      "score": 88.0,
      "summary": "EchoVLA enhances autonomous driving by integrating audio instructions with visual inputs, allowing for user influence and emotional context, significantly improving navigation accuracy and safety.",
      "teaser_image": "https://arxiv.org/html/2601.12142/Figs/overview.png",
      "debug_abstract": "Vision Language Action (VLA) models promise an open-vocabulary interface that can translate perceptual ambiguity into semantically grounded driving decisions, yet they still treat language as a static prior fixed at inference time. As a result, the model must infer continuously shifting objectives from pixels alone, yielding delayed or overly conservative maneuvers. We argue that effective VLAs for autonomous driving need an online channel in which users can influence driving with specific intentions. To this end, we present EchoVLA, a user-aware VLA that couples camera streams with in situ audio instructions. We augment the nuScenes dataset with temporally aligned, intent-specific speech commands generated by converting ego-motion descriptions into synthetic audios. Further, we compose emotional speech-trajectory pairs into a multimodal Chain-of-Thought (CoT) for fine-tuning a Multimodal Large Model (MLM) based on Qwen2.5-Omni. Specifically, we synthesize the audio-augmented dataset with different emotion types paired with corresponding driving behaviors, leveraging the emotional cues embedded in tone, pitch, and speech tempo to reflect varying user states, such as urgent or hesitant intentions, thus enabling our EchoVLA to interpret not only the semantic content but also the emotional context of audio commands for more nuanced and emotionally adaptive driving behavior. In open-loop benchmarks, our approach reduces the average L2 error by $59.4\\%$ and the collision rate by $74.4\\%$ compared to the baseline of vision-only perception. More experiments on nuScenes dataset validate that EchoVLA not only steers the trajectory through audio instructions, but also modulates driving behavior in response to the emotions detected in the user&#39;s speech."
    },
    {
      "title": "HINT: Hierarchical Interaction Modeling for Autoregressive Multi-Human Motion Generation",
      "url": "https://arxiv.org/abs/2601.20383",
      "date": "2026-01-28",
      "authors_text": "Mengge Liu, Yan Di, Gu Wang, Yun Qu, Dekai Zhu",
      "is_highlight": false,
      "score": 78.0,
      "summary": "HINT introduces a novel autoregressive framework for multi-human motion generation, enhancing interaction modeling and performance with variable agent counts and long text guidance.",
      "teaser_image": "https://arxiv.org/html/2601.20383/x2.png",
      "debug_abstract": "Text-driven multi-human motion generation with complex interactions remains a challenging problem. Despite progress in performance, existing offline methods that generate fixed-length motions with a fixed number of agents, are inherently limited in handling long or variable text, and varying agent counts. These limitations naturally encourage autoregressive formulations, which predict future motions step by step conditioned on all past trajectories and current text guidance. In this work, we introduce HINT, the first autoregressive framework for multi-human motion generation with Hierarchical INTeraction modeling in diffusion. First, HINT leverages a disentangled motion representation within a canonicalized latent space, decoupling local motion semantics from inter-person interactions. This design facilitates direct adaptation to varying numbers of human participants without requiring additional refinement. Second, HINT adopts a sliding-window strategy for efficient online generation, and aggregates local within-window and global cross-window conditions to capture past human history, inter-person dependencies, and align with text guidance. This strategy not only enables fine-grained interaction modeling within each window but also preserves long-horizon coherence across all the long sequence. Extensive experiments on public benchmarks demonstrate that HINT matches the performance of strong offline models and surpasses autoregressive baselines. Notably, on InterHuman, HINT achieves an FID of 3.100, significantly improving over the previous state-of-the-art score of 5.154."
    },
    {
      "title": "Self-Supervised Path Planning in Unstructured Environments via Global-Guided Differentiable Hard Constraint Projection",
      "url": "https://arxiv.org/abs/2601.19354",
      "date": "2026-01-27",
      "authors_text": "Ziqian Wang, Chenxi Fang, Zhen Zhang",
      "is_highlight": false,
      "score": 90.0,
      "summary": "This paper presents a self-supervised framework for autonomous navigation in unstructured environments, utilizing a differentiable projection layer to ensure safety and efficiency in path planning.",
      "debug_abstract": "Deploying deep learning agents for autonomous navigation in unstructured environments faces critical challenges regarding safety, data scarcity, and limited computational resources. Traditional solvers often suffer from high latency, while emerging learning-based approaches struggle to ensure deterministic feasibility. To bridge the gap from embodied to embedded intelligence, we propose a self-supervised framework incorporating a differentiable hard constraint projection layer for runtime assurance. To mitigate data scarcity, we construct a Global-Guided Artificial Potential Field (G-APF), which provides dense supervision signals without manual labeling. To enforce actuator limitations and geometric constraints efficiently, we employ an adaptive neural projection layer, which iteratively rectifies the coarse network output onto the feasible manifold. Extensive benchmarks on a test set of 20,000 scenarios demonstrate an 88.75\\% success rate, substantiating the enhanced operational safety. Closed-loop experiments in CARLA further validate the physical realizability of the planned paths under dynamic constraints. Furthermore, deployment verification on an NVIDIA Jetson Orin NX confirms an inference latency of 94 ms, showing real-time feasibility on resource-constrained embedded hardware. This framework offers a generalized paradigm for embedding physical laws into neural architectures, providing a viable direction for solving constrained optimization in mechatronics. Source code is available at: this https URL."
    },
    {
      "title": "AtomMem : Learnable Dynamic Agentic Memory with Atomic Memory Operation",
      "url": "https://arxiv.org/abs/2601.08323",
      "date": "2026-01-27",
      "authors_text": "Yupeng Huo, Yaxi Lu, Zhong Zhang, Haotian Chen, Yankai Lin",
      "is_highlight": false,
      "score": 82.0,
      "summary": "AtomMem introduces a learnable dynamic memory framework that optimizes memory management through atomic CRUD operations, significantly outperforming static methods in long-horizon tasks.",
      "debug_abstract": "Equipping agents with memory is essential for solving real-world long-horizon problems. However, most existing agent memory mechanisms rely on static and hand-crafted workflows. This limits the performance and generalization ability of these memory designs, which highlights the need for a more flexible, learning-based memory framework. In this paper, we propose AtomMem, which reframes memory management as a dynamic decision-making problem. We deconstruct high-level memory processes into fundamental atomic CRUD (Create, Read, Update, Delete) operations, transforming the memory workflow into a learnable decision process. By combining supervised fine-tuning with reinforcement learning, AtomMem learns an autonomous, task-aligned policy to orchestrate memory behaviors tailored to specific task demands. Experimental results across 3 long-context benchmarks demonstrate that the trained AtomMem-8B consistently outperforms prior static-workflow memory methods. Further analysis of training dynamics shows that our learning-based formulation enables the agent to discover structured, task-aligned memory management strategies, highlighting a key advantage over predefined routines."
    },
    {
      "title": "Visual Generation Unlocks Human-Like Reasoning through Multimodal World Models",
      "url": "https://arxiv.org/abs/2601.19834",
      "date": "2026-01-27",
      "authors_text": "Jialong Wu, Xiaoying Zhang, Hongyi Yuan, Xiangcheng Zhang, Tianhao Huang",
      "is_highlight": false,
      "score": 89.0,
      "summary": "This paper demonstrates that visual generation enhances reasoning in multimodal AI, particularly for tasks requiring physical world understanding, outperforming purely verbal approaches.",
      "debug_abstract": "Humans construct internal world models and reason by manipulating the concepts within these models. Recent advances in AI, particularly chain-of-thought (CoT) reasoning, approximate such human cognitive abilities, where world models are believed to be embedded within large language models. Expert-level performance in formal and abstract domains such as mathematics and programming has been achieved in current systems by relying predominantly on verbal reasoning. However, they still lag far behind humans in domains like physical and spatial intelligence, which require richer representations and prior knowledge. The emergence of unified multimodal models (UMMs) capable of both verbal and visual generation has therefore sparked interest in more human-like reasoning grounded in complementary multimodal pathways, though their benefits remain unclear. From a world-model perspective, this paper presents the first principled study of when and how visual generation benefits reasoning. Our key position is the visual superiority hypothesis: for certain tasks--particularly those grounded in the physical world--visual generation more naturally serves as world models, whereas purely verbal world models encounter bottlenecks arising from representational limitations or insufficient prior knowledge. Theoretically, we formalize internal world modeling as a core component of CoT reasoning and analyze distinctions among different forms of world models. Empirically, we identify tasks that necessitate interleaved visual-verbal CoT reasoning, constructing a new evaluation suite, VisWorld-Eval. Controlled experiments on a state-of-the-art UMM show that interleaved CoT significantly outperforms purely verbal CoT on tasks that favor visual world modeling, but offers no clear advantage otherwise. Together, this work clarifies the potential of multimodal world modeling for more powerful, human-like multimodal AI."
    },
    {
      "title": "Scalable Exploration for High-Dimensional Continuous Control via Value-Guided Flow",
      "url": "https://arxiv.org/abs/2601.19707",
      "date": "2026-01-27",
      "authors_text": "Yunyue Wei, Chenhui Zuo, Yanan Sui",
      "is_highlight": false,
      "score": 79.0,
      "summary": "Qflex introduces a scalable reinforcement learning method for high-dimensional continuous control that enhances exploration efficiency by aligning actions with learned value gradients.",
      "debug_abstract": "Controlling high-dimensional systems in biological and robotic applications is challenging due to expansive state-action spaces, where effective exploration is critical. Commonly used exploration strategies in reinforcement learning are largely undirected with sharp degradation as action dimensionality grows. Many existing methods resort to dimensionality reduction, which constrains policy expressiveness and forfeits system flexibility. We introduce Q-guided Flow Exploration (Qflex), a scalable reinforcement learning method that conducts exploration directly in the native high-dimensional action space. During training, Qflex traverses actions from a learnable source distribution along a probability flow induced by the learned value function, aligning exploration with task-relevant gradients rather than isotropic noise. Our proposed method substantially outperforms representative online reinforcement learning baselines across diverse high-dimensional continuous-control benchmarks. Qflex also successfully controls a full-body human musculoskeletal model to perform agile, complex movements, demonstrating superior scalability and sample efficiency in very high-dimensional settings. Our results indicate that value-guided flows offer a principled and practical route to exploration at scale."
    },
    {
      "title": "Latent-Space Contrastive Reinforcement Learning for Stable and Efficient LLM Reasoning",
      "url": "https://arxiv.org/abs/2601.17275",
      "date": "2026-01-24",
      "authors_text": "Lianlei Shan, Han Chen, Yixuan Wang, Zhenjie Liu, Wei Li",
      "is_highlight": false,
      "score": 60.0,
      "summary": "The paper introduces DeepLatent Reasoning (DLR), a latent-space contrastive reinforcement learning framework that enhances LLM reasoning by addressing inefficiencies and stability in multi-step tasks.",
      "debug_abstract": "While Large Language Models (LLMs) demonstrate exceptional performance in surface-level text generation, their nature in handling complex multi-step reasoning tasks often remains one of ``statistical fitting&#39;&#39; rather than systematic logical deduction. Traditional Reinforcement Learning (RL) attempts to mitigate this by introducing a ``think-before-speak&#39;&#39; paradigm. However, applying RL directly in high-dimensional, discrete token spaces faces three inherent challenges: sample-inefficient rollouts, high gradient estimation variance, and the risk of catastrophic forgetting. To fundamentally address these structural bottlenecks, we propose \\textbf{DeepLatent Reasoning (DLR)}, a latent-space bidirectional contrastive reinforcement learning framework. This framework shifts the trial-and-error cost from expensive token-level full sequence generation to the continuous latent manifold. Specifically, we introduce a lightweight assistant model to efficiently sample $K$ reasoning chain encodings within the latent space. These encodings are filtered via a dual reward mechanism based on correctness and formatting; only high-value latent trajectories are fed into a \\textbf{frozen main model} for single-pass decoding. To maximize reasoning diversity while maintaining coherence, we design a contrastive learning objective to enable directed exploration within the latent space. Since the main model parameters remain frozen during optimization, this method mathematically eliminates catastrophic forgetting. Experiments demonstrate that under comparable GPU computational budgets, DLR achieves more stable training convergence, supports longer-horizon reasoning chains, and facilitates the sustainable accumulation of reasoning capabilities, providing a viable path toward reliable and scalable reinforcement learning for LLMs."
    },
    {
      "title": "MetaWorld: Skill Transfer and Composition in a Hierarchical World Model for Grounding High-Level Instructions",
      "url": "https://arxiv.org/abs/2601.17507",
      "date": "2026-01-24",
      "authors_text": "Yutong Shen, Hangxu Liu, Kailin Pei, Ruizhe Xia, Tongtong Feng",
      "is_highlight": false,
      "score": 92.0,
      "summary": "MetaWorld introduces a hierarchical world model that enhances humanoid robot loco-manipulation by integrating semantic planning and physical control through expert policy transfer, improving task performance and motion coherence.",
      "debug_abstract": "Humanoid robot loco-manipulation remains constrained by the semantic-physical gap. Current methods face three limitations: Low sample efficiency in reinforcement learning, poor generalization in imitation learning, and physical inconsistency in VLMs. We propose MetaWorld, a hierarchical world model that integrates semantic planning and physical control via expert policy transfer. The framework decouples tasks into a VLM-driven semantic layer and a latent dynamics model operating in a compact state space. Our dynamic expert selection and motion prior fusion mechanism leverages a pre-trained multi-expert policy library as transferable knowledge, enabling efficient online adaptation via a two-stage framework. VLMs serve as semantic interfaces, mapping instructions to executable skills and bypassing symbol grounding. Experiments on Humanoid-Bench show MetaWorld outperforms world model-based RL in task completion and motion coherence. Our code will be found at this https URL"
    },
    {
      "title": "The Flexibility Trap: Why Arbitrary Order Limits Reasoning Potential in Diffusion Language Models",
      "url": "https://arxiv.org/abs/2601.15165",
      "date": "2026-01-26",
      "authors_text": "Zanlin Ni, Shenzhi Wang, Yang Yue, Tianyu Yu, Weilin Zhao",
      "is_highlight": false,
      "score": 45.0,
      "summary": "This paper argues that arbitrary order generation in diffusion language models limits reasoning potential, advocating for a simpler approach using Group Relative Policy Optimization for improved performance.",
      "debug_abstract": "Diffusion Large Language Models (dLLMs) break the rigid left-to-right constraint of traditional LLMs, enabling token generation in arbitrary orders. Intuitively, this flexibility implies a solution space that strictly supersets the fixed autoregressive trajectory, theoretically unlocking superior reasoning potential for general tasks like mathematics and coding. Consequently, numerous works have leveraged reinforcement learning (RL) to elicit the reasoning capability of dLLMs. In this paper, we reveal a counter-intuitive reality: arbitrary order generation, in its current form, narrows rather than expands the reasoning boundary of dLLMs. We find that dLLMs tend to exploit this order flexibility to bypass high-uncertainty tokens that are crucial for exploration, leading to a premature collapse of the solution space. This observation motivates a rethink of RL approaches for dLLMs, where considerable complexities, such as handling combinatorial trajectories and intractable likelihoods, are often devoted to preserving this flexibility. We demonstrate that effective reasoning can be better elicited by intentionally forgoing arbitrary order and applying standard Group Relative Policy Optimization (GRPO) instead. Our approach, JustGRPO, is minimalist yet surprisingly effective (e.g., 89.1% accuracy on GSM8K) while fully retaining the parallel decoding ability of dLLMs. Project page: this https URL"
    },
    {
      "title": "HomoFM: Deep Homography Estimation with Flow Matching",
      "url": "https://arxiv.org/abs/2601.18222",
      "date": "2026-01-26",
      "authors_text": "Mengfan He, Liangzheng Sun, Chunyu Li, Ziyang Meng",
      "is_highlight": false,
      "score": 49.0,
      "summary": "HomoFM introduces a flow matching technique for deep homography estimation, enhancing accuracy and robustness against domain shifts through velocity field learning and domain-invariant representations.",
      "debug_abstract": "Deep homography estimation has broad applications in computer vision and robotics. Remarkable progresses have been achieved while the existing methods typically treat it as a direct regression or iterative refinement problem and often struggling to capture complex geometric transformations or generalize across different domains. In this work, we propose HomoFM, a new framework that introduces the flow matching technique from generative modeling into the homography estimation task for the first time. Unlike the existing methods, we formulate homography estimation problem as a velocity field learning problem. By modeling a continuous and point-wise velocity field that transforms noisy distributions into registered coordinates, the proposed network recovers high-precision transformations through a conditional flow trajectory. Furthermore, to address the challenge of domain shifts issue, e.g., the cases of multimodal matching or varying illumination scenarios, we integrate a gradient reversal layer (GRL) into the feature extraction backbone. This domain adaptation strategy explicitly constrains the encoder to learn domain-invariant representations, significantly enhancing the network&#39;s robustness. Extensive experiments demonstrate the effectiveness of the proposed method, showing that HomoFM outperforms state-of-the-art methods in both estimation accuracy and robustness on standard benchmarks. Code and data resource are available at this https URL."
    },
    {
      "title": "Dep-Search: Learning Dependency-Aware Reasoning Traces with Persistent Memory",
      "url": "https://arxiv.org/abs/2601.18771",
      "date": "2026-01-26",
      "authors_text": "Yanming Liu, Xinyue Peng, Zixuan Yan, Yanxin Shen, Wenjie Xu",
      "is_highlight": false,
      "score": 46.0,
      "summary": "Dep-Search enhances large language models' multi-hop reasoning by integrating dependency-aware search, structured retrieval, and persistent memory, outperforming existing frameworks in complex tasks.",
      "debug_abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in complex reasoning tasks, particularly when augmented with search mechanisms that enable systematic exploration of external knowledge bases. The field has evolved from traditional retrieval-augmented generation (RAG) frameworks to more sophisticated search-based frameworks that orchestrate multi-step reasoning through explicit search strategies. However, existing search frameworks still rely heavily on implicit natural language reasoning to determine search strategies and how to leverage retrieved information across reasoning steps. This reliance on implicit reasoning creates fundamental challenges for managing dependencies between sub-questions, efficiently reusing previously retrieved knowledge, and learning optimal search strategies through reinforcement learning. To address these limitations, we propose Dep-Search, a dependency-aware search framework that advances beyond existing search frameworks by integrating structured reasoning, retrieval, and persistent memory through GRPO. Dep-Search introduces explicit control mechanisms that enable the model to decompose questions with dependency relationships, retrieve information when needed, access previously stored knowledge from memory, and summarize long reasoning contexts into reusable memory entries. Through extensive experiments on seven diverse question answering datasets, we demonstrate that Dep-Search significantly enhances LLMs&#39; ability to tackle complex multi-hop reasoning tasks, achieving substantial improvements over strong baselines across different model scales."
    },
    {
      "title": "FantasyVLN: Unified Multimodal Chain-of-Thought Reasoning for Vision-Language Navigation",
      "url": "https://arxiv.org/abs/2601.13976",
      "date": "2026-01-23",
      "authors_text": "Jing Zuo, Lingzhou Mu, Fan Jiang, Chengcheng Ma, Mu Xu",
      "is_highlight": false,
      "score": 85.0,
      "summary": "FantasyVLN introduces a unified implicit reasoning framework for Vision-Language Navigation that enhances efficiency and success rates by optimizing Chain-of-Thought reasoning without excessive token overhead.",
      "debug_abstract": "Achieving human-level performance in Vision-and-Language Navigation (VLN) requires an embodied agent to jointly understand multimodal instructions and visual-spatial context while reasoning over long action sequences. Recent works, such as NavCoT and NavGPT-2, demonstrate the potential of Chain-of-Thought (CoT) reasoning for improving interpretability and long-horizon planning. Moreover, multimodal extensions like OctoNav-R1 and CoT-VLA further validate CoT as a promising pathway toward human-like navigation reasoning. However, existing approaches face critical drawbacks: purely textual CoTs lack spatial grounding and easily overfit to sparse annotated reasoning steps, while multimodal CoTs incur severe token inflation by generating imagined visual observations, making real-time navigation impractical. In this work, we propose FantasyVLN, a unified implicit reasoning framework that preserves the benefits of CoT reasoning without explicit token overhead. Specifically, imagined visual tokens are encoded into a compact latent space using a pretrained Visual AutoRegressor (VAR) during CoT reasoning training, and the model jointly learns from textual, visual, and multimodal CoT modes under a unified multi-CoT strategy. At inference, our model performs direct instruction-to-action mapping while still enjoying reasoning-aware representations. Extensive experiments on LH-VLN show that our approach achieves reasoning-aware yet real-time navigation, improving success rates and efficiency while reducing inference latency by an order of magnitude compared to explicit CoT methods."
    },
    {
      "title": "TangramPuzzle: Evaluating Multimodal Large Language Models with Compositional Spatial Reasoning",
      "url": "https://arxiv.org/abs/2601.16520",
      "date": "2026-01-23",
      "authors_text": "Daixian Liu, Jiayi Kuang, Yinghui Li, Yangning Li, Di Yin",
      "is_highlight": false,
      "score": 82.0,
      "summary": "The TangramPuzzle benchmark evaluates multimodal large language models' compositional spatial reasoning through precise geometric tasks, revealing their tendency to prioritize silhouette matching over geometric accuracy.",
      "debug_abstract": "Multimodal Large Language Models (MLLMs) have achieved remarkable progress in visual recognition and semantic understanding. Nevertheless, their ability to perform precise compositional spatial reasoning remains largely unexplored. Existing benchmarks often involve relatively simple tasks and rely on semantic approximations or coarse relative positioning, while their evaluation metrics are typically limited and lack rigorous mathematical formulations. To bridge this gap, we introduce TangramPuzzle, a geometry-grounded benchmark designed to evaluate compositional spatial reasoning through the lens of the classic Tangram game. We propose the Tangram Construction Expression (TCE), a symbolic geometric framework that grounds tangram assemblies in exact, machine-verifiable coordinate specifications, to mitigate the ambiguity of visual approximation. We design two complementary tasks: Outline Prediction, which demands inferring global shapes from local components, and End-to-End Code Generation, which requires solving inverse geometric assembly problems. We conduct extensive evaluation experiments on advanced open-source and proprietary models, revealing an interesting insight: MLLMs tend to prioritize matching the target silhouette while neglecting geometric constraints, leading to distortions or deformations of the pieces."
    },
    {
      "title": "AnchoredDream: Zero-Shot 360° Indoor Scene Generation from a Single View via Geometric Grounding",
      "url": "https://arxiv.org/abs/2601.16532",
      "date": "2026-01-23",
      "authors_text": "Runmao Yao, Junsheng Zhou, Zhen Dong, Yu-Shen Liu",
      "is_highlight": false,
      "score": 72.0,
      "summary": "AnchoredDream introduces a zero-shot method for generating 360° indoor scenes from a single image, enhancing appearance consistency and geometric plausibility through a mutual boosting mechanism.",
      "debug_abstract": "Single-view indoor scene generation plays a crucial role in a range of real-world applications. However, generating a complete 360° scene from a single image remains a highly ill-posed and challenging problem. Recent approaches have made progress by leveraging diffusion models and depth estimation networks, yet they still struggle to maintain appearance consistency and geometric plausibility under large viewpoint changes, limiting their effectiveness in full-scene generation. To address this, we propose AnchoredDream, a novel zero-shot pipeline that anchors 360° scene generation on high-fidelity geometry via an appearance-geometry mutual boosting mechanism. Given a single-view image, our method first performs appearance-guided geometry generation to construct a reliable 3D scene layout. Then, we progressively generate the complete scene through a series of modules: warp-and-inpaint, warp-and-refine, post-optimization, and a novel Grouting Block, which ensures seamless transitions between the input view and generated regions. Extensive experiments demonstrate that AnchoredDream outperforms existing methods by a large margin in both appearance consistency and geometric plausibility--all in a zero-shot manner. Our results highlight the potential of geometric grounding for high-quality, zero-shot single-view scene generation."
    },
    {
      "title": "OnlineSI: Taming Large Language Model for Online 3D Understanding and Grounding",
      "url": "https://arxiv.org/abs/2601.16538",
      "date": "2026-01-23",
      "authors_text": "Zixian Liu, Zhaoxi Chen, Liang Pan, Ziwei Liu",
      "is_highlight": false,
      "score": 88.0,
      "summary": "OnlineSI is a framework that enhances Multimodal Large Language Models' spatial understanding in dynamic environments by integrating continuous learning from video streams and 3D point cloud data.",
      "debug_abstract": "In recent years, researchers have increasingly been interested in how to enable Multimodal Large Language Models (MLLM) to possess spatial understanding and reasoning capabilities. However, most existing methods overlook the importance of the ability to continuously work in an ever-changing world, and lack the possibility of deployment on embodied systems in real-world environments. In this work, we introduce OnlineSI, a framework that can continuously improve its spatial understanding of its surroundings given a video stream. Our core idea is to maintain a finite spatial memory to retain past observations, ensuring the computation required for each inference does not increase as the input accumulates. We further integrate 3D point cloud information with semantic information, helping MLLM to better locate and identify objects in the scene. To evaluate our method, we introduce the Fuzzy $F_1$-Score to mitigate ambiguity, and test our method on two representative datasets. Experiments demonstrate the effectiveness of our method, paving the way towards real-world embodied systems."
    },
    {
      "title": "GRIP: Algorithm-Agnostic Machine Unlearning for Mixture-of-Experts via Geometric Router Constraints",
      "url": "https://arxiv.org/abs/2601.16905",
      "date": "2026-01-23",
      "authors_text": "Andy Zhu, Rongzhe Wei, Yupu Gu, Pan Li",
      "is_highlight": false,
      "score": 62.0,
      "summary": "The GRIP framework enables effective machine unlearning in Mixture-of-Experts models by imposing geometric constraints on router updates, ensuring knowledge erasure without sacrificing model utility.",
      "debug_abstract": "Machine unlearning (MU) for large language models has become critical for AI safety, yet existing methods fail to generalize to Mixture-of-Experts (MoE) architectures. We identify that traditional unlearning methods exploit MoE&#39;s architectural vulnerability: they manipulate routers to redirect queries away from knowledgeable experts rather than erasing knowledge, causing a loss of model utility and superficial forgetting. We propose Geometric Routing Invariance Preservation (GRIP), an algorithm-agnostic framework for unlearning for MoE. Our core contribution is a geometric constraint, implemented by projecting router gradient updates into an expert-specific null-space. Crucially, this decouples routing stability from parameter rigidity: while discrete expert selections remain stable for retained knowledge, the continuous router parameters remain plastic within the null space, allowing the model to undergo necessary internal reconfiguration to satisfy unlearning objectives. This forces the unlearning optimization to erase knowledge directly from expert parameters rather than exploiting the superficial router manipulation shortcut. GRIP functions as an adapter, constraining router parameter updates without modifying the underlying unlearning algorithm. Extensive experiments on large-scale MoE models demonstrate that our adapter eliminates expert selection shift (achieving over 95% routing stability) across all tested unlearning methods while preserving their utility. By preventing existing algorithms from exploiting MoE model&#39;s router vulnerability, GRIP adapts existing unlearning research from dense architectures to MoEs."
    },
    {
      "title": "Render-of-Thought: Rendering Textual Chain-of-Thought as Images for Visual Latent Reasoning",
      "url": "https://arxiv.org/abs/2601.14750",
      "date": "2026-01-22",
      "authors_text": "Yifan Wang, Shiyu Li, Peiming Li, Xiaochen Yang, Yang Tang",
      "is_highlight": false,
      "score": 92.5,
      "summary": "The Render-of-Thought framework transforms Chain-of-Thought reasoning into images, enhancing analyzability and efficiency while maintaining competitive performance in reasoning tasks.",
      "debug_abstract": "Chain-of-Thought (CoT) prompting has achieved remarkable success in unlocking the reasoning capabilities of Large Language Models (LLMs). Although CoT prompting enhances reasoning, its verbosity imposes substantial computational overhead. Recent works often focus exclusively on outcome alignment and lack supervision on the intermediate reasoning process. These deficiencies obscure the analyzability of the latent reasoning chain. To address these challenges, we introduce Render-of-Thought (RoT), the first framework to reify the reasoning chain by rendering textual steps into images, making the latent rationale explicit and traceable. Specifically, we leverage the vision encoders of existing Vision Language Models (VLMs) as semantic anchors to align the vision embeddings with the textual space. This design ensures plug-and-play implementation without incurring additional pre-training overhead. Extensive experiments on mathematical and logical reasoning benchmarks demonstrate that our method achieves 3-4x token compression and substantial inference acceleration compared to explicit CoT. Furthermore, it maintains competitive performance against other methods, validating the feasibility of this paradigm. Our code is available at this https URL"
    },
    {
      "title": "LLM-in-Sandbox Elicits General Agentic Intelligence",
      "url": "https://arxiv.org/abs/2601.16206",
      "date": "2026-01-22",
      "authors_text": "Daixuan Cheng, Shaohan Huang, Yuxian Gu, Huatong Song, Guoxin Chen",
      "is_highlight": false,
      "score": 95.0,
      "summary": "The LLM-in-Sandbox framework enables large language models to demonstrate general intelligence in non-code tasks through sandbox exploration and reinforcement learning, achieving robust generalization across various domains.",
      "debug_abstract": "We introduce LLM-in-Sandbox, enabling LLMs to explore within a code sandbox (i.e., a virtual computer), to elicit general intelligence in non-code domains. We first demonstrate that strong LLMs, without additional training, exhibit generalization capabilities to leverage the code sandbox for non-code tasks. For example, LLMs spontaneously access external resources to acquire new knowledge, leverage the file system to handle long contexts, and execute scripts to satisfy formatting requirements. We further show that these agentic capabilities can be enhanced through LLM-in-Sandbox Reinforcement Learning (LLM-in-Sandbox-RL), which uses only non-agentic data to train models for sandbox exploration. Experiments demonstrate that LLM-in-Sandbox, in both training-free and post-trained settings, achieves robust generalization spanning mathematics, physics, chemistry, biomedicine, long-context understanding, and instruction following. Finally, we analyze LLM-in-Sandbox&#39;s efficiency from computational and system perspectives, and open-source it as a Python package to facilitate real-world deployment."
    },
    {
      "title": "Collision-Free Humanoid Traversal in Cluttered Indoor Scenes",
      "url": "https://arxiv.org/abs/2601.16035",
      "date": "2026-01-22",
      "authors_text": "Han Xue, Sikai Liang, Zhikai Zhang, Zicheng Zeng, Yun Liu",
      "is_highlight": false,
      "score": 81.2,
      "summary": "The paper presents Humanoid Potential Field (HumanoidPF) for enabling collision-free traversal of humanoids in cluttered indoor environments, demonstrating effective RL-based learning and real-world applicability.",
      "debug_abstract": "We study the problem of collision-free humanoid traversal in cluttered indoor scenes, such as hurdling over objects scattered on the floor, crouching under low-hanging obstacles, or squeezing through narrow passages. To achieve this goal, the humanoid needs to map its perception of surrounding obstacles with diverse spatial layouts and geometries to the corresponding traversal skills. However, the lack of an effective representation that captures humanoid-obstacle relationships during collision avoidance makes directly learning such mappings difficult. We therefore propose Humanoid Potential Field (HumanoidPF), which encodes these relationships as collision-free motion directions, significantly facilitating RL-based traversal skill learning. We also find that HumanoidPF exhibits a surprisingly negligible sim-to-real gap as a perceptual representation. To further enable generalizable traversal skills through diverse and challenging cluttered indoor scenes, we further propose a hybrid scene generation method, incorporating crops of realistic 3D indoor scenes and procedurally synthesized obstacles. We successfully transfer our policy to the real world and develop a teleoperation system where users could command the humanoid to traverse in cluttered indoor scenes with just a single click. Extensive experiments are conducted in both simulation and the real world to validate the effectiveness of our method. Demos and code can be found in our website: this https URL."
    }
  ],
  "机器人控制实验室": [
    {
      "title": "EventFlash: Towards Efficient MLLMs for Event-Based Vision",
      "url": "https://arxiv.org/abs/2602.03230",
      "date": "2026-02-03",
      "authors_text": "Shaoyu Liu, Jianing Li, Guanghui Zhao, Yunjian Zhang, Wen Jiang",
      "is_highlight": false,
      "score": 63.0,
      "summary": "EventFlash introduces an efficient MLLM for event-based vision, leveraging spatiotemporal sparsification and adaptive sampling to enhance performance and reduce computational costs.",
      "teaser_image": "https://arxiv.org/html/2602.03230/x2.png",
      "debug_abstract": "Event-based multimodal large language models (MLLMs) enable robust perception in high-speed and low-light scenarios, addressing key limitations of frame-based MLLMs. However, current event-based MLLMs often rely on dense image-like processing paradigms, overlooking the spatiotemporal sparsity of event streams and resulting in high computational cost. In this paper, we propose EventFlash, a novel and efficient MLLM to explore spatiotemporal token sparsification for reducing data redundancy and accelerating inference. Technically, we build EventMind, a large-scale and scene-diverse dataset with over 500k instruction sets, providing both short and long event stream sequences to support our curriculum training strategy. We then present an adaptive temporal window aggregation module for efficient temporal sampling, which adaptively compresses temporal tokens while retaining key temporal cues. Finally, a sparse density-guided attention module is designed to improve spatial token efficiency by selecting informative regions and suppressing empty or sparse areas. Experimental results show that EventFlash achieves a $12.4\\times$ throughput improvement over the baseline (EventFlash-Zero) while maintaining comparable performance. It supports long-range event stream processing with up to 1,000 bins, significantly outperforming the 5-bin limit of EventGPT. We believe EventFlash serves as an efficient foundation model for event-based vision."
    },
    {
      "title": "InstaDrive: Instance-Aware Driving World Models for Realistic and Consistent Video Generation",
      "url": "https://arxiv.org/abs/2602.03242",
      "date": "2026-02-03",
      "authors_text": "Zhuoran Yang, Xi Guo, Chenjing Ding, Chiyu Wang, Wei Wu",
      "is_highlight": false,
      "score": 84.0,
      "summary": "InstaDrive introduces a framework that enhances realistic driving video generation by ensuring instance-level temporal consistency and spatial geometric fidelity through innovative feature extraction and alignment techniques.",
      "teaser_image": "https://arxiv.org/html/2602.03242/x2.png",
      "debug_abstract": "Autonomous driving relies on robust models trained on high-quality, large-scale multi-view driving videos. While world models offer a cost-effective solution for generating realistic driving videos, they struggle to maintain instance-level temporal consistency and spatial geometric fidelity. To address these challenges, we propose InstaDrive, a novel framework that enhances driving video realism through two key advancements: (1) Instance Flow Guider, which extracts and propagates instance features across frames to enforce temporal consistency, preserving instance identity over time. (2) Spatial Geometric Aligner, which improves spatial reasoning, ensures precise instance positioning, and explicitly models occlusion hierarchies. By incorporating these instance-aware mechanisms, InstaDrive achieves state-of-the-art video generation quality and enhances downstream autonomous driving tasks on the nuScenes dataset. Additionally, we utilize CARLA&#39;s autopilot to procedurally and stochastically simulate rare but safety-critical driving scenarios across diverse maps and regions, enabling rigorous safety evaluation for autonomous systems. Our project page is this https URL."
    },
    {
      "title": "Learning Physics-Grounded 4D Dynamics with Neural Gaussian Force Fields",
      "url": "https://arxiv.org/abs/2602.00148",
      "date": "2026-01-29",
      "authors_text": "Shiqian Li, Ruihong Shen, Junfeng Ni, Chang Pan, Chi Zhang",
      "is_highlight": false,
      "score": 82.0,
      "summary": "The Neural Gaussian Force Field (NGFF) framework enables rapid generation of interactive, physically realistic 4D videos from multi-view RGB inputs, enhancing robustness and generalization in physical dynamics.",
      "teaser_image": "https://arxiv.org/html/2602.00148/x1.png",
      "debug_abstract": "Predicting physical dynamics from raw visual data remains a major challenge in AI. While recent video generation models have achieved impressive visual quality, they still cannot consistently generate physically plausible videos due to a lack of modeling of physical laws. Recent approaches combining 3D Gaussian splatting and physics engines can produce physically plausible videos, but are hindered by high computational costs in both reconstruction and simulation, and often lack robustness in complex real-world scenarios. To address these issues, we introduce Neural Gaussian Force Field (NGFF), an end-to-end neural framework that integrates 3D Gaussian perception with physics-based dynamic modeling to generate interactive, physically realistic 4D videos from multi-view RGB inputs, achieving two orders of magnitude faster than prior Gaussian simulators. To support training, we also present GSCollision, a 4D Gaussian dataset featuring diverse materials, multi-object interactions, and complex scenes, totaling over 640k rendered physical videos (~4 TB). Evaluations on synthetic and real 3D scenarios show NGFF&#39;s strong generalization and robustness in physical reasoning, advancing video prediction towards physics-grounded world models."
    },
    {
      "title": "How Far Are LLMs from Professional Poker Players? Revisiting Game-Theoretic Reasoning with Agentic Tool Use",
      "url": "https://arxiv.org/abs/2602.00528",
      "date": "2026-01-31",
      "authors_text": "Minhua Lin, Enyan Dai, Hui Liu, Xianfeng Tang, Yuliang Yan",
      "is_highlight": false,
      "score": 55.0,
      "summary": "This paper evaluates LLMs in poker, revealing their strategic reasoning flaws and proposing ToolPoker, which enhances gameplay and reasoning through integrated external solvers.",
      "teaser_image": "https://arxiv.org/html/2602.00528/x2.png",
      "debug_abstract": "As Large Language Models (LLMs) are increasingly applied in high-stakes domains, their ability to reason strategically under uncertainty becomes critical. Poker provides a rigorous testbed, requiring not only strong actions but also principled, game-theoretic reasoning. In this paper, we conduct a systematic study of LLMs in multiple realistic poker tasks, evaluating both gameplay outcomes and reasoning traces. Our analysis reveals LLMs fail to compete against traditional algorithms and identifies three recurring flaws: reliance on heuristics, factual misunderstandings, and a &#34;knowing-doing&#34; gap where actions diverge from reasoning. An initial attempt with behavior cloning and step-level reinforcement learning improves reasoning style but remains insufficient for accurate game-theoretic play. Motivated by these limitations, we propose ToolPoker, a tool-integrated reasoning framework that combines external solvers for GTO-consistent actions with more precise professional-style explanations. Experiments demonstrate that ToolPoker achieves state-of-the-art gameplay while producing reasoning traces that closely reflect game-theoretic principles."
    },
    {
      "title": "V2X-DSC: Multi-Agent Collaborative Perception with Distributed Source Coding Guided Communication",
      "url": "https://arxiv.org/abs/2602.00687",
      "date": "2026-01-31",
      "authors_text": "Yuankun Zeng, Shaohui Li, Zhi Li, Shulan Ruan, Yu Liu",
      "is_highlight": false,
      "score": 76.0,
      "summary": "V2X-DSC introduces a Conditional Codec for efficient multi-agent collaborative perception by compressing and reconstructing BEV features, optimizing bandwidth usage while maintaining accuracy.",
      "teaser_image": "https://arxiv.org/html/2602.00687/figs/overview.jpg",
      "debug_abstract": "Collaborative perception improves 3D understanding by fusing multi-agent observations, yet intermediate-feature sharing faces strict bandwidth constraints as dense BEV features saturate V2X links. We observe that collaborators view the same physical world, making their features strongly correlated; thus receivers only need innovation beyond their local context. Revisiting this from a distributed source coding perspective, we propose V2X-DSC, a framework with a Conditional Codec (DCC) for bandwidth-constrained fusion. The sender compresses BEV features into compact codes, while the receiver performs conditional reconstruction using its local features as side information, allocating bits to complementary cues rather than redundant content. This conditional structure regularizes learning, encouraging incremental representation and yielding lower-noise features. Experiments on DAIR-V2X, OPV2V, and V2X-Real demonstrate state-of-the-art accuracy-bandwidth trade-offs under KB-level communication, and generalizes as a plug-and-play communication layer across multiple fusion backbones."
    },
    {
      "title": "Resource-Efficient Reinforcement for Reasoning Large Language Models via Dynamic One-Shot Policy Refinement",
      "url": "https://arxiv.org/abs/2602.00815",
      "date": "2026-01-31",
      "authors_text": "Yunjian Zhang, Sudong Wang, Yang Li, Peiran Xu, Conghao Zhou",
      "is_highlight": false,
      "score": 57.0,
      "summary": "The paper introduces Dynamic One-Shot Policy Refinement (DoPR), a resource-efficient reinforcement learning strategy that enhances reasoning in large language models while significantly reducing training costs.",
      "teaser_image": "https://arxiv.org/html/2602.00815/dopr.png",
      "debug_abstract": "Large language models (LLMs) have exhibited remarkable performance on complex reasoning tasks, with reinforcement learning under verifiable rewards (RLVR) emerging as a principled framework for aligning model behavior with reasoning chains. Despite its promise, RLVR remains prohibitively resource-intensive, requiring extensive reward signals and incurring substantial rollout costs during training. In this work, we revisit the fundamental question of data and compute efficiency in RLVR. We first establish a theoretical lower bound on the sample complexity required to unlock reasoning capabilities, and empirically validate that strong performance can be achieved with a surprisingly small number of training instances. To tackle the computational burden, we propose Dynamic One-Shot Policy Refinement (DoPR), an uncertainty-aware RL strategy that dynamically selects a single informative training sample per batch for policy updates, guided by reward volatility and exploration-driven acquisition. DoPR reduces rollout overhead by nearly an order of magnitude while preserving competitive reasoning accuracy, offering a scalable and resource-efficient solution for LLM post-training. This approach offers a practical path toward more efficient and accessible RL-based training for reasoning-intensive LLM applications."
    },
    {
      "title": "Making Avatars Interact: Towards Text-Driven Human-Object Interaction for Controllable Talking Avatars",
      "url": "https://arxiv.org/abs/2602.01538",
      "date": "2026-02-02",
      "authors_text": "Youliang Zhang, Zhengguang Zhou, Zhentao Yu, Ziyao Huang, Teng Hu",
      "is_highlight": false,
      "score": 46.0,
      "summary": "The paper presents InteractAvatar, a dual-stream framework for generating talking avatars that perform text-driven human-object interactions by decoupling perception and planning from video synthesis.",
      "teaser_image": "https://arxiv.org/html/2602.01538/x1.png",
      "debug_abstract": "Generating talking avatars is a fundamental task in video generation. Although existing methods can generate full-body talking avatars with simple human motion, extending this task to grounded human-object interaction (GHOI) remains an open challenge, requiring the avatar to perform text-aligned interactions with surrounding objects. This challenge stems from the need for environmental perception and the control-quality dilemma in GHOI generation. To address this, we propose a novel dual-stream framework, InteractAvatar, which decouples perception and planning from video synthesis for grounded human-object interaction. Leveraging detection to enhance environmental perception, we introduce a Perception and Interaction Module (PIM) to generate text-aligned interaction motions. Additionally, an Audio-Interaction Aware Generation Module (AIM) is proposed to synthesize vivid talking avatars performing object interactions. With a specially designed motion-to-video aligner, PIM and AIM share a similar network structure and enable parallel co-generation of motions and plausible videos, effectively mitigating the control-quality dilemma. Finally, we establish a benchmark, GroundedInter, for evaluating GHOI video generation. Extensive experiments and comparisons demonstrate the effectiveness of our method in generating grounded human-object interactions for talking avatars. Project page: this https URL"
    },
    {
      "title": "Reasoning with Autoregressive-Diffusion Collaborative Thoughts",
      "url": "https://arxiv.org/abs/2602.01608",
      "date": "2026-02-02",
      "authors_text": "Mu Yuan, Liekang Zeng, Guoliang Xing, Lan Zhang, Yunhao Liu",
      "is_highlight": false,
      "score": 41.0,
      "summary": "The paper presents Collaborative Thoughts, a framework integrating autoregressive and diffusion models for enhanced spatial reasoning and generation through iterative feedback and joint reasoning.",
      "teaser_image": "https://arxiv.org/html/2602.01608/x2.png",
      "debug_abstract": "Autoregressive and diffusion models represent two complementary generative paradigms. Autoregressive models excel at sequential planning and constraint composition, yet struggle with tasks that require explicit spatial or physical grounding. Diffusion models, in contrast, capture rich spatial structure through high-dimensional generation, but lack the stepwise logical control needed to satisfy complex, multi-stage constraints or to reliably identify and correct errors. We introduce Collaborative Thoughts, a unified collaborative framework that enables autoregressive and diffusion models to reason and generate jointly through a closed-loop interaction. In Collaborative Thoughts, autoregressive models perform structured planning and constraint management, diffusion models instantiate these constraints as intermediate visual thoughts, and a vision-based critic module evaluates whether the visual thoughts satisfy the intended structural and physical requirements. This feedback is then used to iteratively refine subsequent planning and generation steps, mitigating error propagation across modalities. Importantly, Collaborative Thoughts uses the same collaborative loop regardless of whether the task is autoregressive question answering or diffusion-based visual generation. Through representative examples, we illustrate how Collaborative Thoughts can improve the reliability of spatial reasoning and the controllability of generation."
    },
    {
      "title": "From Frames to Sequences: Temporally Consistent Human-Centric Dense Prediction",
      "url": "https://arxiv.org/abs/2602.01661",
      "date": "2026-02-02",
      "authors_text": "Xingyu Miao, Junting Dong, Qin Zhao, Yuhang Yang, Junhao Chen",
      "is_highlight": false,
      "score": 43.0,
      "summary": "This paper presents a novel synthetic data pipeline and a ViT-based model for achieving temporally consistent human-centric dense prediction in video sequences, outperforming existing methods.",
      "teaser_image": "https://arxiv.org/html/2602.01661/x2.png",
      "debug_abstract": "In this work, we focus on the challenge of temporally consistent human-centric dense prediction across video sequences. Existing models achieve strong per-frame accuracy but often flicker under motion, occlusion, and lighting changes, and they rarely have paired human video supervision for multiple dense tasks. We address this gap with a scalable synthetic data pipeline that generates photorealistic human frames and motion-aligned sequences with pixel-accurate depth, normals, and masks. Unlike prior static data synthetic pipelines, our pipeline provides both frame-level labels for spatial learning and sequence-level supervision for temporal learning. Building on this, we train a unified ViT-based dense predictor that (i) injects an explicit human geometric prior via CSE embeddings and (ii) improves geometry-feature reliability with a lightweight channel reweighting module after feature fusion. Our two-stage training strategy, combining static pretraining with dynamic sequence supervision, enables the model first to acquire robust spatial representations and then refine temporal consistency across motion-aligned sequences. Extensive experiments show that we achieve state-of-the-art performance on THuman2.1 and Hi4D and generalize effectively to in-the-wild videos."
    },
    {
      "title": "ForSim: Stepwise Forward Simulation for Traffic Policy Fine-Tuning",
      "url": "https://arxiv.org/abs/2602.01916",
      "date": "2026-02-02",
      "authors_text": "Keyu Chen, Wenchao Sun, Hao Cheng, Zheng Fu, Sifa Zheng",
      "is_highlight": true,
      "score": 29.0,
      "summary": "ForSim introduces a stepwise closed-loop simulation method that enhances traffic policy fine-tuning by improving agent interactions and preserving multimodal behaviors in autonomous driving.",
      "teaser_image": "https://arxiv.org/html/2602.01916/x2.png",
      "debug_abstract": "As the foundation of closed-loop training and evaluation in autonomous driving, traffic simulation still faces two fundamental challenges: covariate shift introduced by open-loop imitation learning and limited capacity to reflect the multimodal behaviors observed in real-world traffic. Although recent frameworks such as RIFT have partially addressed these issues through group-relative optimization, their forward simulation procedures remain largely non-reactive, leading to unrealistic agent interactions within the virtual domain and ultimately limiting simulation fidelity. To address these issues, we propose ForSim, a stepwise closed-loop forward simulation paradigm. At each virtual timestep, the traffic agent propagates the virtual candidate trajectory that best spatiotemporally matches the reference trajectory through physically grounded motion dynamics, thereby preserving multimodal behavioral diversity while ensuring intra-modality consistency. Other agents are updated with stepwise predictions, yielding coherent and interaction-aware evolution. When incorporated into the RIFT traffic simulation framework, ForSim operates in conjunction with group-relative optimization to fine-tune traffic policy. Extensive experiments confirm that this integration consistently improves safety while maintaining efficiency, realism, and comfort. These results underscore the importance of modeling closed-loop multimodal interactions within forward simulation and enhance the fidelity and reliability of traffic simulation for autonomous driving. Project Page: this https URL"
    },
    {
      "title": "Small Generalizable Prompt Predictive Models Can Steer Efficient RL Post-Training of Large Reasoning Models",
      "url": "https://arxiv.org/abs/2602.01970",
      "date": "2026-02-02",
      "authors_text": "Yun Qu, Qi Wang, Yixiu Mao, Heming Zou, Yuhang Jiang",
      "is_highlight": false,
      "score": 26.0,
      "summary": "The paper presents Generalizable Predictive Prompt Selection (GPS), a method that enhances reinforcement learning efficiency in large language models by utilizing a lightweight generative model for prompt selection.",
      "teaser_image": "https://arxiv.org/html/2602.01970/x1.png",
      "debug_abstract": "Reinforcement learning enhances the reasoning capabilities of large language models but often involves high computational costs due to rollout-intensive optimization. Online prompt selection presents a plausible solution by prioritizing informative prompts to improve training efficiency. However, current methods either depend on costly, exact evaluations or construct prompt-specific predictive models lacking generalization across prompts. This study introduces Generalizable Predictive Prompt Selection (GPS), which performs Bayesian inference towards prompt difficulty using a lightweight generative model trained on the shared optimization history. Intermediate-difficulty prioritization and history-anchored diversity are incorporated into the batch acquisition principle to select informative prompt batches. The small predictive model also generalizes at test-time for efficient computational allocation. Experiments across varied reasoning benchmarks indicate GPS&#39;s substantial improvements in training efficiency, final performance, and test-time efficiency over superior baseline methods."
    },
    {
      "title": "TTT-Parkour: Rapid Test-Time Training for Perceptive Robot Parkour",
      "url": "https://arxiv.org/abs/2602.02331",
      "date": "2026-02-02",
      "authors_text": "Shaoting Zhu, Baijun Ye, Jiaxuan Wang, Jiakang Chen, Ziwen Zhuang",
      "is_highlight": false,
      "score": 18.0,
      "summary": "TTT-Parkour introduces a rapid test-time training framework enabling humanoid robots to effectively navigate complex terrains through efficient real-to-sim-to-real learning and geometry reconstruction.",
      "teaser_image": "https://arxiv.org/html/2602.02331/x1.png",
      "debug_abstract": "Achieving highly dynamic humanoid parkour on unseen, complex terrains remains a challenge in robotics. Although general locomotion policies demonstrate capabilities across broad terrain distributions, they often struggle with arbitrary and highly challenging environments. To overcome this limitation, we propose a real-to-sim-to-real framework that leverages rapid test-time training (TTT) on novel terrains, significantly enhancing the robot&#39;s capability to traverse extremely difficult geometries. We adopt a two-stage end-to-end learning paradigm: a policy is first pre-trained on diverse procedurally generated terrains, followed by rapid fine-tuning on high-fidelity meshes reconstructed from real-world captures. Specifically, we develop a feed-forward, efficient, and high-fidelity geometry reconstruction pipeline using RGB-D inputs, ensuring both speed and quality during test-time training. We demonstrate that TTT-Parkour empowers humanoid robots to master complex obstacles, including wedges, stakes, boxes, trapezoids, and narrow beams. The whole pipeline of capturing, reconstructing, and test-time training requires less than 10 minutes on most tested terrains. Extensive experiments show that the policy after test-time training exhibits robust zero-shot sim-to-real transfer capability."
    },
    {
      "title": "Context Learning for Multi-Agent Discussion",
      "url": "https://arxiv.org/abs/2602.02350",
      "date": "2026-02-02",
      "authors_text": "Xingyuan Hua, Sheng Yue, Xinyi Li, Yizhe Zhao, Jinrui Zhang",
      "is_highlight": false,
      "score": 17.0,
      "summary": "The paper presents M2CL, a multi-LLM context learning method that enhances coherence in Multi-Agent Discussions, improving problem-solving performance by 20%-50% over existing approaches.",
      "teaser_image": "https://arxiv.org/html/2602.02350/x2.png",
      "debug_abstract": "Multi-Agent Discussion (MAD) has garnered increasing attention very recently, where multiple LLM instances collaboratively solve problems via structured discussion. However, we find that current MAD methods easily suffer from discussion inconsistency, LLMs fail to reach a coherent solution, due to the misalignment between their individual this http URL this paper, we introduce a multi-LLM context learning method (M2CL) that learns a context generator for each agent, capable of dynamically generating context instructions per discussion round via automatic information organization and refinement. Specifically, inspired by our theoretical insights on the context instruction, M2CL train the generators to control context coherence and output discrepancies via a carefully crafted self-adaptive this http URL enables LLMs to avoid premature convergence on majority noise and progressively reach the correct consensus. We evaluate M2CL on challenging tasks, including academic reasoning, embodied tasks, and mobile control. The results show that the performance of M2CL significantly surpasses existing methods by 20%--50%, while enjoying favorable transferability and computational efficiency."
    },
    {
      "title": "UV-M3TL: A Unified and Versatile Multimodal Multi-Task Learning Framework for Assistive Driving Perception",
      "url": "https://arxiv.org/abs/2602.01594",
      "date": "2026-02-02",
      "authors_text": "Wenzhuo Liu, Qiannan Guo, Zhen Wang, Wenshuo Wang, Lei Yang",
      "is_highlight": false,
      "score": 8.0,
      "summary": "The UV-M3TL framework enhances ADAS by effectively recognizing driver behavior and context through dual-branch multimodal embedding and adaptive loss, achieving state-of-the-art performance across multiple tasks.",
      "teaser_image": "https://arxiv.org/html/2602.01594/x2.png",
      "debug_abstract": "Advanced Driver Assistance Systems (ADAS) need to understand human driver behavior while perceiving their navigation context, but jointly learning these heterogeneous tasks would cause inter-task negative transfer and impair system performance. Here, we propose a Unified and Versatile Multimodal Multi-Task Learning (UV-M3TL) framework to simultaneously recognize driver behavior, driver emotion, vehicle behavior, and traffic context, while mitigating inter-task negative transfer. Our framework incorporates two core components: dual-branch spatial channel multimodal embedding (DB-SCME) and adaptive feature-decoupled multi-task loss (AFD-Loss). DB-SCME enhances cross-task knowledge transfer while mitigating task conflicts by employing a dual-branch structure to explicitly model salient task-shared and task-specific features. AFD-Loss improves the stability of joint optimization while guiding the model to learn diverse multi-task representations by introducing an adaptive weighting mechanism based on learning dynamics and feature decoupling constraints. We evaluate our method on the AIDE dataset, and the experimental results demonstrate that UV-M3TL achieves state-of-the-art performance across all four tasks. To further prove the versatility, we evaluate UV-M3TL on additional public multi-task perception benchmarks (BDD100K, CityScapes, NYUD-v2, and PASCAL-Context), where it consistently delivers strong performance across diverse task combinations, attaining state-of-the-art results on most tasks."
    },
    {
      "title": "Enhancing Indoor Occupancy Prediction via Sparse Query-Based Multi-Level Consistent Knowledge Distillation",
      "url": "https://arxiv.org/abs/2602.02318",
      "date": "2026-02-02",
      "authors_text": "Xiang Li, Yupeng Zheng, Pengfei Li, Yilun Chen, Ya-Qin Zhang",
      "is_highlight": false,
      "score": 0,
      "summary": "DiScene introduces a sparse query-based framework for efficient and robust indoor occupancy prediction through multi-level knowledge distillation, achieving state-of-the-art performance and faster inference.",
      "teaser_image": "https://arxiv.org/html/2602.02318/x2.png",
      "debug_abstract": "Occupancy prediction provides critical geometric and semantic understanding for robotics but faces efficiency-accuracy trade-offs. Current dense methods suffer computational waste on empty voxels, while sparse query-based approaches lack robustness in diverse and complex indoor scenes. In this paper, we propose DiScene, a novel sparse query-based framework that leverages multi-level distillation to achieve efficient and robust occupancy prediction. In particular, our method incorporates two key innovations: (1) a Multi-level Consistent Knowledge Distillation strategy, which transfers hierarchical representations from large teacher models to lightweight students through coordinated alignment across four levels, including encoder-level feature alignment, query-level feature matching, prior-level spatial guidance, and anchor-level high-confidence knowledge transfer and (2) a Teacher-Guided Initialization policy, employing optimized parameter warm-up to accelerate model convergence. Validated on the Occ-Scannet benchmark, DiScene achieves 23.2 FPS without depth priors while outperforming our baseline method, OPUS, by 36.1% and even better than the depth-enhanced version, OPUS†. With depth integration, DiScene† attains new SOTA performance, surpassing EmbodiedOcc by 3.7% with 1.62$\\times$ faster inference speed. Furthermore, experiments on the Occ3D-nuScenes benchmark and in-the-wild scenarios demonstrate the versatility of our approach in various environments. Code and models can be accessed at this https URL."
    },
    {
      "title": "InspecSafe-V1: A Multimodal Benchmark for Safety Assessment in Industrial Inspection Scenarios",
      "url": "https://arxiv.org/abs/2601.21173",
      "date": "2026-01-29",
      "authors_text": "Zeyi Liu, Shuang Liu, Jihai Min, Zhaoheng Zhang, Jun Cen",
      "is_highlight": false,
      "score": 82.0,
      "summary": "InspecSafe-V1 is a multimodal benchmark dataset for safety assessment in industrial inspections, featuring real-world data from diverse scenarios and multiple sensing modalities.",
      "teaser_image": "https://arxiv.org/html/2601.21173/x1.png",
      "debug_abstract": "With the rapid development of industrial intelligence and unmanned inspection, reliable perception and safety assessment for AI systems in complex and dynamic industrial sites has become a key bottleneck for deploying predictive maintenance and autonomous inspection. Most public datasets remain limited by simulated data sources, single-modality sensing, or the absence of fine-grained object-level annotations, which prevents robust scene understanding and multimodal safety reasoning for industrial foundation models. To address these limitations, InspecSafe-V1 is released as the first multimodal benchmark dataset for industrial inspection safety assessment that is collected from routine operations of real inspection robots in real-world environments. InspecSafe-V1 covers five representative industrial scenarios, including tunnels, power facilities, sintering equipment, oil and gas petrochemical plants, and coal conveyor trestles. The dataset is constructed from 41 wheeled and rail-mounted inspection robots operating at 2,239 valid inspection sites, yielding 5,013 inspection instances. For each instance, pixel-level segmentation annotations are provided for key objects in visible-spectrum images. In addition, a semantic scene description and a corresponding safety level label are provided according to practical inspection tasks. Seven synchronized sensing modalities are further included, including infrared video, audio, depth point clouds, radar point clouds, gas measurements, temperature, and humidity, to support multimodal anomaly recognition, cross-modal fusion, and comprehensive safety assessment in industrial environments."
    },
    {
      "title": "Listen, Look, Drive: Coupling Audio Instructions for User-aware VLA-based Autonomous Driving",
      "url": "https://arxiv.org/abs/2601.12142",
      "date": "2026-01-28",
      "authors_text": "Ziang Guo, Feng Yang, Xuefeng Zhang, Jiaqi Guo, Kun Zhao",
      "is_highlight": false,
      "score": 88.0,
      "summary": "EchoVLA enhances autonomous driving by integrating audio instructions with visual inputs, allowing for user influence and emotional context, significantly improving navigation accuracy and safety.",
      "teaser_image": "https://arxiv.org/html/2601.12142/Figs/overview.png",
      "debug_abstract": "Vision Language Action (VLA) models promise an open-vocabulary interface that can translate perceptual ambiguity into semantically grounded driving decisions, yet they still treat language as a static prior fixed at inference time. As a result, the model must infer continuously shifting objectives from pixels alone, yielding delayed or overly conservative maneuvers. We argue that effective VLAs for autonomous driving need an online channel in which users can influence driving with specific intentions. To this end, we present EchoVLA, a user-aware VLA that couples camera streams with in situ audio instructions. We augment the nuScenes dataset with temporally aligned, intent-specific speech commands generated by converting ego-motion descriptions into synthetic audios. Further, we compose emotional speech-trajectory pairs into a multimodal Chain-of-Thought (CoT) for fine-tuning a Multimodal Large Model (MLM) based on Qwen2.5-Omni. Specifically, we synthesize the audio-augmented dataset with different emotion types paired with corresponding driving behaviors, leveraging the emotional cues embedded in tone, pitch, and speech tempo to reflect varying user states, such as urgent or hesitant intentions, thus enabling our EchoVLA to interpret not only the semantic content but also the emotional context of audio commands for more nuanced and emotionally adaptive driving behavior. In open-loop benchmarks, our approach reduces the average L2 error by $59.4\\%$ and the collision rate by $74.4\\%$ compared to the baseline of vision-only perception. More experiments on nuScenes dataset validate that EchoVLA not only steers the trajectory through audio instructions, but also modulates driving behavior in response to the emotions detected in the user&#39;s speech."
    },
    {
      "title": "HINT: Hierarchical Interaction Modeling for Autoregressive Multi-Human Motion Generation",
      "url": "https://arxiv.org/abs/2601.20383",
      "date": "2026-01-28",
      "authors_text": "Mengge Liu, Yan Di, Gu Wang, Yun Qu, Dekai Zhu",
      "is_highlight": false,
      "score": 78.0,
      "summary": "HINT introduces a novel autoregressive framework for multi-human motion generation, enhancing interaction modeling and performance with variable agent counts and long text guidance.",
      "teaser_image": "https://arxiv.org/html/2601.20383/x2.png",
      "debug_abstract": "Text-driven multi-human motion generation with complex interactions remains a challenging problem. Despite progress in performance, existing offline methods that generate fixed-length motions with a fixed number of agents, are inherently limited in handling long or variable text, and varying agent counts. These limitations naturally encourage autoregressive formulations, which predict future motions step by step conditioned on all past trajectories and current text guidance. In this work, we introduce HINT, the first autoregressive framework for multi-human motion generation with Hierarchical INTeraction modeling in diffusion. First, HINT leverages a disentangled motion representation within a canonicalized latent space, decoupling local motion semantics from inter-person interactions. This design facilitates direct adaptation to varying numbers of human participants without requiring additional refinement. Second, HINT adopts a sliding-window strategy for efficient online generation, and aggregates local within-window and global cross-window conditions to capture past human history, inter-person dependencies, and align with text guidance. This strategy not only enables fine-grained interaction modeling within each window but also preserves long-horizon coherence across all the long sequence. Extensive experiments on public benchmarks demonstrate that HINT matches the performance of strong offline models and surpasses autoregressive baselines. Notably, on InterHuman, HINT achieves an FID of 3.100, significantly improving over the previous state-of-the-art score of 5.154."
    },
    {
      "title": "Self-Supervised Path Planning in Unstructured Environments via Global-Guided Differentiable Hard Constraint Projection",
      "url": "https://arxiv.org/abs/2601.19354",
      "date": "2026-01-27",
      "authors_text": "Ziqian Wang, Chenxi Fang, Zhen Zhang",
      "is_highlight": false,
      "score": 90.0,
      "summary": "This paper presents a self-supervised framework for autonomous navigation in unstructured environments, utilizing a differentiable projection layer to ensure safety and efficiency in path planning.",
      "debug_abstract": "Deploying deep learning agents for autonomous navigation in unstructured environments faces critical challenges regarding safety, data scarcity, and limited computational resources. Traditional solvers often suffer from high latency, while emerging learning-based approaches struggle to ensure deterministic feasibility. To bridge the gap from embodied to embedded intelligence, we propose a self-supervised framework incorporating a differentiable hard constraint projection layer for runtime assurance. To mitigate data scarcity, we construct a Global-Guided Artificial Potential Field (G-APF), which provides dense supervision signals without manual labeling. To enforce actuator limitations and geometric constraints efficiently, we employ an adaptive neural projection layer, which iteratively rectifies the coarse network output onto the feasible manifold. Extensive benchmarks on a test set of 20,000 scenarios demonstrate an 88.75\\% success rate, substantiating the enhanced operational safety. Closed-loop experiments in CARLA further validate the physical realizability of the planned paths under dynamic constraints. Furthermore, deployment verification on an NVIDIA Jetson Orin NX confirms an inference latency of 94 ms, showing real-time feasibility on resource-constrained embedded hardware. This framework offers a generalized paradigm for embedding physical laws into neural architectures, providing a viable direction for solving constrained optimization in mechatronics. Source code is available at: this https URL."
    },
    {
      "title": "AtomMem : Learnable Dynamic Agentic Memory with Atomic Memory Operation",
      "url": "https://arxiv.org/abs/2601.08323",
      "date": "2026-01-27",
      "authors_text": "Yupeng Huo, Yaxi Lu, Zhong Zhang, Haotian Chen, Yankai Lin",
      "is_highlight": false,
      "score": 82.0,
      "summary": "AtomMem introduces a learnable dynamic memory framework that optimizes memory management through atomic CRUD operations, significantly outperforming static methods in long-horizon tasks.",
      "debug_abstract": "Equipping agents with memory is essential for solving real-world long-horizon problems. However, most existing agent memory mechanisms rely on static and hand-crafted workflows. This limits the performance and generalization ability of these memory designs, which highlights the need for a more flexible, learning-based memory framework. In this paper, we propose AtomMem, which reframes memory management as a dynamic decision-making problem. We deconstruct high-level memory processes into fundamental atomic CRUD (Create, Read, Update, Delete) operations, transforming the memory workflow into a learnable decision process. By combining supervised fine-tuning with reinforcement learning, AtomMem learns an autonomous, task-aligned policy to orchestrate memory behaviors tailored to specific task demands. Experimental results across 3 long-context benchmarks demonstrate that the trained AtomMem-8B consistently outperforms prior static-workflow memory methods. Further analysis of training dynamics shows that our learning-based formulation enables the agent to discover structured, task-aligned memory management strategies, highlighting a key advantage over predefined routines."
    },
    {
      "title": "Visual Generation Unlocks Human-Like Reasoning through Multimodal World Models",
      "url": "https://arxiv.org/abs/2601.19834",
      "date": "2026-01-27",
      "authors_text": "Jialong Wu, Xiaoying Zhang, Hongyi Yuan, Xiangcheng Zhang, Tianhao Huang",
      "is_highlight": false,
      "score": 89.0,
      "summary": "This paper demonstrates that visual generation enhances reasoning in multimodal AI, particularly for tasks requiring physical world understanding, outperforming purely verbal approaches.",
      "debug_abstract": "Humans construct internal world models and reason by manipulating the concepts within these models. Recent advances in AI, particularly chain-of-thought (CoT) reasoning, approximate such human cognitive abilities, where world models are believed to be embedded within large language models. Expert-level performance in formal and abstract domains such as mathematics and programming has been achieved in current systems by relying predominantly on verbal reasoning. However, they still lag far behind humans in domains like physical and spatial intelligence, which require richer representations and prior knowledge. The emergence of unified multimodal models (UMMs) capable of both verbal and visual generation has therefore sparked interest in more human-like reasoning grounded in complementary multimodal pathways, though their benefits remain unclear. From a world-model perspective, this paper presents the first principled study of when and how visual generation benefits reasoning. Our key position is the visual superiority hypothesis: for certain tasks--particularly those grounded in the physical world--visual generation more naturally serves as world models, whereas purely verbal world models encounter bottlenecks arising from representational limitations or insufficient prior knowledge. Theoretically, we formalize internal world modeling as a core component of CoT reasoning and analyze distinctions among different forms of world models. Empirically, we identify tasks that necessitate interleaved visual-verbal CoT reasoning, constructing a new evaluation suite, VisWorld-Eval. Controlled experiments on a state-of-the-art UMM show that interleaved CoT significantly outperforms purely verbal CoT on tasks that favor visual world modeling, but offers no clear advantage otherwise. Together, this work clarifies the potential of multimodal world modeling for more powerful, human-like multimodal AI."
    },
    {
      "title": "Scalable Exploration for High-Dimensional Continuous Control via Value-Guided Flow",
      "url": "https://arxiv.org/abs/2601.19707",
      "date": "2026-01-27",
      "authors_text": "Yunyue Wei, Chenhui Zuo, Yanan Sui",
      "is_highlight": false,
      "score": 79.0,
      "summary": "Qflex introduces a scalable reinforcement learning method for high-dimensional continuous control that enhances exploration efficiency by aligning actions with learned value gradients.",
      "debug_abstract": "Controlling high-dimensional systems in biological and robotic applications is challenging due to expansive state-action spaces, where effective exploration is critical. Commonly used exploration strategies in reinforcement learning are largely undirected with sharp degradation as action dimensionality grows. Many existing methods resort to dimensionality reduction, which constrains policy expressiveness and forfeits system flexibility. We introduce Q-guided Flow Exploration (Qflex), a scalable reinforcement learning method that conducts exploration directly in the native high-dimensional action space. During training, Qflex traverses actions from a learnable source distribution along a probability flow induced by the learned value function, aligning exploration with task-relevant gradients rather than isotropic noise. Our proposed method substantially outperforms representative online reinforcement learning baselines across diverse high-dimensional continuous-control benchmarks. Qflex also successfully controls a full-body human musculoskeletal model to perform agile, complex movements, demonstrating superior scalability and sample efficiency in very high-dimensional settings. Our results indicate that value-guided flows offer a principled and practical route to exploration at scale."
    },
    {
      "title": "Latent-Space Contrastive Reinforcement Learning for Stable and Efficient LLM Reasoning",
      "url": "https://arxiv.org/abs/2601.17275",
      "date": "2026-01-24",
      "authors_text": "Lianlei Shan, Han Chen, Yixuan Wang, Zhenjie Liu, Wei Li",
      "is_highlight": false,
      "score": 60.0,
      "summary": "The paper introduces DeepLatent Reasoning (DLR), a latent-space contrastive reinforcement learning framework that enhances LLM reasoning by addressing inefficiencies and stability in multi-step tasks.",
      "debug_abstract": "While Large Language Models (LLMs) demonstrate exceptional performance in surface-level text generation, their nature in handling complex multi-step reasoning tasks often remains one of ``statistical fitting&#39;&#39; rather than systematic logical deduction. Traditional Reinforcement Learning (RL) attempts to mitigate this by introducing a ``think-before-speak&#39;&#39; paradigm. However, applying RL directly in high-dimensional, discrete token spaces faces three inherent challenges: sample-inefficient rollouts, high gradient estimation variance, and the risk of catastrophic forgetting. To fundamentally address these structural bottlenecks, we propose \\textbf{DeepLatent Reasoning (DLR)}, a latent-space bidirectional contrastive reinforcement learning framework. This framework shifts the trial-and-error cost from expensive token-level full sequence generation to the continuous latent manifold. Specifically, we introduce a lightweight assistant model to efficiently sample $K$ reasoning chain encodings within the latent space. These encodings are filtered via a dual reward mechanism based on correctness and formatting; only high-value latent trajectories are fed into a \\textbf{frozen main model} for single-pass decoding. To maximize reasoning diversity while maintaining coherence, we design a contrastive learning objective to enable directed exploration within the latent space. Since the main model parameters remain frozen during optimization, this method mathematically eliminates catastrophic forgetting. Experiments demonstrate that under comparable GPU computational budgets, DLR achieves more stable training convergence, supports longer-horizon reasoning chains, and facilitates the sustainable accumulation of reasoning capabilities, providing a viable path toward reliable and scalable reinforcement learning for LLMs."
    },
    {
      "title": "MetaWorld: Skill Transfer and Composition in a Hierarchical World Model for Grounding High-Level Instructions",
      "url": "https://arxiv.org/abs/2601.17507",
      "date": "2026-01-24",
      "authors_text": "Yutong Shen, Hangxu Liu, Kailin Pei, Ruizhe Xia, Tongtong Feng",
      "is_highlight": false,
      "score": 92.0,
      "summary": "MetaWorld introduces a hierarchical world model that enhances humanoid robot loco-manipulation by integrating semantic planning and physical control through expert policy transfer, improving task performance and motion coherence.",
      "debug_abstract": "Humanoid robot loco-manipulation remains constrained by the semantic-physical gap. Current methods face three limitations: Low sample efficiency in reinforcement learning, poor generalization in imitation learning, and physical inconsistency in VLMs. We propose MetaWorld, a hierarchical world model that integrates semantic planning and physical control via expert policy transfer. The framework decouples tasks into a VLM-driven semantic layer and a latent dynamics model operating in a compact state space. Our dynamic expert selection and motion prior fusion mechanism leverages a pre-trained multi-expert policy library as transferable knowledge, enabling efficient online adaptation via a two-stage framework. VLMs serve as semantic interfaces, mapping instructions to executable skills and bypassing symbol grounding. Experiments on Humanoid-Bench show MetaWorld outperforms world model-based RL in task completion and motion coherence. Our code will be found at this https URL"
    },
    {
      "title": "The Flexibility Trap: Why Arbitrary Order Limits Reasoning Potential in Diffusion Language Models",
      "url": "https://arxiv.org/abs/2601.15165",
      "date": "2026-01-26",
      "authors_text": "Zanlin Ni, Shenzhi Wang, Yang Yue, Tianyu Yu, Weilin Zhao",
      "is_highlight": false,
      "score": 45.0,
      "summary": "This paper argues that arbitrary order generation in diffusion language models limits reasoning potential, advocating for a simpler approach using Group Relative Policy Optimization for improved performance.",
      "debug_abstract": "Diffusion Large Language Models (dLLMs) break the rigid left-to-right constraint of traditional LLMs, enabling token generation in arbitrary orders. Intuitively, this flexibility implies a solution space that strictly supersets the fixed autoregressive trajectory, theoretically unlocking superior reasoning potential for general tasks like mathematics and coding. Consequently, numerous works have leveraged reinforcement learning (RL) to elicit the reasoning capability of dLLMs. In this paper, we reveal a counter-intuitive reality: arbitrary order generation, in its current form, narrows rather than expands the reasoning boundary of dLLMs. We find that dLLMs tend to exploit this order flexibility to bypass high-uncertainty tokens that are crucial for exploration, leading to a premature collapse of the solution space. This observation motivates a rethink of RL approaches for dLLMs, where considerable complexities, such as handling combinatorial trajectories and intractable likelihoods, are often devoted to preserving this flexibility. We demonstrate that effective reasoning can be better elicited by intentionally forgoing arbitrary order and applying standard Group Relative Policy Optimization (GRPO) instead. Our approach, JustGRPO, is minimalist yet surprisingly effective (e.g., 89.1% accuracy on GSM8K) while fully retaining the parallel decoding ability of dLLMs. Project page: this https URL"
    },
    {
      "title": "HomoFM: Deep Homography Estimation with Flow Matching",
      "url": "https://arxiv.org/abs/2601.18222",
      "date": "2026-01-26",
      "authors_text": "Mengfan He, Liangzheng Sun, Chunyu Li, Ziyang Meng",
      "is_highlight": false,
      "score": 49.0,
      "summary": "HomoFM introduces a flow matching technique for deep homography estimation, enhancing accuracy and robustness against domain shifts through velocity field learning and domain-invariant representations.",
      "debug_abstract": "Deep homography estimation has broad applications in computer vision and robotics. Remarkable progresses have been achieved while the existing methods typically treat it as a direct regression or iterative refinement problem and often struggling to capture complex geometric transformations or generalize across different domains. In this work, we propose HomoFM, a new framework that introduces the flow matching technique from generative modeling into the homography estimation task for the first time. Unlike the existing methods, we formulate homography estimation problem as a velocity field learning problem. By modeling a continuous and point-wise velocity field that transforms noisy distributions into registered coordinates, the proposed network recovers high-precision transformations through a conditional flow trajectory. Furthermore, to address the challenge of domain shifts issue, e.g., the cases of multimodal matching or varying illumination scenarios, we integrate a gradient reversal layer (GRL) into the feature extraction backbone. This domain adaptation strategy explicitly constrains the encoder to learn domain-invariant representations, significantly enhancing the network&#39;s robustness. Extensive experiments demonstrate the effectiveness of the proposed method, showing that HomoFM outperforms state-of-the-art methods in both estimation accuracy and robustness on standard benchmarks. Code and data resource are available at this https URL."
    },
    {
      "title": "Dep-Search: Learning Dependency-Aware Reasoning Traces with Persistent Memory",
      "url": "https://arxiv.org/abs/2601.18771",
      "date": "2026-01-26",
      "authors_text": "Yanming Liu, Xinyue Peng, Zixuan Yan, Yanxin Shen, Wenjie Xu",
      "is_highlight": false,
      "score": 46.0,
      "summary": "Dep-Search enhances large language models' multi-hop reasoning by integrating dependency-aware search, structured retrieval, and persistent memory, outperforming existing frameworks in complex tasks.",
      "debug_abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in complex reasoning tasks, particularly when augmented with search mechanisms that enable systematic exploration of external knowledge bases. The field has evolved from traditional retrieval-augmented generation (RAG) frameworks to more sophisticated search-based frameworks that orchestrate multi-step reasoning through explicit search strategies. However, existing search frameworks still rely heavily on implicit natural language reasoning to determine search strategies and how to leverage retrieved information across reasoning steps. This reliance on implicit reasoning creates fundamental challenges for managing dependencies between sub-questions, efficiently reusing previously retrieved knowledge, and learning optimal search strategies through reinforcement learning. To address these limitations, we propose Dep-Search, a dependency-aware search framework that advances beyond existing search frameworks by integrating structured reasoning, retrieval, and persistent memory through GRPO. Dep-Search introduces explicit control mechanisms that enable the model to decompose questions with dependency relationships, retrieve information when needed, access previously stored knowledge from memory, and summarize long reasoning contexts into reusable memory entries. Through extensive experiments on seven diverse question answering datasets, we demonstrate that Dep-Search significantly enhances LLMs&#39; ability to tackle complex multi-hop reasoning tasks, achieving substantial improvements over strong baselines across different model scales."
    },
    {
      "title": "FantasyVLN: Unified Multimodal Chain-of-Thought Reasoning for Vision-Language Navigation",
      "url": "https://arxiv.org/abs/2601.13976",
      "date": "2026-01-23",
      "authors_text": "Jing Zuo, Lingzhou Mu, Fan Jiang, Chengcheng Ma, Mu Xu",
      "is_highlight": false,
      "score": 85.0,
      "summary": "FantasyVLN introduces a unified implicit reasoning framework for Vision-Language Navigation that enhances efficiency and success rates by optimizing Chain-of-Thought reasoning without excessive token overhead.",
      "debug_abstract": "Achieving human-level performance in Vision-and-Language Navigation (VLN) requires an embodied agent to jointly understand multimodal instructions and visual-spatial context while reasoning over long action sequences. Recent works, such as NavCoT and NavGPT-2, demonstrate the potential of Chain-of-Thought (CoT) reasoning for improving interpretability and long-horizon planning. Moreover, multimodal extensions like OctoNav-R1 and CoT-VLA further validate CoT as a promising pathway toward human-like navigation reasoning. However, existing approaches face critical drawbacks: purely textual CoTs lack spatial grounding and easily overfit to sparse annotated reasoning steps, while multimodal CoTs incur severe token inflation by generating imagined visual observations, making real-time navigation impractical. In this work, we propose FantasyVLN, a unified implicit reasoning framework that preserves the benefits of CoT reasoning without explicit token overhead. Specifically, imagined visual tokens are encoded into a compact latent space using a pretrained Visual AutoRegressor (VAR) during CoT reasoning training, and the model jointly learns from textual, visual, and multimodal CoT modes under a unified multi-CoT strategy. At inference, our model performs direct instruction-to-action mapping while still enjoying reasoning-aware representations. Extensive experiments on LH-VLN show that our approach achieves reasoning-aware yet real-time navigation, improving success rates and efficiency while reducing inference latency by an order of magnitude compared to explicit CoT methods."
    },
    {
      "title": "TangramPuzzle: Evaluating Multimodal Large Language Models with Compositional Spatial Reasoning",
      "url": "https://arxiv.org/abs/2601.16520",
      "date": "2026-01-23",
      "authors_text": "Daixian Liu, Jiayi Kuang, Yinghui Li, Yangning Li, Di Yin",
      "is_highlight": false,
      "score": 82.0,
      "summary": "The TangramPuzzle benchmark evaluates multimodal large language models' compositional spatial reasoning through precise geometric tasks, revealing their tendency to prioritize silhouette matching over geometric accuracy.",
      "debug_abstract": "Multimodal Large Language Models (MLLMs) have achieved remarkable progress in visual recognition and semantic understanding. Nevertheless, their ability to perform precise compositional spatial reasoning remains largely unexplored. Existing benchmarks often involve relatively simple tasks and rely on semantic approximations or coarse relative positioning, while their evaluation metrics are typically limited and lack rigorous mathematical formulations. To bridge this gap, we introduce TangramPuzzle, a geometry-grounded benchmark designed to evaluate compositional spatial reasoning through the lens of the classic Tangram game. We propose the Tangram Construction Expression (TCE), a symbolic geometric framework that grounds tangram assemblies in exact, machine-verifiable coordinate specifications, to mitigate the ambiguity of visual approximation. We design two complementary tasks: Outline Prediction, which demands inferring global shapes from local components, and End-to-End Code Generation, which requires solving inverse geometric assembly problems. We conduct extensive evaluation experiments on advanced open-source and proprietary models, revealing an interesting insight: MLLMs tend to prioritize matching the target silhouette while neglecting geometric constraints, leading to distortions or deformations of the pieces."
    },
    {
      "title": "AnchoredDream: Zero-Shot 360° Indoor Scene Generation from a Single View via Geometric Grounding",
      "url": "https://arxiv.org/abs/2601.16532",
      "date": "2026-01-23",
      "authors_text": "Runmao Yao, Junsheng Zhou, Zhen Dong, Yu-Shen Liu",
      "is_highlight": false,
      "score": 72.0,
      "summary": "AnchoredDream introduces a zero-shot method for generating 360° indoor scenes from a single image, enhancing appearance consistency and geometric plausibility through a mutual boosting mechanism.",
      "debug_abstract": "Single-view indoor scene generation plays a crucial role in a range of real-world applications. However, generating a complete 360° scene from a single image remains a highly ill-posed and challenging problem. Recent approaches have made progress by leveraging diffusion models and depth estimation networks, yet they still struggle to maintain appearance consistency and geometric plausibility under large viewpoint changes, limiting their effectiveness in full-scene generation. To address this, we propose AnchoredDream, a novel zero-shot pipeline that anchors 360° scene generation on high-fidelity geometry via an appearance-geometry mutual boosting mechanism. Given a single-view image, our method first performs appearance-guided geometry generation to construct a reliable 3D scene layout. Then, we progressively generate the complete scene through a series of modules: warp-and-inpaint, warp-and-refine, post-optimization, and a novel Grouting Block, which ensures seamless transitions between the input view and generated regions. Extensive experiments demonstrate that AnchoredDream outperforms existing methods by a large margin in both appearance consistency and geometric plausibility--all in a zero-shot manner. Our results highlight the potential of geometric grounding for high-quality, zero-shot single-view scene generation."
    },
    {
      "title": "OnlineSI: Taming Large Language Model for Online 3D Understanding and Grounding",
      "url": "https://arxiv.org/abs/2601.16538",
      "date": "2026-01-23",
      "authors_text": "Zixian Liu, Zhaoxi Chen, Liang Pan, Ziwei Liu",
      "is_highlight": false,
      "score": 88.0,
      "summary": "OnlineSI is a framework that enhances Multimodal Large Language Models' spatial understanding in dynamic environments by integrating continuous learning from video streams and 3D point cloud data.",
      "debug_abstract": "In recent years, researchers have increasingly been interested in how to enable Multimodal Large Language Models (MLLM) to possess spatial understanding and reasoning capabilities. However, most existing methods overlook the importance of the ability to continuously work in an ever-changing world, and lack the possibility of deployment on embodied systems in real-world environments. In this work, we introduce OnlineSI, a framework that can continuously improve its spatial understanding of its surroundings given a video stream. Our core idea is to maintain a finite spatial memory to retain past observations, ensuring the computation required for each inference does not increase as the input accumulates. We further integrate 3D point cloud information with semantic information, helping MLLM to better locate and identify objects in the scene. To evaluate our method, we introduce the Fuzzy $F_1$-Score to mitigate ambiguity, and test our method on two representative datasets. Experiments demonstrate the effectiveness of our method, paving the way towards real-world embodied systems."
    },
    {
      "title": "GRIP: Algorithm-Agnostic Machine Unlearning for Mixture-of-Experts via Geometric Router Constraints",
      "url": "https://arxiv.org/abs/2601.16905",
      "date": "2026-01-23",
      "authors_text": "Andy Zhu, Rongzhe Wei, Yupu Gu, Pan Li",
      "is_highlight": false,
      "score": 62.0,
      "summary": "The GRIP framework enables effective machine unlearning in Mixture-of-Experts models by imposing geometric constraints on router updates, ensuring knowledge erasure without sacrificing model utility.",
      "debug_abstract": "Machine unlearning (MU) for large language models has become critical for AI safety, yet existing methods fail to generalize to Mixture-of-Experts (MoE) architectures. We identify that traditional unlearning methods exploit MoE&#39;s architectural vulnerability: they manipulate routers to redirect queries away from knowledgeable experts rather than erasing knowledge, causing a loss of model utility and superficial forgetting. We propose Geometric Routing Invariance Preservation (GRIP), an algorithm-agnostic framework for unlearning for MoE. Our core contribution is a geometric constraint, implemented by projecting router gradient updates into an expert-specific null-space. Crucially, this decouples routing stability from parameter rigidity: while discrete expert selections remain stable for retained knowledge, the continuous router parameters remain plastic within the null space, allowing the model to undergo necessary internal reconfiguration to satisfy unlearning objectives. This forces the unlearning optimization to erase knowledge directly from expert parameters rather than exploiting the superficial router manipulation shortcut. GRIP functions as an adapter, constraining router parameter updates without modifying the underlying unlearning algorithm. Extensive experiments on large-scale MoE models demonstrate that our adapter eliminates expert selection shift (achieving over 95% routing stability) across all tested unlearning methods while preserving their utility. By preventing existing algorithms from exploiting MoE model&#39;s router vulnerability, GRIP adapts existing unlearning research from dense architectures to MoEs."
    },
    {
      "title": "Render-of-Thought: Rendering Textual Chain-of-Thought as Images for Visual Latent Reasoning",
      "url": "https://arxiv.org/abs/2601.14750",
      "date": "2026-01-22",
      "authors_text": "Yifan Wang, Shiyu Li, Peiming Li, Xiaochen Yang, Yang Tang",
      "is_highlight": false,
      "score": 92.5,
      "summary": "The Render-of-Thought framework transforms Chain-of-Thought reasoning into images, enhancing analyzability and efficiency while maintaining competitive performance in reasoning tasks.",
      "debug_abstract": "Chain-of-Thought (CoT) prompting has achieved remarkable success in unlocking the reasoning capabilities of Large Language Models (LLMs). Although CoT prompting enhances reasoning, its verbosity imposes substantial computational overhead. Recent works often focus exclusively on outcome alignment and lack supervision on the intermediate reasoning process. These deficiencies obscure the analyzability of the latent reasoning chain. To address these challenges, we introduce Render-of-Thought (RoT), the first framework to reify the reasoning chain by rendering textual steps into images, making the latent rationale explicit and traceable. Specifically, we leverage the vision encoders of existing Vision Language Models (VLMs) as semantic anchors to align the vision embeddings with the textual space. This design ensures plug-and-play implementation without incurring additional pre-training overhead. Extensive experiments on mathematical and logical reasoning benchmarks demonstrate that our method achieves 3-4x token compression and substantial inference acceleration compared to explicit CoT. Furthermore, it maintains competitive performance against other methods, validating the feasibility of this paradigm. Our code is available at this https URL"
    },
    {
      "title": "LLM-in-Sandbox Elicits General Agentic Intelligence",
      "url": "https://arxiv.org/abs/2601.16206",
      "date": "2026-01-22",
      "authors_text": "Daixuan Cheng, Shaohan Huang, Yuxian Gu, Huatong Song, Guoxin Chen",
      "is_highlight": false,
      "score": 95.0,
      "summary": "The LLM-in-Sandbox framework enables large language models to demonstrate general intelligence in non-code tasks through sandbox exploration and reinforcement learning, achieving robust generalization across various domains.",
      "debug_abstract": "We introduce LLM-in-Sandbox, enabling LLMs to explore within a code sandbox (i.e., a virtual computer), to elicit general intelligence in non-code domains. We first demonstrate that strong LLMs, without additional training, exhibit generalization capabilities to leverage the code sandbox for non-code tasks. For example, LLMs spontaneously access external resources to acquire new knowledge, leverage the file system to handle long contexts, and execute scripts to satisfy formatting requirements. We further show that these agentic capabilities can be enhanced through LLM-in-Sandbox Reinforcement Learning (LLM-in-Sandbox-RL), which uses only non-agentic data to train models for sandbox exploration. Experiments demonstrate that LLM-in-Sandbox, in both training-free and post-trained settings, achieves robust generalization spanning mathematics, physics, chemistry, biomedicine, long-context understanding, and instruction following. Finally, we analyze LLM-in-Sandbox&#39;s efficiency from computational and system perspectives, and open-source it as a Python package to facilitate real-world deployment."
    },
    {
      "title": "Collision-Free Humanoid Traversal in Cluttered Indoor Scenes",
      "url": "https://arxiv.org/abs/2601.16035",
      "date": "2026-01-22",
      "authors_text": "Han Xue, Sikai Liang, Zhikai Zhang, Zicheng Zeng, Yun Liu",
      "is_highlight": false,
      "score": 81.2,
      "summary": "The paper presents Humanoid Potential Field (HumanoidPF) for enabling collision-free traversal of humanoids in cluttered indoor environments, demonstrating effective RL-based learning and real-world applicability.",
      "debug_abstract": "We study the problem of collision-free humanoid traversal in cluttered indoor scenes, such as hurdling over objects scattered on the floor, crouching under low-hanging obstacles, or squeezing through narrow passages. To achieve this goal, the humanoid needs to map its perception of surrounding obstacles with diverse spatial layouts and geometries to the corresponding traversal skills. However, the lack of an effective representation that captures humanoid-obstacle relationships during collision avoidance makes directly learning such mappings difficult. We therefore propose Humanoid Potential Field (HumanoidPF), which encodes these relationships as collision-free motion directions, significantly facilitating RL-based traversal skill learning. We also find that HumanoidPF exhibits a surprisingly negligible sim-to-real gap as a perceptual representation. To further enable generalizable traversal skills through diverse and challenging cluttered indoor scenes, we further propose a hybrid scene generation method, incorporating crops of realistic 3D indoor scenes and procedurally synthesized obstacles. We successfully transfer our policy to the real world and develop a teleoperation system where users could command the humanoid to traverse in cluttered indoor scenes with just a single click. Extensive experiments are conducted in both simulation and the real world to validate the effectiveness of our method. Demos and code can be found in our website: this https URL."
    }
  ],
  "具身智能实验室 (TEA Lab)": [
    {
      "title": "EventFlash: Towards Efficient MLLMs for Event-Based Vision",
      "url": "https://arxiv.org/abs/2602.03230",
      "date": "2026-02-03",
      "authors_text": "Shaoyu Liu, Jianing Li, Guanghui Zhao, Yunjian Zhang, Wen Jiang",
      "is_highlight": false,
      "score": 63.0,
      "summary": "EventFlash introduces an efficient MLLM for event-based vision, leveraging spatiotemporal sparsification and adaptive sampling to enhance performance and reduce computational costs.",
      "teaser_image": "https://arxiv.org/html/2602.03230/x2.png",
      "debug_abstract": "Event-based multimodal large language models (MLLMs) enable robust perception in high-speed and low-light scenarios, addressing key limitations of frame-based MLLMs. However, current event-based MLLMs often rely on dense image-like processing paradigms, overlooking the spatiotemporal sparsity of event streams and resulting in high computational cost. In this paper, we propose EventFlash, a novel and efficient MLLM to explore spatiotemporal token sparsification for reducing data redundancy and accelerating inference. Technically, we build EventMind, a large-scale and scene-diverse dataset with over 500k instruction sets, providing both short and long event stream sequences to support our curriculum training strategy. We then present an adaptive temporal window aggregation module for efficient temporal sampling, which adaptively compresses temporal tokens while retaining key temporal cues. Finally, a sparse density-guided attention module is designed to improve spatial token efficiency by selecting informative regions and suppressing empty or sparse areas. Experimental results show that EventFlash achieves a $12.4\\times$ throughput improvement over the baseline (EventFlash-Zero) while maintaining comparable performance. It supports long-range event stream processing with up to 1,000 bins, significantly outperforming the 5-bin limit of EventGPT. We believe EventFlash serves as an efficient foundation model for event-based vision."
    },
    {
      "title": "InstaDrive: Instance-Aware Driving World Models for Realistic and Consistent Video Generation",
      "url": "https://arxiv.org/abs/2602.03242",
      "date": "2026-02-03",
      "authors_text": "Zhuoran Yang, Xi Guo, Chenjing Ding, Chiyu Wang, Wei Wu",
      "is_highlight": false,
      "score": 84.0,
      "summary": "InstaDrive introduces a framework that enhances realistic driving video generation by ensuring instance-level temporal consistency and spatial geometric fidelity through innovative feature extraction and alignment techniques.",
      "teaser_image": "https://arxiv.org/html/2602.03242/x2.png",
      "debug_abstract": "Autonomous driving relies on robust models trained on high-quality, large-scale multi-view driving videos. While world models offer a cost-effective solution for generating realistic driving videos, they struggle to maintain instance-level temporal consistency and spatial geometric fidelity. To address these challenges, we propose InstaDrive, a novel framework that enhances driving video realism through two key advancements: (1) Instance Flow Guider, which extracts and propagates instance features across frames to enforce temporal consistency, preserving instance identity over time. (2) Spatial Geometric Aligner, which improves spatial reasoning, ensures precise instance positioning, and explicitly models occlusion hierarchies. By incorporating these instance-aware mechanisms, InstaDrive achieves state-of-the-art video generation quality and enhances downstream autonomous driving tasks on the nuScenes dataset. Additionally, we utilize CARLA&#39;s autopilot to procedurally and stochastically simulate rare but safety-critical driving scenarios across diverse maps and regions, enabling rigorous safety evaluation for autonomous systems. Our project page is this https URL."
    },
    {
      "title": "Learning Physics-Grounded 4D Dynamics with Neural Gaussian Force Fields",
      "url": "https://arxiv.org/abs/2602.00148",
      "date": "2026-01-29",
      "authors_text": "Shiqian Li, Ruihong Shen, Junfeng Ni, Chang Pan, Chi Zhang",
      "is_highlight": false,
      "score": 82.0,
      "summary": "The Neural Gaussian Force Field (NGFF) framework enables rapid generation of interactive, physically realistic 4D videos from multi-view RGB inputs, enhancing robustness and generalization in physical dynamics.",
      "teaser_image": "https://arxiv.org/html/2602.00148/x1.png",
      "debug_abstract": "Predicting physical dynamics from raw visual data remains a major challenge in AI. While recent video generation models have achieved impressive visual quality, they still cannot consistently generate physically plausible videos due to a lack of modeling of physical laws. Recent approaches combining 3D Gaussian splatting and physics engines can produce physically plausible videos, but are hindered by high computational costs in both reconstruction and simulation, and often lack robustness in complex real-world scenarios. To address these issues, we introduce Neural Gaussian Force Field (NGFF), an end-to-end neural framework that integrates 3D Gaussian perception with physics-based dynamic modeling to generate interactive, physically realistic 4D videos from multi-view RGB inputs, achieving two orders of magnitude faster than prior Gaussian simulators. To support training, we also present GSCollision, a 4D Gaussian dataset featuring diverse materials, multi-object interactions, and complex scenes, totaling over 640k rendered physical videos (~4 TB). Evaluations on synthetic and real 3D scenarios show NGFF&#39;s strong generalization and robustness in physical reasoning, advancing video prediction towards physics-grounded world models."
    },
    {
      "title": "How Far Are LLMs from Professional Poker Players? Revisiting Game-Theoretic Reasoning with Agentic Tool Use",
      "url": "https://arxiv.org/abs/2602.00528",
      "date": "2026-01-31",
      "authors_text": "Minhua Lin, Enyan Dai, Hui Liu, Xianfeng Tang, Yuliang Yan",
      "is_highlight": false,
      "score": 55.0,
      "summary": "This paper evaluates LLMs in poker, revealing their strategic reasoning flaws and proposing ToolPoker, which enhances gameplay and reasoning through integrated external solvers.",
      "teaser_image": "https://arxiv.org/html/2602.00528/x2.png",
      "debug_abstract": "As Large Language Models (LLMs) are increasingly applied in high-stakes domains, their ability to reason strategically under uncertainty becomes critical. Poker provides a rigorous testbed, requiring not only strong actions but also principled, game-theoretic reasoning. In this paper, we conduct a systematic study of LLMs in multiple realistic poker tasks, evaluating both gameplay outcomes and reasoning traces. Our analysis reveals LLMs fail to compete against traditional algorithms and identifies three recurring flaws: reliance on heuristics, factual misunderstandings, and a &#34;knowing-doing&#34; gap where actions diverge from reasoning. An initial attempt with behavior cloning and step-level reinforcement learning improves reasoning style but remains insufficient for accurate game-theoretic play. Motivated by these limitations, we propose ToolPoker, a tool-integrated reasoning framework that combines external solvers for GTO-consistent actions with more precise professional-style explanations. Experiments demonstrate that ToolPoker achieves state-of-the-art gameplay while producing reasoning traces that closely reflect game-theoretic principles."
    },
    {
      "title": "V2X-DSC: Multi-Agent Collaborative Perception with Distributed Source Coding Guided Communication",
      "url": "https://arxiv.org/abs/2602.00687",
      "date": "2026-01-31",
      "authors_text": "Yuankun Zeng, Shaohui Li, Zhi Li, Shulan Ruan, Yu Liu",
      "is_highlight": false,
      "score": 76.0,
      "summary": "V2X-DSC introduces a Conditional Codec for efficient multi-agent collaborative perception by compressing and reconstructing BEV features, optimizing bandwidth usage while maintaining accuracy.",
      "teaser_image": "https://arxiv.org/html/2602.00687/figs/overview.jpg",
      "debug_abstract": "Collaborative perception improves 3D understanding by fusing multi-agent observations, yet intermediate-feature sharing faces strict bandwidth constraints as dense BEV features saturate V2X links. We observe that collaborators view the same physical world, making their features strongly correlated; thus receivers only need innovation beyond their local context. Revisiting this from a distributed source coding perspective, we propose V2X-DSC, a framework with a Conditional Codec (DCC) for bandwidth-constrained fusion. The sender compresses BEV features into compact codes, while the receiver performs conditional reconstruction using its local features as side information, allocating bits to complementary cues rather than redundant content. This conditional structure regularizes learning, encouraging incremental representation and yielding lower-noise features. Experiments on DAIR-V2X, OPV2V, and V2X-Real demonstrate state-of-the-art accuracy-bandwidth trade-offs under KB-level communication, and generalizes as a plug-and-play communication layer across multiple fusion backbones."
    },
    {
      "title": "Resource-Efficient Reinforcement for Reasoning Large Language Models via Dynamic One-Shot Policy Refinement",
      "url": "https://arxiv.org/abs/2602.00815",
      "date": "2026-01-31",
      "authors_text": "Yunjian Zhang, Sudong Wang, Yang Li, Peiran Xu, Conghao Zhou",
      "is_highlight": false,
      "score": 57.0,
      "summary": "The paper introduces Dynamic One-Shot Policy Refinement (DoPR), a resource-efficient reinforcement learning strategy that enhances reasoning in large language models while significantly reducing training costs.",
      "teaser_image": "https://arxiv.org/html/2602.00815/dopr.png",
      "debug_abstract": "Large language models (LLMs) have exhibited remarkable performance on complex reasoning tasks, with reinforcement learning under verifiable rewards (RLVR) emerging as a principled framework for aligning model behavior with reasoning chains. Despite its promise, RLVR remains prohibitively resource-intensive, requiring extensive reward signals and incurring substantial rollout costs during training. In this work, we revisit the fundamental question of data and compute efficiency in RLVR. We first establish a theoretical lower bound on the sample complexity required to unlock reasoning capabilities, and empirically validate that strong performance can be achieved with a surprisingly small number of training instances. To tackle the computational burden, we propose Dynamic One-Shot Policy Refinement (DoPR), an uncertainty-aware RL strategy that dynamically selects a single informative training sample per batch for policy updates, guided by reward volatility and exploration-driven acquisition. DoPR reduces rollout overhead by nearly an order of magnitude while preserving competitive reasoning accuracy, offering a scalable and resource-efficient solution for LLM post-training. This approach offers a practical path toward more efficient and accessible RL-based training for reasoning-intensive LLM applications."
    },
    {
      "title": "Making Avatars Interact: Towards Text-Driven Human-Object Interaction for Controllable Talking Avatars",
      "url": "https://arxiv.org/abs/2602.01538",
      "date": "2026-02-02",
      "authors_text": "Youliang Zhang, Zhengguang Zhou, Zhentao Yu, Ziyao Huang, Teng Hu",
      "is_highlight": false,
      "score": 46.0,
      "summary": "The paper presents InteractAvatar, a dual-stream framework for generating talking avatars that perform text-driven human-object interactions by decoupling perception and planning from video synthesis.",
      "teaser_image": "https://arxiv.org/html/2602.01538/x1.png",
      "debug_abstract": "Generating talking avatars is a fundamental task in video generation. Although existing methods can generate full-body talking avatars with simple human motion, extending this task to grounded human-object interaction (GHOI) remains an open challenge, requiring the avatar to perform text-aligned interactions with surrounding objects. This challenge stems from the need for environmental perception and the control-quality dilemma in GHOI generation. To address this, we propose a novel dual-stream framework, InteractAvatar, which decouples perception and planning from video synthesis for grounded human-object interaction. Leveraging detection to enhance environmental perception, we introduce a Perception and Interaction Module (PIM) to generate text-aligned interaction motions. Additionally, an Audio-Interaction Aware Generation Module (AIM) is proposed to synthesize vivid talking avatars performing object interactions. With a specially designed motion-to-video aligner, PIM and AIM share a similar network structure and enable parallel co-generation of motions and plausible videos, effectively mitigating the control-quality dilemma. Finally, we establish a benchmark, GroundedInter, for evaluating GHOI video generation. Extensive experiments and comparisons demonstrate the effectiveness of our method in generating grounded human-object interactions for talking avatars. Project page: this https URL"
    },
    {
      "title": "Reasoning with Autoregressive-Diffusion Collaborative Thoughts",
      "url": "https://arxiv.org/abs/2602.01608",
      "date": "2026-02-02",
      "authors_text": "Mu Yuan, Liekang Zeng, Guoliang Xing, Lan Zhang, Yunhao Liu",
      "is_highlight": false,
      "score": 41.0,
      "summary": "The paper presents Collaborative Thoughts, a framework integrating autoregressive and diffusion models for enhanced spatial reasoning and generation through iterative feedback and joint reasoning.",
      "teaser_image": "https://arxiv.org/html/2602.01608/x2.png",
      "debug_abstract": "Autoregressive and diffusion models represent two complementary generative paradigms. Autoregressive models excel at sequential planning and constraint composition, yet struggle with tasks that require explicit spatial or physical grounding. Diffusion models, in contrast, capture rich spatial structure through high-dimensional generation, but lack the stepwise logical control needed to satisfy complex, multi-stage constraints or to reliably identify and correct errors. We introduce Collaborative Thoughts, a unified collaborative framework that enables autoregressive and diffusion models to reason and generate jointly through a closed-loop interaction. In Collaborative Thoughts, autoregressive models perform structured planning and constraint management, diffusion models instantiate these constraints as intermediate visual thoughts, and a vision-based critic module evaluates whether the visual thoughts satisfy the intended structural and physical requirements. This feedback is then used to iteratively refine subsequent planning and generation steps, mitigating error propagation across modalities. Importantly, Collaborative Thoughts uses the same collaborative loop regardless of whether the task is autoregressive question answering or diffusion-based visual generation. Through representative examples, we illustrate how Collaborative Thoughts can improve the reliability of spatial reasoning and the controllability of generation."
    },
    {
      "title": "From Frames to Sequences: Temporally Consistent Human-Centric Dense Prediction",
      "url": "https://arxiv.org/abs/2602.01661",
      "date": "2026-02-02",
      "authors_text": "Xingyu Miao, Junting Dong, Qin Zhao, Yuhang Yang, Junhao Chen",
      "is_highlight": false,
      "score": 43.0,
      "summary": "This paper presents a novel synthetic data pipeline and a ViT-based model for achieving temporally consistent human-centric dense prediction in video sequences, outperforming existing methods.",
      "teaser_image": "https://arxiv.org/html/2602.01661/x2.png",
      "debug_abstract": "In this work, we focus on the challenge of temporally consistent human-centric dense prediction across video sequences. Existing models achieve strong per-frame accuracy but often flicker under motion, occlusion, and lighting changes, and they rarely have paired human video supervision for multiple dense tasks. We address this gap with a scalable synthetic data pipeline that generates photorealistic human frames and motion-aligned sequences with pixel-accurate depth, normals, and masks. Unlike prior static data synthetic pipelines, our pipeline provides both frame-level labels for spatial learning and sequence-level supervision for temporal learning. Building on this, we train a unified ViT-based dense predictor that (i) injects an explicit human geometric prior via CSE embeddings and (ii) improves geometry-feature reliability with a lightweight channel reweighting module after feature fusion. Our two-stage training strategy, combining static pretraining with dynamic sequence supervision, enables the model first to acquire robust spatial representations and then refine temporal consistency across motion-aligned sequences. Extensive experiments show that we achieve state-of-the-art performance on THuman2.1 and Hi4D and generalize effectively to in-the-wild videos."
    },
    {
      "title": "ForSim: Stepwise Forward Simulation for Traffic Policy Fine-Tuning",
      "url": "https://arxiv.org/abs/2602.01916",
      "date": "2026-02-02",
      "authors_text": "Keyu Chen, Wenchao Sun, Hao Cheng, Zheng Fu, Sifa Zheng",
      "is_highlight": true,
      "score": 29.0,
      "summary": "ForSim introduces a stepwise closed-loop simulation method that enhances traffic policy fine-tuning by improving agent interactions and preserving multimodal behaviors in autonomous driving.",
      "teaser_image": "https://arxiv.org/html/2602.01916/x2.png",
      "debug_abstract": "As the foundation of closed-loop training and evaluation in autonomous driving, traffic simulation still faces two fundamental challenges: covariate shift introduced by open-loop imitation learning and limited capacity to reflect the multimodal behaviors observed in real-world traffic. Although recent frameworks such as RIFT have partially addressed these issues through group-relative optimization, their forward simulation procedures remain largely non-reactive, leading to unrealistic agent interactions within the virtual domain and ultimately limiting simulation fidelity. To address these issues, we propose ForSim, a stepwise closed-loop forward simulation paradigm. At each virtual timestep, the traffic agent propagates the virtual candidate trajectory that best spatiotemporally matches the reference trajectory through physically grounded motion dynamics, thereby preserving multimodal behavioral diversity while ensuring intra-modality consistency. Other agents are updated with stepwise predictions, yielding coherent and interaction-aware evolution. When incorporated into the RIFT traffic simulation framework, ForSim operates in conjunction with group-relative optimization to fine-tune traffic policy. Extensive experiments confirm that this integration consistently improves safety while maintaining efficiency, realism, and comfort. These results underscore the importance of modeling closed-loop multimodal interactions within forward simulation and enhance the fidelity and reliability of traffic simulation for autonomous driving. Project Page: this https URL"
    },
    {
      "title": "Small Generalizable Prompt Predictive Models Can Steer Efficient RL Post-Training of Large Reasoning Models",
      "url": "https://arxiv.org/abs/2602.01970",
      "date": "2026-02-02",
      "authors_text": "Yun Qu, Qi Wang, Yixiu Mao, Heming Zou, Yuhang Jiang",
      "is_highlight": false,
      "score": 26.0,
      "summary": "The paper presents Generalizable Predictive Prompt Selection (GPS), a method that enhances reinforcement learning efficiency in large language models by utilizing a lightweight generative model for prompt selection.",
      "teaser_image": "https://arxiv.org/html/2602.01970/x1.png",
      "debug_abstract": "Reinforcement learning enhances the reasoning capabilities of large language models but often involves high computational costs due to rollout-intensive optimization. Online prompt selection presents a plausible solution by prioritizing informative prompts to improve training efficiency. However, current methods either depend on costly, exact evaluations or construct prompt-specific predictive models lacking generalization across prompts. This study introduces Generalizable Predictive Prompt Selection (GPS), which performs Bayesian inference towards prompt difficulty using a lightweight generative model trained on the shared optimization history. Intermediate-difficulty prioritization and history-anchored diversity are incorporated into the batch acquisition principle to select informative prompt batches. The small predictive model also generalizes at test-time for efficient computational allocation. Experiments across varied reasoning benchmarks indicate GPS&#39;s substantial improvements in training efficiency, final performance, and test-time efficiency over superior baseline methods."
    },
    {
      "title": "TTT-Parkour: Rapid Test-Time Training for Perceptive Robot Parkour",
      "url": "https://arxiv.org/abs/2602.02331",
      "date": "2026-02-02",
      "authors_text": "Shaoting Zhu, Baijun Ye, Jiaxuan Wang, Jiakang Chen, Ziwen Zhuang",
      "is_highlight": false,
      "score": 18.0,
      "summary": "TTT-Parkour introduces a rapid test-time training framework enabling humanoid robots to effectively navigate complex terrains through efficient real-to-sim-to-real learning and geometry reconstruction.",
      "teaser_image": "https://arxiv.org/html/2602.02331/x1.png",
      "debug_abstract": "Achieving highly dynamic humanoid parkour on unseen, complex terrains remains a challenge in robotics. Although general locomotion policies demonstrate capabilities across broad terrain distributions, they often struggle with arbitrary and highly challenging environments. To overcome this limitation, we propose a real-to-sim-to-real framework that leverages rapid test-time training (TTT) on novel terrains, significantly enhancing the robot&#39;s capability to traverse extremely difficult geometries. We adopt a two-stage end-to-end learning paradigm: a policy is first pre-trained on diverse procedurally generated terrains, followed by rapid fine-tuning on high-fidelity meshes reconstructed from real-world captures. Specifically, we develop a feed-forward, efficient, and high-fidelity geometry reconstruction pipeline using RGB-D inputs, ensuring both speed and quality during test-time training. We demonstrate that TTT-Parkour empowers humanoid robots to master complex obstacles, including wedges, stakes, boxes, trapezoids, and narrow beams. The whole pipeline of capturing, reconstructing, and test-time training requires less than 10 minutes on most tested terrains. Extensive experiments show that the policy after test-time training exhibits robust zero-shot sim-to-real transfer capability."
    },
    {
      "title": "Context Learning for Multi-Agent Discussion",
      "url": "https://arxiv.org/abs/2602.02350",
      "date": "2026-02-02",
      "authors_text": "Xingyuan Hua, Sheng Yue, Xinyi Li, Yizhe Zhao, Jinrui Zhang",
      "is_highlight": false,
      "score": 17.0,
      "summary": "The paper presents M2CL, a multi-LLM context learning method that enhances coherence in Multi-Agent Discussions, improving problem-solving performance by 20%-50% over existing approaches.",
      "teaser_image": "https://arxiv.org/html/2602.02350/x2.png",
      "debug_abstract": "Multi-Agent Discussion (MAD) has garnered increasing attention very recently, where multiple LLM instances collaboratively solve problems via structured discussion. However, we find that current MAD methods easily suffer from discussion inconsistency, LLMs fail to reach a coherent solution, due to the misalignment between their individual this http URL this paper, we introduce a multi-LLM context learning method (M2CL) that learns a context generator for each agent, capable of dynamically generating context instructions per discussion round via automatic information organization and refinement. Specifically, inspired by our theoretical insights on the context instruction, M2CL train the generators to control context coherence and output discrepancies via a carefully crafted self-adaptive this http URL enables LLMs to avoid premature convergence on majority noise and progressively reach the correct consensus. We evaluate M2CL on challenging tasks, including academic reasoning, embodied tasks, and mobile control. The results show that the performance of M2CL significantly surpasses existing methods by 20%--50%, while enjoying favorable transferability and computational efficiency."
    },
    {
      "title": "UV-M3TL: A Unified and Versatile Multimodal Multi-Task Learning Framework for Assistive Driving Perception",
      "url": "https://arxiv.org/abs/2602.01594",
      "date": "2026-02-02",
      "authors_text": "Wenzhuo Liu, Qiannan Guo, Zhen Wang, Wenshuo Wang, Lei Yang",
      "is_highlight": false,
      "score": 8.0,
      "summary": "The UV-M3TL framework enhances ADAS by effectively recognizing driver behavior and context through dual-branch multimodal embedding and adaptive loss, achieving state-of-the-art performance across multiple tasks.",
      "teaser_image": "https://arxiv.org/html/2602.01594/x2.png",
      "debug_abstract": "Advanced Driver Assistance Systems (ADAS) need to understand human driver behavior while perceiving their navigation context, but jointly learning these heterogeneous tasks would cause inter-task negative transfer and impair system performance. Here, we propose a Unified and Versatile Multimodal Multi-Task Learning (UV-M3TL) framework to simultaneously recognize driver behavior, driver emotion, vehicle behavior, and traffic context, while mitigating inter-task negative transfer. Our framework incorporates two core components: dual-branch spatial channel multimodal embedding (DB-SCME) and adaptive feature-decoupled multi-task loss (AFD-Loss). DB-SCME enhances cross-task knowledge transfer while mitigating task conflicts by employing a dual-branch structure to explicitly model salient task-shared and task-specific features. AFD-Loss improves the stability of joint optimization while guiding the model to learn diverse multi-task representations by introducing an adaptive weighting mechanism based on learning dynamics and feature decoupling constraints. We evaluate our method on the AIDE dataset, and the experimental results demonstrate that UV-M3TL achieves state-of-the-art performance across all four tasks. To further prove the versatility, we evaluate UV-M3TL on additional public multi-task perception benchmarks (BDD100K, CityScapes, NYUD-v2, and PASCAL-Context), where it consistently delivers strong performance across diverse task combinations, attaining state-of-the-art results on most tasks."
    },
    {
      "title": "Enhancing Indoor Occupancy Prediction via Sparse Query-Based Multi-Level Consistent Knowledge Distillation",
      "url": "https://arxiv.org/abs/2602.02318",
      "date": "2026-02-02",
      "authors_text": "Xiang Li, Yupeng Zheng, Pengfei Li, Yilun Chen, Ya-Qin Zhang",
      "is_highlight": false,
      "score": 0,
      "summary": "DiScene introduces a sparse query-based framework for efficient and robust indoor occupancy prediction through multi-level knowledge distillation, achieving state-of-the-art performance and faster inference.",
      "teaser_image": "https://arxiv.org/html/2602.02318/x2.png",
      "debug_abstract": "Occupancy prediction provides critical geometric and semantic understanding for robotics but faces efficiency-accuracy trade-offs. Current dense methods suffer computational waste on empty voxels, while sparse query-based approaches lack robustness in diverse and complex indoor scenes. In this paper, we propose DiScene, a novel sparse query-based framework that leverages multi-level distillation to achieve efficient and robust occupancy prediction. In particular, our method incorporates two key innovations: (1) a Multi-level Consistent Knowledge Distillation strategy, which transfers hierarchical representations from large teacher models to lightweight students through coordinated alignment across four levels, including encoder-level feature alignment, query-level feature matching, prior-level spatial guidance, and anchor-level high-confidence knowledge transfer and (2) a Teacher-Guided Initialization policy, employing optimized parameter warm-up to accelerate model convergence. Validated on the Occ-Scannet benchmark, DiScene achieves 23.2 FPS without depth priors while outperforming our baseline method, OPUS, by 36.1% and even better than the depth-enhanced version, OPUS†. With depth integration, DiScene† attains new SOTA performance, surpassing EmbodiedOcc by 3.7% with 1.62$\\times$ faster inference speed. Furthermore, experiments on the Occ3D-nuScenes benchmark and in-the-wild scenarios demonstrate the versatility of our approach in various environments. Code and models can be accessed at this https URL."
    },
    {
      "title": "InspecSafe-V1: A Multimodal Benchmark for Safety Assessment in Industrial Inspection Scenarios",
      "url": "https://arxiv.org/abs/2601.21173",
      "date": "2026-01-29",
      "authors_text": "Zeyi Liu, Shuang Liu, Jihai Min, Zhaoheng Zhang, Jun Cen",
      "is_highlight": false,
      "score": 82.0,
      "summary": "InspecSafe-V1 is a multimodal benchmark dataset for safety assessment in industrial inspections, featuring real-world data from diverse scenarios and multiple sensing modalities.",
      "teaser_image": "https://arxiv.org/html/2601.21173/x1.png",
      "debug_abstract": "With the rapid development of industrial intelligence and unmanned inspection, reliable perception and safety assessment for AI systems in complex and dynamic industrial sites has become a key bottleneck for deploying predictive maintenance and autonomous inspection. Most public datasets remain limited by simulated data sources, single-modality sensing, or the absence of fine-grained object-level annotations, which prevents robust scene understanding and multimodal safety reasoning for industrial foundation models. To address these limitations, InspecSafe-V1 is released as the first multimodal benchmark dataset for industrial inspection safety assessment that is collected from routine operations of real inspection robots in real-world environments. InspecSafe-V1 covers five representative industrial scenarios, including tunnels, power facilities, sintering equipment, oil and gas petrochemical plants, and coal conveyor trestles. The dataset is constructed from 41 wheeled and rail-mounted inspection robots operating at 2,239 valid inspection sites, yielding 5,013 inspection instances. For each instance, pixel-level segmentation annotations are provided for key objects in visible-spectrum images. In addition, a semantic scene description and a corresponding safety level label are provided according to practical inspection tasks. Seven synchronized sensing modalities are further included, including infrared video, audio, depth point clouds, radar point clouds, gas measurements, temperature, and humidity, to support multimodal anomaly recognition, cross-modal fusion, and comprehensive safety assessment in industrial environments."
    },
    {
      "title": "Listen, Look, Drive: Coupling Audio Instructions for User-aware VLA-based Autonomous Driving",
      "url": "https://arxiv.org/abs/2601.12142",
      "date": "2026-01-28",
      "authors_text": "Ziang Guo, Feng Yang, Xuefeng Zhang, Jiaqi Guo, Kun Zhao",
      "is_highlight": false,
      "score": 88.0,
      "summary": "EchoVLA enhances autonomous driving by integrating audio instructions with visual inputs, allowing for user influence and emotional context, significantly improving navigation accuracy and safety.",
      "teaser_image": "https://arxiv.org/html/2601.12142/Figs/overview.png",
      "debug_abstract": "Vision Language Action (VLA) models promise an open-vocabulary interface that can translate perceptual ambiguity into semantically grounded driving decisions, yet they still treat language as a static prior fixed at inference time. As a result, the model must infer continuously shifting objectives from pixels alone, yielding delayed or overly conservative maneuvers. We argue that effective VLAs for autonomous driving need an online channel in which users can influence driving with specific intentions. To this end, we present EchoVLA, a user-aware VLA that couples camera streams with in situ audio instructions. We augment the nuScenes dataset with temporally aligned, intent-specific speech commands generated by converting ego-motion descriptions into synthetic audios. Further, we compose emotional speech-trajectory pairs into a multimodal Chain-of-Thought (CoT) for fine-tuning a Multimodal Large Model (MLM) based on Qwen2.5-Omni. Specifically, we synthesize the audio-augmented dataset with different emotion types paired with corresponding driving behaviors, leveraging the emotional cues embedded in tone, pitch, and speech tempo to reflect varying user states, such as urgent or hesitant intentions, thus enabling our EchoVLA to interpret not only the semantic content but also the emotional context of audio commands for more nuanced and emotionally adaptive driving behavior. In open-loop benchmarks, our approach reduces the average L2 error by $59.4\\%$ and the collision rate by $74.4\\%$ compared to the baseline of vision-only perception. More experiments on nuScenes dataset validate that EchoVLA not only steers the trajectory through audio instructions, but also modulates driving behavior in response to the emotions detected in the user&#39;s speech."
    },
    {
      "title": "HINT: Hierarchical Interaction Modeling for Autoregressive Multi-Human Motion Generation",
      "url": "https://arxiv.org/abs/2601.20383",
      "date": "2026-01-28",
      "authors_text": "Mengge Liu, Yan Di, Gu Wang, Yun Qu, Dekai Zhu",
      "is_highlight": false,
      "score": 78.0,
      "summary": "HINT introduces a novel autoregressive framework for multi-human motion generation, enhancing interaction modeling and performance with variable agent counts and long text guidance.",
      "teaser_image": "https://arxiv.org/html/2601.20383/x2.png",
      "debug_abstract": "Text-driven multi-human motion generation with complex interactions remains a challenging problem. Despite progress in performance, existing offline methods that generate fixed-length motions with a fixed number of agents, are inherently limited in handling long or variable text, and varying agent counts. These limitations naturally encourage autoregressive formulations, which predict future motions step by step conditioned on all past trajectories and current text guidance. In this work, we introduce HINT, the first autoregressive framework for multi-human motion generation with Hierarchical INTeraction modeling in diffusion. First, HINT leverages a disentangled motion representation within a canonicalized latent space, decoupling local motion semantics from inter-person interactions. This design facilitates direct adaptation to varying numbers of human participants without requiring additional refinement. Second, HINT adopts a sliding-window strategy for efficient online generation, and aggregates local within-window and global cross-window conditions to capture past human history, inter-person dependencies, and align with text guidance. This strategy not only enables fine-grained interaction modeling within each window but also preserves long-horizon coherence across all the long sequence. Extensive experiments on public benchmarks demonstrate that HINT matches the performance of strong offline models and surpasses autoregressive baselines. Notably, on InterHuman, HINT achieves an FID of 3.100, significantly improving over the previous state-of-the-art score of 5.154."
    },
    {
      "title": "Self-Supervised Path Planning in Unstructured Environments via Global-Guided Differentiable Hard Constraint Projection",
      "url": "https://arxiv.org/abs/2601.19354",
      "date": "2026-01-27",
      "authors_text": "Ziqian Wang, Chenxi Fang, Zhen Zhang",
      "is_highlight": false,
      "score": 90.0,
      "summary": "This paper presents a self-supervised framework for autonomous navigation in unstructured environments, utilizing a differentiable projection layer to ensure safety and efficiency in path planning.",
      "debug_abstract": "Deploying deep learning agents for autonomous navigation in unstructured environments faces critical challenges regarding safety, data scarcity, and limited computational resources. Traditional solvers often suffer from high latency, while emerging learning-based approaches struggle to ensure deterministic feasibility. To bridge the gap from embodied to embedded intelligence, we propose a self-supervised framework incorporating a differentiable hard constraint projection layer for runtime assurance. To mitigate data scarcity, we construct a Global-Guided Artificial Potential Field (G-APF), which provides dense supervision signals without manual labeling. To enforce actuator limitations and geometric constraints efficiently, we employ an adaptive neural projection layer, which iteratively rectifies the coarse network output onto the feasible manifold. Extensive benchmarks on a test set of 20,000 scenarios demonstrate an 88.75\\% success rate, substantiating the enhanced operational safety. Closed-loop experiments in CARLA further validate the physical realizability of the planned paths under dynamic constraints. Furthermore, deployment verification on an NVIDIA Jetson Orin NX confirms an inference latency of 94 ms, showing real-time feasibility on resource-constrained embedded hardware. This framework offers a generalized paradigm for embedding physical laws into neural architectures, providing a viable direction for solving constrained optimization in mechatronics. Source code is available at: this https URL."
    },
    {
      "title": "AtomMem : Learnable Dynamic Agentic Memory with Atomic Memory Operation",
      "url": "https://arxiv.org/abs/2601.08323",
      "date": "2026-01-27",
      "authors_text": "Yupeng Huo, Yaxi Lu, Zhong Zhang, Haotian Chen, Yankai Lin",
      "is_highlight": false,
      "score": 82.0,
      "summary": "AtomMem introduces a learnable dynamic memory framework that optimizes memory management through atomic CRUD operations, significantly outperforming static methods in long-horizon tasks.",
      "debug_abstract": "Equipping agents with memory is essential for solving real-world long-horizon problems. However, most existing agent memory mechanisms rely on static and hand-crafted workflows. This limits the performance and generalization ability of these memory designs, which highlights the need for a more flexible, learning-based memory framework. In this paper, we propose AtomMem, which reframes memory management as a dynamic decision-making problem. We deconstruct high-level memory processes into fundamental atomic CRUD (Create, Read, Update, Delete) operations, transforming the memory workflow into a learnable decision process. By combining supervised fine-tuning with reinforcement learning, AtomMem learns an autonomous, task-aligned policy to orchestrate memory behaviors tailored to specific task demands. Experimental results across 3 long-context benchmarks demonstrate that the trained AtomMem-8B consistently outperforms prior static-workflow memory methods. Further analysis of training dynamics shows that our learning-based formulation enables the agent to discover structured, task-aligned memory management strategies, highlighting a key advantage over predefined routines."
    },
    {
      "title": "Visual Generation Unlocks Human-Like Reasoning through Multimodal World Models",
      "url": "https://arxiv.org/abs/2601.19834",
      "date": "2026-01-27",
      "authors_text": "Jialong Wu, Xiaoying Zhang, Hongyi Yuan, Xiangcheng Zhang, Tianhao Huang",
      "is_highlight": false,
      "score": 89.0,
      "summary": "This paper demonstrates that visual generation enhances reasoning in multimodal AI, particularly for tasks requiring physical world understanding, outperforming purely verbal approaches.",
      "debug_abstract": "Humans construct internal world models and reason by manipulating the concepts within these models. Recent advances in AI, particularly chain-of-thought (CoT) reasoning, approximate such human cognitive abilities, where world models are believed to be embedded within large language models. Expert-level performance in formal and abstract domains such as mathematics and programming has been achieved in current systems by relying predominantly on verbal reasoning. However, they still lag far behind humans in domains like physical and spatial intelligence, which require richer representations and prior knowledge. The emergence of unified multimodal models (UMMs) capable of both verbal and visual generation has therefore sparked interest in more human-like reasoning grounded in complementary multimodal pathways, though their benefits remain unclear. From a world-model perspective, this paper presents the first principled study of when and how visual generation benefits reasoning. Our key position is the visual superiority hypothesis: for certain tasks--particularly those grounded in the physical world--visual generation more naturally serves as world models, whereas purely verbal world models encounter bottlenecks arising from representational limitations or insufficient prior knowledge. Theoretically, we formalize internal world modeling as a core component of CoT reasoning and analyze distinctions among different forms of world models. Empirically, we identify tasks that necessitate interleaved visual-verbal CoT reasoning, constructing a new evaluation suite, VisWorld-Eval. Controlled experiments on a state-of-the-art UMM show that interleaved CoT significantly outperforms purely verbal CoT on tasks that favor visual world modeling, but offers no clear advantage otherwise. Together, this work clarifies the potential of multimodal world modeling for more powerful, human-like multimodal AI."
    },
    {
      "title": "Scalable Exploration for High-Dimensional Continuous Control via Value-Guided Flow",
      "url": "https://arxiv.org/abs/2601.19707",
      "date": "2026-01-27",
      "authors_text": "Yunyue Wei, Chenhui Zuo, Yanan Sui",
      "is_highlight": false,
      "score": 79.0,
      "summary": "Qflex introduces a scalable reinforcement learning method for high-dimensional continuous control that enhances exploration efficiency by aligning actions with learned value gradients.",
      "debug_abstract": "Controlling high-dimensional systems in biological and robotic applications is challenging due to expansive state-action spaces, where effective exploration is critical. Commonly used exploration strategies in reinforcement learning are largely undirected with sharp degradation as action dimensionality grows. Many existing methods resort to dimensionality reduction, which constrains policy expressiveness and forfeits system flexibility. We introduce Q-guided Flow Exploration (Qflex), a scalable reinforcement learning method that conducts exploration directly in the native high-dimensional action space. During training, Qflex traverses actions from a learnable source distribution along a probability flow induced by the learned value function, aligning exploration with task-relevant gradients rather than isotropic noise. Our proposed method substantially outperforms representative online reinforcement learning baselines across diverse high-dimensional continuous-control benchmarks. Qflex also successfully controls a full-body human musculoskeletal model to perform agile, complex movements, demonstrating superior scalability and sample efficiency in very high-dimensional settings. Our results indicate that value-guided flows offer a principled and practical route to exploration at scale."
    },
    {
      "title": "Latent-Space Contrastive Reinforcement Learning for Stable and Efficient LLM Reasoning",
      "url": "https://arxiv.org/abs/2601.17275",
      "date": "2026-01-24",
      "authors_text": "Lianlei Shan, Han Chen, Yixuan Wang, Zhenjie Liu, Wei Li",
      "is_highlight": false,
      "score": 60.0,
      "summary": "The paper introduces DeepLatent Reasoning (DLR), a latent-space contrastive reinforcement learning framework that enhances LLM reasoning by addressing inefficiencies and stability in multi-step tasks.",
      "debug_abstract": "While Large Language Models (LLMs) demonstrate exceptional performance in surface-level text generation, their nature in handling complex multi-step reasoning tasks often remains one of ``statistical fitting&#39;&#39; rather than systematic logical deduction. Traditional Reinforcement Learning (RL) attempts to mitigate this by introducing a ``think-before-speak&#39;&#39; paradigm. However, applying RL directly in high-dimensional, discrete token spaces faces three inherent challenges: sample-inefficient rollouts, high gradient estimation variance, and the risk of catastrophic forgetting. To fundamentally address these structural bottlenecks, we propose \\textbf{DeepLatent Reasoning (DLR)}, a latent-space bidirectional contrastive reinforcement learning framework. This framework shifts the trial-and-error cost from expensive token-level full sequence generation to the continuous latent manifold. Specifically, we introduce a lightweight assistant model to efficiently sample $K$ reasoning chain encodings within the latent space. These encodings are filtered via a dual reward mechanism based on correctness and formatting; only high-value latent trajectories are fed into a \\textbf{frozen main model} for single-pass decoding. To maximize reasoning diversity while maintaining coherence, we design a contrastive learning objective to enable directed exploration within the latent space. Since the main model parameters remain frozen during optimization, this method mathematically eliminates catastrophic forgetting. Experiments demonstrate that under comparable GPU computational budgets, DLR achieves more stable training convergence, supports longer-horizon reasoning chains, and facilitates the sustainable accumulation of reasoning capabilities, providing a viable path toward reliable and scalable reinforcement learning for LLMs."
    },
    {
      "title": "MetaWorld: Skill Transfer and Composition in a Hierarchical World Model for Grounding High-Level Instructions",
      "url": "https://arxiv.org/abs/2601.17507",
      "date": "2026-01-24",
      "authors_text": "Yutong Shen, Hangxu Liu, Kailin Pei, Ruizhe Xia, Tongtong Feng",
      "is_highlight": false,
      "score": 92.0,
      "summary": "MetaWorld introduces a hierarchical world model that enhances humanoid robot loco-manipulation by integrating semantic planning and physical control through expert policy transfer, improving task performance and motion coherence.",
      "debug_abstract": "Humanoid robot loco-manipulation remains constrained by the semantic-physical gap. Current methods face three limitations: Low sample efficiency in reinforcement learning, poor generalization in imitation learning, and physical inconsistency in VLMs. We propose MetaWorld, a hierarchical world model that integrates semantic planning and physical control via expert policy transfer. The framework decouples tasks into a VLM-driven semantic layer and a latent dynamics model operating in a compact state space. Our dynamic expert selection and motion prior fusion mechanism leverages a pre-trained multi-expert policy library as transferable knowledge, enabling efficient online adaptation via a two-stage framework. VLMs serve as semantic interfaces, mapping instructions to executable skills and bypassing symbol grounding. Experiments on Humanoid-Bench show MetaWorld outperforms world model-based RL in task completion and motion coherence. Our code will be found at this https URL"
    },
    {
      "title": "The Flexibility Trap: Why Arbitrary Order Limits Reasoning Potential in Diffusion Language Models",
      "url": "https://arxiv.org/abs/2601.15165",
      "date": "2026-01-26",
      "authors_text": "Zanlin Ni, Shenzhi Wang, Yang Yue, Tianyu Yu, Weilin Zhao",
      "is_highlight": false,
      "score": 45.0,
      "summary": "This paper argues that arbitrary order generation in diffusion language models limits reasoning potential, advocating for a simpler approach using Group Relative Policy Optimization for improved performance.",
      "debug_abstract": "Diffusion Large Language Models (dLLMs) break the rigid left-to-right constraint of traditional LLMs, enabling token generation in arbitrary orders. Intuitively, this flexibility implies a solution space that strictly supersets the fixed autoregressive trajectory, theoretically unlocking superior reasoning potential for general tasks like mathematics and coding. Consequently, numerous works have leveraged reinforcement learning (RL) to elicit the reasoning capability of dLLMs. In this paper, we reveal a counter-intuitive reality: arbitrary order generation, in its current form, narrows rather than expands the reasoning boundary of dLLMs. We find that dLLMs tend to exploit this order flexibility to bypass high-uncertainty tokens that are crucial for exploration, leading to a premature collapse of the solution space. This observation motivates a rethink of RL approaches for dLLMs, where considerable complexities, such as handling combinatorial trajectories and intractable likelihoods, are often devoted to preserving this flexibility. We demonstrate that effective reasoning can be better elicited by intentionally forgoing arbitrary order and applying standard Group Relative Policy Optimization (GRPO) instead. Our approach, JustGRPO, is minimalist yet surprisingly effective (e.g., 89.1% accuracy on GSM8K) while fully retaining the parallel decoding ability of dLLMs. Project page: this https URL"
    },
    {
      "title": "HomoFM: Deep Homography Estimation with Flow Matching",
      "url": "https://arxiv.org/abs/2601.18222",
      "date": "2026-01-26",
      "authors_text": "Mengfan He, Liangzheng Sun, Chunyu Li, Ziyang Meng",
      "is_highlight": false,
      "score": 49.0,
      "summary": "HomoFM introduces a flow matching technique for deep homography estimation, enhancing accuracy and robustness against domain shifts through velocity field learning and domain-invariant representations.",
      "debug_abstract": "Deep homography estimation has broad applications in computer vision and robotics. Remarkable progresses have been achieved while the existing methods typically treat it as a direct regression or iterative refinement problem and often struggling to capture complex geometric transformations or generalize across different domains. In this work, we propose HomoFM, a new framework that introduces the flow matching technique from generative modeling into the homography estimation task for the first time. Unlike the existing methods, we formulate homography estimation problem as a velocity field learning problem. By modeling a continuous and point-wise velocity field that transforms noisy distributions into registered coordinates, the proposed network recovers high-precision transformations through a conditional flow trajectory. Furthermore, to address the challenge of domain shifts issue, e.g., the cases of multimodal matching or varying illumination scenarios, we integrate a gradient reversal layer (GRL) into the feature extraction backbone. This domain adaptation strategy explicitly constrains the encoder to learn domain-invariant representations, significantly enhancing the network&#39;s robustness. Extensive experiments demonstrate the effectiveness of the proposed method, showing that HomoFM outperforms state-of-the-art methods in both estimation accuracy and robustness on standard benchmarks. Code and data resource are available at this https URL."
    },
    {
      "title": "Dep-Search: Learning Dependency-Aware Reasoning Traces with Persistent Memory",
      "url": "https://arxiv.org/abs/2601.18771",
      "date": "2026-01-26",
      "authors_text": "Yanming Liu, Xinyue Peng, Zixuan Yan, Yanxin Shen, Wenjie Xu",
      "is_highlight": false,
      "score": 46.0,
      "summary": "Dep-Search enhances large language models' multi-hop reasoning by integrating dependency-aware search, structured retrieval, and persistent memory, outperforming existing frameworks in complex tasks.",
      "debug_abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in complex reasoning tasks, particularly when augmented with search mechanisms that enable systematic exploration of external knowledge bases. The field has evolved from traditional retrieval-augmented generation (RAG) frameworks to more sophisticated search-based frameworks that orchestrate multi-step reasoning through explicit search strategies. However, existing search frameworks still rely heavily on implicit natural language reasoning to determine search strategies and how to leverage retrieved information across reasoning steps. This reliance on implicit reasoning creates fundamental challenges for managing dependencies between sub-questions, efficiently reusing previously retrieved knowledge, and learning optimal search strategies through reinforcement learning. To address these limitations, we propose Dep-Search, a dependency-aware search framework that advances beyond existing search frameworks by integrating structured reasoning, retrieval, and persistent memory through GRPO. Dep-Search introduces explicit control mechanisms that enable the model to decompose questions with dependency relationships, retrieve information when needed, access previously stored knowledge from memory, and summarize long reasoning contexts into reusable memory entries. Through extensive experiments on seven diverse question answering datasets, we demonstrate that Dep-Search significantly enhances LLMs&#39; ability to tackle complex multi-hop reasoning tasks, achieving substantial improvements over strong baselines across different model scales."
    },
    {
      "title": "FantasyVLN: Unified Multimodal Chain-of-Thought Reasoning for Vision-Language Navigation",
      "url": "https://arxiv.org/abs/2601.13976",
      "date": "2026-01-23",
      "authors_text": "Jing Zuo, Lingzhou Mu, Fan Jiang, Chengcheng Ma, Mu Xu",
      "is_highlight": false,
      "score": 85.0,
      "summary": "FantasyVLN introduces a unified implicit reasoning framework for Vision-Language Navigation that enhances efficiency and success rates by optimizing Chain-of-Thought reasoning without excessive token overhead.",
      "debug_abstract": "Achieving human-level performance in Vision-and-Language Navigation (VLN) requires an embodied agent to jointly understand multimodal instructions and visual-spatial context while reasoning over long action sequences. Recent works, such as NavCoT and NavGPT-2, demonstrate the potential of Chain-of-Thought (CoT) reasoning for improving interpretability and long-horizon planning. Moreover, multimodal extensions like OctoNav-R1 and CoT-VLA further validate CoT as a promising pathway toward human-like navigation reasoning. However, existing approaches face critical drawbacks: purely textual CoTs lack spatial grounding and easily overfit to sparse annotated reasoning steps, while multimodal CoTs incur severe token inflation by generating imagined visual observations, making real-time navigation impractical. In this work, we propose FantasyVLN, a unified implicit reasoning framework that preserves the benefits of CoT reasoning without explicit token overhead. Specifically, imagined visual tokens are encoded into a compact latent space using a pretrained Visual AutoRegressor (VAR) during CoT reasoning training, and the model jointly learns from textual, visual, and multimodal CoT modes under a unified multi-CoT strategy. At inference, our model performs direct instruction-to-action mapping while still enjoying reasoning-aware representations. Extensive experiments on LH-VLN show that our approach achieves reasoning-aware yet real-time navigation, improving success rates and efficiency while reducing inference latency by an order of magnitude compared to explicit CoT methods."
    },
    {
      "title": "TangramPuzzle: Evaluating Multimodal Large Language Models with Compositional Spatial Reasoning",
      "url": "https://arxiv.org/abs/2601.16520",
      "date": "2026-01-23",
      "authors_text": "Daixian Liu, Jiayi Kuang, Yinghui Li, Yangning Li, Di Yin",
      "is_highlight": false,
      "score": 82.0,
      "summary": "The TangramPuzzle benchmark evaluates multimodal large language models' compositional spatial reasoning through precise geometric tasks, revealing their tendency to prioritize silhouette matching over geometric accuracy.",
      "debug_abstract": "Multimodal Large Language Models (MLLMs) have achieved remarkable progress in visual recognition and semantic understanding. Nevertheless, their ability to perform precise compositional spatial reasoning remains largely unexplored. Existing benchmarks often involve relatively simple tasks and rely on semantic approximations or coarse relative positioning, while their evaluation metrics are typically limited and lack rigorous mathematical formulations. To bridge this gap, we introduce TangramPuzzle, a geometry-grounded benchmark designed to evaluate compositional spatial reasoning through the lens of the classic Tangram game. We propose the Tangram Construction Expression (TCE), a symbolic geometric framework that grounds tangram assemblies in exact, machine-verifiable coordinate specifications, to mitigate the ambiguity of visual approximation. We design two complementary tasks: Outline Prediction, which demands inferring global shapes from local components, and End-to-End Code Generation, which requires solving inverse geometric assembly problems. We conduct extensive evaluation experiments on advanced open-source and proprietary models, revealing an interesting insight: MLLMs tend to prioritize matching the target silhouette while neglecting geometric constraints, leading to distortions or deformations of the pieces."
    },
    {
      "title": "AnchoredDream: Zero-Shot 360° Indoor Scene Generation from a Single View via Geometric Grounding",
      "url": "https://arxiv.org/abs/2601.16532",
      "date": "2026-01-23",
      "authors_text": "Runmao Yao, Junsheng Zhou, Zhen Dong, Yu-Shen Liu",
      "is_highlight": false,
      "score": 72.0,
      "summary": "AnchoredDream introduces a zero-shot method for generating 360° indoor scenes from a single image, enhancing appearance consistency and geometric plausibility through a mutual boosting mechanism.",
      "debug_abstract": "Single-view indoor scene generation plays a crucial role in a range of real-world applications. However, generating a complete 360° scene from a single image remains a highly ill-posed and challenging problem. Recent approaches have made progress by leveraging diffusion models and depth estimation networks, yet they still struggle to maintain appearance consistency and geometric plausibility under large viewpoint changes, limiting their effectiveness in full-scene generation. To address this, we propose AnchoredDream, a novel zero-shot pipeline that anchors 360° scene generation on high-fidelity geometry via an appearance-geometry mutual boosting mechanism. Given a single-view image, our method first performs appearance-guided geometry generation to construct a reliable 3D scene layout. Then, we progressively generate the complete scene through a series of modules: warp-and-inpaint, warp-and-refine, post-optimization, and a novel Grouting Block, which ensures seamless transitions between the input view and generated regions. Extensive experiments demonstrate that AnchoredDream outperforms existing methods by a large margin in both appearance consistency and geometric plausibility--all in a zero-shot manner. Our results highlight the potential of geometric grounding for high-quality, zero-shot single-view scene generation."
    },
    {
      "title": "OnlineSI: Taming Large Language Model for Online 3D Understanding and Grounding",
      "url": "https://arxiv.org/abs/2601.16538",
      "date": "2026-01-23",
      "authors_text": "Zixian Liu, Zhaoxi Chen, Liang Pan, Ziwei Liu",
      "is_highlight": false,
      "score": 88.0,
      "summary": "OnlineSI is a framework that enhances Multimodal Large Language Models' spatial understanding in dynamic environments by integrating continuous learning from video streams and 3D point cloud data.",
      "debug_abstract": "In recent years, researchers have increasingly been interested in how to enable Multimodal Large Language Models (MLLM) to possess spatial understanding and reasoning capabilities. However, most existing methods overlook the importance of the ability to continuously work in an ever-changing world, and lack the possibility of deployment on embodied systems in real-world environments. In this work, we introduce OnlineSI, a framework that can continuously improve its spatial understanding of its surroundings given a video stream. Our core idea is to maintain a finite spatial memory to retain past observations, ensuring the computation required for each inference does not increase as the input accumulates. We further integrate 3D point cloud information with semantic information, helping MLLM to better locate and identify objects in the scene. To evaluate our method, we introduce the Fuzzy $F_1$-Score to mitigate ambiguity, and test our method on two representative datasets. Experiments demonstrate the effectiveness of our method, paving the way towards real-world embodied systems."
    },
    {
      "title": "GRIP: Algorithm-Agnostic Machine Unlearning for Mixture-of-Experts via Geometric Router Constraints",
      "url": "https://arxiv.org/abs/2601.16905",
      "date": "2026-01-23",
      "authors_text": "Andy Zhu, Rongzhe Wei, Yupu Gu, Pan Li",
      "is_highlight": false,
      "score": 62.0,
      "summary": "The GRIP framework enables effective machine unlearning in Mixture-of-Experts models by imposing geometric constraints on router updates, ensuring knowledge erasure without sacrificing model utility.",
      "debug_abstract": "Machine unlearning (MU) for large language models has become critical for AI safety, yet existing methods fail to generalize to Mixture-of-Experts (MoE) architectures. We identify that traditional unlearning methods exploit MoE&#39;s architectural vulnerability: they manipulate routers to redirect queries away from knowledgeable experts rather than erasing knowledge, causing a loss of model utility and superficial forgetting. We propose Geometric Routing Invariance Preservation (GRIP), an algorithm-agnostic framework for unlearning for MoE. Our core contribution is a geometric constraint, implemented by projecting router gradient updates into an expert-specific null-space. Crucially, this decouples routing stability from parameter rigidity: while discrete expert selections remain stable for retained knowledge, the continuous router parameters remain plastic within the null space, allowing the model to undergo necessary internal reconfiguration to satisfy unlearning objectives. This forces the unlearning optimization to erase knowledge directly from expert parameters rather than exploiting the superficial router manipulation shortcut. GRIP functions as an adapter, constraining router parameter updates without modifying the underlying unlearning algorithm. Extensive experiments on large-scale MoE models demonstrate that our adapter eliminates expert selection shift (achieving over 95% routing stability) across all tested unlearning methods while preserving their utility. By preventing existing algorithms from exploiting MoE model&#39;s router vulnerability, GRIP adapts existing unlearning research from dense architectures to MoEs."
    },
    {
      "title": "Render-of-Thought: Rendering Textual Chain-of-Thought as Images for Visual Latent Reasoning",
      "url": "https://arxiv.org/abs/2601.14750",
      "date": "2026-01-22",
      "authors_text": "Yifan Wang, Shiyu Li, Peiming Li, Xiaochen Yang, Yang Tang",
      "is_highlight": false,
      "score": 92.5,
      "summary": "The Render-of-Thought framework transforms Chain-of-Thought reasoning into images, enhancing analyzability and efficiency while maintaining competitive performance in reasoning tasks.",
      "debug_abstract": "Chain-of-Thought (CoT) prompting has achieved remarkable success in unlocking the reasoning capabilities of Large Language Models (LLMs). Although CoT prompting enhances reasoning, its verbosity imposes substantial computational overhead. Recent works often focus exclusively on outcome alignment and lack supervision on the intermediate reasoning process. These deficiencies obscure the analyzability of the latent reasoning chain. To address these challenges, we introduce Render-of-Thought (RoT), the first framework to reify the reasoning chain by rendering textual steps into images, making the latent rationale explicit and traceable. Specifically, we leverage the vision encoders of existing Vision Language Models (VLMs) as semantic anchors to align the vision embeddings with the textual space. This design ensures plug-and-play implementation without incurring additional pre-training overhead. Extensive experiments on mathematical and logical reasoning benchmarks demonstrate that our method achieves 3-4x token compression and substantial inference acceleration compared to explicit CoT. Furthermore, it maintains competitive performance against other methods, validating the feasibility of this paradigm. Our code is available at this https URL"
    },
    {
      "title": "LLM-in-Sandbox Elicits General Agentic Intelligence",
      "url": "https://arxiv.org/abs/2601.16206",
      "date": "2026-01-22",
      "authors_text": "Daixuan Cheng, Shaohan Huang, Yuxian Gu, Huatong Song, Guoxin Chen",
      "is_highlight": false,
      "score": 95.0,
      "summary": "The LLM-in-Sandbox framework enables large language models to demonstrate general intelligence in non-code tasks through sandbox exploration and reinforcement learning, achieving robust generalization across various domains.",
      "debug_abstract": "We introduce LLM-in-Sandbox, enabling LLMs to explore within a code sandbox (i.e., a virtual computer), to elicit general intelligence in non-code domains. We first demonstrate that strong LLMs, without additional training, exhibit generalization capabilities to leverage the code sandbox for non-code tasks. For example, LLMs spontaneously access external resources to acquire new knowledge, leverage the file system to handle long contexts, and execute scripts to satisfy formatting requirements. We further show that these agentic capabilities can be enhanced through LLM-in-Sandbox Reinforcement Learning (LLM-in-Sandbox-RL), which uses only non-agentic data to train models for sandbox exploration. Experiments demonstrate that LLM-in-Sandbox, in both training-free and post-trained settings, achieves robust generalization spanning mathematics, physics, chemistry, biomedicine, long-context understanding, and instruction following. Finally, we analyze LLM-in-Sandbox&#39;s efficiency from computational and system perspectives, and open-source it as a Python package to facilitate real-world deployment."
    },
    {
      "title": "Collision-Free Humanoid Traversal in Cluttered Indoor Scenes",
      "url": "https://arxiv.org/abs/2601.16035",
      "date": "2026-01-22",
      "authors_text": "Han Xue, Sikai Liang, Zhikai Zhang, Zicheng Zeng, Yun Liu",
      "is_highlight": false,
      "score": 81.2,
      "summary": "The paper presents Humanoid Potential Field (HumanoidPF) for enabling collision-free traversal of humanoids in cluttered indoor environments, demonstrating effective RL-based learning and real-world applicability.",
      "debug_abstract": "We study the problem of collision-free humanoid traversal in cluttered indoor scenes, such as hurdling over objects scattered on the floor, crouching under low-hanging obstacles, or squeezing through narrow passages. To achieve this goal, the humanoid needs to map its perception of surrounding obstacles with diverse spatial layouts and geometries to the corresponding traversal skills. However, the lack of an effective representation that captures humanoid-obstacle relationships during collision avoidance makes directly learning such mappings difficult. We therefore propose Humanoid Potential Field (HumanoidPF), which encodes these relationships as collision-free motion directions, significantly facilitating RL-based traversal skill learning. We also find that HumanoidPF exhibits a surprisingly negligible sim-to-real gap as a perceptual representation. To further enable generalizable traversal skills through diverse and challenging cluttered indoor scenes, we further propose a hybrid scene generation method, incorporating crops of realistic 3D indoor scenes and procedurally synthesized obstacles. We successfully transfer our policy to the real world and develop a teleoperation system where users could command the humanoid to traverse in cluttered indoor scenes with just a single click. Extensive experiments are conducted in both simulation and the real world to validate the effectiveness of our method. Demos and code can be found in our website: this https URL."
    }
  ],
  "机器人与人工智能实验室 (RAIL)": [
    {
      "title": "Search-R2: Enhancing Search-Integrated Reasoning via Actor-Refiner Collaboration",
      "url": "https://arxiv.org/abs/2602.03647",
      "date": "2026-02-03",
      "authors_text": "Bowei He, Minda Hu, Zenan Xu, Hongru Wang, Licheng Zong",
      "is_highlight": false,
      "score": 52.0,
      "summary": "Search-R2 introduces an Actor-Refiner framework that enhances search-integrated reasoning in language agents through targeted intervention and fine-grained supervision, outperforming existing methods.",
      "teaser_image": "https://arxiv.org/html/2602.03647/x2.png",
      "debug_abstract": "Search-integrated reasoning enables language agents to transcend static parametric knowledge by actively querying external sources. However, training these agents via reinforcement learning is hindered by the multi-scale credit assignment problem: existing methods typically rely on sparse, trajectory-level rewards that fail to distinguish between high-quality reasoning and fortuitous guesses, leading to redundant or misleading search behaviors. To address this, we propose Search-R2, a novel Actor-Refiner collaboration framework that enhances reasoning through targeted intervention, with both components jointly optimized during training. Our approach decomposes the generation process into an Actor, which produces initial reasoning trajectories, and a Meta-Refiner, which selectively diagnoses and repairs flawed steps via a &#39;cut-and-regenerate&#39; mechanism. To provide fine-grained supervision, we introduce a hybrid reward design that couples outcome correctness with a dense process reward quantifying the information density of retrieved evidence. Theoretically, we formalize the Actor-Refiner interaction as a smoothed mixture policy, proving that selective correction yields strict performance gains over strong baselines. Extensive experiments across various general and multi-hop QA datasets demonstrate that Search-R2 consistently outperforms strong RAG and RL-based baselines across model scales, achieving superior reasoning accuracy with minimal overhead."
    },
    {
      "title": "Dual Quaternion SE(3) Synchronization with Recovery Guarantees",
      "url": "https://arxiv.org/abs/2602.00324",
      "date": "2026-01-30",
      "authors_text": "Jianing Zhao, Linglingzhi Zhu, Anthony Man-Cho So",
      "is_highlight": false,
      "score": 75.0,
      "summary": "This paper presents a dual quaternion-based SE(3) synchronization method with a two-stage algorithm that guarantees recovery of absolute poses from noisy transformations, outperforming traditional matrix methods.",
      "teaser_image": "https://arxiv.org/html/2602.00324/x2.png",
      "debug_abstract": "Synchronization over the special Euclidean group SE(3) aims to recover absolute poses from noisy pairwise relative transformations and is a core primitive in robotics and 3D vision. Standard approaches often require multi-step heuristic procedures to recover valid poses, which are difficult to analyze and typically lack theoretical guarantees. This paper adopts a dual quaternion representation and formulates SE(3) synchronization directly over the unit dual quaternion. A two-stage algorithm is developed: A spectral initializer computed via the power method on a Hermitian dual quaternion measurement matrix, followed by a dual quaternion generalized power method (DQGPM) that enforces feasibility through per-iteration projection. The estimation error bounds are established for spectral estimators, and DQGPM is shown to admit a finite-iteration error bound and achieves linear error contraction up to an explicit noise-dependent threshold. Experiments on synthetic benchmarks and real-world multi-scan point-set registration demonstrate that the proposed pipeline improves both accuracy and efficiency over representative matrix-based methods."
    },
    {
      "title": "MedAD-R1: Eliciting Consistent Reasoning in Interpretible Medical Anomaly Detection via Consistency-Reinforced Policy Optimization",
      "url": "https://arxiv.org/abs/2602.01081",
      "date": "2026-02-01",
      "authors_text": "Haitao Zhang, Yingying Wang, Jiaxiang Wang, Haote Xu, Hongyang Zhang",
      "is_highlight": false,
      "score": 64.0,
      "summary": "MedAD-R1 enhances medical anomaly detection by utilizing a two-stage training framework with consistency-reinforced policy optimization, achieving state-of-the-art performance on the MedAD-38K benchmark.",
      "teaser_image": "https://arxiv.org/html/2602.01081/x3.png",
      "debug_abstract": "Medical Anomaly Detection (MedAD) presents a significant opportunity to enhance diagnostic accuracy using Large Multimodal Models (LMMs) to interpret and answer questions based on medical images. However, the reliance on Supervised Fine-Tuning (SFT) on simplistic and fragmented datasets has hindered the development of models capable of plausible reasoning and robust multimodal generalization. To overcome this, we introduce MedAD-38K, the first large-scale, multi-modal, and multi-center benchmark for MedAD featuring diagnostic Chain-of-Thought (CoT) annotations alongside structured Visual Question-Answering (VQA) pairs. On this foundation, we propose a two-stage training framework. The first stage, Cognitive Injection, uses SFT to instill foundational medical knowledge and align the model with a structured think-then-answer paradigm. Given that standard policy optimization can produce reasoning that is disconnected from the final answer, the second stage incorporates Consistency Group Relative Policy Optimization (Con-GRPO). This novel algorithm incorporates a crucial consistency reward to ensure the generated reasoning process is relevant and logically coherent with the final diagnosis. Our proposed model, MedAD-R1, achieves state-of-the-art (SOTA) performance on the MedAD-38K benchmark, outperforming strong baselines by more than 10\\%. This superior performance stems from its ability to generate transparent and logically consistent reasoning pathways, offering a promising approach to enhancing the trustworthiness and interpretability of AI for clinical decision support."
    },
    {
      "title": "Adaptive Rollout Allocation for Online Reinforcement Learning with Verifiable Rewards",
      "url": "https://arxiv.org/abs/2602.01601",
      "date": "2026-02-02",
      "authors_text": "Hieu Trung Nguyen, Bao Nguyen, Wenao Ma, Yuzhi Zhao, Ruifeng She",
      "is_highlight": false,
      "score": 40.0,
      "summary": "The paper presents \\Ours, a variance-informed strategy for adaptive rollout allocation in online reinforcement learning, enhancing sampling efficiency and performance over traditional methods.",
      "teaser_image": "https://arxiv.org/html/2602.01601/x2.png",
      "debug_abstract": "Sampling efficiency is a key bottleneck in reinforcement learning with verifiable rewards. Existing group-based policy optimization methods, such as GRPO, allocate a fixed number of rollouts for all training prompts. This uniform allocation implicitly treats all prompts as equally informative, and could lead to inefficient computational budget usage and impede training progress. We introduce \\Ours, a Variance-Informed Predictive allocation strategy that allocates a given rollout budget to the prompts in the incumbent batch to minimize the expected gradient variance of the policy update. At each iteration, \\Ours~uses a lightweight Gaussian process model to predict per-prompt success probabilities based on recent rollouts. These probability predictions are translated into variance estimates, which are then fed into a convex optimization problem to determine the optimal rollout allocations under a hard compute budget constraint. Empirical results show that \\Ours~consistently improves sampling efficiency and achieves higher performance than uniform or heuristic allocation strategies in multiple benchmarks. Our code will be available at this https URL."
    },
    {
      "title": "Reasoning with Autoregressive-Diffusion Collaborative Thoughts",
      "url": "https://arxiv.org/abs/2602.01608",
      "date": "2026-02-02",
      "authors_text": "Mu Yuan, Liekang Zeng, Guoliang Xing, Lan Zhang, Yunhao Liu",
      "is_highlight": false,
      "score": 41.0,
      "summary": "The paper presents Collaborative Thoughts, a framework integrating autoregressive and diffusion models for enhanced spatial reasoning and generation through iterative feedback and joint reasoning.",
      "teaser_image": "https://arxiv.org/html/2602.01608/x2.png",
      "debug_abstract": "Autoregressive and diffusion models represent two complementary generative paradigms. Autoregressive models excel at sequential planning and constraint composition, yet struggle with tasks that require explicit spatial or physical grounding. Diffusion models, in contrast, capture rich spatial structure through high-dimensional generation, but lack the stepwise logical control needed to satisfy complex, multi-stage constraints or to reliably identify and correct errors. We introduce Collaborative Thoughts, a unified collaborative framework that enables autoregressive and diffusion models to reason and generate jointly through a closed-loop interaction. In Collaborative Thoughts, autoregressive models perform structured planning and constraint management, diffusion models instantiate these constraints as intermediate visual thoughts, and a vision-based critic module evaluates whether the visual thoughts satisfy the intended structural and physical requirements. This feedback is then used to iteratively refine subsequent planning and generation steps, mitigating error propagation across modalities. Importantly, Collaborative Thoughts uses the same collaborative loop regardless of whether the task is autoregressive question answering or diffusion-based visual generation. Through representative examples, we illustrate how Collaborative Thoughts can improve the reliability of spatial reasoning and the controllability of generation."
    },
    {
      "title": "Tilt-Ropter: A Novel Hybrid Aerial and Terrestrial Vehicle with Tilt Rotors and Passive Wheels",
      "url": "https://arxiv.org/abs/2602.01700",
      "date": "2026-02-02",
      "authors_text": "Ruoyu Wang, Xuchen Liu, Zongzhou Wu, Zixuan Guo, Wendi Ding",
      "is_highlight": false,
      "score": 38.0,
      "summary": "The Tilt-Ropter is a hybrid aerial-terrestrial vehicle that utilizes tilt rotors and passive wheels for efficient locomotion, featuring advanced control systems for enhanced mobility and energy savings.",
      "teaser_image": "https://arxiv.org/html/2602.01700/x2.png",
      "debug_abstract": "In this work, we present Tilt-Ropter, a novel hybrid aerial-terrestrial vehicle (HATV) that combines tilt rotors with passive wheels to achieve energy-efficient multi-mode locomotion. Unlike existing under-actuated HATVs, the fully actuated design of Tilt-Ropter enables decoupled force and torque control, greatly enhancing its mobility and environmental adaptability. A nonlinear model predictive controller (NMPC) is developed to track reference trajectories and handle contact constraints across locomotion modes, while a dedicated control allocation module exploits actuation redundancy to achieve energy-efficient control of actuators. Additionally, to enhance robustness during ground contact, we introduce an external wrench estimation algorithm that estimates environmental interaction forces and torques in real time. The system is validated through both simulation and real-world experiments, including seamless air-ground transitions and trajectory tracking. Results show low tracking errors in both modes and highlight a 92.8% reduction in power consumption during ground locomotion, demonstrating the system&#39;s potential for long-duration missions across large-scale and energy-constrained environments."
    },
    {
      "title": "Breaking the Static Graph: Context-Aware Traversal for Robust Retrieval-Augmented Generation",
      "url": "https://arxiv.org/abs/2602.01965",
      "date": "2026-02-02",
      "authors_text": "Kwun Hang Lau, Fangyuan Zhang, Boyu Ruan, Yingli Zhou, Qintian Guo",
      "is_highlight": false,
      "score": 27.0,
      "summary": "CatRAG enhances Retrieval-Augmented Generation by transforming static Knowledge Graphs into query-adaptive structures, improving multi-hop evidence retrieval and reasoning completeness.",
      "teaser_image": "https://arxiv.org/html/2602.01965/image/HubBiasShift.png",
      "debug_abstract": "Recent advances in Retrieval-Augmented Generation (RAG) have shifted from simple vector similarity to structure-aware approaches like HippoRAG, which leverage Knowledge Graphs (KGs) and Personalized PageRank (PPR) to capture multi-hop dependencies. However, these methods suffer from a &#34;Static Graph Fallacy&#34;: they rely on fixed transition probabilities determined during indexing. This rigidity ignores the query-dependent nature of edge relevance, causing semantic drift where random walks are diverted into high-degree &#34;hub&#34; nodes before reaching critical downstream evidence. Consequently, models often achieve high partial recall but fail to retrieve the complete evidence chain required for multi-hop queries. To address this, we propose CatRAG, Context-Aware Traversal for robust RAG, a framework that builds on the HippoRAG 2 architecture and transforms the static KG into a query-adaptive navigation structure. We introduce a multi-faceted framework to steer the random walk: (1) Symbolic Anchoring, which injects weak entity constraints to regularize the random walk; (2) Query-Aware Dynamic Edge Weighting, which dynamically modulates graph structure, to prune irrelevant paths while amplifying those aligned with the query&#39;s intent; and (3) Key-Fact Passage Weight Enhancement, a cost-efficient bias that structurally anchors the random walk to likely evidence. Experiments across four multi-hop benchmarks demonstrate that CatRAG consistently outperforms state of the art baselines. Our analysis reveals that while standard Recall metrics show modest gains, CatRAG achieves substantial improvements in reasoning completeness, the capacity to recover the entire evidence path without gaps. These results reveal that our approach effectively bridges the gap between retrieving partial context and enabling fully grounded reasoning. Resources are available at this https URL."
    },
    {
      "title": "Towards Agentic Intelligence for Materials Science",
      "url": "https://arxiv.org/abs/2602.00169",
      "date": "2026-01-29",
      "authors_text": "Huan Zhang, Yizhan Li, Wenhao Huang, Ziyu Hou, Yu Song",
      "is_highlight": false,
      "score": 12.0,
      "summary": "This paper advocates for developing agentic AI systems in materials science that autonomously plan, act, and learn to enhance discovery processes beyond traditional models.",
      "teaser_image": "https://arxiv.org/html/2602.00169/x1.png",
      "debug_abstract": "The convergence of artificial intelligence and materials science presents a transformative opportunity, but achieving true acceleration in discovery requires moving beyond task-isolated, fine-tuned models toward agentic systems that plan, act, and learn across the full discovery loop. This survey advances a unique pipeline-centric view that spans from corpus curation and pretraining, through domain adaptation and instruction tuning, to goal-conditioned agents interfacing with simulation and experimental platforms. Unlike prior reviews, we treat the entire process as an end-to-end system to be optimized for tangible discovery outcomes rather than proxy benchmarks. This perspective allows us to trace how upstream design choices-such as data curation and training objectives-can be aligned with downstream experimental success through effective credit assignment. To bridge communities and establish a shared frame of reference, we first present an integrated lens that aligns terminology, evaluation, and workflow stages across AI and materials science. We then analyze the field through two focused lenses: From the AI perspective, the survey details LLM strengths in pattern recognition, predictive analytics, and natural language processing for literature mining, materials characterization, and property prediction; from the materials science perspective, it highlights applications in materials design, process optimization, and the acceleration of computational workflows via integration with external tools (e.g., DFT, robotic labs). Finally, we contrast passive, reactive approaches with agentic design, cataloging current contributions while motivating systems that pursue long-horizon goals with autonomy, memory, and tool use. This survey charts a practical roadmap towards autonomous, safety-aware LLM agents aimed at discovering novel and useful materials."
    },
    {
      "title": "FlowSteer: Interactive Agentic Workflow Orchestration via End-to-End Reinforcement Learning",
      "url": "https://arxiv.org/abs/2602.01664",
      "date": "2026-02-02",
      "authors_text": "Mingda Zhang, Haoran Luo, Tiesunlong Shen, Qika Lin, Xiaoying Tang",
      "is_highlight": false,
      "score": 7.0,
      "summary": "FlowSteer is an end-to-end reinforcement learning framework that automates workflow orchestration through interactive learning, addressing manual costs and improving performance across diverse tasks.",
      "teaser_image": "https://arxiv.org/html/2602.01664/x1.png",
      "debug_abstract": "In recent years, a variety of powerful agentic workflows have been applied to solve a wide range of human problems. However, existing workflow orchestration still faces key challenges, including high manual cost, reliance on specific operators/large language models (LLMs), and sparse reward signals. To address these challenges, we propose FlowSteer, an end-to-end reinforcement learning framework that takes a lightweight policy model as the agent and an executable canvas environment, automating workflow orchestration through multi-turn interaction. In this process, the policy model analyzes execution states and selects editing actions, while the canvas executes operators and returns feedback for iterative refinement. Moreover, FlowSteer provides a plug-and-play framework that supports diverse operator libraries and interchangeable LLM backends. To effectively train this interaction paradigm, we propose Canvas Workflow Relative Policy Optimization (CWRPO), which introduces diversity-constrained rewards with conditional release to stabilize learning and suppress shortcut behaviors. Experimental results on twelve datasets show that FlowSteer significantly outperforms baselines across various tasks."
    },
    {
      "title": "Heterogeneous Vertiport Selection Optimization for On-Demand Air Taxi Services: A Deep Reinforcement Learning Approach",
      "url": "https://arxiv.org/abs/2601.21316",
      "date": "2026-01-29",
      "authors_text": "Aoyu Pang, Maonan Wang, Zifan Sha, Wenwei Yue, Changle Li",
      "is_highlight": false,
      "score": 80.0,
      "summary": "The paper presents a deep reinforcement learning framework for optimizing vertiport selection and air taxi routing, achieving significant travel time reductions in urban air mobility systems.",
      "teaser_image": "https://arxiv.org/html/2601.21316/x2.png",
      "debug_abstract": "Urban Air Mobility (UAM) has emerged as a transformative solution to alleviate urban congestion by utilizing low-altitude airspace, thereby reducing pressure on ground transportation networks. To enable truly efficient and seamless door-to-door travel experiences, UAM requires close integration with existing ground transportation infrastructure. However, current research on optimal integrated routing strategies for passengers in air-ground mobility systems remains limited, with a lack of systematic this http URL address this gap, we first propose a unified optimization model that integrates strategy selection for both air and ground transportation. This model captures the dynamic characteristics of multimodal transport networks and incorporates real-time traffic conditions alongside passenger decision-making behavior. Building on this model, we propose a Unified Air-Ground Mobility Coordination (UAGMC) framework, which leverages deep reinforcement learning (RL) and Vehicle-to-Everything (V2X) communication to optimize vertiport selection and dynamically plan air taxi routes. Experimental results demonstrate that UAGMC achieves a 34\\% reduction in average travel time compared to conventional proportional allocation methods, enhancing overall travel efficiency and providing novel insights into the integration and optimization of multimodal transportation systems. This work lays a solid foundation for advancing intelligent urban mobility solutions through the coordination of air and ground transportation modes. The related code can be found at this https URL."
    },
    {
      "title": "Disentangling perception and reasoning for improving data efficiency in learning cloth manipulation without demonstrations",
      "url": "https://arxiv.org/abs/2601.21713",
      "date": "2026-01-29",
      "authors_text": "Donatien Delehelle, Fei Chen, Darwin Caldwell",
      "is_highlight": false,
      "score": 87.0,
      "summary": "This paper presents a modular reinforcement learning approach for cloth manipulation that enhances data efficiency and reduces model size and training time, achieving improved performance in simulation and real-world transfer.",
      "teaser_image": "https://arxiv.org/html/2601.21713/x1.png",
      "debug_abstract": "Cloth manipulation is a ubiquitous task in everyday life, but it remains an open challenge for robotics. The difficulties in developing cloth manipulation policies are attributed to the high-dimensional state space, complex dynamics, and high propensity to self-occlusion exhibited by fabrics. As analytical methods have not been able to provide robust and general manipulation policies, reinforcement learning (RL) is considered a promising approach to these problems. However, to address the large state space and complex dynamics, data-based methods usually rely on large models and long training times. The resulting computational cost significantly hampers the development and adoption of these methods. Additionally, due to the challenge of robust state estimation, garment manipulation policies often adopt an end-to-end learning approach with workspace images as input. While this approach enables a conceptually straightforward sim-to-real transfer via real-world fine-tuning, it also incurs a significant computational cost by training agents on a highly lossy representation of the environment state. This paper questions this common design choice by exploring an efficient and modular approach to RL for cloth manipulation. We show that, through careful design choices, model size and training time can be significantly reduced when learning in simulation. Furthermore, we demonstrate how the resulting simulation-trained model can be transferred to the real world. We evaluate our approach on the SoftGym benchmark and achieve significant performance improvements over available baselines on our task, while using a substantially smaller model."
    },
    {
      "title": "PLANING: A Loosely Coupled Triangle-Gaussian Framework for Streaming 3D Reconstruction",
      "url": "https://arxiv.org/abs/2601.22046",
      "date": "2026-01-29",
      "authors_text": "Changjian Jiang, Kerui Ren, Xudong Li, Kaiwen Song, Linning Xu",
      "is_highlight": false,
      "score": 66.0,
      "summary": "PLANING is a novel streaming 3D reconstruction framework that efficiently combines geometric primitives and neural Gaussians, enhancing quality and speed while reducing redundancy.",
      "teaser_image": "https://arxiv.org/html/2601.22046/x3.png",
      "debug_abstract": "Streaming reconstruction from monocular image sequences remains challenging, as existing methods typically favor either high-quality rendering or accurate geometry, but rarely both. We present PLANING, an efficient on-the-fly reconstruction framework built on a hybrid representation that loosely couples explicit geometric primitives with neural Gaussians, enabling geometry and appearance to be modeled in a decoupled manner. This decoupling supports an online initialization and optimization strategy that separates geometry and appearance updates, yielding stable streaming reconstruction with substantially reduced structural redundancy. PLANING improves dense mesh Chamfer-L2 by 18.52% over PGSR, surpasses ARTDECO by 1.31 dB PSNR, and reconstructs ScanNetV2 scenes in under 100 seconds, over 5x faster than 2D Gaussian Splatting, while matching the quality of offline per-scene optimization. Beyond reconstruction quality, the structural clarity and computational efficiency of \\modelname~make it well suited for a broad range of downstream applications, such as enabling large-scale scene modeling and simulation-ready environments for embodied AI. Project page: this https URL ."
    },
    {
      "title": "Rhombot: Rhombus-shaped Modular Robots for Stable, Medium-Independent Reconfiguration Motion",
      "url": "https://arxiv.org/abs/2601.19529",
      "date": "2026-01-27",
      "authors_text": "Jie Gu, Yirui Sun, Zhihao Xia, Tin Lun Lam, Chunxu Tian",
      "is_highlight": false,
      "score": 86.0,
      "summary": "Rhombot is a modular self-reconfigurable robot featuring rhombus-shaped modules that enable stable, medium-independent morphing and locomotion through a novel motion primitive called morphpivoting.",
      "debug_abstract": "In this paper, we present Rhombot, a novel deformable planar lattice modular self-reconfigurable robot (MSRR) with a rhombus shaped module. Each module consists of a parallelogram skeleton with a single centrally mounted actuator that enables folding and unfolding along its diagonal. The core design philosophy is to achieve essential MSRR functionalities such as morphing, docking, and locomotion with minimal control complexity. This enables a continuous and stable reconfiguration process that is independent of the surrounding medium, allowing the system to reliably form various configurations in diverse environments. To leverage the unique kinematics of Rhombot, we introduce morphpivoting, a novel motion primitive for reconfiguration that differs from advanced MSRR systems, and propose a strategy for its continuous execution. Finally, a series of physical experiments validate the module&#39;s stable reconfiguration ability, as well as its positional and docking accuracy."
    },
    {
      "title": "AC^2-VLA: Action-Context-Aware Adaptive Computation in Vision-Language-Action Models for Efficient Robotic Manipulation",
      "url": "https://arxiv.org/abs/2601.19634",
      "date": "2026-01-27",
      "authors_text": "Wenda Yu, Tianshi Wang, Fengling Li, Jingjing Li, Lei Zhu",
      "is_highlight": false,
      "score": 94.0,
      "summary": "AC^2-VLA enhances robotic manipulation efficiency by adaptively optimizing computation based on action context, achieving significant speedup and reduced resource usage without sacrificing performance.",
      "debug_abstract": "Vision-Language-Action (VLA) models have demonstrated strong performance in robotic manipulation, yet their closed-loop deployment is hindered by the high latency and compute cost of repeatedly running large vision-language backbones at every timestep. We observe that VLA inference exhibits structured redundancies across temporal, spatial, and depth dimensions, and that most existing efficiency methods ignore action context, despite its central role in embodied tasks. To address this gap, we propose Action-Context-aware Adaptive Computation for VLA models (AC^2-VLA), a unified framework that conditions computation on current visual observations, language instructions, and previous action states. Based on this action-centric context, AC^2-VLA adaptively performs cognition reuse across timesteps, token pruning, and selective execution of model components within a unified mechanism. To train the adaptive policy, we introduce an action-guided self-distillation scheme that preserves the behavior of the dense VLA policy while enabling structured sparsification that transfers across tasks and settings. Extensive experiments on robotic manipulation benchmarks show that AC^2-VLA achieves up to a 1.79\\times speedup while reducing FLOPs to 29.4% of the dense baseline, with comparable task success."
    },
    {
      "title": "EgoHandICL: Egocentric 3D Hand Reconstruction with In-Context Learning",
      "url": "https://arxiv.org/abs/2601.19850",
      "date": "2026-01-27",
      "authors_text": "Binzhu Xie, Shi Qiu, Sicheng Zhang, Yinqiao Wang, Hao Xu",
      "is_highlight": false,
      "score": 76.0,
      "summary": "EgoHandICL introduces an innovative in-context learning framework for robust 3D hand reconstruction in egocentric vision, enhancing performance through exemplar retrieval and multimodal context integration.",
      "debug_abstract": "Robust 3D hand reconstruction in egocentric vision is challenging due to depth ambiguity, self-occlusion, and complex hand-object interactions. Prior methods mitigate these issues by scaling training data or adding auxiliary cues, but they often struggle in unseen contexts. We present EgoHandICL, the first in-context learning (ICL) framework for 3D hand reconstruction that improves semantic alignment, visual consistency, and robustness under challenging egocentric conditions. EgoHandICL introduces complementary exemplar retrieval guided by vision-language models (VLMs), an ICL-tailored tokenizer for multimodal context, and a masked autoencoder (MAE)-based architecture trained with hand-guided geometric and perceptual objectives. Experiments on ARCTIC and EgoExo4D show consistent gains over state-of-the-art methods. We also demonstrate real-world generalization and improve EgoVLM hand-object interaction reasoning by using reconstructed hands as visual prompts. Code and data: this https URL"
    },
    {
      "title": "AdaReasoner: Dynamic Tool Orchestration for Iterative Visual Reasoning",
      "url": "https://arxiv.org/abs/2601.18631",
      "date": "2026-01-26",
      "authors_text": "Mingyang Song, Haoyu Sun, Jiawei Gu, Linjie Li, Luxin Xu",
      "is_highlight": false,
      "score": 61.0,
      "summary": "AdaReasoner is a multimodal model that autonomously optimizes tool selection and usage for iterative visual reasoning, achieving state-of-the-art performance on various benchmarks.",
      "debug_abstract": "When humans face problems beyond their immediate capabilities, they rely on tools, providing a promising paradigm for improving visual reasoning in multimodal large language models (MLLMs). Effective reasoning, therefore, hinges on knowing which tools to use, when to invoke them, and how to compose them over multiple steps, even when faced with new tools or new tasks. We introduce \\textbf{AdaReasoner}, a family of multimodal models that learn tool use as a general reasoning skill rather than as tool-specific or explicitly supervised behavior. AdaReasoner is enabled by (i) a scalable data curation pipeline exposing models to long-horizon, multi-step tool interactions; (ii) Tool-GRPO, a reinforcement learning algorithm that optimizes tool selection and sequencing based on end-task success; and (iii) an adaptive learning mechanism that dynamically regulates tool usage. Together, these components allow models to infer tool utility from task context and intermediate outcomes, enabling coordination of multiple tools and generalization to unseen tools. Empirically, AdaReasoner exhibits strong tool-adaptive and generalization behaviors: it autonomously adopts beneficial tools, suppresses irrelevant ones, and adjusts tool usage frequency based on task demands, despite never being explicitly trained to do so. These capabilities translate into state-of-the-art performance across challenging benchmarks, improving the 7B base model by +24.9\\% on average and surpassing strong proprietary systems such as GPT-5 on multiple tasks, including VSP and Jigsaw."
    },
    {
      "title": "Timely Machine: Awareness of Time Makes Test-Time Scaling Agentic",
      "url": "https://arxiv.org/abs/2601.16486",
      "date": "2026-01-23",
      "authors_text": "Yichuan Ma, Linyang Li, Yongkang chen, Peiji Li, Xiaozhe Li",
      "is_highlight": false,
      "score": 78.0,
      "summary": "The paper introduces Timely Machine, redefining test-time scaling for LLMs based on wall-clock time, enhancing performance through dynamic strategy adjustments and reinforcement learning.",
      "debug_abstract": "As large language models (LLMs) increasingly tackle complex reasoning tasks, test-time scaling has become critical for enhancing capabilities. However, in agentic scenarios with frequent tool calls, the traditional generation-length-based definition breaks down: tool latency decouples inference time from generation length. We propose Timely Machine, redefining test-time as wall-clock time, where models dynamically adjust strategies based on time budgets. We introduce Timely-Eval, a benchmark spanning high-frequency tool calls, low-frequency tool calls, and time-constrained reasoning. By varying tool latency, we find smaller models excel with fast feedback through more interactions, while larger models dominate high-latency settings via superior interaction quality. Moreover, existing models fail to adapt reasoning to time budgets. We propose Timely-RL to address this gap. After cold-start supervised fine-tuning, we use reinforcement learning to enhance temporal planning. Timely-RL improves time budget awareness and consistently boosts performance across Timely-Eval. We hope our work offers a new perspective on test-time scaling for the agentic era."
    },
    {
      "title": "Emotion-LLaMAv2 and MMEVerse: A New Framework and Benchmark for Multimodal Emotion Understanding",
      "url": "https://arxiv.org/abs/2601.16449",
      "date": "2026-01-23",
      "authors_text": "Xiaojiang Peng, Jingyi Chen, Zebang Cheng, Bao Peng, Fengyi Wu",
      "is_highlight": false,
      "score": 58.0,
      "summary": "Emotion-LLaMAv2 and MMEVerse introduce a comprehensive framework and benchmark for enhanced multimodal emotion understanding through advanced encoding, feature interaction, and large-scale dataset integration.",
      "debug_abstract": "Understanding human emotions from multimodal signals poses a significant challenge in affective computing and human-robot interaction. While multimodal large language models (MLLMs) have excelled in general vision-language tasks, their capabilities in emotional reasoning remain limited. The field currently suffers from a scarcity of large-scale datasets with high-quality, descriptive emotion annotations and lacks standardized benchmarks for evaluation. Our preliminary framework, Emotion-LLaMA, pioneered instruction-tuned multimodal learning for emotion reasoning but was restricted by explicit face detectors, implicit fusion strategies, and low-quality training data with limited scale. To address these limitations, we present Emotion-LLaMAv2 and the MMEVerse benchmark, establishing an end-to-end pipeline together with a standardized evaluation setting for emotion recognition and reasoning. Emotion-LLaMAv2 introduces three key advances. First, an end-to-end multiview encoder eliminates external face detection and captures nuanced emotional cues via richer spatial and temporal multiview tokens. Second, a Conv Attention pre-fusion module is designed to enable simultaneous local and global multimodal feature interactions external to the LLM backbone. Third, a perception-to-cognition curriculum instruction tuning scheme within the LLaMA2 backbone unifies emotion recognition and free-form emotion reasoning. To support large-scale training and reproducible evaluation, MMEVerse aggregates twelve publicly available emotion datasets, including IEMOCAP, MELD, DFEW, and MAFW, into a unified multimodal instruction format. The data are re-annotated via a multi-agent pipeline involving Qwen2 Audio, Qwen2.5 VL, and GPT 4o, producing 130k training clips and 36k testing clips across 18 evaluation benchmarks."
    },
    {
      "title": "Adaptive Reinforcement and Model Predictive Control Switching for Safe Human-Robot Cooperative Navigation",
      "url": "https://arxiv.org/abs/2601.16686",
      "date": "2026-01-23",
      "authors_text": "Ning Liu, Sen Shen, Zheng Li, Matthew D'Souza, Jen Jen Chung",
      "is_highlight": false,
      "score": 94.0,
      "summary": "The paper presents ARMS, a hybrid control framework combining reinforcement learning and model predictive control for safe human-robot navigation, achieving superior performance in cluttered environments.",
      "debug_abstract": "This paper addresses the challenge of human-guided navigation for mobile collaborative robots under simultaneous proximity regulation and safety constraints. We introduce Adaptive Reinforcement and Model Predictive Control Switching (ARMS), a hybrid learning-control framework that integrates a reinforcement learning follower trained with Proximal Policy Optimization (PPO) and an analytical one-step Model Predictive Control (MPC) formulated as a quadratic program safety filter. To enable robust perception under partial observability and non-stationary human motion, ARMS employs a decoupled sensing architecture with a Long Short-Term Memory (LSTM) temporal encoder for the human-robot relative state and a spatial encoder for 360-degree LiDAR scans. The core contribution is a learned adaptive neural switcher that performs context-aware soft action fusion between the two controllers, favoring conservative, constraint-aware QP-based control in low-risk regions while progressively shifting control authority to the learned follower in highly cluttered or constrained scenarios where maneuverability is critical, and reverting to the follower action when the QP becomes infeasible. Extensive evaluations against Pure Pursuit, Dynamic Window Approach (DWA), and an RL-only baseline demonstrate that ARMS achieves an 82.5 percent success rate in highly cluttered environments, outperforming DWA and RL-only approaches by 7.1 percent and 3.1 percent, respectively, while reducing average computational latency by 33 percent to 5.2 milliseconds compared to a multi-step MPC baseline. Additional simulation transfer in Gazebo and initial real-world deployment results further indicate the practicality and robustness of ARMS for safe and efficient human-robot collaboration. Source code and a demonstration video are available at this https URL."
    },
    {
      "title": "Natural Language-Driven Global Mapping of Martian Landforms",
      "url": "https://arxiv.org/abs/2601.15949",
      "date": "2026-01-22",
      "authors_text": "Yiran Wang, Shuoyuan Wang, Zhaoran Wei, Jiannan Zhao, Zhonghua Yao",
      "is_highlight": false,
      "score": 72.0,
      "summary": "MarScope is a vision-language framework that enables natural language-driven, label-free mapping of Martian landforms, enhancing global geomorphic analysis and user query flexibility.",
      "debug_abstract": "Planetary surfaces are typically analyzed using high-level semantic concepts in natural language, yet vast orbital image archives remain organized at the pixel level. This mismatch limits scalable, open-ended exploration of planetary surfaces. Here we present MarScope, a planetary-scale vision-language framework enabling natural language-driven, label-free mapping of Martian landforms. MarScope aligns planetary images and text in a shared semantic space, trained on over 200,000 curated image-text pairs. This framework transforms global geomorphic mapping on Mars by replacing pre-defined classifications with flexible semantic retrieval, enabling arbitrary user queries across the entire planet in 5 seconds with F1 scores up to 0.978. Applications further show that it extends beyond morphological classification to facilitate process-oriented analysis and similarity-based geomorphological mapping at a planetary scale. MarScope establishes a new paradigm where natural language serves as a direct interface for scientific discovery over massive geospatial datasets."
    },
    {
      "title": "Robust Tool Use via Fission-GRPO: Learning to Recover from Execution Errors",
      "url": "https://arxiv.org/abs/2601.15625",
      "date": "2026-01-22",
      "authors_text": "Zhiwei Zhang, Fei Zhao, Rui Wang, Zezhong Wang, Bin Liang",
      "is_highlight": false,
      "score": 87.1,
      "summary": "Fission-GRPO enhances tool use in LLMs by converting execution errors into corrective supervision, improving error recovery and overall accuracy in multi-turn interactions.",
      "debug_abstract": "Large language models (LLMs) can call tools effectively, yet they remain brittle in multi-turn execution: following a tool call error, smaller models often degenerate into repetitive invalid re-invocations, failing to interpret error feedback and self-correct. This brittleness hinders reliable real-world deployment, where the execution errors are inherently inevitable during tool interaction procedures. We identify a key limitation of current approaches: standard reinforcement learning (RL) treats errors as sparse negative rewards, providing no guidance on how to recover, while pre-collected synthetic error-correction datasets suffer from distribution mismatch with the model&#39;s on-policy error modes. To bridge this gap, we propose Fission-GRPO, a framework that converts execution errors into corrective supervision within the RL training loop. Our core mechanism fissions each failed trajectory into a new training instance by augmenting it with diagnostic feedback from a finetuned Error Simulator, then resampling recovery rollouts on-policy. This enables the model to learn from the precise errors it makes during exploration, rather than from static, pre-collected error cases. On the BFCL v4 Multi-Turn, Fission-GRPO improves the error recovery rate of Qwen3-8B by 5.7% absolute, crucially, yielding a 4% overall accuracy gain (42.75% to 46.75%) over GRPO and outperforming specialized tool-use agents."
    },
    {
      "title": "EvoCUA: Evolving Computer Use Agents via Learning from Scalable Synthetic Experience",
      "url": "https://arxiv.org/abs/2601.15876",
      "date": "2026-01-22",
      "authors_text": "Taofeng Xue, Chong Peng, Mianqiu Huang, Linsen Guo, Tiancheng Han",
      "is_highlight": false,
      "score": 82.6,
      "summary": "EvoCUA introduces a self-sustaining evolutionary model for computer-use agents, leveraging autonomous task generation and iterative learning to significantly outperform existing benchmarks in multimodal AI.",
      "debug_abstract": "The development of native computer-use agents (CUA) represents a significant leap in multimodal AI. However, their potential is currently bottlenecked by the constraints of static data scaling. Existing paradigms relying primarily on passive imitation of static datasets struggle to capture the intricate causal dynamics inherent in long-horizon computer tasks. In this work, we introduce EvoCUA, a native computer use agentic model. Unlike static imitation, EvoCUA integrates data generation and policy optimization into a self-sustaining evolutionary cycle. To mitigate data scarcity, we develop a verifiable synthesis engine that autonomously generates diverse tasks coupled with executable validators. To enable large-scale experience acquisition, we design a scalable infrastructure orchestrating tens of thousands of asynchronous sandbox rollouts. Building on these massive trajectories, we propose an iterative evolving learning strategy to efficiently internalize this experience. This mechanism dynamically regulates policy updates by identifying capability boundaries -- reinforcing successful routines while transforming failure trajectories into rich supervision through error analysis and self-correction. Empirical evaluations on the OSWorld benchmark demonstrate that EvoCUA achieves a success rate of 56.7%, establishing a new open-source state-of-the-art. Notably, EvoCUA significantly outperforms the previous best open-source model, OpenCUA-72B (45.0%), and surpasses leading closed-weights models such as UI-TARS-2 (53.1%). Crucially, our results underscore the generalizability of this approach: the evolving paradigm driven by learning from experience yields consistent performance gains across foundation models of varying scales, establishing a robust and scalable path for advancing native agent capabilities."
    },
    {
      "title": "Collision-Free Humanoid Traversal in Cluttered Indoor Scenes",
      "url": "https://arxiv.org/abs/2601.16035",
      "date": "2026-01-22",
      "authors_text": "Han Xue, Sikai Liang, Zhikai Zhang, Zicheng Zeng, Yun Liu",
      "is_highlight": false,
      "score": 81.2,
      "summary": "The paper presents Humanoid Potential Field (HumanoidPF) for enabling collision-free traversal of humanoids in cluttered indoor environments, demonstrating effective RL-based learning and real-world applicability.",
      "debug_abstract": "We study the problem of collision-free humanoid traversal in cluttered indoor scenes, such as hurdling over objects scattered on the floor, crouching under low-hanging obstacles, or squeezing through narrow passages. To achieve this goal, the humanoid needs to map its perception of surrounding obstacles with diverse spatial layouts and geometries to the corresponding traversal skills. However, the lack of an effective representation that captures humanoid-obstacle relationships during collision avoidance makes directly learning such mappings difficult. We therefore propose Humanoid Potential Field (HumanoidPF), which encodes these relationships as collision-free motion directions, significantly facilitating RL-based traversal skill learning. We also find that HumanoidPF exhibits a surprisingly negligible sim-to-real gap as a perceptual representation. To further enable generalizable traversal skills through diverse and challenging cluttered indoor scenes, we further propose a hybrid scene generation method, incorporating crops of realistic 3D indoor scenes and procedurally synthesized obstacles. We successfully transfer our policy to the real world and develop a teleoperation system where users could command the humanoid to traverse in cluttered indoor scenes with just a single click. Extensive experiments are conducted in both simulation and the real world to validate the effectiveness of our method. Demos and code can be found in our website: this https URL."
    }
  ],
  "智能技术与系统实验室": [
    {
      "title": "EventFlash: Towards Efficient MLLMs for Event-Based Vision",
      "url": "https://arxiv.org/abs/2602.03230",
      "date": "2026-02-03",
      "authors_text": "Shaoyu Liu, Jianing Li, Guanghui Zhao, Yunjian Zhang, Wen Jiang",
      "is_highlight": false,
      "score": 63.0,
      "summary": "EventFlash introduces an efficient MLLM for event-based vision, leveraging spatiotemporal sparsification and adaptive sampling to enhance performance and reduce computational costs.",
      "teaser_image": "https://arxiv.org/html/2602.03230/x2.png",
      "debug_abstract": "Event-based multimodal large language models (MLLMs) enable robust perception in high-speed and low-light scenarios, addressing key limitations of frame-based MLLMs. However, current event-based MLLMs often rely on dense image-like processing paradigms, overlooking the spatiotemporal sparsity of event streams and resulting in high computational cost. In this paper, we propose EventFlash, a novel and efficient MLLM to explore spatiotemporal token sparsification for reducing data redundancy and accelerating inference. Technically, we build EventMind, a large-scale and scene-diverse dataset with over 500k instruction sets, providing both short and long event stream sequences to support our curriculum training strategy. We then present an adaptive temporal window aggregation module for efficient temporal sampling, which adaptively compresses temporal tokens while retaining key temporal cues. Finally, a sparse density-guided attention module is designed to improve spatial token efficiency by selecting informative regions and suppressing empty or sparse areas. Experimental results show that EventFlash achieves a $12.4\\times$ throughput improvement over the baseline (EventFlash-Zero) while maintaining comparable performance. It supports long-range event stream processing with up to 1,000 bins, significantly outperforming the 5-bin limit of EventGPT. We believe EventFlash serves as an efficient foundation model for event-based vision."
    },
    {
      "title": "InstaDrive: Instance-Aware Driving World Models for Realistic and Consistent Video Generation",
      "url": "https://arxiv.org/abs/2602.03242",
      "date": "2026-02-03",
      "authors_text": "Zhuoran Yang, Xi Guo, Chenjing Ding, Chiyu Wang, Wei Wu",
      "is_highlight": false,
      "score": 84.0,
      "summary": "InstaDrive introduces a framework that enhances realistic driving video generation by ensuring instance-level temporal consistency and spatial geometric fidelity through innovative feature extraction and alignment techniques.",
      "teaser_image": "https://arxiv.org/html/2602.03242/x2.png",
      "debug_abstract": "Autonomous driving relies on robust models trained on high-quality, large-scale multi-view driving videos. While world models offer a cost-effective solution for generating realistic driving videos, they struggle to maintain instance-level temporal consistency and spatial geometric fidelity. To address these challenges, we propose InstaDrive, a novel framework that enhances driving video realism through two key advancements: (1) Instance Flow Guider, which extracts and propagates instance features across frames to enforce temporal consistency, preserving instance identity over time. (2) Spatial Geometric Aligner, which improves spatial reasoning, ensures precise instance positioning, and explicitly models occlusion hierarchies. By incorporating these instance-aware mechanisms, InstaDrive achieves state-of-the-art video generation quality and enhances downstream autonomous driving tasks on the nuScenes dataset. Additionally, we utilize CARLA&#39;s autopilot to procedurally and stochastically simulate rare but safety-critical driving scenarios across diverse maps and regions, enabling rigorous safety evaluation for autonomous systems. Our project page is this https URL."
    },
    {
      "title": "Learning Physics-Grounded 4D Dynamics with Neural Gaussian Force Fields",
      "url": "https://arxiv.org/abs/2602.00148",
      "date": "2026-01-29",
      "authors_text": "Shiqian Li, Ruihong Shen, Junfeng Ni, Chang Pan, Chi Zhang",
      "is_highlight": false,
      "score": 82.0,
      "summary": "The Neural Gaussian Force Field (NGFF) framework enables rapid generation of interactive, physically realistic 4D videos from multi-view RGB inputs, enhancing robustness and generalization in physical dynamics.",
      "teaser_image": "https://arxiv.org/html/2602.00148/x1.png",
      "debug_abstract": "Predicting physical dynamics from raw visual data remains a major challenge in AI. While recent video generation models have achieved impressive visual quality, they still cannot consistently generate physically plausible videos due to a lack of modeling of physical laws. Recent approaches combining 3D Gaussian splatting and physics engines can produce physically plausible videos, but are hindered by high computational costs in both reconstruction and simulation, and often lack robustness in complex real-world scenarios. To address these issues, we introduce Neural Gaussian Force Field (NGFF), an end-to-end neural framework that integrates 3D Gaussian perception with physics-based dynamic modeling to generate interactive, physically realistic 4D videos from multi-view RGB inputs, achieving two orders of magnitude faster than prior Gaussian simulators. To support training, we also present GSCollision, a 4D Gaussian dataset featuring diverse materials, multi-object interactions, and complex scenes, totaling over 640k rendered physical videos (~4 TB). Evaluations on synthetic and real 3D scenarios show NGFF&#39;s strong generalization and robustness in physical reasoning, advancing video prediction towards physics-grounded world models."
    },
    {
      "title": "How Far Are LLMs from Professional Poker Players? Revisiting Game-Theoretic Reasoning with Agentic Tool Use",
      "url": "https://arxiv.org/abs/2602.00528",
      "date": "2026-01-31",
      "authors_text": "Minhua Lin, Enyan Dai, Hui Liu, Xianfeng Tang, Yuliang Yan",
      "is_highlight": false,
      "score": 55.0,
      "summary": "This paper evaluates LLMs in poker, revealing their strategic reasoning flaws and proposing ToolPoker, which enhances gameplay and reasoning through integrated external solvers.",
      "teaser_image": "https://arxiv.org/html/2602.00528/x2.png",
      "debug_abstract": "As Large Language Models (LLMs) are increasingly applied in high-stakes domains, their ability to reason strategically under uncertainty becomes critical. Poker provides a rigorous testbed, requiring not only strong actions but also principled, game-theoretic reasoning. In this paper, we conduct a systematic study of LLMs in multiple realistic poker tasks, evaluating both gameplay outcomes and reasoning traces. Our analysis reveals LLMs fail to compete against traditional algorithms and identifies three recurring flaws: reliance on heuristics, factual misunderstandings, and a &#34;knowing-doing&#34; gap where actions diverge from reasoning. An initial attempt with behavior cloning and step-level reinforcement learning improves reasoning style but remains insufficient for accurate game-theoretic play. Motivated by these limitations, we propose ToolPoker, a tool-integrated reasoning framework that combines external solvers for GTO-consistent actions with more precise professional-style explanations. Experiments demonstrate that ToolPoker achieves state-of-the-art gameplay while producing reasoning traces that closely reflect game-theoretic principles."
    },
    {
      "title": "V2X-DSC: Multi-Agent Collaborative Perception with Distributed Source Coding Guided Communication",
      "url": "https://arxiv.org/abs/2602.00687",
      "date": "2026-01-31",
      "authors_text": "Yuankun Zeng, Shaohui Li, Zhi Li, Shulan Ruan, Yu Liu",
      "is_highlight": false,
      "score": 76.0,
      "summary": "V2X-DSC introduces a Conditional Codec for efficient multi-agent collaborative perception by compressing and reconstructing BEV features, optimizing bandwidth usage while maintaining accuracy.",
      "teaser_image": "https://arxiv.org/html/2602.00687/figs/overview.jpg",
      "debug_abstract": "Collaborative perception improves 3D understanding by fusing multi-agent observations, yet intermediate-feature sharing faces strict bandwidth constraints as dense BEV features saturate V2X links. We observe that collaborators view the same physical world, making their features strongly correlated; thus receivers only need innovation beyond their local context. Revisiting this from a distributed source coding perspective, we propose V2X-DSC, a framework with a Conditional Codec (DCC) for bandwidth-constrained fusion. The sender compresses BEV features into compact codes, while the receiver performs conditional reconstruction using its local features as side information, allocating bits to complementary cues rather than redundant content. This conditional structure regularizes learning, encouraging incremental representation and yielding lower-noise features. Experiments on DAIR-V2X, OPV2V, and V2X-Real demonstrate state-of-the-art accuracy-bandwidth trade-offs under KB-level communication, and generalizes as a plug-and-play communication layer across multiple fusion backbones."
    },
    {
      "title": "Resource-Efficient Reinforcement for Reasoning Large Language Models via Dynamic One-Shot Policy Refinement",
      "url": "https://arxiv.org/abs/2602.00815",
      "date": "2026-01-31",
      "authors_text": "Yunjian Zhang, Sudong Wang, Yang Li, Peiran Xu, Conghao Zhou",
      "is_highlight": false,
      "score": 57.0,
      "summary": "The paper introduces Dynamic One-Shot Policy Refinement (DoPR), a resource-efficient reinforcement learning strategy that enhances reasoning in large language models while significantly reducing training costs.",
      "teaser_image": "https://arxiv.org/html/2602.00815/dopr.png",
      "debug_abstract": "Large language models (LLMs) have exhibited remarkable performance on complex reasoning tasks, with reinforcement learning under verifiable rewards (RLVR) emerging as a principled framework for aligning model behavior with reasoning chains. Despite its promise, RLVR remains prohibitively resource-intensive, requiring extensive reward signals and incurring substantial rollout costs during training. In this work, we revisit the fundamental question of data and compute efficiency in RLVR. We first establish a theoretical lower bound on the sample complexity required to unlock reasoning capabilities, and empirically validate that strong performance can be achieved with a surprisingly small number of training instances. To tackle the computational burden, we propose Dynamic One-Shot Policy Refinement (DoPR), an uncertainty-aware RL strategy that dynamically selects a single informative training sample per batch for policy updates, guided by reward volatility and exploration-driven acquisition. DoPR reduces rollout overhead by nearly an order of magnitude while preserving competitive reasoning accuracy, offering a scalable and resource-efficient solution for LLM post-training. This approach offers a practical path toward more efficient and accessible RL-based training for reasoning-intensive LLM applications."
    },
    {
      "title": "Making Avatars Interact: Towards Text-Driven Human-Object Interaction for Controllable Talking Avatars",
      "url": "https://arxiv.org/abs/2602.01538",
      "date": "2026-02-02",
      "authors_text": "Youliang Zhang, Zhengguang Zhou, Zhentao Yu, Ziyao Huang, Teng Hu",
      "is_highlight": false,
      "score": 46.0,
      "summary": "The paper presents InteractAvatar, a dual-stream framework for generating talking avatars that perform text-driven human-object interactions by decoupling perception and planning from video synthesis.",
      "teaser_image": "https://arxiv.org/html/2602.01538/x1.png",
      "debug_abstract": "Generating talking avatars is a fundamental task in video generation. Although existing methods can generate full-body talking avatars with simple human motion, extending this task to grounded human-object interaction (GHOI) remains an open challenge, requiring the avatar to perform text-aligned interactions with surrounding objects. This challenge stems from the need for environmental perception and the control-quality dilemma in GHOI generation. To address this, we propose a novel dual-stream framework, InteractAvatar, which decouples perception and planning from video synthesis for grounded human-object interaction. Leveraging detection to enhance environmental perception, we introduce a Perception and Interaction Module (PIM) to generate text-aligned interaction motions. Additionally, an Audio-Interaction Aware Generation Module (AIM) is proposed to synthesize vivid talking avatars performing object interactions. With a specially designed motion-to-video aligner, PIM and AIM share a similar network structure and enable parallel co-generation of motions and plausible videos, effectively mitigating the control-quality dilemma. Finally, we establish a benchmark, GroundedInter, for evaluating GHOI video generation. Extensive experiments and comparisons demonstrate the effectiveness of our method in generating grounded human-object interactions for talking avatars. Project page: this https URL"
    },
    {
      "title": "Reasoning with Autoregressive-Diffusion Collaborative Thoughts",
      "url": "https://arxiv.org/abs/2602.01608",
      "date": "2026-02-02",
      "authors_text": "Mu Yuan, Liekang Zeng, Guoliang Xing, Lan Zhang, Yunhao Liu",
      "is_highlight": false,
      "score": 41.0,
      "summary": "The paper presents Collaborative Thoughts, a framework integrating autoregressive and diffusion models for enhanced spatial reasoning and generation through iterative feedback and joint reasoning.",
      "teaser_image": "https://arxiv.org/html/2602.01608/x2.png",
      "debug_abstract": "Autoregressive and diffusion models represent two complementary generative paradigms. Autoregressive models excel at sequential planning and constraint composition, yet struggle with tasks that require explicit spatial or physical grounding. Diffusion models, in contrast, capture rich spatial structure through high-dimensional generation, but lack the stepwise logical control needed to satisfy complex, multi-stage constraints or to reliably identify and correct errors. We introduce Collaborative Thoughts, a unified collaborative framework that enables autoregressive and diffusion models to reason and generate jointly through a closed-loop interaction. In Collaborative Thoughts, autoregressive models perform structured planning and constraint management, diffusion models instantiate these constraints as intermediate visual thoughts, and a vision-based critic module evaluates whether the visual thoughts satisfy the intended structural and physical requirements. This feedback is then used to iteratively refine subsequent planning and generation steps, mitigating error propagation across modalities. Importantly, Collaborative Thoughts uses the same collaborative loop regardless of whether the task is autoregressive question answering or diffusion-based visual generation. Through representative examples, we illustrate how Collaborative Thoughts can improve the reliability of spatial reasoning and the controllability of generation."
    },
    {
      "title": "From Frames to Sequences: Temporally Consistent Human-Centric Dense Prediction",
      "url": "https://arxiv.org/abs/2602.01661",
      "date": "2026-02-02",
      "authors_text": "Xingyu Miao, Junting Dong, Qin Zhao, Yuhang Yang, Junhao Chen",
      "is_highlight": false,
      "score": 43.0,
      "summary": "This paper presents a novel synthetic data pipeline and a ViT-based model for achieving temporally consistent human-centric dense prediction in video sequences, outperforming existing methods.",
      "teaser_image": "https://arxiv.org/html/2602.01661/x2.png",
      "debug_abstract": "In this work, we focus on the challenge of temporally consistent human-centric dense prediction across video sequences. Existing models achieve strong per-frame accuracy but often flicker under motion, occlusion, and lighting changes, and they rarely have paired human video supervision for multiple dense tasks. We address this gap with a scalable synthetic data pipeline that generates photorealistic human frames and motion-aligned sequences with pixel-accurate depth, normals, and masks. Unlike prior static data synthetic pipelines, our pipeline provides both frame-level labels for spatial learning and sequence-level supervision for temporal learning. Building on this, we train a unified ViT-based dense predictor that (i) injects an explicit human geometric prior via CSE embeddings and (ii) improves geometry-feature reliability with a lightweight channel reweighting module after feature fusion. Our two-stage training strategy, combining static pretraining with dynamic sequence supervision, enables the model first to acquire robust spatial representations and then refine temporal consistency across motion-aligned sequences. Extensive experiments show that we achieve state-of-the-art performance on THuman2.1 and Hi4D and generalize effectively to in-the-wild videos."
    },
    {
      "title": "ForSim: Stepwise Forward Simulation for Traffic Policy Fine-Tuning",
      "url": "https://arxiv.org/abs/2602.01916",
      "date": "2026-02-02",
      "authors_text": "Keyu Chen, Wenchao Sun, Hao Cheng, Zheng Fu, Sifa Zheng",
      "is_highlight": true,
      "score": 29.0,
      "summary": "ForSim introduces a stepwise closed-loop simulation method that enhances traffic policy fine-tuning by improving agent interactions and preserving multimodal behaviors in autonomous driving.",
      "teaser_image": "https://arxiv.org/html/2602.01916/x2.png",
      "debug_abstract": "As the foundation of closed-loop training and evaluation in autonomous driving, traffic simulation still faces two fundamental challenges: covariate shift introduced by open-loop imitation learning and limited capacity to reflect the multimodal behaviors observed in real-world traffic. Although recent frameworks such as RIFT have partially addressed these issues through group-relative optimization, their forward simulation procedures remain largely non-reactive, leading to unrealistic agent interactions within the virtual domain and ultimately limiting simulation fidelity. To address these issues, we propose ForSim, a stepwise closed-loop forward simulation paradigm. At each virtual timestep, the traffic agent propagates the virtual candidate trajectory that best spatiotemporally matches the reference trajectory through physically grounded motion dynamics, thereby preserving multimodal behavioral diversity while ensuring intra-modality consistency. Other agents are updated with stepwise predictions, yielding coherent and interaction-aware evolution. When incorporated into the RIFT traffic simulation framework, ForSim operates in conjunction with group-relative optimization to fine-tune traffic policy. Extensive experiments confirm that this integration consistently improves safety while maintaining efficiency, realism, and comfort. These results underscore the importance of modeling closed-loop multimodal interactions within forward simulation and enhance the fidelity and reliability of traffic simulation for autonomous driving. Project Page: this https URL"
    },
    {
      "title": "Small Generalizable Prompt Predictive Models Can Steer Efficient RL Post-Training of Large Reasoning Models",
      "url": "https://arxiv.org/abs/2602.01970",
      "date": "2026-02-02",
      "authors_text": "Yun Qu, Qi Wang, Yixiu Mao, Heming Zou, Yuhang Jiang",
      "is_highlight": false,
      "score": 26.0,
      "summary": "The paper presents Generalizable Predictive Prompt Selection (GPS), a method that enhances reinforcement learning efficiency in large language models by utilizing a lightweight generative model for prompt selection.",
      "teaser_image": "https://arxiv.org/html/2602.01970/x1.png",
      "debug_abstract": "Reinforcement learning enhances the reasoning capabilities of large language models but often involves high computational costs due to rollout-intensive optimization. Online prompt selection presents a plausible solution by prioritizing informative prompts to improve training efficiency. However, current methods either depend on costly, exact evaluations or construct prompt-specific predictive models lacking generalization across prompts. This study introduces Generalizable Predictive Prompt Selection (GPS), which performs Bayesian inference towards prompt difficulty using a lightweight generative model trained on the shared optimization history. Intermediate-difficulty prioritization and history-anchored diversity are incorporated into the batch acquisition principle to select informative prompt batches. The small predictive model also generalizes at test-time for efficient computational allocation. Experiments across varied reasoning benchmarks indicate GPS&#39;s substantial improvements in training efficiency, final performance, and test-time efficiency over superior baseline methods."
    },
    {
      "title": "TTT-Parkour: Rapid Test-Time Training for Perceptive Robot Parkour",
      "url": "https://arxiv.org/abs/2602.02331",
      "date": "2026-02-02",
      "authors_text": "Shaoting Zhu, Baijun Ye, Jiaxuan Wang, Jiakang Chen, Ziwen Zhuang",
      "is_highlight": false,
      "score": 18.0,
      "summary": "TTT-Parkour introduces a rapid test-time training framework enabling humanoid robots to effectively navigate complex terrains through efficient real-to-sim-to-real learning and geometry reconstruction.",
      "teaser_image": "https://arxiv.org/html/2602.02331/x1.png",
      "debug_abstract": "Achieving highly dynamic humanoid parkour on unseen, complex terrains remains a challenge in robotics. Although general locomotion policies demonstrate capabilities across broad terrain distributions, they often struggle with arbitrary and highly challenging environments. To overcome this limitation, we propose a real-to-sim-to-real framework that leverages rapid test-time training (TTT) on novel terrains, significantly enhancing the robot&#39;s capability to traverse extremely difficult geometries. We adopt a two-stage end-to-end learning paradigm: a policy is first pre-trained on diverse procedurally generated terrains, followed by rapid fine-tuning on high-fidelity meshes reconstructed from real-world captures. Specifically, we develop a feed-forward, efficient, and high-fidelity geometry reconstruction pipeline using RGB-D inputs, ensuring both speed and quality during test-time training. We demonstrate that TTT-Parkour empowers humanoid robots to master complex obstacles, including wedges, stakes, boxes, trapezoids, and narrow beams. The whole pipeline of capturing, reconstructing, and test-time training requires less than 10 minutes on most tested terrains. Extensive experiments show that the policy after test-time training exhibits robust zero-shot sim-to-real transfer capability."
    },
    {
      "title": "Context Learning for Multi-Agent Discussion",
      "url": "https://arxiv.org/abs/2602.02350",
      "date": "2026-02-02",
      "authors_text": "Xingyuan Hua, Sheng Yue, Xinyi Li, Yizhe Zhao, Jinrui Zhang",
      "is_highlight": false,
      "score": 17.0,
      "summary": "The paper presents M2CL, a multi-LLM context learning method that enhances coherence in Multi-Agent Discussions, improving problem-solving performance by 20%-50% over existing approaches.",
      "teaser_image": "https://arxiv.org/html/2602.02350/x2.png",
      "debug_abstract": "Multi-Agent Discussion (MAD) has garnered increasing attention very recently, where multiple LLM instances collaboratively solve problems via structured discussion. However, we find that current MAD methods easily suffer from discussion inconsistency, LLMs fail to reach a coherent solution, due to the misalignment between their individual this http URL this paper, we introduce a multi-LLM context learning method (M2CL) that learns a context generator for each agent, capable of dynamically generating context instructions per discussion round via automatic information organization and refinement. Specifically, inspired by our theoretical insights on the context instruction, M2CL train the generators to control context coherence and output discrepancies via a carefully crafted self-adaptive this http URL enables LLMs to avoid premature convergence on majority noise and progressively reach the correct consensus. We evaluate M2CL on challenging tasks, including academic reasoning, embodied tasks, and mobile control. The results show that the performance of M2CL significantly surpasses existing methods by 20%--50%, while enjoying favorable transferability and computational efficiency."
    },
    {
      "title": "UV-M3TL: A Unified and Versatile Multimodal Multi-Task Learning Framework for Assistive Driving Perception",
      "url": "https://arxiv.org/abs/2602.01594",
      "date": "2026-02-02",
      "authors_text": "Wenzhuo Liu, Qiannan Guo, Zhen Wang, Wenshuo Wang, Lei Yang",
      "is_highlight": false,
      "score": 8.0,
      "summary": "The UV-M3TL framework enhances ADAS by effectively recognizing driver behavior and context through dual-branch multimodal embedding and adaptive loss, achieving state-of-the-art performance across multiple tasks.",
      "teaser_image": "https://arxiv.org/html/2602.01594/x2.png",
      "debug_abstract": "Advanced Driver Assistance Systems (ADAS) need to understand human driver behavior while perceiving their navigation context, but jointly learning these heterogeneous tasks would cause inter-task negative transfer and impair system performance. Here, we propose a Unified and Versatile Multimodal Multi-Task Learning (UV-M3TL) framework to simultaneously recognize driver behavior, driver emotion, vehicle behavior, and traffic context, while mitigating inter-task negative transfer. Our framework incorporates two core components: dual-branch spatial channel multimodal embedding (DB-SCME) and adaptive feature-decoupled multi-task loss (AFD-Loss). DB-SCME enhances cross-task knowledge transfer while mitigating task conflicts by employing a dual-branch structure to explicitly model salient task-shared and task-specific features. AFD-Loss improves the stability of joint optimization while guiding the model to learn diverse multi-task representations by introducing an adaptive weighting mechanism based on learning dynamics and feature decoupling constraints. We evaluate our method on the AIDE dataset, and the experimental results demonstrate that UV-M3TL achieves state-of-the-art performance across all four tasks. To further prove the versatility, we evaluate UV-M3TL on additional public multi-task perception benchmarks (BDD100K, CityScapes, NYUD-v2, and PASCAL-Context), where it consistently delivers strong performance across diverse task combinations, attaining state-of-the-art results on most tasks."
    },
    {
      "title": "Enhancing Indoor Occupancy Prediction via Sparse Query-Based Multi-Level Consistent Knowledge Distillation",
      "url": "https://arxiv.org/abs/2602.02318",
      "date": "2026-02-02",
      "authors_text": "Xiang Li, Yupeng Zheng, Pengfei Li, Yilun Chen, Ya-Qin Zhang",
      "is_highlight": false,
      "score": 0,
      "summary": "DiScene introduces a sparse query-based framework for efficient and robust indoor occupancy prediction through multi-level knowledge distillation, achieving state-of-the-art performance and faster inference.",
      "teaser_image": "https://arxiv.org/html/2602.02318/x2.png",
      "debug_abstract": "Occupancy prediction provides critical geometric and semantic understanding for robotics but faces efficiency-accuracy trade-offs. Current dense methods suffer computational waste on empty voxels, while sparse query-based approaches lack robustness in diverse and complex indoor scenes. In this paper, we propose DiScene, a novel sparse query-based framework that leverages multi-level distillation to achieve efficient and robust occupancy prediction. In particular, our method incorporates two key innovations: (1) a Multi-level Consistent Knowledge Distillation strategy, which transfers hierarchical representations from large teacher models to lightweight students through coordinated alignment across four levels, including encoder-level feature alignment, query-level feature matching, prior-level spatial guidance, and anchor-level high-confidence knowledge transfer and (2) a Teacher-Guided Initialization policy, employing optimized parameter warm-up to accelerate model convergence. Validated on the Occ-Scannet benchmark, DiScene achieves 23.2 FPS without depth priors while outperforming our baseline method, OPUS, by 36.1% and even better than the depth-enhanced version, OPUS†. With depth integration, DiScene† attains new SOTA performance, surpassing EmbodiedOcc by 3.7% with 1.62$\\times$ faster inference speed. Furthermore, experiments on the Occ3D-nuScenes benchmark and in-the-wild scenarios demonstrate the versatility of our approach in various environments. Code and models can be accessed at this https URL."
    },
    {
      "title": "InspecSafe-V1: A Multimodal Benchmark for Safety Assessment in Industrial Inspection Scenarios",
      "url": "https://arxiv.org/abs/2601.21173",
      "date": "2026-01-29",
      "authors_text": "Zeyi Liu, Shuang Liu, Jihai Min, Zhaoheng Zhang, Jun Cen",
      "is_highlight": false,
      "score": 82.0,
      "summary": "InspecSafe-V1 is a multimodal benchmark dataset for safety assessment in industrial inspections, featuring real-world data from diverse scenarios and multiple sensing modalities.",
      "teaser_image": "https://arxiv.org/html/2601.21173/x1.png",
      "debug_abstract": "With the rapid development of industrial intelligence and unmanned inspection, reliable perception and safety assessment for AI systems in complex and dynamic industrial sites has become a key bottleneck for deploying predictive maintenance and autonomous inspection. Most public datasets remain limited by simulated data sources, single-modality sensing, or the absence of fine-grained object-level annotations, which prevents robust scene understanding and multimodal safety reasoning for industrial foundation models. To address these limitations, InspecSafe-V1 is released as the first multimodal benchmark dataset for industrial inspection safety assessment that is collected from routine operations of real inspection robots in real-world environments. InspecSafe-V1 covers five representative industrial scenarios, including tunnels, power facilities, sintering equipment, oil and gas petrochemical plants, and coal conveyor trestles. The dataset is constructed from 41 wheeled and rail-mounted inspection robots operating at 2,239 valid inspection sites, yielding 5,013 inspection instances. For each instance, pixel-level segmentation annotations are provided for key objects in visible-spectrum images. In addition, a semantic scene description and a corresponding safety level label are provided according to practical inspection tasks. Seven synchronized sensing modalities are further included, including infrared video, audio, depth point clouds, radar point clouds, gas measurements, temperature, and humidity, to support multimodal anomaly recognition, cross-modal fusion, and comprehensive safety assessment in industrial environments."
    },
    {
      "title": "Listen, Look, Drive: Coupling Audio Instructions for User-aware VLA-based Autonomous Driving",
      "url": "https://arxiv.org/abs/2601.12142",
      "date": "2026-01-28",
      "authors_text": "Ziang Guo, Feng Yang, Xuefeng Zhang, Jiaqi Guo, Kun Zhao",
      "is_highlight": false,
      "score": 88.0,
      "summary": "EchoVLA enhances autonomous driving by integrating audio instructions with visual inputs, allowing for user influence and emotional context, significantly improving navigation accuracy and safety.",
      "teaser_image": "https://arxiv.org/html/2601.12142/Figs/overview.png",
      "debug_abstract": "Vision Language Action (VLA) models promise an open-vocabulary interface that can translate perceptual ambiguity into semantically grounded driving decisions, yet they still treat language as a static prior fixed at inference time. As a result, the model must infer continuously shifting objectives from pixels alone, yielding delayed or overly conservative maneuvers. We argue that effective VLAs for autonomous driving need an online channel in which users can influence driving with specific intentions. To this end, we present EchoVLA, a user-aware VLA that couples camera streams with in situ audio instructions. We augment the nuScenes dataset with temporally aligned, intent-specific speech commands generated by converting ego-motion descriptions into synthetic audios. Further, we compose emotional speech-trajectory pairs into a multimodal Chain-of-Thought (CoT) for fine-tuning a Multimodal Large Model (MLM) based on Qwen2.5-Omni. Specifically, we synthesize the audio-augmented dataset with different emotion types paired with corresponding driving behaviors, leveraging the emotional cues embedded in tone, pitch, and speech tempo to reflect varying user states, such as urgent or hesitant intentions, thus enabling our EchoVLA to interpret not only the semantic content but also the emotional context of audio commands for more nuanced and emotionally adaptive driving behavior. In open-loop benchmarks, our approach reduces the average L2 error by $59.4\\%$ and the collision rate by $74.4\\%$ compared to the baseline of vision-only perception. More experiments on nuScenes dataset validate that EchoVLA not only steers the trajectory through audio instructions, but also modulates driving behavior in response to the emotions detected in the user&#39;s speech."
    },
    {
      "title": "HINT: Hierarchical Interaction Modeling for Autoregressive Multi-Human Motion Generation",
      "url": "https://arxiv.org/abs/2601.20383",
      "date": "2026-01-28",
      "authors_text": "Mengge Liu, Yan Di, Gu Wang, Yun Qu, Dekai Zhu",
      "is_highlight": false,
      "score": 78.0,
      "summary": "HINT introduces a novel autoregressive framework for multi-human motion generation, enhancing interaction modeling and performance with variable agent counts and long text guidance.",
      "teaser_image": "https://arxiv.org/html/2601.20383/x2.png",
      "debug_abstract": "Text-driven multi-human motion generation with complex interactions remains a challenging problem. Despite progress in performance, existing offline methods that generate fixed-length motions with a fixed number of agents, are inherently limited in handling long or variable text, and varying agent counts. These limitations naturally encourage autoregressive formulations, which predict future motions step by step conditioned on all past trajectories and current text guidance. In this work, we introduce HINT, the first autoregressive framework for multi-human motion generation with Hierarchical INTeraction modeling in diffusion. First, HINT leverages a disentangled motion representation within a canonicalized latent space, decoupling local motion semantics from inter-person interactions. This design facilitates direct adaptation to varying numbers of human participants without requiring additional refinement. Second, HINT adopts a sliding-window strategy for efficient online generation, and aggregates local within-window and global cross-window conditions to capture past human history, inter-person dependencies, and align with text guidance. This strategy not only enables fine-grained interaction modeling within each window but also preserves long-horizon coherence across all the long sequence. Extensive experiments on public benchmarks demonstrate that HINT matches the performance of strong offline models and surpasses autoregressive baselines. Notably, on InterHuman, HINT achieves an FID of 3.100, significantly improving over the previous state-of-the-art score of 5.154."
    },
    {
      "title": "Self-Supervised Path Planning in Unstructured Environments via Global-Guided Differentiable Hard Constraint Projection",
      "url": "https://arxiv.org/abs/2601.19354",
      "date": "2026-01-27",
      "authors_text": "Ziqian Wang, Chenxi Fang, Zhen Zhang",
      "is_highlight": false,
      "score": 90.0,
      "summary": "This paper presents a self-supervised framework for autonomous navigation in unstructured environments, utilizing a differentiable projection layer to ensure safety and efficiency in path planning.",
      "debug_abstract": "Deploying deep learning agents for autonomous navigation in unstructured environments faces critical challenges regarding safety, data scarcity, and limited computational resources. Traditional solvers often suffer from high latency, while emerging learning-based approaches struggle to ensure deterministic feasibility. To bridge the gap from embodied to embedded intelligence, we propose a self-supervised framework incorporating a differentiable hard constraint projection layer for runtime assurance. To mitigate data scarcity, we construct a Global-Guided Artificial Potential Field (G-APF), which provides dense supervision signals without manual labeling. To enforce actuator limitations and geometric constraints efficiently, we employ an adaptive neural projection layer, which iteratively rectifies the coarse network output onto the feasible manifold. Extensive benchmarks on a test set of 20,000 scenarios demonstrate an 88.75\\% success rate, substantiating the enhanced operational safety. Closed-loop experiments in CARLA further validate the physical realizability of the planned paths under dynamic constraints. Furthermore, deployment verification on an NVIDIA Jetson Orin NX confirms an inference latency of 94 ms, showing real-time feasibility on resource-constrained embedded hardware. This framework offers a generalized paradigm for embedding physical laws into neural architectures, providing a viable direction for solving constrained optimization in mechatronics. Source code is available at: this https URL."
    },
    {
      "title": "AtomMem : Learnable Dynamic Agentic Memory with Atomic Memory Operation",
      "url": "https://arxiv.org/abs/2601.08323",
      "date": "2026-01-27",
      "authors_text": "Yupeng Huo, Yaxi Lu, Zhong Zhang, Haotian Chen, Yankai Lin",
      "is_highlight": false,
      "score": 82.0,
      "summary": "AtomMem introduces a learnable dynamic memory framework that optimizes memory management through atomic CRUD operations, significantly outperforming static methods in long-horizon tasks.",
      "debug_abstract": "Equipping agents with memory is essential for solving real-world long-horizon problems. However, most existing agent memory mechanisms rely on static and hand-crafted workflows. This limits the performance and generalization ability of these memory designs, which highlights the need for a more flexible, learning-based memory framework. In this paper, we propose AtomMem, which reframes memory management as a dynamic decision-making problem. We deconstruct high-level memory processes into fundamental atomic CRUD (Create, Read, Update, Delete) operations, transforming the memory workflow into a learnable decision process. By combining supervised fine-tuning with reinforcement learning, AtomMem learns an autonomous, task-aligned policy to orchestrate memory behaviors tailored to specific task demands. Experimental results across 3 long-context benchmarks demonstrate that the trained AtomMem-8B consistently outperforms prior static-workflow memory methods. Further analysis of training dynamics shows that our learning-based formulation enables the agent to discover structured, task-aligned memory management strategies, highlighting a key advantage over predefined routines."
    },
    {
      "title": "Visual Generation Unlocks Human-Like Reasoning through Multimodal World Models",
      "url": "https://arxiv.org/abs/2601.19834",
      "date": "2026-01-27",
      "authors_text": "Jialong Wu, Xiaoying Zhang, Hongyi Yuan, Xiangcheng Zhang, Tianhao Huang",
      "is_highlight": false,
      "score": 89.0,
      "summary": "This paper demonstrates that visual generation enhances reasoning in multimodal AI, particularly for tasks requiring physical world understanding, outperforming purely verbal approaches.",
      "debug_abstract": "Humans construct internal world models and reason by manipulating the concepts within these models. Recent advances in AI, particularly chain-of-thought (CoT) reasoning, approximate such human cognitive abilities, where world models are believed to be embedded within large language models. Expert-level performance in formal and abstract domains such as mathematics and programming has been achieved in current systems by relying predominantly on verbal reasoning. However, they still lag far behind humans in domains like physical and spatial intelligence, which require richer representations and prior knowledge. The emergence of unified multimodal models (UMMs) capable of both verbal and visual generation has therefore sparked interest in more human-like reasoning grounded in complementary multimodal pathways, though their benefits remain unclear. From a world-model perspective, this paper presents the first principled study of when and how visual generation benefits reasoning. Our key position is the visual superiority hypothesis: for certain tasks--particularly those grounded in the physical world--visual generation more naturally serves as world models, whereas purely verbal world models encounter bottlenecks arising from representational limitations or insufficient prior knowledge. Theoretically, we formalize internal world modeling as a core component of CoT reasoning and analyze distinctions among different forms of world models. Empirically, we identify tasks that necessitate interleaved visual-verbal CoT reasoning, constructing a new evaluation suite, VisWorld-Eval. Controlled experiments on a state-of-the-art UMM show that interleaved CoT significantly outperforms purely verbal CoT on tasks that favor visual world modeling, but offers no clear advantage otherwise. Together, this work clarifies the potential of multimodal world modeling for more powerful, human-like multimodal AI."
    },
    {
      "title": "Scalable Exploration for High-Dimensional Continuous Control via Value-Guided Flow",
      "url": "https://arxiv.org/abs/2601.19707",
      "date": "2026-01-27",
      "authors_text": "Yunyue Wei, Chenhui Zuo, Yanan Sui",
      "is_highlight": false,
      "score": 79.0,
      "summary": "Qflex introduces a scalable reinforcement learning method for high-dimensional continuous control that enhances exploration efficiency by aligning actions with learned value gradients.",
      "debug_abstract": "Controlling high-dimensional systems in biological and robotic applications is challenging due to expansive state-action spaces, where effective exploration is critical. Commonly used exploration strategies in reinforcement learning are largely undirected with sharp degradation as action dimensionality grows. Many existing methods resort to dimensionality reduction, which constrains policy expressiveness and forfeits system flexibility. We introduce Q-guided Flow Exploration (Qflex), a scalable reinforcement learning method that conducts exploration directly in the native high-dimensional action space. During training, Qflex traverses actions from a learnable source distribution along a probability flow induced by the learned value function, aligning exploration with task-relevant gradients rather than isotropic noise. Our proposed method substantially outperforms representative online reinforcement learning baselines across diverse high-dimensional continuous-control benchmarks. Qflex also successfully controls a full-body human musculoskeletal model to perform agile, complex movements, demonstrating superior scalability and sample efficiency in very high-dimensional settings. Our results indicate that value-guided flows offer a principled and practical route to exploration at scale."
    },
    {
      "title": "Latent-Space Contrastive Reinforcement Learning for Stable and Efficient LLM Reasoning",
      "url": "https://arxiv.org/abs/2601.17275",
      "date": "2026-01-24",
      "authors_text": "Lianlei Shan, Han Chen, Yixuan Wang, Zhenjie Liu, Wei Li",
      "is_highlight": false,
      "score": 60.0,
      "summary": "The paper introduces DeepLatent Reasoning (DLR), a latent-space contrastive reinforcement learning framework that enhances LLM reasoning by addressing inefficiencies and stability in multi-step tasks.",
      "debug_abstract": "While Large Language Models (LLMs) demonstrate exceptional performance in surface-level text generation, their nature in handling complex multi-step reasoning tasks often remains one of ``statistical fitting&#39;&#39; rather than systematic logical deduction. Traditional Reinforcement Learning (RL) attempts to mitigate this by introducing a ``think-before-speak&#39;&#39; paradigm. However, applying RL directly in high-dimensional, discrete token spaces faces three inherent challenges: sample-inefficient rollouts, high gradient estimation variance, and the risk of catastrophic forgetting. To fundamentally address these structural bottlenecks, we propose \\textbf{DeepLatent Reasoning (DLR)}, a latent-space bidirectional contrastive reinforcement learning framework. This framework shifts the trial-and-error cost from expensive token-level full sequence generation to the continuous latent manifold. Specifically, we introduce a lightweight assistant model to efficiently sample $K$ reasoning chain encodings within the latent space. These encodings are filtered via a dual reward mechanism based on correctness and formatting; only high-value latent trajectories are fed into a \\textbf{frozen main model} for single-pass decoding. To maximize reasoning diversity while maintaining coherence, we design a contrastive learning objective to enable directed exploration within the latent space. Since the main model parameters remain frozen during optimization, this method mathematically eliminates catastrophic forgetting. Experiments demonstrate that under comparable GPU computational budgets, DLR achieves more stable training convergence, supports longer-horizon reasoning chains, and facilitates the sustainable accumulation of reasoning capabilities, providing a viable path toward reliable and scalable reinforcement learning for LLMs."
    },
    {
      "title": "MetaWorld: Skill Transfer and Composition in a Hierarchical World Model for Grounding High-Level Instructions",
      "url": "https://arxiv.org/abs/2601.17507",
      "date": "2026-01-24",
      "authors_text": "Yutong Shen, Hangxu Liu, Kailin Pei, Ruizhe Xia, Tongtong Feng",
      "is_highlight": false,
      "score": 92.0,
      "summary": "MetaWorld introduces a hierarchical world model that enhances humanoid robot loco-manipulation by integrating semantic planning and physical control through expert policy transfer, improving task performance and motion coherence.",
      "debug_abstract": "Humanoid robot loco-manipulation remains constrained by the semantic-physical gap. Current methods face three limitations: Low sample efficiency in reinforcement learning, poor generalization in imitation learning, and physical inconsistency in VLMs. We propose MetaWorld, a hierarchical world model that integrates semantic planning and physical control via expert policy transfer. The framework decouples tasks into a VLM-driven semantic layer and a latent dynamics model operating in a compact state space. Our dynamic expert selection and motion prior fusion mechanism leverages a pre-trained multi-expert policy library as transferable knowledge, enabling efficient online adaptation via a two-stage framework. VLMs serve as semantic interfaces, mapping instructions to executable skills and bypassing symbol grounding. Experiments on Humanoid-Bench show MetaWorld outperforms world model-based RL in task completion and motion coherence. Our code will be found at this https URL"
    },
    {
      "title": "The Flexibility Trap: Why Arbitrary Order Limits Reasoning Potential in Diffusion Language Models",
      "url": "https://arxiv.org/abs/2601.15165",
      "date": "2026-01-26",
      "authors_text": "Zanlin Ni, Shenzhi Wang, Yang Yue, Tianyu Yu, Weilin Zhao",
      "is_highlight": false,
      "score": 45.0,
      "summary": "This paper argues that arbitrary order generation in diffusion language models limits reasoning potential, advocating for a simpler approach using Group Relative Policy Optimization for improved performance.",
      "debug_abstract": "Diffusion Large Language Models (dLLMs) break the rigid left-to-right constraint of traditional LLMs, enabling token generation in arbitrary orders. Intuitively, this flexibility implies a solution space that strictly supersets the fixed autoregressive trajectory, theoretically unlocking superior reasoning potential for general tasks like mathematics and coding. Consequently, numerous works have leveraged reinforcement learning (RL) to elicit the reasoning capability of dLLMs. In this paper, we reveal a counter-intuitive reality: arbitrary order generation, in its current form, narrows rather than expands the reasoning boundary of dLLMs. We find that dLLMs tend to exploit this order flexibility to bypass high-uncertainty tokens that are crucial for exploration, leading to a premature collapse of the solution space. This observation motivates a rethink of RL approaches for dLLMs, where considerable complexities, such as handling combinatorial trajectories and intractable likelihoods, are often devoted to preserving this flexibility. We demonstrate that effective reasoning can be better elicited by intentionally forgoing arbitrary order and applying standard Group Relative Policy Optimization (GRPO) instead. Our approach, JustGRPO, is minimalist yet surprisingly effective (e.g., 89.1% accuracy on GSM8K) while fully retaining the parallel decoding ability of dLLMs. Project page: this https URL"
    },
    {
      "title": "HomoFM: Deep Homography Estimation with Flow Matching",
      "url": "https://arxiv.org/abs/2601.18222",
      "date": "2026-01-26",
      "authors_text": "Mengfan He, Liangzheng Sun, Chunyu Li, Ziyang Meng",
      "is_highlight": false,
      "score": 49.0,
      "summary": "HomoFM introduces a flow matching technique for deep homography estimation, enhancing accuracy and robustness against domain shifts through velocity field learning and domain-invariant representations.",
      "debug_abstract": "Deep homography estimation has broad applications in computer vision and robotics. Remarkable progresses have been achieved while the existing methods typically treat it as a direct regression or iterative refinement problem and often struggling to capture complex geometric transformations or generalize across different domains. In this work, we propose HomoFM, a new framework that introduces the flow matching technique from generative modeling into the homography estimation task for the first time. Unlike the existing methods, we formulate homography estimation problem as a velocity field learning problem. By modeling a continuous and point-wise velocity field that transforms noisy distributions into registered coordinates, the proposed network recovers high-precision transformations through a conditional flow trajectory. Furthermore, to address the challenge of domain shifts issue, e.g., the cases of multimodal matching or varying illumination scenarios, we integrate a gradient reversal layer (GRL) into the feature extraction backbone. This domain adaptation strategy explicitly constrains the encoder to learn domain-invariant representations, significantly enhancing the network&#39;s robustness. Extensive experiments demonstrate the effectiveness of the proposed method, showing that HomoFM outperforms state-of-the-art methods in both estimation accuracy and robustness on standard benchmarks. Code and data resource are available at this https URL."
    },
    {
      "title": "Dep-Search: Learning Dependency-Aware Reasoning Traces with Persistent Memory",
      "url": "https://arxiv.org/abs/2601.18771",
      "date": "2026-01-26",
      "authors_text": "Yanming Liu, Xinyue Peng, Zixuan Yan, Yanxin Shen, Wenjie Xu",
      "is_highlight": false,
      "score": 46.0,
      "summary": "Dep-Search enhances large language models' multi-hop reasoning by integrating dependency-aware search, structured retrieval, and persistent memory, outperforming existing frameworks in complex tasks.",
      "debug_abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in complex reasoning tasks, particularly when augmented with search mechanisms that enable systematic exploration of external knowledge bases. The field has evolved from traditional retrieval-augmented generation (RAG) frameworks to more sophisticated search-based frameworks that orchestrate multi-step reasoning through explicit search strategies. However, existing search frameworks still rely heavily on implicit natural language reasoning to determine search strategies and how to leverage retrieved information across reasoning steps. This reliance on implicit reasoning creates fundamental challenges for managing dependencies between sub-questions, efficiently reusing previously retrieved knowledge, and learning optimal search strategies through reinforcement learning. To address these limitations, we propose Dep-Search, a dependency-aware search framework that advances beyond existing search frameworks by integrating structured reasoning, retrieval, and persistent memory through GRPO. Dep-Search introduces explicit control mechanisms that enable the model to decompose questions with dependency relationships, retrieve information when needed, access previously stored knowledge from memory, and summarize long reasoning contexts into reusable memory entries. Through extensive experiments on seven diverse question answering datasets, we demonstrate that Dep-Search significantly enhances LLMs&#39; ability to tackle complex multi-hop reasoning tasks, achieving substantial improvements over strong baselines across different model scales."
    },
    {
      "title": "FantasyVLN: Unified Multimodal Chain-of-Thought Reasoning for Vision-Language Navigation",
      "url": "https://arxiv.org/abs/2601.13976",
      "date": "2026-01-23",
      "authors_text": "Jing Zuo, Lingzhou Mu, Fan Jiang, Chengcheng Ma, Mu Xu",
      "is_highlight": false,
      "score": 85.0,
      "summary": "FantasyVLN introduces a unified implicit reasoning framework for Vision-Language Navigation that enhances efficiency and success rates by optimizing Chain-of-Thought reasoning without excessive token overhead.",
      "debug_abstract": "Achieving human-level performance in Vision-and-Language Navigation (VLN) requires an embodied agent to jointly understand multimodal instructions and visual-spatial context while reasoning over long action sequences. Recent works, such as NavCoT and NavGPT-2, demonstrate the potential of Chain-of-Thought (CoT) reasoning for improving interpretability and long-horizon planning. Moreover, multimodal extensions like OctoNav-R1 and CoT-VLA further validate CoT as a promising pathway toward human-like navigation reasoning. However, existing approaches face critical drawbacks: purely textual CoTs lack spatial grounding and easily overfit to sparse annotated reasoning steps, while multimodal CoTs incur severe token inflation by generating imagined visual observations, making real-time navigation impractical. In this work, we propose FantasyVLN, a unified implicit reasoning framework that preserves the benefits of CoT reasoning without explicit token overhead. Specifically, imagined visual tokens are encoded into a compact latent space using a pretrained Visual AutoRegressor (VAR) during CoT reasoning training, and the model jointly learns from textual, visual, and multimodal CoT modes under a unified multi-CoT strategy. At inference, our model performs direct instruction-to-action mapping while still enjoying reasoning-aware representations. Extensive experiments on LH-VLN show that our approach achieves reasoning-aware yet real-time navigation, improving success rates and efficiency while reducing inference latency by an order of magnitude compared to explicit CoT methods."
    },
    {
      "title": "TangramPuzzle: Evaluating Multimodal Large Language Models with Compositional Spatial Reasoning",
      "url": "https://arxiv.org/abs/2601.16520",
      "date": "2026-01-23",
      "authors_text": "Daixian Liu, Jiayi Kuang, Yinghui Li, Yangning Li, Di Yin",
      "is_highlight": false,
      "score": 82.0,
      "summary": "The TangramPuzzle benchmark evaluates multimodal large language models' compositional spatial reasoning through precise geometric tasks, revealing their tendency to prioritize silhouette matching over geometric accuracy.",
      "debug_abstract": "Multimodal Large Language Models (MLLMs) have achieved remarkable progress in visual recognition and semantic understanding. Nevertheless, their ability to perform precise compositional spatial reasoning remains largely unexplored. Existing benchmarks often involve relatively simple tasks and rely on semantic approximations or coarse relative positioning, while their evaluation metrics are typically limited and lack rigorous mathematical formulations. To bridge this gap, we introduce TangramPuzzle, a geometry-grounded benchmark designed to evaluate compositional spatial reasoning through the lens of the classic Tangram game. We propose the Tangram Construction Expression (TCE), a symbolic geometric framework that grounds tangram assemblies in exact, machine-verifiable coordinate specifications, to mitigate the ambiguity of visual approximation. We design two complementary tasks: Outline Prediction, which demands inferring global shapes from local components, and End-to-End Code Generation, which requires solving inverse geometric assembly problems. We conduct extensive evaluation experiments on advanced open-source and proprietary models, revealing an interesting insight: MLLMs tend to prioritize matching the target silhouette while neglecting geometric constraints, leading to distortions or deformations of the pieces."
    },
    {
      "title": "AnchoredDream: Zero-Shot 360° Indoor Scene Generation from a Single View via Geometric Grounding",
      "url": "https://arxiv.org/abs/2601.16532",
      "date": "2026-01-23",
      "authors_text": "Runmao Yao, Junsheng Zhou, Zhen Dong, Yu-Shen Liu",
      "is_highlight": false,
      "score": 72.0,
      "summary": "AnchoredDream introduces a zero-shot method for generating 360° indoor scenes from a single image, enhancing appearance consistency and geometric plausibility through a mutual boosting mechanism.",
      "debug_abstract": "Single-view indoor scene generation plays a crucial role in a range of real-world applications. However, generating a complete 360° scene from a single image remains a highly ill-posed and challenging problem. Recent approaches have made progress by leveraging diffusion models and depth estimation networks, yet they still struggle to maintain appearance consistency and geometric plausibility under large viewpoint changes, limiting their effectiveness in full-scene generation. To address this, we propose AnchoredDream, a novel zero-shot pipeline that anchors 360° scene generation on high-fidelity geometry via an appearance-geometry mutual boosting mechanism. Given a single-view image, our method first performs appearance-guided geometry generation to construct a reliable 3D scene layout. Then, we progressively generate the complete scene through a series of modules: warp-and-inpaint, warp-and-refine, post-optimization, and a novel Grouting Block, which ensures seamless transitions between the input view and generated regions. Extensive experiments demonstrate that AnchoredDream outperforms existing methods by a large margin in both appearance consistency and geometric plausibility--all in a zero-shot manner. Our results highlight the potential of geometric grounding for high-quality, zero-shot single-view scene generation."
    },
    {
      "title": "OnlineSI: Taming Large Language Model for Online 3D Understanding and Grounding",
      "url": "https://arxiv.org/abs/2601.16538",
      "date": "2026-01-23",
      "authors_text": "Zixian Liu, Zhaoxi Chen, Liang Pan, Ziwei Liu",
      "is_highlight": false,
      "score": 88.0,
      "summary": "OnlineSI is a framework that enhances Multimodal Large Language Models' spatial understanding in dynamic environments by integrating continuous learning from video streams and 3D point cloud data.",
      "debug_abstract": "In recent years, researchers have increasingly been interested in how to enable Multimodal Large Language Models (MLLM) to possess spatial understanding and reasoning capabilities. However, most existing methods overlook the importance of the ability to continuously work in an ever-changing world, and lack the possibility of deployment on embodied systems in real-world environments. In this work, we introduce OnlineSI, a framework that can continuously improve its spatial understanding of its surroundings given a video stream. Our core idea is to maintain a finite spatial memory to retain past observations, ensuring the computation required for each inference does not increase as the input accumulates. We further integrate 3D point cloud information with semantic information, helping MLLM to better locate and identify objects in the scene. To evaluate our method, we introduce the Fuzzy $F_1$-Score to mitigate ambiguity, and test our method on two representative datasets. Experiments demonstrate the effectiveness of our method, paving the way towards real-world embodied systems."
    },
    {
      "title": "GRIP: Algorithm-Agnostic Machine Unlearning for Mixture-of-Experts via Geometric Router Constraints",
      "url": "https://arxiv.org/abs/2601.16905",
      "date": "2026-01-23",
      "authors_text": "Andy Zhu, Rongzhe Wei, Yupu Gu, Pan Li",
      "is_highlight": false,
      "score": 62.0,
      "summary": "The GRIP framework enables effective machine unlearning in Mixture-of-Experts models by imposing geometric constraints on router updates, ensuring knowledge erasure without sacrificing model utility.",
      "debug_abstract": "Machine unlearning (MU) for large language models has become critical for AI safety, yet existing methods fail to generalize to Mixture-of-Experts (MoE) architectures. We identify that traditional unlearning methods exploit MoE&#39;s architectural vulnerability: they manipulate routers to redirect queries away from knowledgeable experts rather than erasing knowledge, causing a loss of model utility and superficial forgetting. We propose Geometric Routing Invariance Preservation (GRIP), an algorithm-agnostic framework for unlearning for MoE. Our core contribution is a geometric constraint, implemented by projecting router gradient updates into an expert-specific null-space. Crucially, this decouples routing stability from parameter rigidity: while discrete expert selections remain stable for retained knowledge, the continuous router parameters remain plastic within the null space, allowing the model to undergo necessary internal reconfiguration to satisfy unlearning objectives. This forces the unlearning optimization to erase knowledge directly from expert parameters rather than exploiting the superficial router manipulation shortcut. GRIP functions as an adapter, constraining router parameter updates without modifying the underlying unlearning algorithm. Extensive experiments on large-scale MoE models demonstrate that our adapter eliminates expert selection shift (achieving over 95% routing stability) across all tested unlearning methods while preserving their utility. By preventing existing algorithms from exploiting MoE model&#39;s router vulnerability, GRIP adapts existing unlearning research from dense architectures to MoEs."
    },
    {
      "title": "Render-of-Thought: Rendering Textual Chain-of-Thought as Images for Visual Latent Reasoning",
      "url": "https://arxiv.org/abs/2601.14750",
      "date": "2026-01-22",
      "authors_text": "Yifan Wang, Shiyu Li, Peiming Li, Xiaochen Yang, Yang Tang",
      "is_highlight": false,
      "score": 92.5,
      "summary": "The Render-of-Thought framework transforms Chain-of-Thought reasoning into images, enhancing analyzability and efficiency while maintaining competitive performance in reasoning tasks.",
      "debug_abstract": "Chain-of-Thought (CoT) prompting has achieved remarkable success in unlocking the reasoning capabilities of Large Language Models (LLMs). Although CoT prompting enhances reasoning, its verbosity imposes substantial computational overhead. Recent works often focus exclusively on outcome alignment and lack supervision on the intermediate reasoning process. These deficiencies obscure the analyzability of the latent reasoning chain. To address these challenges, we introduce Render-of-Thought (RoT), the first framework to reify the reasoning chain by rendering textual steps into images, making the latent rationale explicit and traceable. Specifically, we leverage the vision encoders of existing Vision Language Models (VLMs) as semantic anchors to align the vision embeddings with the textual space. This design ensures plug-and-play implementation without incurring additional pre-training overhead. Extensive experiments on mathematical and logical reasoning benchmarks demonstrate that our method achieves 3-4x token compression and substantial inference acceleration compared to explicit CoT. Furthermore, it maintains competitive performance against other methods, validating the feasibility of this paradigm. Our code is available at this https URL"
    },
    {
      "title": "LLM-in-Sandbox Elicits General Agentic Intelligence",
      "url": "https://arxiv.org/abs/2601.16206",
      "date": "2026-01-22",
      "authors_text": "Daixuan Cheng, Shaohan Huang, Yuxian Gu, Huatong Song, Guoxin Chen",
      "is_highlight": false,
      "score": 95.0,
      "summary": "The LLM-in-Sandbox framework enables large language models to demonstrate general intelligence in non-code tasks through sandbox exploration and reinforcement learning, achieving robust generalization across various domains.",
      "debug_abstract": "We introduce LLM-in-Sandbox, enabling LLMs to explore within a code sandbox (i.e., a virtual computer), to elicit general intelligence in non-code domains. We first demonstrate that strong LLMs, without additional training, exhibit generalization capabilities to leverage the code sandbox for non-code tasks. For example, LLMs spontaneously access external resources to acquire new knowledge, leverage the file system to handle long contexts, and execute scripts to satisfy formatting requirements. We further show that these agentic capabilities can be enhanced through LLM-in-Sandbox Reinforcement Learning (LLM-in-Sandbox-RL), which uses only non-agentic data to train models for sandbox exploration. Experiments demonstrate that LLM-in-Sandbox, in both training-free and post-trained settings, achieves robust generalization spanning mathematics, physics, chemistry, biomedicine, long-context understanding, and instruction following. Finally, we analyze LLM-in-Sandbox&#39;s efficiency from computational and system perspectives, and open-source it as a Python package to facilitate real-world deployment."
    },
    {
      "title": "Collision-Free Humanoid Traversal in Cluttered Indoor Scenes",
      "url": "https://arxiv.org/abs/2601.16035",
      "date": "2026-01-22",
      "authors_text": "Han Xue, Sikai Liang, Zhikai Zhang, Zicheng Zeng, Yun Liu",
      "is_highlight": false,
      "score": 81.2,
      "summary": "The paper presents Humanoid Potential Field (HumanoidPF) for enabling collision-free traversal of humanoids in cluttered indoor environments, demonstrating effective RL-based learning and real-world applicability.",
      "debug_abstract": "We study the problem of collision-free humanoid traversal in cluttered indoor scenes, such as hurdling over objects scattered on the floor, crouching under low-hanging obstacles, or squeezing through narrow passages. To achieve this goal, the humanoid needs to map its perception of surrounding obstacles with diverse spatial layouts and geometries to the corresponding traversal skills. However, the lack of an effective representation that captures humanoid-obstacle relationships during collision avoidance makes directly learning such mappings difficult. We therefore propose Humanoid Potential Field (HumanoidPF), which encodes these relationships as collision-free motion directions, significantly facilitating RL-based traversal skill learning. We also find that HumanoidPF exhibits a surprisingly negligible sim-to-real gap as a perceptual representation. To further enable generalizable traversal skills through diverse and challenging cluttered indoor scenes, we further propose a hybrid scene generation method, incorporating crops of realistic 3D indoor scenes and procedurally synthesized obstacles. We successfully transfer our policy to the real world and develop a teleoperation system where users could command the humanoid to traverse in cluttered indoor scenes with just a single click. Extensive experiments are conducted in both simulation and the real world to validate the effectiveness of our method. Demos and code can be found in our website: this https URL."
    }
  ],
  "智能系统与机器人实验室 (ISR Lab)": [
    {
      "title": "EventFlash: Towards Efficient MLLMs for Event-Based Vision",
      "url": "https://arxiv.org/abs/2602.03230",
      "date": "2026-02-03",
      "authors_text": "Shaoyu Liu, Jianing Li, Guanghui Zhao, Yunjian Zhang, Wen Jiang",
      "is_highlight": false,
      "score": 63.0,
      "summary": "EventFlash introduces an efficient MLLM for event-based vision, leveraging spatiotemporal sparsification and adaptive sampling to enhance performance and reduce computational costs.",
      "teaser_image": "https://arxiv.org/html/2602.03230/x2.png",
      "debug_abstract": "Event-based multimodal large language models (MLLMs) enable robust perception in high-speed and low-light scenarios, addressing key limitations of frame-based MLLMs. However, current event-based MLLMs often rely on dense image-like processing paradigms, overlooking the spatiotemporal sparsity of event streams and resulting in high computational cost. In this paper, we propose EventFlash, a novel and efficient MLLM to explore spatiotemporal token sparsification for reducing data redundancy and accelerating inference. Technically, we build EventMind, a large-scale and scene-diverse dataset with over 500k instruction sets, providing both short and long event stream sequences to support our curriculum training strategy. We then present an adaptive temporal window aggregation module for efficient temporal sampling, which adaptively compresses temporal tokens while retaining key temporal cues. Finally, a sparse density-guided attention module is designed to improve spatial token efficiency by selecting informative regions and suppressing empty or sparse areas. Experimental results show that EventFlash achieves a $12.4\\times$ throughput improvement over the baseline (EventFlash-Zero) while maintaining comparable performance. It supports long-range event stream processing with up to 1,000 bins, significantly outperforming the 5-bin limit of EventGPT. We believe EventFlash serves as an efficient foundation model for event-based vision."
    },
    {
      "title": "InstaDrive: Instance-Aware Driving World Models for Realistic and Consistent Video Generation",
      "url": "https://arxiv.org/abs/2602.03242",
      "date": "2026-02-03",
      "authors_text": "Zhuoran Yang, Xi Guo, Chenjing Ding, Chiyu Wang, Wei Wu",
      "is_highlight": false,
      "score": 84.0,
      "summary": "InstaDrive introduces a framework that enhances realistic driving video generation by ensuring instance-level temporal consistency and spatial geometric fidelity through innovative feature extraction and alignment techniques.",
      "teaser_image": "https://arxiv.org/html/2602.03242/x2.png",
      "debug_abstract": "Autonomous driving relies on robust models trained on high-quality, large-scale multi-view driving videos. While world models offer a cost-effective solution for generating realistic driving videos, they struggle to maintain instance-level temporal consistency and spatial geometric fidelity. To address these challenges, we propose InstaDrive, a novel framework that enhances driving video realism through two key advancements: (1) Instance Flow Guider, which extracts and propagates instance features across frames to enforce temporal consistency, preserving instance identity over time. (2) Spatial Geometric Aligner, which improves spatial reasoning, ensures precise instance positioning, and explicitly models occlusion hierarchies. By incorporating these instance-aware mechanisms, InstaDrive achieves state-of-the-art video generation quality and enhances downstream autonomous driving tasks on the nuScenes dataset. Additionally, we utilize CARLA&#39;s autopilot to procedurally and stochastically simulate rare but safety-critical driving scenarios across diverse maps and regions, enabling rigorous safety evaluation for autonomous systems. Our project page is this https URL."
    },
    {
      "title": "Learning Physics-Grounded 4D Dynamics with Neural Gaussian Force Fields",
      "url": "https://arxiv.org/abs/2602.00148",
      "date": "2026-01-29",
      "authors_text": "Shiqian Li, Ruihong Shen, Junfeng Ni, Chang Pan, Chi Zhang",
      "is_highlight": false,
      "score": 82.0,
      "summary": "The Neural Gaussian Force Field (NGFF) framework enables rapid generation of interactive, physically realistic 4D videos from multi-view RGB inputs, enhancing robustness and generalization in physical dynamics.",
      "teaser_image": "https://arxiv.org/html/2602.00148/x1.png",
      "debug_abstract": "Predicting physical dynamics from raw visual data remains a major challenge in AI. While recent video generation models have achieved impressive visual quality, they still cannot consistently generate physically plausible videos due to a lack of modeling of physical laws. Recent approaches combining 3D Gaussian splatting and physics engines can produce physically plausible videos, but are hindered by high computational costs in both reconstruction and simulation, and often lack robustness in complex real-world scenarios. To address these issues, we introduce Neural Gaussian Force Field (NGFF), an end-to-end neural framework that integrates 3D Gaussian perception with physics-based dynamic modeling to generate interactive, physically realistic 4D videos from multi-view RGB inputs, achieving two orders of magnitude faster than prior Gaussian simulators. To support training, we also present GSCollision, a 4D Gaussian dataset featuring diverse materials, multi-object interactions, and complex scenes, totaling over 640k rendered physical videos (~4 TB). Evaluations on synthetic and real 3D scenarios show NGFF&#39;s strong generalization and robustness in physical reasoning, advancing video prediction towards physics-grounded world models."
    },
    {
      "title": "How Far Are LLMs from Professional Poker Players? Revisiting Game-Theoretic Reasoning with Agentic Tool Use",
      "url": "https://arxiv.org/abs/2602.00528",
      "date": "2026-01-31",
      "authors_text": "Minhua Lin, Enyan Dai, Hui Liu, Xianfeng Tang, Yuliang Yan",
      "is_highlight": false,
      "score": 55.0,
      "summary": "This paper evaluates LLMs in poker, revealing their strategic reasoning flaws and proposing ToolPoker, which enhances gameplay and reasoning through integrated external solvers.",
      "teaser_image": "https://arxiv.org/html/2602.00528/x2.png",
      "debug_abstract": "As Large Language Models (LLMs) are increasingly applied in high-stakes domains, their ability to reason strategically under uncertainty becomes critical. Poker provides a rigorous testbed, requiring not only strong actions but also principled, game-theoretic reasoning. In this paper, we conduct a systematic study of LLMs in multiple realistic poker tasks, evaluating both gameplay outcomes and reasoning traces. Our analysis reveals LLMs fail to compete against traditional algorithms and identifies three recurring flaws: reliance on heuristics, factual misunderstandings, and a &#34;knowing-doing&#34; gap where actions diverge from reasoning. An initial attempt with behavior cloning and step-level reinforcement learning improves reasoning style but remains insufficient for accurate game-theoretic play. Motivated by these limitations, we propose ToolPoker, a tool-integrated reasoning framework that combines external solvers for GTO-consistent actions with more precise professional-style explanations. Experiments demonstrate that ToolPoker achieves state-of-the-art gameplay while producing reasoning traces that closely reflect game-theoretic principles."
    },
    {
      "title": "V2X-DSC: Multi-Agent Collaborative Perception with Distributed Source Coding Guided Communication",
      "url": "https://arxiv.org/abs/2602.00687",
      "date": "2026-01-31",
      "authors_text": "Yuankun Zeng, Shaohui Li, Zhi Li, Shulan Ruan, Yu Liu",
      "is_highlight": false,
      "score": 76.0,
      "summary": "V2X-DSC introduces a Conditional Codec for efficient multi-agent collaborative perception by compressing and reconstructing BEV features, optimizing bandwidth usage while maintaining accuracy.",
      "teaser_image": "https://arxiv.org/html/2602.00687/figs/overview.jpg",
      "debug_abstract": "Collaborative perception improves 3D understanding by fusing multi-agent observations, yet intermediate-feature sharing faces strict bandwidth constraints as dense BEV features saturate V2X links. We observe that collaborators view the same physical world, making their features strongly correlated; thus receivers only need innovation beyond their local context. Revisiting this from a distributed source coding perspective, we propose V2X-DSC, a framework with a Conditional Codec (DCC) for bandwidth-constrained fusion. The sender compresses BEV features into compact codes, while the receiver performs conditional reconstruction using its local features as side information, allocating bits to complementary cues rather than redundant content. This conditional structure regularizes learning, encouraging incremental representation and yielding lower-noise features. Experiments on DAIR-V2X, OPV2V, and V2X-Real demonstrate state-of-the-art accuracy-bandwidth trade-offs under KB-level communication, and generalizes as a plug-and-play communication layer across multiple fusion backbones."
    },
    {
      "title": "Resource-Efficient Reinforcement for Reasoning Large Language Models via Dynamic One-Shot Policy Refinement",
      "url": "https://arxiv.org/abs/2602.00815",
      "date": "2026-01-31",
      "authors_text": "Yunjian Zhang, Sudong Wang, Yang Li, Peiran Xu, Conghao Zhou",
      "is_highlight": false,
      "score": 57.0,
      "summary": "The paper introduces Dynamic One-Shot Policy Refinement (DoPR), a resource-efficient reinforcement learning strategy that enhances reasoning in large language models while significantly reducing training costs.",
      "teaser_image": "https://arxiv.org/html/2602.00815/dopr.png",
      "debug_abstract": "Large language models (LLMs) have exhibited remarkable performance on complex reasoning tasks, with reinforcement learning under verifiable rewards (RLVR) emerging as a principled framework for aligning model behavior with reasoning chains. Despite its promise, RLVR remains prohibitively resource-intensive, requiring extensive reward signals and incurring substantial rollout costs during training. In this work, we revisit the fundamental question of data and compute efficiency in RLVR. We first establish a theoretical lower bound on the sample complexity required to unlock reasoning capabilities, and empirically validate that strong performance can be achieved with a surprisingly small number of training instances. To tackle the computational burden, we propose Dynamic One-Shot Policy Refinement (DoPR), an uncertainty-aware RL strategy that dynamically selects a single informative training sample per batch for policy updates, guided by reward volatility and exploration-driven acquisition. DoPR reduces rollout overhead by nearly an order of magnitude while preserving competitive reasoning accuracy, offering a scalable and resource-efficient solution for LLM post-training. This approach offers a practical path toward more efficient and accessible RL-based training for reasoning-intensive LLM applications."
    },
    {
      "title": "Making Avatars Interact: Towards Text-Driven Human-Object Interaction for Controllable Talking Avatars",
      "url": "https://arxiv.org/abs/2602.01538",
      "date": "2026-02-02",
      "authors_text": "Youliang Zhang, Zhengguang Zhou, Zhentao Yu, Ziyao Huang, Teng Hu",
      "is_highlight": false,
      "score": 46.0,
      "summary": "The paper presents InteractAvatar, a dual-stream framework for generating talking avatars that perform text-driven human-object interactions by decoupling perception and planning from video synthesis.",
      "teaser_image": "https://arxiv.org/html/2602.01538/x1.png",
      "debug_abstract": "Generating talking avatars is a fundamental task in video generation. Although existing methods can generate full-body talking avatars with simple human motion, extending this task to grounded human-object interaction (GHOI) remains an open challenge, requiring the avatar to perform text-aligned interactions with surrounding objects. This challenge stems from the need for environmental perception and the control-quality dilemma in GHOI generation. To address this, we propose a novel dual-stream framework, InteractAvatar, which decouples perception and planning from video synthesis for grounded human-object interaction. Leveraging detection to enhance environmental perception, we introduce a Perception and Interaction Module (PIM) to generate text-aligned interaction motions. Additionally, an Audio-Interaction Aware Generation Module (AIM) is proposed to synthesize vivid talking avatars performing object interactions. With a specially designed motion-to-video aligner, PIM and AIM share a similar network structure and enable parallel co-generation of motions and plausible videos, effectively mitigating the control-quality dilemma. Finally, we establish a benchmark, GroundedInter, for evaluating GHOI video generation. Extensive experiments and comparisons demonstrate the effectiveness of our method in generating grounded human-object interactions for talking avatars. Project page: this https URL"
    },
    {
      "title": "Reasoning with Autoregressive-Diffusion Collaborative Thoughts",
      "url": "https://arxiv.org/abs/2602.01608",
      "date": "2026-02-02",
      "authors_text": "Mu Yuan, Liekang Zeng, Guoliang Xing, Lan Zhang, Yunhao Liu",
      "is_highlight": false,
      "score": 41.0,
      "summary": "The paper presents Collaborative Thoughts, a framework integrating autoregressive and diffusion models for enhanced spatial reasoning and generation through iterative feedback and joint reasoning.",
      "teaser_image": "https://arxiv.org/html/2602.01608/x2.png",
      "debug_abstract": "Autoregressive and diffusion models represent two complementary generative paradigms. Autoregressive models excel at sequential planning and constraint composition, yet struggle with tasks that require explicit spatial or physical grounding. Diffusion models, in contrast, capture rich spatial structure through high-dimensional generation, but lack the stepwise logical control needed to satisfy complex, multi-stage constraints or to reliably identify and correct errors. We introduce Collaborative Thoughts, a unified collaborative framework that enables autoregressive and diffusion models to reason and generate jointly through a closed-loop interaction. In Collaborative Thoughts, autoregressive models perform structured planning and constraint management, diffusion models instantiate these constraints as intermediate visual thoughts, and a vision-based critic module evaluates whether the visual thoughts satisfy the intended structural and physical requirements. This feedback is then used to iteratively refine subsequent planning and generation steps, mitigating error propagation across modalities. Importantly, Collaborative Thoughts uses the same collaborative loop regardless of whether the task is autoregressive question answering or diffusion-based visual generation. Through representative examples, we illustrate how Collaborative Thoughts can improve the reliability of spatial reasoning and the controllability of generation."
    },
    {
      "title": "From Frames to Sequences: Temporally Consistent Human-Centric Dense Prediction",
      "url": "https://arxiv.org/abs/2602.01661",
      "date": "2026-02-02",
      "authors_text": "Xingyu Miao, Junting Dong, Qin Zhao, Yuhang Yang, Junhao Chen",
      "is_highlight": false,
      "score": 43.0,
      "summary": "This paper presents a novel synthetic data pipeline and a ViT-based model for achieving temporally consistent human-centric dense prediction in video sequences, outperforming existing methods.",
      "teaser_image": "https://arxiv.org/html/2602.01661/x2.png",
      "debug_abstract": "In this work, we focus on the challenge of temporally consistent human-centric dense prediction across video sequences. Existing models achieve strong per-frame accuracy but often flicker under motion, occlusion, and lighting changes, and they rarely have paired human video supervision for multiple dense tasks. We address this gap with a scalable synthetic data pipeline that generates photorealistic human frames and motion-aligned sequences with pixel-accurate depth, normals, and masks. Unlike prior static data synthetic pipelines, our pipeline provides both frame-level labels for spatial learning and sequence-level supervision for temporal learning. Building on this, we train a unified ViT-based dense predictor that (i) injects an explicit human geometric prior via CSE embeddings and (ii) improves geometry-feature reliability with a lightweight channel reweighting module after feature fusion. Our two-stage training strategy, combining static pretraining with dynamic sequence supervision, enables the model first to acquire robust spatial representations and then refine temporal consistency across motion-aligned sequences. Extensive experiments show that we achieve state-of-the-art performance on THuman2.1 and Hi4D and generalize effectively to in-the-wild videos."
    },
    {
      "title": "ForSim: Stepwise Forward Simulation for Traffic Policy Fine-Tuning",
      "url": "https://arxiv.org/abs/2602.01916",
      "date": "2026-02-02",
      "authors_text": "Keyu Chen, Wenchao Sun, Hao Cheng, Zheng Fu, Sifa Zheng",
      "is_highlight": true,
      "score": 29.0,
      "summary": "ForSim introduces a stepwise closed-loop simulation method that enhances traffic policy fine-tuning by improving agent interactions and preserving multimodal behaviors in autonomous driving.",
      "teaser_image": "https://arxiv.org/html/2602.01916/x2.png",
      "debug_abstract": "As the foundation of closed-loop training and evaluation in autonomous driving, traffic simulation still faces two fundamental challenges: covariate shift introduced by open-loop imitation learning and limited capacity to reflect the multimodal behaviors observed in real-world traffic. Although recent frameworks such as RIFT have partially addressed these issues through group-relative optimization, their forward simulation procedures remain largely non-reactive, leading to unrealistic agent interactions within the virtual domain and ultimately limiting simulation fidelity. To address these issues, we propose ForSim, a stepwise closed-loop forward simulation paradigm. At each virtual timestep, the traffic agent propagates the virtual candidate trajectory that best spatiotemporally matches the reference trajectory through physically grounded motion dynamics, thereby preserving multimodal behavioral diversity while ensuring intra-modality consistency. Other agents are updated with stepwise predictions, yielding coherent and interaction-aware evolution. When incorporated into the RIFT traffic simulation framework, ForSim operates in conjunction with group-relative optimization to fine-tune traffic policy. Extensive experiments confirm that this integration consistently improves safety while maintaining efficiency, realism, and comfort. These results underscore the importance of modeling closed-loop multimodal interactions within forward simulation and enhance the fidelity and reliability of traffic simulation for autonomous driving. Project Page: this https URL"
    },
    {
      "title": "Small Generalizable Prompt Predictive Models Can Steer Efficient RL Post-Training of Large Reasoning Models",
      "url": "https://arxiv.org/abs/2602.01970",
      "date": "2026-02-02",
      "authors_text": "Yun Qu, Qi Wang, Yixiu Mao, Heming Zou, Yuhang Jiang",
      "is_highlight": false,
      "score": 26.0,
      "summary": "The paper presents Generalizable Predictive Prompt Selection (GPS), a method that enhances reinforcement learning efficiency in large language models by utilizing a lightweight generative model for prompt selection.",
      "teaser_image": "https://arxiv.org/html/2602.01970/x1.png",
      "debug_abstract": "Reinforcement learning enhances the reasoning capabilities of large language models but often involves high computational costs due to rollout-intensive optimization. Online prompt selection presents a plausible solution by prioritizing informative prompts to improve training efficiency. However, current methods either depend on costly, exact evaluations or construct prompt-specific predictive models lacking generalization across prompts. This study introduces Generalizable Predictive Prompt Selection (GPS), which performs Bayesian inference towards prompt difficulty using a lightweight generative model trained on the shared optimization history. Intermediate-difficulty prioritization and history-anchored diversity are incorporated into the batch acquisition principle to select informative prompt batches. The small predictive model also generalizes at test-time for efficient computational allocation. Experiments across varied reasoning benchmarks indicate GPS&#39;s substantial improvements in training efficiency, final performance, and test-time efficiency over superior baseline methods."
    },
    {
      "title": "TTT-Parkour: Rapid Test-Time Training for Perceptive Robot Parkour",
      "url": "https://arxiv.org/abs/2602.02331",
      "date": "2026-02-02",
      "authors_text": "Shaoting Zhu, Baijun Ye, Jiaxuan Wang, Jiakang Chen, Ziwen Zhuang",
      "is_highlight": false,
      "score": 18.0,
      "summary": "TTT-Parkour introduces a rapid test-time training framework enabling humanoid robots to effectively navigate complex terrains through efficient real-to-sim-to-real learning and geometry reconstruction.",
      "teaser_image": "https://arxiv.org/html/2602.02331/x1.png",
      "debug_abstract": "Achieving highly dynamic humanoid parkour on unseen, complex terrains remains a challenge in robotics. Although general locomotion policies demonstrate capabilities across broad terrain distributions, they often struggle with arbitrary and highly challenging environments. To overcome this limitation, we propose a real-to-sim-to-real framework that leverages rapid test-time training (TTT) on novel terrains, significantly enhancing the robot&#39;s capability to traverse extremely difficult geometries. We adopt a two-stage end-to-end learning paradigm: a policy is first pre-trained on diverse procedurally generated terrains, followed by rapid fine-tuning on high-fidelity meshes reconstructed from real-world captures. Specifically, we develop a feed-forward, efficient, and high-fidelity geometry reconstruction pipeline using RGB-D inputs, ensuring both speed and quality during test-time training. We demonstrate that TTT-Parkour empowers humanoid robots to master complex obstacles, including wedges, stakes, boxes, trapezoids, and narrow beams. The whole pipeline of capturing, reconstructing, and test-time training requires less than 10 minutes on most tested terrains. Extensive experiments show that the policy after test-time training exhibits robust zero-shot sim-to-real transfer capability."
    },
    {
      "title": "Context Learning for Multi-Agent Discussion",
      "url": "https://arxiv.org/abs/2602.02350",
      "date": "2026-02-02",
      "authors_text": "Xingyuan Hua, Sheng Yue, Xinyi Li, Yizhe Zhao, Jinrui Zhang",
      "is_highlight": false,
      "score": 17.0,
      "summary": "The paper presents M2CL, a multi-LLM context learning method that enhances coherence in Multi-Agent Discussions, improving problem-solving performance by 20%-50% over existing approaches.",
      "teaser_image": "https://arxiv.org/html/2602.02350/x2.png",
      "debug_abstract": "Multi-Agent Discussion (MAD) has garnered increasing attention very recently, where multiple LLM instances collaboratively solve problems via structured discussion. However, we find that current MAD methods easily suffer from discussion inconsistency, LLMs fail to reach a coherent solution, due to the misalignment between their individual this http URL this paper, we introduce a multi-LLM context learning method (M2CL) that learns a context generator for each agent, capable of dynamically generating context instructions per discussion round via automatic information organization and refinement. Specifically, inspired by our theoretical insights on the context instruction, M2CL train the generators to control context coherence and output discrepancies via a carefully crafted self-adaptive this http URL enables LLMs to avoid premature convergence on majority noise and progressively reach the correct consensus. We evaluate M2CL on challenging tasks, including academic reasoning, embodied tasks, and mobile control. The results show that the performance of M2CL significantly surpasses existing methods by 20%--50%, while enjoying favorable transferability and computational efficiency."
    },
    {
      "title": "UV-M3TL: A Unified and Versatile Multimodal Multi-Task Learning Framework for Assistive Driving Perception",
      "url": "https://arxiv.org/abs/2602.01594",
      "date": "2026-02-02",
      "authors_text": "Wenzhuo Liu, Qiannan Guo, Zhen Wang, Wenshuo Wang, Lei Yang",
      "is_highlight": false,
      "score": 8.0,
      "summary": "The UV-M3TL framework enhances ADAS by effectively recognizing driver behavior and context through dual-branch multimodal embedding and adaptive loss, achieving state-of-the-art performance across multiple tasks.",
      "teaser_image": "https://arxiv.org/html/2602.01594/x2.png",
      "debug_abstract": "Advanced Driver Assistance Systems (ADAS) need to understand human driver behavior while perceiving their navigation context, but jointly learning these heterogeneous tasks would cause inter-task negative transfer and impair system performance. Here, we propose a Unified and Versatile Multimodal Multi-Task Learning (UV-M3TL) framework to simultaneously recognize driver behavior, driver emotion, vehicle behavior, and traffic context, while mitigating inter-task negative transfer. Our framework incorporates two core components: dual-branch spatial channel multimodal embedding (DB-SCME) and adaptive feature-decoupled multi-task loss (AFD-Loss). DB-SCME enhances cross-task knowledge transfer while mitigating task conflicts by employing a dual-branch structure to explicitly model salient task-shared and task-specific features. AFD-Loss improves the stability of joint optimization while guiding the model to learn diverse multi-task representations by introducing an adaptive weighting mechanism based on learning dynamics and feature decoupling constraints. We evaluate our method on the AIDE dataset, and the experimental results demonstrate that UV-M3TL achieves state-of-the-art performance across all four tasks. To further prove the versatility, we evaluate UV-M3TL on additional public multi-task perception benchmarks (BDD100K, CityScapes, NYUD-v2, and PASCAL-Context), where it consistently delivers strong performance across diverse task combinations, attaining state-of-the-art results on most tasks."
    },
    {
      "title": "Enhancing Indoor Occupancy Prediction via Sparse Query-Based Multi-Level Consistent Knowledge Distillation",
      "url": "https://arxiv.org/abs/2602.02318",
      "date": "2026-02-02",
      "authors_text": "Xiang Li, Yupeng Zheng, Pengfei Li, Yilun Chen, Ya-Qin Zhang",
      "is_highlight": false,
      "score": 0,
      "summary": "DiScene introduces a sparse query-based framework for efficient and robust indoor occupancy prediction through multi-level knowledge distillation, achieving state-of-the-art performance and faster inference.",
      "teaser_image": "https://arxiv.org/html/2602.02318/x2.png",
      "debug_abstract": "Occupancy prediction provides critical geometric and semantic understanding for robotics but faces efficiency-accuracy trade-offs. Current dense methods suffer computational waste on empty voxels, while sparse query-based approaches lack robustness in diverse and complex indoor scenes. In this paper, we propose DiScene, a novel sparse query-based framework that leverages multi-level distillation to achieve efficient and robust occupancy prediction. In particular, our method incorporates two key innovations: (1) a Multi-level Consistent Knowledge Distillation strategy, which transfers hierarchical representations from large teacher models to lightweight students through coordinated alignment across four levels, including encoder-level feature alignment, query-level feature matching, prior-level spatial guidance, and anchor-level high-confidence knowledge transfer and (2) a Teacher-Guided Initialization policy, employing optimized parameter warm-up to accelerate model convergence. Validated on the Occ-Scannet benchmark, DiScene achieves 23.2 FPS without depth priors while outperforming our baseline method, OPUS, by 36.1% and even better than the depth-enhanced version, OPUS†. With depth integration, DiScene† attains new SOTA performance, surpassing EmbodiedOcc by 3.7% with 1.62$\\times$ faster inference speed. Furthermore, experiments on the Occ3D-nuScenes benchmark and in-the-wild scenarios demonstrate the versatility of our approach in various environments. Code and models can be accessed at this https URL."
    },
    {
      "title": "InspecSafe-V1: A Multimodal Benchmark for Safety Assessment in Industrial Inspection Scenarios",
      "url": "https://arxiv.org/abs/2601.21173",
      "date": "2026-01-29",
      "authors_text": "Zeyi Liu, Shuang Liu, Jihai Min, Zhaoheng Zhang, Jun Cen",
      "is_highlight": false,
      "score": 82.0,
      "summary": "InspecSafe-V1 is a multimodal benchmark dataset for safety assessment in industrial inspections, featuring real-world data from diverse scenarios and multiple sensing modalities.",
      "teaser_image": "https://arxiv.org/html/2601.21173/x1.png",
      "debug_abstract": "With the rapid development of industrial intelligence and unmanned inspection, reliable perception and safety assessment for AI systems in complex and dynamic industrial sites has become a key bottleneck for deploying predictive maintenance and autonomous inspection. Most public datasets remain limited by simulated data sources, single-modality sensing, or the absence of fine-grained object-level annotations, which prevents robust scene understanding and multimodal safety reasoning for industrial foundation models. To address these limitations, InspecSafe-V1 is released as the first multimodal benchmark dataset for industrial inspection safety assessment that is collected from routine operations of real inspection robots in real-world environments. InspecSafe-V1 covers five representative industrial scenarios, including tunnels, power facilities, sintering equipment, oil and gas petrochemical plants, and coal conveyor trestles. The dataset is constructed from 41 wheeled and rail-mounted inspection robots operating at 2,239 valid inspection sites, yielding 5,013 inspection instances. For each instance, pixel-level segmentation annotations are provided for key objects in visible-spectrum images. In addition, a semantic scene description and a corresponding safety level label are provided according to practical inspection tasks. Seven synchronized sensing modalities are further included, including infrared video, audio, depth point clouds, radar point clouds, gas measurements, temperature, and humidity, to support multimodal anomaly recognition, cross-modal fusion, and comprehensive safety assessment in industrial environments."
    },
    {
      "title": "Listen, Look, Drive: Coupling Audio Instructions for User-aware VLA-based Autonomous Driving",
      "url": "https://arxiv.org/abs/2601.12142",
      "date": "2026-01-28",
      "authors_text": "Ziang Guo, Feng Yang, Xuefeng Zhang, Jiaqi Guo, Kun Zhao",
      "is_highlight": false,
      "score": 88.0,
      "summary": "EchoVLA enhances autonomous driving by integrating audio instructions with visual inputs, allowing for user influence and emotional context, significantly improving navigation accuracy and safety.",
      "teaser_image": "https://arxiv.org/html/2601.12142/Figs/overview.png",
      "debug_abstract": "Vision Language Action (VLA) models promise an open-vocabulary interface that can translate perceptual ambiguity into semantically grounded driving decisions, yet they still treat language as a static prior fixed at inference time. As a result, the model must infer continuously shifting objectives from pixels alone, yielding delayed or overly conservative maneuvers. We argue that effective VLAs for autonomous driving need an online channel in which users can influence driving with specific intentions. To this end, we present EchoVLA, a user-aware VLA that couples camera streams with in situ audio instructions. We augment the nuScenes dataset with temporally aligned, intent-specific speech commands generated by converting ego-motion descriptions into synthetic audios. Further, we compose emotional speech-trajectory pairs into a multimodal Chain-of-Thought (CoT) for fine-tuning a Multimodal Large Model (MLM) based on Qwen2.5-Omni. Specifically, we synthesize the audio-augmented dataset with different emotion types paired with corresponding driving behaviors, leveraging the emotional cues embedded in tone, pitch, and speech tempo to reflect varying user states, such as urgent or hesitant intentions, thus enabling our EchoVLA to interpret not only the semantic content but also the emotional context of audio commands for more nuanced and emotionally adaptive driving behavior. In open-loop benchmarks, our approach reduces the average L2 error by $59.4\\%$ and the collision rate by $74.4\\%$ compared to the baseline of vision-only perception. More experiments on nuScenes dataset validate that EchoVLA not only steers the trajectory through audio instructions, but also modulates driving behavior in response to the emotions detected in the user&#39;s speech."
    },
    {
      "title": "HINT: Hierarchical Interaction Modeling for Autoregressive Multi-Human Motion Generation",
      "url": "https://arxiv.org/abs/2601.20383",
      "date": "2026-01-28",
      "authors_text": "Mengge Liu, Yan Di, Gu Wang, Yun Qu, Dekai Zhu",
      "is_highlight": false,
      "score": 78.0,
      "summary": "HINT introduces a novel autoregressive framework for multi-human motion generation, enhancing interaction modeling and performance with variable agent counts and long text guidance.",
      "teaser_image": "https://arxiv.org/html/2601.20383/x2.png",
      "debug_abstract": "Text-driven multi-human motion generation with complex interactions remains a challenging problem. Despite progress in performance, existing offline methods that generate fixed-length motions with a fixed number of agents, are inherently limited in handling long or variable text, and varying agent counts. These limitations naturally encourage autoregressive formulations, which predict future motions step by step conditioned on all past trajectories and current text guidance. In this work, we introduce HINT, the first autoregressive framework for multi-human motion generation with Hierarchical INTeraction modeling in diffusion. First, HINT leverages a disentangled motion representation within a canonicalized latent space, decoupling local motion semantics from inter-person interactions. This design facilitates direct adaptation to varying numbers of human participants without requiring additional refinement. Second, HINT adopts a sliding-window strategy for efficient online generation, and aggregates local within-window and global cross-window conditions to capture past human history, inter-person dependencies, and align with text guidance. This strategy not only enables fine-grained interaction modeling within each window but also preserves long-horizon coherence across all the long sequence. Extensive experiments on public benchmarks demonstrate that HINT matches the performance of strong offline models and surpasses autoregressive baselines. Notably, on InterHuman, HINT achieves an FID of 3.100, significantly improving over the previous state-of-the-art score of 5.154."
    },
    {
      "title": "Self-Supervised Path Planning in Unstructured Environments via Global-Guided Differentiable Hard Constraint Projection",
      "url": "https://arxiv.org/abs/2601.19354",
      "date": "2026-01-27",
      "authors_text": "Ziqian Wang, Chenxi Fang, Zhen Zhang",
      "is_highlight": false,
      "score": 90.0,
      "summary": "This paper presents a self-supervised framework for autonomous navigation in unstructured environments, utilizing a differentiable projection layer to ensure safety and efficiency in path planning.",
      "debug_abstract": "Deploying deep learning agents for autonomous navigation in unstructured environments faces critical challenges regarding safety, data scarcity, and limited computational resources. Traditional solvers often suffer from high latency, while emerging learning-based approaches struggle to ensure deterministic feasibility. To bridge the gap from embodied to embedded intelligence, we propose a self-supervised framework incorporating a differentiable hard constraint projection layer for runtime assurance. To mitigate data scarcity, we construct a Global-Guided Artificial Potential Field (G-APF), which provides dense supervision signals without manual labeling. To enforce actuator limitations and geometric constraints efficiently, we employ an adaptive neural projection layer, which iteratively rectifies the coarse network output onto the feasible manifold. Extensive benchmarks on a test set of 20,000 scenarios demonstrate an 88.75\\% success rate, substantiating the enhanced operational safety. Closed-loop experiments in CARLA further validate the physical realizability of the planned paths under dynamic constraints. Furthermore, deployment verification on an NVIDIA Jetson Orin NX confirms an inference latency of 94 ms, showing real-time feasibility on resource-constrained embedded hardware. This framework offers a generalized paradigm for embedding physical laws into neural architectures, providing a viable direction for solving constrained optimization in mechatronics. Source code is available at: this https URL."
    },
    {
      "title": "AtomMem : Learnable Dynamic Agentic Memory with Atomic Memory Operation",
      "url": "https://arxiv.org/abs/2601.08323",
      "date": "2026-01-27",
      "authors_text": "Yupeng Huo, Yaxi Lu, Zhong Zhang, Haotian Chen, Yankai Lin",
      "is_highlight": false,
      "score": 82.0,
      "summary": "AtomMem introduces a learnable dynamic memory framework that optimizes memory management through atomic CRUD operations, significantly outperforming static methods in long-horizon tasks.",
      "debug_abstract": "Equipping agents with memory is essential for solving real-world long-horizon problems. However, most existing agent memory mechanisms rely on static and hand-crafted workflows. This limits the performance and generalization ability of these memory designs, which highlights the need for a more flexible, learning-based memory framework. In this paper, we propose AtomMem, which reframes memory management as a dynamic decision-making problem. We deconstruct high-level memory processes into fundamental atomic CRUD (Create, Read, Update, Delete) operations, transforming the memory workflow into a learnable decision process. By combining supervised fine-tuning with reinforcement learning, AtomMem learns an autonomous, task-aligned policy to orchestrate memory behaviors tailored to specific task demands. Experimental results across 3 long-context benchmarks demonstrate that the trained AtomMem-8B consistently outperforms prior static-workflow memory methods. Further analysis of training dynamics shows that our learning-based formulation enables the agent to discover structured, task-aligned memory management strategies, highlighting a key advantage over predefined routines."
    },
    {
      "title": "Visual Generation Unlocks Human-Like Reasoning through Multimodal World Models",
      "url": "https://arxiv.org/abs/2601.19834",
      "date": "2026-01-27",
      "authors_text": "Jialong Wu, Xiaoying Zhang, Hongyi Yuan, Xiangcheng Zhang, Tianhao Huang",
      "is_highlight": false,
      "score": 89.0,
      "summary": "This paper demonstrates that visual generation enhances reasoning in multimodal AI, particularly for tasks requiring physical world understanding, outperforming purely verbal approaches.",
      "debug_abstract": "Humans construct internal world models and reason by manipulating the concepts within these models. Recent advances in AI, particularly chain-of-thought (CoT) reasoning, approximate such human cognitive abilities, where world models are believed to be embedded within large language models. Expert-level performance in formal and abstract domains such as mathematics and programming has been achieved in current systems by relying predominantly on verbal reasoning. However, they still lag far behind humans in domains like physical and spatial intelligence, which require richer representations and prior knowledge. The emergence of unified multimodal models (UMMs) capable of both verbal and visual generation has therefore sparked interest in more human-like reasoning grounded in complementary multimodal pathways, though their benefits remain unclear. From a world-model perspective, this paper presents the first principled study of when and how visual generation benefits reasoning. Our key position is the visual superiority hypothesis: for certain tasks--particularly those grounded in the physical world--visual generation more naturally serves as world models, whereas purely verbal world models encounter bottlenecks arising from representational limitations or insufficient prior knowledge. Theoretically, we formalize internal world modeling as a core component of CoT reasoning and analyze distinctions among different forms of world models. Empirically, we identify tasks that necessitate interleaved visual-verbal CoT reasoning, constructing a new evaluation suite, VisWorld-Eval. Controlled experiments on a state-of-the-art UMM show that interleaved CoT significantly outperforms purely verbal CoT on tasks that favor visual world modeling, but offers no clear advantage otherwise. Together, this work clarifies the potential of multimodal world modeling for more powerful, human-like multimodal AI."
    },
    {
      "title": "Scalable Exploration for High-Dimensional Continuous Control via Value-Guided Flow",
      "url": "https://arxiv.org/abs/2601.19707",
      "date": "2026-01-27",
      "authors_text": "Yunyue Wei, Chenhui Zuo, Yanan Sui",
      "is_highlight": false,
      "score": 79.0,
      "summary": "Qflex introduces a scalable reinforcement learning method for high-dimensional continuous control that enhances exploration efficiency by aligning actions with learned value gradients.",
      "debug_abstract": "Controlling high-dimensional systems in biological and robotic applications is challenging due to expansive state-action spaces, where effective exploration is critical. Commonly used exploration strategies in reinforcement learning are largely undirected with sharp degradation as action dimensionality grows. Many existing methods resort to dimensionality reduction, which constrains policy expressiveness and forfeits system flexibility. We introduce Q-guided Flow Exploration (Qflex), a scalable reinforcement learning method that conducts exploration directly in the native high-dimensional action space. During training, Qflex traverses actions from a learnable source distribution along a probability flow induced by the learned value function, aligning exploration with task-relevant gradients rather than isotropic noise. Our proposed method substantially outperforms representative online reinforcement learning baselines across diverse high-dimensional continuous-control benchmarks. Qflex also successfully controls a full-body human musculoskeletal model to perform agile, complex movements, demonstrating superior scalability and sample efficiency in very high-dimensional settings. Our results indicate that value-guided flows offer a principled and practical route to exploration at scale."
    },
    {
      "title": "Latent-Space Contrastive Reinforcement Learning for Stable and Efficient LLM Reasoning",
      "url": "https://arxiv.org/abs/2601.17275",
      "date": "2026-01-24",
      "authors_text": "Lianlei Shan, Han Chen, Yixuan Wang, Zhenjie Liu, Wei Li",
      "is_highlight": false,
      "score": 60.0,
      "summary": "The paper introduces DeepLatent Reasoning (DLR), a latent-space contrastive reinforcement learning framework that enhances LLM reasoning by addressing inefficiencies and stability in multi-step tasks.",
      "debug_abstract": "While Large Language Models (LLMs) demonstrate exceptional performance in surface-level text generation, their nature in handling complex multi-step reasoning tasks often remains one of ``statistical fitting&#39;&#39; rather than systematic logical deduction. Traditional Reinforcement Learning (RL) attempts to mitigate this by introducing a ``think-before-speak&#39;&#39; paradigm. However, applying RL directly in high-dimensional, discrete token spaces faces three inherent challenges: sample-inefficient rollouts, high gradient estimation variance, and the risk of catastrophic forgetting. To fundamentally address these structural bottlenecks, we propose \\textbf{DeepLatent Reasoning (DLR)}, a latent-space bidirectional contrastive reinforcement learning framework. This framework shifts the trial-and-error cost from expensive token-level full sequence generation to the continuous latent manifold. Specifically, we introduce a lightweight assistant model to efficiently sample $K$ reasoning chain encodings within the latent space. These encodings are filtered via a dual reward mechanism based on correctness and formatting; only high-value latent trajectories are fed into a \\textbf{frozen main model} for single-pass decoding. To maximize reasoning diversity while maintaining coherence, we design a contrastive learning objective to enable directed exploration within the latent space. Since the main model parameters remain frozen during optimization, this method mathematically eliminates catastrophic forgetting. Experiments demonstrate that under comparable GPU computational budgets, DLR achieves more stable training convergence, supports longer-horizon reasoning chains, and facilitates the sustainable accumulation of reasoning capabilities, providing a viable path toward reliable and scalable reinforcement learning for LLMs."
    },
    {
      "title": "MetaWorld: Skill Transfer and Composition in a Hierarchical World Model for Grounding High-Level Instructions",
      "url": "https://arxiv.org/abs/2601.17507",
      "date": "2026-01-24",
      "authors_text": "Yutong Shen, Hangxu Liu, Kailin Pei, Ruizhe Xia, Tongtong Feng",
      "is_highlight": false,
      "score": 92.0,
      "summary": "MetaWorld introduces a hierarchical world model that enhances humanoid robot loco-manipulation by integrating semantic planning and physical control through expert policy transfer, improving task performance and motion coherence.",
      "debug_abstract": "Humanoid robot loco-manipulation remains constrained by the semantic-physical gap. Current methods face three limitations: Low sample efficiency in reinforcement learning, poor generalization in imitation learning, and physical inconsistency in VLMs. We propose MetaWorld, a hierarchical world model that integrates semantic planning and physical control via expert policy transfer. The framework decouples tasks into a VLM-driven semantic layer and a latent dynamics model operating in a compact state space. Our dynamic expert selection and motion prior fusion mechanism leverages a pre-trained multi-expert policy library as transferable knowledge, enabling efficient online adaptation via a two-stage framework. VLMs serve as semantic interfaces, mapping instructions to executable skills and bypassing symbol grounding. Experiments on Humanoid-Bench show MetaWorld outperforms world model-based RL in task completion and motion coherence. Our code will be found at this https URL"
    },
    {
      "title": "The Flexibility Trap: Why Arbitrary Order Limits Reasoning Potential in Diffusion Language Models",
      "url": "https://arxiv.org/abs/2601.15165",
      "date": "2026-01-26",
      "authors_text": "Zanlin Ni, Shenzhi Wang, Yang Yue, Tianyu Yu, Weilin Zhao",
      "is_highlight": false,
      "score": 45.0,
      "summary": "This paper argues that arbitrary order generation in diffusion language models limits reasoning potential, advocating for a simpler approach using Group Relative Policy Optimization for improved performance.",
      "debug_abstract": "Diffusion Large Language Models (dLLMs) break the rigid left-to-right constraint of traditional LLMs, enabling token generation in arbitrary orders. Intuitively, this flexibility implies a solution space that strictly supersets the fixed autoregressive trajectory, theoretically unlocking superior reasoning potential for general tasks like mathematics and coding. Consequently, numerous works have leveraged reinforcement learning (RL) to elicit the reasoning capability of dLLMs. In this paper, we reveal a counter-intuitive reality: arbitrary order generation, in its current form, narrows rather than expands the reasoning boundary of dLLMs. We find that dLLMs tend to exploit this order flexibility to bypass high-uncertainty tokens that are crucial for exploration, leading to a premature collapse of the solution space. This observation motivates a rethink of RL approaches for dLLMs, where considerable complexities, such as handling combinatorial trajectories and intractable likelihoods, are often devoted to preserving this flexibility. We demonstrate that effective reasoning can be better elicited by intentionally forgoing arbitrary order and applying standard Group Relative Policy Optimization (GRPO) instead. Our approach, JustGRPO, is minimalist yet surprisingly effective (e.g., 89.1% accuracy on GSM8K) while fully retaining the parallel decoding ability of dLLMs. Project page: this https URL"
    },
    {
      "title": "HomoFM: Deep Homography Estimation with Flow Matching",
      "url": "https://arxiv.org/abs/2601.18222",
      "date": "2026-01-26",
      "authors_text": "Mengfan He, Liangzheng Sun, Chunyu Li, Ziyang Meng",
      "is_highlight": false,
      "score": 49.0,
      "summary": "HomoFM introduces a flow matching technique for deep homography estimation, enhancing accuracy and robustness against domain shifts through velocity field learning and domain-invariant representations.",
      "debug_abstract": "Deep homography estimation has broad applications in computer vision and robotics. Remarkable progresses have been achieved while the existing methods typically treat it as a direct regression or iterative refinement problem and often struggling to capture complex geometric transformations or generalize across different domains. In this work, we propose HomoFM, a new framework that introduces the flow matching technique from generative modeling into the homography estimation task for the first time. Unlike the existing methods, we formulate homography estimation problem as a velocity field learning problem. By modeling a continuous and point-wise velocity field that transforms noisy distributions into registered coordinates, the proposed network recovers high-precision transformations through a conditional flow trajectory. Furthermore, to address the challenge of domain shifts issue, e.g., the cases of multimodal matching or varying illumination scenarios, we integrate a gradient reversal layer (GRL) into the feature extraction backbone. This domain adaptation strategy explicitly constrains the encoder to learn domain-invariant representations, significantly enhancing the network&#39;s robustness. Extensive experiments demonstrate the effectiveness of the proposed method, showing that HomoFM outperforms state-of-the-art methods in both estimation accuracy and robustness on standard benchmarks. Code and data resource are available at this https URL."
    },
    {
      "title": "Dep-Search: Learning Dependency-Aware Reasoning Traces with Persistent Memory",
      "url": "https://arxiv.org/abs/2601.18771",
      "date": "2026-01-26",
      "authors_text": "Yanming Liu, Xinyue Peng, Zixuan Yan, Yanxin Shen, Wenjie Xu",
      "is_highlight": false,
      "score": 46.0,
      "summary": "Dep-Search enhances large language models' multi-hop reasoning by integrating dependency-aware search, structured retrieval, and persistent memory, outperforming existing frameworks in complex tasks.",
      "debug_abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in complex reasoning tasks, particularly when augmented with search mechanisms that enable systematic exploration of external knowledge bases. The field has evolved from traditional retrieval-augmented generation (RAG) frameworks to more sophisticated search-based frameworks that orchestrate multi-step reasoning through explicit search strategies. However, existing search frameworks still rely heavily on implicit natural language reasoning to determine search strategies and how to leverage retrieved information across reasoning steps. This reliance on implicit reasoning creates fundamental challenges for managing dependencies between sub-questions, efficiently reusing previously retrieved knowledge, and learning optimal search strategies through reinforcement learning. To address these limitations, we propose Dep-Search, a dependency-aware search framework that advances beyond existing search frameworks by integrating structured reasoning, retrieval, and persistent memory through GRPO. Dep-Search introduces explicit control mechanisms that enable the model to decompose questions with dependency relationships, retrieve information when needed, access previously stored knowledge from memory, and summarize long reasoning contexts into reusable memory entries. Through extensive experiments on seven diverse question answering datasets, we demonstrate that Dep-Search significantly enhances LLMs&#39; ability to tackle complex multi-hop reasoning tasks, achieving substantial improvements over strong baselines across different model scales."
    },
    {
      "title": "FantasyVLN: Unified Multimodal Chain-of-Thought Reasoning for Vision-Language Navigation",
      "url": "https://arxiv.org/abs/2601.13976",
      "date": "2026-01-23",
      "authors_text": "Jing Zuo, Lingzhou Mu, Fan Jiang, Chengcheng Ma, Mu Xu",
      "is_highlight": false,
      "score": 85.0,
      "summary": "FantasyVLN introduces a unified implicit reasoning framework for Vision-Language Navigation that enhances efficiency and success rates by optimizing Chain-of-Thought reasoning without excessive token overhead.",
      "debug_abstract": "Achieving human-level performance in Vision-and-Language Navigation (VLN) requires an embodied agent to jointly understand multimodal instructions and visual-spatial context while reasoning over long action sequences. Recent works, such as NavCoT and NavGPT-2, demonstrate the potential of Chain-of-Thought (CoT) reasoning for improving interpretability and long-horizon planning. Moreover, multimodal extensions like OctoNav-R1 and CoT-VLA further validate CoT as a promising pathway toward human-like navigation reasoning. However, existing approaches face critical drawbacks: purely textual CoTs lack spatial grounding and easily overfit to sparse annotated reasoning steps, while multimodal CoTs incur severe token inflation by generating imagined visual observations, making real-time navigation impractical. In this work, we propose FantasyVLN, a unified implicit reasoning framework that preserves the benefits of CoT reasoning without explicit token overhead. Specifically, imagined visual tokens are encoded into a compact latent space using a pretrained Visual AutoRegressor (VAR) during CoT reasoning training, and the model jointly learns from textual, visual, and multimodal CoT modes under a unified multi-CoT strategy. At inference, our model performs direct instruction-to-action mapping while still enjoying reasoning-aware representations. Extensive experiments on LH-VLN show that our approach achieves reasoning-aware yet real-time navigation, improving success rates and efficiency while reducing inference latency by an order of magnitude compared to explicit CoT methods."
    },
    {
      "title": "TangramPuzzle: Evaluating Multimodal Large Language Models with Compositional Spatial Reasoning",
      "url": "https://arxiv.org/abs/2601.16520",
      "date": "2026-01-23",
      "authors_text": "Daixian Liu, Jiayi Kuang, Yinghui Li, Yangning Li, Di Yin",
      "is_highlight": false,
      "score": 82.0,
      "summary": "The TangramPuzzle benchmark evaluates multimodal large language models' compositional spatial reasoning through precise geometric tasks, revealing their tendency to prioritize silhouette matching over geometric accuracy.",
      "debug_abstract": "Multimodal Large Language Models (MLLMs) have achieved remarkable progress in visual recognition and semantic understanding. Nevertheless, their ability to perform precise compositional spatial reasoning remains largely unexplored. Existing benchmarks often involve relatively simple tasks and rely on semantic approximations or coarse relative positioning, while their evaluation metrics are typically limited and lack rigorous mathematical formulations. To bridge this gap, we introduce TangramPuzzle, a geometry-grounded benchmark designed to evaluate compositional spatial reasoning through the lens of the classic Tangram game. We propose the Tangram Construction Expression (TCE), a symbolic geometric framework that grounds tangram assemblies in exact, machine-verifiable coordinate specifications, to mitigate the ambiguity of visual approximation. We design two complementary tasks: Outline Prediction, which demands inferring global shapes from local components, and End-to-End Code Generation, which requires solving inverse geometric assembly problems. We conduct extensive evaluation experiments on advanced open-source and proprietary models, revealing an interesting insight: MLLMs tend to prioritize matching the target silhouette while neglecting geometric constraints, leading to distortions or deformations of the pieces."
    },
    {
      "title": "AnchoredDream: Zero-Shot 360° Indoor Scene Generation from a Single View via Geometric Grounding",
      "url": "https://arxiv.org/abs/2601.16532",
      "date": "2026-01-23",
      "authors_text": "Runmao Yao, Junsheng Zhou, Zhen Dong, Yu-Shen Liu",
      "is_highlight": false,
      "score": 72.0,
      "summary": "AnchoredDream introduces a zero-shot method for generating 360° indoor scenes from a single image, enhancing appearance consistency and geometric plausibility through a mutual boosting mechanism.",
      "debug_abstract": "Single-view indoor scene generation plays a crucial role in a range of real-world applications. However, generating a complete 360° scene from a single image remains a highly ill-posed and challenging problem. Recent approaches have made progress by leveraging diffusion models and depth estimation networks, yet they still struggle to maintain appearance consistency and geometric plausibility under large viewpoint changes, limiting their effectiveness in full-scene generation. To address this, we propose AnchoredDream, a novel zero-shot pipeline that anchors 360° scene generation on high-fidelity geometry via an appearance-geometry mutual boosting mechanism. Given a single-view image, our method first performs appearance-guided geometry generation to construct a reliable 3D scene layout. Then, we progressively generate the complete scene through a series of modules: warp-and-inpaint, warp-and-refine, post-optimization, and a novel Grouting Block, which ensures seamless transitions between the input view and generated regions. Extensive experiments demonstrate that AnchoredDream outperforms existing methods by a large margin in both appearance consistency and geometric plausibility--all in a zero-shot manner. Our results highlight the potential of geometric grounding for high-quality, zero-shot single-view scene generation."
    },
    {
      "title": "OnlineSI: Taming Large Language Model for Online 3D Understanding and Grounding",
      "url": "https://arxiv.org/abs/2601.16538",
      "date": "2026-01-23",
      "authors_text": "Zixian Liu, Zhaoxi Chen, Liang Pan, Ziwei Liu",
      "is_highlight": false,
      "score": 88.0,
      "summary": "OnlineSI is a framework that enhances Multimodal Large Language Models' spatial understanding in dynamic environments by integrating continuous learning from video streams and 3D point cloud data.",
      "debug_abstract": "In recent years, researchers have increasingly been interested in how to enable Multimodal Large Language Models (MLLM) to possess spatial understanding and reasoning capabilities. However, most existing methods overlook the importance of the ability to continuously work in an ever-changing world, and lack the possibility of deployment on embodied systems in real-world environments. In this work, we introduce OnlineSI, a framework that can continuously improve its spatial understanding of its surroundings given a video stream. Our core idea is to maintain a finite spatial memory to retain past observations, ensuring the computation required for each inference does not increase as the input accumulates. We further integrate 3D point cloud information with semantic information, helping MLLM to better locate and identify objects in the scene. To evaluate our method, we introduce the Fuzzy $F_1$-Score to mitigate ambiguity, and test our method on two representative datasets. Experiments demonstrate the effectiveness of our method, paving the way towards real-world embodied systems."
    },
    {
      "title": "GRIP: Algorithm-Agnostic Machine Unlearning for Mixture-of-Experts via Geometric Router Constraints",
      "url": "https://arxiv.org/abs/2601.16905",
      "date": "2026-01-23",
      "authors_text": "Andy Zhu, Rongzhe Wei, Yupu Gu, Pan Li",
      "is_highlight": false,
      "score": 62.0,
      "summary": "The GRIP framework enables effective machine unlearning in Mixture-of-Experts models by imposing geometric constraints on router updates, ensuring knowledge erasure without sacrificing model utility.",
      "debug_abstract": "Machine unlearning (MU) for large language models has become critical for AI safety, yet existing methods fail to generalize to Mixture-of-Experts (MoE) architectures. We identify that traditional unlearning methods exploit MoE&#39;s architectural vulnerability: they manipulate routers to redirect queries away from knowledgeable experts rather than erasing knowledge, causing a loss of model utility and superficial forgetting. We propose Geometric Routing Invariance Preservation (GRIP), an algorithm-agnostic framework for unlearning for MoE. Our core contribution is a geometric constraint, implemented by projecting router gradient updates into an expert-specific null-space. Crucially, this decouples routing stability from parameter rigidity: while discrete expert selections remain stable for retained knowledge, the continuous router parameters remain plastic within the null space, allowing the model to undergo necessary internal reconfiguration to satisfy unlearning objectives. This forces the unlearning optimization to erase knowledge directly from expert parameters rather than exploiting the superficial router manipulation shortcut. GRIP functions as an adapter, constraining router parameter updates without modifying the underlying unlearning algorithm. Extensive experiments on large-scale MoE models demonstrate that our adapter eliminates expert selection shift (achieving over 95% routing stability) across all tested unlearning methods while preserving their utility. By preventing existing algorithms from exploiting MoE model&#39;s router vulnerability, GRIP adapts existing unlearning research from dense architectures to MoEs."
    },
    {
      "title": "Render-of-Thought: Rendering Textual Chain-of-Thought as Images for Visual Latent Reasoning",
      "url": "https://arxiv.org/abs/2601.14750",
      "date": "2026-01-22",
      "authors_text": "Yifan Wang, Shiyu Li, Peiming Li, Xiaochen Yang, Yang Tang",
      "is_highlight": false,
      "score": 92.5,
      "summary": "The Render-of-Thought framework transforms Chain-of-Thought reasoning into images, enhancing analyzability and efficiency while maintaining competitive performance in reasoning tasks.",
      "debug_abstract": "Chain-of-Thought (CoT) prompting has achieved remarkable success in unlocking the reasoning capabilities of Large Language Models (LLMs). Although CoT prompting enhances reasoning, its verbosity imposes substantial computational overhead. Recent works often focus exclusively on outcome alignment and lack supervision on the intermediate reasoning process. These deficiencies obscure the analyzability of the latent reasoning chain. To address these challenges, we introduce Render-of-Thought (RoT), the first framework to reify the reasoning chain by rendering textual steps into images, making the latent rationale explicit and traceable. Specifically, we leverage the vision encoders of existing Vision Language Models (VLMs) as semantic anchors to align the vision embeddings with the textual space. This design ensures plug-and-play implementation without incurring additional pre-training overhead. Extensive experiments on mathematical and logical reasoning benchmarks demonstrate that our method achieves 3-4x token compression and substantial inference acceleration compared to explicit CoT. Furthermore, it maintains competitive performance against other methods, validating the feasibility of this paradigm. Our code is available at this https URL"
    },
    {
      "title": "LLM-in-Sandbox Elicits General Agentic Intelligence",
      "url": "https://arxiv.org/abs/2601.16206",
      "date": "2026-01-22",
      "authors_text": "Daixuan Cheng, Shaohan Huang, Yuxian Gu, Huatong Song, Guoxin Chen",
      "is_highlight": false,
      "score": 95.0,
      "summary": "The LLM-in-Sandbox framework enables large language models to demonstrate general intelligence in non-code tasks through sandbox exploration and reinforcement learning, achieving robust generalization across various domains.",
      "debug_abstract": "We introduce LLM-in-Sandbox, enabling LLMs to explore within a code sandbox (i.e., a virtual computer), to elicit general intelligence in non-code domains. We first demonstrate that strong LLMs, without additional training, exhibit generalization capabilities to leverage the code sandbox for non-code tasks. For example, LLMs spontaneously access external resources to acquire new knowledge, leverage the file system to handle long contexts, and execute scripts to satisfy formatting requirements. We further show that these agentic capabilities can be enhanced through LLM-in-Sandbox Reinforcement Learning (LLM-in-Sandbox-RL), which uses only non-agentic data to train models for sandbox exploration. Experiments demonstrate that LLM-in-Sandbox, in both training-free and post-trained settings, achieves robust generalization spanning mathematics, physics, chemistry, biomedicine, long-context understanding, and instruction following. Finally, we analyze LLM-in-Sandbox&#39;s efficiency from computational and system perspectives, and open-source it as a Python package to facilitate real-world deployment."
    },
    {
      "title": "Collision-Free Humanoid Traversal in Cluttered Indoor Scenes",
      "url": "https://arxiv.org/abs/2601.16035",
      "date": "2026-01-22",
      "authors_text": "Han Xue, Sikai Liang, Zhikai Zhang, Zicheng Zeng, Yun Liu",
      "is_highlight": false,
      "score": 81.2,
      "summary": "The paper presents Humanoid Potential Field (HumanoidPF) for enabling collision-free traversal of humanoids in cluttered indoor environments, demonstrating effective RL-based learning and real-world applicability.",
      "debug_abstract": "We study the problem of collision-free humanoid traversal in cluttered indoor scenes, such as hurdling over objects scattered on the floor, crouching under low-hanging obstacles, or squeezing through narrow passages. To achieve this goal, the humanoid needs to map its perception of surrounding obstacles with diverse spatial layouts and geometries to the corresponding traversal skills. However, the lack of an effective representation that captures humanoid-obstacle relationships during collision avoidance makes directly learning such mappings difficult. We therefore propose Humanoid Potential Field (HumanoidPF), which encodes these relationships as collision-free motion directions, significantly facilitating RL-based traversal skill learning. We also find that HumanoidPF exhibits a surprisingly negligible sim-to-real gap as a perceptual representation. To further enable generalizable traversal skills through diverse and challenging cluttered indoor scenes, we further propose a hybrid scene generation method, incorporating crops of realistic 3D indoor scenes and procedurally synthesized obstacles. We successfully transfer our policy to the real world and develop a teleoperation system where users could command the humanoid to traverse in cluttered indoor scenes with just a single click. Extensive experiments are conducted in both simulation and the real world to validate the effectiveness of our method. Demos and code can be found in our website: this https URL."
    }
  ],
  "具身视觉与机器人实验室 (EVAR Lab)": [
    {
      "title": "EventFlash: Towards Efficient MLLMs for Event-Based Vision",
      "url": "https://arxiv.org/abs/2602.03230",
      "date": "2026-02-03",
      "authors_text": "Shaoyu Liu, Jianing Li, Guanghui Zhao, Yunjian Zhang, Wen Jiang",
      "is_highlight": false,
      "score": 63.0,
      "summary": "EventFlash introduces an efficient MLLM for event-based vision, leveraging spatiotemporal sparsification and adaptive sampling to enhance performance and reduce computational costs.",
      "teaser_image": "https://arxiv.org/html/2602.03230/x2.png",
      "debug_abstract": "Event-based multimodal large language models (MLLMs) enable robust perception in high-speed and low-light scenarios, addressing key limitations of frame-based MLLMs. However, current event-based MLLMs often rely on dense image-like processing paradigms, overlooking the spatiotemporal sparsity of event streams and resulting in high computational cost. In this paper, we propose EventFlash, a novel and efficient MLLM to explore spatiotemporal token sparsification for reducing data redundancy and accelerating inference. Technically, we build EventMind, a large-scale and scene-diverse dataset with over 500k instruction sets, providing both short and long event stream sequences to support our curriculum training strategy. We then present an adaptive temporal window aggregation module for efficient temporal sampling, which adaptively compresses temporal tokens while retaining key temporal cues. Finally, a sparse density-guided attention module is designed to improve spatial token efficiency by selecting informative regions and suppressing empty or sparse areas. Experimental results show that EventFlash achieves a $12.4\\times$ throughput improvement over the baseline (EventFlash-Zero) while maintaining comparable performance. It supports long-range event stream processing with up to 1,000 bins, significantly outperforming the 5-bin limit of EventGPT. We believe EventFlash serves as an efficient foundation model for event-based vision."
    },
    {
      "title": "InstaDrive: Instance-Aware Driving World Models for Realistic and Consistent Video Generation",
      "url": "https://arxiv.org/abs/2602.03242",
      "date": "2026-02-03",
      "authors_text": "Zhuoran Yang, Xi Guo, Chenjing Ding, Chiyu Wang, Wei Wu",
      "is_highlight": false,
      "score": 84.0,
      "summary": "InstaDrive introduces a framework that enhances realistic driving video generation by ensuring instance-level temporal consistency and spatial geometric fidelity through innovative feature extraction and alignment techniques.",
      "teaser_image": "https://arxiv.org/html/2602.03242/x2.png",
      "debug_abstract": "Autonomous driving relies on robust models trained on high-quality, large-scale multi-view driving videos. While world models offer a cost-effective solution for generating realistic driving videos, they struggle to maintain instance-level temporal consistency and spatial geometric fidelity. To address these challenges, we propose InstaDrive, a novel framework that enhances driving video realism through two key advancements: (1) Instance Flow Guider, which extracts and propagates instance features across frames to enforce temporal consistency, preserving instance identity over time. (2) Spatial Geometric Aligner, which improves spatial reasoning, ensures precise instance positioning, and explicitly models occlusion hierarchies. By incorporating these instance-aware mechanisms, InstaDrive achieves state-of-the-art video generation quality and enhances downstream autonomous driving tasks on the nuScenes dataset. Additionally, we utilize CARLA&#39;s autopilot to procedurally and stochastically simulate rare but safety-critical driving scenarios across diverse maps and regions, enabling rigorous safety evaluation for autonomous systems. Our project page is this https URL."
    },
    {
      "title": "Learning Physics-Grounded 4D Dynamics with Neural Gaussian Force Fields",
      "url": "https://arxiv.org/abs/2602.00148",
      "date": "2026-01-29",
      "authors_text": "Shiqian Li, Ruihong Shen, Junfeng Ni, Chang Pan, Chi Zhang",
      "is_highlight": false,
      "score": 82.0,
      "summary": "The Neural Gaussian Force Field (NGFF) framework enables rapid generation of interactive, physically realistic 4D videos from multi-view RGB inputs, enhancing robustness and generalization in physical dynamics.",
      "teaser_image": "https://arxiv.org/html/2602.00148/x1.png",
      "debug_abstract": "Predicting physical dynamics from raw visual data remains a major challenge in AI. While recent video generation models have achieved impressive visual quality, they still cannot consistently generate physically plausible videos due to a lack of modeling of physical laws. Recent approaches combining 3D Gaussian splatting and physics engines can produce physically plausible videos, but are hindered by high computational costs in both reconstruction and simulation, and often lack robustness in complex real-world scenarios. To address these issues, we introduce Neural Gaussian Force Field (NGFF), an end-to-end neural framework that integrates 3D Gaussian perception with physics-based dynamic modeling to generate interactive, physically realistic 4D videos from multi-view RGB inputs, achieving two orders of magnitude faster than prior Gaussian simulators. To support training, we also present GSCollision, a 4D Gaussian dataset featuring diverse materials, multi-object interactions, and complex scenes, totaling over 640k rendered physical videos (~4 TB). Evaluations on synthetic and real 3D scenarios show NGFF&#39;s strong generalization and robustness in physical reasoning, advancing video prediction towards physics-grounded world models."
    },
    {
      "title": "How Far Are LLMs from Professional Poker Players? Revisiting Game-Theoretic Reasoning with Agentic Tool Use",
      "url": "https://arxiv.org/abs/2602.00528",
      "date": "2026-01-31",
      "authors_text": "Minhua Lin, Enyan Dai, Hui Liu, Xianfeng Tang, Yuliang Yan",
      "is_highlight": false,
      "score": 55.0,
      "summary": "This paper evaluates LLMs in poker, revealing their strategic reasoning flaws and proposing ToolPoker, which enhances gameplay and reasoning through integrated external solvers.",
      "teaser_image": "https://arxiv.org/html/2602.00528/x2.png",
      "debug_abstract": "As Large Language Models (LLMs) are increasingly applied in high-stakes domains, their ability to reason strategically under uncertainty becomes critical. Poker provides a rigorous testbed, requiring not only strong actions but also principled, game-theoretic reasoning. In this paper, we conduct a systematic study of LLMs in multiple realistic poker tasks, evaluating both gameplay outcomes and reasoning traces. Our analysis reveals LLMs fail to compete against traditional algorithms and identifies three recurring flaws: reliance on heuristics, factual misunderstandings, and a &#34;knowing-doing&#34; gap where actions diverge from reasoning. An initial attempt with behavior cloning and step-level reinforcement learning improves reasoning style but remains insufficient for accurate game-theoretic play. Motivated by these limitations, we propose ToolPoker, a tool-integrated reasoning framework that combines external solvers for GTO-consistent actions with more precise professional-style explanations. Experiments demonstrate that ToolPoker achieves state-of-the-art gameplay while producing reasoning traces that closely reflect game-theoretic principles."
    },
    {
      "title": "V2X-DSC: Multi-Agent Collaborative Perception with Distributed Source Coding Guided Communication",
      "url": "https://arxiv.org/abs/2602.00687",
      "date": "2026-01-31",
      "authors_text": "Yuankun Zeng, Shaohui Li, Zhi Li, Shulan Ruan, Yu Liu",
      "is_highlight": false,
      "score": 76.0,
      "summary": "V2X-DSC introduces a Conditional Codec for efficient multi-agent collaborative perception by compressing and reconstructing BEV features, optimizing bandwidth usage while maintaining accuracy.",
      "teaser_image": "https://arxiv.org/html/2602.00687/figs/overview.jpg",
      "debug_abstract": "Collaborative perception improves 3D understanding by fusing multi-agent observations, yet intermediate-feature sharing faces strict bandwidth constraints as dense BEV features saturate V2X links. We observe that collaborators view the same physical world, making their features strongly correlated; thus receivers only need innovation beyond their local context. Revisiting this from a distributed source coding perspective, we propose V2X-DSC, a framework with a Conditional Codec (DCC) for bandwidth-constrained fusion. The sender compresses BEV features into compact codes, while the receiver performs conditional reconstruction using its local features as side information, allocating bits to complementary cues rather than redundant content. This conditional structure regularizes learning, encouraging incremental representation and yielding lower-noise features. Experiments on DAIR-V2X, OPV2V, and V2X-Real demonstrate state-of-the-art accuracy-bandwidth trade-offs under KB-level communication, and generalizes as a plug-and-play communication layer across multiple fusion backbones."
    },
    {
      "title": "Resource-Efficient Reinforcement for Reasoning Large Language Models via Dynamic One-Shot Policy Refinement",
      "url": "https://arxiv.org/abs/2602.00815",
      "date": "2026-01-31",
      "authors_text": "Yunjian Zhang, Sudong Wang, Yang Li, Peiran Xu, Conghao Zhou",
      "is_highlight": false,
      "score": 57.0,
      "summary": "The paper introduces Dynamic One-Shot Policy Refinement (DoPR), a resource-efficient reinforcement learning strategy that enhances reasoning in large language models while significantly reducing training costs.",
      "teaser_image": "https://arxiv.org/html/2602.00815/dopr.png",
      "debug_abstract": "Large language models (LLMs) have exhibited remarkable performance on complex reasoning tasks, with reinforcement learning under verifiable rewards (RLVR) emerging as a principled framework for aligning model behavior with reasoning chains. Despite its promise, RLVR remains prohibitively resource-intensive, requiring extensive reward signals and incurring substantial rollout costs during training. In this work, we revisit the fundamental question of data and compute efficiency in RLVR. We first establish a theoretical lower bound on the sample complexity required to unlock reasoning capabilities, and empirically validate that strong performance can be achieved with a surprisingly small number of training instances. To tackle the computational burden, we propose Dynamic One-Shot Policy Refinement (DoPR), an uncertainty-aware RL strategy that dynamically selects a single informative training sample per batch for policy updates, guided by reward volatility and exploration-driven acquisition. DoPR reduces rollout overhead by nearly an order of magnitude while preserving competitive reasoning accuracy, offering a scalable and resource-efficient solution for LLM post-training. This approach offers a practical path toward more efficient and accessible RL-based training for reasoning-intensive LLM applications."
    },
    {
      "title": "Making Avatars Interact: Towards Text-Driven Human-Object Interaction for Controllable Talking Avatars",
      "url": "https://arxiv.org/abs/2602.01538",
      "date": "2026-02-02",
      "authors_text": "Youliang Zhang, Zhengguang Zhou, Zhentao Yu, Ziyao Huang, Teng Hu",
      "is_highlight": false,
      "score": 46.0,
      "summary": "The paper presents InteractAvatar, a dual-stream framework for generating talking avatars that perform text-driven human-object interactions by decoupling perception and planning from video synthesis.",
      "teaser_image": "https://arxiv.org/html/2602.01538/x1.png",
      "debug_abstract": "Generating talking avatars is a fundamental task in video generation. Although existing methods can generate full-body talking avatars with simple human motion, extending this task to grounded human-object interaction (GHOI) remains an open challenge, requiring the avatar to perform text-aligned interactions with surrounding objects. This challenge stems from the need for environmental perception and the control-quality dilemma in GHOI generation. To address this, we propose a novel dual-stream framework, InteractAvatar, which decouples perception and planning from video synthesis for grounded human-object interaction. Leveraging detection to enhance environmental perception, we introduce a Perception and Interaction Module (PIM) to generate text-aligned interaction motions. Additionally, an Audio-Interaction Aware Generation Module (AIM) is proposed to synthesize vivid talking avatars performing object interactions. With a specially designed motion-to-video aligner, PIM and AIM share a similar network structure and enable parallel co-generation of motions and plausible videos, effectively mitigating the control-quality dilemma. Finally, we establish a benchmark, GroundedInter, for evaluating GHOI video generation. Extensive experiments and comparisons demonstrate the effectiveness of our method in generating grounded human-object interactions for talking avatars. Project page: this https URL"
    },
    {
      "title": "Reasoning with Autoregressive-Diffusion Collaborative Thoughts",
      "url": "https://arxiv.org/abs/2602.01608",
      "date": "2026-02-02",
      "authors_text": "Mu Yuan, Liekang Zeng, Guoliang Xing, Lan Zhang, Yunhao Liu",
      "is_highlight": false,
      "score": 41.0,
      "summary": "The paper presents Collaborative Thoughts, a framework integrating autoregressive and diffusion models for enhanced spatial reasoning and generation through iterative feedback and joint reasoning.",
      "teaser_image": "https://arxiv.org/html/2602.01608/x2.png",
      "debug_abstract": "Autoregressive and diffusion models represent two complementary generative paradigms. Autoregressive models excel at sequential planning and constraint composition, yet struggle with tasks that require explicit spatial or physical grounding. Diffusion models, in contrast, capture rich spatial structure through high-dimensional generation, but lack the stepwise logical control needed to satisfy complex, multi-stage constraints or to reliably identify and correct errors. We introduce Collaborative Thoughts, a unified collaborative framework that enables autoregressive and diffusion models to reason and generate jointly through a closed-loop interaction. In Collaborative Thoughts, autoregressive models perform structured planning and constraint management, diffusion models instantiate these constraints as intermediate visual thoughts, and a vision-based critic module evaluates whether the visual thoughts satisfy the intended structural and physical requirements. This feedback is then used to iteratively refine subsequent planning and generation steps, mitigating error propagation across modalities. Importantly, Collaborative Thoughts uses the same collaborative loop regardless of whether the task is autoregressive question answering or diffusion-based visual generation. Through representative examples, we illustrate how Collaborative Thoughts can improve the reliability of spatial reasoning and the controllability of generation."
    },
    {
      "title": "From Frames to Sequences: Temporally Consistent Human-Centric Dense Prediction",
      "url": "https://arxiv.org/abs/2602.01661",
      "date": "2026-02-02",
      "authors_text": "Xingyu Miao, Junting Dong, Qin Zhao, Yuhang Yang, Junhao Chen",
      "is_highlight": false,
      "score": 43.0,
      "summary": "This paper presents a novel synthetic data pipeline and a ViT-based model for achieving temporally consistent human-centric dense prediction in video sequences, outperforming existing methods.",
      "teaser_image": "https://arxiv.org/html/2602.01661/x2.png",
      "debug_abstract": "In this work, we focus on the challenge of temporally consistent human-centric dense prediction across video sequences. Existing models achieve strong per-frame accuracy but often flicker under motion, occlusion, and lighting changes, and they rarely have paired human video supervision for multiple dense tasks. We address this gap with a scalable synthetic data pipeline that generates photorealistic human frames and motion-aligned sequences with pixel-accurate depth, normals, and masks. Unlike prior static data synthetic pipelines, our pipeline provides both frame-level labels for spatial learning and sequence-level supervision for temporal learning. Building on this, we train a unified ViT-based dense predictor that (i) injects an explicit human geometric prior via CSE embeddings and (ii) improves geometry-feature reliability with a lightweight channel reweighting module after feature fusion. Our two-stage training strategy, combining static pretraining with dynamic sequence supervision, enables the model first to acquire robust spatial representations and then refine temporal consistency across motion-aligned sequences. Extensive experiments show that we achieve state-of-the-art performance on THuman2.1 and Hi4D and generalize effectively to in-the-wild videos."
    },
    {
      "title": "ForSim: Stepwise Forward Simulation for Traffic Policy Fine-Tuning",
      "url": "https://arxiv.org/abs/2602.01916",
      "date": "2026-02-02",
      "authors_text": "Keyu Chen, Wenchao Sun, Hao Cheng, Zheng Fu, Sifa Zheng",
      "is_highlight": true,
      "score": 29.0,
      "summary": "ForSim introduces a stepwise closed-loop simulation method that enhances traffic policy fine-tuning by improving agent interactions and preserving multimodal behaviors in autonomous driving.",
      "teaser_image": "https://arxiv.org/html/2602.01916/x2.png",
      "debug_abstract": "As the foundation of closed-loop training and evaluation in autonomous driving, traffic simulation still faces two fundamental challenges: covariate shift introduced by open-loop imitation learning and limited capacity to reflect the multimodal behaviors observed in real-world traffic. Although recent frameworks such as RIFT have partially addressed these issues through group-relative optimization, their forward simulation procedures remain largely non-reactive, leading to unrealistic agent interactions within the virtual domain and ultimately limiting simulation fidelity. To address these issues, we propose ForSim, a stepwise closed-loop forward simulation paradigm. At each virtual timestep, the traffic agent propagates the virtual candidate trajectory that best spatiotemporally matches the reference trajectory through physically grounded motion dynamics, thereby preserving multimodal behavioral diversity while ensuring intra-modality consistency. Other agents are updated with stepwise predictions, yielding coherent and interaction-aware evolution. When incorporated into the RIFT traffic simulation framework, ForSim operates in conjunction with group-relative optimization to fine-tune traffic policy. Extensive experiments confirm that this integration consistently improves safety while maintaining efficiency, realism, and comfort. These results underscore the importance of modeling closed-loop multimodal interactions within forward simulation and enhance the fidelity and reliability of traffic simulation for autonomous driving. Project Page: this https URL"
    },
    {
      "title": "Small Generalizable Prompt Predictive Models Can Steer Efficient RL Post-Training of Large Reasoning Models",
      "url": "https://arxiv.org/abs/2602.01970",
      "date": "2026-02-02",
      "authors_text": "Yun Qu, Qi Wang, Yixiu Mao, Heming Zou, Yuhang Jiang",
      "is_highlight": false,
      "score": 26.0,
      "summary": "The paper presents Generalizable Predictive Prompt Selection (GPS), a method that enhances reinforcement learning efficiency in large language models by utilizing a lightweight generative model for prompt selection.",
      "teaser_image": "https://arxiv.org/html/2602.01970/x1.png",
      "debug_abstract": "Reinforcement learning enhances the reasoning capabilities of large language models but often involves high computational costs due to rollout-intensive optimization. Online prompt selection presents a plausible solution by prioritizing informative prompts to improve training efficiency. However, current methods either depend on costly, exact evaluations or construct prompt-specific predictive models lacking generalization across prompts. This study introduces Generalizable Predictive Prompt Selection (GPS), which performs Bayesian inference towards prompt difficulty using a lightweight generative model trained on the shared optimization history. Intermediate-difficulty prioritization and history-anchored diversity are incorporated into the batch acquisition principle to select informative prompt batches. The small predictive model also generalizes at test-time for efficient computational allocation. Experiments across varied reasoning benchmarks indicate GPS&#39;s substantial improvements in training efficiency, final performance, and test-time efficiency over superior baseline methods."
    },
    {
      "title": "TTT-Parkour: Rapid Test-Time Training for Perceptive Robot Parkour",
      "url": "https://arxiv.org/abs/2602.02331",
      "date": "2026-02-02",
      "authors_text": "Shaoting Zhu, Baijun Ye, Jiaxuan Wang, Jiakang Chen, Ziwen Zhuang",
      "is_highlight": false,
      "score": 18.0,
      "summary": "TTT-Parkour introduces a rapid test-time training framework enabling humanoid robots to effectively navigate complex terrains through efficient real-to-sim-to-real learning and geometry reconstruction.",
      "teaser_image": "https://arxiv.org/html/2602.02331/x1.png",
      "debug_abstract": "Achieving highly dynamic humanoid parkour on unseen, complex terrains remains a challenge in robotics. Although general locomotion policies demonstrate capabilities across broad terrain distributions, they often struggle with arbitrary and highly challenging environments. To overcome this limitation, we propose a real-to-sim-to-real framework that leverages rapid test-time training (TTT) on novel terrains, significantly enhancing the robot&#39;s capability to traverse extremely difficult geometries. We adopt a two-stage end-to-end learning paradigm: a policy is first pre-trained on diverse procedurally generated terrains, followed by rapid fine-tuning on high-fidelity meshes reconstructed from real-world captures. Specifically, we develop a feed-forward, efficient, and high-fidelity geometry reconstruction pipeline using RGB-D inputs, ensuring both speed and quality during test-time training. We demonstrate that TTT-Parkour empowers humanoid robots to master complex obstacles, including wedges, stakes, boxes, trapezoids, and narrow beams. The whole pipeline of capturing, reconstructing, and test-time training requires less than 10 minutes on most tested terrains. Extensive experiments show that the policy after test-time training exhibits robust zero-shot sim-to-real transfer capability."
    },
    {
      "title": "Context Learning for Multi-Agent Discussion",
      "url": "https://arxiv.org/abs/2602.02350",
      "date": "2026-02-02",
      "authors_text": "Xingyuan Hua, Sheng Yue, Xinyi Li, Yizhe Zhao, Jinrui Zhang",
      "is_highlight": false,
      "score": 17.0,
      "summary": "The paper presents M2CL, a multi-LLM context learning method that enhances coherence in Multi-Agent Discussions, improving problem-solving performance by 20%-50% over existing approaches.",
      "teaser_image": "https://arxiv.org/html/2602.02350/x2.png",
      "debug_abstract": "Multi-Agent Discussion (MAD) has garnered increasing attention very recently, where multiple LLM instances collaboratively solve problems via structured discussion. However, we find that current MAD methods easily suffer from discussion inconsistency, LLMs fail to reach a coherent solution, due to the misalignment between their individual this http URL this paper, we introduce a multi-LLM context learning method (M2CL) that learns a context generator for each agent, capable of dynamically generating context instructions per discussion round via automatic information organization and refinement. Specifically, inspired by our theoretical insights on the context instruction, M2CL train the generators to control context coherence and output discrepancies via a carefully crafted self-adaptive this http URL enables LLMs to avoid premature convergence on majority noise and progressively reach the correct consensus. We evaluate M2CL on challenging tasks, including academic reasoning, embodied tasks, and mobile control. The results show that the performance of M2CL significantly surpasses existing methods by 20%--50%, while enjoying favorable transferability and computational efficiency."
    },
    {
      "title": "UV-M3TL: A Unified and Versatile Multimodal Multi-Task Learning Framework for Assistive Driving Perception",
      "url": "https://arxiv.org/abs/2602.01594",
      "date": "2026-02-02",
      "authors_text": "Wenzhuo Liu, Qiannan Guo, Zhen Wang, Wenshuo Wang, Lei Yang",
      "is_highlight": false,
      "score": 8.0,
      "summary": "The UV-M3TL framework enhances ADAS by effectively recognizing driver behavior and context through dual-branch multimodal embedding and adaptive loss, achieving state-of-the-art performance across multiple tasks.",
      "teaser_image": "https://arxiv.org/html/2602.01594/x2.png",
      "debug_abstract": "Advanced Driver Assistance Systems (ADAS) need to understand human driver behavior while perceiving their navigation context, but jointly learning these heterogeneous tasks would cause inter-task negative transfer and impair system performance. Here, we propose a Unified and Versatile Multimodal Multi-Task Learning (UV-M3TL) framework to simultaneously recognize driver behavior, driver emotion, vehicle behavior, and traffic context, while mitigating inter-task negative transfer. Our framework incorporates two core components: dual-branch spatial channel multimodal embedding (DB-SCME) and adaptive feature-decoupled multi-task loss (AFD-Loss). DB-SCME enhances cross-task knowledge transfer while mitigating task conflicts by employing a dual-branch structure to explicitly model salient task-shared and task-specific features. AFD-Loss improves the stability of joint optimization while guiding the model to learn diverse multi-task representations by introducing an adaptive weighting mechanism based on learning dynamics and feature decoupling constraints. We evaluate our method on the AIDE dataset, and the experimental results demonstrate that UV-M3TL achieves state-of-the-art performance across all four tasks. To further prove the versatility, we evaluate UV-M3TL on additional public multi-task perception benchmarks (BDD100K, CityScapes, NYUD-v2, and PASCAL-Context), where it consistently delivers strong performance across diverse task combinations, attaining state-of-the-art results on most tasks."
    },
    {
      "title": "Enhancing Indoor Occupancy Prediction via Sparse Query-Based Multi-Level Consistent Knowledge Distillation",
      "url": "https://arxiv.org/abs/2602.02318",
      "date": "2026-02-02",
      "authors_text": "Xiang Li, Yupeng Zheng, Pengfei Li, Yilun Chen, Ya-Qin Zhang",
      "is_highlight": false,
      "score": 0,
      "summary": "DiScene introduces a sparse query-based framework for efficient and robust indoor occupancy prediction through multi-level knowledge distillation, achieving state-of-the-art performance and faster inference.",
      "teaser_image": "https://arxiv.org/html/2602.02318/x2.png",
      "debug_abstract": "Occupancy prediction provides critical geometric and semantic understanding for robotics but faces efficiency-accuracy trade-offs. Current dense methods suffer computational waste on empty voxels, while sparse query-based approaches lack robustness in diverse and complex indoor scenes. In this paper, we propose DiScene, a novel sparse query-based framework that leverages multi-level distillation to achieve efficient and robust occupancy prediction. In particular, our method incorporates two key innovations: (1) a Multi-level Consistent Knowledge Distillation strategy, which transfers hierarchical representations from large teacher models to lightweight students through coordinated alignment across four levels, including encoder-level feature alignment, query-level feature matching, prior-level spatial guidance, and anchor-level high-confidence knowledge transfer and (2) a Teacher-Guided Initialization policy, employing optimized parameter warm-up to accelerate model convergence. Validated on the Occ-Scannet benchmark, DiScene achieves 23.2 FPS without depth priors while outperforming our baseline method, OPUS, by 36.1% and even better than the depth-enhanced version, OPUS†. With depth integration, DiScene† attains new SOTA performance, surpassing EmbodiedOcc by 3.7% with 1.62$\\times$ faster inference speed. Furthermore, experiments on the Occ3D-nuScenes benchmark and in-the-wild scenarios demonstrate the versatility of our approach in various environments. Code and models can be accessed at this https URL."
    },
    {
      "title": "InspecSafe-V1: A Multimodal Benchmark for Safety Assessment in Industrial Inspection Scenarios",
      "url": "https://arxiv.org/abs/2601.21173",
      "date": "2026-01-29",
      "authors_text": "Zeyi Liu, Shuang Liu, Jihai Min, Zhaoheng Zhang, Jun Cen",
      "is_highlight": false,
      "score": 82.0,
      "summary": "InspecSafe-V1 is a multimodal benchmark dataset for safety assessment in industrial inspections, featuring real-world data from diverse scenarios and multiple sensing modalities.",
      "teaser_image": "https://arxiv.org/html/2601.21173/x1.png",
      "debug_abstract": "With the rapid development of industrial intelligence and unmanned inspection, reliable perception and safety assessment for AI systems in complex and dynamic industrial sites has become a key bottleneck for deploying predictive maintenance and autonomous inspection. Most public datasets remain limited by simulated data sources, single-modality sensing, or the absence of fine-grained object-level annotations, which prevents robust scene understanding and multimodal safety reasoning for industrial foundation models. To address these limitations, InspecSafe-V1 is released as the first multimodal benchmark dataset for industrial inspection safety assessment that is collected from routine operations of real inspection robots in real-world environments. InspecSafe-V1 covers five representative industrial scenarios, including tunnels, power facilities, sintering equipment, oil and gas petrochemical plants, and coal conveyor trestles. The dataset is constructed from 41 wheeled and rail-mounted inspection robots operating at 2,239 valid inspection sites, yielding 5,013 inspection instances. For each instance, pixel-level segmentation annotations are provided for key objects in visible-spectrum images. In addition, a semantic scene description and a corresponding safety level label are provided according to practical inspection tasks. Seven synchronized sensing modalities are further included, including infrared video, audio, depth point clouds, radar point clouds, gas measurements, temperature, and humidity, to support multimodal anomaly recognition, cross-modal fusion, and comprehensive safety assessment in industrial environments."
    },
    {
      "title": "Listen, Look, Drive: Coupling Audio Instructions for User-aware VLA-based Autonomous Driving",
      "url": "https://arxiv.org/abs/2601.12142",
      "date": "2026-01-28",
      "authors_text": "Ziang Guo, Feng Yang, Xuefeng Zhang, Jiaqi Guo, Kun Zhao",
      "is_highlight": false,
      "score": 88.0,
      "summary": "EchoVLA enhances autonomous driving by integrating audio instructions with visual inputs, allowing for user influence and emotional context, significantly improving navigation accuracy and safety.",
      "teaser_image": "https://arxiv.org/html/2601.12142/Figs/overview.png",
      "debug_abstract": "Vision Language Action (VLA) models promise an open-vocabulary interface that can translate perceptual ambiguity into semantically grounded driving decisions, yet they still treat language as a static prior fixed at inference time. As a result, the model must infer continuously shifting objectives from pixels alone, yielding delayed or overly conservative maneuvers. We argue that effective VLAs for autonomous driving need an online channel in which users can influence driving with specific intentions. To this end, we present EchoVLA, a user-aware VLA that couples camera streams with in situ audio instructions. We augment the nuScenes dataset with temporally aligned, intent-specific speech commands generated by converting ego-motion descriptions into synthetic audios. Further, we compose emotional speech-trajectory pairs into a multimodal Chain-of-Thought (CoT) for fine-tuning a Multimodal Large Model (MLM) based on Qwen2.5-Omni. Specifically, we synthesize the audio-augmented dataset with different emotion types paired with corresponding driving behaviors, leveraging the emotional cues embedded in tone, pitch, and speech tempo to reflect varying user states, such as urgent or hesitant intentions, thus enabling our EchoVLA to interpret not only the semantic content but also the emotional context of audio commands for more nuanced and emotionally adaptive driving behavior. In open-loop benchmarks, our approach reduces the average L2 error by $59.4\\%$ and the collision rate by $74.4\\%$ compared to the baseline of vision-only perception. More experiments on nuScenes dataset validate that EchoVLA not only steers the trajectory through audio instructions, but also modulates driving behavior in response to the emotions detected in the user&#39;s speech."
    },
    {
      "title": "HINT: Hierarchical Interaction Modeling for Autoregressive Multi-Human Motion Generation",
      "url": "https://arxiv.org/abs/2601.20383",
      "date": "2026-01-28",
      "authors_text": "Mengge Liu, Yan Di, Gu Wang, Yun Qu, Dekai Zhu",
      "is_highlight": false,
      "score": 78.0,
      "summary": "HINT introduces a novel autoregressive framework for multi-human motion generation, enhancing interaction modeling and performance with variable agent counts and long text guidance.",
      "teaser_image": "https://arxiv.org/html/2601.20383/x2.png",
      "debug_abstract": "Text-driven multi-human motion generation with complex interactions remains a challenging problem. Despite progress in performance, existing offline methods that generate fixed-length motions with a fixed number of agents, are inherently limited in handling long or variable text, and varying agent counts. These limitations naturally encourage autoregressive formulations, which predict future motions step by step conditioned on all past trajectories and current text guidance. In this work, we introduce HINT, the first autoregressive framework for multi-human motion generation with Hierarchical INTeraction modeling in diffusion. First, HINT leverages a disentangled motion representation within a canonicalized latent space, decoupling local motion semantics from inter-person interactions. This design facilitates direct adaptation to varying numbers of human participants without requiring additional refinement. Second, HINT adopts a sliding-window strategy for efficient online generation, and aggregates local within-window and global cross-window conditions to capture past human history, inter-person dependencies, and align with text guidance. This strategy not only enables fine-grained interaction modeling within each window but also preserves long-horizon coherence across all the long sequence. Extensive experiments on public benchmarks demonstrate that HINT matches the performance of strong offline models and surpasses autoregressive baselines. Notably, on InterHuman, HINT achieves an FID of 3.100, significantly improving over the previous state-of-the-art score of 5.154."
    },
    {
      "title": "Self-Supervised Path Planning in Unstructured Environments via Global-Guided Differentiable Hard Constraint Projection",
      "url": "https://arxiv.org/abs/2601.19354",
      "date": "2026-01-27",
      "authors_text": "Ziqian Wang, Chenxi Fang, Zhen Zhang",
      "is_highlight": false,
      "score": 90.0,
      "summary": "This paper presents a self-supervised framework for autonomous navigation in unstructured environments, utilizing a differentiable projection layer to ensure safety and efficiency in path planning.",
      "debug_abstract": "Deploying deep learning agents for autonomous navigation in unstructured environments faces critical challenges regarding safety, data scarcity, and limited computational resources. Traditional solvers often suffer from high latency, while emerging learning-based approaches struggle to ensure deterministic feasibility. To bridge the gap from embodied to embedded intelligence, we propose a self-supervised framework incorporating a differentiable hard constraint projection layer for runtime assurance. To mitigate data scarcity, we construct a Global-Guided Artificial Potential Field (G-APF), which provides dense supervision signals without manual labeling. To enforce actuator limitations and geometric constraints efficiently, we employ an adaptive neural projection layer, which iteratively rectifies the coarse network output onto the feasible manifold. Extensive benchmarks on a test set of 20,000 scenarios demonstrate an 88.75\\% success rate, substantiating the enhanced operational safety. Closed-loop experiments in CARLA further validate the physical realizability of the planned paths under dynamic constraints. Furthermore, deployment verification on an NVIDIA Jetson Orin NX confirms an inference latency of 94 ms, showing real-time feasibility on resource-constrained embedded hardware. This framework offers a generalized paradigm for embedding physical laws into neural architectures, providing a viable direction for solving constrained optimization in mechatronics. Source code is available at: this https URL."
    },
    {
      "title": "AtomMem : Learnable Dynamic Agentic Memory with Atomic Memory Operation",
      "url": "https://arxiv.org/abs/2601.08323",
      "date": "2026-01-27",
      "authors_text": "Yupeng Huo, Yaxi Lu, Zhong Zhang, Haotian Chen, Yankai Lin",
      "is_highlight": false,
      "score": 82.0,
      "summary": "AtomMem introduces a learnable dynamic memory framework that optimizes memory management through atomic CRUD operations, significantly outperforming static methods in long-horizon tasks.",
      "debug_abstract": "Equipping agents with memory is essential for solving real-world long-horizon problems. However, most existing agent memory mechanisms rely on static and hand-crafted workflows. This limits the performance and generalization ability of these memory designs, which highlights the need for a more flexible, learning-based memory framework. In this paper, we propose AtomMem, which reframes memory management as a dynamic decision-making problem. We deconstruct high-level memory processes into fundamental atomic CRUD (Create, Read, Update, Delete) operations, transforming the memory workflow into a learnable decision process. By combining supervised fine-tuning with reinforcement learning, AtomMem learns an autonomous, task-aligned policy to orchestrate memory behaviors tailored to specific task demands. Experimental results across 3 long-context benchmarks demonstrate that the trained AtomMem-8B consistently outperforms prior static-workflow memory methods. Further analysis of training dynamics shows that our learning-based formulation enables the agent to discover structured, task-aligned memory management strategies, highlighting a key advantage over predefined routines."
    },
    {
      "title": "Visual Generation Unlocks Human-Like Reasoning through Multimodal World Models",
      "url": "https://arxiv.org/abs/2601.19834",
      "date": "2026-01-27",
      "authors_text": "Jialong Wu, Xiaoying Zhang, Hongyi Yuan, Xiangcheng Zhang, Tianhao Huang",
      "is_highlight": false,
      "score": 89.0,
      "summary": "This paper demonstrates that visual generation enhances reasoning in multimodal AI, particularly for tasks requiring physical world understanding, outperforming purely verbal approaches.",
      "debug_abstract": "Humans construct internal world models and reason by manipulating the concepts within these models. Recent advances in AI, particularly chain-of-thought (CoT) reasoning, approximate such human cognitive abilities, where world models are believed to be embedded within large language models. Expert-level performance in formal and abstract domains such as mathematics and programming has been achieved in current systems by relying predominantly on verbal reasoning. However, they still lag far behind humans in domains like physical and spatial intelligence, which require richer representations and prior knowledge. The emergence of unified multimodal models (UMMs) capable of both verbal and visual generation has therefore sparked interest in more human-like reasoning grounded in complementary multimodal pathways, though their benefits remain unclear. From a world-model perspective, this paper presents the first principled study of when and how visual generation benefits reasoning. Our key position is the visual superiority hypothesis: for certain tasks--particularly those grounded in the physical world--visual generation more naturally serves as world models, whereas purely verbal world models encounter bottlenecks arising from representational limitations or insufficient prior knowledge. Theoretically, we formalize internal world modeling as a core component of CoT reasoning and analyze distinctions among different forms of world models. Empirically, we identify tasks that necessitate interleaved visual-verbal CoT reasoning, constructing a new evaluation suite, VisWorld-Eval. Controlled experiments on a state-of-the-art UMM show that interleaved CoT significantly outperforms purely verbal CoT on tasks that favor visual world modeling, but offers no clear advantage otherwise. Together, this work clarifies the potential of multimodal world modeling for more powerful, human-like multimodal AI."
    },
    {
      "title": "Scalable Exploration for High-Dimensional Continuous Control via Value-Guided Flow",
      "url": "https://arxiv.org/abs/2601.19707",
      "date": "2026-01-27",
      "authors_text": "Yunyue Wei, Chenhui Zuo, Yanan Sui",
      "is_highlight": false,
      "score": 79.0,
      "summary": "Qflex introduces a scalable reinforcement learning method for high-dimensional continuous control that enhances exploration efficiency by aligning actions with learned value gradients.",
      "debug_abstract": "Controlling high-dimensional systems in biological and robotic applications is challenging due to expansive state-action spaces, where effective exploration is critical. Commonly used exploration strategies in reinforcement learning are largely undirected with sharp degradation as action dimensionality grows. Many existing methods resort to dimensionality reduction, which constrains policy expressiveness and forfeits system flexibility. We introduce Q-guided Flow Exploration (Qflex), a scalable reinforcement learning method that conducts exploration directly in the native high-dimensional action space. During training, Qflex traverses actions from a learnable source distribution along a probability flow induced by the learned value function, aligning exploration with task-relevant gradients rather than isotropic noise. Our proposed method substantially outperforms representative online reinforcement learning baselines across diverse high-dimensional continuous-control benchmarks. Qflex also successfully controls a full-body human musculoskeletal model to perform agile, complex movements, demonstrating superior scalability and sample efficiency in very high-dimensional settings. Our results indicate that value-guided flows offer a principled and practical route to exploration at scale."
    },
    {
      "title": "Latent-Space Contrastive Reinforcement Learning for Stable and Efficient LLM Reasoning",
      "url": "https://arxiv.org/abs/2601.17275",
      "date": "2026-01-24",
      "authors_text": "Lianlei Shan, Han Chen, Yixuan Wang, Zhenjie Liu, Wei Li",
      "is_highlight": false,
      "score": 60.0,
      "summary": "The paper introduces DeepLatent Reasoning (DLR), a latent-space contrastive reinforcement learning framework that enhances LLM reasoning by addressing inefficiencies and stability in multi-step tasks.",
      "debug_abstract": "While Large Language Models (LLMs) demonstrate exceptional performance in surface-level text generation, their nature in handling complex multi-step reasoning tasks often remains one of ``statistical fitting&#39;&#39; rather than systematic logical deduction. Traditional Reinforcement Learning (RL) attempts to mitigate this by introducing a ``think-before-speak&#39;&#39; paradigm. However, applying RL directly in high-dimensional, discrete token spaces faces three inherent challenges: sample-inefficient rollouts, high gradient estimation variance, and the risk of catastrophic forgetting. To fundamentally address these structural bottlenecks, we propose \\textbf{DeepLatent Reasoning (DLR)}, a latent-space bidirectional contrastive reinforcement learning framework. This framework shifts the trial-and-error cost from expensive token-level full sequence generation to the continuous latent manifold. Specifically, we introduce a lightweight assistant model to efficiently sample $K$ reasoning chain encodings within the latent space. These encodings are filtered via a dual reward mechanism based on correctness and formatting; only high-value latent trajectories are fed into a \\textbf{frozen main model} for single-pass decoding. To maximize reasoning diversity while maintaining coherence, we design a contrastive learning objective to enable directed exploration within the latent space. Since the main model parameters remain frozen during optimization, this method mathematically eliminates catastrophic forgetting. Experiments demonstrate that under comparable GPU computational budgets, DLR achieves more stable training convergence, supports longer-horizon reasoning chains, and facilitates the sustainable accumulation of reasoning capabilities, providing a viable path toward reliable and scalable reinforcement learning for LLMs."
    },
    {
      "title": "MetaWorld: Skill Transfer and Composition in a Hierarchical World Model for Grounding High-Level Instructions",
      "url": "https://arxiv.org/abs/2601.17507",
      "date": "2026-01-24",
      "authors_text": "Yutong Shen, Hangxu Liu, Kailin Pei, Ruizhe Xia, Tongtong Feng",
      "is_highlight": false,
      "score": 92.0,
      "summary": "MetaWorld introduces a hierarchical world model that enhances humanoid robot loco-manipulation by integrating semantic planning and physical control through expert policy transfer, improving task performance and motion coherence.",
      "debug_abstract": "Humanoid robot loco-manipulation remains constrained by the semantic-physical gap. Current methods face three limitations: Low sample efficiency in reinforcement learning, poor generalization in imitation learning, and physical inconsistency in VLMs. We propose MetaWorld, a hierarchical world model that integrates semantic planning and physical control via expert policy transfer. The framework decouples tasks into a VLM-driven semantic layer and a latent dynamics model operating in a compact state space. Our dynamic expert selection and motion prior fusion mechanism leverages a pre-trained multi-expert policy library as transferable knowledge, enabling efficient online adaptation via a two-stage framework. VLMs serve as semantic interfaces, mapping instructions to executable skills and bypassing symbol grounding. Experiments on Humanoid-Bench show MetaWorld outperforms world model-based RL in task completion and motion coherence. Our code will be found at this https URL"
    },
    {
      "title": "The Flexibility Trap: Why Arbitrary Order Limits Reasoning Potential in Diffusion Language Models",
      "url": "https://arxiv.org/abs/2601.15165",
      "date": "2026-01-26",
      "authors_text": "Zanlin Ni, Shenzhi Wang, Yang Yue, Tianyu Yu, Weilin Zhao",
      "is_highlight": false,
      "score": 45.0,
      "summary": "This paper argues that arbitrary order generation in diffusion language models limits reasoning potential, advocating for a simpler approach using Group Relative Policy Optimization for improved performance.",
      "debug_abstract": "Diffusion Large Language Models (dLLMs) break the rigid left-to-right constraint of traditional LLMs, enabling token generation in arbitrary orders. Intuitively, this flexibility implies a solution space that strictly supersets the fixed autoregressive trajectory, theoretically unlocking superior reasoning potential for general tasks like mathematics and coding. Consequently, numerous works have leveraged reinforcement learning (RL) to elicit the reasoning capability of dLLMs. In this paper, we reveal a counter-intuitive reality: arbitrary order generation, in its current form, narrows rather than expands the reasoning boundary of dLLMs. We find that dLLMs tend to exploit this order flexibility to bypass high-uncertainty tokens that are crucial for exploration, leading to a premature collapse of the solution space. This observation motivates a rethink of RL approaches for dLLMs, where considerable complexities, such as handling combinatorial trajectories and intractable likelihoods, are often devoted to preserving this flexibility. We demonstrate that effective reasoning can be better elicited by intentionally forgoing arbitrary order and applying standard Group Relative Policy Optimization (GRPO) instead. Our approach, JustGRPO, is minimalist yet surprisingly effective (e.g., 89.1% accuracy on GSM8K) while fully retaining the parallel decoding ability of dLLMs. Project page: this https URL"
    },
    {
      "title": "HomoFM: Deep Homography Estimation with Flow Matching",
      "url": "https://arxiv.org/abs/2601.18222",
      "date": "2026-01-26",
      "authors_text": "Mengfan He, Liangzheng Sun, Chunyu Li, Ziyang Meng",
      "is_highlight": false,
      "score": 49.0,
      "summary": "HomoFM introduces a flow matching technique for deep homography estimation, enhancing accuracy and robustness against domain shifts through velocity field learning and domain-invariant representations.",
      "debug_abstract": "Deep homography estimation has broad applications in computer vision and robotics. Remarkable progresses have been achieved while the existing methods typically treat it as a direct regression or iterative refinement problem and often struggling to capture complex geometric transformations or generalize across different domains. In this work, we propose HomoFM, a new framework that introduces the flow matching technique from generative modeling into the homography estimation task for the first time. Unlike the existing methods, we formulate homography estimation problem as a velocity field learning problem. By modeling a continuous and point-wise velocity field that transforms noisy distributions into registered coordinates, the proposed network recovers high-precision transformations through a conditional flow trajectory. Furthermore, to address the challenge of domain shifts issue, e.g., the cases of multimodal matching or varying illumination scenarios, we integrate a gradient reversal layer (GRL) into the feature extraction backbone. This domain adaptation strategy explicitly constrains the encoder to learn domain-invariant representations, significantly enhancing the network&#39;s robustness. Extensive experiments demonstrate the effectiveness of the proposed method, showing that HomoFM outperforms state-of-the-art methods in both estimation accuracy and robustness on standard benchmarks. Code and data resource are available at this https URL."
    },
    {
      "title": "Dep-Search: Learning Dependency-Aware Reasoning Traces with Persistent Memory",
      "url": "https://arxiv.org/abs/2601.18771",
      "date": "2026-01-26",
      "authors_text": "Yanming Liu, Xinyue Peng, Zixuan Yan, Yanxin Shen, Wenjie Xu",
      "is_highlight": false,
      "score": 46.0,
      "summary": "Dep-Search enhances large language models' multi-hop reasoning by integrating dependency-aware search, structured retrieval, and persistent memory, outperforming existing frameworks in complex tasks.",
      "debug_abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in complex reasoning tasks, particularly when augmented with search mechanisms that enable systematic exploration of external knowledge bases. The field has evolved from traditional retrieval-augmented generation (RAG) frameworks to more sophisticated search-based frameworks that orchestrate multi-step reasoning through explicit search strategies. However, existing search frameworks still rely heavily on implicit natural language reasoning to determine search strategies and how to leverage retrieved information across reasoning steps. This reliance on implicit reasoning creates fundamental challenges for managing dependencies between sub-questions, efficiently reusing previously retrieved knowledge, and learning optimal search strategies through reinforcement learning. To address these limitations, we propose Dep-Search, a dependency-aware search framework that advances beyond existing search frameworks by integrating structured reasoning, retrieval, and persistent memory through GRPO. Dep-Search introduces explicit control mechanisms that enable the model to decompose questions with dependency relationships, retrieve information when needed, access previously stored knowledge from memory, and summarize long reasoning contexts into reusable memory entries. Through extensive experiments on seven diverse question answering datasets, we demonstrate that Dep-Search significantly enhances LLMs&#39; ability to tackle complex multi-hop reasoning tasks, achieving substantial improvements over strong baselines across different model scales."
    },
    {
      "title": "FantasyVLN: Unified Multimodal Chain-of-Thought Reasoning for Vision-Language Navigation",
      "url": "https://arxiv.org/abs/2601.13976",
      "date": "2026-01-23",
      "authors_text": "Jing Zuo, Lingzhou Mu, Fan Jiang, Chengcheng Ma, Mu Xu",
      "is_highlight": false,
      "score": 85.0,
      "summary": "FantasyVLN introduces a unified implicit reasoning framework for Vision-Language Navigation that enhances efficiency and success rates by optimizing Chain-of-Thought reasoning without excessive token overhead.",
      "debug_abstract": "Achieving human-level performance in Vision-and-Language Navigation (VLN) requires an embodied agent to jointly understand multimodal instructions and visual-spatial context while reasoning over long action sequences. Recent works, such as NavCoT and NavGPT-2, demonstrate the potential of Chain-of-Thought (CoT) reasoning for improving interpretability and long-horizon planning. Moreover, multimodal extensions like OctoNav-R1 and CoT-VLA further validate CoT as a promising pathway toward human-like navigation reasoning. However, existing approaches face critical drawbacks: purely textual CoTs lack spatial grounding and easily overfit to sparse annotated reasoning steps, while multimodal CoTs incur severe token inflation by generating imagined visual observations, making real-time navigation impractical. In this work, we propose FantasyVLN, a unified implicit reasoning framework that preserves the benefits of CoT reasoning without explicit token overhead. Specifically, imagined visual tokens are encoded into a compact latent space using a pretrained Visual AutoRegressor (VAR) during CoT reasoning training, and the model jointly learns from textual, visual, and multimodal CoT modes under a unified multi-CoT strategy. At inference, our model performs direct instruction-to-action mapping while still enjoying reasoning-aware representations. Extensive experiments on LH-VLN show that our approach achieves reasoning-aware yet real-time navigation, improving success rates and efficiency while reducing inference latency by an order of magnitude compared to explicit CoT methods."
    },
    {
      "title": "TangramPuzzle: Evaluating Multimodal Large Language Models with Compositional Spatial Reasoning",
      "url": "https://arxiv.org/abs/2601.16520",
      "date": "2026-01-23",
      "authors_text": "Daixian Liu, Jiayi Kuang, Yinghui Li, Yangning Li, Di Yin",
      "is_highlight": false,
      "score": 82.0,
      "summary": "The TangramPuzzle benchmark evaluates multimodal large language models' compositional spatial reasoning through precise geometric tasks, revealing their tendency to prioritize silhouette matching over geometric accuracy.",
      "debug_abstract": "Multimodal Large Language Models (MLLMs) have achieved remarkable progress in visual recognition and semantic understanding. Nevertheless, their ability to perform precise compositional spatial reasoning remains largely unexplored. Existing benchmarks often involve relatively simple tasks and rely on semantic approximations or coarse relative positioning, while their evaluation metrics are typically limited and lack rigorous mathematical formulations. To bridge this gap, we introduce TangramPuzzle, a geometry-grounded benchmark designed to evaluate compositional spatial reasoning through the lens of the classic Tangram game. We propose the Tangram Construction Expression (TCE), a symbolic geometric framework that grounds tangram assemblies in exact, machine-verifiable coordinate specifications, to mitigate the ambiguity of visual approximation. We design two complementary tasks: Outline Prediction, which demands inferring global shapes from local components, and End-to-End Code Generation, which requires solving inverse geometric assembly problems. We conduct extensive evaluation experiments on advanced open-source and proprietary models, revealing an interesting insight: MLLMs tend to prioritize matching the target silhouette while neglecting geometric constraints, leading to distortions or deformations of the pieces."
    },
    {
      "title": "AnchoredDream: Zero-Shot 360° Indoor Scene Generation from a Single View via Geometric Grounding",
      "url": "https://arxiv.org/abs/2601.16532",
      "date": "2026-01-23",
      "authors_text": "Runmao Yao, Junsheng Zhou, Zhen Dong, Yu-Shen Liu",
      "is_highlight": false,
      "score": 72.0,
      "summary": "AnchoredDream introduces a zero-shot method for generating 360° indoor scenes from a single image, enhancing appearance consistency and geometric plausibility through a mutual boosting mechanism.",
      "debug_abstract": "Single-view indoor scene generation plays a crucial role in a range of real-world applications. However, generating a complete 360° scene from a single image remains a highly ill-posed and challenging problem. Recent approaches have made progress by leveraging diffusion models and depth estimation networks, yet they still struggle to maintain appearance consistency and geometric plausibility under large viewpoint changes, limiting their effectiveness in full-scene generation. To address this, we propose AnchoredDream, a novel zero-shot pipeline that anchors 360° scene generation on high-fidelity geometry via an appearance-geometry mutual boosting mechanism. Given a single-view image, our method first performs appearance-guided geometry generation to construct a reliable 3D scene layout. Then, we progressively generate the complete scene through a series of modules: warp-and-inpaint, warp-and-refine, post-optimization, and a novel Grouting Block, which ensures seamless transitions between the input view and generated regions. Extensive experiments demonstrate that AnchoredDream outperforms existing methods by a large margin in both appearance consistency and geometric plausibility--all in a zero-shot manner. Our results highlight the potential of geometric grounding for high-quality, zero-shot single-view scene generation."
    },
    {
      "title": "OnlineSI: Taming Large Language Model for Online 3D Understanding and Grounding",
      "url": "https://arxiv.org/abs/2601.16538",
      "date": "2026-01-23",
      "authors_text": "Zixian Liu, Zhaoxi Chen, Liang Pan, Ziwei Liu",
      "is_highlight": false,
      "score": 88.0,
      "summary": "OnlineSI is a framework that enhances Multimodal Large Language Models' spatial understanding in dynamic environments by integrating continuous learning from video streams and 3D point cloud data.",
      "debug_abstract": "In recent years, researchers have increasingly been interested in how to enable Multimodal Large Language Models (MLLM) to possess spatial understanding and reasoning capabilities. However, most existing methods overlook the importance of the ability to continuously work in an ever-changing world, and lack the possibility of deployment on embodied systems in real-world environments. In this work, we introduce OnlineSI, a framework that can continuously improve its spatial understanding of its surroundings given a video stream. Our core idea is to maintain a finite spatial memory to retain past observations, ensuring the computation required for each inference does not increase as the input accumulates. We further integrate 3D point cloud information with semantic information, helping MLLM to better locate and identify objects in the scene. To evaluate our method, we introduce the Fuzzy $F_1$-Score to mitigate ambiguity, and test our method on two representative datasets. Experiments demonstrate the effectiveness of our method, paving the way towards real-world embodied systems."
    },
    {
      "title": "GRIP: Algorithm-Agnostic Machine Unlearning for Mixture-of-Experts via Geometric Router Constraints",
      "url": "https://arxiv.org/abs/2601.16905",
      "date": "2026-01-23",
      "authors_text": "Andy Zhu, Rongzhe Wei, Yupu Gu, Pan Li",
      "is_highlight": false,
      "score": 62.0,
      "summary": "The GRIP framework enables effective machine unlearning in Mixture-of-Experts models by imposing geometric constraints on router updates, ensuring knowledge erasure without sacrificing model utility.",
      "debug_abstract": "Machine unlearning (MU) for large language models has become critical for AI safety, yet existing methods fail to generalize to Mixture-of-Experts (MoE) architectures. We identify that traditional unlearning methods exploit MoE&#39;s architectural vulnerability: they manipulate routers to redirect queries away from knowledgeable experts rather than erasing knowledge, causing a loss of model utility and superficial forgetting. We propose Geometric Routing Invariance Preservation (GRIP), an algorithm-agnostic framework for unlearning for MoE. Our core contribution is a geometric constraint, implemented by projecting router gradient updates into an expert-specific null-space. Crucially, this decouples routing stability from parameter rigidity: while discrete expert selections remain stable for retained knowledge, the continuous router parameters remain plastic within the null space, allowing the model to undergo necessary internal reconfiguration to satisfy unlearning objectives. This forces the unlearning optimization to erase knowledge directly from expert parameters rather than exploiting the superficial router manipulation shortcut. GRIP functions as an adapter, constraining router parameter updates without modifying the underlying unlearning algorithm. Extensive experiments on large-scale MoE models demonstrate that our adapter eliminates expert selection shift (achieving over 95% routing stability) across all tested unlearning methods while preserving their utility. By preventing existing algorithms from exploiting MoE model&#39;s router vulnerability, GRIP adapts existing unlearning research from dense architectures to MoEs."
    },
    {
      "title": "Render-of-Thought: Rendering Textual Chain-of-Thought as Images for Visual Latent Reasoning",
      "url": "https://arxiv.org/abs/2601.14750",
      "date": "2026-01-22",
      "authors_text": "Yifan Wang, Shiyu Li, Peiming Li, Xiaochen Yang, Yang Tang",
      "is_highlight": false,
      "score": 92.5,
      "summary": "The Render-of-Thought framework transforms Chain-of-Thought reasoning into images, enhancing analyzability and efficiency while maintaining competitive performance in reasoning tasks.",
      "debug_abstract": "Chain-of-Thought (CoT) prompting has achieved remarkable success in unlocking the reasoning capabilities of Large Language Models (LLMs). Although CoT prompting enhances reasoning, its verbosity imposes substantial computational overhead. Recent works often focus exclusively on outcome alignment and lack supervision on the intermediate reasoning process. These deficiencies obscure the analyzability of the latent reasoning chain. To address these challenges, we introduce Render-of-Thought (RoT), the first framework to reify the reasoning chain by rendering textual steps into images, making the latent rationale explicit and traceable. Specifically, we leverage the vision encoders of existing Vision Language Models (VLMs) as semantic anchors to align the vision embeddings with the textual space. This design ensures plug-and-play implementation without incurring additional pre-training overhead. Extensive experiments on mathematical and logical reasoning benchmarks demonstrate that our method achieves 3-4x token compression and substantial inference acceleration compared to explicit CoT. Furthermore, it maintains competitive performance against other methods, validating the feasibility of this paradigm. Our code is available at this https URL"
    },
    {
      "title": "LLM-in-Sandbox Elicits General Agentic Intelligence",
      "url": "https://arxiv.org/abs/2601.16206",
      "date": "2026-01-22",
      "authors_text": "Daixuan Cheng, Shaohan Huang, Yuxian Gu, Huatong Song, Guoxin Chen",
      "is_highlight": false,
      "score": 95.0,
      "summary": "The LLM-in-Sandbox framework enables large language models to demonstrate general intelligence in non-code tasks through sandbox exploration and reinforcement learning, achieving robust generalization across various domains.",
      "debug_abstract": "We introduce LLM-in-Sandbox, enabling LLMs to explore within a code sandbox (i.e., a virtual computer), to elicit general intelligence in non-code domains. We first demonstrate that strong LLMs, without additional training, exhibit generalization capabilities to leverage the code sandbox for non-code tasks. For example, LLMs spontaneously access external resources to acquire new knowledge, leverage the file system to handle long contexts, and execute scripts to satisfy formatting requirements. We further show that these agentic capabilities can be enhanced through LLM-in-Sandbox Reinforcement Learning (LLM-in-Sandbox-RL), which uses only non-agentic data to train models for sandbox exploration. Experiments demonstrate that LLM-in-Sandbox, in both training-free and post-trained settings, achieves robust generalization spanning mathematics, physics, chemistry, biomedicine, long-context understanding, and instruction following. Finally, we analyze LLM-in-Sandbox&#39;s efficiency from computational and system perspectives, and open-source it as a Python package to facilitate real-world deployment."
    },
    {
      "title": "Collision-Free Humanoid Traversal in Cluttered Indoor Scenes",
      "url": "https://arxiv.org/abs/2601.16035",
      "date": "2026-01-22",
      "authors_text": "Han Xue, Sikai Liang, Zhikai Zhang, Zicheng Zeng, Yun Liu",
      "is_highlight": false,
      "score": 81.2,
      "summary": "The paper presents Humanoid Potential Field (HumanoidPF) for enabling collision-free traversal of humanoids in cluttered indoor environments, demonstrating effective RL-based learning and real-world applicability.",
      "debug_abstract": "We study the problem of collision-free humanoid traversal in cluttered indoor scenes, such as hurdling over objects scattered on the floor, crouching under low-hanging obstacles, or squeezing through narrow passages. To achieve this goal, the humanoid needs to map its perception of surrounding obstacles with diverse spatial layouts and geometries to the corresponding traversal skills. However, the lack of an effective representation that captures humanoid-obstacle relationships during collision avoidance makes directly learning such mappings difficult. We therefore propose Humanoid Potential Field (HumanoidPF), which encodes these relationships as collision-free motion directions, significantly facilitating RL-based traversal skill learning. We also find that HumanoidPF exhibits a surprisingly negligible sim-to-real gap as a perceptual representation. To further enable generalizable traversal skills through diverse and challenging cluttered indoor scenes, we further propose a hybrid scene generation method, incorporating crops of realistic 3D indoor scenes and procedurally synthesized obstacles. We successfully transfer our policy to the real world and develop a teleoperation system where users could command the humanoid to traverse in cluttered indoor scenes with just a single click. Extensive experiments are conducted in both simulation and the real world to validate the effectiveness of our method. Demos and code can be found in our website: this https URL."
    }
  ],
  "智能产业研究院 (AIR)": [
    {
      "title": "EventFlash: Towards Efficient MLLMs for Event-Based Vision",
      "url": "https://arxiv.org/abs/2602.03230",
      "date": "2026-02-03",
      "authors_text": "Shaoyu Liu, Jianing Li, Guanghui Zhao, Yunjian Zhang, Wen Jiang",
      "is_highlight": false,
      "score": 63.0,
      "summary": "EventFlash introduces an efficient MLLM for event-based vision, leveraging spatiotemporal sparsification and adaptive sampling to enhance performance and reduce computational costs.",
      "teaser_image": "https://arxiv.org/html/2602.03230/x2.png",
      "debug_abstract": "Event-based multimodal large language models (MLLMs) enable robust perception in high-speed and low-light scenarios, addressing key limitations of frame-based MLLMs. However, current event-based MLLMs often rely on dense image-like processing paradigms, overlooking the spatiotemporal sparsity of event streams and resulting in high computational cost. In this paper, we propose EventFlash, a novel and efficient MLLM to explore spatiotemporal token sparsification for reducing data redundancy and accelerating inference. Technically, we build EventMind, a large-scale and scene-diverse dataset with over 500k instruction sets, providing both short and long event stream sequences to support our curriculum training strategy. We then present an adaptive temporal window aggregation module for efficient temporal sampling, which adaptively compresses temporal tokens while retaining key temporal cues. Finally, a sparse density-guided attention module is designed to improve spatial token efficiency by selecting informative regions and suppressing empty or sparse areas. Experimental results show that EventFlash achieves a $12.4\\times$ throughput improvement over the baseline (EventFlash-Zero) while maintaining comparable performance. It supports long-range event stream processing with up to 1,000 bins, significantly outperforming the 5-bin limit of EventGPT. We believe EventFlash serves as an efficient foundation model for event-based vision."
    },
    {
      "title": "InstaDrive: Instance-Aware Driving World Models for Realistic and Consistent Video Generation",
      "url": "https://arxiv.org/abs/2602.03242",
      "date": "2026-02-03",
      "authors_text": "Zhuoran Yang, Xi Guo, Chenjing Ding, Chiyu Wang, Wei Wu",
      "is_highlight": false,
      "score": 84.0,
      "summary": "InstaDrive introduces a framework that enhances realistic driving video generation by ensuring instance-level temporal consistency and spatial geometric fidelity through innovative feature extraction and alignment techniques.",
      "teaser_image": "https://arxiv.org/html/2602.03242/x2.png",
      "debug_abstract": "Autonomous driving relies on robust models trained on high-quality, large-scale multi-view driving videos. While world models offer a cost-effective solution for generating realistic driving videos, they struggle to maintain instance-level temporal consistency and spatial geometric fidelity. To address these challenges, we propose InstaDrive, a novel framework that enhances driving video realism through two key advancements: (1) Instance Flow Guider, which extracts and propagates instance features across frames to enforce temporal consistency, preserving instance identity over time. (2) Spatial Geometric Aligner, which improves spatial reasoning, ensures precise instance positioning, and explicitly models occlusion hierarchies. By incorporating these instance-aware mechanisms, InstaDrive achieves state-of-the-art video generation quality and enhances downstream autonomous driving tasks on the nuScenes dataset. Additionally, we utilize CARLA&#39;s autopilot to procedurally and stochastically simulate rare but safety-critical driving scenarios across diverse maps and regions, enabling rigorous safety evaluation for autonomous systems. Our project page is this https URL."
    },
    {
      "title": "Learning Physics-Grounded 4D Dynamics with Neural Gaussian Force Fields",
      "url": "https://arxiv.org/abs/2602.00148",
      "date": "2026-01-29",
      "authors_text": "Shiqian Li, Ruihong Shen, Junfeng Ni, Chang Pan, Chi Zhang",
      "is_highlight": false,
      "score": 82.0,
      "summary": "The Neural Gaussian Force Field (NGFF) framework enables rapid generation of interactive, physically realistic 4D videos from multi-view RGB inputs, enhancing robustness and generalization in physical dynamics.",
      "teaser_image": "https://arxiv.org/html/2602.00148/x1.png",
      "debug_abstract": "Predicting physical dynamics from raw visual data remains a major challenge in AI. While recent video generation models have achieved impressive visual quality, they still cannot consistently generate physically plausible videos due to a lack of modeling of physical laws. Recent approaches combining 3D Gaussian splatting and physics engines can produce physically plausible videos, but are hindered by high computational costs in both reconstruction and simulation, and often lack robustness in complex real-world scenarios. To address these issues, we introduce Neural Gaussian Force Field (NGFF), an end-to-end neural framework that integrates 3D Gaussian perception with physics-based dynamic modeling to generate interactive, physically realistic 4D videos from multi-view RGB inputs, achieving two orders of magnitude faster than prior Gaussian simulators. To support training, we also present GSCollision, a 4D Gaussian dataset featuring diverse materials, multi-object interactions, and complex scenes, totaling over 640k rendered physical videos (~4 TB). Evaluations on synthetic and real 3D scenarios show NGFF&#39;s strong generalization and robustness in physical reasoning, advancing video prediction towards physics-grounded world models."
    },
    {
      "title": "How Far Are LLMs from Professional Poker Players? Revisiting Game-Theoretic Reasoning with Agentic Tool Use",
      "url": "https://arxiv.org/abs/2602.00528",
      "date": "2026-01-31",
      "authors_text": "Minhua Lin, Enyan Dai, Hui Liu, Xianfeng Tang, Yuliang Yan",
      "is_highlight": false,
      "score": 55.0,
      "summary": "This paper evaluates LLMs in poker, revealing their strategic reasoning flaws and proposing ToolPoker, which enhances gameplay and reasoning through integrated external solvers.",
      "teaser_image": "https://arxiv.org/html/2602.00528/x2.png",
      "debug_abstract": "As Large Language Models (LLMs) are increasingly applied in high-stakes domains, their ability to reason strategically under uncertainty becomes critical. Poker provides a rigorous testbed, requiring not only strong actions but also principled, game-theoretic reasoning. In this paper, we conduct a systematic study of LLMs in multiple realistic poker tasks, evaluating both gameplay outcomes and reasoning traces. Our analysis reveals LLMs fail to compete against traditional algorithms and identifies three recurring flaws: reliance on heuristics, factual misunderstandings, and a &#34;knowing-doing&#34; gap where actions diverge from reasoning. An initial attempt with behavior cloning and step-level reinforcement learning improves reasoning style but remains insufficient for accurate game-theoretic play. Motivated by these limitations, we propose ToolPoker, a tool-integrated reasoning framework that combines external solvers for GTO-consistent actions with more precise professional-style explanations. Experiments demonstrate that ToolPoker achieves state-of-the-art gameplay while producing reasoning traces that closely reflect game-theoretic principles."
    },
    {
      "title": "V2X-DSC: Multi-Agent Collaborative Perception with Distributed Source Coding Guided Communication",
      "url": "https://arxiv.org/abs/2602.00687",
      "date": "2026-01-31",
      "authors_text": "Yuankun Zeng, Shaohui Li, Zhi Li, Shulan Ruan, Yu Liu",
      "is_highlight": false,
      "score": 76.0,
      "summary": "V2X-DSC introduces a Conditional Codec for efficient multi-agent collaborative perception by compressing and reconstructing BEV features, optimizing bandwidth usage while maintaining accuracy.",
      "teaser_image": "https://arxiv.org/html/2602.00687/figs/overview.jpg",
      "debug_abstract": "Collaborative perception improves 3D understanding by fusing multi-agent observations, yet intermediate-feature sharing faces strict bandwidth constraints as dense BEV features saturate V2X links. We observe that collaborators view the same physical world, making their features strongly correlated; thus receivers only need innovation beyond their local context. Revisiting this from a distributed source coding perspective, we propose V2X-DSC, a framework with a Conditional Codec (DCC) for bandwidth-constrained fusion. The sender compresses BEV features into compact codes, while the receiver performs conditional reconstruction using its local features as side information, allocating bits to complementary cues rather than redundant content. This conditional structure regularizes learning, encouraging incremental representation and yielding lower-noise features. Experiments on DAIR-V2X, OPV2V, and V2X-Real demonstrate state-of-the-art accuracy-bandwidth trade-offs under KB-level communication, and generalizes as a plug-and-play communication layer across multiple fusion backbones."
    },
    {
      "title": "Resource-Efficient Reinforcement for Reasoning Large Language Models via Dynamic One-Shot Policy Refinement",
      "url": "https://arxiv.org/abs/2602.00815",
      "date": "2026-01-31",
      "authors_text": "Yunjian Zhang, Sudong Wang, Yang Li, Peiran Xu, Conghao Zhou",
      "is_highlight": false,
      "score": 57.0,
      "summary": "The paper introduces Dynamic One-Shot Policy Refinement (DoPR), a resource-efficient reinforcement learning strategy that enhances reasoning in large language models while significantly reducing training costs.",
      "teaser_image": "https://arxiv.org/html/2602.00815/dopr.png",
      "debug_abstract": "Large language models (LLMs) have exhibited remarkable performance on complex reasoning tasks, with reinforcement learning under verifiable rewards (RLVR) emerging as a principled framework for aligning model behavior with reasoning chains. Despite its promise, RLVR remains prohibitively resource-intensive, requiring extensive reward signals and incurring substantial rollout costs during training. In this work, we revisit the fundamental question of data and compute efficiency in RLVR. We first establish a theoretical lower bound on the sample complexity required to unlock reasoning capabilities, and empirically validate that strong performance can be achieved with a surprisingly small number of training instances. To tackle the computational burden, we propose Dynamic One-Shot Policy Refinement (DoPR), an uncertainty-aware RL strategy that dynamically selects a single informative training sample per batch for policy updates, guided by reward volatility and exploration-driven acquisition. DoPR reduces rollout overhead by nearly an order of magnitude while preserving competitive reasoning accuracy, offering a scalable and resource-efficient solution for LLM post-training. This approach offers a practical path toward more efficient and accessible RL-based training for reasoning-intensive LLM applications."
    },
    {
      "title": "Making Avatars Interact: Towards Text-Driven Human-Object Interaction for Controllable Talking Avatars",
      "url": "https://arxiv.org/abs/2602.01538",
      "date": "2026-02-02",
      "authors_text": "Youliang Zhang, Zhengguang Zhou, Zhentao Yu, Ziyao Huang, Teng Hu",
      "is_highlight": false,
      "score": 46.0,
      "summary": "The paper presents InteractAvatar, a dual-stream framework for generating talking avatars that perform text-driven human-object interactions by decoupling perception and planning from video synthesis.",
      "teaser_image": "https://arxiv.org/html/2602.01538/x1.png",
      "debug_abstract": "Generating talking avatars is a fundamental task in video generation. Although existing methods can generate full-body talking avatars with simple human motion, extending this task to grounded human-object interaction (GHOI) remains an open challenge, requiring the avatar to perform text-aligned interactions with surrounding objects. This challenge stems from the need for environmental perception and the control-quality dilemma in GHOI generation. To address this, we propose a novel dual-stream framework, InteractAvatar, which decouples perception and planning from video synthesis for grounded human-object interaction. Leveraging detection to enhance environmental perception, we introduce a Perception and Interaction Module (PIM) to generate text-aligned interaction motions. Additionally, an Audio-Interaction Aware Generation Module (AIM) is proposed to synthesize vivid talking avatars performing object interactions. With a specially designed motion-to-video aligner, PIM and AIM share a similar network structure and enable parallel co-generation of motions and plausible videos, effectively mitigating the control-quality dilemma. Finally, we establish a benchmark, GroundedInter, for evaluating GHOI video generation. Extensive experiments and comparisons demonstrate the effectiveness of our method in generating grounded human-object interactions for talking avatars. Project page: this https URL"
    },
    {
      "title": "Reasoning with Autoregressive-Diffusion Collaborative Thoughts",
      "url": "https://arxiv.org/abs/2602.01608",
      "date": "2026-02-02",
      "authors_text": "Mu Yuan, Liekang Zeng, Guoliang Xing, Lan Zhang, Yunhao Liu",
      "is_highlight": false,
      "score": 41.0,
      "summary": "The paper presents Collaborative Thoughts, a framework integrating autoregressive and diffusion models for enhanced spatial reasoning and generation through iterative feedback and joint reasoning.",
      "teaser_image": "https://arxiv.org/html/2602.01608/x2.png",
      "debug_abstract": "Autoregressive and diffusion models represent two complementary generative paradigms. Autoregressive models excel at sequential planning and constraint composition, yet struggle with tasks that require explicit spatial or physical grounding. Diffusion models, in contrast, capture rich spatial structure through high-dimensional generation, but lack the stepwise logical control needed to satisfy complex, multi-stage constraints or to reliably identify and correct errors. We introduce Collaborative Thoughts, a unified collaborative framework that enables autoregressive and diffusion models to reason and generate jointly through a closed-loop interaction. In Collaborative Thoughts, autoregressive models perform structured planning and constraint management, diffusion models instantiate these constraints as intermediate visual thoughts, and a vision-based critic module evaluates whether the visual thoughts satisfy the intended structural and physical requirements. This feedback is then used to iteratively refine subsequent planning and generation steps, mitigating error propagation across modalities. Importantly, Collaborative Thoughts uses the same collaborative loop regardless of whether the task is autoregressive question answering or diffusion-based visual generation. Through representative examples, we illustrate how Collaborative Thoughts can improve the reliability of spatial reasoning and the controllability of generation."
    },
    {
      "title": "From Frames to Sequences: Temporally Consistent Human-Centric Dense Prediction",
      "url": "https://arxiv.org/abs/2602.01661",
      "date": "2026-02-02",
      "authors_text": "Xingyu Miao, Junting Dong, Qin Zhao, Yuhang Yang, Junhao Chen",
      "is_highlight": false,
      "score": 43.0,
      "summary": "This paper presents a novel synthetic data pipeline and a ViT-based model for achieving temporally consistent human-centric dense prediction in video sequences, outperforming existing methods.",
      "teaser_image": "https://arxiv.org/html/2602.01661/x2.png",
      "debug_abstract": "In this work, we focus on the challenge of temporally consistent human-centric dense prediction across video sequences. Existing models achieve strong per-frame accuracy but often flicker under motion, occlusion, and lighting changes, and they rarely have paired human video supervision for multiple dense tasks. We address this gap with a scalable synthetic data pipeline that generates photorealistic human frames and motion-aligned sequences with pixel-accurate depth, normals, and masks. Unlike prior static data synthetic pipelines, our pipeline provides both frame-level labels for spatial learning and sequence-level supervision for temporal learning. Building on this, we train a unified ViT-based dense predictor that (i) injects an explicit human geometric prior via CSE embeddings and (ii) improves geometry-feature reliability with a lightweight channel reweighting module after feature fusion. Our two-stage training strategy, combining static pretraining with dynamic sequence supervision, enables the model first to acquire robust spatial representations and then refine temporal consistency across motion-aligned sequences. Extensive experiments show that we achieve state-of-the-art performance on THuman2.1 and Hi4D and generalize effectively to in-the-wild videos."
    },
    {
      "title": "ForSim: Stepwise Forward Simulation for Traffic Policy Fine-Tuning",
      "url": "https://arxiv.org/abs/2602.01916",
      "date": "2026-02-02",
      "authors_text": "Keyu Chen, Wenchao Sun, Hao Cheng, Zheng Fu, Sifa Zheng",
      "is_highlight": true,
      "score": 29.0,
      "summary": "ForSim introduces a stepwise closed-loop simulation method that enhances traffic policy fine-tuning by improving agent interactions and preserving multimodal behaviors in autonomous driving.",
      "teaser_image": "https://arxiv.org/html/2602.01916/x2.png",
      "debug_abstract": "As the foundation of closed-loop training and evaluation in autonomous driving, traffic simulation still faces two fundamental challenges: covariate shift introduced by open-loop imitation learning and limited capacity to reflect the multimodal behaviors observed in real-world traffic. Although recent frameworks such as RIFT have partially addressed these issues through group-relative optimization, their forward simulation procedures remain largely non-reactive, leading to unrealistic agent interactions within the virtual domain and ultimately limiting simulation fidelity. To address these issues, we propose ForSim, a stepwise closed-loop forward simulation paradigm. At each virtual timestep, the traffic agent propagates the virtual candidate trajectory that best spatiotemporally matches the reference trajectory through physically grounded motion dynamics, thereby preserving multimodal behavioral diversity while ensuring intra-modality consistency. Other agents are updated with stepwise predictions, yielding coherent and interaction-aware evolution. When incorporated into the RIFT traffic simulation framework, ForSim operates in conjunction with group-relative optimization to fine-tune traffic policy. Extensive experiments confirm that this integration consistently improves safety while maintaining efficiency, realism, and comfort. These results underscore the importance of modeling closed-loop multimodal interactions within forward simulation and enhance the fidelity and reliability of traffic simulation for autonomous driving. Project Page: this https URL"
    },
    {
      "title": "Small Generalizable Prompt Predictive Models Can Steer Efficient RL Post-Training of Large Reasoning Models",
      "url": "https://arxiv.org/abs/2602.01970",
      "date": "2026-02-02",
      "authors_text": "Yun Qu, Qi Wang, Yixiu Mao, Heming Zou, Yuhang Jiang",
      "is_highlight": false,
      "score": 26.0,
      "summary": "The paper presents Generalizable Predictive Prompt Selection (GPS), a method that enhances reinforcement learning efficiency in large language models by utilizing a lightweight generative model for prompt selection.",
      "teaser_image": "https://arxiv.org/html/2602.01970/x1.png",
      "debug_abstract": "Reinforcement learning enhances the reasoning capabilities of large language models but often involves high computational costs due to rollout-intensive optimization. Online prompt selection presents a plausible solution by prioritizing informative prompts to improve training efficiency. However, current methods either depend on costly, exact evaluations or construct prompt-specific predictive models lacking generalization across prompts. This study introduces Generalizable Predictive Prompt Selection (GPS), which performs Bayesian inference towards prompt difficulty using a lightweight generative model trained on the shared optimization history. Intermediate-difficulty prioritization and history-anchored diversity are incorporated into the batch acquisition principle to select informative prompt batches. The small predictive model also generalizes at test-time for efficient computational allocation. Experiments across varied reasoning benchmarks indicate GPS&#39;s substantial improvements in training efficiency, final performance, and test-time efficiency over superior baseline methods."
    },
    {
      "title": "TTT-Parkour: Rapid Test-Time Training for Perceptive Robot Parkour",
      "url": "https://arxiv.org/abs/2602.02331",
      "date": "2026-02-02",
      "authors_text": "Shaoting Zhu, Baijun Ye, Jiaxuan Wang, Jiakang Chen, Ziwen Zhuang",
      "is_highlight": false,
      "score": 18.0,
      "summary": "TTT-Parkour introduces a rapid test-time training framework enabling humanoid robots to effectively navigate complex terrains through efficient real-to-sim-to-real learning and geometry reconstruction.",
      "teaser_image": "https://arxiv.org/html/2602.02331/x1.png",
      "debug_abstract": "Achieving highly dynamic humanoid parkour on unseen, complex terrains remains a challenge in robotics. Although general locomotion policies demonstrate capabilities across broad terrain distributions, they often struggle with arbitrary and highly challenging environments. To overcome this limitation, we propose a real-to-sim-to-real framework that leverages rapid test-time training (TTT) on novel terrains, significantly enhancing the robot&#39;s capability to traverse extremely difficult geometries. We adopt a two-stage end-to-end learning paradigm: a policy is first pre-trained on diverse procedurally generated terrains, followed by rapid fine-tuning on high-fidelity meshes reconstructed from real-world captures. Specifically, we develop a feed-forward, efficient, and high-fidelity geometry reconstruction pipeline using RGB-D inputs, ensuring both speed and quality during test-time training. We demonstrate that TTT-Parkour empowers humanoid robots to master complex obstacles, including wedges, stakes, boxes, trapezoids, and narrow beams. The whole pipeline of capturing, reconstructing, and test-time training requires less than 10 minutes on most tested terrains. Extensive experiments show that the policy after test-time training exhibits robust zero-shot sim-to-real transfer capability."
    },
    {
      "title": "Context Learning for Multi-Agent Discussion",
      "url": "https://arxiv.org/abs/2602.02350",
      "date": "2026-02-02",
      "authors_text": "Xingyuan Hua, Sheng Yue, Xinyi Li, Yizhe Zhao, Jinrui Zhang",
      "is_highlight": false,
      "score": 17.0,
      "summary": "The paper presents M2CL, a multi-LLM context learning method that enhances coherence in Multi-Agent Discussions, improving problem-solving performance by 20%-50% over existing approaches.",
      "teaser_image": "https://arxiv.org/html/2602.02350/x2.png",
      "debug_abstract": "Multi-Agent Discussion (MAD) has garnered increasing attention very recently, where multiple LLM instances collaboratively solve problems via structured discussion. However, we find that current MAD methods easily suffer from discussion inconsistency, LLMs fail to reach a coherent solution, due to the misalignment between their individual this http URL this paper, we introduce a multi-LLM context learning method (M2CL) that learns a context generator for each agent, capable of dynamically generating context instructions per discussion round via automatic information organization and refinement. Specifically, inspired by our theoretical insights on the context instruction, M2CL train the generators to control context coherence and output discrepancies via a carefully crafted self-adaptive this http URL enables LLMs to avoid premature convergence on majority noise and progressively reach the correct consensus. We evaluate M2CL on challenging tasks, including academic reasoning, embodied tasks, and mobile control. The results show that the performance of M2CL significantly surpasses existing methods by 20%--50%, while enjoying favorable transferability and computational efficiency."
    },
    {
      "title": "UV-M3TL: A Unified and Versatile Multimodal Multi-Task Learning Framework for Assistive Driving Perception",
      "url": "https://arxiv.org/abs/2602.01594",
      "date": "2026-02-02",
      "authors_text": "Wenzhuo Liu, Qiannan Guo, Zhen Wang, Wenshuo Wang, Lei Yang",
      "is_highlight": false,
      "score": 8.0,
      "summary": "The UV-M3TL framework enhances ADAS by effectively recognizing driver behavior and context through dual-branch multimodal embedding and adaptive loss, achieving state-of-the-art performance across multiple tasks.",
      "teaser_image": "https://arxiv.org/html/2602.01594/x2.png",
      "debug_abstract": "Advanced Driver Assistance Systems (ADAS) need to understand human driver behavior while perceiving their navigation context, but jointly learning these heterogeneous tasks would cause inter-task negative transfer and impair system performance. Here, we propose a Unified and Versatile Multimodal Multi-Task Learning (UV-M3TL) framework to simultaneously recognize driver behavior, driver emotion, vehicle behavior, and traffic context, while mitigating inter-task negative transfer. Our framework incorporates two core components: dual-branch spatial channel multimodal embedding (DB-SCME) and adaptive feature-decoupled multi-task loss (AFD-Loss). DB-SCME enhances cross-task knowledge transfer while mitigating task conflicts by employing a dual-branch structure to explicitly model salient task-shared and task-specific features. AFD-Loss improves the stability of joint optimization while guiding the model to learn diverse multi-task representations by introducing an adaptive weighting mechanism based on learning dynamics and feature decoupling constraints. We evaluate our method on the AIDE dataset, and the experimental results demonstrate that UV-M3TL achieves state-of-the-art performance across all four tasks. To further prove the versatility, we evaluate UV-M3TL on additional public multi-task perception benchmarks (BDD100K, CityScapes, NYUD-v2, and PASCAL-Context), where it consistently delivers strong performance across diverse task combinations, attaining state-of-the-art results on most tasks."
    },
    {
      "title": "Enhancing Indoor Occupancy Prediction via Sparse Query-Based Multi-Level Consistent Knowledge Distillation",
      "url": "https://arxiv.org/abs/2602.02318",
      "date": "2026-02-02",
      "authors_text": "Xiang Li, Yupeng Zheng, Pengfei Li, Yilun Chen, Ya-Qin Zhang",
      "is_highlight": false,
      "score": 0,
      "summary": "DiScene introduces a sparse query-based framework for efficient and robust indoor occupancy prediction through multi-level knowledge distillation, achieving state-of-the-art performance and faster inference.",
      "teaser_image": "https://arxiv.org/html/2602.02318/x2.png",
      "debug_abstract": "Occupancy prediction provides critical geometric and semantic understanding for robotics but faces efficiency-accuracy trade-offs. Current dense methods suffer computational waste on empty voxels, while sparse query-based approaches lack robustness in diverse and complex indoor scenes. In this paper, we propose DiScene, a novel sparse query-based framework that leverages multi-level distillation to achieve efficient and robust occupancy prediction. In particular, our method incorporates two key innovations: (1) a Multi-level Consistent Knowledge Distillation strategy, which transfers hierarchical representations from large teacher models to lightweight students through coordinated alignment across four levels, including encoder-level feature alignment, query-level feature matching, prior-level spatial guidance, and anchor-level high-confidence knowledge transfer and (2) a Teacher-Guided Initialization policy, employing optimized parameter warm-up to accelerate model convergence. Validated on the Occ-Scannet benchmark, DiScene achieves 23.2 FPS without depth priors while outperforming our baseline method, OPUS, by 36.1% and even better than the depth-enhanced version, OPUS†. With depth integration, DiScene† attains new SOTA performance, surpassing EmbodiedOcc by 3.7% with 1.62$\\times$ faster inference speed. Furthermore, experiments on the Occ3D-nuScenes benchmark and in-the-wild scenarios demonstrate the versatility of our approach in various environments. Code and models can be accessed at this https URL."
    },
    {
      "title": "InspecSafe-V1: A Multimodal Benchmark for Safety Assessment in Industrial Inspection Scenarios",
      "url": "https://arxiv.org/abs/2601.21173",
      "date": "2026-01-29",
      "authors_text": "Zeyi Liu, Shuang Liu, Jihai Min, Zhaoheng Zhang, Jun Cen",
      "is_highlight": false,
      "score": 82.0,
      "summary": "InspecSafe-V1 is a multimodal benchmark dataset for safety assessment in industrial inspections, featuring real-world data from diverse scenarios and multiple sensing modalities.",
      "teaser_image": "https://arxiv.org/html/2601.21173/x1.png",
      "debug_abstract": "With the rapid development of industrial intelligence and unmanned inspection, reliable perception and safety assessment for AI systems in complex and dynamic industrial sites has become a key bottleneck for deploying predictive maintenance and autonomous inspection. Most public datasets remain limited by simulated data sources, single-modality sensing, or the absence of fine-grained object-level annotations, which prevents robust scene understanding and multimodal safety reasoning for industrial foundation models. To address these limitations, InspecSafe-V1 is released as the first multimodal benchmark dataset for industrial inspection safety assessment that is collected from routine operations of real inspection robots in real-world environments. InspecSafe-V1 covers five representative industrial scenarios, including tunnels, power facilities, sintering equipment, oil and gas petrochemical plants, and coal conveyor trestles. The dataset is constructed from 41 wheeled and rail-mounted inspection robots operating at 2,239 valid inspection sites, yielding 5,013 inspection instances. For each instance, pixel-level segmentation annotations are provided for key objects in visible-spectrum images. In addition, a semantic scene description and a corresponding safety level label are provided according to practical inspection tasks. Seven synchronized sensing modalities are further included, including infrared video, audio, depth point clouds, radar point clouds, gas measurements, temperature, and humidity, to support multimodal anomaly recognition, cross-modal fusion, and comprehensive safety assessment in industrial environments."
    },
    {
      "title": "Listen, Look, Drive: Coupling Audio Instructions for User-aware VLA-based Autonomous Driving",
      "url": "https://arxiv.org/abs/2601.12142",
      "date": "2026-01-28",
      "authors_text": "Ziang Guo, Feng Yang, Xuefeng Zhang, Jiaqi Guo, Kun Zhao",
      "is_highlight": false,
      "score": 88.0,
      "summary": "EchoVLA enhances autonomous driving by integrating audio instructions with visual inputs, allowing for user influence and emotional context, significantly improving navigation accuracy and safety.",
      "teaser_image": "https://arxiv.org/html/2601.12142/Figs/overview.png",
      "debug_abstract": "Vision Language Action (VLA) models promise an open-vocabulary interface that can translate perceptual ambiguity into semantically grounded driving decisions, yet they still treat language as a static prior fixed at inference time. As a result, the model must infer continuously shifting objectives from pixels alone, yielding delayed or overly conservative maneuvers. We argue that effective VLAs for autonomous driving need an online channel in which users can influence driving with specific intentions. To this end, we present EchoVLA, a user-aware VLA that couples camera streams with in situ audio instructions. We augment the nuScenes dataset with temporally aligned, intent-specific speech commands generated by converting ego-motion descriptions into synthetic audios. Further, we compose emotional speech-trajectory pairs into a multimodal Chain-of-Thought (CoT) for fine-tuning a Multimodal Large Model (MLM) based on Qwen2.5-Omni. Specifically, we synthesize the audio-augmented dataset with different emotion types paired with corresponding driving behaviors, leveraging the emotional cues embedded in tone, pitch, and speech tempo to reflect varying user states, such as urgent or hesitant intentions, thus enabling our EchoVLA to interpret not only the semantic content but also the emotional context of audio commands for more nuanced and emotionally adaptive driving behavior. In open-loop benchmarks, our approach reduces the average L2 error by $59.4\\%$ and the collision rate by $74.4\\%$ compared to the baseline of vision-only perception. More experiments on nuScenes dataset validate that EchoVLA not only steers the trajectory through audio instructions, but also modulates driving behavior in response to the emotions detected in the user&#39;s speech."
    },
    {
      "title": "HINT: Hierarchical Interaction Modeling for Autoregressive Multi-Human Motion Generation",
      "url": "https://arxiv.org/abs/2601.20383",
      "date": "2026-01-28",
      "authors_text": "Mengge Liu, Yan Di, Gu Wang, Yun Qu, Dekai Zhu",
      "is_highlight": false,
      "score": 78.0,
      "summary": "HINT introduces a novel autoregressive framework for multi-human motion generation, enhancing interaction modeling and performance with variable agent counts and long text guidance.",
      "teaser_image": "https://arxiv.org/html/2601.20383/x2.png",
      "debug_abstract": "Text-driven multi-human motion generation with complex interactions remains a challenging problem. Despite progress in performance, existing offline methods that generate fixed-length motions with a fixed number of agents, are inherently limited in handling long or variable text, and varying agent counts. These limitations naturally encourage autoregressive formulations, which predict future motions step by step conditioned on all past trajectories and current text guidance. In this work, we introduce HINT, the first autoregressive framework for multi-human motion generation with Hierarchical INTeraction modeling in diffusion. First, HINT leverages a disentangled motion representation within a canonicalized latent space, decoupling local motion semantics from inter-person interactions. This design facilitates direct adaptation to varying numbers of human participants without requiring additional refinement. Second, HINT adopts a sliding-window strategy for efficient online generation, and aggregates local within-window and global cross-window conditions to capture past human history, inter-person dependencies, and align with text guidance. This strategy not only enables fine-grained interaction modeling within each window but also preserves long-horizon coherence across all the long sequence. Extensive experiments on public benchmarks demonstrate that HINT matches the performance of strong offline models and surpasses autoregressive baselines. Notably, on InterHuman, HINT achieves an FID of 3.100, significantly improving over the previous state-of-the-art score of 5.154."
    },
    {
      "title": "Self-Supervised Path Planning in Unstructured Environments via Global-Guided Differentiable Hard Constraint Projection",
      "url": "https://arxiv.org/abs/2601.19354",
      "date": "2026-01-27",
      "authors_text": "Ziqian Wang, Chenxi Fang, Zhen Zhang",
      "is_highlight": false,
      "score": 90.0,
      "summary": "This paper presents a self-supervised framework for autonomous navigation in unstructured environments, utilizing a differentiable projection layer to ensure safety and efficiency in path planning.",
      "debug_abstract": "Deploying deep learning agents for autonomous navigation in unstructured environments faces critical challenges regarding safety, data scarcity, and limited computational resources. Traditional solvers often suffer from high latency, while emerging learning-based approaches struggle to ensure deterministic feasibility. To bridge the gap from embodied to embedded intelligence, we propose a self-supervised framework incorporating a differentiable hard constraint projection layer for runtime assurance. To mitigate data scarcity, we construct a Global-Guided Artificial Potential Field (G-APF), which provides dense supervision signals without manual labeling. To enforce actuator limitations and geometric constraints efficiently, we employ an adaptive neural projection layer, which iteratively rectifies the coarse network output onto the feasible manifold. Extensive benchmarks on a test set of 20,000 scenarios demonstrate an 88.75\\% success rate, substantiating the enhanced operational safety. Closed-loop experiments in CARLA further validate the physical realizability of the planned paths under dynamic constraints. Furthermore, deployment verification on an NVIDIA Jetson Orin NX confirms an inference latency of 94 ms, showing real-time feasibility on resource-constrained embedded hardware. This framework offers a generalized paradigm for embedding physical laws into neural architectures, providing a viable direction for solving constrained optimization in mechatronics. Source code is available at: this https URL."
    },
    {
      "title": "AtomMem : Learnable Dynamic Agentic Memory with Atomic Memory Operation",
      "url": "https://arxiv.org/abs/2601.08323",
      "date": "2026-01-27",
      "authors_text": "Yupeng Huo, Yaxi Lu, Zhong Zhang, Haotian Chen, Yankai Lin",
      "is_highlight": false,
      "score": 82.0,
      "summary": "AtomMem introduces a learnable dynamic memory framework that optimizes memory management through atomic CRUD operations, significantly outperforming static methods in long-horizon tasks.",
      "debug_abstract": "Equipping agents with memory is essential for solving real-world long-horizon problems. However, most existing agent memory mechanisms rely on static and hand-crafted workflows. This limits the performance and generalization ability of these memory designs, which highlights the need for a more flexible, learning-based memory framework. In this paper, we propose AtomMem, which reframes memory management as a dynamic decision-making problem. We deconstruct high-level memory processes into fundamental atomic CRUD (Create, Read, Update, Delete) operations, transforming the memory workflow into a learnable decision process. By combining supervised fine-tuning with reinforcement learning, AtomMem learns an autonomous, task-aligned policy to orchestrate memory behaviors tailored to specific task demands. Experimental results across 3 long-context benchmarks demonstrate that the trained AtomMem-8B consistently outperforms prior static-workflow memory methods. Further analysis of training dynamics shows that our learning-based formulation enables the agent to discover structured, task-aligned memory management strategies, highlighting a key advantage over predefined routines."
    },
    {
      "title": "Visual Generation Unlocks Human-Like Reasoning through Multimodal World Models",
      "url": "https://arxiv.org/abs/2601.19834",
      "date": "2026-01-27",
      "authors_text": "Jialong Wu, Xiaoying Zhang, Hongyi Yuan, Xiangcheng Zhang, Tianhao Huang",
      "is_highlight": false,
      "score": 89.0,
      "summary": "This paper demonstrates that visual generation enhances reasoning in multimodal AI, particularly for tasks requiring physical world understanding, outperforming purely verbal approaches.",
      "debug_abstract": "Humans construct internal world models and reason by manipulating the concepts within these models. Recent advances in AI, particularly chain-of-thought (CoT) reasoning, approximate such human cognitive abilities, where world models are believed to be embedded within large language models. Expert-level performance in formal and abstract domains such as mathematics and programming has been achieved in current systems by relying predominantly on verbal reasoning. However, they still lag far behind humans in domains like physical and spatial intelligence, which require richer representations and prior knowledge. The emergence of unified multimodal models (UMMs) capable of both verbal and visual generation has therefore sparked interest in more human-like reasoning grounded in complementary multimodal pathways, though their benefits remain unclear. From a world-model perspective, this paper presents the first principled study of when and how visual generation benefits reasoning. Our key position is the visual superiority hypothesis: for certain tasks--particularly those grounded in the physical world--visual generation more naturally serves as world models, whereas purely verbal world models encounter bottlenecks arising from representational limitations or insufficient prior knowledge. Theoretically, we formalize internal world modeling as a core component of CoT reasoning and analyze distinctions among different forms of world models. Empirically, we identify tasks that necessitate interleaved visual-verbal CoT reasoning, constructing a new evaluation suite, VisWorld-Eval. Controlled experiments on a state-of-the-art UMM show that interleaved CoT significantly outperforms purely verbal CoT on tasks that favor visual world modeling, but offers no clear advantage otherwise. Together, this work clarifies the potential of multimodal world modeling for more powerful, human-like multimodal AI."
    },
    {
      "title": "Scalable Exploration for High-Dimensional Continuous Control via Value-Guided Flow",
      "url": "https://arxiv.org/abs/2601.19707",
      "date": "2026-01-27",
      "authors_text": "Yunyue Wei, Chenhui Zuo, Yanan Sui",
      "is_highlight": false,
      "score": 79.0,
      "summary": "Qflex introduces a scalable reinforcement learning method for high-dimensional continuous control that enhances exploration efficiency by aligning actions with learned value gradients.",
      "debug_abstract": "Controlling high-dimensional systems in biological and robotic applications is challenging due to expansive state-action spaces, where effective exploration is critical. Commonly used exploration strategies in reinforcement learning are largely undirected with sharp degradation as action dimensionality grows. Many existing methods resort to dimensionality reduction, which constrains policy expressiveness and forfeits system flexibility. We introduce Q-guided Flow Exploration (Qflex), a scalable reinforcement learning method that conducts exploration directly in the native high-dimensional action space. During training, Qflex traverses actions from a learnable source distribution along a probability flow induced by the learned value function, aligning exploration with task-relevant gradients rather than isotropic noise. Our proposed method substantially outperforms representative online reinforcement learning baselines across diverse high-dimensional continuous-control benchmarks. Qflex also successfully controls a full-body human musculoskeletal model to perform agile, complex movements, demonstrating superior scalability and sample efficiency in very high-dimensional settings. Our results indicate that value-guided flows offer a principled and practical route to exploration at scale."
    },
    {
      "title": "Latent-Space Contrastive Reinforcement Learning for Stable and Efficient LLM Reasoning",
      "url": "https://arxiv.org/abs/2601.17275",
      "date": "2026-01-24",
      "authors_text": "Lianlei Shan, Han Chen, Yixuan Wang, Zhenjie Liu, Wei Li",
      "is_highlight": false,
      "score": 60.0,
      "summary": "The paper introduces DeepLatent Reasoning (DLR), a latent-space contrastive reinforcement learning framework that enhances LLM reasoning by addressing inefficiencies and stability in multi-step tasks.",
      "debug_abstract": "While Large Language Models (LLMs) demonstrate exceptional performance in surface-level text generation, their nature in handling complex multi-step reasoning tasks often remains one of ``statistical fitting&#39;&#39; rather than systematic logical deduction. Traditional Reinforcement Learning (RL) attempts to mitigate this by introducing a ``think-before-speak&#39;&#39; paradigm. However, applying RL directly in high-dimensional, discrete token spaces faces three inherent challenges: sample-inefficient rollouts, high gradient estimation variance, and the risk of catastrophic forgetting. To fundamentally address these structural bottlenecks, we propose \\textbf{DeepLatent Reasoning (DLR)}, a latent-space bidirectional contrastive reinforcement learning framework. This framework shifts the trial-and-error cost from expensive token-level full sequence generation to the continuous latent manifold. Specifically, we introduce a lightweight assistant model to efficiently sample $K$ reasoning chain encodings within the latent space. These encodings are filtered via a dual reward mechanism based on correctness and formatting; only high-value latent trajectories are fed into a \\textbf{frozen main model} for single-pass decoding. To maximize reasoning diversity while maintaining coherence, we design a contrastive learning objective to enable directed exploration within the latent space. Since the main model parameters remain frozen during optimization, this method mathematically eliminates catastrophic forgetting. Experiments demonstrate that under comparable GPU computational budgets, DLR achieves more stable training convergence, supports longer-horizon reasoning chains, and facilitates the sustainable accumulation of reasoning capabilities, providing a viable path toward reliable and scalable reinforcement learning for LLMs."
    },
    {
      "title": "MetaWorld: Skill Transfer and Composition in a Hierarchical World Model for Grounding High-Level Instructions",
      "url": "https://arxiv.org/abs/2601.17507",
      "date": "2026-01-24",
      "authors_text": "Yutong Shen, Hangxu Liu, Kailin Pei, Ruizhe Xia, Tongtong Feng",
      "is_highlight": false,
      "score": 92.0,
      "summary": "MetaWorld introduces a hierarchical world model that enhances humanoid robot loco-manipulation by integrating semantic planning and physical control through expert policy transfer, improving task performance and motion coherence.",
      "debug_abstract": "Humanoid robot loco-manipulation remains constrained by the semantic-physical gap. Current methods face three limitations: Low sample efficiency in reinforcement learning, poor generalization in imitation learning, and physical inconsistency in VLMs. We propose MetaWorld, a hierarchical world model that integrates semantic planning and physical control via expert policy transfer. The framework decouples tasks into a VLM-driven semantic layer and a latent dynamics model operating in a compact state space. Our dynamic expert selection and motion prior fusion mechanism leverages a pre-trained multi-expert policy library as transferable knowledge, enabling efficient online adaptation via a two-stage framework. VLMs serve as semantic interfaces, mapping instructions to executable skills and bypassing symbol grounding. Experiments on Humanoid-Bench show MetaWorld outperforms world model-based RL in task completion and motion coherence. Our code will be found at this https URL"
    },
    {
      "title": "The Flexibility Trap: Why Arbitrary Order Limits Reasoning Potential in Diffusion Language Models",
      "url": "https://arxiv.org/abs/2601.15165",
      "date": "2026-01-26",
      "authors_text": "Zanlin Ni, Shenzhi Wang, Yang Yue, Tianyu Yu, Weilin Zhao",
      "is_highlight": false,
      "score": 45.0,
      "summary": "This paper argues that arbitrary order generation in diffusion language models limits reasoning potential, advocating for a simpler approach using Group Relative Policy Optimization for improved performance.",
      "debug_abstract": "Diffusion Large Language Models (dLLMs) break the rigid left-to-right constraint of traditional LLMs, enabling token generation in arbitrary orders. Intuitively, this flexibility implies a solution space that strictly supersets the fixed autoregressive trajectory, theoretically unlocking superior reasoning potential for general tasks like mathematics and coding. Consequently, numerous works have leveraged reinforcement learning (RL) to elicit the reasoning capability of dLLMs. In this paper, we reveal a counter-intuitive reality: arbitrary order generation, in its current form, narrows rather than expands the reasoning boundary of dLLMs. We find that dLLMs tend to exploit this order flexibility to bypass high-uncertainty tokens that are crucial for exploration, leading to a premature collapse of the solution space. This observation motivates a rethink of RL approaches for dLLMs, where considerable complexities, such as handling combinatorial trajectories and intractable likelihoods, are often devoted to preserving this flexibility. We demonstrate that effective reasoning can be better elicited by intentionally forgoing arbitrary order and applying standard Group Relative Policy Optimization (GRPO) instead. Our approach, JustGRPO, is minimalist yet surprisingly effective (e.g., 89.1% accuracy on GSM8K) while fully retaining the parallel decoding ability of dLLMs. Project page: this https URL"
    },
    {
      "title": "HomoFM: Deep Homography Estimation with Flow Matching",
      "url": "https://arxiv.org/abs/2601.18222",
      "date": "2026-01-26",
      "authors_text": "Mengfan He, Liangzheng Sun, Chunyu Li, Ziyang Meng",
      "is_highlight": false,
      "score": 49.0,
      "summary": "HomoFM introduces a flow matching technique for deep homography estimation, enhancing accuracy and robustness against domain shifts through velocity field learning and domain-invariant representations.",
      "debug_abstract": "Deep homography estimation has broad applications in computer vision and robotics. Remarkable progresses have been achieved while the existing methods typically treat it as a direct regression or iterative refinement problem and often struggling to capture complex geometric transformations or generalize across different domains. In this work, we propose HomoFM, a new framework that introduces the flow matching technique from generative modeling into the homography estimation task for the first time. Unlike the existing methods, we formulate homography estimation problem as a velocity field learning problem. By modeling a continuous and point-wise velocity field that transforms noisy distributions into registered coordinates, the proposed network recovers high-precision transformations through a conditional flow trajectory. Furthermore, to address the challenge of domain shifts issue, e.g., the cases of multimodal matching or varying illumination scenarios, we integrate a gradient reversal layer (GRL) into the feature extraction backbone. This domain adaptation strategy explicitly constrains the encoder to learn domain-invariant representations, significantly enhancing the network&#39;s robustness. Extensive experiments demonstrate the effectiveness of the proposed method, showing that HomoFM outperforms state-of-the-art methods in both estimation accuracy and robustness on standard benchmarks. Code and data resource are available at this https URL."
    },
    {
      "title": "Dep-Search: Learning Dependency-Aware Reasoning Traces with Persistent Memory",
      "url": "https://arxiv.org/abs/2601.18771",
      "date": "2026-01-26",
      "authors_text": "Yanming Liu, Xinyue Peng, Zixuan Yan, Yanxin Shen, Wenjie Xu",
      "is_highlight": false,
      "score": 46.0,
      "summary": "Dep-Search enhances large language models' multi-hop reasoning by integrating dependency-aware search, structured retrieval, and persistent memory, outperforming existing frameworks in complex tasks.",
      "debug_abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in complex reasoning tasks, particularly when augmented with search mechanisms that enable systematic exploration of external knowledge bases. The field has evolved from traditional retrieval-augmented generation (RAG) frameworks to more sophisticated search-based frameworks that orchestrate multi-step reasoning through explicit search strategies. However, existing search frameworks still rely heavily on implicit natural language reasoning to determine search strategies and how to leverage retrieved information across reasoning steps. This reliance on implicit reasoning creates fundamental challenges for managing dependencies between sub-questions, efficiently reusing previously retrieved knowledge, and learning optimal search strategies through reinforcement learning. To address these limitations, we propose Dep-Search, a dependency-aware search framework that advances beyond existing search frameworks by integrating structured reasoning, retrieval, and persistent memory through GRPO. Dep-Search introduces explicit control mechanisms that enable the model to decompose questions with dependency relationships, retrieve information when needed, access previously stored knowledge from memory, and summarize long reasoning contexts into reusable memory entries. Through extensive experiments on seven diverse question answering datasets, we demonstrate that Dep-Search significantly enhances LLMs&#39; ability to tackle complex multi-hop reasoning tasks, achieving substantial improvements over strong baselines across different model scales."
    },
    {
      "title": "FantasyVLN: Unified Multimodal Chain-of-Thought Reasoning for Vision-Language Navigation",
      "url": "https://arxiv.org/abs/2601.13976",
      "date": "2026-01-23",
      "authors_text": "Jing Zuo, Lingzhou Mu, Fan Jiang, Chengcheng Ma, Mu Xu",
      "is_highlight": false,
      "score": 85.0,
      "summary": "FantasyVLN introduces a unified implicit reasoning framework for Vision-Language Navigation that enhances efficiency and success rates by optimizing Chain-of-Thought reasoning without excessive token overhead.",
      "debug_abstract": "Achieving human-level performance in Vision-and-Language Navigation (VLN) requires an embodied agent to jointly understand multimodal instructions and visual-spatial context while reasoning over long action sequences. Recent works, such as NavCoT and NavGPT-2, demonstrate the potential of Chain-of-Thought (CoT) reasoning for improving interpretability and long-horizon planning. Moreover, multimodal extensions like OctoNav-R1 and CoT-VLA further validate CoT as a promising pathway toward human-like navigation reasoning. However, existing approaches face critical drawbacks: purely textual CoTs lack spatial grounding and easily overfit to sparse annotated reasoning steps, while multimodal CoTs incur severe token inflation by generating imagined visual observations, making real-time navigation impractical. In this work, we propose FantasyVLN, a unified implicit reasoning framework that preserves the benefits of CoT reasoning without explicit token overhead. Specifically, imagined visual tokens are encoded into a compact latent space using a pretrained Visual AutoRegressor (VAR) during CoT reasoning training, and the model jointly learns from textual, visual, and multimodal CoT modes under a unified multi-CoT strategy. At inference, our model performs direct instruction-to-action mapping while still enjoying reasoning-aware representations. Extensive experiments on LH-VLN show that our approach achieves reasoning-aware yet real-time navigation, improving success rates and efficiency while reducing inference latency by an order of magnitude compared to explicit CoT methods."
    },
    {
      "title": "TangramPuzzle: Evaluating Multimodal Large Language Models with Compositional Spatial Reasoning",
      "url": "https://arxiv.org/abs/2601.16520",
      "date": "2026-01-23",
      "authors_text": "Daixian Liu, Jiayi Kuang, Yinghui Li, Yangning Li, Di Yin",
      "is_highlight": false,
      "score": 82.0,
      "summary": "The TangramPuzzle benchmark evaluates multimodal large language models' compositional spatial reasoning through precise geometric tasks, revealing their tendency to prioritize silhouette matching over geometric accuracy.",
      "debug_abstract": "Multimodal Large Language Models (MLLMs) have achieved remarkable progress in visual recognition and semantic understanding. Nevertheless, their ability to perform precise compositional spatial reasoning remains largely unexplored. Existing benchmarks often involve relatively simple tasks and rely on semantic approximations or coarse relative positioning, while their evaluation metrics are typically limited and lack rigorous mathematical formulations. To bridge this gap, we introduce TangramPuzzle, a geometry-grounded benchmark designed to evaluate compositional spatial reasoning through the lens of the classic Tangram game. We propose the Tangram Construction Expression (TCE), a symbolic geometric framework that grounds tangram assemblies in exact, machine-verifiable coordinate specifications, to mitigate the ambiguity of visual approximation. We design two complementary tasks: Outline Prediction, which demands inferring global shapes from local components, and End-to-End Code Generation, which requires solving inverse geometric assembly problems. We conduct extensive evaluation experiments on advanced open-source and proprietary models, revealing an interesting insight: MLLMs tend to prioritize matching the target silhouette while neglecting geometric constraints, leading to distortions or deformations of the pieces."
    },
    {
      "title": "AnchoredDream: Zero-Shot 360° Indoor Scene Generation from a Single View via Geometric Grounding",
      "url": "https://arxiv.org/abs/2601.16532",
      "date": "2026-01-23",
      "authors_text": "Runmao Yao, Junsheng Zhou, Zhen Dong, Yu-Shen Liu",
      "is_highlight": false,
      "score": 72.0,
      "summary": "AnchoredDream introduces a zero-shot method for generating 360° indoor scenes from a single image, enhancing appearance consistency and geometric plausibility through a mutual boosting mechanism.",
      "debug_abstract": "Single-view indoor scene generation plays a crucial role in a range of real-world applications. However, generating a complete 360° scene from a single image remains a highly ill-posed and challenging problem. Recent approaches have made progress by leveraging diffusion models and depth estimation networks, yet they still struggle to maintain appearance consistency and geometric plausibility under large viewpoint changes, limiting their effectiveness in full-scene generation. To address this, we propose AnchoredDream, a novel zero-shot pipeline that anchors 360° scene generation on high-fidelity geometry via an appearance-geometry mutual boosting mechanism. Given a single-view image, our method first performs appearance-guided geometry generation to construct a reliable 3D scene layout. Then, we progressively generate the complete scene through a series of modules: warp-and-inpaint, warp-and-refine, post-optimization, and a novel Grouting Block, which ensures seamless transitions between the input view and generated regions. Extensive experiments demonstrate that AnchoredDream outperforms existing methods by a large margin in both appearance consistency and geometric plausibility--all in a zero-shot manner. Our results highlight the potential of geometric grounding for high-quality, zero-shot single-view scene generation."
    },
    {
      "title": "OnlineSI: Taming Large Language Model for Online 3D Understanding and Grounding",
      "url": "https://arxiv.org/abs/2601.16538",
      "date": "2026-01-23",
      "authors_text": "Zixian Liu, Zhaoxi Chen, Liang Pan, Ziwei Liu",
      "is_highlight": false,
      "score": 88.0,
      "summary": "OnlineSI is a framework that enhances Multimodal Large Language Models' spatial understanding in dynamic environments by integrating continuous learning from video streams and 3D point cloud data.",
      "debug_abstract": "In recent years, researchers have increasingly been interested in how to enable Multimodal Large Language Models (MLLM) to possess spatial understanding and reasoning capabilities. However, most existing methods overlook the importance of the ability to continuously work in an ever-changing world, and lack the possibility of deployment on embodied systems in real-world environments. In this work, we introduce OnlineSI, a framework that can continuously improve its spatial understanding of its surroundings given a video stream. Our core idea is to maintain a finite spatial memory to retain past observations, ensuring the computation required for each inference does not increase as the input accumulates. We further integrate 3D point cloud information with semantic information, helping MLLM to better locate and identify objects in the scene. To evaluate our method, we introduce the Fuzzy $F_1$-Score to mitigate ambiguity, and test our method on two representative datasets. Experiments demonstrate the effectiveness of our method, paving the way towards real-world embodied systems."
    },
    {
      "title": "GRIP: Algorithm-Agnostic Machine Unlearning for Mixture-of-Experts via Geometric Router Constraints",
      "url": "https://arxiv.org/abs/2601.16905",
      "date": "2026-01-23",
      "authors_text": "Andy Zhu, Rongzhe Wei, Yupu Gu, Pan Li",
      "is_highlight": false,
      "score": 62.0,
      "summary": "The GRIP framework enables effective machine unlearning in Mixture-of-Experts models by imposing geometric constraints on router updates, ensuring knowledge erasure without sacrificing model utility.",
      "debug_abstract": "Machine unlearning (MU) for large language models has become critical for AI safety, yet existing methods fail to generalize to Mixture-of-Experts (MoE) architectures. We identify that traditional unlearning methods exploit MoE&#39;s architectural vulnerability: they manipulate routers to redirect queries away from knowledgeable experts rather than erasing knowledge, causing a loss of model utility and superficial forgetting. We propose Geometric Routing Invariance Preservation (GRIP), an algorithm-agnostic framework for unlearning for MoE. Our core contribution is a geometric constraint, implemented by projecting router gradient updates into an expert-specific null-space. Crucially, this decouples routing stability from parameter rigidity: while discrete expert selections remain stable for retained knowledge, the continuous router parameters remain plastic within the null space, allowing the model to undergo necessary internal reconfiguration to satisfy unlearning objectives. This forces the unlearning optimization to erase knowledge directly from expert parameters rather than exploiting the superficial router manipulation shortcut. GRIP functions as an adapter, constraining router parameter updates without modifying the underlying unlearning algorithm. Extensive experiments on large-scale MoE models demonstrate that our adapter eliminates expert selection shift (achieving over 95% routing stability) across all tested unlearning methods while preserving their utility. By preventing existing algorithms from exploiting MoE model&#39;s router vulnerability, GRIP adapts existing unlearning research from dense architectures to MoEs."
    },
    {
      "title": "Render-of-Thought: Rendering Textual Chain-of-Thought as Images for Visual Latent Reasoning",
      "url": "https://arxiv.org/abs/2601.14750",
      "date": "2026-01-22",
      "authors_text": "Yifan Wang, Shiyu Li, Peiming Li, Xiaochen Yang, Yang Tang",
      "is_highlight": false,
      "score": 92.5,
      "summary": "The Render-of-Thought framework transforms Chain-of-Thought reasoning into images, enhancing analyzability and efficiency while maintaining competitive performance in reasoning tasks.",
      "debug_abstract": "Chain-of-Thought (CoT) prompting has achieved remarkable success in unlocking the reasoning capabilities of Large Language Models (LLMs). Although CoT prompting enhances reasoning, its verbosity imposes substantial computational overhead. Recent works often focus exclusively on outcome alignment and lack supervision on the intermediate reasoning process. These deficiencies obscure the analyzability of the latent reasoning chain. To address these challenges, we introduce Render-of-Thought (RoT), the first framework to reify the reasoning chain by rendering textual steps into images, making the latent rationale explicit and traceable. Specifically, we leverage the vision encoders of existing Vision Language Models (VLMs) as semantic anchors to align the vision embeddings with the textual space. This design ensures plug-and-play implementation without incurring additional pre-training overhead. Extensive experiments on mathematical and logical reasoning benchmarks demonstrate that our method achieves 3-4x token compression and substantial inference acceleration compared to explicit CoT. Furthermore, it maintains competitive performance against other methods, validating the feasibility of this paradigm. Our code is available at this https URL"
    },
    {
      "title": "LLM-in-Sandbox Elicits General Agentic Intelligence",
      "url": "https://arxiv.org/abs/2601.16206",
      "date": "2026-01-22",
      "authors_text": "Daixuan Cheng, Shaohan Huang, Yuxian Gu, Huatong Song, Guoxin Chen",
      "is_highlight": false,
      "score": 95.0,
      "summary": "The LLM-in-Sandbox framework enables large language models to demonstrate general intelligence in non-code tasks through sandbox exploration and reinforcement learning, achieving robust generalization across various domains.",
      "debug_abstract": "We introduce LLM-in-Sandbox, enabling LLMs to explore within a code sandbox (i.e., a virtual computer), to elicit general intelligence in non-code domains. We first demonstrate that strong LLMs, without additional training, exhibit generalization capabilities to leverage the code sandbox for non-code tasks. For example, LLMs spontaneously access external resources to acquire new knowledge, leverage the file system to handle long contexts, and execute scripts to satisfy formatting requirements. We further show that these agentic capabilities can be enhanced through LLM-in-Sandbox Reinforcement Learning (LLM-in-Sandbox-RL), which uses only non-agentic data to train models for sandbox exploration. Experiments demonstrate that LLM-in-Sandbox, in both training-free and post-trained settings, achieves robust generalization spanning mathematics, physics, chemistry, biomedicine, long-context understanding, and instruction following. Finally, we analyze LLM-in-Sandbox&#39;s efficiency from computational and system perspectives, and open-source it as a Python package to facilitate real-world deployment."
    },
    {
      "title": "Collision-Free Humanoid Traversal in Cluttered Indoor Scenes",
      "url": "https://arxiv.org/abs/2601.16035",
      "date": "2026-01-22",
      "authors_text": "Han Xue, Sikai Liang, Zhikai Zhang, Zicheng Zeng, Yun Liu",
      "is_highlight": false,
      "score": 81.2,
      "summary": "The paper presents Humanoid Potential Field (HumanoidPF) for enabling collision-free traversal of humanoids in cluttered indoor environments, demonstrating effective RL-based learning and real-world applicability.",
      "debug_abstract": "We study the problem of collision-free humanoid traversal in cluttered indoor scenes, such as hurdling over objects scattered on the floor, crouching under low-hanging obstacles, or squeezing through narrow passages. To achieve this goal, the humanoid needs to map its perception of surrounding obstacles with diverse spatial layouts and geometries to the corresponding traversal skills. However, the lack of an effective representation that captures humanoid-obstacle relationships during collision avoidance makes directly learning such mappings difficult. We therefore propose Humanoid Potential Field (HumanoidPF), which encodes these relationships as collision-free motion directions, significantly facilitating RL-based traversal skill learning. We also find that HumanoidPF exhibits a surprisingly negligible sim-to-real gap as a perceptual representation. To further enable generalizable traversal skills through diverse and challenging cluttered indoor scenes, we further propose a hybrid scene generation method, incorporating crops of realistic 3D indoor scenes and procedurally synthesized obstacles. We successfully transfer our policy to the real world and develop a teleoperation system where users could command the humanoid to traverse in cluttered indoor scenes with just a single click. Extensive experiments are conducted in both simulation and the real world to validate the effectiveness of our method. Demos and code can be found in our website: this https URL."
    }
  ],
  "SU Lab": [
    {
      "title": "Learning Modal-Mixed Chain-of-Thought Reasoning with Latent Embeddings",
      "url": "https://arxiv.org/abs/2602.00574",
      "date": "2026-01-31",
      "authors_text": "Yifei Shao, Kun Zhou, Ziming Xu, Mohammad Atif Quamar, Shibo Hao",
      "is_highlight": false,
      "score": 65.0,
      "summary": "The paper introduces modal-mixed chain-of-thought reasoning, integrating textual and visual latent embeddings to enhance multimodal reasoning in vision-intensive tasks, outperforming traditional methods.",
      "teaser_image": "https://arxiv.org/html/2602.00574/x1.png",
      "debug_abstract": "We study how to extend chain-of-thought (CoT) beyond language to better handle multimodal reasoning. While CoT helps LLMs and VLMs articulate intermediate steps, its text-only form often fails on vision-intensive problems where key intermediate states are inherently visual. We introduce modal-mixed CoT, which interleaves textual tokens with compact visual sketches represented as latent embeddings. To bridge the modality gap without eroding the original knowledge and capability of the VLM, we use the VLM itself as an encoder and train the language backbone to reconstruct its own intermediate vision embeddings, to guarantee the semantic alignment of the visual latent space. We further attach a diffusion-based latent decoder, invoked by a special control token and conditioned on hidden states from the VLM. In this way, the diffusion head carries fine-grained perceptual details while the VLM specifies high-level intent, which cleanly disentangles roles and reduces the optimization pressure of the VLM. Training proceeds in two stages: supervised fine-tuning on traces that interleave text and latents with a joint next-token and latent-reconstruction objective, followed by reinforcement learning that teaches when to switch modalities and how to compose long reasoning chains. Extensive experiments across 11 diverse multimodal reasoning tasks, demonstrate that our method yields better performance than language-only and other CoT methods. Our code will be publicly released."
    },
    {
      "title": "Towards Agentic Intelligence for Materials Science",
      "url": "https://arxiv.org/abs/2602.00169",
      "date": "2026-01-29",
      "authors_text": "Huan Zhang, Yizhan Li, Wenhao Huang, Ziyu Hou, Yu Song",
      "is_highlight": false,
      "score": 12.0,
      "summary": "This paper advocates for developing agentic AI systems in materials science that autonomously plan, act, and learn to enhance discovery processes beyond traditional models.",
      "teaser_image": "https://arxiv.org/html/2602.00169/x1.png",
      "debug_abstract": "The convergence of artificial intelligence and materials science presents a transformative opportunity, but achieving true acceleration in discovery requires moving beyond task-isolated, fine-tuned models toward agentic systems that plan, act, and learn across the full discovery loop. This survey advances a unique pipeline-centric view that spans from corpus curation and pretraining, through domain adaptation and instruction tuning, to goal-conditioned agents interfacing with simulation and experimental platforms. Unlike prior reviews, we treat the entire process as an end-to-end system to be optimized for tangible discovery outcomes rather than proxy benchmarks. This perspective allows us to trace how upstream design choices-such as data curation and training objectives-can be aligned with downstream experimental success through effective credit assignment. To bridge communities and establish a shared frame of reference, we first present an integrated lens that aligns terminology, evaluation, and workflow stages across AI and materials science. We then analyze the field through two focused lenses: From the AI perspective, the survey details LLM strengths in pattern recognition, predictive analytics, and natural language processing for literature mining, materials characterization, and property prediction; from the materials science perspective, it highlights applications in materials design, process optimization, and the acceleration of computational workflows via integration with external tools (e.g., DFT, robotic labs). Finally, we contrast passive, reactive approaches with agentic design, cataloging current contributions while motivating systems that pursue long-horizon goals with autonomy, memory, and tool use. This survey charts a practical roadmap towards autonomous, safety-aware LLM agents aimed at discovering novel and useful materials."
    },
    {
      "title": "Llama-3.1-FoundationAI-SecurityLLM-Reasoning-8B Technical Report",
      "url": "https://arxiv.org/abs/2601.21051",
      "date": "2026-01-28",
      "authors_text": "Zhuoran Yang, Ed Li, Jianliang He, Aman Priyanshu, Baturay Saglam",
      "is_highlight": false,
      "score": 70.0,
      "summary": "Foundation-Sec-8B-Reasoning is an open-source cybersecurity reasoning model that excels in specialized tasks while retaining strong general capabilities through innovative training methods.",
      "teaser_image": "https://arxiv.org/html/2601.21051/x2.png",
      "debug_abstract": "We present Foundation-Sec-8B-Reasoning, the first open-source native reasoning model for cybersecurity. Built upon our previously released Foundation-Sec-8B base model (derived from Llama-3.1-8B-Base), the model is trained through a two-stage process combining supervised fine-tuning (SFT) and reinforcement learning from verifiable rewards (RLVR). Our training leverages proprietary reasoning data spanning cybersecurity analysis, instruction-following, and mathematical reasoning. Evaluation across 10 cybersecurity benchmarks and 10 general-purpose benchmarks demonstrates performance competitive with significantly larger models on cybersecurity tasks while maintaining strong general capabilities. The model shows effective generalization on multi-hop reasoning tasks and strong safety performance when deployed with appropriate system prompts and guardrails. This work demonstrates that domain-specialized reasoning models can achieve strong performance on specialized tasks while maintaining broad general capabilities. We release the model publicly at this https URL."
    },
    {
      "title": "DTP: A Simple yet Effective Distracting Token Pruning Framework for Vision-Language Action Models",
      "url": "https://arxiv.org/abs/2601.16065",
      "date": "2026-01-22",
      "authors_text": "Chenyang Li, Jieyuan Liu, Bin Li, Bo Gao, Yilin Yuan",
      "is_highlight": false,
      "score": 76.1,
      "summary": "The paper presents a Distracting Token Pruning framework that enhances Vision-Language Action models by eliminating irrelevant image tokens, improving task success rates without altering model architecture.",
      "debug_abstract": "Vision-Language Action (VLA) models have shown remarkable progress in robotic manipulation by leveraging the powerful perception abilities of Vision-Language Models (VLMs) to understand environments and directly output actions. However, by default, VLA models may overly attend to image tokens in the task-irrelevant region, which we describe as &#39;distracting tokens&#39;. This behavior can disturb the model from the generation of the desired action tokens in each step, affecting the success rate of tasks. In this paper, we introduce a simple yet effective plug-and-play Distracting Token Pruning (DTP) framework, which dynamically detects and prunes these distracting image tokens. By correcting the model&#39;s visual attention patterns, we aim to improve the task success rate, as well as exploring the performance upper boundaries of the model without altering its original architecture or adding additional inputs. Experiments on the SIMPLER Benchmark (Li et al., 2024) show that our method consistently achieving relative improvements in task success rates across different types of novel VLA models, demonstrating generalizability to transformer-based VLAs. Further analysis reveals a negative correlation between the task success rate and the amount of attentions in the task-irrelevant region for all models tested, highlighting a common phenomenon of VLA models that could guide future research. We also publish our code at: this https URL."
    }
  ],
  "Stanford AI Lab (SAIL)": [
    {
      "title": "Accelerating Structured Chain-of-Thought in Autonomous Vehicles",
      "url": "https://arxiv.org/abs/2602.02864",
      "date": "2026-02-02",
      "authors_text": "Yi Gu, Yan Wang, Yuxiao Chen, Yurong You, Wenjie Luo",
      "is_highlight": false,
      "score": 76.0,
      "summary": "FastDriveCoT accelerates Chain-of-Thought reasoning in autonomous vehicles by enabling parallel decoding of sub-tasks, achieving 3-4× speedup while maintaining decision-making improvements.",
      "teaser_image": "https://arxiv.org/html/2602.02864/x2.png",
      "debug_abstract": "Chain-of-Thought (CoT) reasoning enhances the decision-making capabilities of vision-language-action models in autonomous driving, but its autoregressive nature introduces significant inference latency, making it impractical for real-time applications. To address this, we introduce FastDriveCoT, a novel parallel decoding method that accelerates template-structured CoT. Our approach decomposes the reasoning process into a dependency graph of distinct sub-tasks, such as identifying critical objects and summarizing traffic rules, some of which can be generated in parallel. By generating multiple independent reasoning steps concurrently within a single forward pass, we significantly reduce the number of sequential computations. Experiments demonstrate a 3-4$\\times$ speedup in CoT generation and a substantial reduction in end-to-end latency across various model architectures, all while preserving the original downstream task improvements brought by incorporating CoT reasoning."
    },
    {
      "title": "Med3D-R1: Incentivizing Clinical Reasoning in 3D Medical Vision-Language Models for Abnormality Diagnosis",
      "url": "https://arxiv.org/abs/2602.01200",
      "date": "2026-02-01",
      "authors_text": "Haoran Lai, Zihang Jiang, Kun Zhang, Qingsong Yao, Rongsheng Wang",
      "is_highlight": false,
      "score": 54.0,
      "summary": "Med3D-R1 introduces a two-stage reinforcement learning framework that enhances 3D medical vision-language models for improved abnormality diagnosis and clinical reasoning.",
      "teaser_image": "https://arxiv.org/html/2602.01200/x2.png",
      "debug_abstract": "Developing 3D vision-language models with robust clinical reasoning remains a challenge due to the inherent complexity of volumetric medical imaging, the tendency of models to overfit superficial report patterns, and the lack of interpretability-aware reward designs. In this paper, we propose Med3D-R1, a reinforcement learning framework with a two-stage training process: Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL). During SFT stage, we introduce a residual alignment mechanism to bridge the gap between high-dimensional 3D features and textual embeddings, and an abnormality re-weighting strategy to emphasize clinically informative tokens and reduce structural bias in reports. In RL stage, we redesign the consistency reward to explicitly promote coherent, step-by-step diagnostic reasoning. We evaluate our method on medical multiple-choice visual question answering using two 3D diagnostic benchmarks, CT-RATE and RAD-ChestCT, where our model attains state-of-the-art accuracies of 41.92\\% on CT-RATE and 44.99\\% on RAD-ChestCT. These results indicate improved abnormality diagnosis and clinical reasoning and outperform prior methods on both benchmarks. Overall, our approach holds promise for enhancing real-world diagnostic workflows by enabling more reliable and transparent 3D medical vision-language systems."
    },
    {
      "title": "SQL-Trail: Multi-Turn Reinforcement Learning with Interleaved Feedback for Text-to-SQL",
      "url": "https://arxiv.org/abs/2601.17699",
      "date": "2026-01-25",
      "authors_text": "Harper Hua, Zhen Han, Zhengyuan Shen, Jeremy Lee, Patrick Guan",
      "is_highlight": false,
      "score": 30.0,
      "summary": "SQL-Trail introduces a multi-turn reinforcement learning framework for Text-to-SQL that enhances query generation through iterative feedback and adaptive exploration, achieving state-of-the-art performance.",
      "teaser_image": null,
      "debug_abstract": "While large language models (LLMs) have substantially improved Text-to-SQL generation, a pronounced gap remains between AI systems and human experts on challenging benchmarks such as BIRD-SQL. We argue this gap stems largely from the prevailing single-pass paradigm, which lacks the iterative reasoning, schema exploration, and error-correction behaviors that humans naturally employ. To address this limitation, we introduce SQL-Trail, a multi-turn reinforcement learning (RL) agentic framework for Text-to-SQL. Rather than producing a query in one shot, SQL-Trail interacts with the database environment and uses execution feedback to iteratively refine its predictions. Our approach centers on two key ideas: (i) an adaptive turn-budget allocation mechanism that scales the agent&#39;s interaction depth to match question difficulty, and (ii) a composite reward panel that jointly incentivizes SQL correctness and efficient exploration. Across benchmarks, SQL-Trail sets a new state of the art and delivers strong data efficiency--up to 18x higher than prior single-pass RL state-of-the-art methods. Notably, our 7B and 14B models outperform substantially larger proprietary systems by 5% on average, underscoring the effectiveness of interactive, agentic workflows for robust Text-to-SQL generation."
    },
    {
      "title": "Teaching LLMs to Ask: Self-Querying Category-Theoretic Planning for Under-Specified Reasoning",
      "url": "https://arxiv.org/abs/2601.20014",
      "date": "2026-01-27",
      "authors_text": "Shuhui Qu",
      "is_highlight": false,
      "score": 68.0,
      "summary": "The paper presents SQ-BCP, a self-querying planning method for large language models that effectively resolves unknown preconditions, reducing resource violations significantly while ensuring goal compatibility.",
      "teaser_image": "https://arxiv.org/html/2601.20014/123.png",
      "debug_abstract": "Inference-time planning with large language models frequently breaks under partial observability: when task-critical preconditions are not specified at query time, models tend to hallucinate missing facts or produce plans that violate hard constraints. We introduce \\textbf{Self-Querying Bidirectional Categorical Planning (SQ-BCP)}, which explicitly represents precondition status (\\texttt{Sat}/\\texttt{Viol}/\\texttt{Unk}) and resolves unknowns via (i) targeted self-queries to an oracle/user or (ii) \\emph{bridging} hypotheses that establish the missing condition through an additional action. SQ-BCP performs bidirectional search and invokes a pullback-based verifier as a categorical certificate of goal compatibility, while using distance-based scores only for ranking and pruning. We prove that when the verifier succeeds and hard constraints pass deterministic checks, accepted plans are compatible with goal requirements; under bounded branching and finite resolution depth, SQ-BCP finds an accepting plan when one exists. Across WikiHow and RecipeNLG tasks with withheld preconditions, SQ-BCP reduces resource-violation rates to \\textbf{14.9\\%} and \\textbf{5.8\\%} (vs.\\ \\textbf{26.0\\%} and \\textbf{15.7\\%} for the best baseline), while maintaining competitive reference quality."
    },
    {
      "title": "Fuzzy Categorical Planning: Autonomous Goal Satisfaction with Graded Semantic Constraints",
      "url": "https://arxiv.org/abs/2601.20021",
      "date": "2026-01-27",
      "authors_text": "Shuhui Qu",
      "is_highlight": false,
      "score": 77.0,
      "summary": "Fuzzy Category-theoretic Planning (FCP) enhances natural-language planning by incorporating graded semantic constraints, improving goal satisfaction and reducing violations in recipe substitution tasks.",
      "teaser_image": "https://arxiv.org/html/2601.20021/345.png",
      "debug_abstract": "Natural-language planning often involves vague predicates (e.g., suitable substitute, stable enough) whose satisfaction is inherently graded. Existing category-theoretic planners provide compositional structure and pullback-based hard-constraint verification, but treat applicability as crisp, forcing thresholding that collapses meaningful distinctions and cannot track quality degradation across multi-step plans. We propose Fuzzy Category-theoretic Planning (FCP), which annotates each action (morphism) with a degree in [0,1], composes plan quality via a t-norm Lukasiewicz, and retains crisp executability checks via pullback verification. FCP grounds graded applicability from language using an LLM with k-sample median aggregation and supports meeting-in-the-middle search using residuum-based backward requirements. We evaluate on (i) public PDDL3 preference/oversubscription benchmarks and (ii) RecipeNLG-Subs, a missing-substitute recipe-planning benchmark built from RecipeNLG with substitution candidates from Recipe1MSubs and FoodKG. FCP improves success and reduces hard-constraint violations on RecipeNLG-Subs compared to LLM-only and ReAct-style baselines, while remaining competitive with classical PDDL3 planners."
    },
    {
      "title": "Open-Vocabulary Functional 3D Human-Scene Interaction Generation",
      "url": "https://arxiv.org/abs/2601.20835",
      "date": "2026-01-28",
      "authors_text": "Jie Liu, Yu Sun, Alpar Cseke, Yao Feng, Nicolas Heron",
      "is_highlight": false,
      "score": 81.0,
      "summary": "The FunHSI framework generates functionally correct 3D human-scene interactions from open-vocabulary prompts by employing functionality-aware contact reasoning and vision-language models.",
      "teaser_image": "https://arxiv.org/html/2601.20835/x2.png",
      "debug_abstract": "Generating 3D humans that functionally interact with 3D scenes remains an open problem with applications in embodied AI, robotics, and interactive content creation. The key challenge involves reasoning about both the semantics of functional elements in 3D scenes and the 3D human poses required to achieve functionality-aware interaction. Unfortunately, existing methods typically lack explicit reasoning over object functionality and the corresponding human-scene contact, resulting in implausible or functionally incorrect interactions. In this work, we propose FunHSI, a training-free, functionality-driven framework that enables functionally correct human-scene interactions from open-vocabulary task prompts. Given a task prompt, FunHSI performs functionality-aware contact reasoning to identify functional scene elements, reconstruct their 3D geometry, and model high-level interactions via a contact graph. We then leverage vision-language models to synthesize a human performing the task in the image and estimate proposed 3D body and hand poses. Finally, the proposed 3D body configuration is refined via stage-wise optimization to ensure physical plausibility and functional correctness. In contrast to existing methods, FunHSI not only synthesizes more plausible general 3D interactions, such as &#34;sitting on a sofa&#39;&#39;, while supporting fine-grained functional human-scene interactions, e.g., &#34;increasing the room temperature&#39;&#39;. Extensive experiments demonstrate that FunHSI consistently generates functionally correct and physically plausible human-scene interactions across diverse indoor and outdoor scenes."
    },
    {
      "title": "DeFM: Learning Foundation Representations from Depth for Robotics",
      "url": "https://arxiv.org/abs/2601.18923",
      "date": "2026-01-26",
      "authors_text": "Manthan Patel, Jonas Frey, Mayank Mittal, Fan Yang, Alexander Hansson",
      "is_highlight": true,
      "score": 94.0,
      "summary": "DeFM is a self-supervised foundation model that learns robust geometric and semantic representations from depth images, achieving state-of-the-art performance in various robotic tasks and environments.",
      "teaser_image": "https://arxiv.org/html/2601.18923/x1.png",
      "debug_abstract": "Depth sensors are widely deployed across robotic platforms, and advances in fast, high-fidelity depth simulation have enabled robotic policies trained on depth observations to achieve robust sim-to-real transfer for a wide range of tasks. Despite this, representation learning for depth modality remains underexplored compared to RGB, where large-scale foundation models now define the state of the art. To address this gap, we present DeFM, a self-supervised foundation model trained entirely on depth images for robotic applications. Using a DINO-style self-distillation objective on a curated dataset of 60M depth images, DeFM learns geometric and semantic representations that generalize to diverse environments, tasks, and sensors. To retain metric awareness across multiple scales, we introduce a novel input normalization strategy. We further distill DeFM into compact models suitable for resource-constrained robotic systems. When evaluated on depth-based classification, segmentation, navigation, locomotion, and manipulation benchmarks, DeFM achieves state-of-the-art performance and demonstrates strong generalization from simulation to real-world environments. We release all our pretrained models, which can be adopted off-the-shelf for depth-based robotic learning without task-specific fine-tuning. Webpage: this https URL"
    },
    {
      "title": "Image2Garment: Simulation-ready Garment Generation from a Single Image",
      "url": "https://arxiv.org/abs/2601.09658",
      "date": "2026-01-25",
      "authors_text": "Selim Emir Can, Jan Ackermann, Kiyohiro Nakayama, Ruofan Liu, Tong Wu",
      "is_highlight": false,
      "score": 57.0,
      "summary": "The paper presents Image2Garment, a novel framework that generates simulation-ready garments from a single image by inferring material properties and fabric attributes without iterative optimization.",
      "debug_abstract": "Estimating physically accurate, simulation-ready garments from a single image is challenging due to the absence of image-to-physics datasets and the ill-posed nature of this problem. Prior methods either require multi-view capture and expensive differentiable simulation or predict only garment geometry without the material properties required for realistic simulation. We propose a feed-forward framework that sidesteps these limitations by first fine-tuning a vision-language model to infer material composition and fabric attributes from real images, and then training a lightweight predictor that maps these attributes to the corresponding physical fabric parameters using a small dataset of material-physics measurements. Our approach introduces two new datasets (FTAG and T2P) and delivers simulation-ready garments from a single image without iterative optimization. Experiments show that our estimator achieves superior accuracy in material composition estimation and fabric attribute prediction, and by passing them through our physics parameter estimator, we further achieve higher-fidelity simulations compared to state-of-the-art image-to-garment methods."
    },
    {
      "title": "PaperSearchQA: Learning to Search and Reason over Scientific Papers with RLVR",
      "url": "https://arxiv.org/abs/2601.18207",
      "date": "2026-01-26",
      "authors_text": "James Burgess, Jan N. Hansen, Duo Peng, Yuhui Zhang, Alejandro Lozano",
      "is_highlight": false,
      "score": 53.0,
      "summary": "The paper introduces PaperSearchQA, a reinforcement learning framework for training search agents to effectively reason and answer questions using a corpus of biomedical paper abstracts.",
      "debug_abstract": "Search agents are language models (LMs) that reason and search knowledge bases (or the web) to answer questions; recent methods supervise only the final answer accuracy using reinforcement learning with verifiable rewards (RLVR). Most RLVR search agents tackle general-domain QA, which limits their relevance to technical AI systems in science, engineering, and medicine. In this work we propose training agents to search and reason over scientific papers -- this tests technical question-answering, it is directly relevant to real scientists, and the capabilities will be crucial to future AI Scientist systems. Concretely, we release a search corpus of 16 million biomedical paper abstracts and construct a challenging factoid QA dataset called PaperSearchQA with 60k samples answerable from the corpus, along with benchmarks. We train search agents in this environment to outperform non-RL retrieval baselines; we also perform further quantitative analysis and observe interesting agent behaviors like planning, reasoning, and self-verification. Our corpus, datasets, and benchmarks are usable with the popular Search-R1 codebase for RLVR training and released on this https URL. Finally, our data creation methods are scalable and easily extendable to other scientific domains."
    },
    {
      "title": "Cosmos Policy: Fine-Tuning Video Models for Visuomotor Control and Planning",
      "url": "https://arxiv.org/abs/2601.16163",
      "date": "2026-01-22",
      "authors_text": "Moo Jin Kim, Yihuai Gao, Tsung-Yi Lin, Yen-Chen Lin, Yunhao Ge",
      "is_highlight": false,
      "score": 90.4,
      "summary": "Cosmos Policy efficiently fine-tunes a pretrained video model for robot action generation and planning, achieving state-of-the-art performance in various benchmarks without architectural changes.",
      "debug_abstract": "Recent video generation models demonstrate remarkable ability to capture complex physical interactions and scene evolution over time. To leverage their spatiotemporal priors, robotics works have adapted video models for policy learning but introduce complexity by requiring multiple stages of post-training and new architectural components for action generation. In this work, we introduce Cosmos Policy, a simple approach for adapting a large pretrained video model (Cosmos-Predict2) into an effective robot policy through a single stage of post-training on the robot demonstration data collected on the target platform, with no architectural modifications. Cosmos Policy learns to directly generate robot actions encoded as latent frames within the video model&#39;s latent diffusion process, harnessing the model&#39;s pretrained priors and core learning algorithm to capture complex action distributions. Additionally, Cosmos Policy generates future state images and values (expected cumulative rewards), which are similarly encoded as latent frames, enabling test-time planning of action trajectories with higher likelihood of success. In our evaluations, Cosmos Policy achieves state-of-the-art performance on the LIBERO and RoboCasa simulation benchmarks (98.5% and 67.1% average success rates, respectively) and the highest average score in challenging real-world bimanual manipulation tasks, outperforming strong diffusion policies trained from scratch, video model-based policies, and state-of-the-art vision-language-action models fine-tuned on the same robot demonstrations. Furthermore, given policy rollout data, Cosmos Policy can learn from experience to refine its world model and value function and leverage model-based planning to achieve even higher success rates in challenging tasks. We release code, models, and training data at this https URL"
    }
  ],
  "Robotics and Embodied Artificial Intelligence Lab (REAL)": [
    {
      "title": "Accelerating Structured Chain-of-Thought in Autonomous Vehicles",
      "url": "https://arxiv.org/abs/2602.02864",
      "date": "2026-02-02",
      "authors_text": "Yi Gu, Yan Wang, Yuxiao Chen, Yurong You, Wenjie Luo",
      "is_highlight": false,
      "score": 76.0,
      "summary": "FastDriveCoT accelerates Chain-of-Thought reasoning in autonomous vehicles by enabling parallel decoding of sub-tasks, achieving 3-4× speedup while maintaining decision-making improvements.",
      "teaser_image": "https://arxiv.org/html/2602.02864/x2.png",
      "debug_abstract": "Chain-of-Thought (CoT) reasoning enhances the decision-making capabilities of vision-language-action models in autonomous driving, but its autoregressive nature introduces significant inference latency, making it impractical for real-time applications. To address this, we introduce FastDriveCoT, a novel parallel decoding method that accelerates template-structured CoT. Our approach decomposes the reasoning process into a dependency graph of distinct sub-tasks, such as identifying critical objects and summarizing traffic rules, some of which can be generated in parallel. By generating multiple independent reasoning steps concurrently within a single forward pass, we significantly reduce the number of sequential computations. Experiments demonstrate a 3-4$\\times$ speedup in CoT generation and a substantial reduction in end-to-end latency across various model architectures, all while preserving the original downstream task improvements brought by incorporating CoT reasoning."
    },
    {
      "title": "Med3D-R1: Incentivizing Clinical Reasoning in 3D Medical Vision-Language Models for Abnormality Diagnosis",
      "url": "https://arxiv.org/abs/2602.01200",
      "date": "2026-02-01",
      "authors_text": "Haoran Lai, Zihang Jiang, Kun Zhang, Qingsong Yao, Rongsheng Wang",
      "is_highlight": false,
      "score": 54.0,
      "summary": "Med3D-R1 introduces a two-stage reinforcement learning framework that enhances 3D medical vision-language models for improved abnormality diagnosis and clinical reasoning.",
      "teaser_image": "https://arxiv.org/html/2602.01200/x2.png",
      "debug_abstract": "Developing 3D vision-language models with robust clinical reasoning remains a challenge due to the inherent complexity of volumetric medical imaging, the tendency of models to overfit superficial report patterns, and the lack of interpretability-aware reward designs. In this paper, we propose Med3D-R1, a reinforcement learning framework with a two-stage training process: Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL). During SFT stage, we introduce a residual alignment mechanism to bridge the gap between high-dimensional 3D features and textual embeddings, and an abnormality re-weighting strategy to emphasize clinically informative tokens and reduce structural bias in reports. In RL stage, we redesign the consistency reward to explicitly promote coherent, step-by-step diagnostic reasoning. We evaluate our method on medical multiple-choice visual question answering using two 3D diagnostic benchmarks, CT-RATE and RAD-ChestCT, where our model attains state-of-the-art accuracies of 41.92\\% on CT-RATE and 44.99\\% on RAD-ChestCT. These results indicate improved abnormality diagnosis and clinical reasoning and outperform prior methods on both benchmarks. Overall, our approach holds promise for enhancing real-world diagnostic workflows by enabling more reliable and transparent 3D medical vision-language systems."
    },
    {
      "title": "SQL-Trail: Multi-Turn Reinforcement Learning with Interleaved Feedback for Text-to-SQL",
      "url": "https://arxiv.org/abs/2601.17699",
      "date": "2026-01-25",
      "authors_text": "Harper Hua, Zhen Han, Zhengyuan Shen, Jeremy Lee, Patrick Guan",
      "is_highlight": false,
      "score": 30.0,
      "summary": "SQL-Trail introduces a multi-turn reinforcement learning framework for Text-to-SQL that enhances query generation through iterative feedback and adaptive exploration, achieving state-of-the-art performance.",
      "teaser_image": null,
      "debug_abstract": "While large language models (LLMs) have substantially improved Text-to-SQL generation, a pronounced gap remains between AI systems and human experts on challenging benchmarks such as BIRD-SQL. We argue this gap stems largely from the prevailing single-pass paradigm, which lacks the iterative reasoning, schema exploration, and error-correction behaviors that humans naturally employ. To address this limitation, we introduce SQL-Trail, a multi-turn reinforcement learning (RL) agentic framework for Text-to-SQL. Rather than producing a query in one shot, SQL-Trail interacts with the database environment and uses execution feedback to iteratively refine its predictions. Our approach centers on two key ideas: (i) an adaptive turn-budget allocation mechanism that scales the agent&#39;s interaction depth to match question difficulty, and (ii) a composite reward panel that jointly incentivizes SQL correctness and efficient exploration. Across benchmarks, SQL-Trail sets a new state of the art and delivers strong data efficiency--up to 18x higher than prior single-pass RL state-of-the-art methods. Notably, our 7B and 14B models outperform substantially larger proprietary systems by 5% on average, underscoring the effectiveness of interactive, agentic workflows for robust Text-to-SQL generation."
    },
    {
      "title": "Teaching LLMs to Ask: Self-Querying Category-Theoretic Planning for Under-Specified Reasoning",
      "url": "https://arxiv.org/abs/2601.20014",
      "date": "2026-01-27",
      "authors_text": "Shuhui Qu",
      "is_highlight": false,
      "score": 68.0,
      "summary": "The paper presents SQ-BCP, a self-querying planning method for large language models that effectively resolves unknown preconditions, reducing resource violations significantly while ensuring goal compatibility.",
      "teaser_image": "https://arxiv.org/html/2601.20014/123.png",
      "debug_abstract": "Inference-time planning with large language models frequently breaks under partial observability: when task-critical preconditions are not specified at query time, models tend to hallucinate missing facts or produce plans that violate hard constraints. We introduce \\textbf{Self-Querying Bidirectional Categorical Planning (SQ-BCP)}, which explicitly represents precondition status (\\texttt{Sat}/\\texttt{Viol}/\\texttt{Unk}) and resolves unknowns via (i) targeted self-queries to an oracle/user or (ii) \\emph{bridging} hypotheses that establish the missing condition through an additional action. SQ-BCP performs bidirectional search and invokes a pullback-based verifier as a categorical certificate of goal compatibility, while using distance-based scores only for ranking and pruning. We prove that when the verifier succeeds and hard constraints pass deterministic checks, accepted plans are compatible with goal requirements; under bounded branching and finite resolution depth, SQ-BCP finds an accepting plan when one exists. Across WikiHow and RecipeNLG tasks with withheld preconditions, SQ-BCP reduces resource-violation rates to \\textbf{14.9\\%} and \\textbf{5.8\\%} (vs.\\ \\textbf{26.0\\%} and \\textbf{15.7\\%} for the best baseline), while maintaining competitive reference quality."
    },
    {
      "title": "Fuzzy Categorical Planning: Autonomous Goal Satisfaction with Graded Semantic Constraints",
      "url": "https://arxiv.org/abs/2601.20021",
      "date": "2026-01-27",
      "authors_text": "Shuhui Qu",
      "is_highlight": false,
      "score": 77.0,
      "summary": "Fuzzy Category-theoretic Planning (FCP) enhances natural-language planning by incorporating graded semantic constraints, improving goal satisfaction and reducing violations in recipe substitution tasks.",
      "teaser_image": "https://arxiv.org/html/2601.20021/345.png",
      "debug_abstract": "Natural-language planning often involves vague predicates (e.g., suitable substitute, stable enough) whose satisfaction is inherently graded. Existing category-theoretic planners provide compositional structure and pullback-based hard-constraint verification, but treat applicability as crisp, forcing thresholding that collapses meaningful distinctions and cannot track quality degradation across multi-step plans. We propose Fuzzy Category-theoretic Planning (FCP), which annotates each action (morphism) with a degree in [0,1], composes plan quality via a t-norm Lukasiewicz, and retains crisp executability checks via pullback verification. FCP grounds graded applicability from language using an LLM with k-sample median aggregation and supports meeting-in-the-middle search using residuum-based backward requirements. We evaluate on (i) public PDDL3 preference/oversubscription benchmarks and (ii) RecipeNLG-Subs, a missing-substitute recipe-planning benchmark built from RecipeNLG with substitution candidates from Recipe1MSubs and FoodKG. FCP improves success and reduces hard-constraint violations on RecipeNLG-Subs compared to LLM-only and ReAct-style baselines, while remaining competitive with classical PDDL3 planners."
    },
    {
      "title": "Open-Vocabulary Functional 3D Human-Scene Interaction Generation",
      "url": "https://arxiv.org/abs/2601.20835",
      "date": "2026-01-28",
      "authors_text": "Jie Liu, Yu Sun, Alpar Cseke, Yao Feng, Nicolas Heron",
      "is_highlight": false,
      "score": 81.0,
      "summary": "The FunHSI framework generates functionally correct 3D human-scene interactions from open-vocabulary prompts by employing functionality-aware contact reasoning and vision-language models.",
      "teaser_image": "https://arxiv.org/html/2601.20835/x2.png",
      "debug_abstract": "Generating 3D humans that functionally interact with 3D scenes remains an open problem with applications in embodied AI, robotics, and interactive content creation. The key challenge involves reasoning about both the semantics of functional elements in 3D scenes and the 3D human poses required to achieve functionality-aware interaction. Unfortunately, existing methods typically lack explicit reasoning over object functionality and the corresponding human-scene contact, resulting in implausible or functionally incorrect interactions. In this work, we propose FunHSI, a training-free, functionality-driven framework that enables functionally correct human-scene interactions from open-vocabulary task prompts. Given a task prompt, FunHSI performs functionality-aware contact reasoning to identify functional scene elements, reconstruct their 3D geometry, and model high-level interactions via a contact graph. We then leverage vision-language models to synthesize a human performing the task in the image and estimate proposed 3D body and hand poses. Finally, the proposed 3D body configuration is refined via stage-wise optimization to ensure physical plausibility and functional correctness. In contrast to existing methods, FunHSI not only synthesizes more plausible general 3D interactions, such as &#34;sitting on a sofa&#39;&#39;, while supporting fine-grained functional human-scene interactions, e.g., &#34;increasing the room temperature&#39;&#39;. Extensive experiments demonstrate that FunHSI consistently generates functionally correct and physically plausible human-scene interactions across diverse indoor and outdoor scenes."
    },
    {
      "title": "DeFM: Learning Foundation Representations from Depth for Robotics",
      "url": "https://arxiv.org/abs/2601.18923",
      "date": "2026-01-26",
      "authors_text": "Manthan Patel, Jonas Frey, Mayank Mittal, Fan Yang, Alexander Hansson",
      "is_highlight": true,
      "score": 94.0,
      "summary": "DeFM is a self-supervised foundation model that learns robust geometric and semantic representations from depth images, achieving state-of-the-art performance in various robotic tasks and environments.",
      "teaser_image": "https://arxiv.org/html/2601.18923/x1.png",
      "debug_abstract": "Depth sensors are widely deployed across robotic platforms, and advances in fast, high-fidelity depth simulation have enabled robotic policies trained on depth observations to achieve robust sim-to-real transfer for a wide range of tasks. Despite this, representation learning for depth modality remains underexplored compared to RGB, where large-scale foundation models now define the state of the art. To address this gap, we present DeFM, a self-supervised foundation model trained entirely on depth images for robotic applications. Using a DINO-style self-distillation objective on a curated dataset of 60M depth images, DeFM learns geometric and semantic representations that generalize to diverse environments, tasks, and sensors. To retain metric awareness across multiple scales, we introduce a novel input normalization strategy. We further distill DeFM into compact models suitable for resource-constrained robotic systems. When evaluated on depth-based classification, segmentation, navigation, locomotion, and manipulation benchmarks, DeFM achieves state-of-the-art performance and demonstrates strong generalization from simulation to real-world environments. We release all our pretrained models, which can be adopted off-the-shelf for depth-based robotic learning without task-specific fine-tuning. Webpage: this https URL"
    },
    {
      "title": "Image2Garment: Simulation-ready Garment Generation from a Single Image",
      "url": "https://arxiv.org/abs/2601.09658",
      "date": "2026-01-25",
      "authors_text": "Selim Emir Can, Jan Ackermann, Kiyohiro Nakayama, Ruofan Liu, Tong Wu",
      "is_highlight": false,
      "score": 57.0,
      "summary": "The paper presents Image2Garment, a novel framework that generates simulation-ready garments from a single image by inferring material properties and fabric attributes without iterative optimization.",
      "debug_abstract": "Estimating physically accurate, simulation-ready garments from a single image is challenging due to the absence of image-to-physics datasets and the ill-posed nature of this problem. Prior methods either require multi-view capture and expensive differentiable simulation or predict only garment geometry without the material properties required for realistic simulation. We propose a feed-forward framework that sidesteps these limitations by first fine-tuning a vision-language model to infer material composition and fabric attributes from real images, and then training a lightweight predictor that maps these attributes to the corresponding physical fabric parameters using a small dataset of material-physics measurements. Our approach introduces two new datasets (FTAG and T2P) and delivers simulation-ready garments from a single image without iterative optimization. Experiments show that our estimator achieves superior accuracy in material composition estimation and fabric attribute prediction, and by passing them through our physics parameter estimator, we further achieve higher-fidelity simulations compared to state-of-the-art image-to-garment methods."
    },
    {
      "title": "PaperSearchQA: Learning to Search and Reason over Scientific Papers with RLVR",
      "url": "https://arxiv.org/abs/2601.18207",
      "date": "2026-01-26",
      "authors_text": "James Burgess, Jan N. Hansen, Duo Peng, Yuhui Zhang, Alejandro Lozano",
      "is_highlight": false,
      "score": 53.0,
      "summary": "The paper introduces PaperSearchQA, a reinforcement learning framework for training search agents to effectively reason and answer questions using a corpus of biomedical paper abstracts.",
      "debug_abstract": "Search agents are language models (LMs) that reason and search knowledge bases (or the web) to answer questions; recent methods supervise only the final answer accuracy using reinforcement learning with verifiable rewards (RLVR). Most RLVR search agents tackle general-domain QA, which limits their relevance to technical AI systems in science, engineering, and medicine. In this work we propose training agents to search and reason over scientific papers -- this tests technical question-answering, it is directly relevant to real scientists, and the capabilities will be crucial to future AI Scientist systems. Concretely, we release a search corpus of 16 million biomedical paper abstracts and construct a challenging factoid QA dataset called PaperSearchQA with 60k samples answerable from the corpus, along with benchmarks. We train search agents in this environment to outperform non-RL retrieval baselines; we also perform further quantitative analysis and observe interesting agent behaviors like planning, reasoning, and self-verification. Our corpus, datasets, and benchmarks are usable with the popular Search-R1 codebase for RLVR training and released on this https URL. Finally, our data creation methods are scalable and easily extendable to other scientific domains."
    },
    {
      "title": "Cosmos Policy: Fine-Tuning Video Models for Visuomotor Control and Planning",
      "url": "https://arxiv.org/abs/2601.16163",
      "date": "2026-01-22",
      "authors_text": "Moo Jin Kim, Yihuai Gao, Tsung-Yi Lin, Yen-Chen Lin, Yunhao Ge",
      "is_highlight": false,
      "score": 90.4,
      "summary": "Cosmos Policy efficiently fine-tunes a pretrained video model for robot action generation and planning, achieving state-of-the-art performance in various benchmarks without architectural changes.",
      "debug_abstract": "Recent video generation models demonstrate remarkable ability to capture complex physical interactions and scene evolution over time. To leverage their spatiotemporal priors, robotics works have adapted video models for policy learning but introduce complexity by requiring multiple stages of post-training and new architectural components for action generation. In this work, we introduce Cosmos Policy, a simple approach for adapting a large pretrained video model (Cosmos-Predict2) into an effective robot policy through a single stage of post-training on the robot demonstration data collected on the target platform, with no architectural modifications. Cosmos Policy learns to directly generate robot actions encoded as latent frames within the video model&#39;s latent diffusion process, harnessing the model&#39;s pretrained priors and core learning algorithm to capture complex action distributions. Additionally, Cosmos Policy generates future state images and values (expected cumulative rewards), which are similarly encoded as latent frames, enabling test-time planning of action trajectories with higher likelihood of success. In our evaluations, Cosmos Policy achieves state-of-the-art performance on the LIBERO and RoboCasa simulation benchmarks (98.5% and 67.1% average success rates, respectively) and the highest average score in challenging real-world bimanual manipulation tasks, outperforming strong diffusion policies trained from scratch, video model-based policies, and state-of-the-art vision-language-action models fine-tuned on the same robot demonstrations. Furthermore, given policy rollout data, Cosmos Policy can learn from experience to refine its world model and value function and leverage model-based planning to achieve even higher success rates in challenging tasks. We release code, models, and training data at this https URL"
    }
  ],
  "斯坦福-人工智能实验室 (SAIL)": [
    {
      "title": "Accelerating Structured Chain-of-Thought in Autonomous Vehicles",
      "url": "https://arxiv.org/abs/2602.02864",
      "date": "2026-02-02",
      "authors_text": "Yi Gu, Yan Wang, Yuxiao Chen, Yurong You, Wenjie Luo",
      "is_highlight": false,
      "score": 76.0,
      "summary": "FastDriveCoT accelerates Chain-of-Thought reasoning in autonomous vehicles by enabling parallel decoding of sub-tasks, achieving 3-4× speedup while maintaining decision-making improvements.",
      "teaser_image": "https://arxiv.org/html/2602.02864/x2.png",
      "debug_abstract": "Chain-of-Thought (CoT) reasoning enhances the decision-making capabilities of vision-language-action models in autonomous driving, but its autoregressive nature introduces significant inference latency, making it impractical for real-time applications. To address this, we introduce FastDriveCoT, a novel parallel decoding method that accelerates template-structured CoT. Our approach decomposes the reasoning process into a dependency graph of distinct sub-tasks, such as identifying critical objects and summarizing traffic rules, some of which can be generated in parallel. By generating multiple independent reasoning steps concurrently within a single forward pass, we significantly reduce the number of sequential computations. Experiments demonstrate a 3-4$\\times$ speedup in CoT generation and a substantial reduction in end-to-end latency across various model architectures, all while preserving the original downstream task improvements brought by incorporating CoT reasoning."
    },
    {
      "title": "Med3D-R1: Incentivizing Clinical Reasoning in 3D Medical Vision-Language Models for Abnormality Diagnosis",
      "url": "https://arxiv.org/abs/2602.01200",
      "date": "2026-02-01",
      "authors_text": "Haoran Lai, Zihang Jiang, Kun Zhang, Qingsong Yao, Rongsheng Wang",
      "is_highlight": false,
      "score": 54.0,
      "summary": "Med3D-R1 introduces a two-stage reinforcement learning framework that enhances 3D medical vision-language models for improved abnormality diagnosis and clinical reasoning.",
      "teaser_image": "https://arxiv.org/html/2602.01200/x2.png",
      "debug_abstract": "Developing 3D vision-language models with robust clinical reasoning remains a challenge due to the inherent complexity of volumetric medical imaging, the tendency of models to overfit superficial report patterns, and the lack of interpretability-aware reward designs. In this paper, we propose Med3D-R1, a reinforcement learning framework with a two-stage training process: Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL). During SFT stage, we introduce a residual alignment mechanism to bridge the gap between high-dimensional 3D features and textual embeddings, and an abnormality re-weighting strategy to emphasize clinically informative tokens and reduce structural bias in reports. In RL stage, we redesign the consistency reward to explicitly promote coherent, step-by-step diagnostic reasoning. We evaluate our method on medical multiple-choice visual question answering using two 3D diagnostic benchmarks, CT-RATE and RAD-ChestCT, where our model attains state-of-the-art accuracies of 41.92\\% on CT-RATE and 44.99\\% on RAD-ChestCT. These results indicate improved abnormality diagnosis and clinical reasoning and outperform prior methods on both benchmarks. Overall, our approach holds promise for enhancing real-world diagnostic workflows by enabling more reliable and transparent 3D medical vision-language systems."
    },
    {
      "title": "SQL-Trail: Multi-Turn Reinforcement Learning with Interleaved Feedback for Text-to-SQL",
      "url": "https://arxiv.org/abs/2601.17699",
      "date": "2026-01-25",
      "authors_text": "Harper Hua, Zhen Han, Zhengyuan Shen, Jeremy Lee, Patrick Guan",
      "is_highlight": false,
      "score": 30.0,
      "summary": "SQL-Trail introduces a multi-turn reinforcement learning framework for Text-to-SQL that enhances query generation through iterative feedback and adaptive exploration, achieving state-of-the-art performance.",
      "teaser_image": null,
      "debug_abstract": "While large language models (LLMs) have substantially improved Text-to-SQL generation, a pronounced gap remains between AI systems and human experts on challenging benchmarks such as BIRD-SQL. We argue this gap stems largely from the prevailing single-pass paradigm, which lacks the iterative reasoning, schema exploration, and error-correction behaviors that humans naturally employ. To address this limitation, we introduce SQL-Trail, a multi-turn reinforcement learning (RL) agentic framework for Text-to-SQL. Rather than producing a query in one shot, SQL-Trail interacts with the database environment and uses execution feedback to iteratively refine its predictions. Our approach centers on two key ideas: (i) an adaptive turn-budget allocation mechanism that scales the agent&#39;s interaction depth to match question difficulty, and (ii) a composite reward panel that jointly incentivizes SQL correctness and efficient exploration. Across benchmarks, SQL-Trail sets a new state of the art and delivers strong data efficiency--up to 18x higher than prior single-pass RL state-of-the-art methods. Notably, our 7B and 14B models outperform substantially larger proprietary systems by 5% on average, underscoring the effectiveness of interactive, agentic workflows for robust Text-to-SQL generation."
    },
    {
      "title": "Teaching LLMs to Ask: Self-Querying Category-Theoretic Planning for Under-Specified Reasoning",
      "url": "https://arxiv.org/abs/2601.20014",
      "date": "2026-01-27",
      "authors_text": "Shuhui Qu",
      "is_highlight": false,
      "score": 68.0,
      "summary": "The paper presents SQ-BCP, a self-querying planning method for large language models that effectively resolves unknown preconditions, reducing resource violations significantly while ensuring goal compatibility.",
      "teaser_image": "https://arxiv.org/html/2601.20014/123.png",
      "debug_abstract": "Inference-time planning with large language models frequently breaks under partial observability: when task-critical preconditions are not specified at query time, models tend to hallucinate missing facts or produce plans that violate hard constraints. We introduce \\textbf{Self-Querying Bidirectional Categorical Planning (SQ-BCP)}, which explicitly represents precondition status (\\texttt{Sat}/\\texttt{Viol}/\\texttt{Unk}) and resolves unknowns via (i) targeted self-queries to an oracle/user or (ii) \\emph{bridging} hypotheses that establish the missing condition through an additional action. SQ-BCP performs bidirectional search and invokes a pullback-based verifier as a categorical certificate of goal compatibility, while using distance-based scores only for ranking and pruning. We prove that when the verifier succeeds and hard constraints pass deterministic checks, accepted plans are compatible with goal requirements; under bounded branching and finite resolution depth, SQ-BCP finds an accepting plan when one exists. Across WikiHow and RecipeNLG tasks with withheld preconditions, SQ-BCP reduces resource-violation rates to \\textbf{14.9\\%} and \\textbf{5.8\\%} (vs.\\ \\textbf{26.0\\%} and \\textbf{15.7\\%} for the best baseline), while maintaining competitive reference quality."
    },
    {
      "title": "Fuzzy Categorical Planning: Autonomous Goal Satisfaction with Graded Semantic Constraints",
      "url": "https://arxiv.org/abs/2601.20021",
      "date": "2026-01-27",
      "authors_text": "Shuhui Qu",
      "is_highlight": false,
      "score": 77.0,
      "summary": "Fuzzy Category-theoretic Planning (FCP) enhances natural-language planning by incorporating graded semantic constraints, improving goal satisfaction and reducing violations in recipe substitution tasks.",
      "teaser_image": "https://arxiv.org/html/2601.20021/345.png",
      "debug_abstract": "Natural-language planning often involves vague predicates (e.g., suitable substitute, stable enough) whose satisfaction is inherently graded. Existing category-theoretic planners provide compositional structure and pullback-based hard-constraint verification, but treat applicability as crisp, forcing thresholding that collapses meaningful distinctions and cannot track quality degradation across multi-step plans. We propose Fuzzy Category-theoretic Planning (FCP), which annotates each action (morphism) with a degree in [0,1], composes plan quality via a t-norm Lukasiewicz, and retains crisp executability checks via pullback verification. FCP grounds graded applicability from language using an LLM with k-sample median aggregation and supports meeting-in-the-middle search using residuum-based backward requirements. We evaluate on (i) public PDDL3 preference/oversubscription benchmarks and (ii) RecipeNLG-Subs, a missing-substitute recipe-planning benchmark built from RecipeNLG with substitution candidates from Recipe1MSubs and FoodKG. FCP improves success and reduces hard-constraint violations on RecipeNLG-Subs compared to LLM-only and ReAct-style baselines, while remaining competitive with classical PDDL3 planners."
    },
    {
      "title": "Open-Vocabulary Functional 3D Human-Scene Interaction Generation",
      "url": "https://arxiv.org/abs/2601.20835",
      "date": "2026-01-28",
      "authors_text": "Jie Liu, Yu Sun, Alpar Cseke, Yao Feng, Nicolas Heron",
      "is_highlight": false,
      "score": 81.0,
      "summary": "The FunHSI framework generates functionally correct 3D human-scene interactions from open-vocabulary prompts by employing functionality-aware contact reasoning and vision-language models.",
      "teaser_image": "https://arxiv.org/html/2601.20835/x2.png",
      "debug_abstract": "Generating 3D humans that functionally interact with 3D scenes remains an open problem with applications in embodied AI, robotics, and interactive content creation. The key challenge involves reasoning about both the semantics of functional elements in 3D scenes and the 3D human poses required to achieve functionality-aware interaction. Unfortunately, existing methods typically lack explicit reasoning over object functionality and the corresponding human-scene contact, resulting in implausible or functionally incorrect interactions. In this work, we propose FunHSI, a training-free, functionality-driven framework that enables functionally correct human-scene interactions from open-vocabulary task prompts. Given a task prompt, FunHSI performs functionality-aware contact reasoning to identify functional scene elements, reconstruct their 3D geometry, and model high-level interactions via a contact graph. We then leverage vision-language models to synthesize a human performing the task in the image and estimate proposed 3D body and hand poses. Finally, the proposed 3D body configuration is refined via stage-wise optimization to ensure physical plausibility and functional correctness. In contrast to existing methods, FunHSI not only synthesizes more plausible general 3D interactions, such as &#34;sitting on a sofa&#39;&#39;, while supporting fine-grained functional human-scene interactions, e.g., &#34;increasing the room temperature&#39;&#39;. Extensive experiments demonstrate that FunHSI consistently generates functionally correct and physically plausible human-scene interactions across diverse indoor and outdoor scenes."
    },
    {
      "title": "DeFM: Learning Foundation Representations from Depth for Robotics",
      "url": "https://arxiv.org/abs/2601.18923",
      "date": "2026-01-26",
      "authors_text": "Manthan Patel, Jonas Frey, Mayank Mittal, Fan Yang, Alexander Hansson",
      "is_highlight": true,
      "score": 94.0,
      "summary": "DeFM is a self-supervised foundation model that learns robust geometric and semantic representations from depth images, achieving state-of-the-art performance in various robotic tasks and environments.",
      "teaser_image": "https://arxiv.org/html/2601.18923/x1.png",
      "debug_abstract": "Depth sensors are widely deployed across robotic platforms, and advances in fast, high-fidelity depth simulation have enabled robotic policies trained on depth observations to achieve robust sim-to-real transfer for a wide range of tasks. Despite this, representation learning for depth modality remains underexplored compared to RGB, where large-scale foundation models now define the state of the art. To address this gap, we present DeFM, a self-supervised foundation model trained entirely on depth images for robotic applications. Using a DINO-style self-distillation objective on a curated dataset of 60M depth images, DeFM learns geometric and semantic representations that generalize to diverse environments, tasks, and sensors. To retain metric awareness across multiple scales, we introduce a novel input normalization strategy. We further distill DeFM into compact models suitable for resource-constrained robotic systems. When evaluated on depth-based classification, segmentation, navigation, locomotion, and manipulation benchmarks, DeFM achieves state-of-the-art performance and demonstrates strong generalization from simulation to real-world environments. We release all our pretrained models, which can be adopted off-the-shelf for depth-based robotic learning without task-specific fine-tuning. Webpage: this https URL"
    },
    {
      "title": "Image2Garment: Simulation-ready Garment Generation from a Single Image",
      "url": "https://arxiv.org/abs/2601.09658",
      "date": "2026-01-25",
      "authors_text": "Selim Emir Can, Jan Ackermann, Kiyohiro Nakayama, Ruofan Liu, Tong Wu",
      "is_highlight": false,
      "score": 57.0,
      "summary": "The paper presents Image2Garment, a novel framework that generates simulation-ready garments from a single image by inferring material properties and fabric attributes without iterative optimization.",
      "debug_abstract": "Estimating physically accurate, simulation-ready garments from a single image is challenging due to the absence of image-to-physics datasets and the ill-posed nature of this problem. Prior methods either require multi-view capture and expensive differentiable simulation or predict only garment geometry without the material properties required for realistic simulation. We propose a feed-forward framework that sidesteps these limitations by first fine-tuning a vision-language model to infer material composition and fabric attributes from real images, and then training a lightweight predictor that maps these attributes to the corresponding physical fabric parameters using a small dataset of material-physics measurements. Our approach introduces two new datasets (FTAG and T2P) and delivers simulation-ready garments from a single image without iterative optimization. Experiments show that our estimator achieves superior accuracy in material composition estimation and fabric attribute prediction, and by passing them through our physics parameter estimator, we further achieve higher-fidelity simulations compared to state-of-the-art image-to-garment methods."
    },
    {
      "title": "PaperSearchQA: Learning to Search and Reason over Scientific Papers with RLVR",
      "url": "https://arxiv.org/abs/2601.18207",
      "date": "2026-01-26",
      "authors_text": "James Burgess, Jan N. Hansen, Duo Peng, Yuhui Zhang, Alejandro Lozano",
      "is_highlight": false,
      "score": 53.0,
      "summary": "The paper introduces PaperSearchQA, a reinforcement learning framework for training search agents to effectively reason and answer questions using a corpus of biomedical paper abstracts.",
      "debug_abstract": "Search agents are language models (LMs) that reason and search knowledge bases (or the web) to answer questions; recent methods supervise only the final answer accuracy using reinforcement learning with verifiable rewards (RLVR). Most RLVR search agents tackle general-domain QA, which limits their relevance to technical AI systems in science, engineering, and medicine. In this work we propose training agents to search and reason over scientific papers -- this tests technical question-answering, it is directly relevant to real scientists, and the capabilities will be crucial to future AI Scientist systems. Concretely, we release a search corpus of 16 million biomedical paper abstracts and construct a challenging factoid QA dataset called PaperSearchQA with 60k samples answerable from the corpus, along with benchmarks. We train search agents in this environment to outperform non-RL retrieval baselines; we also perform further quantitative analysis and observe interesting agent behaviors like planning, reasoning, and self-verification. Our corpus, datasets, and benchmarks are usable with the popular Search-R1 codebase for RLVR training and released on this https URL. Finally, our data creation methods are scalable and easily extendable to other scientific domains."
    },
    {
      "title": "Cosmos Policy: Fine-Tuning Video Models for Visuomotor Control and Planning",
      "url": "https://arxiv.org/abs/2601.16163",
      "date": "2026-01-22",
      "authors_text": "Moo Jin Kim, Yihuai Gao, Tsung-Yi Lin, Yen-Chen Lin, Yunhao Ge",
      "is_highlight": false,
      "score": 90.4,
      "summary": "Cosmos Policy efficiently fine-tunes a pretrained video model for robot action generation and planning, achieving state-of-the-art performance in various benchmarks without architectural changes.",
      "debug_abstract": "Recent video generation models demonstrate remarkable ability to capture complex physical interactions and scene evolution over time. To leverage their spatiotemporal priors, robotics works have adapted video models for policy learning but introduce complexity by requiring multiple stages of post-training and new architectural components for action generation. In this work, we introduce Cosmos Policy, a simple approach for adapting a large pretrained video model (Cosmos-Predict2) into an effective robot policy through a single stage of post-training on the robot demonstration data collected on the target platform, with no architectural modifications. Cosmos Policy learns to directly generate robot actions encoded as latent frames within the video model&#39;s latent diffusion process, harnessing the model&#39;s pretrained priors and core learning algorithm to capture complex action distributions. Additionally, Cosmos Policy generates future state images and values (expected cumulative rewards), which are similarly encoded as latent frames, enabling test-time planning of action trajectories with higher likelihood of success. In our evaluations, Cosmos Policy achieves state-of-the-art performance on the LIBERO and RoboCasa simulation benchmarks (98.5% and 67.1% average success rates, respectively) and the highest average score in challenging real-world bimanual manipulation tasks, outperforming strong diffusion policies trained from scratch, video model-based policies, and state-of-the-art vision-language-action models fine-tuned on the same robot demonstrations. Furthermore, given policy rollout data, Cosmos Policy can learn from experience to refine its world model and value function and leverage model-based planning to achieve even higher success rates in challenging tasks. We release code, models, and training data at this https URL"
    }
  ],
  "斯坦福-机器人与具身智能实验室 (REAL)": [
    {
      "title": "Accelerating Structured Chain-of-Thought in Autonomous Vehicles",
      "url": "https://arxiv.org/abs/2602.02864",
      "date": "2026-02-02",
      "authors_text": "Yi Gu, Yan Wang, Yuxiao Chen, Yurong You, Wenjie Luo",
      "is_highlight": false,
      "score": 76.0,
      "summary": "FastDriveCoT accelerates Chain-of-Thought reasoning in autonomous vehicles by enabling parallel decoding of sub-tasks, achieving 3-4× speedup while maintaining decision-making improvements.",
      "teaser_image": "https://arxiv.org/html/2602.02864/x2.png",
      "debug_abstract": "Chain-of-Thought (CoT) reasoning enhances the decision-making capabilities of vision-language-action models in autonomous driving, but its autoregressive nature introduces significant inference latency, making it impractical for real-time applications. To address this, we introduce FastDriveCoT, a novel parallel decoding method that accelerates template-structured CoT. Our approach decomposes the reasoning process into a dependency graph of distinct sub-tasks, such as identifying critical objects and summarizing traffic rules, some of which can be generated in parallel. By generating multiple independent reasoning steps concurrently within a single forward pass, we significantly reduce the number of sequential computations. Experiments demonstrate a 3-4$\\times$ speedup in CoT generation and a substantial reduction in end-to-end latency across various model architectures, all while preserving the original downstream task improvements brought by incorporating CoT reasoning."
    },
    {
      "title": "Med3D-R1: Incentivizing Clinical Reasoning in 3D Medical Vision-Language Models for Abnormality Diagnosis",
      "url": "https://arxiv.org/abs/2602.01200",
      "date": "2026-02-01",
      "authors_text": "Haoran Lai, Zihang Jiang, Kun Zhang, Qingsong Yao, Rongsheng Wang",
      "is_highlight": false,
      "score": 54.0,
      "summary": "Med3D-R1 introduces a two-stage reinforcement learning framework that enhances 3D medical vision-language models for improved abnormality diagnosis and clinical reasoning.",
      "teaser_image": "https://arxiv.org/html/2602.01200/x2.png",
      "debug_abstract": "Developing 3D vision-language models with robust clinical reasoning remains a challenge due to the inherent complexity of volumetric medical imaging, the tendency of models to overfit superficial report patterns, and the lack of interpretability-aware reward designs. In this paper, we propose Med3D-R1, a reinforcement learning framework with a two-stage training process: Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL). During SFT stage, we introduce a residual alignment mechanism to bridge the gap between high-dimensional 3D features and textual embeddings, and an abnormality re-weighting strategy to emphasize clinically informative tokens and reduce structural bias in reports. In RL stage, we redesign the consistency reward to explicitly promote coherent, step-by-step diagnostic reasoning. We evaluate our method on medical multiple-choice visual question answering using two 3D diagnostic benchmarks, CT-RATE and RAD-ChestCT, where our model attains state-of-the-art accuracies of 41.92\\% on CT-RATE and 44.99\\% on RAD-ChestCT. These results indicate improved abnormality diagnosis and clinical reasoning and outperform prior methods on both benchmarks. Overall, our approach holds promise for enhancing real-world diagnostic workflows by enabling more reliable and transparent 3D medical vision-language systems."
    },
    {
      "title": "SQL-Trail: Multi-Turn Reinforcement Learning with Interleaved Feedback for Text-to-SQL",
      "url": "https://arxiv.org/abs/2601.17699",
      "date": "2026-01-25",
      "authors_text": "Harper Hua, Zhen Han, Zhengyuan Shen, Jeremy Lee, Patrick Guan",
      "is_highlight": false,
      "score": 30.0,
      "summary": "SQL-Trail introduces a multi-turn reinforcement learning framework for Text-to-SQL that enhances query generation through iterative feedback and adaptive exploration, achieving state-of-the-art performance.",
      "teaser_image": null,
      "debug_abstract": "While large language models (LLMs) have substantially improved Text-to-SQL generation, a pronounced gap remains between AI systems and human experts on challenging benchmarks such as BIRD-SQL. We argue this gap stems largely from the prevailing single-pass paradigm, which lacks the iterative reasoning, schema exploration, and error-correction behaviors that humans naturally employ. To address this limitation, we introduce SQL-Trail, a multi-turn reinforcement learning (RL) agentic framework for Text-to-SQL. Rather than producing a query in one shot, SQL-Trail interacts with the database environment and uses execution feedback to iteratively refine its predictions. Our approach centers on two key ideas: (i) an adaptive turn-budget allocation mechanism that scales the agent&#39;s interaction depth to match question difficulty, and (ii) a composite reward panel that jointly incentivizes SQL correctness and efficient exploration. Across benchmarks, SQL-Trail sets a new state of the art and delivers strong data efficiency--up to 18x higher than prior single-pass RL state-of-the-art methods. Notably, our 7B and 14B models outperform substantially larger proprietary systems by 5% on average, underscoring the effectiveness of interactive, agentic workflows for robust Text-to-SQL generation."
    },
    {
      "title": "Teaching LLMs to Ask: Self-Querying Category-Theoretic Planning for Under-Specified Reasoning",
      "url": "https://arxiv.org/abs/2601.20014",
      "date": "2026-01-27",
      "authors_text": "Shuhui Qu",
      "is_highlight": false,
      "score": 68.0,
      "summary": "The paper presents SQ-BCP, a self-querying planning method for large language models that effectively resolves unknown preconditions, reducing resource violations significantly while ensuring goal compatibility.",
      "teaser_image": "https://arxiv.org/html/2601.20014/123.png",
      "debug_abstract": "Inference-time planning with large language models frequently breaks under partial observability: when task-critical preconditions are not specified at query time, models tend to hallucinate missing facts or produce plans that violate hard constraints. We introduce \\textbf{Self-Querying Bidirectional Categorical Planning (SQ-BCP)}, which explicitly represents precondition status (\\texttt{Sat}/\\texttt{Viol}/\\texttt{Unk}) and resolves unknowns via (i) targeted self-queries to an oracle/user or (ii) \\emph{bridging} hypotheses that establish the missing condition through an additional action. SQ-BCP performs bidirectional search and invokes a pullback-based verifier as a categorical certificate of goal compatibility, while using distance-based scores only for ranking and pruning. We prove that when the verifier succeeds and hard constraints pass deterministic checks, accepted plans are compatible with goal requirements; under bounded branching and finite resolution depth, SQ-BCP finds an accepting plan when one exists. Across WikiHow and RecipeNLG tasks with withheld preconditions, SQ-BCP reduces resource-violation rates to \\textbf{14.9\\%} and \\textbf{5.8\\%} (vs.\\ \\textbf{26.0\\%} and \\textbf{15.7\\%} for the best baseline), while maintaining competitive reference quality."
    },
    {
      "title": "Fuzzy Categorical Planning: Autonomous Goal Satisfaction with Graded Semantic Constraints",
      "url": "https://arxiv.org/abs/2601.20021",
      "date": "2026-01-27",
      "authors_text": "Shuhui Qu",
      "is_highlight": false,
      "score": 77.0,
      "summary": "Fuzzy Category-theoretic Planning (FCP) enhances natural-language planning by incorporating graded semantic constraints, improving goal satisfaction and reducing violations in recipe substitution tasks.",
      "teaser_image": "https://arxiv.org/html/2601.20021/345.png",
      "debug_abstract": "Natural-language planning often involves vague predicates (e.g., suitable substitute, stable enough) whose satisfaction is inherently graded. Existing category-theoretic planners provide compositional structure and pullback-based hard-constraint verification, but treat applicability as crisp, forcing thresholding that collapses meaningful distinctions and cannot track quality degradation across multi-step plans. We propose Fuzzy Category-theoretic Planning (FCP), which annotates each action (morphism) with a degree in [0,1], composes plan quality via a t-norm Lukasiewicz, and retains crisp executability checks via pullback verification. FCP grounds graded applicability from language using an LLM with k-sample median aggregation and supports meeting-in-the-middle search using residuum-based backward requirements. We evaluate on (i) public PDDL3 preference/oversubscription benchmarks and (ii) RecipeNLG-Subs, a missing-substitute recipe-planning benchmark built from RecipeNLG with substitution candidates from Recipe1MSubs and FoodKG. FCP improves success and reduces hard-constraint violations on RecipeNLG-Subs compared to LLM-only and ReAct-style baselines, while remaining competitive with classical PDDL3 planners."
    },
    {
      "title": "Open-Vocabulary Functional 3D Human-Scene Interaction Generation",
      "url": "https://arxiv.org/abs/2601.20835",
      "date": "2026-01-28",
      "authors_text": "Jie Liu, Yu Sun, Alpar Cseke, Yao Feng, Nicolas Heron",
      "is_highlight": false,
      "score": 81.0,
      "summary": "The FunHSI framework generates functionally correct 3D human-scene interactions from open-vocabulary prompts by employing functionality-aware contact reasoning and vision-language models.",
      "teaser_image": "https://arxiv.org/html/2601.20835/x2.png",
      "debug_abstract": "Generating 3D humans that functionally interact with 3D scenes remains an open problem with applications in embodied AI, robotics, and interactive content creation. The key challenge involves reasoning about both the semantics of functional elements in 3D scenes and the 3D human poses required to achieve functionality-aware interaction. Unfortunately, existing methods typically lack explicit reasoning over object functionality and the corresponding human-scene contact, resulting in implausible or functionally incorrect interactions. In this work, we propose FunHSI, a training-free, functionality-driven framework that enables functionally correct human-scene interactions from open-vocabulary task prompts. Given a task prompt, FunHSI performs functionality-aware contact reasoning to identify functional scene elements, reconstruct their 3D geometry, and model high-level interactions via a contact graph. We then leverage vision-language models to synthesize a human performing the task in the image and estimate proposed 3D body and hand poses. Finally, the proposed 3D body configuration is refined via stage-wise optimization to ensure physical plausibility and functional correctness. In contrast to existing methods, FunHSI not only synthesizes more plausible general 3D interactions, such as &#34;sitting on a sofa&#39;&#39;, while supporting fine-grained functional human-scene interactions, e.g., &#34;increasing the room temperature&#39;&#39;. Extensive experiments demonstrate that FunHSI consistently generates functionally correct and physically plausible human-scene interactions across diverse indoor and outdoor scenes."
    },
    {
      "title": "DeFM: Learning Foundation Representations from Depth for Robotics",
      "url": "https://arxiv.org/abs/2601.18923",
      "date": "2026-01-26",
      "authors_text": "Manthan Patel, Jonas Frey, Mayank Mittal, Fan Yang, Alexander Hansson",
      "is_highlight": true,
      "score": 94.0,
      "summary": "DeFM is a self-supervised foundation model that learns robust geometric and semantic representations from depth images, achieving state-of-the-art performance in various robotic tasks and environments.",
      "teaser_image": "https://arxiv.org/html/2601.18923/x1.png",
      "debug_abstract": "Depth sensors are widely deployed across robotic platforms, and advances in fast, high-fidelity depth simulation have enabled robotic policies trained on depth observations to achieve robust sim-to-real transfer for a wide range of tasks. Despite this, representation learning for depth modality remains underexplored compared to RGB, where large-scale foundation models now define the state of the art. To address this gap, we present DeFM, a self-supervised foundation model trained entirely on depth images for robotic applications. Using a DINO-style self-distillation objective on a curated dataset of 60M depth images, DeFM learns geometric and semantic representations that generalize to diverse environments, tasks, and sensors. To retain metric awareness across multiple scales, we introduce a novel input normalization strategy. We further distill DeFM into compact models suitable for resource-constrained robotic systems. When evaluated on depth-based classification, segmentation, navigation, locomotion, and manipulation benchmarks, DeFM achieves state-of-the-art performance and demonstrates strong generalization from simulation to real-world environments. We release all our pretrained models, which can be adopted off-the-shelf for depth-based robotic learning without task-specific fine-tuning. Webpage: this https URL"
    },
    {
      "title": "Image2Garment: Simulation-ready Garment Generation from a Single Image",
      "url": "https://arxiv.org/abs/2601.09658",
      "date": "2026-01-25",
      "authors_text": "Selim Emir Can, Jan Ackermann, Kiyohiro Nakayama, Ruofan Liu, Tong Wu",
      "is_highlight": false,
      "score": 57.0,
      "summary": "The paper presents Image2Garment, a novel framework that generates simulation-ready garments from a single image by inferring material properties and fabric attributes without iterative optimization.",
      "debug_abstract": "Estimating physically accurate, simulation-ready garments from a single image is challenging due to the absence of image-to-physics datasets and the ill-posed nature of this problem. Prior methods either require multi-view capture and expensive differentiable simulation or predict only garment geometry without the material properties required for realistic simulation. We propose a feed-forward framework that sidesteps these limitations by first fine-tuning a vision-language model to infer material composition and fabric attributes from real images, and then training a lightweight predictor that maps these attributes to the corresponding physical fabric parameters using a small dataset of material-physics measurements. Our approach introduces two new datasets (FTAG and T2P) and delivers simulation-ready garments from a single image without iterative optimization. Experiments show that our estimator achieves superior accuracy in material composition estimation and fabric attribute prediction, and by passing them through our physics parameter estimator, we further achieve higher-fidelity simulations compared to state-of-the-art image-to-garment methods."
    },
    {
      "title": "PaperSearchQA: Learning to Search and Reason over Scientific Papers with RLVR",
      "url": "https://arxiv.org/abs/2601.18207",
      "date": "2026-01-26",
      "authors_text": "James Burgess, Jan N. Hansen, Duo Peng, Yuhui Zhang, Alejandro Lozano",
      "is_highlight": false,
      "score": 53.0,
      "summary": "The paper introduces PaperSearchQA, a reinforcement learning framework for training search agents to effectively reason and answer questions using a corpus of biomedical paper abstracts.",
      "debug_abstract": "Search agents are language models (LMs) that reason and search knowledge bases (or the web) to answer questions; recent methods supervise only the final answer accuracy using reinforcement learning with verifiable rewards (RLVR). Most RLVR search agents tackle general-domain QA, which limits their relevance to technical AI systems in science, engineering, and medicine. In this work we propose training agents to search and reason over scientific papers -- this tests technical question-answering, it is directly relevant to real scientists, and the capabilities will be crucial to future AI Scientist systems. Concretely, we release a search corpus of 16 million biomedical paper abstracts and construct a challenging factoid QA dataset called PaperSearchQA with 60k samples answerable from the corpus, along with benchmarks. We train search agents in this environment to outperform non-RL retrieval baselines; we also perform further quantitative analysis and observe interesting agent behaviors like planning, reasoning, and self-verification. Our corpus, datasets, and benchmarks are usable with the popular Search-R1 codebase for RLVR training and released on this https URL. Finally, our data creation methods are scalable and easily extendable to other scientific domains."
    },
    {
      "title": "Cosmos Policy: Fine-Tuning Video Models for Visuomotor Control and Planning",
      "url": "https://arxiv.org/abs/2601.16163",
      "date": "2026-01-22",
      "authors_text": "Moo Jin Kim, Yihuai Gao, Tsung-Yi Lin, Yen-Chen Lin, Yunhao Ge",
      "is_highlight": false,
      "score": 90.4,
      "summary": "Cosmos Policy efficiently fine-tunes a pretrained video model for robot action generation and planning, achieving state-of-the-art performance in various benchmarks without architectural changes.",
      "debug_abstract": "Recent video generation models demonstrate remarkable ability to capture complex physical interactions and scene evolution over time. To leverage their spatiotemporal priors, robotics works have adapted video models for policy learning but introduce complexity by requiring multiple stages of post-training and new architectural components for action generation. In this work, we introduce Cosmos Policy, a simple approach for adapting a large pretrained video model (Cosmos-Predict2) into an effective robot policy through a single stage of post-training on the robot demonstration data collected on the target platform, with no architectural modifications. Cosmos Policy learns to directly generate robot actions encoded as latent frames within the video model&#39;s latent diffusion process, harnessing the model&#39;s pretrained priors and core learning algorithm to capture complex action distributions. Additionally, Cosmos Policy generates future state images and values (expected cumulative rewards), which are similarly encoded as latent frames, enabling test-time planning of action trajectories with higher likelihood of success. In our evaluations, Cosmos Policy achieves state-of-the-art performance on the LIBERO and RoboCasa simulation benchmarks (98.5% and 67.1% average success rates, respectively) and the highest average score in challenging real-world bimanual manipulation tasks, outperforming strong diffusion policies trained from scratch, video model-based policies, and state-of-the-art vision-language-action models fine-tuned on the same robot demonstrations. Furthermore, given policy rollout data, Cosmos Policy can learn from experience to refine its world model and value function and leverage model-based planning to achieve even higher success rates in challenging tasks. We release code, models, and training data at this https URL"
    }
  ],
  "斯坦福-视觉与学习实验室 (SVL)": [
    {
      "title": "Accelerating Structured Chain-of-Thought in Autonomous Vehicles",
      "url": "https://arxiv.org/abs/2602.02864",
      "date": "2026-02-02",
      "authors_text": "Yi Gu, Yan Wang, Yuxiao Chen, Yurong You, Wenjie Luo",
      "is_highlight": false,
      "score": 76.0,
      "summary": "FastDriveCoT accelerates Chain-of-Thought reasoning in autonomous vehicles by enabling parallel decoding of sub-tasks, achieving 3-4× speedup while maintaining decision-making improvements.",
      "teaser_image": "https://arxiv.org/html/2602.02864/x2.png",
      "debug_abstract": "Chain-of-Thought (CoT) reasoning enhances the decision-making capabilities of vision-language-action models in autonomous driving, but its autoregressive nature introduces significant inference latency, making it impractical for real-time applications. To address this, we introduce FastDriveCoT, a novel parallel decoding method that accelerates template-structured CoT. Our approach decomposes the reasoning process into a dependency graph of distinct sub-tasks, such as identifying critical objects and summarizing traffic rules, some of which can be generated in parallel. By generating multiple independent reasoning steps concurrently within a single forward pass, we significantly reduce the number of sequential computations. Experiments demonstrate a 3-4$\\times$ speedup in CoT generation and a substantial reduction in end-to-end latency across various model architectures, all while preserving the original downstream task improvements brought by incorporating CoT reasoning."
    },
    {
      "title": "Med3D-R1: Incentivizing Clinical Reasoning in 3D Medical Vision-Language Models for Abnormality Diagnosis",
      "url": "https://arxiv.org/abs/2602.01200",
      "date": "2026-02-01",
      "authors_text": "Haoran Lai, Zihang Jiang, Kun Zhang, Qingsong Yao, Rongsheng Wang",
      "is_highlight": false,
      "score": 54.0,
      "summary": "Med3D-R1 introduces a two-stage reinforcement learning framework that enhances 3D medical vision-language models for improved abnormality diagnosis and clinical reasoning.",
      "teaser_image": "https://arxiv.org/html/2602.01200/x2.png",
      "debug_abstract": "Developing 3D vision-language models with robust clinical reasoning remains a challenge due to the inherent complexity of volumetric medical imaging, the tendency of models to overfit superficial report patterns, and the lack of interpretability-aware reward designs. In this paper, we propose Med3D-R1, a reinforcement learning framework with a two-stage training process: Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL). During SFT stage, we introduce a residual alignment mechanism to bridge the gap between high-dimensional 3D features and textual embeddings, and an abnormality re-weighting strategy to emphasize clinically informative tokens and reduce structural bias in reports. In RL stage, we redesign the consistency reward to explicitly promote coherent, step-by-step diagnostic reasoning. We evaluate our method on medical multiple-choice visual question answering using two 3D diagnostic benchmarks, CT-RATE and RAD-ChestCT, where our model attains state-of-the-art accuracies of 41.92\\% on CT-RATE and 44.99\\% on RAD-ChestCT. These results indicate improved abnormality diagnosis and clinical reasoning and outperform prior methods on both benchmarks. Overall, our approach holds promise for enhancing real-world diagnostic workflows by enabling more reliable and transparent 3D medical vision-language systems."
    },
    {
      "title": "SQL-Trail: Multi-Turn Reinforcement Learning with Interleaved Feedback for Text-to-SQL",
      "url": "https://arxiv.org/abs/2601.17699",
      "date": "2026-01-25",
      "authors_text": "Harper Hua, Zhen Han, Zhengyuan Shen, Jeremy Lee, Patrick Guan",
      "is_highlight": false,
      "score": 30.0,
      "summary": "SQL-Trail introduces a multi-turn reinforcement learning framework for Text-to-SQL that enhances query generation through iterative feedback and adaptive exploration, achieving state-of-the-art performance.",
      "teaser_image": null,
      "debug_abstract": "While large language models (LLMs) have substantially improved Text-to-SQL generation, a pronounced gap remains between AI systems and human experts on challenging benchmarks such as BIRD-SQL. We argue this gap stems largely from the prevailing single-pass paradigm, which lacks the iterative reasoning, schema exploration, and error-correction behaviors that humans naturally employ. To address this limitation, we introduce SQL-Trail, a multi-turn reinforcement learning (RL) agentic framework for Text-to-SQL. Rather than producing a query in one shot, SQL-Trail interacts with the database environment and uses execution feedback to iteratively refine its predictions. Our approach centers on two key ideas: (i) an adaptive turn-budget allocation mechanism that scales the agent&#39;s interaction depth to match question difficulty, and (ii) a composite reward panel that jointly incentivizes SQL correctness and efficient exploration. Across benchmarks, SQL-Trail sets a new state of the art and delivers strong data efficiency--up to 18x higher than prior single-pass RL state-of-the-art methods. Notably, our 7B and 14B models outperform substantially larger proprietary systems by 5% on average, underscoring the effectiveness of interactive, agentic workflows for robust Text-to-SQL generation."
    },
    {
      "title": "Teaching LLMs to Ask: Self-Querying Category-Theoretic Planning for Under-Specified Reasoning",
      "url": "https://arxiv.org/abs/2601.20014",
      "date": "2026-01-27",
      "authors_text": "Shuhui Qu",
      "is_highlight": false,
      "score": 68.0,
      "summary": "The paper presents SQ-BCP, a self-querying planning method for large language models that effectively resolves unknown preconditions, reducing resource violations significantly while ensuring goal compatibility.",
      "teaser_image": "https://arxiv.org/html/2601.20014/123.png",
      "debug_abstract": "Inference-time planning with large language models frequently breaks under partial observability: when task-critical preconditions are not specified at query time, models tend to hallucinate missing facts or produce plans that violate hard constraints. We introduce \\textbf{Self-Querying Bidirectional Categorical Planning (SQ-BCP)}, which explicitly represents precondition status (\\texttt{Sat}/\\texttt{Viol}/\\texttt{Unk}) and resolves unknowns via (i) targeted self-queries to an oracle/user or (ii) \\emph{bridging} hypotheses that establish the missing condition through an additional action. SQ-BCP performs bidirectional search and invokes a pullback-based verifier as a categorical certificate of goal compatibility, while using distance-based scores only for ranking and pruning. We prove that when the verifier succeeds and hard constraints pass deterministic checks, accepted plans are compatible with goal requirements; under bounded branching and finite resolution depth, SQ-BCP finds an accepting plan when one exists. Across WikiHow and RecipeNLG tasks with withheld preconditions, SQ-BCP reduces resource-violation rates to \\textbf{14.9\\%} and \\textbf{5.8\\%} (vs.\\ \\textbf{26.0\\%} and \\textbf{15.7\\%} for the best baseline), while maintaining competitive reference quality."
    },
    {
      "title": "Fuzzy Categorical Planning: Autonomous Goal Satisfaction with Graded Semantic Constraints",
      "url": "https://arxiv.org/abs/2601.20021",
      "date": "2026-01-27",
      "authors_text": "Shuhui Qu",
      "is_highlight": false,
      "score": 77.0,
      "summary": "Fuzzy Category-theoretic Planning (FCP) enhances natural-language planning by incorporating graded semantic constraints, improving goal satisfaction and reducing violations in recipe substitution tasks.",
      "teaser_image": "https://arxiv.org/html/2601.20021/345.png",
      "debug_abstract": "Natural-language planning often involves vague predicates (e.g., suitable substitute, stable enough) whose satisfaction is inherently graded. Existing category-theoretic planners provide compositional structure and pullback-based hard-constraint verification, but treat applicability as crisp, forcing thresholding that collapses meaningful distinctions and cannot track quality degradation across multi-step plans. We propose Fuzzy Category-theoretic Planning (FCP), which annotates each action (morphism) with a degree in [0,1], composes plan quality via a t-norm Lukasiewicz, and retains crisp executability checks via pullback verification. FCP grounds graded applicability from language using an LLM with k-sample median aggregation and supports meeting-in-the-middle search using residuum-based backward requirements. We evaluate on (i) public PDDL3 preference/oversubscription benchmarks and (ii) RecipeNLG-Subs, a missing-substitute recipe-planning benchmark built from RecipeNLG with substitution candidates from Recipe1MSubs and FoodKG. FCP improves success and reduces hard-constraint violations on RecipeNLG-Subs compared to LLM-only and ReAct-style baselines, while remaining competitive with classical PDDL3 planners."
    },
    {
      "title": "Open-Vocabulary Functional 3D Human-Scene Interaction Generation",
      "url": "https://arxiv.org/abs/2601.20835",
      "date": "2026-01-28",
      "authors_text": "Jie Liu, Yu Sun, Alpar Cseke, Yao Feng, Nicolas Heron",
      "is_highlight": false,
      "score": 81.0,
      "summary": "The FunHSI framework generates functionally correct 3D human-scene interactions from open-vocabulary prompts by employing functionality-aware contact reasoning and vision-language models.",
      "teaser_image": "https://arxiv.org/html/2601.20835/x2.png",
      "debug_abstract": "Generating 3D humans that functionally interact with 3D scenes remains an open problem with applications in embodied AI, robotics, and interactive content creation. The key challenge involves reasoning about both the semantics of functional elements in 3D scenes and the 3D human poses required to achieve functionality-aware interaction. Unfortunately, existing methods typically lack explicit reasoning over object functionality and the corresponding human-scene contact, resulting in implausible or functionally incorrect interactions. In this work, we propose FunHSI, a training-free, functionality-driven framework that enables functionally correct human-scene interactions from open-vocabulary task prompts. Given a task prompt, FunHSI performs functionality-aware contact reasoning to identify functional scene elements, reconstruct their 3D geometry, and model high-level interactions via a contact graph. We then leverage vision-language models to synthesize a human performing the task in the image and estimate proposed 3D body and hand poses. Finally, the proposed 3D body configuration is refined via stage-wise optimization to ensure physical plausibility and functional correctness. In contrast to existing methods, FunHSI not only synthesizes more plausible general 3D interactions, such as &#34;sitting on a sofa&#39;&#39;, while supporting fine-grained functional human-scene interactions, e.g., &#34;increasing the room temperature&#39;&#39;. Extensive experiments demonstrate that FunHSI consistently generates functionally correct and physically plausible human-scene interactions across diverse indoor and outdoor scenes."
    },
    {
      "title": "DeFM: Learning Foundation Representations from Depth for Robotics",
      "url": "https://arxiv.org/abs/2601.18923",
      "date": "2026-01-26",
      "authors_text": "Manthan Patel, Jonas Frey, Mayank Mittal, Fan Yang, Alexander Hansson",
      "is_highlight": true,
      "score": 94.0,
      "summary": "DeFM is a self-supervised foundation model that learns robust geometric and semantic representations from depth images, achieving state-of-the-art performance in various robotic tasks and environments.",
      "teaser_image": "https://arxiv.org/html/2601.18923/x1.png",
      "debug_abstract": "Depth sensors are widely deployed across robotic platforms, and advances in fast, high-fidelity depth simulation have enabled robotic policies trained on depth observations to achieve robust sim-to-real transfer for a wide range of tasks. Despite this, representation learning for depth modality remains underexplored compared to RGB, where large-scale foundation models now define the state of the art. To address this gap, we present DeFM, a self-supervised foundation model trained entirely on depth images for robotic applications. Using a DINO-style self-distillation objective on a curated dataset of 60M depth images, DeFM learns geometric and semantic representations that generalize to diverse environments, tasks, and sensors. To retain metric awareness across multiple scales, we introduce a novel input normalization strategy. We further distill DeFM into compact models suitable for resource-constrained robotic systems. When evaluated on depth-based classification, segmentation, navigation, locomotion, and manipulation benchmarks, DeFM achieves state-of-the-art performance and demonstrates strong generalization from simulation to real-world environments. We release all our pretrained models, which can be adopted off-the-shelf for depth-based robotic learning without task-specific fine-tuning. Webpage: this https URL"
    },
    {
      "title": "Image2Garment: Simulation-ready Garment Generation from a Single Image",
      "url": "https://arxiv.org/abs/2601.09658",
      "date": "2026-01-25",
      "authors_text": "Selim Emir Can, Jan Ackermann, Kiyohiro Nakayama, Ruofan Liu, Tong Wu",
      "is_highlight": false,
      "score": 57.0,
      "summary": "The paper presents Image2Garment, a novel framework that generates simulation-ready garments from a single image by inferring material properties and fabric attributes without iterative optimization.",
      "debug_abstract": "Estimating physically accurate, simulation-ready garments from a single image is challenging due to the absence of image-to-physics datasets and the ill-posed nature of this problem. Prior methods either require multi-view capture and expensive differentiable simulation or predict only garment geometry without the material properties required for realistic simulation. We propose a feed-forward framework that sidesteps these limitations by first fine-tuning a vision-language model to infer material composition and fabric attributes from real images, and then training a lightweight predictor that maps these attributes to the corresponding physical fabric parameters using a small dataset of material-physics measurements. Our approach introduces two new datasets (FTAG and T2P) and delivers simulation-ready garments from a single image without iterative optimization. Experiments show that our estimator achieves superior accuracy in material composition estimation and fabric attribute prediction, and by passing them through our physics parameter estimator, we further achieve higher-fidelity simulations compared to state-of-the-art image-to-garment methods."
    },
    {
      "title": "PaperSearchQA: Learning to Search and Reason over Scientific Papers with RLVR",
      "url": "https://arxiv.org/abs/2601.18207",
      "date": "2026-01-26",
      "authors_text": "James Burgess, Jan N. Hansen, Duo Peng, Yuhui Zhang, Alejandro Lozano",
      "is_highlight": false,
      "score": 53.0,
      "summary": "The paper introduces PaperSearchQA, a reinforcement learning framework for training search agents to effectively reason and answer questions using a corpus of biomedical paper abstracts.",
      "debug_abstract": "Search agents are language models (LMs) that reason and search knowledge bases (or the web) to answer questions; recent methods supervise only the final answer accuracy using reinforcement learning with verifiable rewards (RLVR). Most RLVR search agents tackle general-domain QA, which limits their relevance to technical AI systems in science, engineering, and medicine. In this work we propose training agents to search and reason over scientific papers -- this tests technical question-answering, it is directly relevant to real scientists, and the capabilities will be crucial to future AI Scientist systems. Concretely, we release a search corpus of 16 million biomedical paper abstracts and construct a challenging factoid QA dataset called PaperSearchQA with 60k samples answerable from the corpus, along with benchmarks. We train search agents in this environment to outperform non-RL retrieval baselines; we also perform further quantitative analysis and observe interesting agent behaviors like planning, reasoning, and self-verification. Our corpus, datasets, and benchmarks are usable with the popular Search-R1 codebase for RLVR training and released on this https URL. Finally, our data creation methods are scalable and easily extendable to other scientific domains."
    },
    {
      "title": "Cosmos Policy: Fine-Tuning Video Models for Visuomotor Control and Planning",
      "url": "https://arxiv.org/abs/2601.16163",
      "date": "2026-01-22",
      "authors_text": "Moo Jin Kim, Yihuai Gao, Tsung-Yi Lin, Yen-Chen Lin, Yunhao Ge",
      "is_highlight": false,
      "score": 90.4,
      "summary": "Cosmos Policy efficiently fine-tunes a pretrained video model for robot action generation and planning, achieving state-of-the-art performance in various benchmarks without architectural changes.",
      "debug_abstract": "Recent video generation models demonstrate remarkable ability to capture complex physical interactions and scene evolution over time. To leverage their spatiotemporal priors, robotics works have adapted video models for policy learning but introduce complexity by requiring multiple stages of post-training and new architectural components for action generation. In this work, we introduce Cosmos Policy, a simple approach for adapting a large pretrained video model (Cosmos-Predict2) into an effective robot policy through a single stage of post-training on the robot demonstration data collected on the target platform, with no architectural modifications. Cosmos Policy learns to directly generate robot actions encoded as latent frames within the video model&#39;s latent diffusion process, harnessing the model&#39;s pretrained priors and core learning algorithm to capture complex action distributions. Additionally, Cosmos Policy generates future state images and values (expected cumulative rewards), which are similarly encoded as latent frames, enabling test-time planning of action trajectories with higher likelihood of success. In our evaluations, Cosmos Policy achieves state-of-the-art performance on the LIBERO and RoboCasa simulation benchmarks (98.5% and 67.1% average success rates, respectively) and the highest average score in challenging real-world bimanual manipulation tasks, outperforming strong diffusion policies trained from scratch, video model-based policies, and state-of-the-art vision-language-action models fine-tuned on the same robot demonstrations. Furthermore, given policy rollout data, Cosmos Policy can learn from experience to refine its world model and value function and leverage model-based planning to achieve even higher success rates in challenging tasks. We release code, models, and training data at this https URL"
    }
  ],
  "HMI Lab": [
    {
      "title": "Multi-Resolution Alignment for Voxel Sparsity in Camera-Based 3D Semantic Scene Completion",
      "url": "https://arxiv.org/abs/2602.03371",
      "date": "2026-02-03",
      "authors_text": "Zhiwen Yang, Yuxin Peng",
      "is_highlight": false,
      "score": 61.0,
      "summary": "The paper presents a Multi-Resolution Alignment approach to enhance camera-based 3D semantic scene completion by addressing voxel sparsity through scene and instance-level feature alignment.",
      "teaser_image": "https://arxiv.org/html/2602.03371/x2.png",
      "debug_abstract": "Camera-based 3D semantic scene completion (SSC) offers a cost-effective solution for assessing the geometric occupancy and semantic labels of each voxel in the surrounding 3D scene with image inputs, providing a voxel-level scene perception foundation for the perception-prediction-planning autonomous driving systems. Although significant progress has been made in existing methods, their optimization rely solely on the supervision from voxel labels and face the challenge of voxel sparsity as a large portion of voxels in autonomous driving scenarios are empty, which limits both optimization efficiency and model performance. To address this issue, we propose a \\textit{Multi-Resolution Alignment (MRA)} approach to mitigate voxel sparsity in camera-based 3D semantic scene completion, which exploits the scene and instance level alignment across multi-resolution 3D features as auxiliary supervision. Specifically, we first propose the Multi-resolution View Transformer module, which projects 2D image features into multi-resolution 3D features and aligns them at the scene level through fusing discriminative seed features. Furthermore, we design the Cubic Semantic Anisotropy module to identify the instance-level semantic significance of each voxel, accounting for the semantic differences of a specific voxel against its neighboring voxels within a cubic area. Finally, we devise a Critical Distribution Alignment module, which selects critical voxels as instance-level anchors with the guidance of cubic semantic anisotropy, and applies a circulated loss for auxiliary supervision on the critical feature distribution consistency across different resolutions. The code is available at this https URL."
    },
    {
      "title": "Learning Physics-Grounded 4D Dynamics with Neural Gaussian Force Fields",
      "url": "https://arxiv.org/abs/2602.00148",
      "date": "2026-01-29",
      "authors_text": "Shiqian Li, Ruihong Shen, Junfeng Ni, Chang Pan, Chi Zhang",
      "is_highlight": false,
      "score": 82.0,
      "summary": "The Neural Gaussian Force Field (NGFF) framework enables rapid generation of interactive, physically realistic 4D videos from multi-view RGB inputs, enhancing robustness and generalization in physical dynamics.",
      "teaser_image": "https://arxiv.org/html/2602.00148/x1.png",
      "debug_abstract": "Predicting physical dynamics from raw visual data remains a major challenge in AI. While recent video generation models have achieved impressive visual quality, they still cannot consistently generate physically plausible videos due to a lack of modeling of physical laws. Recent approaches combining 3D Gaussian splatting and physics engines can produce physically plausible videos, but are hindered by high computational costs in both reconstruction and simulation, and often lack robustness in complex real-world scenarios. To address these issues, we introduce Neural Gaussian Force Field (NGFF), an end-to-end neural framework that integrates 3D Gaussian perception with physics-based dynamic modeling to generate interactive, physically realistic 4D videos from multi-view RGB inputs, achieving two orders of magnitude faster than prior Gaussian simulators. To support training, we also present GSCollision, a 4D Gaussian dataset featuring diverse materials, multi-object interactions, and complex scenes, totaling over 640k rendered physical videos (~4 TB). Evaluations on synthetic and real 3D scenarios show NGFF&#39;s strong generalization and robustness in physical reasoning, advancing video prediction towards physics-grounded world models."
    },
    {
      "title": "Resource-Efficient Reinforcement for Reasoning Large Language Models via Dynamic One-Shot Policy Refinement",
      "url": "https://arxiv.org/abs/2602.00815",
      "date": "2026-01-31",
      "authors_text": "Yunjian Zhang, Sudong Wang, Yang Li, Peiran Xu, Conghao Zhou",
      "is_highlight": false,
      "score": 57.0,
      "summary": "The paper introduces Dynamic One-Shot Policy Refinement (DoPR), a resource-efficient reinforcement learning strategy that enhances reasoning in large language models while significantly reducing training costs.",
      "teaser_image": "https://arxiv.org/html/2602.00815/dopr.png",
      "debug_abstract": "Large language models (LLMs) have exhibited remarkable performance on complex reasoning tasks, with reinforcement learning under verifiable rewards (RLVR) emerging as a principled framework for aligning model behavior with reasoning chains. Despite its promise, RLVR remains prohibitively resource-intensive, requiring extensive reward signals and incurring substantial rollout costs during training. In this work, we revisit the fundamental question of data and compute efficiency in RLVR. We first establish a theoretical lower bound on the sample complexity required to unlock reasoning capabilities, and empirically validate that strong performance can be achieved with a surprisingly small number of training instances. To tackle the computational burden, we propose Dynamic One-Shot Policy Refinement (DoPR), an uncertainty-aware RL strategy that dynamically selects a single informative training sample per batch for policy updates, guided by reward volatility and exploration-driven acquisition. DoPR reduces rollout overhead by nearly an order of magnitude while preserving competitive reasoning accuracy, offering a scalable and resource-efficient solution for LLM post-training. This approach offers a practical path toward more efficient and accessible RL-based training for reasoning-intensive LLM applications."
    },
    {
      "title": "Residual Decoding: Mitigating Hallucinations in Large Vision-Language Models via History-Aware Residual Guidance",
      "url": "https://arxiv.org/abs/2602.01047",
      "date": "2026-02-01",
      "authors_text": "Xinrong Chen, Xu Chu, Yingmin Qiu, Hengyuan Zhang, Jing Xiong",
      "is_highlight": false,
      "score": 53.0,
      "summary": "Residual Decoding (ResDec) mitigates hallucinations in Large Vision-Language Models by leveraging historical information to enhance decoding and improve visual grounding.",
      "teaser_image": "https://arxiv.org/html/2602.01047/x1.png",
      "debug_abstract": "Large Vision-Language Models (LVLMs) can reason effectively from image-text inputs and perform well in various multimodal tasks. Despite this success, they are affected by language priors and often produce hallucinations. Hallucinations denote generated content that is grammatically and syntactically coherent, yet bears no match or direct relevance to actual visual input. To address this problem, we propose Residual Decoding (ResDec). It is a novel training-free method that uses historical information to aid decoding. The method relies on the internal implicit reasoning mechanism and token logits evolution mechanism of LVLMs to correct biases. Extensive experiments demonstrate that ResDec effectively suppresses hallucinations induced by language priors, significantly improves visual grounding, and reduces object hallucinations. In addition to mitigating hallucinations, ResDec also performs exceptionally well on comprehensive LVLM benchmarks, highlighting its broad applicability."
    },
    {
      "title": "Not All Preferences Are Created Equal: Stability-Aware and Gradient-Efficient Alignment for Reasoning Models",
      "url": "https://arxiv.org/abs/2602.01207",
      "date": "2026-02-01",
      "authors_text": "Hui Wu, Hengyi Cai, Jinman Zhao, Xinran Chen, Ziheng Li",
      "is_highlight": false,
      "score": 48.0,
      "summary": "The paper introduces SAGE, a dynamic framework for preference-based alignment in reasoning models that enhances optimization stability and efficiency by prioritizing informative training instances.",
      "teaser_image": "https://arxiv.org/html/2602.01207/x2.png",
      "debug_abstract": "Preference-based alignment is pivotal for training large reasoning models; however, standard methods like Direct Preference Optimization (DPO) typically treat all preference pairs uniformly, overlooking the evolving utility of training instances. This static approach often leads to inefficient or unstable optimization, as it wastes computation on trivial pairs with negligible gradients and suffers from noise induced by samples near uncertain decision boundaries. Facing these challenges, we propose SAGE (Stability-Aware Gradient Efficiency), a dynamic framework designed to enhance alignment reliability by maximizing the Signal-to-Noise Ratio of policy updates. Concretely, SAGE integrates a coarse-grained curriculum mechanism that refreshes candidate pools based on model competence with a fine-grained, stability-aware scoring function that prioritizes informative, confident errors while filtering out unstable samples. Experiments on multiple mathematical reasoning benchmarks demonstrate that SAGE significantly accelerates convergence and outperforms static baselines, highlighting the critical role of policy-aware, stability-conscious data selection in reasoning alignment."
    },
    {
      "title": "Superman: Unifying Skeleton and Vision for Human Motion Perception and Generation",
      "url": "https://arxiv.org/abs/2602.02401",
      "date": "2026-02-02",
      "authors_text": "Xinshun Wang, Peiming Li, Ziyi Wang, Zhongbin Fang, Zhichao Deng",
      "is_highlight": false,
      "score": 15.0,
      "summary": "Superman unifies visual perception and skeleton-based motion generation through a Vision-Guided Motion Tokenizer, enabling robust cross-modal learning and superior performance in human motion tasks.",
      "teaser_image": "https://arxiv.org/html/2602.02401/x2.png",
      "debug_abstract": "Human motion analysis tasks, such as temporal 3D pose estimation, motion prediction, and motion in-betweening, play an essential role in computer vision. However, current paradigms suffer from severe fragmentation. First, the field is split between ``perception&#39;&#39; models that understand motion from video but only output text, and ``generation&#39;&#39; models that cannot perceive from raw visual input. Second, generative MLLMs are often limited to single-frame, static poses using dense, parametric SMPL models, failing to handle temporal motion. Third, existing motion vocabularies are built from skeleton data alone, severing the link to the visual domain. To address these challenges, we introduce Superman, a unified framework that bridges visual perception with temporal, skeleton-based motion generation. Our solution is twofold. First, to overcome the modality disconnect, we propose a Vision-Guided Motion Tokenizer. Leveraging the natural geometric alignment between 3D skeletons and visual data, this module pioneers robust joint learning from both modalities, creating a unified, cross-modal motion vocabulary. Second, grounded in this motion language, a single, unified MLLM architecture is trained to handle all tasks. This module flexibly processes diverse, temporal inputs, unifying 3D skeleton pose estimation from video (perception) with skeleton-based motion prediction and in-betweening (generation). Extensive experiments on standard benchmarks, including Human3.6M, demonstrate that our unified method achieves state-of-the-art or competitive performance across all motion tasks. This showcases a more efficient and scalable path for generative motion analysis using skeletons."
    },
    {
      "title": "Disturbance-Aware Flight Control of Robotic Gliding Blimp via Moving Mass Actuation",
      "url": "https://arxiv.org/abs/2601.21188",
      "date": "2026-01-29",
      "authors_text": "Hao Cheng, Feitian Zhang",
      "is_highlight": false,
      "score": 86.0,
      "summary": "This paper presents a disturbance-aware flight control system for robotic blimps using moving mass actuation and a combined moving horizon estimator and model predictive controller for enhanced stability.",
      "teaser_image": "https://arxiv.org/html/2601.21188/x2.png",
      "debug_abstract": "Robotic blimps, as lighter-than-air (LTA) aerial systems, offer long endurance and inherently safe operation but remain highly susceptible to wind disturbances. Building on recent advances in moving mass actuation, this paper addresses the lack of disturbance-aware control frameworks for LTA platforms by explicitly modeling and compensating for wind-induced effects. A moving horizon estimator (MHE) infers real-time wind perturbations and provides these estimates to a model predictive controller (MPC), enabling robust trajectory and heading regulation under varying wind conditions. The proposed approach leverages a two-degree-of-freedom (2-DoF) moving-mass mechanism to generate both inertial and aerodynamic moments for attitude and heading control, thereby enhancing flight stability in disturbance-prone environments. Extensive flight experiments under headwind and crosswind conditions show that the integrated MHE-MPC framework significantly outperforms baseline PID control, demonstrating its effectiveness for disturbance-aware LTA flight."
    },
    {
      "title": "GVGS: Gaussian Visibility-Aware Multi-View Geometry for Accurate Surface Reconstruction",
      "url": "https://arxiv.org/abs/2601.20331",
      "date": "2026-01-28",
      "authors_text": "Mai Su, Qihan Yu, Zhongtao Wang, Yilong Li, Chengwei Pan",
      "is_highlight": true,
      "score": 60.0,
      "summary": "The paper presents GVGS, a method enhancing 3D surface reconstruction accuracy by integrating Gaussian visibility-aware constraints and progressive monocular depth calibration to overcome existing limitations.",
      "teaser_image": "https://arxiv.org/html/2601.20331/x2.png",
      "debug_abstract": "3D Gaussian Splatting enables efficient optimization and high-quality rendering, yet accurate surface reconstruction remains challenging. Prior methods improve surface reconstruction by refining Gaussian depth estimates, either via multi-view geometric consistency or through monocular depth priors. However, multi-view constraints become unreliable under large geometric discrepancies, while monocular priors suffer from scale ambiguity and local inconsistency, ultimately leading to inaccurate Gaussian depth supervision. To address these limitations, we introduce a Gaussian visibility-aware multi-view geometric consistency constraint that aggregates the visibility of shared Gaussian primitives across views, enabling more accurate and stable geometric supervision. In addition, we propose a progressive quadtree-calibrated Monocular depth constraint that performs block-wise affine calibration from coarse to fine spatial scales, mitigating the scale ambiguity of depth priors while preserving fine-grained surface details. Extensive experiments on DTU and TNT datasets demonstrate consistent improvements in geometric accuracy over prior Gaussian-based and implicit surface reconstruction methods. Codes are available at an anonymous repository: this https URL."
    },
    {
      "title": "Uni-RS: A Spatially Faithful Unified Understanding and Generation Model for Remote Sensing",
      "url": "https://arxiv.org/abs/2601.17673",
      "date": "2026-01-25",
      "authors_text": "Weiyu Zhang, Yuan Hu, Yong Li, Yu Liu",
      "is_highlight": false,
      "score": 58.0,
      "summary": "Uni-RS is a novel unified multimodal model for remote sensing that enhances spatial faithfulness in text-to-image generation by integrating spatial layout planning and supervision techniques.",
      "debug_abstract": "Unified remote sensing multimodal models exhibit a pronounced spatial reversal curse: Although they can accurately recognize and describe object locations in images, they often fail to faithfully execute the same spatial relations during text-to-image generation, where such relations constitute core semantic information in remote sensing. Motivated by this observation, we propose Uni-RS, the first unified multimodal model tailored for remote sensing, to explicitly address the spatial asymmetry between understanding and generation. Specifically, we first introduce explicit Spatial-Layout Planning to transform textual instructions into spatial layout plans, decoupling geometric planning from visual synthesis. We then impose Spatial-Aware Query Supervision to bias learnable queries toward spatial relations explicitly specified in the instruction. Finally, we develop Image-Caption Spatial Layout Variation to expose the model to systematic geometry-consistent spatial transformations. Extensive experiments across multiple benchmarks show that our approach substantially improves spatial faithfulness in text-to-image generation, while maintaining strong performance on multimodal understanding tasks like image captioning, visual grounding, and VQA tasks."
    },
    {
      "title": "Agentic reinforcement learning empowers next-generation chemical language models for molecular design and synthesis",
      "url": "https://arxiv.org/abs/2601.17687",
      "date": "2026-01-25",
      "authors_text": "Hao Li, He Cao, Shenyao Peng, Zijing Liu, Bin Feng",
      "is_highlight": false,
      "score": 62.0,
      "summary": "ChemCRAFT utilizes agentic reinforcement learning to enhance small language models for efficient, cost-effective, and privacy-preserving molecular design and synthesis, outperforming larger cloud-based models.",
      "debug_abstract": "Language models are revolutionizing the biochemistry domain, assisting scientists in drug design and chemical synthesis with high efficiency. Yet current approaches struggle between small language models prone to hallucination and limited knowledge retention, and large cloud-based language models plagued by privacy risks and high inference costs. To bridge this gap, we introduce ChemCRAFT, a novel framework leveraging agentic reinforcement learning to decouple chemical reasoning from knowledge storage. Instead of forcing the model to memorize vast chemical data, our approach empowers the language model to interact with a sandbox for precise information retrieval. This externalization of knowledge allows a locally deployable small model to achieve superior performance with minimal inference costs. To enable small language models for agent-calling ability, we build an agentic trajectory construction pipeline and a comprehensive chemical-agent sandbox. Based on sandbox interactions, we constructed ChemToolDataset, the first large-scale chemical tool trajectory dataset. Simultaneously, we propose SMILES-GRPO to build a dense chemical reward function, promoting the model&#39;s ability to call chemical agents. Evaluations across diverse aspects of drug design show that ChemCRAFT outperforms current cloud-based LLMs in molecular structure analysis, molecular optimization, and synthesis pathway prediction, demonstrating that scientific reasoning is not solely an emergent ability of model scale, but a learnable policy of tool orchestration. This work establishes a cost-effective and privacy-preserving paradigm for AI-aided chemistry, opening new avenues for accelerating molecular discovery with locally deployable agents."
    },
    {
      "title": "TC-IDM: Grounding Video Generation for Executable Zero-shot Robot Motion",
      "url": "https://arxiv.org/abs/2601.18323",
      "date": "2026-01-26",
      "authors_text": "Weishi Mi, Yong Bao, Xiaowei Chi, Xiaozhu Ju, Zhiyuan Qin",
      "is_highlight": false,
      "score": 89.0,
      "summary": "The TC-IDM model enhances robot motion execution by bridging visual planning and physical control through tool-centric trajectory synthesis, achieving superior generalization and performance in diverse tasks.",
      "debug_abstract": "The vision-language-action (VLA) paradigm has enabled powerful robotic control by leveraging vision-language models, but its reliance on large-scale, high-quality robot data limits its generalization. Generative world models offer a promising alternative for general-purpose embodied AI, yet a critical gap remains between their pixel-level plans and physically executable actions. To this end, we propose the Tool-Centric Inverse Dynamics Model (TC-IDM). By focusing on the tool&#39;s imagined trajectory as synthesized by the world model, TC-IDM establishes a robust intermediate representation that bridges the gap between visual planning and physical control. TC-IDM extracts the tool&#39;s point cloud trajectories via segmentation and 3D motion estimation from generated videos. Considering diverse tool attributes, our architecture employs decoupled action heads to project these planned trajectories into 6-DoF end-effector motions and corresponding control signals. This plan-and-translate paradigm not only supports a wide range of end-effectors but also significantly improves viewpoint invariance. Furthermore, it exhibits strong generalization capabilities across long-horizon and out-of-distribution tasks, including interacting with deformable objects. In real-world evaluations, the world model with TC-IDM achieves an average success rate of 61.11 percent, with 77.7 percent on simple tasks and 38.46 percent on zero-shot deformable object tasks. It substantially outperforms end-to-end VLA-style baselines and other inverse dynamics models."
    },
    {
      "title": "Order from Chaos: Physical World Understanding from Glitchy Gameplay Videos",
      "url": "https://arxiv.org/abs/2601.16471",
      "date": "2026-01-23",
      "authors_text": "Meng Cao, Haoran Tang, Haoze Zhao, Mingfei Han, Ruyang Liu",
      "is_highlight": false,
      "score": 80.0,
      "summary": "This paper introduces PhysGame, a dataset leveraging gameplay glitches to enhance AI's understanding of physical principles, demonstrating improved reasoning capabilities through extensive experimentation.",
      "debug_abstract": "Understanding the physical world, including object dynamics, material properties, and causal interactions, remains a core challenge in artificial intelligence. Although recent multi-modal large language models (MLLMs) have demonstrated impressive general reasoning capabilities, they still fall short of achieving human-level understanding of physical principles. Existing datasets for physical reasoning either rely on real-world videos, which incur high annotation costs, or on synthetic simulations, which suffer from limited realism and diversity. In this paper, we propose a novel paradigm that leverages glitches in gameplay videos, referring to visual anomalies that violate predefined physical laws, as a rich and scalable supervision source for physical world understanding. We introduce PhysGame, an meta information guided instruction-tuning dataset containing 140,057 glitch-centric question-answer pairs across five physical domains and sixteen fine-grained categories. To ensure data accuracy, we design a prompting strategy that utilizes gameplay metadata such as titles and descriptions to guide high-quality QA generation. Complementing PhysGame, we construct GameBench, an expert-annotated benchmark with 880 glitch-identified gameplay videos designed to evaluate physical reasoning capabilities. Extensive experiments show that PhysGame significantly enhances both Game2Real transferability, improving the real world physical reasoning performance of Qwen2.5VL by 2.5% on PhysBench, and Game2General transferability, yielding a 1.9% gain on the MVBench benchmark. Moreover, PhysGame-tuned models achieve a 3.7% absolute improvement on GameBench, demonstrating enhanced robustness in detecting physical implausibilities. These results indicate that learning from gameplay anomalies offers a scalable and effective pathway toward advancing physical world understanding in multimodal intelligence."
    },
    {
      "title": "Render-of-Thought: Rendering Textual Chain-of-Thought as Images for Visual Latent Reasoning",
      "url": "https://arxiv.org/abs/2601.14750",
      "date": "2026-01-22",
      "authors_text": "Yifan Wang, Shiyu Li, Peiming Li, Xiaochen Yang, Yang Tang",
      "is_highlight": false,
      "score": 92.5,
      "summary": "The Render-of-Thought framework transforms Chain-of-Thought reasoning into images, enhancing analyzability and efficiency while maintaining competitive performance in reasoning tasks.",
      "debug_abstract": "Chain-of-Thought (CoT) prompting has achieved remarkable success in unlocking the reasoning capabilities of Large Language Models (LLMs). Although CoT prompting enhances reasoning, its verbosity imposes substantial computational overhead. Recent works often focus exclusively on outcome alignment and lack supervision on the intermediate reasoning process. These deficiencies obscure the analyzability of the latent reasoning chain. To address these challenges, we introduce Render-of-Thought (RoT), the first framework to reify the reasoning chain by rendering textual steps into images, making the latent rationale explicit and traceable. Specifically, we leverage the vision encoders of existing Vision Language Models (VLMs) as semantic anchors to align the vision embeddings with the textual space. This design ensures plug-and-play implementation without incurring additional pre-training overhead. Extensive experiments on mathematical and logical reasoning benchmarks demonstrate that our method achieves 3-4x token compression and substantial inference acceleration compared to explicit CoT. Furthermore, it maintains competitive performance against other methods, validating the feasibility of this paradigm. Our code is available at this https URL"
    },
    {
      "title": "Decoupling Return-to-Go for Efficient Decision Transformer",
      "url": "https://arxiv.org/abs/2601.15953",
      "date": "2026-01-22",
      "authors_text": "Yongyi Wang, Hanyu Liu, Lingfeng Li, Bozhou Chen, Ang Li",
      "is_highlight": false,
      "score": 75.2,
      "summary": "The Decoupled Decision Transformer (DDT) enhances offline reinforcement learning by using only the latest Return-to-Go for action prediction, improving performance and reducing computational costs.",
      "debug_abstract": "The Decision Transformer (DT) has established a powerful sequence modeling approach to offline reinforcement learning. It conditions its action predictions on Return-to-Go (RTG), using it both to distinguish trajectory quality during training and to guide action generation at inference. In this work, we identify a critical redundancy in this design: feeding the entire sequence of RTGs into the Transformer is theoretically unnecessary, as only the most recent RTG affects action prediction. We show that this redundancy can impair DT&#39;s performance through experiments. To resolve this, we propose the Decoupled DT (DDT). DDT simplifies the architecture by processing only observation and action sequences through the Transformer, using the latest RTG to guide the action prediction. This streamlined approach not only improves performance but also reduces computational cost. Our experiments show that DDT significantly outperforms DT and establishes competitive performance against state-of-the-art DT variants across multiple offline RL tasks."
    },
    {
      "title": "PhysicsMind: Sim and Real Mechanics Benchmarking for Physical Reasoning and Prediction in Foundational VLMs and World Models",
      "url": "https://arxiv.org/abs/2601.16007",
      "date": "2026-01-22",
      "authors_text": "Chak-Wing Mak, Guanyu Zhu, Boyi Zhang, Hongji Li, Xiaowei Chi",
      "is_highlight": false,
      "score": 94.1,
      "summary": "PhysicsMind introduces a benchmark for evaluating physical reasoning in multimodal models through real and simulated tasks focused on fundamental physics principles, revealing significant gaps in current models' understanding.",
      "debug_abstract": "Modern foundational Multimodal Large Language Models (MLLMs) and video world models have advanced significantly in mathematical, common-sense, and visual reasoning, but their grasp of the underlying physics remains underexplored. Existing benchmarks attempting to measure this matter rely on synthetic, Visual Question Answer templates or focus on perceptual video quality that is tangential to measuring how well the video abides by physical laws. To address this fragmentation, we introduce PhysicsMind, a unified benchmark with both real and simulation environments that evaluates law-consistent reasoning and generation over three canonical principles: Center of Mass, Lever Equilibrium, and Newton&#39;s First Law. PhysicsMind comprises two main tasks: i) VQA tasks, testing whether models can reason and determine physical quantities and values from images or short videos, and ii) Video Generation(VG) tasks, evaluating if predicted motion trajectories obey the same center-of-mass, torque, and inertial constraints as the ground truth. A broad range of recent models and video generation models is evaluated on PhysicsMind and found to rely on appearance heuristics while often violating basic mechanics. These gaps indicate that current scaling and training are still insufficient for robust physical understanding, underscoring PhysicsMind as a focused testbed for physics-aware multimodal models. Our data will be released upon acceptance."
    },
    {
      "title": "Collision-Free Humanoid Traversal in Cluttered Indoor Scenes",
      "url": "https://arxiv.org/abs/2601.16035",
      "date": "2026-01-22",
      "authors_text": "Han Xue, Sikai Liang, Zhikai Zhang, Zicheng Zeng, Yun Liu",
      "is_highlight": false,
      "score": 77.3,
      "summary": "The paper presents Humanoid Potential Field (HumanoidPF) for enabling collision-free traversal of humanoids in cluttered indoor environments, enhancing RL-based skill learning and real-world application.",
      "debug_abstract": "We study the problem of collision-free humanoid traversal in cluttered indoor scenes, such as hurdling over objects scattered on the floor, crouching under low-hanging obstacles, or squeezing through narrow passages. To achieve this goal, the humanoid needs to map its perception of surrounding obstacles with diverse spatial layouts and geometries to the corresponding traversal skills. However, the lack of an effective representation that captures humanoid-obstacle relationships during collision avoidance makes directly learning such mappings difficult. We therefore propose Humanoid Potential Field (HumanoidPF), which encodes these relationships as collision-free motion directions, significantly facilitating RL-based traversal skill learning. We also find that HumanoidPF exhibits a surprisingly negligible sim-to-real gap as a perceptual representation. To further enable generalizable traversal skills through diverse and challenging cluttered indoor scenes, we further propose a hybrid scene generation method, incorporating crops of realistic 3D indoor scenes and procedurally synthesized obstacles. We successfully transfer our policy to the real world and develop a teleoperation system where users could command the humanoid to traverse in cluttered indoor scenes with just a single click. Extensive experiments are conducted in both simulation and the real world to validate the effectiveness of our method. Demos and code can be found in our website: this https URL."
    }
  ],
  "具身感知与交互实验室 (EPIC Lab)": [
    {
      "title": "Multi-Resolution Alignment for Voxel Sparsity in Camera-Based 3D Semantic Scene Completion",
      "url": "https://arxiv.org/abs/2602.03371",
      "date": "2026-02-03",
      "authors_text": "Zhiwen Yang, Yuxin Peng",
      "is_highlight": false,
      "score": 61.0,
      "summary": "The paper presents a Multi-Resolution Alignment approach to enhance camera-based 3D semantic scene completion by addressing voxel sparsity through scene and instance-level feature alignment.",
      "teaser_image": "https://arxiv.org/html/2602.03371/x2.png",
      "debug_abstract": "Camera-based 3D semantic scene completion (SSC) offers a cost-effective solution for assessing the geometric occupancy and semantic labels of each voxel in the surrounding 3D scene with image inputs, providing a voxel-level scene perception foundation for the perception-prediction-planning autonomous driving systems. Although significant progress has been made in existing methods, their optimization rely solely on the supervision from voxel labels and face the challenge of voxel sparsity as a large portion of voxels in autonomous driving scenarios are empty, which limits both optimization efficiency and model performance. To address this issue, we propose a \\textit{Multi-Resolution Alignment (MRA)} approach to mitigate voxel sparsity in camera-based 3D semantic scene completion, which exploits the scene and instance level alignment across multi-resolution 3D features as auxiliary supervision. Specifically, we first propose the Multi-resolution View Transformer module, which projects 2D image features into multi-resolution 3D features and aligns them at the scene level through fusing discriminative seed features. Furthermore, we design the Cubic Semantic Anisotropy module to identify the instance-level semantic significance of each voxel, accounting for the semantic differences of a specific voxel against its neighboring voxels within a cubic area. Finally, we devise a Critical Distribution Alignment module, which selects critical voxels as instance-level anchors with the guidance of cubic semantic anisotropy, and applies a circulated loss for auxiliary supervision on the critical feature distribution consistency across different resolutions. The code is available at this https URL."
    },
    {
      "title": "Learning Physics-Grounded 4D Dynamics with Neural Gaussian Force Fields",
      "url": "https://arxiv.org/abs/2602.00148",
      "date": "2026-01-29",
      "authors_text": "Shiqian Li, Ruihong Shen, Junfeng Ni, Chang Pan, Chi Zhang",
      "is_highlight": false,
      "score": 82.0,
      "summary": "The Neural Gaussian Force Field (NGFF) framework enables rapid generation of interactive, physically realistic 4D videos from multi-view RGB inputs, enhancing robustness and generalization in physical dynamics.",
      "teaser_image": "https://arxiv.org/html/2602.00148/x1.png",
      "debug_abstract": "Predicting physical dynamics from raw visual data remains a major challenge in AI. While recent video generation models have achieved impressive visual quality, they still cannot consistently generate physically plausible videos due to a lack of modeling of physical laws. Recent approaches combining 3D Gaussian splatting and physics engines can produce physically plausible videos, but are hindered by high computational costs in both reconstruction and simulation, and often lack robustness in complex real-world scenarios. To address these issues, we introduce Neural Gaussian Force Field (NGFF), an end-to-end neural framework that integrates 3D Gaussian perception with physics-based dynamic modeling to generate interactive, physically realistic 4D videos from multi-view RGB inputs, achieving two orders of magnitude faster than prior Gaussian simulators. To support training, we also present GSCollision, a 4D Gaussian dataset featuring diverse materials, multi-object interactions, and complex scenes, totaling over 640k rendered physical videos (~4 TB). Evaluations on synthetic and real 3D scenarios show NGFF&#39;s strong generalization and robustness in physical reasoning, advancing video prediction towards physics-grounded world models."
    },
    {
      "title": "Resource-Efficient Reinforcement for Reasoning Large Language Models via Dynamic One-Shot Policy Refinement",
      "url": "https://arxiv.org/abs/2602.00815",
      "date": "2026-01-31",
      "authors_text": "Yunjian Zhang, Sudong Wang, Yang Li, Peiran Xu, Conghao Zhou",
      "is_highlight": false,
      "score": 57.0,
      "summary": "The paper introduces Dynamic One-Shot Policy Refinement (DoPR), a resource-efficient reinforcement learning strategy that enhances reasoning in large language models while significantly reducing training costs.",
      "teaser_image": "https://arxiv.org/html/2602.00815/dopr.png",
      "debug_abstract": "Large language models (LLMs) have exhibited remarkable performance on complex reasoning tasks, with reinforcement learning under verifiable rewards (RLVR) emerging as a principled framework for aligning model behavior with reasoning chains. Despite its promise, RLVR remains prohibitively resource-intensive, requiring extensive reward signals and incurring substantial rollout costs during training. In this work, we revisit the fundamental question of data and compute efficiency in RLVR. We first establish a theoretical lower bound on the sample complexity required to unlock reasoning capabilities, and empirically validate that strong performance can be achieved with a surprisingly small number of training instances. To tackle the computational burden, we propose Dynamic One-Shot Policy Refinement (DoPR), an uncertainty-aware RL strategy that dynamically selects a single informative training sample per batch for policy updates, guided by reward volatility and exploration-driven acquisition. DoPR reduces rollout overhead by nearly an order of magnitude while preserving competitive reasoning accuracy, offering a scalable and resource-efficient solution for LLM post-training. This approach offers a practical path toward more efficient and accessible RL-based training for reasoning-intensive LLM applications."
    },
    {
      "title": "Residual Decoding: Mitigating Hallucinations in Large Vision-Language Models via History-Aware Residual Guidance",
      "url": "https://arxiv.org/abs/2602.01047",
      "date": "2026-02-01",
      "authors_text": "Xinrong Chen, Xu Chu, Yingmin Qiu, Hengyuan Zhang, Jing Xiong",
      "is_highlight": false,
      "score": 53.0,
      "summary": "Residual Decoding (ResDec) mitigates hallucinations in Large Vision-Language Models by leveraging historical information to enhance decoding and improve visual grounding.",
      "teaser_image": "https://arxiv.org/html/2602.01047/x1.png",
      "debug_abstract": "Large Vision-Language Models (LVLMs) can reason effectively from image-text inputs and perform well in various multimodal tasks. Despite this success, they are affected by language priors and often produce hallucinations. Hallucinations denote generated content that is grammatically and syntactically coherent, yet bears no match or direct relevance to actual visual input. To address this problem, we propose Residual Decoding (ResDec). It is a novel training-free method that uses historical information to aid decoding. The method relies on the internal implicit reasoning mechanism and token logits evolution mechanism of LVLMs to correct biases. Extensive experiments demonstrate that ResDec effectively suppresses hallucinations induced by language priors, significantly improves visual grounding, and reduces object hallucinations. In addition to mitigating hallucinations, ResDec also performs exceptionally well on comprehensive LVLM benchmarks, highlighting its broad applicability."
    },
    {
      "title": "Not All Preferences Are Created Equal: Stability-Aware and Gradient-Efficient Alignment for Reasoning Models",
      "url": "https://arxiv.org/abs/2602.01207",
      "date": "2026-02-01",
      "authors_text": "Hui Wu, Hengyi Cai, Jinman Zhao, Xinran Chen, Ziheng Li",
      "is_highlight": false,
      "score": 48.0,
      "summary": "The paper introduces SAGE, a dynamic framework for preference-based alignment in reasoning models that enhances optimization stability and efficiency by prioritizing informative training instances.",
      "teaser_image": "https://arxiv.org/html/2602.01207/x2.png",
      "debug_abstract": "Preference-based alignment is pivotal for training large reasoning models; however, standard methods like Direct Preference Optimization (DPO) typically treat all preference pairs uniformly, overlooking the evolving utility of training instances. This static approach often leads to inefficient or unstable optimization, as it wastes computation on trivial pairs with negligible gradients and suffers from noise induced by samples near uncertain decision boundaries. Facing these challenges, we propose SAGE (Stability-Aware Gradient Efficiency), a dynamic framework designed to enhance alignment reliability by maximizing the Signal-to-Noise Ratio of policy updates. Concretely, SAGE integrates a coarse-grained curriculum mechanism that refreshes candidate pools based on model competence with a fine-grained, stability-aware scoring function that prioritizes informative, confident errors while filtering out unstable samples. Experiments on multiple mathematical reasoning benchmarks demonstrate that SAGE significantly accelerates convergence and outperforms static baselines, highlighting the critical role of policy-aware, stability-conscious data selection in reasoning alignment."
    },
    {
      "title": "Superman: Unifying Skeleton and Vision for Human Motion Perception and Generation",
      "url": "https://arxiv.org/abs/2602.02401",
      "date": "2026-02-02",
      "authors_text": "Xinshun Wang, Peiming Li, Ziyi Wang, Zhongbin Fang, Zhichao Deng",
      "is_highlight": false,
      "score": 15.0,
      "summary": "Superman unifies visual perception and skeleton-based motion generation through a Vision-Guided Motion Tokenizer, enabling robust cross-modal learning and superior performance in human motion tasks.",
      "teaser_image": "https://arxiv.org/html/2602.02401/x2.png",
      "debug_abstract": "Human motion analysis tasks, such as temporal 3D pose estimation, motion prediction, and motion in-betweening, play an essential role in computer vision. However, current paradigms suffer from severe fragmentation. First, the field is split between ``perception&#39;&#39; models that understand motion from video but only output text, and ``generation&#39;&#39; models that cannot perceive from raw visual input. Second, generative MLLMs are often limited to single-frame, static poses using dense, parametric SMPL models, failing to handle temporal motion. Third, existing motion vocabularies are built from skeleton data alone, severing the link to the visual domain. To address these challenges, we introduce Superman, a unified framework that bridges visual perception with temporal, skeleton-based motion generation. Our solution is twofold. First, to overcome the modality disconnect, we propose a Vision-Guided Motion Tokenizer. Leveraging the natural geometric alignment between 3D skeletons and visual data, this module pioneers robust joint learning from both modalities, creating a unified, cross-modal motion vocabulary. Second, grounded in this motion language, a single, unified MLLM architecture is trained to handle all tasks. This module flexibly processes diverse, temporal inputs, unifying 3D skeleton pose estimation from video (perception) with skeleton-based motion prediction and in-betweening (generation). Extensive experiments on standard benchmarks, including Human3.6M, demonstrate that our unified method achieves state-of-the-art or competitive performance across all motion tasks. This showcases a more efficient and scalable path for generative motion analysis using skeletons."
    },
    {
      "title": "Disturbance-Aware Flight Control of Robotic Gliding Blimp via Moving Mass Actuation",
      "url": "https://arxiv.org/abs/2601.21188",
      "date": "2026-01-29",
      "authors_text": "Hao Cheng, Feitian Zhang",
      "is_highlight": false,
      "score": 86.0,
      "summary": "This paper presents a disturbance-aware flight control system for robotic blimps using moving mass actuation and a combined moving horizon estimator and model predictive controller for enhanced stability.",
      "teaser_image": "https://arxiv.org/html/2601.21188/x2.png",
      "debug_abstract": "Robotic blimps, as lighter-than-air (LTA) aerial systems, offer long endurance and inherently safe operation but remain highly susceptible to wind disturbances. Building on recent advances in moving mass actuation, this paper addresses the lack of disturbance-aware control frameworks for LTA platforms by explicitly modeling and compensating for wind-induced effects. A moving horizon estimator (MHE) infers real-time wind perturbations and provides these estimates to a model predictive controller (MPC), enabling robust trajectory and heading regulation under varying wind conditions. The proposed approach leverages a two-degree-of-freedom (2-DoF) moving-mass mechanism to generate both inertial and aerodynamic moments for attitude and heading control, thereby enhancing flight stability in disturbance-prone environments. Extensive flight experiments under headwind and crosswind conditions show that the integrated MHE-MPC framework significantly outperforms baseline PID control, demonstrating its effectiveness for disturbance-aware LTA flight."
    },
    {
      "title": "GVGS: Gaussian Visibility-Aware Multi-View Geometry for Accurate Surface Reconstruction",
      "url": "https://arxiv.org/abs/2601.20331",
      "date": "2026-01-28",
      "authors_text": "Mai Su, Qihan Yu, Zhongtao Wang, Yilong Li, Chengwei Pan",
      "is_highlight": true,
      "score": 60.0,
      "summary": "The paper presents GVGS, a method enhancing 3D surface reconstruction accuracy by integrating Gaussian visibility-aware constraints and progressive monocular depth calibration to overcome existing limitations.",
      "teaser_image": "https://arxiv.org/html/2601.20331/x2.png",
      "debug_abstract": "3D Gaussian Splatting enables efficient optimization and high-quality rendering, yet accurate surface reconstruction remains challenging. Prior methods improve surface reconstruction by refining Gaussian depth estimates, either via multi-view geometric consistency or through monocular depth priors. However, multi-view constraints become unreliable under large geometric discrepancies, while monocular priors suffer from scale ambiguity and local inconsistency, ultimately leading to inaccurate Gaussian depth supervision. To address these limitations, we introduce a Gaussian visibility-aware multi-view geometric consistency constraint that aggregates the visibility of shared Gaussian primitives across views, enabling more accurate and stable geometric supervision. In addition, we propose a progressive quadtree-calibrated Monocular depth constraint that performs block-wise affine calibration from coarse to fine spatial scales, mitigating the scale ambiguity of depth priors while preserving fine-grained surface details. Extensive experiments on DTU and TNT datasets demonstrate consistent improvements in geometric accuracy over prior Gaussian-based and implicit surface reconstruction methods. Codes are available at an anonymous repository: this https URL."
    },
    {
      "title": "Uni-RS: A Spatially Faithful Unified Understanding and Generation Model for Remote Sensing",
      "url": "https://arxiv.org/abs/2601.17673",
      "date": "2026-01-25",
      "authors_text": "Weiyu Zhang, Yuan Hu, Yong Li, Yu Liu",
      "is_highlight": false,
      "score": 58.0,
      "summary": "Uni-RS is a novel unified multimodal model for remote sensing that enhances spatial faithfulness in text-to-image generation by integrating spatial layout planning and supervision techniques.",
      "debug_abstract": "Unified remote sensing multimodal models exhibit a pronounced spatial reversal curse: Although they can accurately recognize and describe object locations in images, they often fail to faithfully execute the same spatial relations during text-to-image generation, where such relations constitute core semantic information in remote sensing. Motivated by this observation, we propose Uni-RS, the first unified multimodal model tailored for remote sensing, to explicitly address the spatial asymmetry between understanding and generation. Specifically, we first introduce explicit Spatial-Layout Planning to transform textual instructions into spatial layout plans, decoupling geometric planning from visual synthesis. We then impose Spatial-Aware Query Supervision to bias learnable queries toward spatial relations explicitly specified in the instruction. Finally, we develop Image-Caption Spatial Layout Variation to expose the model to systematic geometry-consistent spatial transformations. Extensive experiments across multiple benchmarks show that our approach substantially improves spatial faithfulness in text-to-image generation, while maintaining strong performance on multimodal understanding tasks like image captioning, visual grounding, and VQA tasks."
    },
    {
      "title": "Agentic reinforcement learning empowers next-generation chemical language models for molecular design and synthesis",
      "url": "https://arxiv.org/abs/2601.17687",
      "date": "2026-01-25",
      "authors_text": "Hao Li, He Cao, Shenyao Peng, Zijing Liu, Bin Feng",
      "is_highlight": false,
      "score": 62.0,
      "summary": "ChemCRAFT utilizes agentic reinforcement learning to enhance small language models for efficient, cost-effective, and privacy-preserving molecular design and synthesis, outperforming larger cloud-based models.",
      "debug_abstract": "Language models are revolutionizing the biochemistry domain, assisting scientists in drug design and chemical synthesis with high efficiency. Yet current approaches struggle between small language models prone to hallucination and limited knowledge retention, and large cloud-based language models plagued by privacy risks and high inference costs. To bridge this gap, we introduce ChemCRAFT, a novel framework leveraging agentic reinforcement learning to decouple chemical reasoning from knowledge storage. Instead of forcing the model to memorize vast chemical data, our approach empowers the language model to interact with a sandbox for precise information retrieval. This externalization of knowledge allows a locally deployable small model to achieve superior performance with minimal inference costs. To enable small language models for agent-calling ability, we build an agentic trajectory construction pipeline and a comprehensive chemical-agent sandbox. Based on sandbox interactions, we constructed ChemToolDataset, the first large-scale chemical tool trajectory dataset. Simultaneously, we propose SMILES-GRPO to build a dense chemical reward function, promoting the model&#39;s ability to call chemical agents. Evaluations across diverse aspects of drug design show that ChemCRAFT outperforms current cloud-based LLMs in molecular structure analysis, molecular optimization, and synthesis pathway prediction, demonstrating that scientific reasoning is not solely an emergent ability of model scale, but a learnable policy of tool orchestration. This work establishes a cost-effective and privacy-preserving paradigm for AI-aided chemistry, opening new avenues for accelerating molecular discovery with locally deployable agents."
    },
    {
      "title": "TC-IDM: Grounding Video Generation for Executable Zero-shot Robot Motion",
      "url": "https://arxiv.org/abs/2601.18323",
      "date": "2026-01-26",
      "authors_text": "Weishi Mi, Yong Bao, Xiaowei Chi, Xiaozhu Ju, Zhiyuan Qin",
      "is_highlight": false,
      "score": 89.0,
      "summary": "The TC-IDM model enhances robot motion execution by bridging visual planning and physical control through tool-centric trajectory synthesis, achieving superior generalization and performance in diverse tasks.",
      "debug_abstract": "The vision-language-action (VLA) paradigm has enabled powerful robotic control by leveraging vision-language models, but its reliance on large-scale, high-quality robot data limits its generalization. Generative world models offer a promising alternative for general-purpose embodied AI, yet a critical gap remains between their pixel-level plans and physically executable actions. To this end, we propose the Tool-Centric Inverse Dynamics Model (TC-IDM). By focusing on the tool&#39;s imagined trajectory as synthesized by the world model, TC-IDM establishes a robust intermediate representation that bridges the gap between visual planning and physical control. TC-IDM extracts the tool&#39;s point cloud trajectories via segmentation and 3D motion estimation from generated videos. Considering diverse tool attributes, our architecture employs decoupled action heads to project these planned trajectories into 6-DoF end-effector motions and corresponding control signals. This plan-and-translate paradigm not only supports a wide range of end-effectors but also significantly improves viewpoint invariance. Furthermore, it exhibits strong generalization capabilities across long-horizon and out-of-distribution tasks, including interacting with deformable objects. In real-world evaluations, the world model with TC-IDM achieves an average success rate of 61.11 percent, with 77.7 percent on simple tasks and 38.46 percent on zero-shot deformable object tasks. It substantially outperforms end-to-end VLA-style baselines and other inverse dynamics models."
    },
    {
      "title": "Order from Chaos: Physical World Understanding from Glitchy Gameplay Videos",
      "url": "https://arxiv.org/abs/2601.16471",
      "date": "2026-01-23",
      "authors_text": "Meng Cao, Haoran Tang, Haoze Zhao, Mingfei Han, Ruyang Liu",
      "is_highlight": false,
      "score": 80.0,
      "summary": "This paper introduces PhysGame, a dataset leveraging gameplay glitches to enhance AI's understanding of physical principles, demonstrating improved reasoning capabilities through extensive experimentation.",
      "debug_abstract": "Understanding the physical world, including object dynamics, material properties, and causal interactions, remains a core challenge in artificial intelligence. Although recent multi-modal large language models (MLLMs) have demonstrated impressive general reasoning capabilities, they still fall short of achieving human-level understanding of physical principles. Existing datasets for physical reasoning either rely on real-world videos, which incur high annotation costs, or on synthetic simulations, which suffer from limited realism and diversity. In this paper, we propose a novel paradigm that leverages glitches in gameplay videos, referring to visual anomalies that violate predefined physical laws, as a rich and scalable supervision source for physical world understanding. We introduce PhysGame, an meta information guided instruction-tuning dataset containing 140,057 glitch-centric question-answer pairs across five physical domains and sixteen fine-grained categories. To ensure data accuracy, we design a prompting strategy that utilizes gameplay metadata such as titles and descriptions to guide high-quality QA generation. Complementing PhysGame, we construct GameBench, an expert-annotated benchmark with 880 glitch-identified gameplay videos designed to evaluate physical reasoning capabilities. Extensive experiments show that PhysGame significantly enhances both Game2Real transferability, improving the real world physical reasoning performance of Qwen2.5VL by 2.5% on PhysBench, and Game2General transferability, yielding a 1.9% gain on the MVBench benchmark. Moreover, PhysGame-tuned models achieve a 3.7% absolute improvement on GameBench, demonstrating enhanced robustness in detecting physical implausibilities. These results indicate that learning from gameplay anomalies offers a scalable and effective pathway toward advancing physical world understanding in multimodal intelligence."
    },
    {
      "title": "Render-of-Thought: Rendering Textual Chain-of-Thought as Images for Visual Latent Reasoning",
      "url": "https://arxiv.org/abs/2601.14750",
      "date": "2026-01-22",
      "authors_text": "Yifan Wang, Shiyu Li, Peiming Li, Xiaochen Yang, Yang Tang",
      "is_highlight": false,
      "score": 92.5,
      "summary": "The Render-of-Thought framework transforms Chain-of-Thought reasoning into images, enhancing analyzability and efficiency while maintaining competitive performance in reasoning tasks.",
      "debug_abstract": "Chain-of-Thought (CoT) prompting has achieved remarkable success in unlocking the reasoning capabilities of Large Language Models (LLMs). Although CoT prompting enhances reasoning, its verbosity imposes substantial computational overhead. Recent works often focus exclusively on outcome alignment and lack supervision on the intermediate reasoning process. These deficiencies obscure the analyzability of the latent reasoning chain. To address these challenges, we introduce Render-of-Thought (RoT), the first framework to reify the reasoning chain by rendering textual steps into images, making the latent rationale explicit and traceable. Specifically, we leverage the vision encoders of existing Vision Language Models (VLMs) as semantic anchors to align the vision embeddings with the textual space. This design ensures plug-and-play implementation without incurring additional pre-training overhead. Extensive experiments on mathematical and logical reasoning benchmarks demonstrate that our method achieves 3-4x token compression and substantial inference acceleration compared to explicit CoT. Furthermore, it maintains competitive performance against other methods, validating the feasibility of this paradigm. Our code is available at this https URL"
    },
    {
      "title": "Decoupling Return-to-Go for Efficient Decision Transformer",
      "url": "https://arxiv.org/abs/2601.15953",
      "date": "2026-01-22",
      "authors_text": "Yongyi Wang, Hanyu Liu, Lingfeng Li, Bozhou Chen, Ang Li",
      "is_highlight": false,
      "score": 75.2,
      "summary": "The Decoupled Decision Transformer (DDT) enhances offline reinforcement learning by using only the latest Return-to-Go for action prediction, improving performance and reducing computational costs.",
      "debug_abstract": "The Decision Transformer (DT) has established a powerful sequence modeling approach to offline reinforcement learning. It conditions its action predictions on Return-to-Go (RTG), using it both to distinguish trajectory quality during training and to guide action generation at inference. In this work, we identify a critical redundancy in this design: feeding the entire sequence of RTGs into the Transformer is theoretically unnecessary, as only the most recent RTG affects action prediction. We show that this redundancy can impair DT&#39;s performance through experiments. To resolve this, we propose the Decoupled DT (DDT). DDT simplifies the architecture by processing only observation and action sequences through the Transformer, using the latest RTG to guide the action prediction. This streamlined approach not only improves performance but also reduces computational cost. Our experiments show that DDT significantly outperforms DT and establishes competitive performance against state-of-the-art DT variants across multiple offline RL tasks."
    },
    {
      "title": "PhysicsMind: Sim and Real Mechanics Benchmarking for Physical Reasoning and Prediction in Foundational VLMs and World Models",
      "url": "https://arxiv.org/abs/2601.16007",
      "date": "2026-01-22",
      "authors_text": "Chak-Wing Mak, Guanyu Zhu, Boyi Zhang, Hongji Li, Xiaowei Chi",
      "is_highlight": false,
      "score": 94.1,
      "summary": "PhysicsMind introduces a benchmark for evaluating physical reasoning in multimodal models through real and simulated tasks focused on fundamental physics principles, revealing significant gaps in current models' understanding.",
      "debug_abstract": "Modern foundational Multimodal Large Language Models (MLLMs) and video world models have advanced significantly in mathematical, common-sense, and visual reasoning, but their grasp of the underlying physics remains underexplored. Existing benchmarks attempting to measure this matter rely on synthetic, Visual Question Answer templates or focus on perceptual video quality that is tangential to measuring how well the video abides by physical laws. To address this fragmentation, we introduce PhysicsMind, a unified benchmark with both real and simulation environments that evaluates law-consistent reasoning and generation over three canonical principles: Center of Mass, Lever Equilibrium, and Newton&#39;s First Law. PhysicsMind comprises two main tasks: i) VQA tasks, testing whether models can reason and determine physical quantities and values from images or short videos, and ii) Video Generation(VG) tasks, evaluating if predicted motion trajectories obey the same center-of-mass, torque, and inertial constraints as the ground truth. A broad range of recent models and video generation models is evaluated on PhysicsMind and found to rely on appearance heuristics while often violating basic mechanics. These gaps indicate that current scaling and training are still insufficient for robust physical understanding, underscoring PhysicsMind as a focused testbed for physics-aware multimodal models. Our data will be released upon acceptance."
    },
    {
      "title": "Collision-Free Humanoid Traversal in Cluttered Indoor Scenes",
      "url": "https://arxiv.org/abs/2601.16035",
      "date": "2026-01-22",
      "authors_text": "Han Xue, Sikai Liang, Zhikai Zhang, Zicheng Zeng, Yun Liu",
      "is_highlight": false,
      "score": 77.3,
      "summary": "The paper presents Humanoid Potential Field (HumanoidPF) for enabling collision-free traversal of humanoids in cluttered indoor environments, enhancing RL-based skill learning and real-world application.",
      "debug_abstract": "We study the problem of collision-free humanoid traversal in cluttered indoor scenes, such as hurdling over objects scattered on the floor, crouching under low-hanging obstacles, or squeezing through narrow passages. To achieve this goal, the humanoid needs to map its perception of surrounding obstacles with diverse spatial layouts and geometries to the corresponding traversal skills. However, the lack of an effective representation that captures humanoid-obstacle relationships during collision avoidance makes directly learning such mappings difficult. We therefore propose Humanoid Potential Field (HumanoidPF), which encodes these relationships as collision-free motion directions, significantly facilitating RL-based traversal skill learning. We also find that HumanoidPF exhibits a surprisingly negligible sim-to-real gap as a perceptual representation. To further enable generalizable traversal skills through diverse and challenging cluttered indoor scenes, we further propose a hybrid scene generation method, incorporating crops of realistic 3D indoor scenes and procedurally synthesized obstacles. We successfully transfer our policy to the real world and develop a teleoperation system where users could command the humanoid to traverse in cluttered indoor scenes with just a single click. Extensive experiments are conducted in both simulation and the real world to validate the effectiveness of our method. Demos and code can be found in our website: this https URL."
    }
  ],
  "智元机器人联合实验室 (PKU-Agibot)": [
    {
      "title": "Multi-Resolution Alignment for Voxel Sparsity in Camera-Based 3D Semantic Scene Completion",
      "url": "https://arxiv.org/abs/2602.03371",
      "date": "2026-02-03",
      "authors_text": "Zhiwen Yang, Yuxin Peng",
      "is_highlight": false,
      "score": 61.0,
      "summary": "The paper presents a Multi-Resolution Alignment approach to enhance camera-based 3D semantic scene completion by addressing voxel sparsity through scene and instance-level feature alignment.",
      "teaser_image": "https://arxiv.org/html/2602.03371/x2.png",
      "debug_abstract": "Camera-based 3D semantic scene completion (SSC) offers a cost-effective solution for assessing the geometric occupancy and semantic labels of each voxel in the surrounding 3D scene with image inputs, providing a voxel-level scene perception foundation for the perception-prediction-planning autonomous driving systems. Although significant progress has been made in existing methods, their optimization rely solely on the supervision from voxel labels and face the challenge of voxel sparsity as a large portion of voxels in autonomous driving scenarios are empty, which limits both optimization efficiency and model performance. To address this issue, we propose a \\textit{Multi-Resolution Alignment (MRA)} approach to mitigate voxel sparsity in camera-based 3D semantic scene completion, which exploits the scene and instance level alignment across multi-resolution 3D features as auxiliary supervision. Specifically, we first propose the Multi-resolution View Transformer module, which projects 2D image features into multi-resolution 3D features and aligns them at the scene level through fusing discriminative seed features. Furthermore, we design the Cubic Semantic Anisotropy module to identify the instance-level semantic significance of each voxel, accounting for the semantic differences of a specific voxel against its neighboring voxels within a cubic area. Finally, we devise a Critical Distribution Alignment module, which selects critical voxels as instance-level anchors with the guidance of cubic semantic anisotropy, and applies a circulated loss for auxiliary supervision on the critical feature distribution consistency across different resolutions. The code is available at this https URL."
    },
    {
      "title": "Learning Physics-Grounded 4D Dynamics with Neural Gaussian Force Fields",
      "url": "https://arxiv.org/abs/2602.00148",
      "date": "2026-01-29",
      "authors_text": "Shiqian Li, Ruihong Shen, Junfeng Ni, Chang Pan, Chi Zhang",
      "is_highlight": false,
      "score": 82.0,
      "summary": "The Neural Gaussian Force Field (NGFF) framework enables rapid generation of interactive, physically realistic 4D videos from multi-view RGB inputs, enhancing robustness and generalization in physical dynamics.",
      "teaser_image": "https://arxiv.org/html/2602.00148/x1.png",
      "debug_abstract": "Predicting physical dynamics from raw visual data remains a major challenge in AI. While recent video generation models have achieved impressive visual quality, they still cannot consistently generate physically plausible videos due to a lack of modeling of physical laws. Recent approaches combining 3D Gaussian splatting and physics engines can produce physically plausible videos, but are hindered by high computational costs in both reconstruction and simulation, and often lack robustness in complex real-world scenarios. To address these issues, we introduce Neural Gaussian Force Field (NGFF), an end-to-end neural framework that integrates 3D Gaussian perception with physics-based dynamic modeling to generate interactive, physically realistic 4D videos from multi-view RGB inputs, achieving two orders of magnitude faster than prior Gaussian simulators. To support training, we also present GSCollision, a 4D Gaussian dataset featuring diverse materials, multi-object interactions, and complex scenes, totaling over 640k rendered physical videos (~4 TB). Evaluations on synthetic and real 3D scenarios show NGFF&#39;s strong generalization and robustness in physical reasoning, advancing video prediction towards physics-grounded world models."
    },
    {
      "title": "Resource-Efficient Reinforcement for Reasoning Large Language Models via Dynamic One-Shot Policy Refinement",
      "url": "https://arxiv.org/abs/2602.00815",
      "date": "2026-01-31",
      "authors_text": "Yunjian Zhang, Sudong Wang, Yang Li, Peiran Xu, Conghao Zhou",
      "is_highlight": false,
      "score": 57.0,
      "summary": "The paper introduces Dynamic One-Shot Policy Refinement (DoPR), a resource-efficient reinforcement learning strategy that enhances reasoning in large language models while significantly reducing training costs.",
      "teaser_image": "https://arxiv.org/html/2602.00815/dopr.png",
      "debug_abstract": "Large language models (LLMs) have exhibited remarkable performance on complex reasoning tasks, with reinforcement learning under verifiable rewards (RLVR) emerging as a principled framework for aligning model behavior with reasoning chains. Despite its promise, RLVR remains prohibitively resource-intensive, requiring extensive reward signals and incurring substantial rollout costs during training. In this work, we revisit the fundamental question of data and compute efficiency in RLVR. We first establish a theoretical lower bound on the sample complexity required to unlock reasoning capabilities, and empirically validate that strong performance can be achieved with a surprisingly small number of training instances. To tackle the computational burden, we propose Dynamic One-Shot Policy Refinement (DoPR), an uncertainty-aware RL strategy that dynamically selects a single informative training sample per batch for policy updates, guided by reward volatility and exploration-driven acquisition. DoPR reduces rollout overhead by nearly an order of magnitude while preserving competitive reasoning accuracy, offering a scalable and resource-efficient solution for LLM post-training. This approach offers a practical path toward more efficient and accessible RL-based training for reasoning-intensive LLM applications."
    },
    {
      "title": "Residual Decoding: Mitigating Hallucinations in Large Vision-Language Models via History-Aware Residual Guidance",
      "url": "https://arxiv.org/abs/2602.01047",
      "date": "2026-02-01",
      "authors_text": "Xinrong Chen, Xu Chu, Yingmin Qiu, Hengyuan Zhang, Jing Xiong",
      "is_highlight": false,
      "score": 53.0,
      "summary": "Residual Decoding (ResDec) mitigates hallucinations in Large Vision-Language Models by leveraging historical information to enhance decoding and improve visual grounding.",
      "teaser_image": "https://arxiv.org/html/2602.01047/x1.png",
      "debug_abstract": "Large Vision-Language Models (LVLMs) can reason effectively from image-text inputs and perform well in various multimodal tasks. Despite this success, they are affected by language priors and often produce hallucinations. Hallucinations denote generated content that is grammatically and syntactically coherent, yet bears no match or direct relevance to actual visual input. To address this problem, we propose Residual Decoding (ResDec). It is a novel training-free method that uses historical information to aid decoding. The method relies on the internal implicit reasoning mechanism and token logits evolution mechanism of LVLMs to correct biases. Extensive experiments demonstrate that ResDec effectively suppresses hallucinations induced by language priors, significantly improves visual grounding, and reduces object hallucinations. In addition to mitigating hallucinations, ResDec also performs exceptionally well on comprehensive LVLM benchmarks, highlighting its broad applicability."
    },
    {
      "title": "Not All Preferences Are Created Equal: Stability-Aware and Gradient-Efficient Alignment for Reasoning Models",
      "url": "https://arxiv.org/abs/2602.01207",
      "date": "2026-02-01",
      "authors_text": "Hui Wu, Hengyi Cai, Jinman Zhao, Xinran Chen, Ziheng Li",
      "is_highlight": false,
      "score": 48.0,
      "summary": "The paper introduces SAGE, a dynamic framework for preference-based alignment in reasoning models that enhances optimization stability and efficiency by prioritizing informative training instances.",
      "teaser_image": "https://arxiv.org/html/2602.01207/x2.png",
      "debug_abstract": "Preference-based alignment is pivotal for training large reasoning models; however, standard methods like Direct Preference Optimization (DPO) typically treat all preference pairs uniformly, overlooking the evolving utility of training instances. This static approach often leads to inefficient or unstable optimization, as it wastes computation on trivial pairs with negligible gradients and suffers from noise induced by samples near uncertain decision boundaries. Facing these challenges, we propose SAGE (Stability-Aware Gradient Efficiency), a dynamic framework designed to enhance alignment reliability by maximizing the Signal-to-Noise Ratio of policy updates. Concretely, SAGE integrates a coarse-grained curriculum mechanism that refreshes candidate pools based on model competence with a fine-grained, stability-aware scoring function that prioritizes informative, confident errors while filtering out unstable samples. Experiments on multiple mathematical reasoning benchmarks demonstrate that SAGE significantly accelerates convergence and outperforms static baselines, highlighting the critical role of policy-aware, stability-conscious data selection in reasoning alignment."
    },
    {
      "title": "Superman: Unifying Skeleton and Vision for Human Motion Perception and Generation",
      "url": "https://arxiv.org/abs/2602.02401",
      "date": "2026-02-02",
      "authors_text": "Xinshun Wang, Peiming Li, Ziyi Wang, Zhongbin Fang, Zhichao Deng",
      "is_highlight": false,
      "score": 15.0,
      "summary": "Superman unifies visual perception and skeleton-based motion generation through a Vision-Guided Motion Tokenizer, enabling robust cross-modal learning and superior performance in human motion tasks.",
      "teaser_image": "https://arxiv.org/html/2602.02401/x2.png",
      "debug_abstract": "Human motion analysis tasks, such as temporal 3D pose estimation, motion prediction, and motion in-betweening, play an essential role in computer vision. However, current paradigms suffer from severe fragmentation. First, the field is split between ``perception&#39;&#39; models that understand motion from video but only output text, and ``generation&#39;&#39; models that cannot perceive from raw visual input. Second, generative MLLMs are often limited to single-frame, static poses using dense, parametric SMPL models, failing to handle temporal motion. Third, existing motion vocabularies are built from skeleton data alone, severing the link to the visual domain. To address these challenges, we introduce Superman, a unified framework that bridges visual perception with temporal, skeleton-based motion generation. Our solution is twofold. First, to overcome the modality disconnect, we propose a Vision-Guided Motion Tokenizer. Leveraging the natural geometric alignment between 3D skeletons and visual data, this module pioneers robust joint learning from both modalities, creating a unified, cross-modal motion vocabulary. Second, grounded in this motion language, a single, unified MLLM architecture is trained to handle all tasks. This module flexibly processes diverse, temporal inputs, unifying 3D skeleton pose estimation from video (perception) with skeleton-based motion prediction and in-betweening (generation). Extensive experiments on standard benchmarks, including Human3.6M, demonstrate that our unified method achieves state-of-the-art or competitive performance across all motion tasks. This showcases a more efficient and scalable path for generative motion analysis using skeletons."
    },
    {
      "title": "Disturbance-Aware Flight Control of Robotic Gliding Blimp via Moving Mass Actuation",
      "url": "https://arxiv.org/abs/2601.21188",
      "date": "2026-01-29",
      "authors_text": "Hao Cheng, Feitian Zhang",
      "is_highlight": false,
      "score": 86.0,
      "summary": "This paper presents a disturbance-aware flight control system for robotic blimps using moving mass actuation and a combined moving horizon estimator and model predictive controller for enhanced stability.",
      "teaser_image": "https://arxiv.org/html/2601.21188/x2.png",
      "debug_abstract": "Robotic blimps, as lighter-than-air (LTA) aerial systems, offer long endurance and inherently safe operation but remain highly susceptible to wind disturbances. Building on recent advances in moving mass actuation, this paper addresses the lack of disturbance-aware control frameworks for LTA platforms by explicitly modeling and compensating for wind-induced effects. A moving horizon estimator (MHE) infers real-time wind perturbations and provides these estimates to a model predictive controller (MPC), enabling robust trajectory and heading regulation under varying wind conditions. The proposed approach leverages a two-degree-of-freedom (2-DoF) moving-mass mechanism to generate both inertial and aerodynamic moments for attitude and heading control, thereby enhancing flight stability in disturbance-prone environments. Extensive flight experiments under headwind and crosswind conditions show that the integrated MHE-MPC framework significantly outperforms baseline PID control, demonstrating its effectiveness for disturbance-aware LTA flight."
    },
    {
      "title": "GVGS: Gaussian Visibility-Aware Multi-View Geometry for Accurate Surface Reconstruction",
      "url": "https://arxiv.org/abs/2601.20331",
      "date": "2026-01-28",
      "authors_text": "Mai Su, Qihan Yu, Zhongtao Wang, Yilong Li, Chengwei Pan",
      "is_highlight": true,
      "score": 60.0,
      "summary": "The paper presents GVGS, a method enhancing 3D surface reconstruction accuracy by integrating Gaussian visibility-aware constraints and progressive monocular depth calibration to overcome existing limitations.",
      "teaser_image": "https://arxiv.org/html/2601.20331/x2.png",
      "debug_abstract": "3D Gaussian Splatting enables efficient optimization and high-quality rendering, yet accurate surface reconstruction remains challenging. Prior methods improve surface reconstruction by refining Gaussian depth estimates, either via multi-view geometric consistency or through monocular depth priors. However, multi-view constraints become unreliable under large geometric discrepancies, while monocular priors suffer from scale ambiguity and local inconsistency, ultimately leading to inaccurate Gaussian depth supervision. To address these limitations, we introduce a Gaussian visibility-aware multi-view geometric consistency constraint that aggregates the visibility of shared Gaussian primitives across views, enabling more accurate and stable geometric supervision. In addition, we propose a progressive quadtree-calibrated Monocular depth constraint that performs block-wise affine calibration from coarse to fine spatial scales, mitigating the scale ambiguity of depth priors while preserving fine-grained surface details. Extensive experiments on DTU and TNT datasets demonstrate consistent improvements in geometric accuracy over prior Gaussian-based and implicit surface reconstruction methods. Codes are available at an anonymous repository: this https URL."
    },
    {
      "title": "Uni-RS: A Spatially Faithful Unified Understanding and Generation Model for Remote Sensing",
      "url": "https://arxiv.org/abs/2601.17673",
      "date": "2026-01-25",
      "authors_text": "Weiyu Zhang, Yuan Hu, Yong Li, Yu Liu",
      "is_highlight": false,
      "score": 58.0,
      "summary": "Uni-RS is a novel unified multimodal model for remote sensing that enhances spatial faithfulness in text-to-image generation by integrating spatial layout planning and supervision techniques.",
      "debug_abstract": "Unified remote sensing multimodal models exhibit a pronounced spatial reversal curse: Although they can accurately recognize and describe object locations in images, they often fail to faithfully execute the same spatial relations during text-to-image generation, where such relations constitute core semantic information in remote sensing. Motivated by this observation, we propose Uni-RS, the first unified multimodal model tailored for remote sensing, to explicitly address the spatial asymmetry between understanding and generation. Specifically, we first introduce explicit Spatial-Layout Planning to transform textual instructions into spatial layout plans, decoupling geometric planning from visual synthesis. We then impose Spatial-Aware Query Supervision to bias learnable queries toward spatial relations explicitly specified in the instruction. Finally, we develop Image-Caption Spatial Layout Variation to expose the model to systematic geometry-consistent spatial transformations. Extensive experiments across multiple benchmarks show that our approach substantially improves spatial faithfulness in text-to-image generation, while maintaining strong performance on multimodal understanding tasks like image captioning, visual grounding, and VQA tasks."
    },
    {
      "title": "Agentic reinforcement learning empowers next-generation chemical language models for molecular design and synthesis",
      "url": "https://arxiv.org/abs/2601.17687",
      "date": "2026-01-25",
      "authors_text": "Hao Li, He Cao, Shenyao Peng, Zijing Liu, Bin Feng",
      "is_highlight": false,
      "score": 62.0,
      "summary": "ChemCRAFT utilizes agentic reinforcement learning to enhance small language models for efficient, cost-effective, and privacy-preserving molecular design and synthesis, outperforming larger cloud-based models.",
      "debug_abstract": "Language models are revolutionizing the biochemistry domain, assisting scientists in drug design and chemical synthesis with high efficiency. Yet current approaches struggle between small language models prone to hallucination and limited knowledge retention, and large cloud-based language models plagued by privacy risks and high inference costs. To bridge this gap, we introduce ChemCRAFT, a novel framework leveraging agentic reinforcement learning to decouple chemical reasoning from knowledge storage. Instead of forcing the model to memorize vast chemical data, our approach empowers the language model to interact with a sandbox for precise information retrieval. This externalization of knowledge allows a locally deployable small model to achieve superior performance with minimal inference costs. To enable small language models for agent-calling ability, we build an agentic trajectory construction pipeline and a comprehensive chemical-agent sandbox. Based on sandbox interactions, we constructed ChemToolDataset, the first large-scale chemical tool trajectory dataset. Simultaneously, we propose SMILES-GRPO to build a dense chemical reward function, promoting the model&#39;s ability to call chemical agents. Evaluations across diverse aspects of drug design show that ChemCRAFT outperforms current cloud-based LLMs in molecular structure analysis, molecular optimization, and synthesis pathway prediction, demonstrating that scientific reasoning is not solely an emergent ability of model scale, but a learnable policy of tool orchestration. This work establishes a cost-effective and privacy-preserving paradigm for AI-aided chemistry, opening new avenues for accelerating molecular discovery with locally deployable agents."
    },
    {
      "title": "TC-IDM: Grounding Video Generation for Executable Zero-shot Robot Motion",
      "url": "https://arxiv.org/abs/2601.18323",
      "date": "2026-01-26",
      "authors_text": "Weishi Mi, Yong Bao, Xiaowei Chi, Xiaozhu Ju, Zhiyuan Qin",
      "is_highlight": false,
      "score": 89.0,
      "summary": "The TC-IDM model enhances robot motion execution by bridging visual planning and physical control through tool-centric trajectory synthesis, achieving superior generalization and performance in diverse tasks.",
      "debug_abstract": "The vision-language-action (VLA) paradigm has enabled powerful robotic control by leveraging vision-language models, but its reliance on large-scale, high-quality robot data limits its generalization. Generative world models offer a promising alternative for general-purpose embodied AI, yet a critical gap remains between their pixel-level plans and physically executable actions. To this end, we propose the Tool-Centric Inverse Dynamics Model (TC-IDM). By focusing on the tool&#39;s imagined trajectory as synthesized by the world model, TC-IDM establishes a robust intermediate representation that bridges the gap between visual planning and physical control. TC-IDM extracts the tool&#39;s point cloud trajectories via segmentation and 3D motion estimation from generated videos. Considering diverse tool attributes, our architecture employs decoupled action heads to project these planned trajectories into 6-DoF end-effector motions and corresponding control signals. This plan-and-translate paradigm not only supports a wide range of end-effectors but also significantly improves viewpoint invariance. Furthermore, it exhibits strong generalization capabilities across long-horizon and out-of-distribution tasks, including interacting with deformable objects. In real-world evaluations, the world model with TC-IDM achieves an average success rate of 61.11 percent, with 77.7 percent on simple tasks and 38.46 percent on zero-shot deformable object tasks. It substantially outperforms end-to-end VLA-style baselines and other inverse dynamics models."
    },
    {
      "title": "Order from Chaos: Physical World Understanding from Glitchy Gameplay Videos",
      "url": "https://arxiv.org/abs/2601.16471",
      "date": "2026-01-23",
      "authors_text": "Meng Cao, Haoran Tang, Haoze Zhao, Mingfei Han, Ruyang Liu",
      "is_highlight": false,
      "score": 80.0,
      "summary": "This paper introduces PhysGame, a dataset leveraging gameplay glitches to enhance AI's understanding of physical principles, demonstrating improved reasoning capabilities through extensive experimentation.",
      "debug_abstract": "Understanding the physical world, including object dynamics, material properties, and causal interactions, remains a core challenge in artificial intelligence. Although recent multi-modal large language models (MLLMs) have demonstrated impressive general reasoning capabilities, they still fall short of achieving human-level understanding of physical principles. Existing datasets for physical reasoning either rely on real-world videos, which incur high annotation costs, or on synthetic simulations, which suffer from limited realism and diversity. In this paper, we propose a novel paradigm that leverages glitches in gameplay videos, referring to visual anomalies that violate predefined physical laws, as a rich and scalable supervision source for physical world understanding. We introduce PhysGame, an meta information guided instruction-tuning dataset containing 140,057 glitch-centric question-answer pairs across five physical domains and sixteen fine-grained categories. To ensure data accuracy, we design a prompting strategy that utilizes gameplay metadata such as titles and descriptions to guide high-quality QA generation. Complementing PhysGame, we construct GameBench, an expert-annotated benchmark with 880 glitch-identified gameplay videos designed to evaluate physical reasoning capabilities. Extensive experiments show that PhysGame significantly enhances both Game2Real transferability, improving the real world physical reasoning performance of Qwen2.5VL by 2.5% on PhysBench, and Game2General transferability, yielding a 1.9% gain on the MVBench benchmark. Moreover, PhysGame-tuned models achieve a 3.7% absolute improvement on GameBench, demonstrating enhanced robustness in detecting physical implausibilities. These results indicate that learning from gameplay anomalies offers a scalable and effective pathway toward advancing physical world understanding in multimodal intelligence."
    },
    {
      "title": "Render-of-Thought: Rendering Textual Chain-of-Thought as Images for Visual Latent Reasoning",
      "url": "https://arxiv.org/abs/2601.14750",
      "date": "2026-01-22",
      "authors_text": "Yifan Wang, Shiyu Li, Peiming Li, Xiaochen Yang, Yang Tang",
      "is_highlight": false,
      "score": 92.5,
      "summary": "The Render-of-Thought framework transforms Chain-of-Thought reasoning into images, enhancing analyzability and efficiency while maintaining competitive performance in reasoning tasks.",
      "debug_abstract": "Chain-of-Thought (CoT) prompting has achieved remarkable success in unlocking the reasoning capabilities of Large Language Models (LLMs). Although CoT prompting enhances reasoning, its verbosity imposes substantial computational overhead. Recent works often focus exclusively on outcome alignment and lack supervision on the intermediate reasoning process. These deficiencies obscure the analyzability of the latent reasoning chain. To address these challenges, we introduce Render-of-Thought (RoT), the first framework to reify the reasoning chain by rendering textual steps into images, making the latent rationale explicit and traceable. Specifically, we leverage the vision encoders of existing Vision Language Models (VLMs) as semantic anchors to align the vision embeddings with the textual space. This design ensures plug-and-play implementation without incurring additional pre-training overhead. Extensive experiments on mathematical and logical reasoning benchmarks demonstrate that our method achieves 3-4x token compression and substantial inference acceleration compared to explicit CoT. Furthermore, it maintains competitive performance against other methods, validating the feasibility of this paradigm. Our code is available at this https URL"
    },
    {
      "title": "Decoupling Return-to-Go for Efficient Decision Transformer",
      "url": "https://arxiv.org/abs/2601.15953",
      "date": "2026-01-22",
      "authors_text": "Yongyi Wang, Hanyu Liu, Lingfeng Li, Bozhou Chen, Ang Li",
      "is_highlight": false,
      "score": 75.2,
      "summary": "The Decoupled Decision Transformer (DDT) enhances offline reinforcement learning by using only the latest Return-to-Go for action prediction, improving performance and reducing computational costs.",
      "debug_abstract": "The Decision Transformer (DT) has established a powerful sequence modeling approach to offline reinforcement learning. It conditions its action predictions on Return-to-Go (RTG), using it both to distinguish trajectory quality during training and to guide action generation at inference. In this work, we identify a critical redundancy in this design: feeding the entire sequence of RTGs into the Transformer is theoretically unnecessary, as only the most recent RTG affects action prediction. We show that this redundancy can impair DT&#39;s performance through experiments. To resolve this, we propose the Decoupled DT (DDT). DDT simplifies the architecture by processing only observation and action sequences through the Transformer, using the latest RTG to guide the action prediction. This streamlined approach not only improves performance but also reduces computational cost. Our experiments show that DDT significantly outperforms DT and establishes competitive performance against state-of-the-art DT variants across multiple offline RL tasks."
    },
    {
      "title": "PhysicsMind: Sim and Real Mechanics Benchmarking for Physical Reasoning and Prediction in Foundational VLMs and World Models",
      "url": "https://arxiv.org/abs/2601.16007",
      "date": "2026-01-22",
      "authors_text": "Chak-Wing Mak, Guanyu Zhu, Boyi Zhang, Hongji Li, Xiaowei Chi",
      "is_highlight": false,
      "score": 94.1,
      "summary": "PhysicsMind introduces a benchmark for evaluating physical reasoning in multimodal models through real and simulated tasks focused on fundamental physics principles, revealing significant gaps in current models' understanding.",
      "debug_abstract": "Modern foundational Multimodal Large Language Models (MLLMs) and video world models have advanced significantly in mathematical, common-sense, and visual reasoning, but their grasp of the underlying physics remains underexplored. Existing benchmarks attempting to measure this matter rely on synthetic, Visual Question Answer templates or focus on perceptual video quality that is tangential to measuring how well the video abides by physical laws. To address this fragmentation, we introduce PhysicsMind, a unified benchmark with both real and simulation environments that evaluates law-consistent reasoning and generation over three canonical principles: Center of Mass, Lever Equilibrium, and Newton&#39;s First Law. PhysicsMind comprises two main tasks: i) VQA tasks, testing whether models can reason and determine physical quantities and values from images or short videos, and ii) Video Generation(VG) tasks, evaluating if predicted motion trajectories obey the same center-of-mass, torque, and inertial constraints as the ground truth. A broad range of recent models and video generation models is evaluated on PhysicsMind and found to rely on appearance heuristics while often violating basic mechanics. These gaps indicate that current scaling and training are still insufficient for robust physical understanding, underscoring PhysicsMind as a focused testbed for physics-aware multimodal models. Our data will be released upon acceptance."
    },
    {
      "title": "Collision-Free Humanoid Traversal in Cluttered Indoor Scenes",
      "url": "https://arxiv.org/abs/2601.16035",
      "date": "2026-01-22",
      "authors_text": "Han Xue, Sikai Liang, Zhikai Zhang, Zicheng Zeng, Yun Liu",
      "is_highlight": false,
      "score": 77.3,
      "summary": "The paper presents Humanoid Potential Field (HumanoidPF) for enabling collision-free traversal of humanoids in cluttered indoor environments, enhancing RL-based skill learning and real-world application.",
      "debug_abstract": "We study the problem of collision-free humanoid traversal in cluttered indoor scenes, such as hurdling over objects scattered on the floor, crouching under low-hanging obstacles, or squeezing through narrow passages. To achieve this goal, the humanoid needs to map its perception of surrounding obstacles with diverse spatial layouts and geometries to the corresponding traversal skills. However, the lack of an effective representation that captures humanoid-obstacle relationships during collision avoidance makes directly learning such mappings difficult. We therefore propose Humanoid Potential Field (HumanoidPF), which encodes these relationships as collision-free motion directions, significantly facilitating RL-based traversal skill learning. We also find that HumanoidPF exhibits a surprisingly negligible sim-to-real gap as a perceptual representation. To further enable generalizable traversal skills through diverse and challenging cluttered indoor scenes, we further propose a hybrid scene generation method, incorporating crops of realistic 3D indoor scenes and procedurally synthesized obstacles. We successfully transfer our policy to the real world and develop a teleoperation system where users could command the humanoid to traverse in cluttered indoor scenes with just a single click. Extensive experiments are conducted in both simulation and the real world to validate the effectiveness of our method. Demos and code can be found in our website: this https URL."
    }
  ],
  "情感与认知智能机器人实验室 (ACIR)": [
    {
      "title": "Multi-Resolution Alignment for Voxel Sparsity in Camera-Based 3D Semantic Scene Completion",
      "url": "https://arxiv.org/abs/2602.03371",
      "date": "2026-02-03",
      "authors_text": "Zhiwen Yang, Yuxin Peng",
      "is_highlight": false,
      "score": 61.0,
      "summary": "The paper presents a Multi-Resolution Alignment approach to enhance camera-based 3D semantic scene completion by addressing voxel sparsity through scene and instance-level feature alignment.",
      "teaser_image": "https://arxiv.org/html/2602.03371/x2.png",
      "debug_abstract": "Camera-based 3D semantic scene completion (SSC) offers a cost-effective solution for assessing the geometric occupancy and semantic labels of each voxel in the surrounding 3D scene with image inputs, providing a voxel-level scene perception foundation for the perception-prediction-planning autonomous driving systems. Although significant progress has been made in existing methods, their optimization rely solely on the supervision from voxel labels and face the challenge of voxel sparsity as a large portion of voxels in autonomous driving scenarios are empty, which limits both optimization efficiency and model performance. To address this issue, we propose a \\textit{Multi-Resolution Alignment (MRA)} approach to mitigate voxel sparsity in camera-based 3D semantic scene completion, which exploits the scene and instance level alignment across multi-resolution 3D features as auxiliary supervision. Specifically, we first propose the Multi-resolution View Transformer module, which projects 2D image features into multi-resolution 3D features and aligns them at the scene level through fusing discriminative seed features. Furthermore, we design the Cubic Semantic Anisotropy module to identify the instance-level semantic significance of each voxel, accounting for the semantic differences of a specific voxel against its neighboring voxels within a cubic area. Finally, we devise a Critical Distribution Alignment module, which selects critical voxels as instance-level anchors with the guidance of cubic semantic anisotropy, and applies a circulated loss for auxiliary supervision on the critical feature distribution consistency across different resolutions. The code is available at this https URL."
    },
    {
      "title": "Learning Physics-Grounded 4D Dynamics with Neural Gaussian Force Fields",
      "url": "https://arxiv.org/abs/2602.00148",
      "date": "2026-01-29",
      "authors_text": "Shiqian Li, Ruihong Shen, Junfeng Ni, Chang Pan, Chi Zhang",
      "is_highlight": false,
      "score": 82.0,
      "summary": "The Neural Gaussian Force Field (NGFF) framework enables rapid generation of interactive, physically realistic 4D videos from multi-view RGB inputs, enhancing robustness and generalization in physical dynamics.",
      "teaser_image": "https://arxiv.org/html/2602.00148/x1.png",
      "debug_abstract": "Predicting physical dynamics from raw visual data remains a major challenge in AI. While recent video generation models have achieved impressive visual quality, they still cannot consistently generate physically plausible videos due to a lack of modeling of physical laws. Recent approaches combining 3D Gaussian splatting and physics engines can produce physically plausible videos, but are hindered by high computational costs in both reconstruction and simulation, and often lack robustness in complex real-world scenarios. To address these issues, we introduce Neural Gaussian Force Field (NGFF), an end-to-end neural framework that integrates 3D Gaussian perception with physics-based dynamic modeling to generate interactive, physically realistic 4D videos from multi-view RGB inputs, achieving two orders of magnitude faster than prior Gaussian simulators. To support training, we also present GSCollision, a 4D Gaussian dataset featuring diverse materials, multi-object interactions, and complex scenes, totaling over 640k rendered physical videos (~4 TB). Evaluations on synthetic and real 3D scenarios show NGFF&#39;s strong generalization and robustness in physical reasoning, advancing video prediction towards physics-grounded world models."
    },
    {
      "title": "Resource-Efficient Reinforcement for Reasoning Large Language Models via Dynamic One-Shot Policy Refinement",
      "url": "https://arxiv.org/abs/2602.00815",
      "date": "2026-01-31",
      "authors_text": "Yunjian Zhang, Sudong Wang, Yang Li, Peiran Xu, Conghao Zhou",
      "is_highlight": false,
      "score": 57.0,
      "summary": "The paper introduces Dynamic One-Shot Policy Refinement (DoPR), a resource-efficient reinforcement learning strategy that enhances reasoning in large language models while significantly reducing training costs.",
      "teaser_image": "https://arxiv.org/html/2602.00815/dopr.png",
      "debug_abstract": "Large language models (LLMs) have exhibited remarkable performance on complex reasoning tasks, with reinforcement learning under verifiable rewards (RLVR) emerging as a principled framework for aligning model behavior with reasoning chains. Despite its promise, RLVR remains prohibitively resource-intensive, requiring extensive reward signals and incurring substantial rollout costs during training. In this work, we revisit the fundamental question of data and compute efficiency in RLVR. We first establish a theoretical lower bound on the sample complexity required to unlock reasoning capabilities, and empirically validate that strong performance can be achieved with a surprisingly small number of training instances. To tackle the computational burden, we propose Dynamic One-Shot Policy Refinement (DoPR), an uncertainty-aware RL strategy that dynamically selects a single informative training sample per batch for policy updates, guided by reward volatility and exploration-driven acquisition. DoPR reduces rollout overhead by nearly an order of magnitude while preserving competitive reasoning accuracy, offering a scalable and resource-efficient solution for LLM post-training. This approach offers a practical path toward more efficient and accessible RL-based training for reasoning-intensive LLM applications."
    },
    {
      "title": "Residual Decoding: Mitigating Hallucinations in Large Vision-Language Models via History-Aware Residual Guidance",
      "url": "https://arxiv.org/abs/2602.01047",
      "date": "2026-02-01",
      "authors_text": "Xinrong Chen, Xu Chu, Yingmin Qiu, Hengyuan Zhang, Jing Xiong",
      "is_highlight": false,
      "score": 53.0,
      "summary": "Residual Decoding (ResDec) mitigates hallucinations in Large Vision-Language Models by leveraging historical information to enhance decoding and improve visual grounding.",
      "teaser_image": "https://arxiv.org/html/2602.01047/x1.png",
      "debug_abstract": "Large Vision-Language Models (LVLMs) can reason effectively from image-text inputs and perform well in various multimodal tasks. Despite this success, they are affected by language priors and often produce hallucinations. Hallucinations denote generated content that is grammatically and syntactically coherent, yet bears no match or direct relevance to actual visual input. To address this problem, we propose Residual Decoding (ResDec). It is a novel training-free method that uses historical information to aid decoding. The method relies on the internal implicit reasoning mechanism and token logits evolution mechanism of LVLMs to correct biases. Extensive experiments demonstrate that ResDec effectively suppresses hallucinations induced by language priors, significantly improves visual grounding, and reduces object hallucinations. In addition to mitigating hallucinations, ResDec also performs exceptionally well on comprehensive LVLM benchmarks, highlighting its broad applicability."
    },
    {
      "title": "Not All Preferences Are Created Equal: Stability-Aware and Gradient-Efficient Alignment for Reasoning Models",
      "url": "https://arxiv.org/abs/2602.01207",
      "date": "2026-02-01",
      "authors_text": "Hui Wu, Hengyi Cai, Jinman Zhao, Xinran Chen, Ziheng Li",
      "is_highlight": false,
      "score": 48.0,
      "summary": "The paper introduces SAGE, a dynamic framework for preference-based alignment in reasoning models that enhances optimization stability and efficiency by prioritizing informative training instances.",
      "teaser_image": "https://arxiv.org/html/2602.01207/x2.png",
      "debug_abstract": "Preference-based alignment is pivotal for training large reasoning models; however, standard methods like Direct Preference Optimization (DPO) typically treat all preference pairs uniformly, overlooking the evolving utility of training instances. This static approach often leads to inefficient or unstable optimization, as it wastes computation on trivial pairs with negligible gradients and suffers from noise induced by samples near uncertain decision boundaries. Facing these challenges, we propose SAGE (Stability-Aware Gradient Efficiency), a dynamic framework designed to enhance alignment reliability by maximizing the Signal-to-Noise Ratio of policy updates. Concretely, SAGE integrates a coarse-grained curriculum mechanism that refreshes candidate pools based on model competence with a fine-grained, stability-aware scoring function that prioritizes informative, confident errors while filtering out unstable samples. Experiments on multiple mathematical reasoning benchmarks demonstrate that SAGE significantly accelerates convergence and outperforms static baselines, highlighting the critical role of policy-aware, stability-conscious data selection in reasoning alignment."
    },
    {
      "title": "Superman: Unifying Skeleton and Vision for Human Motion Perception and Generation",
      "url": "https://arxiv.org/abs/2602.02401",
      "date": "2026-02-02",
      "authors_text": "Xinshun Wang, Peiming Li, Ziyi Wang, Zhongbin Fang, Zhichao Deng",
      "is_highlight": false,
      "score": 15.0,
      "summary": "Superman unifies visual perception and skeleton-based motion generation through a Vision-Guided Motion Tokenizer, enabling robust cross-modal learning and superior performance in human motion tasks.",
      "teaser_image": "https://arxiv.org/html/2602.02401/x2.png",
      "debug_abstract": "Human motion analysis tasks, such as temporal 3D pose estimation, motion prediction, and motion in-betweening, play an essential role in computer vision. However, current paradigms suffer from severe fragmentation. First, the field is split between ``perception&#39;&#39; models that understand motion from video but only output text, and ``generation&#39;&#39; models that cannot perceive from raw visual input. Second, generative MLLMs are often limited to single-frame, static poses using dense, parametric SMPL models, failing to handle temporal motion. Third, existing motion vocabularies are built from skeleton data alone, severing the link to the visual domain. To address these challenges, we introduce Superman, a unified framework that bridges visual perception with temporal, skeleton-based motion generation. Our solution is twofold. First, to overcome the modality disconnect, we propose a Vision-Guided Motion Tokenizer. Leveraging the natural geometric alignment between 3D skeletons and visual data, this module pioneers robust joint learning from both modalities, creating a unified, cross-modal motion vocabulary. Second, grounded in this motion language, a single, unified MLLM architecture is trained to handle all tasks. This module flexibly processes diverse, temporal inputs, unifying 3D skeleton pose estimation from video (perception) with skeleton-based motion prediction and in-betweening (generation). Extensive experiments on standard benchmarks, including Human3.6M, demonstrate that our unified method achieves state-of-the-art or competitive performance across all motion tasks. This showcases a more efficient and scalable path for generative motion analysis using skeletons."
    },
    {
      "title": "Disturbance-Aware Flight Control of Robotic Gliding Blimp via Moving Mass Actuation",
      "url": "https://arxiv.org/abs/2601.21188",
      "date": "2026-01-29",
      "authors_text": "Hao Cheng, Feitian Zhang",
      "is_highlight": false,
      "score": 86.0,
      "summary": "This paper presents a disturbance-aware flight control system for robotic blimps using moving mass actuation and a combined moving horizon estimator and model predictive controller for enhanced stability.",
      "teaser_image": "https://arxiv.org/html/2601.21188/x2.png",
      "debug_abstract": "Robotic blimps, as lighter-than-air (LTA) aerial systems, offer long endurance and inherently safe operation but remain highly susceptible to wind disturbances. Building on recent advances in moving mass actuation, this paper addresses the lack of disturbance-aware control frameworks for LTA platforms by explicitly modeling and compensating for wind-induced effects. A moving horizon estimator (MHE) infers real-time wind perturbations and provides these estimates to a model predictive controller (MPC), enabling robust trajectory and heading regulation under varying wind conditions. The proposed approach leverages a two-degree-of-freedom (2-DoF) moving-mass mechanism to generate both inertial and aerodynamic moments for attitude and heading control, thereby enhancing flight stability in disturbance-prone environments. Extensive flight experiments under headwind and crosswind conditions show that the integrated MHE-MPC framework significantly outperforms baseline PID control, demonstrating its effectiveness for disturbance-aware LTA flight."
    },
    {
      "title": "GVGS: Gaussian Visibility-Aware Multi-View Geometry for Accurate Surface Reconstruction",
      "url": "https://arxiv.org/abs/2601.20331",
      "date": "2026-01-28",
      "authors_text": "Mai Su, Qihan Yu, Zhongtao Wang, Yilong Li, Chengwei Pan",
      "is_highlight": true,
      "score": 60.0,
      "summary": "The paper presents GVGS, a method enhancing 3D surface reconstruction accuracy by integrating Gaussian visibility-aware constraints and progressive monocular depth calibration to overcome existing limitations.",
      "teaser_image": "https://arxiv.org/html/2601.20331/x2.png",
      "debug_abstract": "3D Gaussian Splatting enables efficient optimization and high-quality rendering, yet accurate surface reconstruction remains challenging. Prior methods improve surface reconstruction by refining Gaussian depth estimates, either via multi-view geometric consistency or through monocular depth priors. However, multi-view constraints become unreliable under large geometric discrepancies, while monocular priors suffer from scale ambiguity and local inconsistency, ultimately leading to inaccurate Gaussian depth supervision. To address these limitations, we introduce a Gaussian visibility-aware multi-view geometric consistency constraint that aggregates the visibility of shared Gaussian primitives across views, enabling more accurate and stable geometric supervision. In addition, we propose a progressive quadtree-calibrated Monocular depth constraint that performs block-wise affine calibration from coarse to fine spatial scales, mitigating the scale ambiguity of depth priors while preserving fine-grained surface details. Extensive experiments on DTU and TNT datasets demonstrate consistent improvements in geometric accuracy over prior Gaussian-based and implicit surface reconstruction methods. Codes are available at an anonymous repository: this https URL."
    },
    {
      "title": "Uni-RS: A Spatially Faithful Unified Understanding and Generation Model for Remote Sensing",
      "url": "https://arxiv.org/abs/2601.17673",
      "date": "2026-01-25",
      "authors_text": "Weiyu Zhang, Yuan Hu, Yong Li, Yu Liu",
      "is_highlight": false,
      "score": 58.0,
      "summary": "Uni-RS is a novel unified multimodal model for remote sensing that enhances spatial faithfulness in text-to-image generation by integrating spatial layout planning and supervision techniques.",
      "debug_abstract": "Unified remote sensing multimodal models exhibit a pronounced spatial reversal curse: Although they can accurately recognize and describe object locations in images, they often fail to faithfully execute the same spatial relations during text-to-image generation, where such relations constitute core semantic information in remote sensing. Motivated by this observation, we propose Uni-RS, the first unified multimodal model tailored for remote sensing, to explicitly address the spatial asymmetry between understanding and generation. Specifically, we first introduce explicit Spatial-Layout Planning to transform textual instructions into spatial layout plans, decoupling geometric planning from visual synthesis. We then impose Spatial-Aware Query Supervision to bias learnable queries toward spatial relations explicitly specified in the instruction. Finally, we develop Image-Caption Spatial Layout Variation to expose the model to systematic geometry-consistent spatial transformations. Extensive experiments across multiple benchmarks show that our approach substantially improves spatial faithfulness in text-to-image generation, while maintaining strong performance on multimodal understanding tasks like image captioning, visual grounding, and VQA tasks."
    },
    {
      "title": "Agentic reinforcement learning empowers next-generation chemical language models for molecular design and synthesis",
      "url": "https://arxiv.org/abs/2601.17687",
      "date": "2026-01-25",
      "authors_text": "Hao Li, He Cao, Shenyao Peng, Zijing Liu, Bin Feng",
      "is_highlight": false,
      "score": 62.0,
      "summary": "ChemCRAFT utilizes agentic reinforcement learning to enhance small language models for efficient, cost-effective, and privacy-preserving molecular design and synthesis, outperforming larger cloud-based models.",
      "debug_abstract": "Language models are revolutionizing the biochemistry domain, assisting scientists in drug design and chemical synthesis with high efficiency. Yet current approaches struggle between small language models prone to hallucination and limited knowledge retention, and large cloud-based language models plagued by privacy risks and high inference costs. To bridge this gap, we introduce ChemCRAFT, a novel framework leveraging agentic reinforcement learning to decouple chemical reasoning from knowledge storage. Instead of forcing the model to memorize vast chemical data, our approach empowers the language model to interact with a sandbox for precise information retrieval. This externalization of knowledge allows a locally deployable small model to achieve superior performance with minimal inference costs. To enable small language models for agent-calling ability, we build an agentic trajectory construction pipeline and a comprehensive chemical-agent sandbox. Based on sandbox interactions, we constructed ChemToolDataset, the first large-scale chemical tool trajectory dataset. Simultaneously, we propose SMILES-GRPO to build a dense chemical reward function, promoting the model&#39;s ability to call chemical agents. Evaluations across diverse aspects of drug design show that ChemCRAFT outperforms current cloud-based LLMs in molecular structure analysis, molecular optimization, and synthesis pathway prediction, demonstrating that scientific reasoning is not solely an emergent ability of model scale, but a learnable policy of tool orchestration. This work establishes a cost-effective and privacy-preserving paradigm for AI-aided chemistry, opening new avenues for accelerating molecular discovery with locally deployable agents."
    },
    {
      "title": "TC-IDM: Grounding Video Generation for Executable Zero-shot Robot Motion",
      "url": "https://arxiv.org/abs/2601.18323",
      "date": "2026-01-26",
      "authors_text": "Weishi Mi, Yong Bao, Xiaowei Chi, Xiaozhu Ju, Zhiyuan Qin",
      "is_highlight": false,
      "score": 89.0,
      "summary": "The TC-IDM model enhances robot motion execution by bridging visual planning and physical control through tool-centric trajectory synthesis, achieving superior generalization and performance in diverse tasks.",
      "debug_abstract": "The vision-language-action (VLA) paradigm has enabled powerful robotic control by leveraging vision-language models, but its reliance on large-scale, high-quality robot data limits its generalization. Generative world models offer a promising alternative for general-purpose embodied AI, yet a critical gap remains between their pixel-level plans and physically executable actions. To this end, we propose the Tool-Centric Inverse Dynamics Model (TC-IDM). By focusing on the tool&#39;s imagined trajectory as synthesized by the world model, TC-IDM establishes a robust intermediate representation that bridges the gap between visual planning and physical control. TC-IDM extracts the tool&#39;s point cloud trajectories via segmentation and 3D motion estimation from generated videos. Considering diverse tool attributes, our architecture employs decoupled action heads to project these planned trajectories into 6-DoF end-effector motions and corresponding control signals. This plan-and-translate paradigm not only supports a wide range of end-effectors but also significantly improves viewpoint invariance. Furthermore, it exhibits strong generalization capabilities across long-horizon and out-of-distribution tasks, including interacting with deformable objects. In real-world evaluations, the world model with TC-IDM achieves an average success rate of 61.11 percent, with 77.7 percent on simple tasks and 38.46 percent on zero-shot deformable object tasks. It substantially outperforms end-to-end VLA-style baselines and other inverse dynamics models."
    },
    {
      "title": "Order from Chaos: Physical World Understanding from Glitchy Gameplay Videos",
      "url": "https://arxiv.org/abs/2601.16471",
      "date": "2026-01-23",
      "authors_text": "Meng Cao, Haoran Tang, Haoze Zhao, Mingfei Han, Ruyang Liu",
      "is_highlight": false,
      "score": 80.0,
      "summary": "This paper introduces PhysGame, a dataset leveraging gameplay glitches to enhance AI's understanding of physical principles, demonstrating improved reasoning capabilities through extensive experimentation.",
      "debug_abstract": "Understanding the physical world, including object dynamics, material properties, and causal interactions, remains a core challenge in artificial intelligence. Although recent multi-modal large language models (MLLMs) have demonstrated impressive general reasoning capabilities, they still fall short of achieving human-level understanding of physical principles. Existing datasets for physical reasoning either rely on real-world videos, which incur high annotation costs, or on synthetic simulations, which suffer from limited realism and diversity. In this paper, we propose a novel paradigm that leverages glitches in gameplay videos, referring to visual anomalies that violate predefined physical laws, as a rich and scalable supervision source for physical world understanding. We introduce PhysGame, an meta information guided instruction-tuning dataset containing 140,057 glitch-centric question-answer pairs across five physical domains and sixteen fine-grained categories. To ensure data accuracy, we design a prompting strategy that utilizes gameplay metadata such as titles and descriptions to guide high-quality QA generation. Complementing PhysGame, we construct GameBench, an expert-annotated benchmark with 880 glitch-identified gameplay videos designed to evaluate physical reasoning capabilities. Extensive experiments show that PhysGame significantly enhances both Game2Real transferability, improving the real world physical reasoning performance of Qwen2.5VL by 2.5% on PhysBench, and Game2General transferability, yielding a 1.9% gain on the MVBench benchmark. Moreover, PhysGame-tuned models achieve a 3.7% absolute improvement on GameBench, demonstrating enhanced robustness in detecting physical implausibilities. These results indicate that learning from gameplay anomalies offers a scalable and effective pathway toward advancing physical world understanding in multimodal intelligence."
    },
    {
      "title": "Render-of-Thought: Rendering Textual Chain-of-Thought as Images for Visual Latent Reasoning",
      "url": "https://arxiv.org/abs/2601.14750",
      "date": "2026-01-22",
      "authors_text": "Yifan Wang, Shiyu Li, Peiming Li, Xiaochen Yang, Yang Tang",
      "is_highlight": false,
      "score": 92.5,
      "summary": "The Render-of-Thought framework transforms Chain-of-Thought reasoning into images, enhancing analyzability and efficiency while maintaining competitive performance in reasoning tasks.",
      "debug_abstract": "Chain-of-Thought (CoT) prompting has achieved remarkable success in unlocking the reasoning capabilities of Large Language Models (LLMs). Although CoT prompting enhances reasoning, its verbosity imposes substantial computational overhead. Recent works often focus exclusively on outcome alignment and lack supervision on the intermediate reasoning process. These deficiencies obscure the analyzability of the latent reasoning chain. To address these challenges, we introduce Render-of-Thought (RoT), the first framework to reify the reasoning chain by rendering textual steps into images, making the latent rationale explicit and traceable. Specifically, we leverage the vision encoders of existing Vision Language Models (VLMs) as semantic anchors to align the vision embeddings with the textual space. This design ensures plug-and-play implementation without incurring additional pre-training overhead. Extensive experiments on mathematical and logical reasoning benchmarks demonstrate that our method achieves 3-4x token compression and substantial inference acceleration compared to explicit CoT. Furthermore, it maintains competitive performance against other methods, validating the feasibility of this paradigm. Our code is available at this https URL"
    },
    {
      "title": "Decoupling Return-to-Go for Efficient Decision Transformer",
      "url": "https://arxiv.org/abs/2601.15953",
      "date": "2026-01-22",
      "authors_text": "Yongyi Wang, Hanyu Liu, Lingfeng Li, Bozhou Chen, Ang Li",
      "is_highlight": false,
      "score": 75.2,
      "summary": "The Decoupled Decision Transformer (DDT) enhances offline reinforcement learning by using only the latest Return-to-Go for action prediction, improving performance and reducing computational costs.",
      "debug_abstract": "The Decision Transformer (DT) has established a powerful sequence modeling approach to offline reinforcement learning. It conditions its action predictions on Return-to-Go (RTG), using it both to distinguish trajectory quality during training and to guide action generation at inference. In this work, we identify a critical redundancy in this design: feeding the entire sequence of RTGs into the Transformer is theoretically unnecessary, as only the most recent RTG affects action prediction. We show that this redundancy can impair DT&#39;s performance through experiments. To resolve this, we propose the Decoupled DT (DDT). DDT simplifies the architecture by processing only observation and action sequences through the Transformer, using the latest RTG to guide the action prediction. This streamlined approach not only improves performance but also reduces computational cost. Our experiments show that DDT significantly outperforms DT and establishes competitive performance against state-of-the-art DT variants across multiple offline RL tasks."
    },
    {
      "title": "PhysicsMind: Sim and Real Mechanics Benchmarking for Physical Reasoning and Prediction in Foundational VLMs and World Models",
      "url": "https://arxiv.org/abs/2601.16007",
      "date": "2026-01-22",
      "authors_text": "Chak-Wing Mak, Guanyu Zhu, Boyi Zhang, Hongji Li, Xiaowei Chi",
      "is_highlight": false,
      "score": 94.1,
      "summary": "PhysicsMind introduces a benchmark for evaluating physical reasoning in multimodal models through real and simulated tasks focused on fundamental physics principles, revealing significant gaps in current models' understanding.",
      "debug_abstract": "Modern foundational Multimodal Large Language Models (MLLMs) and video world models have advanced significantly in mathematical, common-sense, and visual reasoning, but their grasp of the underlying physics remains underexplored. Existing benchmarks attempting to measure this matter rely on synthetic, Visual Question Answer templates or focus on perceptual video quality that is tangential to measuring how well the video abides by physical laws. To address this fragmentation, we introduce PhysicsMind, a unified benchmark with both real and simulation environments that evaluates law-consistent reasoning and generation over three canonical principles: Center of Mass, Lever Equilibrium, and Newton&#39;s First Law. PhysicsMind comprises two main tasks: i) VQA tasks, testing whether models can reason and determine physical quantities and values from images or short videos, and ii) Video Generation(VG) tasks, evaluating if predicted motion trajectories obey the same center-of-mass, torque, and inertial constraints as the ground truth. A broad range of recent models and video generation models is evaluated on PhysicsMind and found to rely on appearance heuristics while often violating basic mechanics. These gaps indicate that current scaling and training are still insufficient for robust physical understanding, underscoring PhysicsMind as a focused testbed for physics-aware multimodal models. Our data will be released upon acceptance."
    },
    {
      "title": "Collision-Free Humanoid Traversal in Cluttered Indoor Scenes",
      "url": "https://arxiv.org/abs/2601.16035",
      "date": "2026-01-22",
      "authors_text": "Han Xue, Sikai Liang, Zhikai Zhang, Zicheng Zeng, Yun Liu",
      "is_highlight": false,
      "score": 77.3,
      "summary": "The paper presents Humanoid Potential Field (HumanoidPF) for enabling collision-free traversal of humanoids in cluttered indoor environments, enhancing RL-based skill learning and real-world application.",
      "debug_abstract": "We study the problem of collision-free humanoid traversal in cluttered indoor scenes, such as hurdling over objects scattered on the floor, crouching under low-hanging obstacles, or squeezing through narrow passages. To achieve this goal, the humanoid needs to map its perception of surrounding obstacles with diverse spatial layouts and geometries to the corresponding traversal skills. However, the lack of an effective representation that captures humanoid-obstacle relationships during collision avoidance makes directly learning such mappings difficult. We therefore propose Humanoid Potential Field (HumanoidPF), which encodes these relationships as collision-free motion directions, significantly facilitating RL-based traversal skill learning. We also find that HumanoidPF exhibits a surprisingly negligible sim-to-real gap as a perceptual representation. To further enable generalizable traversal skills through diverse and challenging cluttered indoor scenes, we further propose a hybrid scene generation method, incorporating crops of realistic 3D indoor scenes and procedurally synthesized obstacles. We successfully transfer our policy to the real world and develop a teleoperation system where users could command the humanoid to traverse in cluttered indoor scenes with just a single click. Extensive experiments are conducted in both simulation and the real world to validate the effectiveness of our method. Demos and code can be found in our website: this https URL."
    }
  ],
  "北京航空航天大学": [
    {
      "title": "USS-Nav: Unified Spatio-Semantic Scene Graph for Lightweight UAV Zero-Shot Object Navigation",
      "url": "https://arxiv.org/abs/2602.00708",
      "date": "2026-01-31",
      "authors_text": "Weiqi Gai, Yuman Gao, Yuan Zhou, Yufan Xie, Zhiyang Liu",
      "is_highlight": false,
      "score": 80.0,
      "summary": "USS-Nav introduces a lightweight framework for UAV zero-shot object navigation, utilizing a Unified Spatio-Semantic scene graph for efficient exploration and improved computational performance.",
      "teaser_image": "https://arxiv.org/html/2602.00708/figs/skeleton_gen.png",
      "debug_abstract": "Zero-Shot Object Navigation in unknown environments poses significant challenges for Unmanned Aerial Vehicles (UAVs) due to the conflict between high-level semantic reasoning requirements and limited onboard computational resources. To address this, we present USS-Nav, a lightweight framework that incrementally constructs a Unified Spatio-Semantic scene graph and enables efficient Large Language Model (LLM)-augmented Zero-Shot Object Navigation in unknown environments. Specifically, we introduce an incremental Spatial Connectivity Graph generation method utilizing polyhedral expansion to capture global geometric topology, which is dynamically partitioned into semantic regions via graph clustering. Concurrently, open-vocabulary object semantics are instantiated and anchored to this topology to form a hierarchical environmental representation. Leveraging this hierarchical structure, we present a coarse-to-fine exploration strategy: LLM grounded in the scene graph&#39;s semantics to determine global target regions, while a local planner optimizes frontier coverage based on information gain. Experimental results demonstrate that our framework outperforms state-of-the-art methods in terms of computational efficiency and real-time update frequency (15 Hz) on a resource-constrained platform. Furthermore, ablation studies confirm the effectiveness of our framework, showing substantial improvements in Success weighted by Path Length (SPL). The source code will be made publicly available to foster further research."
    },
    {
      "title": "Meanshift Shape Formation Control Using Discrete Mass Distribution",
      "url": "https://arxiv.org/abs/2602.00980",
      "date": "2026-02-01",
      "authors_text": "Yichen Cai, Yuan Gao, Pengpeng Li, Wei Wang, Guibin Sun",
      "is_highlight": false,
      "score": 81.0,
      "summary": "This paper presents a decentralized control strategy using discrete mass distribution for adaptive swarm formation of complex shapes, validated through simulations and real-world experiments.",
      "teaser_image": "https://arxiv.org/html/2602.00980/x1.png",
      "debug_abstract": "The density-distribution method has recently become a promising paradigm owing to its adaptability to variations in swarm size. However, existing studies face practical challenges in achieving complex shape representation and decentralized implementation. This motivates us to develop a fully decentralized, distribution-based control strategy with the dual capability of forming complex shapes and adapting to swarm-size variations. Specifically, we first propose a discrete mass-distribution function defined over a set of sample points to model swarm formation. In contrast to the continuous density-distribution method, our model eliminates the requirement for defining continuous density functions-a task that is difficult for complex shapes. Second, we design a decentralized meanshift control law to coordinate the swarm&#39;s global distribution to fit the sample-point distribution by feeding back mass estimates. The mass estimates for all sample points are achieved by the robots in a decentralized manner via the designed mass estimator. It is shown that the mass estimates of the sample points can asymptotically converge to the true global values. To validate the proposed strategy, we conduct comprehensive simulations and real-world experiments to evaluate the efficiency of complex shape formation and adaptability to swarm-size variations."
    },
    {
      "title": "MedAD-R1: Eliciting Consistent Reasoning in Interpretible Medical Anomaly Detection via Consistency-Reinforced Policy Optimization",
      "url": "https://arxiv.org/abs/2602.01081",
      "date": "2026-02-01",
      "authors_text": "Haitao Zhang, Yingying Wang, Jiaxiang Wang, Haote Xu, Hongyang Zhang",
      "is_highlight": false,
      "score": 64.0,
      "summary": "MedAD-R1 enhances medical anomaly detection by utilizing a two-stage training framework with consistency-reinforced policy optimization, achieving state-of-the-art performance on the MedAD-38K benchmark.",
      "teaser_image": "https://arxiv.org/html/2602.01081/x3.png",
      "debug_abstract": "Medical Anomaly Detection (MedAD) presents a significant opportunity to enhance diagnostic accuracy using Large Multimodal Models (LMMs) to interpret and answer questions based on medical images. However, the reliance on Supervised Fine-Tuning (SFT) on simplistic and fragmented datasets has hindered the development of models capable of plausible reasoning and robust multimodal generalization. To overcome this, we introduce MedAD-38K, the first large-scale, multi-modal, and multi-center benchmark for MedAD featuring diagnostic Chain-of-Thought (CoT) annotations alongside structured Visual Question-Answering (VQA) pairs. On this foundation, we propose a two-stage training framework. The first stage, Cognitive Injection, uses SFT to instill foundational medical knowledge and align the model with a structured think-then-answer paradigm. Given that standard policy optimization can produce reasoning that is disconnected from the final answer, the second stage incorporates Consistency Group Relative Policy Optimization (Con-GRPO). This novel algorithm incorporates a crucial consistency reward to ensure the generated reasoning process is relevant and logically coherent with the final diagnosis. Our proposed model, MedAD-R1, achieves state-of-the-art (SOTA) performance on the MedAD-38K benchmark, outperforming strong baselines by more than 10\\%. This superior performance stems from its ability to generate transparent and logically consistent reasoning pathways, offering a promising approach to enhancing the trustworthiness and interpretability of AI for clinical decision support."
    },
    {
      "title": "UrbanGS: A Scalable and Efficient Architecture for Geometrically Accurate Large-Scene Reconstruction",
      "url": "https://arxiv.org/abs/2602.02089",
      "date": "2026-02-02",
      "authors_text": "Changbai Li, Haodong Zhu, Hanlin Chen, Xiuping Liang, Tongfei Chen",
      "is_highlight": false,
      "score": 22.0,
      "summary": "UrbanGS is a scalable framework that enhances geometric accuracy and memory efficiency in large-scale urban scene reconstruction through innovative regularization and adaptive Gaussian pruning techniques.",
      "teaser_image": "https://arxiv.org/html/2602.02089/figure/main.png",
      "debug_abstract": "While 3D Gaussian Splatting (3DGS) enables high-quality, real-time rendering for bounded scenes, its extension to large-scale urban environments gives rise to critical challenges in terms of geometric consistency, memory efficiency, and computational scalability. To address these issues, we present UrbanGS, a scalable reconstruction framework that effectively tackles these challenges for city-scale applications. First, we propose a Depth-Consistent D-Normal Regularization module. Unlike existing approaches that rely solely on monocular normal estimators, which can effectively update rotation parameters yet struggle to update position parameters, our method integrates D-Normal constraints with external depth supervision. This allows for comprehensive updates of all geometric parameters. By further incorporating an adaptive confidence weighting mechanism based on gradient consistency and inverse depth deviation, our approach significantly enhances multi-view depth alignment and geometric coherence, which effectively resolves the issue of geometric accuracy in complex large-scale scenes. To improve scalability, we introduce a Spatially Adaptive Gaussian Pruning (SAGP) strategy, which dynamically adjusts Gaussian density based on local geometric complexity and visibility to reduce redundancy. Additionally, a unified partitioning and view assignment scheme is designed to eliminate boundary artifacts and optimize computational load. Extensive experiments on multiple urban datasets demonstrate that UrbanGS achieves superior performance in rendering quality, geometric accuracy, and memory efficiency, providing a systematic solution for high-fidelity large-scale scene reconstruction."
    },
    {
      "title": "UV-M3TL: A Unified and Versatile Multimodal Multi-Task Learning Framework for Assistive Driving Perception",
      "url": "https://arxiv.org/abs/2602.01594",
      "date": "2026-02-02",
      "authors_text": "Wenzhuo Liu, Qiannan Guo, Zhen Wang, Wenshuo Wang, Lei Yang",
      "is_highlight": false,
      "score": 8.0,
      "summary": "The UV-M3TL framework enhances ADAS by effectively recognizing driver behavior and context through dual-branch multimodal embedding and adaptive loss, achieving state-of-the-art performance across multiple tasks.",
      "teaser_image": "https://arxiv.org/html/2602.01594/x2.png",
      "debug_abstract": "Advanced Driver Assistance Systems (ADAS) need to understand human driver behavior while perceiving their navigation context, but jointly learning these heterogeneous tasks would cause inter-task negative transfer and impair system performance. Here, we propose a Unified and Versatile Multimodal Multi-Task Learning (UV-M3TL) framework to simultaneously recognize driver behavior, driver emotion, vehicle behavior, and traffic context, while mitigating inter-task negative transfer. Our framework incorporates two core components: dual-branch spatial channel multimodal embedding (DB-SCME) and adaptive feature-decoupled multi-task loss (AFD-Loss). DB-SCME enhances cross-task knowledge transfer while mitigating task conflicts by employing a dual-branch structure to explicitly model salient task-shared and task-specific features. AFD-Loss improves the stability of joint optimization while guiding the model to learn diverse multi-task representations by introducing an adaptive weighting mechanism based on learning dynamics and feature decoupling constraints. We evaluate our method on the AIDE dataset, and the experimental results demonstrate that UV-M3TL achieves state-of-the-art performance across all four tasks. To further prove the versatility, we evaluate UV-M3TL on additional public multi-task perception benchmarks (BDD100K, CityScapes, NYUD-v2, and PASCAL-Context), where it consistently delivers strong performance across diverse task combinations, attaining state-of-the-art results on most tasks."
    },
    {
      "title": "Concise Geometric Description as a Bridge: Unleashing the Potential of LLM for Plane Geometry Problem Solving",
      "url": "https://arxiv.org/abs/2601.21164",
      "date": "2026-01-29",
      "authors_text": "Jingyun Wang, Dian Li, Xiaohan Wang, Gang Liu, Jiahong Yan",
      "is_highlight": false,
      "score": 65.0,
      "summary": "This paper proposes a method using a MLLM Interpreter to convert visual geometric information into concise textual descriptions, enhancing LLMs' reasoning capabilities for plane geometry problem solving.",
      "teaser_image": "https://arxiv.org/html/2601.21164/x2.png",
      "debug_abstract": "Plane Geometry Problem Solving (PGPS) is a multimodal reasoning task that aims to solve a plane geometric problem based on a geometric diagram and problem textual descriptions. Although Large Language Models (LLMs) possess strong reasoning skills, their direct application to PGPS is hindered by their inability to process visual diagrams. Existing works typically fine-tune Multimodal LLMs (MLLMs) end-to-end on large-scale PGPS data to enhance visual understanding and reasoning simultaneously. However, such joint optimization may compromise base LLMs&#39; inherent reasoning capability. In this work, we observe that LLM itself is potentially a powerful PGPS solver when appropriately formulating visual information as textual descriptions. We propose to train a MLLM Interpreter to generate geometric descriptions for the visual diagram, and an off-the-shelf LLM is utilized to perform reasoning. Specifically, we choose Conditional Declaration Language (CDL) as the geometric description as its conciseness eases the MLLM Interpreter training. The MLLM Interpreter is fine-tuned via CoT (Chain-of-Thought)-augmented SFT followed by GRPO to generate CDL. Instead of using a conventional solution-based reward that compares the reasoning result with the ground-truth answer, we design CDL matching rewards to facilitate more effective GRPO training, which provides more direct and denser guidance for CDL generation. To support training, we construct a new dataset, Formalgeo7k-Rec-CoT, by manually reviewing Formalgeo7k v2 and incorporating CoT annotations. Extensive experiments on Formalgeo7k-Rec-CoT, Unigeo, and MathVista show our method (finetuned on only 5.5k data) performs favorably against leading open-source and closed-source MLLMs."
    },
    {
      "title": "Latent Chain-of-Thought as Planning: Decoupling Reasoning from Verbalization",
      "url": "https://arxiv.org/abs/2601.21358",
      "date": "2026-01-29",
      "authors_text": "Jiecong Wang, Hao Peng, Chunyang Liu",
      "is_highlight": false,
      "score": 74.0,
      "summary": "PLaT decouples reasoning from verbalization in LLMs by modeling latent reasoning as planning, enhancing scalability and diversity despite lower initial accuracy.",
      "teaser_image": "https://arxiv.org/html/2601.21358/x2.png",
      "debug_abstract": "Chain-of-Thought (CoT) empowers Large Language Models (LLMs) to tackle complex problems, but remains constrained by the computational cost and reasoning path collapse when grounded in discrete token spaces. Recent latent reasoning approaches attempt to optimize efficiency by performing reasoning within continuous hidden states. However, these methods typically operate as opaque end-to-end mappings from explicit reasoning steps to latent states, and often require a pre-defined number of latent steps during inference. In this work, we introduce PLaT (Planning with Latent Thoughts), a framework that reformulates latent reasoning as planning by fundamentally decouple reasoning from verbalization. We model reasoning as a deterministic trajectory of latent planning states, while a separate Decoder grounds these thoughts into text when necessary. This decoupling allows the model to dynamically determine when to terminate reasoning rather than relying on fixed hyperparameters. Empirical results on mathematical benchmarks reveal a distinct trade-off: while PLaT achieves lower greedy accuracy than baselines, it demonstrates superior scalability in terms of reasoning diversity. This indicates that PLaT learns a robust, broader solution space, offering a transparent and scalable foundation for inference-time search."
    },
    {
      "title": "Tactile-Force Alignment in Vision-Language-Action Models for Force-aware Manipulation",
      "url": "https://arxiv.org/abs/2601.20321",
      "date": "2026-01-28",
      "authors_text": "Yuzhe Huang, Pei Lin, Wanlin Li, Daohan Li, Jiajun Li",
      "is_highlight": false,
      "score": 91.0,
      "summary": "The paper introduces TaF-VLA, a framework that enhances Vision-Language-Action models by integrating tactile-force alignment for improved force-aware robotic manipulation in contact-rich tasks.",
      "teaser_image": "https://arxiv.org/html/2601.20321/figures/fig1-teaser.png",
      "debug_abstract": "Vision-Language-Action (VLA) models have recently emerged as powerful generalists for robotic manipulation. However, due to their predominant reliance on visual modalities, they fundamentally lack the physical intuition required for contact-rich tasks that require precise force regulation and physical reasoning. Existing attempts to incorporate vision-based tactile sensing into VLA models typically treat tactile inputs as auxiliary visual textures, thereby overlooking the underlying correlation between surface deformation and interaction dynamics. To bridge this gap, we propose a paradigm shift from tactile-vision alignment to tactile-force alignment. Here, we introduce TaF-VLA, a framework that explicitly grounds high-dimensional tactile observations in physical interaction forces. To facilitate this, we develop an automated tactile-force data acquisition device and curate the TaF-Dataset, comprising over 10 million synchronized tactile observations, 6-axis force/torque, and matrix force map. To align sequential tactile observations with interaction forces, the central component of our approach is the Tactile-Force Adapter (TaF-Adapter), a tactile sensor encoder that extracts discretized latent information for encoding tactile observations. This mechanism ensures that the learned representations capture history-dependent, noise-insensitive physical dynamics rather than static visual textures. Finally, we integrate this force-aligned encoder into a VLA backbone. Extensive real-world experiments demonstrate that TaF-VLA policy significantly outperforms state-of-the-art tactile-vision-aligned and vision-only baselines on contact-rich tasks, verifying its ability to achieve robust, force-aware manipulation through cross-modal physical reasoning."
    },
    {
      "title": "GVGS: Gaussian Visibility-Aware Multi-View Geometry for Accurate Surface Reconstruction",
      "url": "https://arxiv.org/abs/2601.20331",
      "date": "2026-01-28",
      "authors_text": "Mai Su, Qihan Yu, Zhongtao Wang, Yilong Li, Chengwei Pan",
      "is_highlight": true,
      "score": 60.0,
      "summary": "The paper presents GVGS, a method enhancing 3D surface reconstruction accuracy by integrating Gaussian visibility-aware constraints and progressive monocular depth calibration to overcome existing limitations.",
      "teaser_image": "https://arxiv.org/html/2601.20331/x2.png",
      "debug_abstract": "3D Gaussian Splatting enables efficient optimization and high-quality rendering, yet accurate surface reconstruction remains challenging. Prior methods improve surface reconstruction by refining Gaussian depth estimates, either via multi-view geometric consistency or through monocular depth priors. However, multi-view constraints become unreliable under large geometric discrepancies, while monocular priors suffer from scale ambiguity and local inconsistency, ultimately leading to inaccurate Gaussian depth supervision. To address these limitations, we introduce a Gaussian visibility-aware multi-view geometric consistency constraint that aggregates the visibility of shared Gaussian primitives across views, enabling more accurate and stable geometric supervision. In addition, we propose a progressive quadtree-calibrated Monocular depth constraint that performs block-wise affine calibration from coarse to fine spatial scales, mitigating the scale ambiguity of depth priors while preserving fine-grained surface details. Extensive experiments on DTU and TNT datasets demonstrate consistent improvements in geometric accuracy over prior Gaussian-based and implicit surface reconstruction methods. Codes are available at an anonymous repository: this https URL."
    },
    {
      "title": "GPO: Growing Policy Optimization for Legged Robot Locomotion and Whole-Body Control",
      "url": "https://arxiv.org/abs/2601.20668",
      "date": "2026-01-28",
      "authors_text": "Shuhao Liao, Peizhuo Li, Xinrong Yang, Linnan Chang, Zhaoxin Fan",
      "is_highlight": false,
      "score": 92.0,
      "summary": "The paper presents Growing Policy Optimization (GPO), a framework that enhances reinforcement learning for legged robots by progressively expanding the action space to improve data collection and policy performance.",
      "teaser_image": "https://arxiv.org/html/2601.20668/pic/framework.png",
      "debug_abstract": "Training reinforcement learning (RL) policies for legged robots remains challenging due to high-dimensional continuous actions, hardware constraints, and limited exploration. Existing methods for locomotion and whole-body control work well for position-based control with environment-specific heuristics (e.g., reward shaping, curriculum design, and manual initialization), but are less effective for torque-based control, where sufficiently exploring the action space and obtaining informative gradient signals for training is significantly more difficult. We introduce Growing Policy Optimization (GPO), a training framework that applies a time-varying action transformation to restrict the effective action space in the early stage, thereby encouraging more effective data collection and policy learning, and then progressively expands it to enhance exploration and achieve higher expected return. We prove that this transformation preserves the PPO update rule and introduces only bounded, vanishing gradient distortion, thereby ensuring stable training. We evaluate GPO on both quadruped and hexapod robots, including zero-shot deployment of simulation-trained policies on hardware. Policies trained with GPO consistently achieve better performance. These results suggest that GPO provides a general, environment-agnostic optimization framework for learning legged locomotion."
    },
    {
      "title": "Dynamic Worlds, Dynamic Humans: Generating Virtual Human-Scene Interaction Motion in Dynamic Scenes",
      "url": "https://arxiv.org/abs/2601.19484",
      "date": "2026-01-27",
      "authors_text": "Yin Wang, Zhiying Leng, Haitian Liu, Frederick W. B. Li, Mu Li",
      "is_highlight": false,
      "score": 78.0,
      "summary": "Dyn-HSI introduces a cognitive architecture for generating adaptive human-scene interactions in dynamic environments, enhancing motion quality through perception, memory, and control mechanisms.",
      "debug_abstract": "Scenes are continuously undergoing dynamic changes in the real world. However, existing human-scene interaction generation methods typically treat the scene as static, which deviates from reality. Inspired by world models, we introduce Dyn-HSI, the first cognitive architecture for dynamic human-scene interaction, which endows virtual humans with three humanoid components. (1)Vision (human eyes): we equip the virtual human with a Dynamic Scene-Aware Navigation, which continuously perceives changes in the surrounding environment and adaptively predicts the next waypoint. (2)Memory (human brain): we equip the virtual human with a Hierarchical Experience Memory, which stores and updates experiential data accumulated during training. This allows the model to leverage prior knowledge during inference for context-aware motion priming, thereby enhancing both motion quality and generalization. (3) Control (human body): we equip the virtual human with Human-Scene Interaction Diffusion Model, which generates high-fidelity interaction motions conditioned on multimodal inputs. To evaluate performance in dynamic scenes, we extend the existing static human-scene interaction datasets to construct a dynamic benchmark, Dyn-Scenes. We conduct extensive qualitative and quantitative experiments to validate Dyn-HSI, showing that our method consistently outperforms existing approaches and generates high-quality human-scene interaction motions in both static and dynamic settings."
    },
    {
      "title": "Seeing through Light and Darkness: Sensor-Physics Grounded Deblurring HDR NeRF from Single-Exposure Images and Events",
      "url": "https://arxiv.org/abs/2601.15475",
      "date": "2026-01-21",
      "authors_text": "Yunshan Qi, Lin Zhu, Nan Bao, Yifan Zhao, Jia Li",
      "is_highlight": false,
      "score": 88.3,
      "summary": "The paper presents a unified NeRF framework that utilizes sensor-physics principles to enhance HDR novel view synthesis from single-exposure blurry LDR images and event data.",
      "debug_abstract": "Novel view synthesis from low dynamic range (LDR) blurry images, which are common in the wild, struggles to recover high dynamic range (HDR) and sharp 3D representations in extreme lighting conditions. Although existing methods employ event data to address this issue, they ignore the sensor-physics mismatches between the camera output and physical world radiance, resulting in suboptimal HDR and deblurring results. To cope with this problem, we propose a unified sensor-physics grounded NeRF framework for sharp HDR novel view synthesis from single-exposure blurry LDR images and corresponding events. We employ NeRF to directly represent the actual radiance of the 3D scene in the HDR domain and model raw HDR scene rays hitting the sensor pixels as in the physical world. A pixel-wise RGB mapping field is introduced to align the above rendered pixel values with the sensor-recorded LDR pixel values of the input images. A novel event mapping field is also designed to bridge the physical scene dynamics and actual event sensor output. The two mapping fields are jointly optimized with the NeRF network, leveraging the spatial and temporal dynamic information in events to enhance the sharp HDR 3D representation learning. Experiments on the collected and public datasets demonstrate that our method can achieve state-of-the-art deblurring HDR novel view synthesis results with single-exposure blurry LDR images and corresponding events."
    },
    {
      "title": "Evolving Without Ending: Unifying Multimodal Incremental Learning for Continual Panoptic Perception",
      "url": "https://arxiv.org/abs/2601.15643",
      "date": "2026-01-22",
      "authors_text": "Bo Yuan, Danpei Zhao, Wentao Li, Tian Li, Zhiguo Jiang",
      "is_highlight": false,
      "score": 90.2,
      "summary": "This paper presents a novel continual panoptic perception model that integrates multimodal and multi-task learning to enhance image perception while mitigating catastrophic forgetting.",
      "debug_abstract": "Continual learning (CL) is a great endeavour in developing intelligent perception AI systems. However, the pioneer research has predominantly focus on single-task CL, which restricts the potential in multi-task and multimodal scenarios. Beyond the well-known issue of catastrophic forgetting, the multi-task CL also brings semantic obfuscation across multimodal alignment, leading to severe model degradation during incremental training steps. In this paper, we extend CL to continual panoptic perception (CPP), integrating multimodal and multi-task CL to enhance comprehensive image perception through pixel-level, instance-level, and image-level joint interpretation. We formalize the CL task in multimodal scenarios and propose an end-to-end continual panoptic perception model. Concretely, CPP model features a collaborative cross-modal encoder (CCE) for multimodal embedding. We also propose a malleable knowledge inheritance module via contrastive feature distillation and instance distillation, addressing catastrophic forgetting from task-interactive boosting manner. Furthermore, we propose a cross-modal consistency constraint and develop CPP+, ensuring multimodal semantic alignment for model updating under multi-task incremental scenarios. Additionally, our proposed model incorporates an asymmetric pseudo-labeling manner, enabling model evolving without exemplar replay. Extensive experiments on multimodal datasets and diverse CL tasks demonstrate the superiority of the proposed model, particularly in fine-grained CL tasks."
    }
  ],
  "Oxford Robotics Institute (ORI)": [
    {
      "title": "KAN We Flow? Advancing Robotic Manipulation with 3D Flow Matching via KAN & RWKV",
      "url": "https://arxiv.org/abs/2602.01115",
      "date": "2026-02-01",
      "authors_text": "Zhihao Chen, Yiyuan Ge, Ziyang Wang",
      "is_highlight": false,
      "score": 94.0,
      "summary": "KAN-We-Flow introduces a lightweight flow-matching policy utilizing RWKV and KAN for efficient 3D robotic manipulation, achieving state-of-the-art performance with significantly reduced parameters.",
      "teaser_image": "https://arxiv.org/html/2602.01115/image/fig2.png",
      "debug_abstract": "Diffusion-based visuomotor policies excel at modeling action distributions but are inference-inefficient, since recursively denoising from noise to policy requires many steps and heavy UNet backbones, which hinders deployment on resource-constrained robots. Flow matching alleviates the sampling burden by learning a one-step vector field, yet prior implementations still inherit large UNet-style architectures. In this work, we present KAN-We-Flow, a flow-matching policy that draws on recent advances in Receptance Weighted Key Value (RWKV) and Kolmogorov-Arnold Networks (KAN) from vision to build a lightweight and highly expressive backbone for 3D manipulation. Concretely, we introduce an RWKV-KAN block: an RWKV first performs efficient time/channel mixing to propagate task context, and a subsequent GroupKAN layer applies learnable spline-based, groupwise functional mappings to perform feature-wise nonlinear calibration of the action mapping on RWKV outputs. Moreover, we introduce an Action Consistency Regularization (ACR), a lightweight auxiliary loss that enforces alignment between predicted action trajectories and expert demonstrations via Euler extrapolation, providing additional supervision to stabilize training and improve policy precision. Without resorting to large UNets, our design reduces parameters by 86.8\\%, maintains fast runtime, and achieves state-of-the-art success rates on Adroit, Meta-World, and DexArt benchmarks. Our project page can be viewed in \\href{this https URL}{\\textcolor{red}{link}}"
    },
    {
      "title": "TreeLoc: 6-DoF LiDAR Global Localization in Forests via Inter-Tree Geometric Matching",
      "url": "https://arxiv.org/abs/2602.01501",
      "date": "2026-02-02",
      "authors_text": "Minwoo Jung, Nived Chebrolu, Lucas Carvalho de Lima, Haedam Oh, Maurice Fallon",
      "is_highlight": false,
      "score": 97.0,
      "summary": "TreeLoc is a LiDAR-based framework for 6-DoF global localization in forests, utilizing tree geometry for robust pose estimation and outperforming traditional methods.",
      "teaser_image": "https://arxiv.org/html/2602.01501/x2.png",
      "debug_abstract": "Reliable localization is crucial for navigation in forests, where GPS is often degraded and LiDAR measurements are repetitive, occluded, and structurally complex. These conditions weaken the assumptions of traditional urban-centric localization methods, which assume that consistent features arise from unique structural patterns, necessitating forest-centric solutions to achieve robustness in these environments. To address these challenges, we propose TreeLoc, a LiDAR-based global localization framework for forests that handles place recognition and 6-DoF pose estimation. We represent scenes using tree stems and their Diameter at Breast Height (DBH), which are aligned to a common reference frame via their axes and summarized using the tree distribution histogram (TDH) for coarse matching, followed by fine matching with a 2D triangle descriptor. Finally, pose estimation is achieved through a two-step geometric verification. On diverse forest benchmarks, TreeLoc outperforms baselines, achieving precise localization. Ablation studies validate the contribution of each component. We also propose applications for long-term forest management using descriptors from a compact global tree database. TreeLoc is open-sourced for the robotics community at this https URL."
    },
    {
      "title": "3D Foundation Model-Based Loop Closing for Decentralized Collaborative SLAM",
      "url": "https://arxiv.org/abs/2602.02430",
      "date": "2026-02-02",
      "authors_text": "Pierre-Yves Lajoie, Benjamin Ramtoula, Daniele De Martini, Giovanni Beltrame",
      "is_highlight": false,
      "score": 14.0,
      "summary": "This paper presents a decentralized C-SLAM method utilizing 3D foundation models for robust loop closing and efficient inter-robot measurements, enhancing mapping accuracy and resource efficiency.",
      "teaser_image": "https://arxiv.org/html/2602.02430/x1.png",
      "debug_abstract": "Decentralized Collaborative Simultaneous Localization And Mapping (C-SLAM) techniques often struggle to identify map overlaps due to significant viewpoint variations among robots. Motivated by recent advancements in 3D foundation models, which can register images despite large viewpoint differences, we propose a robust loop closing approach that leverages these models to establish inter-robot measurements. In contrast to resource-intensive methods requiring full 3D reconstruction within a centralized map, our approach integrates foundation models into existing SLAM pipelines, yielding scalable and robust multi-robot mapping. Our contributions include: (1) integrating 3D foundation models to reliably estimate relative poses from monocular image pairs within decentralized C-SLAM; (2) introducing robust outlier mitigation techniques critical to the use of these relative poses; and (3) developing specialized pose graph optimization formulations that efficiently resolve scale ambiguities. We evaluate our method against state-of-the-art approaches, demonstrating improvements in localization and mapping accuracy, alongside significant gains in computational and memory efficiency. These results highlight the potential of our approach for deployment in large-scale multi-robot scenarios."
    },
    {
      "title": "From Generative Engines to Actionable Simulators: The Imperative of Physical Grounding in World Models",
      "url": "https://arxiv.org/abs/2601.15533",
      "date": "2026-01-21",
      "authors_text": "Zhikang Chen, Tingting Zhu",
      "is_highlight": false,
      "score": 91.0,
      "summary": "The paper critiques current world models for prioritizing visual realism over causal understanding, advocating for actionable simulators that emphasize structured dynamics and long-term stability.",
      "debug_abstract": "A world model is an AI system that simulates how an environment evolves under actions, enabling planning through imagined futures rather than reactive perception. Current world models, however, suffer from visual conflation: the mistaken assumption that high-fidelity video generation implies an understanding of physical and causal dynamics. We show that while modern models excel at predicting pixels, they frequently violate invariant constraints, fail under intervention, and break down in safety-critical decision-making. This survey argues that visual realism is an unreliable proxy for world understanding. Instead, effective world models must encode causal structure, respect domain-specific constraints, and remain stable over long horizons. We propose a reframing of world models as actionable simulators rather than visual engines, emphasizing structured 4D interfaces, constraint-aware dynamics, and closed-loop evaluation. Using medical decision-making as an epistemic stress test, where trial-and-error is impossible and errors are irreversible, we demonstrate that a world model&#39;s value is determined not by how realistic its rollouts appear, but by its ability to support counterfactual reasoning, intervention planning, and robust long-horizon foresight."
    }
  ],
  "机器人与自动化研究中心": [
    {
      "title": "Search-R2: Enhancing Search-Integrated Reasoning via Actor-Refiner Collaboration",
      "url": "https://arxiv.org/abs/2602.03647",
      "date": "2026-02-03",
      "authors_text": "Bowei He, Minda Hu, Zenan Xu, Hongru Wang, Licheng Zong",
      "is_highlight": false,
      "score": 52.0,
      "summary": "Search-R2 introduces an Actor-Refiner framework that enhances search-integrated reasoning in language agents through targeted intervention and fine-grained supervision, outperforming existing methods.",
      "teaser_image": "https://arxiv.org/html/2602.03647/x2.png",
      "debug_abstract": "Search-integrated reasoning enables language agents to transcend static parametric knowledge by actively querying external sources. However, training these agents via reinforcement learning is hindered by the multi-scale credit assignment problem: existing methods typically rely on sparse, trajectory-level rewards that fail to distinguish between high-quality reasoning and fortuitous guesses, leading to redundant or misleading search behaviors. To address this, we propose Search-R2, a novel Actor-Refiner collaboration framework that enhances reasoning through targeted intervention, with both components jointly optimized during training. Our approach decomposes the generation process into an Actor, which produces initial reasoning trajectories, and a Meta-Refiner, which selectively diagnoses and repairs flawed steps via a &#39;cut-and-regenerate&#39; mechanism. To provide fine-grained supervision, we introduce a hybrid reward design that couples outcome correctness with a dense process reward quantifying the information density of retrieved evidence. Theoretically, we formalize the Actor-Refiner interaction as a smoothed mixture policy, proving that selective correction yields strict performance gains over strong baselines. Extensive experiments across various general and multi-hop QA datasets demonstrate that Search-R2 consistently outperforms strong RAG and RL-based baselines across model scales, achieving superior reasoning accuracy with minimal overhead."
    },
    {
      "title": "Dual Quaternion SE(3) Synchronization with Recovery Guarantees",
      "url": "https://arxiv.org/abs/2602.00324",
      "date": "2026-01-30",
      "authors_text": "Jianing Zhao, Linglingzhi Zhu, Anthony Man-Cho So",
      "is_highlight": false,
      "score": 75.0,
      "summary": "This paper presents a dual quaternion-based SE(3) synchronization method with a two-stage algorithm that guarantees recovery of absolute poses from noisy transformations, outperforming traditional matrix methods.",
      "teaser_image": "https://arxiv.org/html/2602.00324/x2.png",
      "debug_abstract": "Synchronization over the special Euclidean group SE(3) aims to recover absolute poses from noisy pairwise relative transformations and is a core primitive in robotics and 3D vision. Standard approaches often require multi-step heuristic procedures to recover valid poses, which are difficult to analyze and typically lack theoretical guarantees. This paper adopts a dual quaternion representation and formulates SE(3) synchronization directly over the unit dual quaternion. A two-stage algorithm is developed: A spectral initializer computed via the power method on a Hermitian dual quaternion measurement matrix, followed by a dual quaternion generalized power method (DQGPM) that enforces feasibility through per-iteration projection. The estimation error bounds are established for spectral estimators, and DQGPM is shown to admit a finite-iteration error bound and achieves linear error contraction up to an explicit noise-dependent threshold. Experiments on synthetic benchmarks and real-world multi-scan point-set registration demonstrate that the proposed pipeline improves both accuracy and efficiency over representative matrix-based methods."
    },
    {
      "title": "MedAD-R1: Eliciting Consistent Reasoning in Interpretible Medical Anomaly Detection via Consistency-Reinforced Policy Optimization",
      "url": "https://arxiv.org/abs/2602.01081",
      "date": "2026-02-01",
      "authors_text": "Haitao Zhang, Yingying Wang, Jiaxiang Wang, Haote Xu, Hongyang Zhang",
      "is_highlight": false,
      "score": 64.0,
      "summary": "MedAD-R1 enhances medical anomaly detection by utilizing a two-stage training framework with consistency-reinforced policy optimization, achieving state-of-the-art performance on the MedAD-38K benchmark.",
      "teaser_image": "https://arxiv.org/html/2602.01081/x3.png",
      "debug_abstract": "Medical Anomaly Detection (MedAD) presents a significant opportunity to enhance diagnostic accuracy using Large Multimodal Models (LMMs) to interpret and answer questions based on medical images. However, the reliance on Supervised Fine-Tuning (SFT) on simplistic and fragmented datasets has hindered the development of models capable of plausible reasoning and robust multimodal generalization. To overcome this, we introduce MedAD-38K, the first large-scale, multi-modal, and multi-center benchmark for MedAD featuring diagnostic Chain-of-Thought (CoT) annotations alongside structured Visual Question-Answering (VQA) pairs. On this foundation, we propose a two-stage training framework. The first stage, Cognitive Injection, uses SFT to instill foundational medical knowledge and align the model with a structured think-then-answer paradigm. Given that standard policy optimization can produce reasoning that is disconnected from the final answer, the second stage incorporates Consistency Group Relative Policy Optimization (Con-GRPO). This novel algorithm incorporates a crucial consistency reward to ensure the generated reasoning process is relevant and logically coherent with the final diagnosis. Our proposed model, MedAD-R1, achieves state-of-the-art (SOTA) performance on the MedAD-38K benchmark, outperforming strong baselines by more than 10\\%. This superior performance stems from its ability to generate transparent and logically consistent reasoning pathways, offering a promising approach to enhancing the trustworthiness and interpretability of AI for clinical decision support."
    },
    {
      "title": "Adaptive Rollout Allocation for Online Reinforcement Learning with Verifiable Rewards",
      "url": "https://arxiv.org/abs/2602.01601",
      "date": "2026-02-02",
      "authors_text": "Hieu Trung Nguyen, Bao Nguyen, Wenao Ma, Yuzhi Zhao, Ruifeng She",
      "is_highlight": false,
      "score": 40.0,
      "summary": "The paper presents \\Ours, a variance-informed strategy for adaptive rollout allocation in online reinforcement learning, enhancing sampling efficiency and performance over traditional methods.",
      "teaser_image": "https://arxiv.org/html/2602.01601/x2.png",
      "debug_abstract": "Sampling efficiency is a key bottleneck in reinforcement learning with verifiable rewards. Existing group-based policy optimization methods, such as GRPO, allocate a fixed number of rollouts for all training prompts. This uniform allocation implicitly treats all prompts as equally informative, and could lead to inefficient computational budget usage and impede training progress. We introduce \\Ours, a Variance-Informed Predictive allocation strategy that allocates a given rollout budget to the prompts in the incumbent batch to minimize the expected gradient variance of the policy update. At each iteration, \\Ours~uses a lightweight Gaussian process model to predict per-prompt success probabilities based on recent rollouts. These probability predictions are translated into variance estimates, which are then fed into a convex optimization problem to determine the optimal rollout allocations under a hard compute budget constraint. Empirical results show that \\Ours~consistently improves sampling efficiency and achieves higher performance than uniform or heuristic allocation strategies in multiple benchmarks. Our code will be available at this https URL."
    },
    {
      "title": "Reasoning with Autoregressive-Diffusion Collaborative Thoughts",
      "url": "https://arxiv.org/abs/2602.01608",
      "date": "2026-02-02",
      "authors_text": "Mu Yuan, Liekang Zeng, Guoliang Xing, Lan Zhang, Yunhao Liu",
      "is_highlight": false,
      "score": 41.0,
      "summary": "The paper presents Collaborative Thoughts, a framework integrating autoregressive and diffusion models for enhanced spatial reasoning and generation through iterative feedback and joint reasoning.",
      "teaser_image": "https://arxiv.org/html/2602.01608/x2.png",
      "debug_abstract": "Autoregressive and diffusion models represent two complementary generative paradigms. Autoregressive models excel at sequential planning and constraint composition, yet struggle with tasks that require explicit spatial or physical grounding. Diffusion models, in contrast, capture rich spatial structure through high-dimensional generation, but lack the stepwise logical control needed to satisfy complex, multi-stage constraints or to reliably identify and correct errors. We introduce Collaborative Thoughts, a unified collaborative framework that enables autoregressive and diffusion models to reason and generate jointly through a closed-loop interaction. In Collaborative Thoughts, autoregressive models perform structured planning and constraint management, diffusion models instantiate these constraints as intermediate visual thoughts, and a vision-based critic module evaluates whether the visual thoughts satisfy the intended structural and physical requirements. This feedback is then used to iteratively refine subsequent planning and generation steps, mitigating error propagation across modalities. Importantly, Collaborative Thoughts uses the same collaborative loop regardless of whether the task is autoregressive question answering or diffusion-based visual generation. Through representative examples, we illustrate how Collaborative Thoughts can improve the reliability of spatial reasoning and the controllability of generation."
    },
    {
      "title": "Tilt-Ropter: A Novel Hybrid Aerial and Terrestrial Vehicle with Tilt Rotors and Passive Wheels",
      "url": "https://arxiv.org/abs/2602.01700",
      "date": "2026-02-02",
      "authors_text": "Ruoyu Wang, Xuchen Liu, Zongzhou Wu, Zixuan Guo, Wendi Ding",
      "is_highlight": false,
      "score": 38.0,
      "summary": "The Tilt-Ropter is a hybrid aerial-terrestrial vehicle that utilizes tilt rotors and passive wheels for efficient locomotion, featuring advanced control systems for enhanced mobility and energy savings.",
      "teaser_image": "https://arxiv.org/html/2602.01700/x2.png",
      "debug_abstract": "In this work, we present Tilt-Ropter, a novel hybrid aerial-terrestrial vehicle (HATV) that combines tilt rotors with passive wheels to achieve energy-efficient multi-mode locomotion. Unlike existing under-actuated HATVs, the fully actuated design of Tilt-Ropter enables decoupled force and torque control, greatly enhancing its mobility and environmental adaptability. A nonlinear model predictive controller (NMPC) is developed to track reference trajectories and handle contact constraints across locomotion modes, while a dedicated control allocation module exploits actuation redundancy to achieve energy-efficient control of actuators. Additionally, to enhance robustness during ground contact, we introduce an external wrench estimation algorithm that estimates environmental interaction forces and torques in real time. The system is validated through both simulation and real-world experiments, including seamless air-ground transitions and trajectory tracking. Results show low tracking errors in both modes and highlight a 92.8% reduction in power consumption during ground locomotion, demonstrating the system&#39;s potential for long-duration missions across large-scale and energy-constrained environments."
    },
    {
      "title": "Breaking the Static Graph: Context-Aware Traversal for Robust Retrieval-Augmented Generation",
      "url": "https://arxiv.org/abs/2602.01965",
      "date": "2026-02-02",
      "authors_text": "Kwun Hang Lau, Fangyuan Zhang, Boyu Ruan, Yingli Zhou, Qintian Guo",
      "is_highlight": false,
      "score": 27.0,
      "summary": "CatRAG enhances Retrieval-Augmented Generation by transforming static Knowledge Graphs into query-adaptive structures, improving multi-hop evidence retrieval and reasoning completeness.",
      "teaser_image": "https://arxiv.org/html/2602.01965/image/HubBiasShift.png",
      "debug_abstract": "Recent advances in Retrieval-Augmented Generation (RAG) have shifted from simple vector similarity to structure-aware approaches like HippoRAG, which leverage Knowledge Graphs (KGs) and Personalized PageRank (PPR) to capture multi-hop dependencies. However, these methods suffer from a &#34;Static Graph Fallacy&#34;: they rely on fixed transition probabilities determined during indexing. This rigidity ignores the query-dependent nature of edge relevance, causing semantic drift where random walks are diverted into high-degree &#34;hub&#34; nodes before reaching critical downstream evidence. Consequently, models often achieve high partial recall but fail to retrieve the complete evidence chain required for multi-hop queries. To address this, we propose CatRAG, Context-Aware Traversal for robust RAG, a framework that builds on the HippoRAG 2 architecture and transforms the static KG into a query-adaptive navigation structure. We introduce a multi-faceted framework to steer the random walk: (1) Symbolic Anchoring, which injects weak entity constraints to regularize the random walk; (2) Query-Aware Dynamic Edge Weighting, which dynamically modulates graph structure, to prune irrelevant paths while amplifying those aligned with the query&#39;s intent; and (3) Key-Fact Passage Weight Enhancement, a cost-efficient bias that structurally anchors the random walk to likely evidence. Experiments across four multi-hop benchmarks demonstrate that CatRAG consistently outperforms state of the art baselines. Our analysis reveals that while standard Recall metrics show modest gains, CatRAG achieves substantial improvements in reasoning completeness, the capacity to recover the entire evidence path without gaps. These results reveal that our approach effectively bridges the gap between retrieving partial context and enabling fully grounded reasoning. Resources are available at this https URL."
    },
    {
      "title": "Towards Agentic Intelligence for Materials Science",
      "url": "https://arxiv.org/abs/2602.00169",
      "date": "2026-01-29",
      "authors_text": "Huan Zhang, Yizhan Li, Wenhao Huang, Ziyu Hou, Yu Song",
      "is_highlight": false,
      "score": 12.0,
      "summary": "This paper advocates for developing agentic AI systems in materials science that autonomously plan, act, and learn to enhance discovery processes beyond traditional models.",
      "teaser_image": "https://arxiv.org/html/2602.00169/x1.png",
      "debug_abstract": "The convergence of artificial intelligence and materials science presents a transformative opportunity, but achieving true acceleration in discovery requires moving beyond task-isolated, fine-tuned models toward agentic systems that plan, act, and learn across the full discovery loop. This survey advances a unique pipeline-centric view that spans from corpus curation and pretraining, through domain adaptation and instruction tuning, to goal-conditioned agents interfacing with simulation and experimental platforms. Unlike prior reviews, we treat the entire process as an end-to-end system to be optimized for tangible discovery outcomes rather than proxy benchmarks. This perspective allows us to trace how upstream design choices-such as data curation and training objectives-can be aligned with downstream experimental success through effective credit assignment. To bridge communities and establish a shared frame of reference, we first present an integrated lens that aligns terminology, evaluation, and workflow stages across AI and materials science. We then analyze the field through two focused lenses: From the AI perspective, the survey details LLM strengths in pattern recognition, predictive analytics, and natural language processing for literature mining, materials characterization, and property prediction; from the materials science perspective, it highlights applications in materials design, process optimization, and the acceleration of computational workflows via integration with external tools (e.g., DFT, robotic labs). Finally, we contrast passive, reactive approaches with agentic design, cataloging current contributions while motivating systems that pursue long-horizon goals with autonomy, memory, and tool use. This survey charts a practical roadmap towards autonomous, safety-aware LLM agents aimed at discovering novel and useful materials."
    },
    {
      "title": "FlowSteer: Interactive Agentic Workflow Orchestration via End-to-End Reinforcement Learning",
      "url": "https://arxiv.org/abs/2602.01664",
      "date": "2026-02-02",
      "authors_text": "Mingda Zhang, Haoran Luo, Tiesunlong Shen, Qika Lin, Xiaoying Tang",
      "is_highlight": false,
      "score": 7.0,
      "summary": "FlowSteer is an end-to-end reinforcement learning framework that automates workflow orchestration through interactive learning, addressing manual costs and improving performance across diverse tasks.",
      "teaser_image": "https://arxiv.org/html/2602.01664/x1.png",
      "debug_abstract": "In recent years, a variety of powerful agentic workflows have been applied to solve a wide range of human problems. However, existing workflow orchestration still faces key challenges, including high manual cost, reliance on specific operators/large language models (LLMs), and sparse reward signals. To address these challenges, we propose FlowSteer, an end-to-end reinforcement learning framework that takes a lightweight policy model as the agent and an executable canvas environment, automating workflow orchestration through multi-turn interaction. In this process, the policy model analyzes execution states and selects editing actions, while the canvas executes operators and returns feedback for iterative refinement. Moreover, FlowSteer provides a plug-and-play framework that supports diverse operator libraries and interchangeable LLM backends. To effectively train this interaction paradigm, we propose Canvas Workflow Relative Policy Optimization (CWRPO), which introduces diversity-constrained rewards with conditional release to stabilize learning and suppress shortcut behaviors. Experimental results on twelve datasets show that FlowSteer significantly outperforms baselines across various tasks."
    },
    {
      "title": "Heterogeneous Vertiport Selection Optimization for On-Demand Air Taxi Services: A Deep Reinforcement Learning Approach",
      "url": "https://arxiv.org/abs/2601.21316",
      "date": "2026-01-29",
      "authors_text": "Aoyu Pang, Maonan Wang, Zifan Sha, Wenwei Yue, Changle Li",
      "is_highlight": false,
      "score": 80.0,
      "summary": "The paper presents a deep reinforcement learning framework for optimizing vertiport selection and air taxi routing, achieving significant travel time reductions in urban air mobility systems.",
      "teaser_image": "https://arxiv.org/html/2601.21316/x2.png",
      "debug_abstract": "Urban Air Mobility (UAM) has emerged as a transformative solution to alleviate urban congestion by utilizing low-altitude airspace, thereby reducing pressure on ground transportation networks. To enable truly efficient and seamless door-to-door travel experiences, UAM requires close integration with existing ground transportation infrastructure. However, current research on optimal integrated routing strategies for passengers in air-ground mobility systems remains limited, with a lack of systematic this http URL address this gap, we first propose a unified optimization model that integrates strategy selection for both air and ground transportation. This model captures the dynamic characteristics of multimodal transport networks and incorporates real-time traffic conditions alongside passenger decision-making behavior. Building on this model, we propose a Unified Air-Ground Mobility Coordination (UAGMC) framework, which leverages deep reinforcement learning (RL) and Vehicle-to-Everything (V2X) communication to optimize vertiport selection and dynamically plan air taxi routes. Experimental results demonstrate that UAGMC achieves a 34\\% reduction in average travel time compared to conventional proportional allocation methods, enhancing overall travel efficiency and providing novel insights into the integration and optimization of multimodal transportation systems. This work lays a solid foundation for advancing intelligent urban mobility solutions through the coordination of air and ground transportation modes. The related code can be found at this https URL."
    },
    {
      "title": "Disentangling perception and reasoning for improving data efficiency in learning cloth manipulation without demonstrations",
      "url": "https://arxiv.org/abs/2601.21713",
      "date": "2026-01-29",
      "authors_text": "Donatien Delehelle, Fei Chen, Darwin Caldwell",
      "is_highlight": false,
      "score": 87.0,
      "summary": "This paper presents a modular reinforcement learning approach for cloth manipulation that enhances data efficiency and reduces model size and training time, achieving improved performance in simulation and real-world transfer.",
      "teaser_image": "https://arxiv.org/html/2601.21713/x1.png",
      "debug_abstract": "Cloth manipulation is a ubiquitous task in everyday life, but it remains an open challenge for robotics. The difficulties in developing cloth manipulation policies are attributed to the high-dimensional state space, complex dynamics, and high propensity to self-occlusion exhibited by fabrics. As analytical methods have not been able to provide robust and general manipulation policies, reinforcement learning (RL) is considered a promising approach to these problems. However, to address the large state space and complex dynamics, data-based methods usually rely on large models and long training times. The resulting computational cost significantly hampers the development and adoption of these methods. Additionally, due to the challenge of robust state estimation, garment manipulation policies often adopt an end-to-end learning approach with workspace images as input. While this approach enables a conceptually straightforward sim-to-real transfer via real-world fine-tuning, it also incurs a significant computational cost by training agents on a highly lossy representation of the environment state. This paper questions this common design choice by exploring an efficient and modular approach to RL for cloth manipulation. We show that, through careful design choices, model size and training time can be significantly reduced when learning in simulation. Furthermore, we demonstrate how the resulting simulation-trained model can be transferred to the real world. We evaluate our approach on the SoftGym benchmark and achieve significant performance improvements over available baselines on our task, while using a substantially smaller model."
    },
    {
      "title": "Dynamic Topology Awareness: Breaking the Granularity Rigidity in Vision-Language Navigation",
      "url": "https://arxiv.org/abs/2601.21751",
      "date": "2026-01-29",
      "authors_text": "Jiankun Peng, Jianyuan Guo, Ying Xu, Yue Liu, Jiashuang Yan",
      "is_highlight": false,
      "score": 84.0,
      "summary": "The paper presents DGNav, a dynamic framework for Vision-Language Navigation that adapts topological map density and connectivity to enhance navigation efficiency and safety in complex environments.",
      "teaser_image": "https://arxiv.org/html/2601.21751/x1.png",
      "debug_abstract": "Vision-Language Navigation in Continuous Environments (VLN-CE) presents a core challenge: grounding high-level linguistic instructions into precise, safe, and long-horizon spatial actions. Explicit topological maps have proven to be a vital solution for providing robust spatial memory in such tasks. However, existing topological planning methods suffer from a &#34;Granularity Rigidity&#34; problem. Specifically, these methods typically rely on fixed geometric thresholds to sample nodes, which fails to adapt to varying environmental complexities. This rigidity leads to a critical mismatch: the model tends to over-sample in simple areas, causing computational redundancy, while under-sampling in high-uncertainty regions, increasing collision risks and compromising precision. To address this, we propose DGNav, a framework for Dynamic Topological Navigation, introducing a context-aware mechanism to modulate map density and connectivity on-the-fly. Our approach comprises two core innovations: (1) A Scene-Aware Adaptive Strategy that dynamically modulates graph construction thresholds based on the dispersion of predicted waypoints, enabling &#34;densification on demand&#34; in challenging environments; (2) A Dynamic Graph Transformer that reconstructs graph connectivity by fusing visual, linguistic, and geometric cues into dynamic edge weights, enabling the agent to filter out topological noise and enhancing instruction adherence. Extensive experiments on the R2R-CE and RxR-CE benchmarks demonstrate DGNav exhibits superior navigation performance and strong generalization capabilities. Furthermore, ablation studies confirm that our framework achieves an optimal trade-off between navigation efficiency and safe exploration. The code is available at this https URL."
    },
    {
      "title": "PLANING: A Loosely Coupled Triangle-Gaussian Framework for Streaming 3D Reconstruction",
      "url": "https://arxiv.org/abs/2601.22046",
      "date": "2026-01-29",
      "authors_text": "Changjian Jiang, Kerui Ren, Xudong Li, Kaiwen Song, Linning Xu",
      "is_highlight": false,
      "score": 66.0,
      "summary": "PLANING is a novel streaming 3D reconstruction framework that efficiently combines geometric primitives and neural Gaussians, enhancing quality and speed while reducing redundancy.",
      "teaser_image": "https://arxiv.org/html/2601.22046/x3.png",
      "debug_abstract": "Streaming reconstruction from monocular image sequences remains challenging, as existing methods typically favor either high-quality rendering or accurate geometry, but rarely both. We present PLANING, an efficient on-the-fly reconstruction framework built on a hybrid representation that loosely couples explicit geometric primitives with neural Gaussians, enabling geometry and appearance to be modeled in a decoupled manner. This decoupling supports an online initialization and optimization strategy that separates geometry and appearance updates, yielding stable streaming reconstruction with substantially reduced structural redundancy. PLANING improves dense mesh Chamfer-L2 by 18.52% over PGSR, surpasses ARTDECO by 1.31 dB PSNR, and reconstructs ScanNetV2 scenes in under 100 seconds, over 5x faster than 2D Gaussian Splatting, while matching the quality of offline per-scene optimization. Beyond reconstruction quality, the structural clarity and computational efficiency of \\modelname~make it well suited for a broad range of downstream applications, such as enabling large-scale scene modeling and simulation-ready environments for embodied AI. Project page: this https URL ."
    },
    {
      "title": "Rhombot: Rhombus-shaped Modular Robots for Stable, Medium-Independent Reconfiguration Motion",
      "url": "https://arxiv.org/abs/2601.19529",
      "date": "2026-01-27",
      "authors_text": "Jie Gu, Yirui Sun, Zhihao Xia, Tin Lun Lam, Chunxu Tian",
      "is_highlight": false,
      "score": 86.0,
      "summary": "Rhombot is a modular self-reconfigurable robot featuring rhombus-shaped modules that enable stable, medium-independent morphing and locomotion through a novel motion primitive called morphpivoting.",
      "debug_abstract": "In this paper, we present Rhombot, a novel deformable planar lattice modular self-reconfigurable robot (MSRR) with a rhombus shaped module. Each module consists of a parallelogram skeleton with a single centrally mounted actuator that enables folding and unfolding along its diagonal. The core design philosophy is to achieve essential MSRR functionalities such as morphing, docking, and locomotion with minimal control complexity. This enables a continuous and stable reconfiguration process that is independent of the surrounding medium, allowing the system to reliably form various configurations in diverse environments. To leverage the unique kinematics of Rhombot, we introduce morphpivoting, a novel motion primitive for reconfiguration that differs from advanced MSRR systems, and propose a strategy for its continuous execution. Finally, a series of physical experiments validate the module&#39;s stable reconfiguration ability, as well as its positional and docking accuracy."
    },
    {
      "title": "Component-Level Lesioning of Language Models Reveals Clinically Aligned Aphasia Phenotypes",
      "url": "https://arxiv.org/abs/2601.19723",
      "date": "2026-01-27",
      "authors_text": "Yifan Wang, Jichen Zheng, Jingyuan Sun, Yunhao Zhang, Chunyu Ye",
      "is_highlight": false,
      "score": 62.0,
      "summary": "This study presents a framework for simulating aphasia in language models by selectively perturbing components, revealing clinically relevant language impairments and enhancing understanding of language function degradation.",
      "debug_abstract": "Large language models (LLMs) increasingly exhibit human-like linguistic behaviors and internal representations that they could serve as computational simulators of language cognition. We ask whether LLMs can be systematically manipulated to reproduce language-production impairments characteristic of aphasia following focal brain lesions. Such models could provide scalable proxies for testing rehabilitation hypotheses, and offer a controlled framework for probing the functional organization of language. We introduce a clinically grounded, component-level framework that simulates aphasia by selectively perturbing functional components in LLMs, and apply it to both modular Mixture-of-Experts models and dense Transformers using a unified intervention interface. Our pipeline (i) identifies subtype-linked components for Broca&#39;s and Wernicke&#39;s aphasia, (ii) interprets these components with linguistic probing tasks, and (iii) induces graded impairments by progressively perturbing the top-k subtype-linked components, evaluating outcomes with Western Aphasia Battery (WAB) subtests summarized by Aphasia Quotient (AQ). Across architectures and lesioning strategies, subtype-targeted perturbations yield more systematic, aphasia-like regressions than size-matched random perturbations, and MoE modularity supports more localized and interpretable phenotype-to-component mappings. These findings suggest that modular LLMs, combined with clinically informed component perturbations, provide a promising platform for simulating aphasic language production and studying how distinct language functions degrade under targeted disruptions."
    },
    {
      "title": "EgoHandICL: Egocentric 3D Hand Reconstruction with In-Context Learning",
      "url": "https://arxiv.org/abs/2601.19850",
      "date": "2026-01-27",
      "authors_text": "Binzhu Xie, Shi Qiu, Sicheng Zhang, Yinqiao Wang, Hao Xu",
      "is_highlight": false,
      "score": 76.0,
      "summary": "EgoHandICL introduces an innovative in-context learning framework for robust 3D hand reconstruction in egocentric vision, enhancing performance through exemplar retrieval and multimodal context integration.",
      "debug_abstract": "Robust 3D hand reconstruction in egocentric vision is challenging due to depth ambiguity, self-occlusion, and complex hand-object interactions. Prior methods mitigate these issues by scaling training data or adding auxiliary cues, but they often struggle in unseen contexts. We present EgoHandICL, the first in-context learning (ICL) framework for 3D hand reconstruction that improves semantic alignment, visual consistency, and robustness under challenging egocentric conditions. EgoHandICL introduces complementary exemplar retrieval guided by vision-language models (VLMs), an ICL-tailored tokenizer for multimodal context, and a masked autoencoder (MAE)-based architecture trained with hand-guided geometric and perceptual objectives. Experiments on ARCTIC and EgoExo4D show consistent gains over state-of-the-art methods. We also demonstrate real-world generalization and improve EgoVLM hand-object interaction reasoning by using reconstructed hands as visual prompts. Code and data: this https URL"
    },
    {
      "title": "Advances and Innovations in the Multi-Agent Robotic System (MARS) Challenge",
      "url": "https://arxiv.org/abs/2601.18733",
      "date": "2026-01-26",
      "authors_text": "Li Kang, Heng Zhou, Xiufeng Song, Rui Li, Bruno N. Y. Chen",
      "is_highlight": false,
      "score": 94.0,
      "summary": "The MARS Challenge promotes advancements in multi-agent robotic systems by exploring vision-language models for collaborative task execution and planning in dynamic environments.",
      "debug_abstract": "Recent advancements in multimodal large language models and vision-languageaction models have significantly driven progress in Embodied AI. As the field transitions toward more complex task scenarios, multi-agent system frameworks are becoming essential for achieving scalable, efficient, and collaborative solutions. This shift is fueled by three primary factors: increasing agent capabilities, enhancing system efficiency through task delegation, and enabling advanced human-agent interactions. To address the challenges posed by multi-agent collaboration, we propose the Multi-Agent Robotic System (MARS) Challenge, held at the NeurIPS 2025 Workshop on SpaVLE. The competition focuses on two critical areas: planning and control, where participants explore multi-agent embodied planning using vision-language models (VLMs) to coordinate tasks and policy execution to perform robotic manipulation in dynamic environments. By evaluating solutions submitted by participants, the challenge provides valuable insights into the design and coordination of embodied multi-agent systems, contributing to the future development of advanced collaborative AI systems."
    },
    {
      "title": "AdaReasoner: Dynamic Tool Orchestration for Iterative Visual Reasoning",
      "url": "https://arxiv.org/abs/2601.18631",
      "date": "2026-01-26",
      "authors_text": "Mingyang Song, Haoyu Sun, Jiawei Gu, Linjie Li, Luxin Xu",
      "is_highlight": false,
      "score": 61.0,
      "summary": "AdaReasoner is a multimodal model that autonomously optimizes tool selection and usage for iterative visual reasoning, achieving state-of-the-art performance on various benchmarks.",
      "debug_abstract": "When humans face problems beyond their immediate capabilities, they rely on tools, providing a promising paradigm for improving visual reasoning in multimodal large language models (MLLMs). Effective reasoning, therefore, hinges on knowing which tools to use, when to invoke them, and how to compose them over multiple steps, even when faced with new tools or new tasks. We introduce \\textbf{AdaReasoner}, a family of multimodal models that learn tool use as a general reasoning skill rather than as tool-specific or explicitly supervised behavior. AdaReasoner is enabled by (i) a scalable data curation pipeline exposing models to long-horizon, multi-step tool interactions; (ii) Tool-GRPO, a reinforcement learning algorithm that optimizes tool selection and sequencing based on end-task success; and (iii) an adaptive learning mechanism that dynamically regulates tool usage. Together, these components allow models to infer tool utility from task context and intermediate outcomes, enabling coordination of multiple tools and generalization to unseen tools. Empirically, AdaReasoner exhibits strong tool-adaptive and generalization behaviors: it autonomously adopts beneficial tools, suppresses irrelevant ones, and adjusts tool usage frequency based on task demands, despite never being explicitly trained to do so. These capabilities translate into state-of-the-art performance across challenging benchmarks, improving the 7B base model by +24.9\\% on average and surpassing strong proprietary systems such as GPT-5 on multiple tasks, including VSP and Jigsaw."
    },
    {
      "title": "Timely Machine: Awareness of Time Makes Test-Time Scaling Agentic",
      "url": "https://arxiv.org/abs/2601.16486",
      "date": "2026-01-23",
      "authors_text": "Yichuan Ma, Linyang Li, Yongkang chen, Peiji Li, Xiaozhe Li",
      "is_highlight": false,
      "score": 78.0,
      "summary": "The paper introduces Timely Machine, redefining test-time scaling for LLMs based on wall-clock time, enhancing performance through dynamic strategy adjustments and reinforcement learning.",
      "debug_abstract": "As large language models (LLMs) increasingly tackle complex reasoning tasks, test-time scaling has become critical for enhancing capabilities. However, in agentic scenarios with frequent tool calls, the traditional generation-length-based definition breaks down: tool latency decouples inference time from generation length. We propose Timely Machine, redefining test-time as wall-clock time, where models dynamically adjust strategies based on time budgets. We introduce Timely-Eval, a benchmark spanning high-frequency tool calls, low-frequency tool calls, and time-constrained reasoning. By varying tool latency, we find smaller models excel with fast feedback through more interactions, while larger models dominate high-latency settings via superior interaction quality. Moreover, existing models fail to adapt reasoning to time budgets. We propose Timely-RL to address this gap. After cold-start supervised fine-tuning, we use reinforcement learning to enhance temporal planning. Timely-RL improves time budget awareness and consistently boosts performance across Timely-Eval. We hope our work offers a new perspective on test-time scaling for the agentic era."
    },
    {
      "title": "Adaptive Reinforcement and Model Predictive Control Switching for Safe Human-Robot Cooperative Navigation",
      "url": "https://arxiv.org/abs/2601.16686",
      "date": "2026-01-23",
      "authors_text": "Ning Liu, Sen Shen, Zheng Li, Matthew D'Souza, Jen Jen Chung",
      "is_highlight": false,
      "score": 94.0,
      "summary": "The paper presents ARMS, a hybrid control framework combining reinforcement learning and model predictive control for safe human-robot navigation, achieving superior performance in cluttered environments.",
      "debug_abstract": "This paper addresses the challenge of human-guided navigation for mobile collaborative robots under simultaneous proximity regulation and safety constraints. We introduce Adaptive Reinforcement and Model Predictive Control Switching (ARMS), a hybrid learning-control framework that integrates a reinforcement learning follower trained with Proximal Policy Optimization (PPO) and an analytical one-step Model Predictive Control (MPC) formulated as a quadratic program safety filter. To enable robust perception under partial observability and non-stationary human motion, ARMS employs a decoupled sensing architecture with a Long Short-Term Memory (LSTM) temporal encoder for the human-robot relative state and a spatial encoder for 360-degree LiDAR scans. The core contribution is a learned adaptive neural switcher that performs context-aware soft action fusion between the two controllers, favoring conservative, constraint-aware QP-based control in low-risk regions while progressively shifting control authority to the learned follower in highly cluttered or constrained scenarios where maneuverability is critical, and reverting to the follower action when the QP becomes infeasible. Extensive evaluations against Pure Pursuit, Dynamic Window Approach (DWA), and an RL-only baseline demonstrate that ARMS achieves an 82.5 percent success rate in highly cluttered environments, outperforming DWA and RL-only approaches by 7.1 percent and 3.1 percent, respectively, while reducing average computational latency by 33 percent to 5.2 milliseconds compared to a multi-step MPC baseline. Additional simulation transfer in Gazebo and initial real-world deployment results further indicate the practicality and robustness of ARMS for safe and efficient human-robot collaboration. Source code and a demonstration video are available at this https URL."
    },
    {
      "title": "Natural Language-Driven Global Mapping of Martian Landforms",
      "url": "https://arxiv.org/abs/2601.15949",
      "date": "2026-01-22",
      "authors_text": "Yiran Wang, Shuoyuan Wang, Zhaoran Wei, Jiannan Zhao, Zhonghua Yao",
      "is_highlight": false,
      "score": 72.0,
      "summary": "MarScope is a vision-language framework that enables natural language-driven, label-free mapping of Martian landforms, enhancing global geomorphic analysis and user query flexibility.",
      "debug_abstract": "Planetary surfaces are typically analyzed using high-level semantic concepts in natural language, yet vast orbital image archives remain organized at the pixel level. This mismatch limits scalable, open-ended exploration of planetary surfaces. Here we present MarScope, a planetary-scale vision-language framework enabling natural language-driven, label-free mapping of Martian landforms. MarScope aligns planetary images and text in a shared semantic space, trained on over 200,000 curated image-text pairs. This framework transforms global geomorphic mapping on Mars by replacing pre-defined classifications with flexible semantic retrieval, enabling arbitrary user queries across the entire planet in 5 seconds with F1 scores up to 0.978. Applications further show that it extends beyond morphological classification to facilitate process-oriented analysis and similarity-based geomorphological mapping at a planetary scale. MarScope establishes a new paradigm where natural language serves as a direct interface for scientific discovery over massive geospatial datasets."
    },
    {
      "title": "Robust Tool Use via Fission-GRPO: Learning to Recover from Execution Errors",
      "url": "https://arxiv.org/abs/2601.15625",
      "date": "2026-01-22",
      "authors_text": "Zhiwei Zhang, Fei Zhao, Rui Wang, Zezhong Wang, Bin Liang",
      "is_highlight": false,
      "score": 87.1,
      "summary": "Fission-GRPO enhances tool use in LLMs by converting execution errors into corrective supervision, improving error recovery and overall accuracy in multi-turn interactions.",
      "debug_abstract": "Large language models (LLMs) can call tools effectively, yet they remain brittle in multi-turn execution: following a tool call error, smaller models often degenerate into repetitive invalid re-invocations, failing to interpret error feedback and self-correct. This brittleness hinders reliable real-world deployment, where the execution errors are inherently inevitable during tool interaction procedures. We identify a key limitation of current approaches: standard reinforcement learning (RL) treats errors as sparse negative rewards, providing no guidance on how to recover, while pre-collected synthetic error-correction datasets suffer from distribution mismatch with the model&#39;s on-policy error modes. To bridge this gap, we propose Fission-GRPO, a framework that converts execution errors into corrective supervision within the RL training loop. Our core mechanism fissions each failed trajectory into a new training instance by augmenting it with diagnostic feedback from a finetuned Error Simulator, then resampling recovery rollouts on-policy. This enables the model to learn from the precise errors it makes during exploration, rather than from static, pre-collected error cases. On the BFCL v4 Multi-Turn, Fission-GRPO improves the error recovery rate of Qwen3-8B by 5.7% absolute, crucially, yielding a 4% overall accuracy gain (42.75% to 46.75%) over GRPO and outperforming specialized tool-use agents."
    },
    {
      "title": "PhysicsMind: Sim and Real Mechanics Benchmarking for Physical Reasoning and Prediction in Foundational VLMs and World Models",
      "url": "https://arxiv.org/abs/2601.16007",
      "date": "2026-01-22",
      "authors_text": "Chak-Wing Mak, Guanyu Zhu, Boyi Zhang, Hongji Li, Xiaowei Chi",
      "is_highlight": false,
      "score": 94.1,
      "summary": "PhysicsMind introduces a benchmark for evaluating physical reasoning in multimodal models through real and simulated tasks focused on fundamental physics principles, revealing significant gaps in current models' understanding.",
      "debug_abstract": "Modern foundational Multimodal Large Language Models (MLLMs) and video world models have advanced significantly in mathematical, common-sense, and visual reasoning, but their grasp of the underlying physics remains underexplored. Existing benchmarks attempting to measure this matter rely on synthetic, Visual Question Answer templates or focus on perceptual video quality that is tangential to measuring how well the video abides by physical laws. To address this fragmentation, we introduce PhysicsMind, a unified benchmark with both real and simulation environments that evaluates law-consistent reasoning and generation over three canonical principles: Center of Mass, Lever Equilibrium, and Newton&#39;s First Law. PhysicsMind comprises two main tasks: i) VQA tasks, testing whether models can reason and determine physical quantities and values from images or short videos, and ii) Video Generation(VG) tasks, evaluating if predicted motion trajectories obey the same center-of-mass, torque, and inertial constraints as the ground truth. A broad range of recent models and video generation models is evaluated on PhysicsMind and found to rely on appearance heuristics while often violating basic mechanics. These gaps indicate that current scaling and training are still insufficient for robust physical understanding, underscoring PhysicsMind as a focused testbed for physics-aware multimodal models. Our data will be released upon acceptance."
    }
  ],
  "机器人与人工智能实验室": [
    {
      "title": "Search-R2: Enhancing Search-Integrated Reasoning via Actor-Refiner Collaboration",
      "url": "https://arxiv.org/abs/2602.03647",
      "date": "2026-02-03",
      "authors_text": "Bowei He, Minda Hu, Zenan Xu, Hongru Wang, Licheng Zong",
      "is_highlight": false,
      "score": 52.0,
      "summary": "Search-R2 introduces an Actor-Refiner framework that enhances search-integrated reasoning in language agents through targeted intervention and fine-grained supervision, outperforming existing methods.",
      "teaser_image": "https://arxiv.org/html/2602.03647/x2.png",
      "debug_abstract": "Search-integrated reasoning enables language agents to transcend static parametric knowledge by actively querying external sources. However, training these agents via reinforcement learning is hindered by the multi-scale credit assignment problem: existing methods typically rely on sparse, trajectory-level rewards that fail to distinguish between high-quality reasoning and fortuitous guesses, leading to redundant or misleading search behaviors. To address this, we propose Search-R2, a novel Actor-Refiner collaboration framework that enhances reasoning through targeted intervention, with both components jointly optimized during training. Our approach decomposes the generation process into an Actor, which produces initial reasoning trajectories, and a Meta-Refiner, which selectively diagnoses and repairs flawed steps via a &#39;cut-and-regenerate&#39; mechanism. To provide fine-grained supervision, we introduce a hybrid reward design that couples outcome correctness with a dense process reward quantifying the information density of retrieved evidence. Theoretically, we formalize the Actor-Refiner interaction as a smoothed mixture policy, proving that selective correction yields strict performance gains over strong baselines. Extensive experiments across various general and multi-hop QA datasets demonstrate that Search-R2 consistently outperforms strong RAG and RL-based baselines across model scales, achieving superior reasoning accuracy with minimal overhead."
    },
    {
      "title": "Dual Quaternion SE(3) Synchronization with Recovery Guarantees",
      "url": "https://arxiv.org/abs/2602.00324",
      "date": "2026-01-30",
      "authors_text": "Jianing Zhao, Linglingzhi Zhu, Anthony Man-Cho So",
      "is_highlight": false,
      "score": 75.0,
      "summary": "This paper presents a dual quaternion-based SE(3) synchronization method with a two-stage algorithm that guarantees recovery of absolute poses from noisy transformations, outperforming traditional matrix methods.",
      "teaser_image": "https://arxiv.org/html/2602.00324/x2.png",
      "debug_abstract": "Synchronization over the special Euclidean group SE(3) aims to recover absolute poses from noisy pairwise relative transformations and is a core primitive in robotics and 3D vision. Standard approaches often require multi-step heuristic procedures to recover valid poses, which are difficult to analyze and typically lack theoretical guarantees. This paper adopts a dual quaternion representation and formulates SE(3) synchronization directly over the unit dual quaternion. A two-stage algorithm is developed: A spectral initializer computed via the power method on a Hermitian dual quaternion measurement matrix, followed by a dual quaternion generalized power method (DQGPM) that enforces feasibility through per-iteration projection. The estimation error bounds are established for spectral estimators, and DQGPM is shown to admit a finite-iteration error bound and achieves linear error contraction up to an explicit noise-dependent threshold. Experiments on synthetic benchmarks and real-world multi-scan point-set registration demonstrate that the proposed pipeline improves both accuracy and efficiency over representative matrix-based methods."
    },
    {
      "title": "MedAD-R1: Eliciting Consistent Reasoning in Interpretible Medical Anomaly Detection via Consistency-Reinforced Policy Optimization",
      "url": "https://arxiv.org/abs/2602.01081",
      "date": "2026-02-01",
      "authors_text": "Haitao Zhang, Yingying Wang, Jiaxiang Wang, Haote Xu, Hongyang Zhang",
      "is_highlight": false,
      "score": 64.0,
      "summary": "MedAD-R1 enhances medical anomaly detection by utilizing a two-stage training framework with consistency-reinforced policy optimization, achieving state-of-the-art performance on the MedAD-38K benchmark.",
      "teaser_image": "https://arxiv.org/html/2602.01081/x3.png",
      "debug_abstract": "Medical Anomaly Detection (MedAD) presents a significant opportunity to enhance diagnostic accuracy using Large Multimodal Models (LMMs) to interpret and answer questions based on medical images. However, the reliance on Supervised Fine-Tuning (SFT) on simplistic and fragmented datasets has hindered the development of models capable of plausible reasoning and robust multimodal generalization. To overcome this, we introduce MedAD-38K, the first large-scale, multi-modal, and multi-center benchmark for MedAD featuring diagnostic Chain-of-Thought (CoT) annotations alongside structured Visual Question-Answering (VQA) pairs. On this foundation, we propose a two-stage training framework. The first stage, Cognitive Injection, uses SFT to instill foundational medical knowledge and align the model with a structured think-then-answer paradigm. Given that standard policy optimization can produce reasoning that is disconnected from the final answer, the second stage incorporates Consistency Group Relative Policy Optimization (Con-GRPO). This novel algorithm incorporates a crucial consistency reward to ensure the generated reasoning process is relevant and logically coherent with the final diagnosis. Our proposed model, MedAD-R1, achieves state-of-the-art (SOTA) performance on the MedAD-38K benchmark, outperforming strong baselines by more than 10\\%. This superior performance stems from its ability to generate transparent and logically consistent reasoning pathways, offering a promising approach to enhancing the trustworthiness and interpretability of AI for clinical decision support."
    },
    {
      "title": "Adaptive Rollout Allocation for Online Reinforcement Learning with Verifiable Rewards",
      "url": "https://arxiv.org/abs/2602.01601",
      "date": "2026-02-02",
      "authors_text": "Hieu Trung Nguyen, Bao Nguyen, Wenao Ma, Yuzhi Zhao, Ruifeng She",
      "is_highlight": false,
      "score": 40.0,
      "summary": "The paper presents \\Ours, a variance-informed strategy for adaptive rollout allocation in online reinforcement learning, enhancing sampling efficiency and performance over traditional methods.",
      "teaser_image": "https://arxiv.org/html/2602.01601/x2.png",
      "debug_abstract": "Sampling efficiency is a key bottleneck in reinforcement learning with verifiable rewards. Existing group-based policy optimization methods, such as GRPO, allocate a fixed number of rollouts for all training prompts. This uniform allocation implicitly treats all prompts as equally informative, and could lead to inefficient computational budget usage and impede training progress. We introduce \\Ours, a Variance-Informed Predictive allocation strategy that allocates a given rollout budget to the prompts in the incumbent batch to minimize the expected gradient variance of the policy update. At each iteration, \\Ours~uses a lightweight Gaussian process model to predict per-prompt success probabilities based on recent rollouts. These probability predictions are translated into variance estimates, which are then fed into a convex optimization problem to determine the optimal rollout allocations under a hard compute budget constraint. Empirical results show that \\Ours~consistently improves sampling efficiency and achieves higher performance than uniform or heuristic allocation strategies in multiple benchmarks. Our code will be available at this https URL."
    },
    {
      "title": "Reasoning with Autoregressive-Diffusion Collaborative Thoughts",
      "url": "https://arxiv.org/abs/2602.01608",
      "date": "2026-02-02",
      "authors_text": "Mu Yuan, Liekang Zeng, Guoliang Xing, Lan Zhang, Yunhao Liu",
      "is_highlight": false,
      "score": 41.0,
      "summary": "The paper presents Collaborative Thoughts, a framework integrating autoregressive and diffusion models for enhanced spatial reasoning and generation through iterative feedback and joint reasoning.",
      "teaser_image": "https://arxiv.org/html/2602.01608/x2.png",
      "debug_abstract": "Autoregressive and diffusion models represent two complementary generative paradigms. Autoregressive models excel at sequential planning and constraint composition, yet struggle with tasks that require explicit spatial or physical grounding. Diffusion models, in contrast, capture rich spatial structure through high-dimensional generation, but lack the stepwise logical control needed to satisfy complex, multi-stage constraints or to reliably identify and correct errors. We introduce Collaborative Thoughts, a unified collaborative framework that enables autoregressive and diffusion models to reason and generate jointly through a closed-loop interaction. In Collaborative Thoughts, autoregressive models perform structured planning and constraint management, diffusion models instantiate these constraints as intermediate visual thoughts, and a vision-based critic module evaluates whether the visual thoughts satisfy the intended structural and physical requirements. This feedback is then used to iteratively refine subsequent planning and generation steps, mitigating error propagation across modalities. Importantly, Collaborative Thoughts uses the same collaborative loop regardless of whether the task is autoregressive question answering or diffusion-based visual generation. Through representative examples, we illustrate how Collaborative Thoughts can improve the reliability of spatial reasoning and the controllability of generation."
    },
    {
      "title": "Tilt-Ropter: A Novel Hybrid Aerial and Terrestrial Vehicle with Tilt Rotors and Passive Wheels",
      "url": "https://arxiv.org/abs/2602.01700",
      "date": "2026-02-02",
      "authors_text": "Ruoyu Wang, Xuchen Liu, Zongzhou Wu, Zixuan Guo, Wendi Ding",
      "is_highlight": false,
      "score": 38.0,
      "summary": "The Tilt-Ropter is a hybrid aerial-terrestrial vehicle that utilizes tilt rotors and passive wheels for efficient locomotion, featuring advanced control systems for enhanced mobility and energy savings.",
      "teaser_image": "https://arxiv.org/html/2602.01700/x2.png",
      "debug_abstract": "In this work, we present Tilt-Ropter, a novel hybrid aerial-terrestrial vehicle (HATV) that combines tilt rotors with passive wheels to achieve energy-efficient multi-mode locomotion. Unlike existing under-actuated HATVs, the fully actuated design of Tilt-Ropter enables decoupled force and torque control, greatly enhancing its mobility and environmental adaptability. A nonlinear model predictive controller (NMPC) is developed to track reference trajectories and handle contact constraints across locomotion modes, while a dedicated control allocation module exploits actuation redundancy to achieve energy-efficient control of actuators. Additionally, to enhance robustness during ground contact, we introduce an external wrench estimation algorithm that estimates environmental interaction forces and torques in real time. The system is validated through both simulation and real-world experiments, including seamless air-ground transitions and trajectory tracking. Results show low tracking errors in both modes and highlight a 92.8% reduction in power consumption during ground locomotion, demonstrating the system&#39;s potential for long-duration missions across large-scale and energy-constrained environments."
    },
    {
      "title": "Breaking the Static Graph: Context-Aware Traversal for Robust Retrieval-Augmented Generation",
      "url": "https://arxiv.org/abs/2602.01965",
      "date": "2026-02-02",
      "authors_text": "Kwun Hang Lau, Fangyuan Zhang, Boyu Ruan, Yingli Zhou, Qintian Guo",
      "is_highlight": false,
      "score": 27.0,
      "summary": "CatRAG enhances Retrieval-Augmented Generation by transforming static Knowledge Graphs into query-adaptive structures, improving multi-hop evidence retrieval and reasoning completeness.",
      "teaser_image": "https://arxiv.org/html/2602.01965/image/HubBiasShift.png",
      "debug_abstract": "Recent advances in Retrieval-Augmented Generation (RAG) have shifted from simple vector similarity to structure-aware approaches like HippoRAG, which leverage Knowledge Graphs (KGs) and Personalized PageRank (PPR) to capture multi-hop dependencies. However, these methods suffer from a &#34;Static Graph Fallacy&#34;: they rely on fixed transition probabilities determined during indexing. This rigidity ignores the query-dependent nature of edge relevance, causing semantic drift where random walks are diverted into high-degree &#34;hub&#34; nodes before reaching critical downstream evidence. Consequently, models often achieve high partial recall but fail to retrieve the complete evidence chain required for multi-hop queries. To address this, we propose CatRAG, Context-Aware Traversal for robust RAG, a framework that builds on the HippoRAG 2 architecture and transforms the static KG into a query-adaptive navigation structure. We introduce a multi-faceted framework to steer the random walk: (1) Symbolic Anchoring, which injects weak entity constraints to regularize the random walk; (2) Query-Aware Dynamic Edge Weighting, which dynamically modulates graph structure, to prune irrelevant paths while amplifying those aligned with the query&#39;s intent; and (3) Key-Fact Passage Weight Enhancement, a cost-efficient bias that structurally anchors the random walk to likely evidence. Experiments across four multi-hop benchmarks demonstrate that CatRAG consistently outperforms state of the art baselines. Our analysis reveals that while standard Recall metrics show modest gains, CatRAG achieves substantial improvements in reasoning completeness, the capacity to recover the entire evidence path without gaps. These results reveal that our approach effectively bridges the gap between retrieving partial context and enabling fully grounded reasoning. Resources are available at this https URL."
    },
    {
      "title": "Towards Agentic Intelligence for Materials Science",
      "url": "https://arxiv.org/abs/2602.00169",
      "date": "2026-01-29",
      "authors_text": "Huan Zhang, Yizhan Li, Wenhao Huang, Ziyu Hou, Yu Song",
      "is_highlight": false,
      "score": 12.0,
      "summary": "This paper advocates for developing agentic AI systems in materials science that autonomously plan, act, and learn to enhance discovery processes beyond traditional models.",
      "teaser_image": "https://arxiv.org/html/2602.00169/x1.png",
      "debug_abstract": "The convergence of artificial intelligence and materials science presents a transformative opportunity, but achieving true acceleration in discovery requires moving beyond task-isolated, fine-tuned models toward agentic systems that plan, act, and learn across the full discovery loop. This survey advances a unique pipeline-centric view that spans from corpus curation and pretraining, through domain adaptation and instruction tuning, to goal-conditioned agents interfacing with simulation and experimental platforms. Unlike prior reviews, we treat the entire process as an end-to-end system to be optimized for tangible discovery outcomes rather than proxy benchmarks. This perspective allows us to trace how upstream design choices-such as data curation and training objectives-can be aligned with downstream experimental success through effective credit assignment. To bridge communities and establish a shared frame of reference, we first present an integrated lens that aligns terminology, evaluation, and workflow stages across AI and materials science. We then analyze the field through two focused lenses: From the AI perspective, the survey details LLM strengths in pattern recognition, predictive analytics, and natural language processing for literature mining, materials characterization, and property prediction; from the materials science perspective, it highlights applications in materials design, process optimization, and the acceleration of computational workflows via integration with external tools (e.g., DFT, robotic labs). Finally, we contrast passive, reactive approaches with agentic design, cataloging current contributions while motivating systems that pursue long-horizon goals with autonomy, memory, and tool use. This survey charts a practical roadmap towards autonomous, safety-aware LLM agents aimed at discovering novel and useful materials."
    },
    {
      "title": "FlowSteer: Interactive Agentic Workflow Orchestration via End-to-End Reinforcement Learning",
      "url": "https://arxiv.org/abs/2602.01664",
      "date": "2026-02-02",
      "authors_text": "Mingda Zhang, Haoran Luo, Tiesunlong Shen, Qika Lin, Xiaoying Tang",
      "is_highlight": false,
      "score": 7.0,
      "summary": "FlowSteer is an end-to-end reinforcement learning framework that automates workflow orchestration through interactive learning, addressing manual costs and improving performance across diverse tasks.",
      "teaser_image": "https://arxiv.org/html/2602.01664/x1.png",
      "debug_abstract": "In recent years, a variety of powerful agentic workflows have been applied to solve a wide range of human problems. However, existing workflow orchestration still faces key challenges, including high manual cost, reliance on specific operators/large language models (LLMs), and sparse reward signals. To address these challenges, we propose FlowSteer, an end-to-end reinforcement learning framework that takes a lightweight policy model as the agent and an executable canvas environment, automating workflow orchestration through multi-turn interaction. In this process, the policy model analyzes execution states and selects editing actions, while the canvas executes operators and returns feedback for iterative refinement. Moreover, FlowSteer provides a plug-and-play framework that supports diverse operator libraries and interchangeable LLM backends. To effectively train this interaction paradigm, we propose Canvas Workflow Relative Policy Optimization (CWRPO), which introduces diversity-constrained rewards with conditional release to stabilize learning and suppress shortcut behaviors. Experimental results on twelve datasets show that FlowSteer significantly outperforms baselines across various tasks."
    },
    {
      "title": "Heterogeneous Vertiport Selection Optimization for On-Demand Air Taxi Services: A Deep Reinforcement Learning Approach",
      "url": "https://arxiv.org/abs/2601.21316",
      "date": "2026-01-29",
      "authors_text": "Aoyu Pang, Maonan Wang, Zifan Sha, Wenwei Yue, Changle Li",
      "is_highlight": false,
      "score": 80.0,
      "summary": "The paper presents a deep reinforcement learning framework for optimizing vertiport selection and air taxi routing, achieving significant travel time reductions in urban air mobility systems.",
      "teaser_image": "https://arxiv.org/html/2601.21316/x2.png",
      "debug_abstract": "Urban Air Mobility (UAM) has emerged as a transformative solution to alleviate urban congestion by utilizing low-altitude airspace, thereby reducing pressure on ground transportation networks. To enable truly efficient and seamless door-to-door travel experiences, UAM requires close integration with existing ground transportation infrastructure. However, current research on optimal integrated routing strategies for passengers in air-ground mobility systems remains limited, with a lack of systematic this http URL address this gap, we first propose a unified optimization model that integrates strategy selection for both air and ground transportation. This model captures the dynamic characteristics of multimodal transport networks and incorporates real-time traffic conditions alongside passenger decision-making behavior. Building on this model, we propose a Unified Air-Ground Mobility Coordination (UAGMC) framework, which leverages deep reinforcement learning (RL) and Vehicle-to-Everything (V2X) communication to optimize vertiport selection and dynamically plan air taxi routes. Experimental results demonstrate that UAGMC achieves a 34\\% reduction in average travel time compared to conventional proportional allocation methods, enhancing overall travel efficiency and providing novel insights into the integration and optimization of multimodal transportation systems. This work lays a solid foundation for advancing intelligent urban mobility solutions through the coordination of air and ground transportation modes. The related code can be found at this https URL."
    },
    {
      "title": "Disentangling perception and reasoning for improving data efficiency in learning cloth manipulation without demonstrations",
      "url": "https://arxiv.org/abs/2601.21713",
      "date": "2026-01-29",
      "authors_text": "Donatien Delehelle, Fei Chen, Darwin Caldwell",
      "is_highlight": false,
      "score": 87.0,
      "summary": "This paper presents a modular reinforcement learning approach for cloth manipulation that enhances data efficiency and reduces model size and training time, achieving improved performance in simulation and real-world transfer.",
      "teaser_image": "https://arxiv.org/html/2601.21713/x1.png",
      "debug_abstract": "Cloth manipulation is a ubiquitous task in everyday life, but it remains an open challenge for robotics. The difficulties in developing cloth manipulation policies are attributed to the high-dimensional state space, complex dynamics, and high propensity to self-occlusion exhibited by fabrics. As analytical methods have not been able to provide robust and general manipulation policies, reinforcement learning (RL) is considered a promising approach to these problems. However, to address the large state space and complex dynamics, data-based methods usually rely on large models and long training times. The resulting computational cost significantly hampers the development and adoption of these methods. Additionally, due to the challenge of robust state estimation, garment manipulation policies often adopt an end-to-end learning approach with workspace images as input. While this approach enables a conceptually straightforward sim-to-real transfer via real-world fine-tuning, it also incurs a significant computational cost by training agents on a highly lossy representation of the environment state. This paper questions this common design choice by exploring an efficient and modular approach to RL for cloth manipulation. We show that, through careful design choices, model size and training time can be significantly reduced when learning in simulation. Furthermore, we demonstrate how the resulting simulation-trained model can be transferred to the real world. We evaluate our approach on the SoftGym benchmark and achieve significant performance improvements over available baselines on our task, while using a substantially smaller model."
    },
    {
      "title": "PLANING: A Loosely Coupled Triangle-Gaussian Framework for Streaming 3D Reconstruction",
      "url": "https://arxiv.org/abs/2601.22046",
      "date": "2026-01-29",
      "authors_text": "Changjian Jiang, Kerui Ren, Xudong Li, Kaiwen Song, Linning Xu",
      "is_highlight": false,
      "score": 66.0,
      "summary": "PLANING is a novel streaming 3D reconstruction framework that efficiently combines geometric primitives and neural Gaussians, enhancing quality and speed while reducing redundancy.",
      "teaser_image": "https://arxiv.org/html/2601.22046/x3.png",
      "debug_abstract": "Streaming reconstruction from monocular image sequences remains challenging, as existing methods typically favor either high-quality rendering or accurate geometry, but rarely both. We present PLANING, an efficient on-the-fly reconstruction framework built on a hybrid representation that loosely couples explicit geometric primitives with neural Gaussians, enabling geometry and appearance to be modeled in a decoupled manner. This decoupling supports an online initialization and optimization strategy that separates geometry and appearance updates, yielding stable streaming reconstruction with substantially reduced structural redundancy. PLANING improves dense mesh Chamfer-L2 by 18.52% over PGSR, surpasses ARTDECO by 1.31 dB PSNR, and reconstructs ScanNetV2 scenes in under 100 seconds, over 5x faster than 2D Gaussian Splatting, while matching the quality of offline per-scene optimization. Beyond reconstruction quality, the structural clarity and computational efficiency of \\modelname~make it well suited for a broad range of downstream applications, such as enabling large-scale scene modeling and simulation-ready environments for embodied AI. Project page: this https URL ."
    },
    {
      "title": "Rhombot: Rhombus-shaped Modular Robots for Stable, Medium-Independent Reconfiguration Motion",
      "url": "https://arxiv.org/abs/2601.19529",
      "date": "2026-01-27",
      "authors_text": "Jie Gu, Yirui Sun, Zhihao Xia, Tin Lun Lam, Chunxu Tian",
      "is_highlight": false,
      "score": 86.0,
      "summary": "Rhombot is a modular self-reconfigurable robot featuring rhombus-shaped modules that enable stable, medium-independent morphing and locomotion through a novel motion primitive called morphpivoting.",
      "debug_abstract": "In this paper, we present Rhombot, a novel deformable planar lattice modular self-reconfigurable robot (MSRR) with a rhombus shaped module. Each module consists of a parallelogram skeleton with a single centrally mounted actuator that enables folding and unfolding along its diagonal. The core design philosophy is to achieve essential MSRR functionalities such as morphing, docking, and locomotion with minimal control complexity. This enables a continuous and stable reconfiguration process that is independent of the surrounding medium, allowing the system to reliably form various configurations in diverse environments. To leverage the unique kinematics of Rhombot, we introduce morphpivoting, a novel motion primitive for reconfiguration that differs from advanced MSRR systems, and propose a strategy for its continuous execution. Finally, a series of physical experiments validate the module&#39;s stable reconfiguration ability, as well as its positional and docking accuracy."
    },
    {
      "title": "EgoHandICL: Egocentric 3D Hand Reconstruction with In-Context Learning",
      "url": "https://arxiv.org/abs/2601.19850",
      "date": "2026-01-27",
      "authors_text": "Binzhu Xie, Shi Qiu, Sicheng Zhang, Yinqiao Wang, Hao Xu",
      "is_highlight": false,
      "score": 76.0,
      "summary": "EgoHandICL introduces an innovative in-context learning framework for robust 3D hand reconstruction in egocentric vision, enhancing performance through exemplar retrieval and multimodal context integration.",
      "debug_abstract": "Robust 3D hand reconstruction in egocentric vision is challenging due to depth ambiguity, self-occlusion, and complex hand-object interactions. Prior methods mitigate these issues by scaling training data or adding auxiliary cues, but they often struggle in unseen contexts. We present EgoHandICL, the first in-context learning (ICL) framework for 3D hand reconstruction that improves semantic alignment, visual consistency, and robustness under challenging egocentric conditions. EgoHandICL introduces complementary exemplar retrieval guided by vision-language models (VLMs), an ICL-tailored tokenizer for multimodal context, and a masked autoencoder (MAE)-based architecture trained with hand-guided geometric and perceptual objectives. Experiments on ARCTIC and EgoExo4D show consistent gains over state-of-the-art methods. We also demonstrate real-world generalization and improve EgoVLM hand-object interaction reasoning by using reconstructed hands as visual prompts. Code and data: this https URL"
    },
    {
      "title": "AdaReasoner: Dynamic Tool Orchestration for Iterative Visual Reasoning",
      "url": "https://arxiv.org/abs/2601.18631",
      "date": "2026-01-26",
      "authors_text": "Mingyang Song, Haoyu Sun, Jiawei Gu, Linjie Li, Luxin Xu",
      "is_highlight": false,
      "score": 61.0,
      "summary": "AdaReasoner is a multimodal model that autonomously optimizes tool selection and usage for iterative visual reasoning, achieving state-of-the-art performance on various benchmarks.",
      "debug_abstract": "When humans face problems beyond their immediate capabilities, they rely on tools, providing a promising paradigm for improving visual reasoning in multimodal large language models (MLLMs). Effective reasoning, therefore, hinges on knowing which tools to use, when to invoke them, and how to compose them over multiple steps, even when faced with new tools or new tasks. We introduce \\textbf{AdaReasoner}, a family of multimodal models that learn tool use as a general reasoning skill rather than as tool-specific or explicitly supervised behavior. AdaReasoner is enabled by (i) a scalable data curation pipeline exposing models to long-horizon, multi-step tool interactions; (ii) Tool-GRPO, a reinforcement learning algorithm that optimizes tool selection and sequencing based on end-task success; and (iii) an adaptive learning mechanism that dynamically regulates tool usage. Together, these components allow models to infer tool utility from task context and intermediate outcomes, enabling coordination of multiple tools and generalization to unseen tools. Empirically, AdaReasoner exhibits strong tool-adaptive and generalization behaviors: it autonomously adopts beneficial tools, suppresses irrelevant ones, and adjusts tool usage frequency based on task demands, despite never being explicitly trained to do so. These capabilities translate into state-of-the-art performance across challenging benchmarks, improving the 7B base model by +24.9\\% on average and surpassing strong proprietary systems such as GPT-5 on multiple tasks, including VSP and Jigsaw."
    },
    {
      "title": "Timely Machine: Awareness of Time Makes Test-Time Scaling Agentic",
      "url": "https://arxiv.org/abs/2601.16486",
      "date": "2026-01-23",
      "authors_text": "Yichuan Ma, Linyang Li, Yongkang chen, Peiji Li, Xiaozhe Li",
      "is_highlight": false,
      "score": 78.0,
      "summary": "The paper introduces Timely Machine, redefining test-time scaling for LLMs based on wall-clock time, enhancing performance through dynamic strategy adjustments and reinforcement learning.",
      "debug_abstract": "As large language models (LLMs) increasingly tackle complex reasoning tasks, test-time scaling has become critical for enhancing capabilities. However, in agentic scenarios with frequent tool calls, the traditional generation-length-based definition breaks down: tool latency decouples inference time from generation length. We propose Timely Machine, redefining test-time as wall-clock time, where models dynamically adjust strategies based on time budgets. We introduce Timely-Eval, a benchmark spanning high-frequency tool calls, low-frequency tool calls, and time-constrained reasoning. By varying tool latency, we find smaller models excel with fast feedback through more interactions, while larger models dominate high-latency settings via superior interaction quality. Moreover, existing models fail to adapt reasoning to time budgets. We propose Timely-RL to address this gap. After cold-start supervised fine-tuning, we use reinforcement learning to enhance temporal planning. Timely-RL improves time budget awareness and consistently boosts performance across Timely-Eval. We hope our work offers a new perspective on test-time scaling for the agentic era."
    },
    {
      "title": "Adaptive Reinforcement and Model Predictive Control Switching for Safe Human-Robot Cooperative Navigation",
      "url": "https://arxiv.org/abs/2601.16686",
      "date": "2026-01-23",
      "authors_text": "Ning Liu, Sen Shen, Zheng Li, Matthew D'Souza, Jen Jen Chung",
      "is_highlight": false,
      "score": 94.0,
      "summary": "The paper presents ARMS, a hybrid control framework combining reinforcement learning and model predictive control for safe human-robot navigation, achieving superior performance in cluttered environments.",
      "debug_abstract": "This paper addresses the challenge of human-guided navigation for mobile collaborative robots under simultaneous proximity regulation and safety constraints. We introduce Adaptive Reinforcement and Model Predictive Control Switching (ARMS), a hybrid learning-control framework that integrates a reinforcement learning follower trained with Proximal Policy Optimization (PPO) and an analytical one-step Model Predictive Control (MPC) formulated as a quadratic program safety filter. To enable robust perception under partial observability and non-stationary human motion, ARMS employs a decoupled sensing architecture with a Long Short-Term Memory (LSTM) temporal encoder for the human-robot relative state and a spatial encoder for 360-degree LiDAR scans. The core contribution is a learned adaptive neural switcher that performs context-aware soft action fusion between the two controllers, favoring conservative, constraint-aware QP-based control in low-risk regions while progressively shifting control authority to the learned follower in highly cluttered or constrained scenarios where maneuverability is critical, and reverting to the follower action when the QP becomes infeasible. Extensive evaluations against Pure Pursuit, Dynamic Window Approach (DWA), and an RL-only baseline demonstrate that ARMS achieves an 82.5 percent success rate in highly cluttered environments, outperforming DWA and RL-only approaches by 7.1 percent and 3.1 percent, respectively, while reducing average computational latency by 33 percent to 5.2 milliseconds compared to a multi-step MPC baseline. Additional simulation transfer in Gazebo and initial real-world deployment results further indicate the practicality and robustness of ARMS for safe and efficient human-robot collaboration. Source code and a demonstration video are available at this https URL."
    },
    {
      "title": "Natural Language-Driven Global Mapping of Martian Landforms",
      "url": "https://arxiv.org/abs/2601.15949",
      "date": "2026-01-22",
      "authors_text": "Yiran Wang, Shuoyuan Wang, Zhaoran Wei, Jiannan Zhao, Zhonghua Yao",
      "is_highlight": false,
      "score": 72.0,
      "summary": "MarScope is a vision-language framework that enables natural language-driven, label-free mapping of Martian landforms, enhancing global geomorphic analysis and user query flexibility.",
      "debug_abstract": "Planetary surfaces are typically analyzed using high-level semantic concepts in natural language, yet vast orbital image archives remain organized at the pixel level. This mismatch limits scalable, open-ended exploration of planetary surfaces. Here we present MarScope, a planetary-scale vision-language framework enabling natural language-driven, label-free mapping of Martian landforms. MarScope aligns planetary images and text in a shared semantic space, trained on over 200,000 curated image-text pairs. This framework transforms global geomorphic mapping on Mars by replacing pre-defined classifications with flexible semantic retrieval, enabling arbitrary user queries across the entire planet in 5 seconds with F1 scores up to 0.978. Applications further show that it extends beyond morphological classification to facilitate process-oriented analysis and similarity-based geomorphological mapping at a planetary scale. MarScope establishes a new paradigm where natural language serves as a direct interface for scientific discovery over massive geospatial datasets."
    },
    {
      "title": "Robust Tool Use via Fission-GRPO: Learning to Recover from Execution Errors",
      "url": "https://arxiv.org/abs/2601.15625",
      "date": "2026-01-22",
      "authors_text": "Zhiwei Zhang, Fei Zhao, Rui Wang, Zezhong Wang, Bin Liang",
      "is_highlight": false,
      "score": 87.1,
      "summary": "Fission-GRPO enhances tool use in LLMs by converting execution errors into corrective supervision, improving error recovery and overall accuracy in multi-turn interactions.",
      "debug_abstract": "Large language models (LLMs) can call tools effectively, yet they remain brittle in multi-turn execution: following a tool call error, smaller models often degenerate into repetitive invalid re-invocations, failing to interpret error feedback and self-correct. This brittleness hinders reliable real-world deployment, where the execution errors are inherently inevitable during tool interaction procedures. We identify a key limitation of current approaches: standard reinforcement learning (RL) treats errors as sparse negative rewards, providing no guidance on how to recover, while pre-collected synthetic error-correction datasets suffer from distribution mismatch with the model&#39;s on-policy error modes. To bridge this gap, we propose Fission-GRPO, a framework that converts execution errors into corrective supervision within the RL training loop. Our core mechanism fissions each failed trajectory into a new training instance by augmenting it with diagnostic feedback from a finetuned Error Simulator, then resampling recovery rollouts on-policy. This enables the model to learn from the precise errors it makes during exploration, rather than from static, pre-collected error cases. On the BFCL v4 Multi-Turn, Fission-GRPO improves the error recovery rate of Qwen3-8B by 5.7% absolute, crucially, yielding a 4% overall accuracy gain (42.75% to 46.75%) over GRPO and outperforming specialized tool-use agents."
    }
  ],
  "Hengshuang Zhao老师实验室": [
    {
      "title": "Large Language Models Can Take False First Steps at Inference-time Planning",
      "url": "https://arxiv.org/abs/2602.02991",
      "date": "2026-02-03",
      "authors_text": "Haijiang Yan, Jian-Qiao Zhu, Adam Sanborn",
      "is_highlight": false,
      "score": 73.0,
      "summary": "The paper explains that large language models exhibit inconsistent planning at inference due to evolving self-generated context, supported by experiments demonstrating planning shifts and reduced biases.",
      "teaser_image": "https://arxiv.org/html/2602.02991/images/llama_regression.png",
      "debug_abstract": "Large language models (LLMs) have been shown to acquire sequence-level planning abilities during training, yet their planning behavior exhibited at inference time often appears short-sighted and inconsistent with these capabilities. We propose a Bayesian account for this gap by grounding planning behavior in the evolving generative context: given the subtle differences between natural language and the language internalized by LLMs, accumulated self-generated context drives a planning-shift during inference and thereby creates the appearance of compromised planning behavior. We further validate the proposed model through two controlled experiments: a random-generation task demonstrating constrained planning under human prompts and increasing planning strength as self-generated context accumulates, and a Gaussian-sampling task showing reduced initial bias when conditioning on self-generated sequences. These findings provide a theoretical explanation along with empirical evidence for characterizing how LLMs plan ahead during inference."
    },
    {
      "title": "Search-R2: Enhancing Search-Integrated Reasoning via Actor-Refiner Collaboration",
      "url": "https://arxiv.org/abs/2602.03647",
      "date": "2026-02-03",
      "authors_text": "Bowei He, Minda Hu, Zenan Xu, Hongru Wang, Licheng Zong",
      "is_highlight": false,
      "score": 52.0,
      "summary": "Search-R2 introduces an Actor-Refiner framework that enhances search-integrated reasoning in language agents through targeted intervention and fine-grained supervision, outperforming existing methods.",
      "teaser_image": "https://arxiv.org/html/2602.03647/x2.png",
      "debug_abstract": "Search-integrated reasoning enables language agents to transcend static parametric knowledge by actively querying external sources. However, training these agents via reinforcement learning is hindered by the multi-scale credit assignment problem: existing methods typically rely on sparse, trajectory-level rewards that fail to distinguish between high-quality reasoning and fortuitous guesses, leading to redundant or misleading search behaviors. To address this, we propose Search-R2, a novel Actor-Refiner collaboration framework that enhances reasoning through targeted intervention, with both components jointly optimized during training. Our approach decomposes the generation process into an Actor, which produces initial reasoning trajectories, and a Meta-Refiner, which selectively diagnoses and repairs flawed steps via a &#39;cut-and-regenerate&#39; mechanism. To provide fine-grained supervision, we introduce a hybrid reward design that couples outcome correctness with a dense process reward quantifying the information density of retrieved evidence. Theoretically, we formalize the Actor-Refiner interaction as a smoothed mixture policy, proving that selective correction yields strict performance gains over strong baselines. Extensive experiments across various general and multi-hop QA datasets demonstrate that Search-R2 consistently outperforms strong RAG and RL-based baselines across model scales, achieving superior reasoning accuracy with minimal overhead."
    },
    {
      "title": "Dual Quaternion SE(3) Synchronization with Recovery Guarantees",
      "url": "https://arxiv.org/abs/2602.00324",
      "date": "2026-01-30",
      "authors_text": "Jianing Zhao, Linglingzhi Zhu, Anthony Man-Cho So",
      "is_highlight": false,
      "score": 75.0,
      "summary": "This paper presents a dual quaternion-based SE(3) synchronization method with a two-stage algorithm that guarantees recovery of absolute poses from noisy transformations, outperforming traditional matrix methods.",
      "teaser_image": "https://arxiv.org/html/2602.00324/x2.png",
      "debug_abstract": "Synchronization over the special Euclidean group SE(3) aims to recover absolute poses from noisy pairwise relative transformations and is a core primitive in robotics and 3D vision. Standard approaches often require multi-step heuristic procedures to recover valid poses, which are difficult to analyze and typically lack theoretical guarantees. This paper adopts a dual quaternion representation and formulates SE(3) synchronization directly over the unit dual quaternion. A two-stage algorithm is developed: A spectral initializer computed via the power method on a Hermitian dual quaternion measurement matrix, followed by a dual quaternion generalized power method (DQGPM) that enforces feasibility through per-iteration projection. The estimation error bounds are established for spectral estimators, and DQGPM is shown to admit a finite-iteration error bound and achieves linear error contraction up to an explicit noise-dependent threshold. Experiments on synthetic benchmarks and real-world multi-scan point-set registration demonstrate that the proposed pipeline improves both accuracy and efficiency over representative matrix-based methods."
    },
    {
      "title": "Offline Discovery of Interpretable Skills from Multi-Task Trajectories",
      "url": "https://arxiv.org/abs/2602.01018",
      "date": "2026-02-01",
      "authors_text": "Chongyu Zhu, Mithun Vanniasinghe, Jiayu Chen, Chi-Guhn Lee",
      "is_highlight": false,
      "score": 77.0,
      "summary": "LOKI is a three-stage framework for offline skill discovery and hierarchical imitation learning, effectively extracting interpretable skills from multi-task trajectories without explicit rewards.",
      "teaser_image": "https://arxiv.org/html/2602.01018/fig/two_stage_skill_learning.png",
      "debug_abstract": "Hierarchical Imitation Learning is a powerful paradigm for acquiring complex robot behaviors from demonstrations. A central challenge, however, lies in discovering reusable skills from long-horizon, multi-task offline data, especially when the data lacks explicit rewards or subtask annotations. In this work, we introduce LOKI, a three-stage end-to-end learning framework designed for offline skill discovery and hierarchical imitation. The framework commences with a two-stage, weakly supervised skill discovery process: Stage one performs coarse, task-aware macro-segmentation by employing an alignment-enforced Vector Quantized VAE guided by weak task labels. Stage two then refines these segments at a micro-level using a self-supervised sequential model, followed by an iterative clustering process to consolidate skill boundaries. The third stage then leverages these precise boundaries to construct a hierarchical policy within an option-based framework-complete with a learned termination condition beta for explicit skill switching. LOKI achieves high success rates on the challenging D4RL Kitchen benchmark and outperforms standard HIL baselines. Furthermore, we demonstrate that the discovered skills are semantically meaningful, aligning with human intuition, and exhibit compositionality by successfully sequencing them to solve a novel, unseen task."
    },
    {
      "title": "Residual Decoding: Mitigating Hallucinations in Large Vision-Language Models via History-Aware Residual Guidance",
      "url": "https://arxiv.org/abs/2602.01047",
      "date": "2026-02-01",
      "authors_text": "Xinrong Chen, Xu Chu, Yingmin Qiu, Hengyuan Zhang, Jing Xiong",
      "is_highlight": false,
      "score": 53.0,
      "summary": "Residual Decoding (ResDec) mitigates hallucinations in Large Vision-Language Models by leveraging historical information to enhance decoding and improve visual grounding.",
      "teaser_image": "https://arxiv.org/html/2602.01047/x1.png",
      "debug_abstract": "Large Vision-Language Models (LVLMs) can reason effectively from image-text inputs and perform well in various multimodal tasks. Despite this success, they are affected by language priors and often produce hallucinations. Hallucinations denote generated content that is grammatically and syntactically coherent, yet bears no match or direct relevance to actual visual input. To address this problem, we propose Residual Decoding (ResDec). It is a novel training-free method that uses historical information to aid decoding. The method relies on the internal implicit reasoning mechanism and token logits evolution mechanism of LVLMs to correct biases. Extensive experiments demonstrate that ResDec effectively suppresses hallucinations induced by language priors, significantly improves visual grounding, and reduces object hallucinations. In addition to mitigating hallucinations, ResDec also performs exceptionally well on comprehensive LVLM benchmarks, highlighting its broad applicability."
    },
    {
      "title": "MedAD-R1: Eliciting Consistent Reasoning in Interpretible Medical Anomaly Detection via Consistency-Reinforced Policy Optimization",
      "url": "https://arxiv.org/abs/2602.01081",
      "date": "2026-02-01",
      "authors_text": "Haitao Zhang, Yingying Wang, Jiaxiang Wang, Haote Xu, Hongyang Zhang",
      "is_highlight": false,
      "score": 64.0,
      "summary": "MedAD-R1 enhances medical anomaly detection by utilizing a two-stage training framework with consistency-reinforced policy optimization, achieving state-of-the-art performance on the MedAD-38K benchmark.",
      "teaser_image": "https://arxiv.org/html/2602.01081/x3.png",
      "debug_abstract": "Medical Anomaly Detection (MedAD) presents a significant opportunity to enhance diagnostic accuracy using Large Multimodal Models (LMMs) to interpret and answer questions based on medical images. However, the reliance on Supervised Fine-Tuning (SFT) on simplistic and fragmented datasets has hindered the development of models capable of plausible reasoning and robust multimodal generalization. To overcome this, we introduce MedAD-38K, the first large-scale, multi-modal, and multi-center benchmark for MedAD featuring diagnostic Chain-of-Thought (CoT) annotations alongside structured Visual Question-Answering (VQA) pairs. On this foundation, we propose a two-stage training framework. The first stage, Cognitive Injection, uses SFT to instill foundational medical knowledge and align the model with a structured think-then-answer paradigm. Given that standard policy optimization can produce reasoning that is disconnected from the final answer, the second stage incorporates Consistency Group Relative Policy Optimization (Con-GRPO). This novel algorithm incorporates a crucial consistency reward to ensure the generated reasoning process is relevant and logically coherent with the final diagnosis. Our proposed model, MedAD-R1, achieves state-of-the-art (SOTA) performance on the MedAD-38K benchmark, outperforming strong baselines by more than 10\\%. This superior performance stems from its ability to generate transparent and logically consistent reasoning pathways, offering a promising approach to enhancing the trustworthiness and interpretability of AI for clinical decision support."
    },
    {
      "title": "Adaptive Rollout Allocation for Online Reinforcement Learning with Verifiable Rewards",
      "url": "https://arxiv.org/abs/2602.01601",
      "date": "2026-02-02",
      "authors_text": "Hieu Trung Nguyen, Bao Nguyen, Wenao Ma, Yuzhi Zhao, Ruifeng She",
      "is_highlight": false,
      "score": 40.0,
      "summary": "The paper presents \\Ours, a variance-informed strategy for adaptive rollout allocation in online reinforcement learning, enhancing sampling efficiency and performance over traditional methods.",
      "teaser_image": "https://arxiv.org/html/2602.01601/x2.png",
      "debug_abstract": "Sampling efficiency is a key bottleneck in reinforcement learning with verifiable rewards. Existing group-based policy optimization methods, such as GRPO, allocate a fixed number of rollouts for all training prompts. This uniform allocation implicitly treats all prompts as equally informative, and could lead to inefficient computational budget usage and impede training progress. We introduce \\Ours, a Variance-Informed Predictive allocation strategy that allocates a given rollout budget to the prompts in the incumbent batch to minimize the expected gradient variance of the policy update. At each iteration, \\Ours~uses a lightweight Gaussian process model to predict per-prompt success probabilities based on recent rollouts. These probability predictions are translated into variance estimates, which are then fed into a convex optimization problem to determine the optimal rollout allocations under a hard compute budget constraint. Empirical results show that \\Ours~consistently improves sampling efficiency and achieves higher performance than uniform or heuristic allocation strategies in multiple benchmarks. Our code will be available at this https URL."
    },
    {
      "title": "Reasoning with Autoregressive-Diffusion Collaborative Thoughts",
      "url": "https://arxiv.org/abs/2602.01608",
      "date": "2026-02-02",
      "authors_text": "Mu Yuan, Liekang Zeng, Guoliang Xing, Lan Zhang, Yunhao Liu",
      "is_highlight": false,
      "score": 41.0,
      "summary": "The paper presents Collaborative Thoughts, a framework integrating autoregressive and diffusion models for enhanced spatial reasoning and generation through iterative feedback and joint reasoning.",
      "teaser_image": "https://arxiv.org/html/2602.01608/x2.png",
      "debug_abstract": "Autoregressive and diffusion models represent two complementary generative paradigms. Autoregressive models excel at sequential planning and constraint composition, yet struggle with tasks that require explicit spatial or physical grounding. Diffusion models, in contrast, capture rich spatial structure through high-dimensional generation, but lack the stepwise logical control needed to satisfy complex, multi-stage constraints or to reliably identify and correct errors. We introduce Collaborative Thoughts, a unified collaborative framework that enables autoregressive and diffusion models to reason and generate jointly through a closed-loop interaction. In Collaborative Thoughts, autoregressive models perform structured planning and constraint management, diffusion models instantiate these constraints as intermediate visual thoughts, and a vision-based critic module evaluates whether the visual thoughts satisfy the intended structural and physical requirements. This feedback is then used to iteratively refine subsequent planning and generation steps, mitigating error propagation across modalities. Importantly, Collaborative Thoughts uses the same collaborative loop regardless of whether the task is autoregressive question answering or diffusion-based visual generation. Through representative examples, we illustrate how Collaborative Thoughts can improve the reliability of spatial reasoning and the controllability of generation."
    },
    {
      "title": "Tilt-Ropter: A Novel Hybrid Aerial and Terrestrial Vehicle with Tilt Rotors and Passive Wheels",
      "url": "https://arxiv.org/abs/2602.01700",
      "date": "2026-02-02",
      "authors_text": "Ruoyu Wang, Xuchen Liu, Zongzhou Wu, Zixuan Guo, Wendi Ding",
      "is_highlight": false,
      "score": 38.0,
      "summary": "The Tilt-Ropter is a hybrid aerial-terrestrial vehicle that utilizes tilt rotors and passive wheels for efficient locomotion, featuring advanced control systems for enhanced mobility and energy savings.",
      "teaser_image": "https://arxiv.org/html/2602.01700/x2.png",
      "debug_abstract": "In this work, we present Tilt-Ropter, a novel hybrid aerial-terrestrial vehicle (HATV) that combines tilt rotors with passive wheels to achieve energy-efficient multi-mode locomotion. Unlike existing under-actuated HATVs, the fully actuated design of Tilt-Ropter enables decoupled force and torque control, greatly enhancing its mobility and environmental adaptability. A nonlinear model predictive controller (NMPC) is developed to track reference trajectories and handle contact constraints across locomotion modes, while a dedicated control allocation module exploits actuation redundancy to achieve energy-efficient control of actuators. Additionally, to enhance robustness during ground contact, we introduce an external wrench estimation algorithm that estimates environmental interaction forces and torques in real time. The system is validated through both simulation and real-world experiments, including seamless air-ground transitions and trajectory tracking. Results show low tracking errors in both modes and highlight a 92.8% reduction in power consumption during ground locomotion, demonstrating the system&#39;s potential for long-duration missions across large-scale and energy-constrained environments."
    },
    {
      "title": "Breaking the Static Graph: Context-Aware Traversal for Robust Retrieval-Augmented Generation",
      "url": "https://arxiv.org/abs/2602.01965",
      "date": "2026-02-02",
      "authors_text": "Kwun Hang Lau, Fangyuan Zhang, Boyu Ruan, Yingli Zhou, Qintian Guo",
      "is_highlight": false,
      "score": 27.0,
      "summary": "CatRAG enhances Retrieval-Augmented Generation by transforming static Knowledge Graphs into query-adaptive structures, improving multi-hop evidence retrieval and reasoning completeness.",
      "teaser_image": "https://arxiv.org/html/2602.01965/image/HubBiasShift.png",
      "debug_abstract": "Recent advances in Retrieval-Augmented Generation (RAG) have shifted from simple vector similarity to structure-aware approaches like HippoRAG, which leverage Knowledge Graphs (KGs) and Personalized PageRank (PPR) to capture multi-hop dependencies. However, these methods suffer from a &#34;Static Graph Fallacy&#34;: they rely on fixed transition probabilities determined during indexing. This rigidity ignores the query-dependent nature of edge relevance, causing semantic drift where random walks are diverted into high-degree &#34;hub&#34; nodes before reaching critical downstream evidence. Consequently, models often achieve high partial recall but fail to retrieve the complete evidence chain required for multi-hop queries. To address this, we propose CatRAG, Context-Aware Traversal for robust RAG, a framework that builds on the HippoRAG 2 architecture and transforms the static KG into a query-adaptive navigation structure. We introduce a multi-faceted framework to steer the random walk: (1) Symbolic Anchoring, which injects weak entity constraints to regularize the random walk; (2) Query-Aware Dynamic Edge Weighting, which dynamically modulates graph structure, to prune irrelevant paths while amplifying those aligned with the query&#39;s intent; and (3) Key-Fact Passage Weight Enhancement, a cost-efficient bias that structurally anchors the random walk to likely evidence. Experiments across four multi-hop benchmarks demonstrate that CatRAG consistently outperforms state of the art baselines. Our analysis reveals that while standard Recall metrics show modest gains, CatRAG achieves substantial improvements in reasoning completeness, the capacity to recover the entire evidence path without gaps. These results reveal that our approach effectively bridges the gap between retrieving partial context and enabling fully grounded reasoning. Resources are available at this https URL."
    },
    {
      "title": "TIDE: Trajectory-based Diagnostic Evaluation of Test-Time Improvement in LLM Agents",
      "url": "https://arxiv.org/abs/2602.02196",
      "date": "2026-02-02",
      "authors_text": "Hang Yan, Xinyu Che, Fangzhi Xu, Qiushi Sun, Zichen Ding",
      "is_highlight": false,
      "score": 20.0,
      "summary": "The TIDE framework evaluates Test-Time Improvement in LLM agents by analyzing task completion dynamics, recursive behaviors, and memory constraints to enhance performance optimization.",
      "teaser_image": "https://arxiv.org/html/2602.02196/x1.png",
      "debug_abstract": "Recent advances in autonomous LLM agents demonstrate their ability to improve performance through iterative interaction with the environment. We define this paradigm as Test-Time Improvement (TTI). However, the mechanisms under how and why TTI succeed or fail remain poorly understood, and existing evaluation metrics fail to capture their task optimization efficiency, behavior adaptation after erroneous actions, and the specific utility of working memory for task completion. To address these gaps, we propose Test-time Improvement Diagnostic Evaluation (TIDE), an agent-agnostic and environment-agnostic framework that decomposes TTI into three comprehensive and interconnected dimensions. The framework measures (1) the overall temporal dynamics of task completion and (2) identifies whether performance is primarily constrained by recursive looping behaviors or (3) by burdensome accumulated memory. Through extensive experiments across diverse agents and environments, TIDE highlights that improving agent performance requires more than scaling internal reasoning, calling for explicitly optimizing the interaction dynamics between the agent and the environment."
    },
    {
      "title": "Towards Agentic Intelligence for Materials Science",
      "url": "https://arxiv.org/abs/2602.00169",
      "date": "2026-01-29",
      "authors_text": "Huan Zhang, Yizhan Li, Wenhao Huang, Ziyu Hou, Yu Song",
      "is_highlight": false,
      "score": 12.0,
      "summary": "This paper advocates for developing agentic AI systems in materials science that autonomously plan, act, and learn to enhance discovery processes beyond traditional models.",
      "teaser_image": "https://arxiv.org/html/2602.00169/x1.png",
      "debug_abstract": "The convergence of artificial intelligence and materials science presents a transformative opportunity, but achieving true acceleration in discovery requires moving beyond task-isolated, fine-tuned models toward agentic systems that plan, act, and learn across the full discovery loop. This survey advances a unique pipeline-centric view that spans from corpus curation and pretraining, through domain adaptation and instruction tuning, to goal-conditioned agents interfacing with simulation and experimental platforms. Unlike prior reviews, we treat the entire process as an end-to-end system to be optimized for tangible discovery outcomes rather than proxy benchmarks. This perspective allows us to trace how upstream design choices-such as data curation and training objectives-can be aligned with downstream experimental success through effective credit assignment. To bridge communities and establish a shared frame of reference, we first present an integrated lens that aligns terminology, evaluation, and workflow stages across AI and materials science. We then analyze the field through two focused lenses: From the AI perspective, the survey details LLM strengths in pattern recognition, predictive analytics, and natural language processing for literature mining, materials characterization, and property prediction; from the materials science perspective, it highlights applications in materials design, process optimization, and the acceleration of computational workflows via integration with external tools (e.g., DFT, robotic labs). Finally, we contrast passive, reactive approaches with agentic design, cataloging current contributions while motivating systems that pursue long-horizon goals with autonomy, memory, and tool use. This survey charts a practical roadmap towards autonomous, safety-aware LLM agents aimed at discovering novel and useful materials."
    },
    {
      "title": "Failure-Aware Bimanual Teleoperation via Conservative Value Guided Assistance",
      "url": "https://arxiv.org/abs/2602.01092",
      "date": "2026-02-01",
      "authors_text": "Peng Zhou, Zhongxuan Li, Jinsong Wu, Jiaming Qi, Jun Hu",
      "is_highlight": false,
      "score": 9.0,
      "summary": "This paper presents a failure-aware bimanual teleoperation framework that uses conservative value learning to enhance task success and reduce operator workload through compliant haptic assistance.",
      "teaser_image": "https://arxiv.org/html/2602.01092/x1.png",
      "debug_abstract": "Teleoperation of high-precision manipulation is con-strained by tight success tolerances and complex contact dy-namics, which make impending failures difficult for human operators to anticipate under partial observability. This paper proposes a value-guided, failure-aware framework for bimanual teleoperation that provides compliant haptic assistance while pre-serving continuous human authority. The framework is trained entirely from heterogeneous offline teleoperation data containing both successful and failed executions. Task feasibility is mod-eled as a conservative success score learned via Conservative Value Learning, yielding a risk-sensitive estimate that remains reliable under distribution shift. During online operation, the learned success score regulates the level of assistance, while a learned actor provides a corrective motion direction. Both are integrated through a joint-space impedance interface on the master side, yielding continuous guidance that steers the operator away from failure-prone actions without overriding intent. Experimental results on contact-rich manipulation tasks demonstrate improved task success rates and reduced operator workload compared to conventional teleoperation and shared-autonomy baselines, indicating that conservative value learning provides an effective mechanism for embedding failure awareness into bilateral teleoperation. Experimental videos are available at this https URL"
    },
    {
      "title": "FlowSteer: Interactive Agentic Workflow Orchestration via End-to-End Reinforcement Learning",
      "url": "https://arxiv.org/abs/2602.01664",
      "date": "2026-02-02",
      "authors_text": "Mingda Zhang, Haoran Luo, Tiesunlong Shen, Qika Lin, Xiaoying Tang",
      "is_highlight": false,
      "score": 7.0,
      "summary": "FlowSteer is an end-to-end reinforcement learning framework that automates workflow orchestration through interactive learning, addressing manual costs and improving performance across diverse tasks.",
      "teaser_image": "https://arxiv.org/html/2602.01664/x1.png",
      "debug_abstract": "In recent years, a variety of powerful agentic workflows have been applied to solve a wide range of human problems. However, existing workflow orchestration still faces key challenges, including high manual cost, reliance on specific operators/large language models (LLMs), and sparse reward signals. To address these challenges, we propose FlowSteer, an end-to-end reinforcement learning framework that takes a lightweight policy model as the agent and an executable canvas environment, automating workflow orchestration through multi-turn interaction. In this process, the policy model analyzes execution states and selects editing actions, while the canvas executes operators and returns feedback for iterative refinement. Moreover, FlowSteer provides a plug-and-play framework that supports diverse operator libraries and interchangeable LLM backends. To effectively train this interaction paradigm, we propose Canvas Workflow Relative Policy Optimization (CWRPO), which introduces diversity-constrained rewards with conditional release to stabilize learning and suppress shortcut behaviors. Experimental results on twelve datasets show that FlowSteer significantly outperforms baselines across various tasks."
    },
    {
      "title": "Diffusion Model-based Reinforcement Learning for Version Age of Information Scheduling: Average and Tail-Risk-Sensitive Control",
      "url": "https://arxiv.org/abs/2601.18069",
      "date": "2026-01-26",
      "authors_text": "Haoyuan Pan, Sizhao Chen, Zhaorui Wang, Tse-Tin Chan",
      "is_highlight": false,
      "score": 57.0,
      "summary": "This paper presents a novel reinforcement learning approach, RS-D3SAC, for optimizing Version Age of Information scheduling by minimizing both average and tail-risk-sensitive metrics in wireless systems.",
      "teaser_image": null,
      "debug_abstract": "Ensuring timely and semantically accurate information delivery is critical in real-time wireless systems. While Age of Information (AoI) quantifies temporal freshness, Version Age of Information (VAoI) captures semantic staleness by accounting for version evolution between transmitters and receivers. Existing VAoI scheduling approaches primarily focus on minimizing average VAoI, overlooking rare but severe staleness events that can compromise reliability under stochastic packet arrivals and unreliable channels. This paper investigates both average-oriented and tail-risk-sensitive VAoI scheduling in a multi-user status update system with long-term transmission cost constraints. We first formulate the average VAoI minimization problem as a constrained Markov decision process and introduce a deep diffusion-based Soft Actor-Critic (D2SAC) algorithm. By generating actions through a diffusion-based denoising process, D2SAC enhances policy expressiveness and establishes a strong baseline for mean performance. Building on this foundation, we put forth RS-D3SAC, a risk-sensitive deep distributional diffusion-based Soft Actor-Critic algorithm. RS-D3SAC integrates a diffusion-based actor with a quantile-based distributional critic, explicitly modeling the full VAoI return distribution. This enables principled tail-risk optimization via Conditional Value-at-Risk (CVaR) while satisfying long-term transmission cost constraints. Extensive simulations show that, while D2SAC reduces average VAoI, RS-D3SAC consistently achieves substantial reductions in CVaR without sacrificing mean performance. The dominant gain in tail-risk reduction stems from the distributional critic, with the diffusion-based actor providing complementary refinement to stabilize and enrich policy decisions, highlighting their effectiveness for robust and risk-aware VAoI scheduling in multi-user wireless systems."
    },
    {
      "title": "LoD-Structured 3D Gaussian Splatting for Streaming Video Reconstruction",
      "url": "https://arxiv.org/abs/2601.18475",
      "date": "2026-01-26",
      "authors_text": "Xinhui Liu, Can Wang, Lei Liu, Zhenghao Chen, Wei Jiang",
      "is_highlight": false,
      "score": 52.0,
      "summary": "The StreamLoD-GS framework enhances Streaming Free-Viewpoint Video reconstruction by optimizing 3D Gaussian Splatting for efficient, high-fidelity rendering with reduced storage needs.",
      "teaser_image": null,
      "debug_abstract": "Free-Viewpoint Video (FVV) reconstruction enables photorealistic and interactive 3D scene visualization; however, real-time streaming is often bottlenecked by sparse-view inputs, prohibitive training costs, and bandwidth constraints. While recent 3D Gaussian Splatting (3DGS) has advanced FVV due to its superior rendering speed, Streaming Free-Viewpoint Video (SFVV) introduces additional demands for rapid optimization, high-fidelity reconstruction under sparse constraints, and minimal storage footprints. To bridge this gap, we propose StreamLoD-GS, an LoD-based Gaussian Splatting framework designed specifically for SFVV. Our approach integrates three core innovations: 1) an Anchor- and Octree-based LoD-structured 3DGS with a hierarchical Gaussian dropout technique to ensure efficient and stable optimization while maintaining high-quality rendering; 2) a GMM-based motion partitioning mechanism that separates dynamic and static content, refining dynamic regions while preserving background stability; and 3) a quantized residual refinement framework that significantly reduces storage requirements without compromising visual fidelity. Extensive experiments demonstrate that StreamLoD-GS achieves competitive or state-of-the-art performance in terms of quality, efficiency, and storage."
    },
    {
      "title": "Heterogeneous Vertiport Selection Optimization for On-Demand Air Taxi Services: A Deep Reinforcement Learning Approach",
      "url": "https://arxiv.org/abs/2601.21316",
      "date": "2026-01-29",
      "authors_text": "Aoyu Pang, Maonan Wang, Zifan Sha, Wenwei Yue, Changle Li",
      "is_highlight": false,
      "score": 80.0,
      "summary": "The paper presents a deep reinforcement learning framework for optimizing vertiport selection and air taxi routing, achieving significant travel time reductions in urban air mobility systems.",
      "teaser_image": "https://arxiv.org/html/2601.21316/x2.png",
      "debug_abstract": "Urban Air Mobility (UAM) has emerged as a transformative solution to alleviate urban congestion by utilizing low-altitude airspace, thereby reducing pressure on ground transportation networks. To enable truly efficient and seamless door-to-door travel experiences, UAM requires close integration with existing ground transportation infrastructure. However, current research on optimal integrated routing strategies for passengers in air-ground mobility systems remains limited, with a lack of systematic this http URL address this gap, we first propose a unified optimization model that integrates strategy selection for both air and ground transportation. This model captures the dynamic characteristics of multimodal transport networks and incorporates real-time traffic conditions alongside passenger decision-making behavior. Building on this model, we propose a Unified Air-Ground Mobility Coordination (UAGMC) framework, which leverages deep reinforcement learning (RL) and Vehicle-to-Everything (V2X) communication to optimize vertiport selection and dynamically plan air taxi routes. Experimental results demonstrate that UAGMC achieves a 34\\% reduction in average travel time compared to conventional proportional allocation methods, enhancing overall travel efficiency and providing novel insights into the integration and optimization of multimodal transportation systems. This work lays a solid foundation for advancing intelligent urban mobility solutions through the coordination of air and ground transportation modes. The related code can be found at this https URL."
    },
    {
      "title": "Disentangling perception and reasoning for improving data efficiency in learning cloth manipulation without demonstrations",
      "url": "https://arxiv.org/abs/2601.21713",
      "date": "2026-01-29",
      "authors_text": "Donatien Delehelle, Fei Chen, Darwin Caldwell",
      "is_highlight": false,
      "score": 87.0,
      "summary": "This paper presents a modular reinforcement learning approach for cloth manipulation that enhances data efficiency and reduces model size and training time, achieving improved performance in simulation and real-world transfer.",
      "teaser_image": "https://arxiv.org/html/2601.21713/x1.png",
      "debug_abstract": "Cloth manipulation is a ubiquitous task in everyday life, but it remains an open challenge for robotics. The difficulties in developing cloth manipulation policies are attributed to the high-dimensional state space, complex dynamics, and high propensity to self-occlusion exhibited by fabrics. As analytical methods have not been able to provide robust and general manipulation policies, reinforcement learning (RL) is considered a promising approach to these problems. However, to address the large state space and complex dynamics, data-based methods usually rely on large models and long training times. The resulting computational cost significantly hampers the development and adoption of these methods. Additionally, due to the challenge of robust state estimation, garment manipulation policies often adopt an end-to-end learning approach with workspace images as input. While this approach enables a conceptually straightforward sim-to-real transfer via real-world fine-tuning, it also incurs a significant computational cost by training agents on a highly lossy representation of the environment state. This paper questions this common design choice by exploring an efficient and modular approach to RL for cloth manipulation. We show that, through careful design choices, model size and training time can be significantly reduced when learning in simulation. Furthermore, we demonstrate how the resulting simulation-trained model can be transferred to the real world. We evaluate our approach on the SoftGym benchmark and achieve significant performance improvements over available baselines on our task, while using a substantially smaller model."
    },
    {
      "title": "Dynamic Topology Awareness: Breaking the Granularity Rigidity in Vision-Language Navigation",
      "url": "https://arxiv.org/abs/2601.21751",
      "date": "2026-01-29",
      "authors_text": "Jiankun Peng, Jianyuan Guo, Ying Xu, Yue Liu, Jiashuang Yan",
      "is_highlight": false,
      "score": 84.0,
      "summary": "The paper presents DGNav, a dynamic framework for Vision-Language Navigation that adapts topological map density and connectivity to enhance navigation efficiency and safety in complex environments.",
      "teaser_image": "https://arxiv.org/html/2601.21751/x1.png",
      "debug_abstract": "Vision-Language Navigation in Continuous Environments (VLN-CE) presents a core challenge: grounding high-level linguistic instructions into precise, safe, and long-horizon spatial actions. Explicit topological maps have proven to be a vital solution for providing robust spatial memory in such tasks. However, existing topological planning methods suffer from a &#34;Granularity Rigidity&#34; problem. Specifically, these methods typically rely on fixed geometric thresholds to sample nodes, which fails to adapt to varying environmental complexities. This rigidity leads to a critical mismatch: the model tends to over-sample in simple areas, causing computational redundancy, while under-sampling in high-uncertainty regions, increasing collision risks and compromising precision. To address this, we propose DGNav, a framework for Dynamic Topological Navigation, introducing a context-aware mechanism to modulate map density and connectivity on-the-fly. Our approach comprises two core innovations: (1) A Scene-Aware Adaptive Strategy that dynamically modulates graph construction thresholds based on the dispersion of predicted waypoints, enabling &#34;densification on demand&#34; in challenging environments; (2) A Dynamic Graph Transformer that reconstructs graph connectivity by fusing visual, linguistic, and geometric cues into dynamic edge weights, enabling the agent to filter out topological noise and enhancing instruction adherence. Extensive experiments on the R2R-CE and RxR-CE benchmarks demonstrate DGNav exhibits superior navigation performance and strong generalization capabilities. Furthermore, ablation studies confirm that our framework achieves an optimal trade-off between navigation efficiency and safe exploration. The code is available at this https URL."
    },
    {
      "title": "PLANING: A Loosely Coupled Triangle-Gaussian Framework for Streaming 3D Reconstruction",
      "url": "https://arxiv.org/abs/2601.22046",
      "date": "2026-01-29",
      "authors_text": "Changjian Jiang, Kerui Ren, Xudong Li, Kaiwen Song, Linning Xu",
      "is_highlight": false,
      "score": 66.0,
      "summary": "PLANING is a novel streaming 3D reconstruction framework that efficiently combines geometric primitives and neural Gaussians, enhancing quality and speed while reducing redundancy.",
      "teaser_image": "https://arxiv.org/html/2601.22046/x3.png",
      "debug_abstract": "Streaming reconstruction from monocular image sequences remains challenging, as existing methods typically favor either high-quality rendering or accurate geometry, but rarely both. We present PLANING, an efficient on-the-fly reconstruction framework built on a hybrid representation that loosely couples explicit geometric primitives with neural Gaussians, enabling geometry and appearance to be modeled in a decoupled manner. This decoupling supports an online initialization and optimization strategy that separates geometry and appearance updates, yielding stable streaming reconstruction with substantially reduced structural redundancy. PLANING improves dense mesh Chamfer-L2 by 18.52% over PGSR, surpasses ARTDECO by 1.31 dB PSNR, and reconstructs ScanNetV2 scenes in under 100 seconds, over 5x faster than 2D Gaussian Splatting, while matching the quality of offline per-scene optimization. Beyond reconstruction quality, the structural clarity and computational efficiency of \\modelname~make it well suited for a broad range of downstream applications, such as enabling large-scale scene modeling and simulation-ready environments for embodied AI. Project page: this https URL ."
    },
    {
      "title": "Listen, Look, Drive: Coupling Audio Instructions for User-aware VLA-based Autonomous Driving",
      "url": "https://arxiv.org/abs/2601.12142",
      "date": "2026-01-28",
      "authors_text": "Ziang Guo, Feng Yang, Xuefeng Zhang, Jiaqi Guo, Kun Zhao",
      "is_highlight": false,
      "score": 88.0,
      "summary": "EchoVLA enhances autonomous driving by integrating audio instructions with visual inputs, allowing for user influence and emotional context, significantly improving navigation accuracy and safety.",
      "teaser_image": "https://arxiv.org/html/2601.12142/Figs/overview.png",
      "debug_abstract": "Vision Language Action (VLA) models promise an open-vocabulary interface that can translate perceptual ambiguity into semantically grounded driving decisions, yet they still treat language as a static prior fixed at inference time. As a result, the model must infer continuously shifting objectives from pixels alone, yielding delayed or overly conservative maneuvers. We argue that effective VLAs for autonomous driving need an online channel in which users can influence driving with specific intentions. To this end, we present EchoVLA, a user-aware VLA that couples camera streams with in situ audio instructions. We augment the nuScenes dataset with temporally aligned, intent-specific speech commands generated by converting ego-motion descriptions into synthetic audios. Further, we compose emotional speech-trajectory pairs into a multimodal Chain-of-Thought (CoT) for fine-tuning a Multimodal Large Model (MLM) based on Qwen2.5-Omni. Specifically, we synthesize the audio-augmented dataset with different emotion types paired with corresponding driving behaviors, leveraging the emotional cues embedded in tone, pitch, and speech tempo to reflect varying user states, such as urgent or hesitant intentions, thus enabling our EchoVLA to interpret not only the semantic content but also the emotional context of audio commands for more nuanced and emotionally adaptive driving behavior. In open-loop benchmarks, our approach reduces the average L2 error by $59.4\\%$ and the collision rate by $74.4\\%$ compared to the baseline of vision-only perception. More experiments on nuScenes dataset validate that EchoVLA not only steers the trajectory through audio instructions, but also modulates driving behavior in response to the emotions detected in the user&#39;s speech."
    },
    {
      "title": "Tactile-Force Alignment in Vision-Language-Action Models for Force-aware Manipulation",
      "url": "https://arxiv.org/abs/2601.20321",
      "date": "2026-01-28",
      "authors_text": "Yuzhe Huang, Pei Lin, Wanlin Li, Daohan Li, Jiajun Li",
      "is_highlight": false,
      "score": 91.0,
      "summary": "The paper introduces TaF-VLA, a framework that enhances Vision-Language-Action models by integrating tactile-force alignment for improved force-aware robotic manipulation in contact-rich tasks.",
      "teaser_image": "https://arxiv.org/html/2601.20321/figures/fig1-teaser.png",
      "debug_abstract": "Vision-Language-Action (VLA) models have recently emerged as powerful generalists for robotic manipulation. However, due to their predominant reliance on visual modalities, they fundamentally lack the physical intuition required for contact-rich tasks that require precise force regulation and physical reasoning. Existing attempts to incorporate vision-based tactile sensing into VLA models typically treat tactile inputs as auxiliary visual textures, thereby overlooking the underlying correlation between surface deformation and interaction dynamics. To bridge this gap, we propose a paradigm shift from tactile-vision alignment to tactile-force alignment. Here, we introduce TaF-VLA, a framework that explicitly grounds high-dimensional tactile observations in physical interaction forces. To facilitate this, we develop an automated tactile-force data acquisition device and curate the TaF-Dataset, comprising over 10 million synchronized tactile observations, 6-axis force/torque, and matrix force map. To align sequential tactile observations with interaction forces, the central component of our approach is the Tactile-Force Adapter (TaF-Adapter), a tactile sensor encoder that extracts discretized latent information for encoding tactile observations. This mechanism ensures that the learned representations capture history-dependent, noise-insensitive physical dynamics rather than static visual textures. Finally, we integrate this force-aligned encoder into a VLA backbone. Extensive real-world experiments demonstrate that TaF-VLA policy significantly outperforms state-of-the-art tactile-vision-aligned and vision-only baselines on contact-rich tasks, verifying its ability to achieve robust, force-aware manipulation through cross-modal physical reasoning."
    },
    {
      "title": "Rhombot: Rhombus-shaped Modular Robots for Stable, Medium-Independent Reconfiguration Motion",
      "url": "https://arxiv.org/abs/2601.19529",
      "date": "2026-01-27",
      "authors_text": "Jie Gu, Yirui Sun, Zhihao Xia, Tin Lun Lam, Chunxu Tian",
      "is_highlight": false,
      "score": 86.0,
      "summary": "Rhombot is a modular self-reconfigurable robot featuring rhombus-shaped modules that enable stable, medium-independent morphing and locomotion through a novel motion primitive called morphpivoting.",
      "debug_abstract": "In this paper, we present Rhombot, a novel deformable planar lattice modular self-reconfigurable robot (MSRR) with a rhombus shaped module. Each module consists of a parallelogram skeleton with a single centrally mounted actuator that enables folding and unfolding along its diagonal. The core design philosophy is to achieve essential MSRR functionalities such as morphing, docking, and locomotion with minimal control complexity. This enables a continuous and stable reconfiguration process that is independent of the surrounding medium, allowing the system to reliably form various configurations in diverse environments. To leverage the unique kinematics of Rhombot, we introduce morphpivoting, a novel motion primitive for reconfiguration that differs from advanced MSRR systems, and propose a strategy for its continuous execution. Finally, a series of physical experiments validate the module&#39;s stable reconfiguration ability, as well as its positional and docking accuracy."
    },
    {
      "title": "Component-Level Lesioning of Language Models Reveals Clinically Aligned Aphasia Phenotypes",
      "url": "https://arxiv.org/abs/2601.19723",
      "date": "2026-01-27",
      "authors_text": "Yifan Wang, Jichen Zheng, Jingyuan Sun, Yunhao Zhang, Chunyu Ye",
      "is_highlight": false,
      "score": 62.0,
      "summary": "This study presents a framework for simulating aphasia in language models by selectively perturbing components, revealing clinically relevant language impairments and enhancing understanding of language function degradation.",
      "debug_abstract": "Large language models (LLMs) increasingly exhibit human-like linguistic behaviors and internal representations that they could serve as computational simulators of language cognition. We ask whether LLMs can be systematically manipulated to reproduce language-production impairments characteristic of aphasia following focal brain lesions. Such models could provide scalable proxies for testing rehabilitation hypotheses, and offer a controlled framework for probing the functional organization of language. We introduce a clinically grounded, component-level framework that simulates aphasia by selectively perturbing functional components in LLMs, and apply it to both modular Mixture-of-Experts models and dense Transformers using a unified intervention interface. Our pipeline (i) identifies subtype-linked components for Broca&#39;s and Wernicke&#39;s aphasia, (ii) interprets these components with linguistic probing tasks, and (iii) induces graded impairments by progressively perturbing the top-k subtype-linked components, evaluating outcomes with Western Aphasia Battery (WAB) subtests summarized by Aphasia Quotient (AQ). Across architectures and lesioning strategies, subtype-targeted perturbations yield more systematic, aphasia-like regressions than size-matched random perturbations, and MoE modularity supports more localized and interpretable phenotype-to-component mappings. These findings suggest that modular LLMs, combined with clinically informed component perturbations, provide a promising platform for simulating aphasic language production and studying how distinct language functions degrade under targeted disruptions."
    },
    {
      "title": "EgoHandICL: Egocentric 3D Hand Reconstruction with In-Context Learning",
      "url": "https://arxiv.org/abs/2601.19850",
      "date": "2026-01-27",
      "authors_text": "Binzhu Xie, Shi Qiu, Sicheng Zhang, Yinqiao Wang, Hao Xu",
      "is_highlight": false,
      "score": 76.0,
      "summary": "EgoHandICL introduces an innovative in-context learning framework for robust 3D hand reconstruction in egocentric vision, enhancing performance through exemplar retrieval and multimodal context integration.",
      "debug_abstract": "Robust 3D hand reconstruction in egocentric vision is challenging due to depth ambiguity, self-occlusion, and complex hand-object interactions. Prior methods mitigate these issues by scaling training data or adding auxiliary cues, but they often struggle in unseen contexts. We present EgoHandICL, the first in-context learning (ICL) framework for 3D hand reconstruction that improves semantic alignment, visual consistency, and robustness under challenging egocentric conditions. EgoHandICL introduces complementary exemplar retrieval guided by vision-language models (VLMs), an ICL-tailored tokenizer for multimodal context, and a masked autoencoder (MAE)-based architecture trained with hand-guided geometric and perceptual objectives. Experiments on ARCTIC and EgoExo4D show consistent gains over state-of-the-art methods. We also demonstrate real-world generalization and improve EgoVLM hand-object interaction reasoning by using reconstructed hands as visual prompts. Code and data: this https URL"
    },
    {
      "title": "Advances and Innovations in the Multi-Agent Robotic System (MARS) Challenge",
      "url": "https://arxiv.org/abs/2601.18733",
      "date": "2026-01-26",
      "authors_text": "Li Kang, Heng Zhou, Xiufeng Song, Rui Li, Bruno N. Y. Chen",
      "is_highlight": false,
      "score": 94.0,
      "summary": "The MARS Challenge promotes advancements in multi-agent robotic systems by exploring vision-language models for collaborative task execution and planning in dynamic environments.",
      "debug_abstract": "Recent advancements in multimodal large language models and vision-languageaction models have significantly driven progress in Embodied AI. As the field transitions toward more complex task scenarios, multi-agent system frameworks are becoming essential for achieving scalable, efficient, and collaborative solutions. This shift is fueled by three primary factors: increasing agent capabilities, enhancing system efficiency through task delegation, and enabling advanced human-agent interactions. To address the challenges posed by multi-agent collaboration, we propose the Multi-Agent Robotic System (MARS) Challenge, held at the NeurIPS 2025 Workshop on SpaVLE. The competition focuses on two critical areas: planning and control, where participants explore multi-agent embodied planning using vision-language models (VLMs) to coordinate tasks and policy execution to perform robotic manipulation in dynamic environments. By evaluating solutions submitted by participants, the challenge provides valuable insights into the design and coordination of embodied multi-agent systems, contributing to the future development of advanced collaborative AI systems."
    },
    {
      "title": "Fast and Safe Trajectory Optimization for Mobile Manipulators With Neural Configuration Space Distance Field",
      "url": "https://arxiv.org/abs/2601.18548",
      "date": "2026-01-26",
      "authors_text": "Yulin Li, Zhiyuan Song, Yiming Li, Zhicheng Song, Kai Chen",
      "is_highlight": false,
      "score": 86.0,
      "summary": "This paper introduces Generalized Configuration Space Distance Fields (GCDF) for efficient trajectory optimization in mobile manipulators, enhancing collision reasoning and enabling agile motion in complex environments.",
      "debug_abstract": "Mobile manipulators promise agile, long-horizon behavior by coordinating base and arm motion, yet whole-body trajectory optimization in cluttered, confined spaces remains difficult due to high-dimensional nonconvexity and the need for fast, accurate collision reasoning. Configuration Space Distance Fields (CDF) enable fixed-base manipulators to model collisions directly in configuration space via smooth, implicit distances. This representation holds strong potential to bypass the nonlinear configuration-to-workspace mapping while preserving accurate whole-body geometry and providing optimization-friendly collision costs. Yet, extending this capability to mobile manipulators is hindered by unbounded workspaces and tighter base-arm coupling. We lift this promise to mobile manipulation with Generalized Configuration Space Distance Fields (GCDF), extending CDF to robots with both translational and rotational joints in unbounded workspaces with tighter base-arm coupling. We prove that GCDF preserves Euclidean-like local distance structure and accurately encodes whole-body geometry in configuration space, and develop a data generation and training pipeline that yields continuous neural GCDFs with accurate values and gradients, supporting efficient GPU-batched queries. Building on this representation, we develop a high-performance sequential convex optimization framework centered on GCDF-based collision reasoning. The solver scales to large numbers of implicit constraints through (i) online specification of neural constraints, (ii) sparsity-aware active-set detection with parallel batched evaluation across thousands of constraints, and (iii) incremental constraint management for rapid replanning under scene changes."
    },
    {
      "title": "Resisting Manipulative Bots in Meme Coin Copy Trading: A Multi-Agent Approach with Chain-of-Thought Reasoning",
      "url": "https://arxiv.org/abs/2601.08641",
      "date": "2026-01-26",
      "authors_text": "Yichen Luo, Yebo Feng, Jiahua Xu, Yang Liu",
      "is_highlight": false,
      "score": 67.0,
      "summary": "This paper presents a multi-agent system utilizing explainable LLMs to defend against manipulative bots in meme coin copy trading, enhancing prediction accuracy and economic performance.",
      "debug_abstract": "Copy trading has become the dominant entry strategy in meme coin markets. However, due to the market&#39;s extreme illiquid and volatile nature, the strategy exposes an exploitable attack surface: adversaries deploy manipulative bots to front-run trades, conceal positions, and fabricate sentiment, systematically extracting value from naïve copiers at scale. Despite its prevalence, bot-driven manipulation remains largely unexplored, and no robust defensive framework exists. We propose a manipulation-resistant copy-trading system based on a multi-agent architecture powered by a multi-modal, explainable large language model (LLM). Our system decomposes copy trading into three specialized agents for coin evaluation, wallet selection, and timing assessment. Evaluated on historical data from over 6,000 meme coins, our approach outperforms zero-shot and most statistic-driven baselines in prediction accuracy as well as all baselines in economic performance, achieving an average return of 14% for identified smart-money trades and an estimated copier return of 3% per trade under realistic market frictions. Overall, our results demonstrate the effectiveness of agent-based defenses and predictability of trader profitability in adversarial meme coin markets, providing a practical foundation for robust copy trading."
    },
    {
      "title": "AdaReasoner: Dynamic Tool Orchestration for Iterative Visual Reasoning",
      "url": "https://arxiv.org/abs/2601.18631",
      "date": "2026-01-26",
      "authors_text": "Mingyang Song, Haoyu Sun, Jiawei Gu, Linjie Li, Luxin Xu",
      "is_highlight": false,
      "score": 61.0,
      "summary": "AdaReasoner is a multimodal model that autonomously optimizes tool selection and usage for iterative visual reasoning, achieving state-of-the-art performance on various benchmarks.",
      "debug_abstract": "When humans face problems beyond their immediate capabilities, they rely on tools, providing a promising paradigm for improving visual reasoning in multimodal large language models (MLLMs). Effective reasoning, therefore, hinges on knowing which tools to use, when to invoke them, and how to compose them over multiple steps, even when faced with new tools or new tasks. We introduce \\textbf{AdaReasoner}, a family of multimodal models that learn tool use as a general reasoning skill rather than as tool-specific or explicitly supervised behavior. AdaReasoner is enabled by (i) a scalable data curation pipeline exposing models to long-horizon, multi-step tool interactions; (ii) Tool-GRPO, a reinforcement learning algorithm that optimizes tool selection and sequencing based on end-task success; and (iii) an adaptive learning mechanism that dynamically regulates tool usage. Together, these components allow models to infer tool utility from task context and intermediate outcomes, enabling coordination of multiple tools and generalization to unseen tools. Empirically, AdaReasoner exhibits strong tool-adaptive and generalization behaviors: it autonomously adopts beneficial tools, suppresses irrelevant ones, and adjusts tool usage frequency based on task demands, despite never being explicitly trained to do so. These capabilities translate into state-of-the-art performance across challenging benchmarks, improving the 7B base model by +24.9\\% on average and surpassing strong proprietary systems such as GPT-5 on multiple tasks, including VSP and Jigsaw."
    },
    {
      "title": "Timely Machine: Awareness of Time Makes Test-Time Scaling Agentic",
      "url": "https://arxiv.org/abs/2601.16486",
      "date": "2026-01-23",
      "authors_text": "Yichuan Ma, Linyang Li, Yongkang chen, Peiji Li, Xiaozhe Li",
      "is_highlight": false,
      "score": 78.0,
      "summary": "The paper introduces Timely Machine, redefining test-time scaling for LLMs based on wall-clock time, enhancing performance through dynamic strategy adjustments and reinforcement learning.",
      "debug_abstract": "As large language models (LLMs) increasingly tackle complex reasoning tasks, test-time scaling has become critical for enhancing capabilities. However, in agentic scenarios with frequent tool calls, the traditional generation-length-based definition breaks down: tool latency decouples inference time from generation length. We propose Timely Machine, redefining test-time as wall-clock time, where models dynamically adjust strategies based on time budgets. We introduce Timely-Eval, a benchmark spanning high-frequency tool calls, low-frequency tool calls, and time-constrained reasoning. By varying tool latency, we find smaller models excel with fast feedback through more interactions, while larger models dominate high-latency settings via superior interaction quality. Moreover, existing models fail to adapt reasoning to time budgets. We propose Timely-RL to address this gap. After cold-start supervised fine-tuning, we use reinforcement learning to enhance temporal planning. Timely-RL improves time budget awareness and consistently boosts performance across Timely-Eval. We hope our work offers a new perspective on test-time scaling for the agentic era."
    },
    {
      "title": "Adaptive Reinforcement and Model Predictive Control Switching for Safe Human-Robot Cooperative Navigation",
      "url": "https://arxiv.org/abs/2601.16686",
      "date": "2026-01-23",
      "authors_text": "Ning Liu, Sen Shen, Zheng Li, Matthew D'Souza, Jen Jen Chung",
      "is_highlight": false,
      "score": 94.0,
      "summary": "The paper presents ARMS, a hybrid control framework combining reinforcement learning and model predictive control for safe human-robot navigation, achieving superior performance in cluttered environments.",
      "debug_abstract": "This paper addresses the challenge of human-guided navigation for mobile collaborative robots under simultaneous proximity regulation and safety constraints. We introduce Adaptive Reinforcement and Model Predictive Control Switching (ARMS), a hybrid learning-control framework that integrates a reinforcement learning follower trained with Proximal Policy Optimization (PPO) and an analytical one-step Model Predictive Control (MPC) formulated as a quadratic program safety filter. To enable robust perception under partial observability and non-stationary human motion, ARMS employs a decoupled sensing architecture with a Long Short-Term Memory (LSTM) temporal encoder for the human-robot relative state and a spatial encoder for 360-degree LiDAR scans. The core contribution is a learned adaptive neural switcher that performs context-aware soft action fusion between the two controllers, favoring conservative, constraint-aware QP-based control in low-risk regions while progressively shifting control authority to the learned follower in highly cluttered or constrained scenarios where maneuverability is critical, and reverting to the follower action when the QP becomes infeasible. Extensive evaluations against Pure Pursuit, Dynamic Window Approach (DWA), and an RL-only baseline demonstrate that ARMS achieves an 82.5 percent success rate in highly cluttered environments, outperforming DWA and RL-only approaches by 7.1 percent and 3.1 percent, respectively, while reducing average computational latency by 33 percent to 5.2 milliseconds compared to a multi-step MPC baseline. Additional simulation transfer in Gazebo and initial real-world deployment results further indicate the practicality and robustness of ARMS for safe and efficient human-robot collaboration. Source code and a demonstration video are available at this https URL."
    },
    {
      "title": "Natural Language-Driven Global Mapping of Martian Landforms",
      "url": "https://arxiv.org/abs/2601.15949",
      "date": "2026-01-22",
      "authors_text": "Yiran Wang, Shuoyuan Wang, Zhaoran Wei, Jiannan Zhao, Zhonghua Yao",
      "is_highlight": false,
      "score": 72.0,
      "summary": "MarScope is a vision-language framework that enables natural language-driven, label-free mapping of Martian landforms, enhancing global geomorphic analysis and user query flexibility.",
      "debug_abstract": "Planetary surfaces are typically analyzed using high-level semantic concepts in natural language, yet vast orbital image archives remain organized at the pixel level. This mismatch limits scalable, open-ended exploration of planetary surfaces. Here we present MarScope, a planetary-scale vision-language framework enabling natural language-driven, label-free mapping of Martian landforms. MarScope aligns planetary images and text in a shared semantic space, trained on over 200,000 curated image-text pairs. This framework transforms global geomorphic mapping on Mars by replacing pre-defined classifications with flexible semantic retrieval, enabling arbitrary user queries across the entire planet in 5 seconds with F1 scores up to 0.978. Applications further show that it extends beyond morphological classification to facilitate process-oriented analysis and similarity-based geomorphological mapping at a planetary scale. MarScope establishes a new paradigm where natural language serves as a direct interface for scientific discovery over massive geospatial datasets."
    },
    {
      "title": "Robust Tool Use via Fission-GRPO: Learning to Recover from Execution Errors",
      "url": "https://arxiv.org/abs/2601.15625",
      "date": "2026-01-22",
      "authors_text": "Zhiwei Zhang, Fei Zhao, Rui Wang, Zezhong Wang, Bin Liang",
      "is_highlight": false,
      "score": 87.1,
      "summary": "Fission-GRPO enhances tool use in LLMs by converting execution errors into corrective supervision, improving error recovery and overall accuracy in multi-turn interactions.",
      "debug_abstract": "Large language models (LLMs) can call tools effectively, yet they remain brittle in multi-turn execution: following a tool call error, smaller models often degenerate into repetitive invalid re-invocations, failing to interpret error feedback and self-correct. This brittleness hinders reliable real-world deployment, where the execution errors are inherently inevitable during tool interaction procedures. We identify a key limitation of current approaches: standard reinforcement learning (RL) treats errors as sparse negative rewards, providing no guidance on how to recover, while pre-collected synthetic error-correction datasets suffer from distribution mismatch with the model&#39;s on-policy error modes. To bridge this gap, we propose Fission-GRPO, a framework that converts execution errors into corrective supervision within the RL training loop. Our core mechanism fissions each failed trajectory into a new training instance by augmenting it with diagnostic feedback from a finetuned Error Simulator, then resampling recovery rollouts on-policy. This enables the model to learn from the precise errors it makes during exploration, rather than from static, pre-collected error cases. On the BFCL v4 Multi-Turn, Fission-GRPO improves the error recovery rate of Qwen3-8B by 5.7% absolute, crucially, yielding a 4% overall accuracy gain (42.75% to 46.75%) over GRPO and outperforming specialized tool-use agents."
    },
    {
      "title": "PhysicsMind: Sim and Real Mechanics Benchmarking for Physical Reasoning and Prediction in Foundational VLMs and World Models",
      "url": "https://arxiv.org/abs/2601.16007",
      "date": "2026-01-22",
      "authors_text": "Chak-Wing Mak, Guanyu Zhu, Boyi Zhang, Hongji Li, Xiaowei Chi",
      "is_highlight": false,
      "score": 94.1,
      "summary": "PhysicsMind introduces a benchmark for evaluating physical reasoning in multimodal models through real and simulated tasks focused on fundamental physics principles, revealing significant gaps in current models' understanding.",
      "debug_abstract": "Modern foundational Multimodal Large Language Models (MLLMs) and video world models have advanced significantly in mathematical, common-sense, and visual reasoning, but their grasp of the underlying physics remains underexplored. Existing benchmarks attempting to measure this matter rely on synthetic, Visual Question Answer templates or focus on perceptual video quality that is tangential to measuring how well the video abides by physical laws. To address this fragmentation, we introduce PhysicsMind, a unified benchmark with both real and simulation environments that evaluates law-consistent reasoning and generation over three canonical principles: Center of Mass, Lever Equilibrium, and Newton&#39;s First Law. PhysicsMind comprises two main tasks: i) VQA tasks, testing whether models can reason and determine physical quantities and values from images or short videos, and ii) Video Generation(VG) tasks, evaluating if predicted motion trajectories obey the same center-of-mass, torque, and inertial constraints as the ground truth. A broad range of recent models and video generation models is evaluated on PhysicsMind and found to rely on appearance heuristics while often violating basic mechanics. These gaps indicate that current scaling and training are still insufficient for robust physical understanding, underscoring PhysicsMind as a focused testbed for physics-aware multimodal models. Our data will be released upon acceptance."
    }
  ],
  "香港大学机械工程系机器人实验室": [
    {
      "title": "Large Language Models Can Take False First Steps at Inference-time Planning",
      "url": "https://arxiv.org/abs/2602.02991",
      "date": "2026-02-03",
      "authors_text": "Haijiang Yan, Jian-Qiao Zhu, Adam Sanborn",
      "is_highlight": false,
      "score": 73.0,
      "summary": "The paper explains that large language models exhibit inconsistent planning at inference due to evolving self-generated context, supported by experiments demonstrating planning shifts and reduced biases.",
      "teaser_image": "https://arxiv.org/html/2602.02991/images/llama_regression.png",
      "debug_abstract": "Large language models (LLMs) have been shown to acquire sequence-level planning abilities during training, yet their planning behavior exhibited at inference time often appears short-sighted and inconsistent with these capabilities. We propose a Bayesian account for this gap by grounding planning behavior in the evolving generative context: given the subtle differences between natural language and the language internalized by LLMs, accumulated self-generated context drives a planning-shift during inference and thereby creates the appearance of compromised planning behavior. We further validate the proposed model through two controlled experiments: a random-generation task demonstrating constrained planning under human prompts and increasing planning strength as self-generated context accumulates, and a Gaussian-sampling task showing reduced initial bias when conditioning on self-generated sequences. These findings provide a theoretical explanation along with empirical evidence for characterizing how LLMs plan ahead during inference."
    },
    {
      "title": "Search-R2: Enhancing Search-Integrated Reasoning via Actor-Refiner Collaboration",
      "url": "https://arxiv.org/abs/2602.03647",
      "date": "2026-02-03",
      "authors_text": "Bowei He, Minda Hu, Zenan Xu, Hongru Wang, Licheng Zong",
      "is_highlight": false,
      "score": 52.0,
      "summary": "Search-R2 introduces an Actor-Refiner framework that enhances search-integrated reasoning in language agents through targeted intervention and fine-grained supervision, outperforming existing methods.",
      "teaser_image": "https://arxiv.org/html/2602.03647/x2.png",
      "debug_abstract": "Search-integrated reasoning enables language agents to transcend static parametric knowledge by actively querying external sources. However, training these agents via reinforcement learning is hindered by the multi-scale credit assignment problem: existing methods typically rely on sparse, trajectory-level rewards that fail to distinguish between high-quality reasoning and fortuitous guesses, leading to redundant or misleading search behaviors. To address this, we propose Search-R2, a novel Actor-Refiner collaboration framework that enhances reasoning through targeted intervention, with both components jointly optimized during training. Our approach decomposes the generation process into an Actor, which produces initial reasoning trajectories, and a Meta-Refiner, which selectively diagnoses and repairs flawed steps via a &#39;cut-and-regenerate&#39; mechanism. To provide fine-grained supervision, we introduce a hybrid reward design that couples outcome correctness with a dense process reward quantifying the information density of retrieved evidence. Theoretically, we formalize the Actor-Refiner interaction as a smoothed mixture policy, proving that selective correction yields strict performance gains over strong baselines. Extensive experiments across various general and multi-hop QA datasets demonstrate that Search-R2 consistently outperforms strong RAG and RL-based baselines across model scales, achieving superior reasoning accuracy with minimal overhead."
    },
    {
      "title": "Dual Quaternion SE(3) Synchronization with Recovery Guarantees",
      "url": "https://arxiv.org/abs/2602.00324",
      "date": "2026-01-30",
      "authors_text": "Jianing Zhao, Linglingzhi Zhu, Anthony Man-Cho So",
      "is_highlight": false,
      "score": 75.0,
      "summary": "This paper presents a dual quaternion-based SE(3) synchronization method with a two-stage algorithm that guarantees recovery of absolute poses from noisy transformations, outperforming traditional matrix methods.",
      "teaser_image": "https://arxiv.org/html/2602.00324/x2.png",
      "debug_abstract": "Synchronization over the special Euclidean group SE(3) aims to recover absolute poses from noisy pairwise relative transformations and is a core primitive in robotics and 3D vision. Standard approaches often require multi-step heuristic procedures to recover valid poses, which are difficult to analyze and typically lack theoretical guarantees. This paper adopts a dual quaternion representation and formulates SE(3) synchronization directly over the unit dual quaternion. A two-stage algorithm is developed: A spectral initializer computed via the power method on a Hermitian dual quaternion measurement matrix, followed by a dual quaternion generalized power method (DQGPM) that enforces feasibility through per-iteration projection. The estimation error bounds are established for spectral estimators, and DQGPM is shown to admit a finite-iteration error bound and achieves linear error contraction up to an explicit noise-dependent threshold. Experiments on synthetic benchmarks and real-world multi-scan point-set registration demonstrate that the proposed pipeline improves both accuracy and efficiency over representative matrix-based methods."
    },
    {
      "title": "Offline Discovery of Interpretable Skills from Multi-Task Trajectories",
      "url": "https://arxiv.org/abs/2602.01018",
      "date": "2026-02-01",
      "authors_text": "Chongyu Zhu, Mithun Vanniasinghe, Jiayu Chen, Chi-Guhn Lee",
      "is_highlight": false,
      "score": 77.0,
      "summary": "LOKI is a three-stage framework for offline skill discovery and hierarchical imitation learning, effectively extracting interpretable skills from multi-task trajectories without explicit rewards.",
      "teaser_image": "https://arxiv.org/html/2602.01018/fig/two_stage_skill_learning.png",
      "debug_abstract": "Hierarchical Imitation Learning is a powerful paradigm for acquiring complex robot behaviors from demonstrations. A central challenge, however, lies in discovering reusable skills from long-horizon, multi-task offline data, especially when the data lacks explicit rewards or subtask annotations. In this work, we introduce LOKI, a three-stage end-to-end learning framework designed for offline skill discovery and hierarchical imitation. The framework commences with a two-stage, weakly supervised skill discovery process: Stage one performs coarse, task-aware macro-segmentation by employing an alignment-enforced Vector Quantized VAE guided by weak task labels. Stage two then refines these segments at a micro-level using a self-supervised sequential model, followed by an iterative clustering process to consolidate skill boundaries. The third stage then leverages these precise boundaries to construct a hierarchical policy within an option-based framework-complete with a learned termination condition beta for explicit skill switching. LOKI achieves high success rates on the challenging D4RL Kitchen benchmark and outperforms standard HIL baselines. Furthermore, we demonstrate that the discovered skills are semantically meaningful, aligning with human intuition, and exhibit compositionality by successfully sequencing them to solve a novel, unseen task."
    },
    {
      "title": "Residual Decoding: Mitigating Hallucinations in Large Vision-Language Models via History-Aware Residual Guidance",
      "url": "https://arxiv.org/abs/2602.01047",
      "date": "2026-02-01",
      "authors_text": "Xinrong Chen, Xu Chu, Yingmin Qiu, Hengyuan Zhang, Jing Xiong",
      "is_highlight": false,
      "score": 53.0,
      "summary": "Residual Decoding (ResDec) mitigates hallucinations in Large Vision-Language Models by leveraging historical information to enhance decoding and improve visual grounding.",
      "teaser_image": "https://arxiv.org/html/2602.01047/x1.png",
      "debug_abstract": "Large Vision-Language Models (LVLMs) can reason effectively from image-text inputs and perform well in various multimodal tasks. Despite this success, they are affected by language priors and often produce hallucinations. Hallucinations denote generated content that is grammatically and syntactically coherent, yet bears no match or direct relevance to actual visual input. To address this problem, we propose Residual Decoding (ResDec). It is a novel training-free method that uses historical information to aid decoding. The method relies on the internal implicit reasoning mechanism and token logits evolution mechanism of LVLMs to correct biases. Extensive experiments demonstrate that ResDec effectively suppresses hallucinations induced by language priors, significantly improves visual grounding, and reduces object hallucinations. In addition to mitigating hallucinations, ResDec also performs exceptionally well on comprehensive LVLM benchmarks, highlighting its broad applicability."
    },
    {
      "title": "MedAD-R1: Eliciting Consistent Reasoning in Interpretible Medical Anomaly Detection via Consistency-Reinforced Policy Optimization",
      "url": "https://arxiv.org/abs/2602.01081",
      "date": "2026-02-01",
      "authors_text": "Haitao Zhang, Yingying Wang, Jiaxiang Wang, Haote Xu, Hongyang Zhang",
      "is_highlight": false,
      "score": 64.0,
      "summary": "MedAD-R1 enhances medical anomaly detection by utilizing a two-stage training framework with consistency-reinforced policy optimization, achieving state-of-the-art performance on the MedAD-38K benchmark.",
      "teaser_image": "https://arxiv.org/html/2602.01081/x3.png",
      "debug_abstract": "Medical Anomaly Detection (MedAD) presents a significant opportunity to enhance diagnostic accuracy using Large Multimodal Models (LMMs) to interpret and answer questions based on medical images. However, the reliance on Supervised Fine-Tuning (SFT) on simplistic and fragmented datasets has hindered the development of models capable of plausible reasoning and robust multimodal generalization. To overcome this, we introduce MedAD-38K, the first large-scale, multi-modal, and multi-center benchmark for MedAD featuring diagnostic Chain-of-Thought (CoT) annotations alongside structured Visual Question-Answering (VQA) pairs. On this foundation, we propose a two-stage training framework. The first stage, Cognitive Injection, uses SFT to instill foundational medical knowledge and align the model with a structured think-then-answer paradigm. Given that standard policy optimization can produce reasoning that is disconnected from the final answer, the second stage incorporates Consistency Group Relative Policy Optimization (Con-GRPO). This novel algorithm incorporates a crucial consistency reward to ensure the generated reasoning process is relevant and logically coherent with the final diagnosis. Our proposed model, MedAD-R1, achieves state-of-the-art (SOTA) performance on the MedAD-38K benchmark, outperforming strong baselines by more than 10\\%. This superior performance stems from its ability to generate transparent and logically consistent reasoning pathways, offering a promising approach to enhancing the trustworthiness and interpretability of AI for clinical decision support."
    },
    {
      "title": "Adaptive Rollout Allocation for Online Reinforcement Learning with Verifiable Rewards",
      "url": "https://arxiv.org/abs/2602.01601",
      "date": "2026-02-02",
      "authors_text": "Hieu Trung Nguyen, Bao Nguyen, Wenao Ma, Yuzhi Zhao, Ruifeng She",
      "is_highlight": false,
      "score": 40.0,
      "summary": "The paper presents \\Ours, a variance-informed strategy for adaptive rollout allocation in online reinforcement learning, enhancing sampling efficiency and performance over traditional methods.",
      "teaser_image": "https://arxiv.org/html/2602.01601/x2.png",
      "debug_abstract": "Sampling efficiency is a key bottleneck in reinforcement learning with verifiable rewards. Existing group-based policy optimization methods, such as GRPO, allocate a fixed number of rollouts for all training prompts. This uniform allocation implicitly treats all prompts as equally informative, and could lead to inefficient computational budget usage and impede training progress. We introduce \\Ours, a Variance-Informed Predictive allocation strategy that allocates a given rollout budget to the prompts in the incumbent batch to minimize the expected gradient variance of the policy update. At each iteration, \\Ours~uses a lightweight Gaussian process model to predict per-prompt success probabilities based on recent rollouts. These probability predictions are translated into variance estimates, which are then fed into a convex optimization problem to determine the optimal rollout allocations under a hard compute budget constraint. Empirical results show that \\Ours~consistently improves sampling efficiency and achieves higher performance than uniform or heuristic allocation strategies in multiple benchmarks. Our code will be available at this https URL."
    },
    {
      "title": "Reasoning with Autoregressive-Diffusion Collaborative Thoughts",
      "url": "https://arxiv.org/abs/2602.01608",
      "date": "2026-02-02",
      "authors_text": "Mu Yuan, Liekang Zeng, Guoliang Xing, Lan Zhang, Yunhao Liu",
      "is_highlight": false,
      "score": 41.0,
      "summary": "The paper presents Collaborative Thoughts, a framework integrating autoregressive and diffusion models for enhanced spatial reasoning and generation through iterative feedback and joint reasoning.",
      "teaser_image": "https://arxiv.org/html/2602.01608/x2.png",
      "debug_abstract": "Autoregressive and diffusion models represent two complementary generative paradigms. Autoregressive models excel at sequential planning and constraint composition, yet struggle with tasks that require explicit spatial or physical grounding. Diffusion models, in contrast, capture rich spatial structure through high-dimensional generation, but lack the stepwise logical control needed to satisfy complex, multi-stage constraints or to reliably identify and correct errors. We introduce Collaborative Thoughts, a unified collaborative framework that enables autoregressive and diffusion models to reason and generate jointly through a closed-loop interaction. In Collaborative Thoughts, autoregressive models perform structured planning and constraint management, diffusion models instantiate these constraints as intermediate visual thoughts, and a vision-based critic module evaluates whether the visual thoughts satisfy the intended structural and physical requirements. This feedback is then used to iteratively refine subsequent planning and generation steps, mitigating error propagation across modalities. Importantly, Collaborative Thoughts uses the same collaborative loop regardless of whether the task is autoregressive question answering or diffusion-based visual generation. Through representative examples, we illustrate how Collaborative Thoughts can improve the reliability of spatial reasoning and the controllability of generation."
    },
    {
      "title": "Tilt-Ropter: A Novel Hybrid Aerial and Terrestrial Vehicle with Tilt Rotors and Passive Wheels",
      "url": "https://arxiv.org/abs/2602.01700",
      "date": "2026-02-02",
      "authors_text": "Ruoyu Wang, Xuchen Liu, Zongzhou Wu, Zixuan Guo, Wendi Ding",
      "is_highlight": false,
      "score": 38.0,
      "summary": "The Tilt-Ropter is a hybrid aerial-terrestrial vehicle that utilizes tilt rotors and passive wheels for efficient locomotion, featuring advanced control systems for enhanced mobility and energy savings.",
      "teaser_image": "https://arxiv.org/html/2602.01700/x2.png",
      "debug_abstract": "In this work, we present Tilt-Ropter, a novel hybrid aerial-terrestrial vehicle (HATV) that combines tilt rotors with passive wheels to achieve energy-efficient multi-mode locomotion. Unlike existing under-actuated HATVs, the fully actuated design of Tilt-Ropter enables decoupled force and torque control, greatly enhancing its mobility and environmental adaptability. A nonlinear model predictive controller (NMPC) is developed to track reference trajectories and handle contact constraints across locomotion modes, while a dedicated control allocation module exploits actuation redundancy to achieve energy-efficient control of actuators. Additionally, to enhance robustness during ground contact, we introduce an external wrench estimation algorithm that estimates environmental interaction forces and torques in real time. The system is validated through both simulation and real-world experiments, including seamless air-ground transitions and trajectory tracking. Results show low tracking errors in both modes and highlight a 92.8% reduction in power consumption during ground locomotion, demonstrating the system&#39;s potential for long-duration missions across large-scale and energy-constrained environments."
    },
    {
      "title": "Breaking the Static Graph: Context-Aware Traversal for Robust Retrieval-Augmented Generation",
      "url": "https://arxiv.org/abs/2602.01965",
      "date": "2026-02-02",
      "authors_text": "Kwun Hang Lau, Fangyuan Zhang, Boyu Ruan, Yingli Zhou, Qintian Guo",
      "is_highlight": false,
      "score": 27.0,
      "summary": "CatRAG enhances Retrieval-Augmented Generation by transforming static Knowledge Graphs into query-adaptive structures, improving multi-hop evidence retrieval and reasoning completeness.",
      "teaser_image": "https://arxiv.org/html/2602.01965/image/HubBiasShift.png",
      "debug_abstract": "Recent advances in Retrieval-Augmented Generation (RAG) have shifted from simple vector similarity to structure-aware approaches like HippoRAG, which leverage Knowledge Graphs (KGs) and Personalized PageRank (PPR) to capture multi-hop dependencies. However, these methods suffer from a &#34;Static Graph Fallacy&#34;: they rely on fixed transition probabilities determined during indexing. This rigidity ignores the query-dependent nature of edge relevance, causing semantic drift where random walks are diverted into high-degree &#34;hub&#34; nodes before reaching critical downstream evidence. Consequently, models often achieve high partial recall but fail to retrieve the complete evidence chain required for multi-hop queries. To address this, we propose CatRAG, Context-Aware Traversal for robust RAG, a framework that builds on the HippoRAG 2 architecture and transforms the static KG into a query-adaptive navigation structure. We introduce a multi-faceted framework to steer the random walk: (1) Symbolic Anchoring, which injects weak entity constraints to regularize the random walk; (2) Query-Aware Dynamic Edge Weighting, which dynamically modulates graph structure, to prune irrelevant paths while amplifying those aligned with the query&#39;s intent; and (3) Key-Fact Passage Weight Enhancement, a cost-efficient bias that structurally anchors the random walk to likely evidence. Experiments across four multi-hop benchmarks demonstrate that CatRAG consistently outperforms state of the art baselines. Our analysis reveals that while standard Recall metrics show modest gains, CatRAG achieves substantial improvements in reasoning completeness, the capacity to recover the entire evidence path without gaps. These results reveal that our approach effectively bridges the gap between retrieving partial context and enabling fully grounded reasoning. Resources are available at this https URL."
    },
    {
      "title": "TIDE: Trajectory-based Diagnostic Evaluation of Test-Time Improvement in LLM Agents",
      "url": "https://arxiv.org/abs/2602.02196",
      "date": "2026-02-02",
      "authors_text": "Hang Yan, Xinyu Che, Fangzhi Xu, Qiushi Sun, Zichen Ding",
      "is_highlight": false,
      "score": 20.0,
      "summary": "The TIDE framework evaluates Test-Time Improvement in LLM agents by analyzing task completion dynamics, recursive behaviors, and memory constraints to enhance performance optimization.",
      "teaser_image": "https://arxiv.org/html/2602.02196/x1.png",
      "debug_abstract": "Recent advances in autonomous LLM agents demonstrate their ability to improve performance through iterative interaction with the environment. We define this paradigm as Test-Time Improvement (TTI). However, the mechanisms under how and why TTI succeed or fail remain poorly understood, and existing evaluation metrics fail to capture their task optimization efficiency, behavior adaptation after erroneous actions, and the specific utility of working memory for task completion. To address these gaps, we propose Test-time Improvement Diagnostic Evaluation (TIDE), an agent-agnostic and environment-agnostic framework that decomposes TTI into three comprehensive and interconnected dimensions. The framework measures (1) the overall temporal dynamics of task completion and (2) identifies whether performance is primarily constrained by recursive looping behaviors or (3) by burdensome accumulated memory. Through extensive experiments across diverse agents and environments, TIDE highlights that improving agent performance requires more than scaling internal reasoning, calling for explicitly optimizing the interaction dynamics between the agent and the environment."
    },
    {
      "title": "Towards Agentic Intelligence for Materials Science",
      "url": "https://arxiv.org/abs/2602.00169",
      "date": "2026-01-29",
      "authors_text": "Huan Zhang, Yizhan Li, Wenhao Huang, Ziyu Hou, Yu Song",
      "is_highlight": false,
      "score": 12.0,
      "summary": "This paper advocates for developing agentic AI systems in materials science that autonomously plan, act, and learn to enhance discovery processes beyond traditional models.",
      "teaser_image": "https://arxiv.org/html/2602.00169/x1.png",
      "debug_abstract": "The convergence of artificial intelligence and materials science presents a transformative opportunity, but achieving true acceleration in discovery requires moving beyond task-isolated, fine-tuned models toward agentic systems that plan, act, and learn across the full discovery loop. This survey advances a unique pipeline-centric view that spans from corpus curation and pretraining, through domain adaptation and instruction tuning, to goal-conditioned agents interfacing with simulation and experimental platforms. Unlike prior reviews, we treat the entire process as an end-to-end system to be optimized for tangible discovery outcomes rather than proxy benchmarks. This perspective allows us to trace how upstream design choices-such as data curation and training objectives-can be aligned with downstream experimental success through effective credit assignment. To bridge communities and establish a shared frame of reference, we first present an integrated lens that aligns terminology, evaluation, and workflow stages across AI and materials science. We then analyze the field through two focused lenses: From the AI perspective, the survey details LLM strengths in pattern recognition, predictive analytics, and natural language processing for literature mining, materials characterization, and property prediction; from the materials science perspective, it highlights applications in materials design, process optimization, and the acceleration of computational workflows via integration with external tools (e.g., DFT, robotic labs). Finally, we contrast passive, reactive approaches with agentic design, cataloging current contributions while motivating systems that pursue long-horizon goals with autonomy, memory, and tool use. This survey charts a practical roadmap towards autonomous, safety-aware LLM agents aimed at discovering novel and useful materials."
    },
    {
      "title": "Failure-Aware Bimanual Teleoperation via Conservative Value Guided Assistance",
      "url": "https://arxiv.org/abs/2602.01092",
      "date": "2026-02-01",
      "authors_text": "Peng Zhou, Zhongxuan Li, Jinsong Wu, Jiaming Qi, Jun Hu",
      "is_highlight": false,
      "score": 9.0,
      "summary": "This paper presents a failure-aware bimanual teleoperation framework that uses conservative value learning to enhance task success and reduce operator workload through compliant haptic assistance.",
      "teaser_image": "https://arxiv.org/html/2602.01092/x1.png",
      "debug_abstract": "Teleoperation of high-precision manipulation is con-strained by tight success tolerances and complex contact dy-namics, which make impending failures difficult for human operators to anticipate under partial observability. This paper proposes a value-guided, failure-aware framework for bimanual teleoperation that provides compliant haptic assistance while pre-serving continuous human authority. The framework is trained entirely from heterogeneous offline teleoperation data containing both successful and failed executions. Task feasibility is mod-eled as a conservative success score learned via Conservative Value Learning, yielding a risk-sensitive estimate that remains reliable under distribution shift. During online operation, the learned success score regulates the level of assistance, while a learned actor provides a corrective motion direction. Both are integrated through a joint-space impedance interface on the master side, yielding continuous guidance that steers the operator away from failure-prone actions without overriding intent. Experimental results on contact-rich manipulation tasks demonstrate improved task success rates and reduced operator workload compared to conventional teleoperation and shared-autonomy baselines, indicating that conservative value learning provides an effective mechanism for embedding failure awareness into bilateral teleoperation. Experimental videos are available at this https URL"
    },
    {
      "title": "FlowSteer: Interactive Agentic Workflow Orchestration via End-to-End Reinforcement Learning",
      "url": "https://arxiv.org/abs/2602.01664",
      "date": "2026-02-02",
      "authors_text": "Mingda Zhang, Haoran Luo, Tiesunlong Shen, Qika Lin, Xiaoying Tang",
      "is_highlight": false,
      "score": 7.0,
      "summary": "FlowSteer is an end-to-end reinforcement learning framework that automates workflow orchestration through interactive learning, addressing manual costs and improving performance across diverse tasks.",
      "teaser_image": "https://arxiv.org/html/2602.01664/x1.png",
      "debug_abstract": "In recent years, a variety of powerful agentic workflows have been applied to solve a wide range of human problems. However, existing workflow orchestration still faces key challenges, including high manual cost, reliance on specific operators/large language models (LLMs), and sparse reward signals. To address these challenges, we propose FlowSteer, an end-to-end reinforcement learning framework that takes a lightweight policy model as the agent and an executable canvas environment, automating workflow orchestration through multi-turn interaction. In this process, the policy model analyzes execution states and selects editing actions, while the canvas executes operators and returns feedback for iterative refinement. Moreover, FlowSteer provides a plug-and-play framework that supports diverse operator libraries and interchangeable LLM backends. To effectively train this interaction paradigm, we propose Canvas Workflow Relative Policy Optimization (CWRPO), which introduces diversity-constrained rewards with conditional release to stabilize learning and suppress shortcut behaviors. Experimental results on twelve datasets show that FlowSteer significantly outperforms baselines across various tasks."
    },
    {
      "title": "Diffusion Model-based Reinforcement Learning for Version Age of Information Scheduling: Average and Tail-Risk-Sensitive Control",
      "url": "https://arxiv.org/abs/2601.18069",
      "date": "2026-01-26",
      "authors_text": "Haoyuan Pan, Sizhao Chen, Zhaorui Wang, Tse-Tin Chan",
      "is_highlight": false,
      "score": 57.0,
      "summary": "This paper presents a novel reinforcement learning approach, RS-D3SAC, for optimizing Version Age of Information scheduling by minimizing both average and tail-risk-sensitive metrics in wireless systems.",
      "teaser_image": null,
      "debug_abstract": "Ensuring timely and semantically accurate information delivery is critical in real-time wireless systems. While Age of Information (AoI) quantifies temporal freshness, Version Age of Information (VAoI) captures semantic staleness by accounting for version evolution between transmitters and receivers. Existing VAoI scheduling approaches primarily focus on minimizing average VAoI, overlooking rare but severe staleness events that can compromise reliability under stochastic packet arrivals and unreliable channels. This paper investigates both average-oriented and tail-risk-sensitive VAoI scheduling in a multi-user status update system with long-term transmission cost constraints. We first formulate the average VAoI minimization problem as a constrained Markov decision process and introduce a deep diffusion-based Soft Actor-Critic (D2SAC) algorithm. By generating actions through a diffusion-based denoising process, D2SAC enhances policy expressiveness and establishes a strong baseline for mean performance. Building on this foundation, we put forth RS-D3SAC, a risk-sensitive deep distributional diffusion-based Soft Actor-Critic algorithm. RS-D3SAC integrates a diffusion-based actor with a quantile-based distributional critic, explicitly modeling the full VAoI return distribution. This enables principled tail-risk optimization via Conditional Value-at-Risk (CVaR) while satisfying long-term transmission cost constraints. Extensive simulations show that, while D2SAC reduces average VAoI, RS-D3SAC consistently achieves substantial reductions in CVaR without sacrificing mean performance. The dominant gain in tail-risk reduction stems from the distributional critic, with the diffusion-based actor providing complementary refinement to stabilize and enrich policy decisions, highlighting their effectiveness for robust and risk-aware VAoI scheduling in multi-user wireless systems."
    },
    {
      "title": "LoD-Structured 3D Gaussian Splatting for Streaming Video Reconstruction",
      "url": "https://arxiv.org/abs/2601.18475",
      "date": "2026-01-26",
      "authors_text": "Xinhui Liu, Can Wang, Lei Liu, Zhenghao Chen, Wei Jiang",
      "is_highlight": false,
      "score": 52.0,
      "summary": "The StreamLoD-GS framework enhances Streaming Free-Viewpoint Video reconstruction by optimizing 3D Gaussian Splatting for efficient, high-fidelity rendering with reduced storage needs.",
      "teaser_image": null,
      "debug_abstract": "Free-Viewpoint Video (FVV) reconstruction enables photorealistic and interactive 3D scene visualization; however, real-time streaming is often bottlenecked by sparse-view inputs, prohibitive training costs, and bandwidth constraints. While recent 3D Gaussian Splatting (3DGS) has advanced FVV due to its superior rendering speed, Streaming Free-Viewpoint Video (SFVV) introduces additional demands for rapid optimization, high-fidelity reconstruction under sparse constraints, and minimal storage footprints. To bridge this gap, we propose StreamLoD-GS, an LoD-based Gaussian Splatting framework designed specifically for SFVV. Our approach integrates three core innovations: 1) an Anchor- and Octree-based LoD-structured 3DGS with a hierarchical Gaussian dropout technique to ensure efficient and stable optimization while maintaining high-quality rendering; 2) a GMM-based motion partitioning mechanism that separates dynamic and static content, refining dynamic regions while preserving background stability; and 3) a quantized residual refinement framework that significantly reduces storage requirements without compromising visual fidelity. Extensive experiments demonstrate that StreamLoD-GS achieves competitive or state-of-the-art performance in terms of quality, efficiency, and storage."
    },
    {
      "title": "Heterogeneous Vertiport Selection Optimization for On-Demand Air Taxi Services: A Deep Reinforcement Learning Approach",
      "url": "https://arxiv.org/abs/2601.21316",
      "date": "2026-01-29",
      "authors_text": "Aoyu Pang, Maonan Wang, Zifan Sha, Wenwei Yue, Changle Li",
      "is_highlight": false,
      "score": 80.0,
      "summary": "The paper presents a deep reinforcement learning framework for optimizing vertiport selection and air taxi routing, achieving significant travel time reductions in urban air mobility systems.",
      "teaser_image": "https://arxiv.org/html/2601.21316/x2.png",
      "debug_abstract": "Urban Air Mobility (UAM) has emerged as a transformative solution to alleviate urban congestion by utilizing low-altitude airspace, thereby reducing pressure on ground transportation networks. To enable truly efficient and seamless door-to-door travel experiences, UAM requires close integration with existing ground transportation infrastructure. However, current research on optimal integrated routing strategies for passengers in air-ground mobility systems remains limited, with a lack of systematic this http URL address this gap, we first propose a unified optimization model that integrates strategy selection for both air and ground transportation. This model captures the dynamic characteristics of multimodal transport networks and incorporates real-time traffic conditions alongside passenger decision-making behavior. Building on this model, we propose a Unified Air-Ground Mobility Coordination (UAGMC) framework, which leverages deep reinforcement learning (RL) and Vehicle-to-Everything (V2X) communication to optimize vertiport selection and dynamically plan air taxi routes. Experimental results demonstrate that UAGMC achieves a 34\\% reduction in average travel time compared to conventional proportional allocation methods, enhancing overall travel efficiency and providing novel insights into the integration and optimization of multimodal transportation systems. This work lays a solid foundation for advancing intelligent urban mobility solutions through the coordination of air and ground transportation modes. The related code can be found at this https URL."
    },
    {
      "title": "Disentangling perception and reasoning for improving data efficiency in learning cloth manipulation without demonstrations",
      "url": "https://arxiv.org/abs/2601.21713",
      "date": "2026-01-29",
      "authors_text": "Donatien Delehelle, Fei Chen, Darwin Caldwell",
      "is_highlight": false,
      "score": 87.0,
      "summary": "This paper presents a modular reinforcement learning approach for cloth manipulation that enhances data efficiency and reduces model size and training time, achieving improved performance in simulation and real-world transfer.",
      "teaser_image": "https://arxiv.org/html/2601.21713/x1.png",
      "debug_abstract": "Cloth manipulation is a ubiquitous task in everyday life, but it remains an open challenge for robotics. The difficulties in developing cloth manipulation policies are attributed to the high-dimensional state space, complex dynamics, and high propensity to self-occlusion exhibited by fabrics. As analytical methods have not been able to provide robust and general manipulation policies, reinforcement learning (RL) is considered a promising approach to these problems. However, to address the large state space and complex dynamics, data-based methods usually rely on large models and long training times. The resulting computational cost significantly hampers the development and adoption of these methods. Additionally, due to the challenge of robust state estimation, garment manipulation policies often adopt an end-to-end learning approach with workspace images as input. While this approach enables a conceptually straightforward sim-to-real transfer via real-world fine-tuning, it also incurs a significant computational cost by training agents on a highly lossy representation of the environment state. This paper questions this common design choice by exploring an efficient and modular approach to RL for cloth manipulation. We show that, through careful design choices, model size and training time can be significantly reduced when learning in simulation. Furthermore, we demonstrate how the resulting simulation-trained model can be transferred to the real world. We evaluate our approach on the SoftGym benchmark and achieve significant performance improvements over available baselines on our task, while using a substantially smaller model."
    },
    {
      "title": "Dynamic Topology Awareness: Breaking the Granularity Rigidity in Vision-Language Navigation",
      "url": "https://arxiv.org/abs/2601.21751",
      "date": "2026-01-29",
      "authors_text": "Jiankun Peng, Jianyuan Guo, Ying Xu, Yue Liu, Jiashuang Yan",
      "is_highlight": false,
      "score": 84.0,
      "summary": "The paper presents DGNav, a dynamic framework for Vision-Language Navigation that adapts topological map density and connectivity to enhance navigation efficiency and safety in complex environments.",
      "teaser_image": "https://arxiv.org/html/2601.21751/x1.png",
      "debug_abstract": "Vision-Language Navigation in Continuous Environments (VLN-CE) presents a core challenge: grounding high-level linguistic instructions into precise, safe, and long-horizon spatial actions. Explicit topological maps have proven to be a vital solution for providing robust spatial memory in such tasks. However, existing topological planning methods suffer from a &#34;Granularity Rigidity&#34; problem. Specifically, these methods typically rely on fixed geometric thresholds to sample nodes, which fails to adapt to varying environmental complexities. This rigidity leads to a critical mismatch: the model tends to over-sample in simple areas, causing computational redundancy, while under-sampling in high-uncertainty regions, increasing collision risks and compromising precision. To address this, we propose DGNav, a framework for Dynamic Topological Navigation, introducing a context-aware mechanism to modulate map density and connectivity on-the-fly. Our approach comprises two core innovations: (1) A Scene-Aware Adaptive Strategy that dynamically modulates graph construction thresholds based on the dispersion of predicted waypoints, enabling &#34;densification on demand&#34; in challenging environments; (2) A Dynamic Graph Transformer that reconstructs graph connectivity by fusing visual, linguistic, and geometric cues into dynamic edge weights, enabling the agent to filter out topological noise and enhancing instruction adherence. Extensive experiments on the R2R-CE and RxR-CE benchmarks demonstrate DGNav exhibits superior navigation performance and strong generalization capabilities. Furthermore, ablation studies confirm that our framework achieves an optimal trade-off between navigation efficiency and safe exploration. The code is available at this https URL."
    },
    {
      "title": "PLANING: A Loosely Coupled Triangle-Gaussian Framework for Streaming 3D Reconstruction",
      "url": "https://arxiv.org/abs/2601.22046",
      "date": "2026-01-29",
      "authors_text": "Changjian Jiang, Kerui Ren, Xudong Li, Kaiwen Song, Linning Xu",
      "is_highlight": false,
      "score": 66.0,
      "summary": "PLANING is a novel streaming 3D reconstruction framework that efficiently combines geometric primitives and neural Gaussians, enhancing quality and speed while reducing redundancy.",
      "teaser_image": "https://arxiv.org/html/2601.22046/x3.png",
      "debug_abstract": "Streaming reconstruction from monocular image sequences remains challenging, as existing methods typically favor either high-quality rendering or accurate geometry, but rarely both. We present PLANING, an efficient on-the-fly reconstruction framework built on a hybrid representation that loosely couples explicit geometric primitives with neural Gaussians, enabling geometry and appearance to be modeled in a decoupled manner. This decoupling supports an online initialization and optimization strategy that separates geometry and appearance updates, yielding stable streaming reconstruction with substantially reduced structural redundancy. PLANING improves dense mesh Chamfer-L2 by 18.52% over PGSR, surpasses ARTDECO by 1.31 dB PSNR, and reconstructs ScanNetV2 scenes in under 100 seconds, over 5x faster than 2D Gaussian Splatting, while matching the quality of offline per-scene optimization. Beyond reconstruction quality, the structural clarity and computational efficiency of \\modelname~make it well suited for a broad range of downstream applications, such as enabling large-scale scene modeling and simulation-ready environments for embodied AI. Project page: this https URL ."
    },
    {
      "title": "Listen, Look, Drive: Coupling Audio Instructions for User-aware VLA-based Autonomous Driving",
      "url": "https://arxiv.org/abs/2601.12142",
      "date": "2026-01-28",
      "authors_text": "Ziang Guo, Feng Yang, Xuefeng Zhang, Jiaqi Guo, Kun Zhao",
      "is_highlight": false,
      "score": 88.0,
      "summary": "EchoVLA enhances autonomous driving by integrating audio instructions with visual inputs, allowing for user influence and emotional context, significantly improving navigation accuracy and safety.",
      "teaser_image": "https://arxiv.org/html/2601.12142/Figs/overview.png",
      "debug_abstract": "Vision Language Action (VLA) models promise an open-vocabulary interface that can translate perceptual ambiguity into semantically grounded driving decisions, yet they still treat language as a static prior fixed at inference time. As a result, the model must infer continuously shifting objectives from pixels alone, yielding delayed or overly conservative maneuvers. We argue that effective VLAs for autonomous driving need an online channel in which users can influence driving with specific intentions. To this end, we present EchoVLA, a user-aware VLA that couples camera streams with in situ audio instructions. We augment the nuScenes dataset with temporally aligned, intent-specific speech commands generated by converting ego-motion descriptions into synthetic audios. Further, we compose emotional speech-trajectory pairs into a multimodal Chain-of-Thought (CoT) for fine-tuning a Multimodal Large Model (MLM) based on Qwen2.5-Omni. Specifically, we synthesize the audio-augmented dataset with different emotion types paired with corresponding driving behaviors, leveraging the emotional cues embedded in tone, pitch, and speech tempo to reflect varying user states, such as urgent or hesitant intentions, thus enabling our EchoVLA to interpret not only the semantic content but also the emotional context of audio commands for more nuanced and emotionally adaptive driving behavior. In open-loop benchmarks, our approach reduces the average L2 error by $59.4\\%$ and the collision rate by $74.4\\%$ compared to the baseline of vision-only perception. More experiments on nuScenes dataset validate that EchoVLA not only steers the trajectory through audio instructions, but also modulates driving behavior in response to the emotions detected in the user&#39;s speech."
    },
    {
      "title": "Tactile-Force Alignment in Vision-Language-Action Models for Force-aware Manipulation",
      "url": "https://arxiv.org/abs/2601.20321",
      "date": "2026-01-28",
      "authors_text": "Yuzhe Huang, Pei Lin, Wanlin Li, Daohan Li, Jiajun Li",
      "is_highlight": false,
      "score": 91.0,
      "summary": "The paper introduces TaF-VLA, a framework that enhances Vision-Language-Action models by integrating tactile-force alignment for improved force-aware robotic manipulation in contact-rich tasks.",
      "teaser_image": "https://arxiv.org/html/2601.20321/figures/fig1-teaser.png",
      "debug_abstract": "Vision-Language-Action (VLA) models have recently emerged as powerful generalists for robotic manipulation. However, due to their predominant reliance on visual modalities, they fundamentally lack the physical intuition required for contact-rich tasks that require precise force regulation and physical reasoning. Existing attempts to incorporate vision-based tactile sensing into VLA models typically treat tactile inputs as auxiliary visual textures, thereby overlooking the underlying correlation between surface deformation and interaction dynamics. To bridge this gap, we propose a paradigm shift from tactile-vision alignment to tactile-force alignment. Here, we introduce TaF-VLA, a framework that explicitly grounds high-dimensional tactile observations in physical interaction forces. To facilitate this, we develop an automated tactile-force data acquisition device and curate the TaF-Dataset, comprising over 10 million synchronized tactile observations, 6-axis force/torque, and matrix force map. To align sequential tactile observations with interaction forces, the central component of our approach is the Tactile-Force Adapter (TaF-Adapter), a tactile sensor encoder that extracts discretized latent information for encoding tactile observations. This mechanism ensures that the learned representations capture history-dependent, noise-insensitive physical dynamics rather than static visual textures. Finally, we integrate this force-aligned encoder into a VLA backbone. Extensive real-world experiments demonstrate that TaF-VLA policy significantly outperforms state-of-the-art tactile-vision-aligned and vision-only baselines on contact-rich tasks, verifying its ability to achieve robust, force-aware manipulation through cross-modal physical reasoning."
    },
    {
      "title": "Rhombot: Rhombus-shaped Modular Robots for Stable, Medium-Independent Reconfiguration Motion",
      "url": "https://arxiv.org/abs/2601.19529",
      "date": "2026-01-27",
      "authors_text": "Jie Gu, Yirui Sun, Zhihao Xia, Tin Lun Lam, Chunxu Tian",
      "is_highlight": false,
      "score": 86.0,
      "summary": "Rhombot is a modular self-reconfigurable robot featuring rhombus-shaped modules that enable stable, medium-independent morphing and locomotion through a novel motion primitive called morphpivoting.",
      "debug_abstract": "In this paper, we present Rhombot, a novel deformable planar lattice modular self-reconfigurable robot (MSRR) with a rhombus shaped module. Each module consists of a parallelogram skeleton with a single centrally mounted actuator that enables folding and unfolding along its diagonal. The core design philosophy is to achieve essential MSRR functionalities such as morphing, docking, and locomotion with minimal control complexity. This enables a continuous and stable reconfiguration process that is independent of the surrounding medium, allowing the system to reliably form various configurations in diverse environments. To leverage the unique kinematics of Rhombot, we introduce morphpivoting, a novel motion primitive for reconfiguration that differs from advanced MSRR systems, and propose a strategy for its continuous execution. Finally, a series of physical experiments validate the module&#39;s stable reconfiguration ability, as well as its positional and docking accuracy."
    },
    {
      "title": "Component-Level Lesioning of Language Models Reveals Clinically Aligned Aphasia Phenotypes",
      "url": "https://arxiv.org/abs/2601.19723",
      "date": "2026-01-27",
      "authors_text": "Yifan Wang, Jichen Zheng, Jingyuan Sun, Yunhao Zhang, Chunyu Ye",
      "is_highlight": false,
      "score": 62.0,
      "summary": "This study presents a framework for simulating aphasia in language models by selectively perturbing components, revealing clinically relevant language impairments and enhancing understanding of language function degradation.",
      "debug_abstract": "Large language models (LLMs) increasingly exhibit human-like linguistic behaviors and internal representations that they could serve as computational simulators of language cognition. We ask whether LLMs can be systematically manipulated to reproduce language-production impairments characteristic of aphasia following focal brain lesions. Such models could provide scalable proxies for testing rehabilitation hypotheses, and offer a controlled framework for probing the functional organization of language. We introduce a clinically grounded, component-level framework that simulates aphasia by selectively perturbing functional components in LLMs, and apply it to both modular Mixture-of-Experts models and dense Transformers using a unified intervention interface. Our pipeline (i) identifies subtype-linked components for Broca&#39;s and Wernicke&#39;s aphasia, (ii) interprets these components with linguistic probing tasks, and (iii) induces graded impairments by progressively perturbing the top-k subtype-linked components, evaluating outcomes with Western Aphasia Battery (WAB) subtests summarized by Aphasia Quotient (AQ). Across architectures and lesioning strategies, subtype-targeted perturbations yield more systematic, aphasia-like regressions than size-matched random perturbations, and MoE modularity supports more localized and interpretable phenotype-to-component mappings. These findings suggest that modular LLMs, combined with clinically informed component perturbations, provide a promising platform for simulating aphasic language production and studying how distinct language functions degrade under targeted disruptions."
    },
    {
      "title": "EgoHandICL: Egocentric 3D Hand Reconstruction with In-Context Learning",
      "url": "https://arxiv.org/abs/2601.19850",
      "date": "2026-01-27",
      "authors_text": "Binzhu Xie, Shi Qiu, Sicheng Zhang, Yinqiao Wang, Hao Xu",
      "is_highlight": false,
      "score": 76.0,
      "summary": "EgoHandICL introduces an innovative in-context learning framework for robust 3D hand reconstruction in egocentric vision, enhancing performance through exemplar retrieval and multimodal context integration.",
      "debug_abstract": "Robust 3D hand reconstruction in egocentric vision is challenging due to depth ambiguity, self-occlusion, and complex hand-object interactions. Prior methods mitigate these issues by scaling training data or adding auxiliary cues, but they often struggle in unseen contexts. We present EgoHandICL, the first in-context learning (ICL) framework for 3D hand reconstruction that improves semantic alignment, visual consistency, and robustness under challenging egocentric conditions. EgoHandICL introduces complementary exemplar retrieval guided by vision-language models (VLMs), an ICL-tailored tokenizer for multimodal context, and a masked autoencoder (MAE)-based architecture trained with hand-guided geometric and perceptual objectives. Experiments on ARCTIC and EgoExo4D show consistent gains over state-of-the-art methods. We also demonstrate real-world generalization and improve EgoVLM hand-object interaction reasoning by using reconstructed hands as visual prompts. Code and data: this https URL"
    },
    {
      "title": "Advances and Innovations in the Multi-Agent Robotic System (MARS) Challenge",
      "url": "https://arxiv.org/abs/2601.18733",
      "date": "2026-01-26",
      "authors_text": "Li Kang, Heng Zhou, Xiufeng Song, Rui Li, Bruno N. Y. Chen",
      "is_highlight": false,
      "score": 94.0,
      "summary": "The MARS Challenge promotes advancements in multi-agent robotic systems by exploring vision-language models for collaborative task execution and planning in dynamic environments.",
      "debug_abstract": "Recent advancements in multimodal large language models and vision-languageaction models have significantly driven progress in Embodied AI. As the field transitions toward more complex task scenarios, multi-agent system frameworks are becoming essential for achieving scalable, efficient, and collaborative solutions. This shift is fueled by three primary factors: increasing agent capabilities, enhancing system efficiency through task delegation, and enabling advanced human-agent interactions. To address the challenges posed by multi-agent collaboration, we propose the Multi-Agent Robotic System (MARS) Challenge, held at the NeurIPS 2025 Workshop on SpaVLE. The competition focuses on two critical areas: planning and control, where participants explore multi-agent embodied planning using vision-language models (VLMs) to coordinate tasks and policy execution to perform robotic manipulation in dynamic environments. By evaluating solutions submitted by participants, the challenge provides valuable insights into the design and coordination of embodied multi-agent systems, contributing to the future development of advanced collaborative AI systems."
    },
    {
      "title": "Fast and Safe Trajectory Optimization for Mobile Manipulators With Neural Configuration Space Distance Field",
      "url": "https://arxiv.org/abs/2601.18548",
      "date": "2026-01-26",
      "authors_text": "Yulin Li, Zhiyuan Song, Yiming Li, Zhicheng Song, Kai Chen",
      "is_highlight": false,
      "score": 86.0,
      "summary": "This paper introduces Generalized Configuration Space Distance Fields (GCDF) for efficient trajectory optimization in mobile manipulators, enhancing collision reasoning and enabling agile motion in complex environments.",
      "debug_abstract": "Mobile manipulators promise agile, long-horizon behavior by coordinating base and arm motion, yet whole-body trajectory optimization in cluttered, confined spaces remains difficult due to high-dimensional nonconvexity and the need for fast, accurate collision reasoning. Configuration Space Distance Fields (CDF) enable fixed-base manipulators to model collisions directly in configuration space via smooth, implicit distances. This representation holds strong potential to bypass the nonlinear configuration-to-workspace mapping while preserving accurate whole-body geometry and providing optimization-friendly collision costs. Yet, extending this capability to mobile manipulators is hindered by unbounded workspaces and tighter base-arm coupling. We lift this promise to mobile manipulation with Generalized Configuration Space Distance Fields (GCDF), extending CDF to robots with both translational and rotational joints in unbounded workspaces with tighter base-arm coupling. We prove that GCDF preserves Euclidean-like local distance structure and accurately encodes whole-body geometry in configuration space, and develop a data generation and training pipeline that yields continuous neural GCDFs with accurate values and gradients, supporting efficient GPU-batched queries. Building on this representation, we develop a high-performance sequential convex optimization framework centered on GCDF-based collision reasoning. The solver scales to large numbers of implicit constraints through (i) online specification of neural constraints, (ii) sparsity-aware active-set detection with parallel batched evaluation across thousands of constraints, and (iii) incremental constraint management for rapid replanning under scene changes."
    },
    {
      "title": "Resisting Manipulative Bots in Meme Coin Copy Trading: A Multi-Agent Approach with Chain-of-Thought Reasoning",
      "url": "https://arxiv.org/abs/2601.08641",
      "date": "2026-01-26",
      "authors_text": "Yichen Luo, Yebo Feng, Jiahua Xu, Yang Liu",
      "is_highlight": false,
      "score": 67.0,
      "summary": "This paper presents a multi-agent system utilizing explainable LLMs to defend against manipulative bots in meme coin copy trading, enhancing prediction accuracy and economic performance.",
      "debug_abstract": "Copy trading has become the dominant entry strategy in meme coin markets. However, due to the market&#39;s extreme illiquid and volatile nature, the strategy exposes an exploitable attack surface: adversaries deploy manipulative bots to front-run trades, conceal positions, and fabricate sentiment, systematically extracting value from naïve copiers at scale. Despite its prevalence, bot-driven manipulation remains largely unexplored, and no robust defensive framework exists. We propose a manipulation-resistant copy-trading system based on a multi-agent architecture powered by a multi-modal, explainable large language model (LLM). Our system decomposes copy trading into three specialized agents for coin evaluation, wallet selection, and timing assessment. Evaluated on historical data from over 6,000 meme coins, our approach outperforms zero-shot and most statistic-driven baselines in prediction accuracy as well as all baselines in economic performance, achieving an average return of 14% for identified smart-money trades and an estimated copier return of 3% per trade under realistic market frictions. Overall, our results demonstrate the effectiveness of agent-based defenses and predictability of trader profitability in adversarial meme coin markets, providing a practical foundation for robust copy trading."
    },
    {
      "title": "AdaReasoner: Dynamic Tool Orchestration for Iterative Visual Reasoning",
      "url": "https://arxiv.org/abs/2601.18631",
      "date": "2026-01-26",
      "authors_text": "Mingyang Song, Haoyu Sun, Jiawei Gu, Linjie Li, Luxin Xu",
      "is_highlight": false,
      "score": 61.0,
      "summary": "AdaReasoner is a multimodal model that autonomously optimizes tool selection and usage for iterative visual reasoning, achieving state-of-the-art performance on various benchmarks.",
      "debug_abstract": "When humans face problems beyond their immediate capabilities, they rely on tools, providing a promising paradigm for improving visual reasoning in multimodal large language models (MLLMs). Effective reasoning, therefore, hinges on knowing which tools to use, when to invoke them, and how to compose them over multiple steps, even when faced with new tools or new tasks. We introduce \\textbf{AdaReasoner}, a family of multimodal models that learn tool use as a general reasoning skill rather than as tool-specific or explicitly supervised behavior. AdaReasoner is enabled by (i) a scalable data curation pipeline exposing models to long-horizon, multi-step tool interactions; (ii) Tool-GRPO, a reinforcement learning algorithm that optimizes tool selection and sequencing based on end-task success; and (iii) an adaptive learning mechanism that dynamically regulates tool usage. Together, these components allow models to infer tool utility from task context and intermediate outcomes, enabling coordination of multiple tools and generalization to unseen tools. Empirically, AdaReasoner exhibits strong tool-adaptive and generalization behaviors: it autonomously adopts beneficial tools, suppresses irrelevant ones, and adjusts tool usage frequency based on task demands, despite never being explicitly trained to do so. These capabilities translate into state-of-the-art performance across challenging benchmarks, improving the 7B base model by +24.9\\% on average and surpassing strong proprietary systems such as GPT-5 on multiple tasks, including VSP and Jigsaw."
    },
    {
      "title": "Timely Machine: Awareness of Time Makes Test-Time Scaling Agentic",
      "url": "https://arxiv.org/abs/2601.16486",
      "date": "2026-01-23",
      "authors_text": "Yichuan Ma, Linyang Li, Yongkang chen, Peiji Li, Xiaozhe Li",
      "is_highlight": false,
      "score": 78.0,
      "summary": "The paper introduces Timely Machine, redefining test-time scaling for LLMs based on wall-clock time, enhancing performance through dynamic strategy adjustments and reinforcement learning.",
      "debug_abstract": "As large language models (LLMs) increasingly tackle complex reasoning tasks, test-time scaling has become critical for enhancing capabilities. However, in agentic scenarios with frequent tool calls, the traditional generation-length-based definition breaks down: tool latency decouples inference time from generation length. We propose Timely Machine, redefining test-time as wall-clock time, where models dynamically adjust strategies based on time budgets. We introduce Timely-Eval, a benchmark spanning high-frequency tool calls, low-frequency tool calls, and time-constrained reasoning. By varying tool latency, we find smaller models excel with fast feedback through more interactions, while larger models dominate high-latency settings via superior interaction quality. Moreover, existing models fail to adapt reasoning to time budgets. We propose Timely-RL to address this gap. After cold-start supervised fine-tuning, we use reinforcement learning to enhance temporal planning. Timely-RL improves time budget awareness and consistently boosts performance across Timely-Eval. We hope our work offers a new perspective on test-time scaling for the agentic era."
    },
    {
      "title": "Adaptive Reinforcement and Model Predictive Control Switching for Safe Human-Robot Cooperative Navigation",
      "url": "https://arxiv.org/abs/2601.16686",
      "date": "2026-01-23",
      "authors_text": "Ning Liu, Sen Shen, Zheng Li, Matthew D'Souza, Jen Jen Chung",
      "is_highlight": false,
      "score": 94.0,
      "summary": "The paper presents ARMS, a hybrid control framework combining reinforcement learning and model predictive control for safe human-robot navigation, achieving superior performance in cluttered environments.",
      "debug_abstract": "This paper addresses the challenge of human-guided navigation for mobile collaborative robots under simultaneous proximity regulation and safety constraints. We introduce Adaptive Reinforcement and Model Predictive Control Switching (ARMS), a hybrid learning-control framework that integrates a reinforcement learning follower trained with Proximal Policy Optimization (PPO) and an analytical one-step Model Predictive Control (MPC) formulated as a quadratic program safety filter. To enable robust perception under partial observability and non-stationary human motion, ARMS employs a decoupled sensing architecture with a Long Short-Term Memory (LSTM) temporal encoder for the human-robot relative state and a spatial encoder for 360-degree LiDAR scans. The core contribution is a learned adaptive neural switcher that performs context-aware soft action fusion between the two controllers, favoring conservative, constraint-aware QP-based control in low-risk regions while progressively shifting control authority to the learned follower in highly cluttered or constrained scenarios where maneuverability is critical, and reverting to the follower action when the QP becomes infeasible. Extensive evaluations against Pure Pursuit, Dynamic Window Approach (DWA), and an RL-only baseline demonstrate that ARMS achieves an 82.5 percent success rate in highly cluttered environments, outperforming DWA and RL-only approaches by 7.1 percent and 3.1 percent, respectively, while reducing average computational latency by 33 percent to 5.2 milliseconds compared to a multi-step MPC baseline. Additional simulation transfer in Gazebo and initial real-world deployment results further indicate the practicality and robustness of ARMS for safe and efficient human-robot collaboration. Source code and a demonstration video are available at this https URL."
    },
    {
      "title": "Natural Language-Driven Global Mapping of Martian Landforms",
      "url": "https://arxiv.org/abs/2601.15949",
      "date": "2026-01-22",
      "authors_text": "Yiran Wang, Shuoyuan Wang, Zhaoran Wei, Jiannan Zhao, Zhonghua Yao",
      "is_highlight": false,
      "score": 72.0,
      "summary": "MarScope is a vision-language framework that enables natural language-driven, label-free mapping of Martian landforms, enhancing global geomorphic analysis and user query flexibility.",
      "debug_abstract": "Planetary surfaces are typically analyzed using high-level semantic concepts in natural language, yet vast orbital image archives remain organized at the pixel level. This mismatch limits scalable, open-ended exploration of planetary surfaces. Here we present MarScope, a planetary-scale vision-language framework enabling natural language-driven, label-free mapping of Martian landforms. MarScope aligns planetary images and text in a shared semantic space, trained on over 200,000 curated image-text pairs. This framework transforms global geomorphic mapping on Mars by replacing pre-defined classifications with flexible semantic retrieval, enabling arbitrary user queries across the entire planet in 5 seconds with F1 scores up to 0.978. Applications further show that it extends beyond morphological classification to facilitate process-oriented analysis and similarity-based geomorphological mapping at a planetary scale. MarScope establishes a new paradigm where natural language serves as a direct interface for scientific discovery over massive geospatial datasets."
    },
    {
      "title": "Robust Tool Use via Fission-GRPO: Learning to Recover from Execution Errors",
      "url": "https://arxiv.org/abs/2601.15625",
      "date": "2026-01-22",
      "authors_text": "Zhiwei Zhang, Fei Zhao, Rui Wang, Zezhong Wang, Bin Liang",
      "is_highlight": false,
      "score": 87.1,
      "summary": "Fission-GRPO enhances tool use in LLMs by converting execution errors into corrective supervision, improving error recovery and overall accuracy in multi-turn interactions.",
      "debug_abstract": "Large language models (LLMs) can call tools effectively, yet they remain brittle in multi-turn execution: following a tool call error, smaller models often degenerate into repetitive invalid re-invocations, failing to interpret error feedback and self-correct. This brittleness hinders reliable real-world deployment, where the execution errors are inherently inevitable during tool interaction procedures. We identify a key limitation of current approaches: standard reinforcement learning (RL) treats errors as sparse negative rewards, providing no guidance on how to recover, while pre-collected synthetic error-correction datasets suffer from distribution mismatch with the model&#39;s on-policy error modes. To bridge this gap, we propose Fission-GRPO, a framework that converts execution errors into corrective supervision within the RL training loop. Our core mechanism fissions each failed trajectory into a new training instance by augmenting it with diagnostic feedback from a finetuned Error Simulator, then resampling recovery rollouts on-policy. This enables the model to learn from the precise errors it makes during exploration, rather than from static, pre-collected error cases. On the BFCL v4 Multi-Turn, Fission-GRPO improves the error recovery rate of Qwen3-8B by 5.7% absolute, crucially, yielding a 4% overall accuracy gain (42.75% to 46.75%) over GRPO and outperforming specialized tool-use agents."
    },
    {
      "title": "PhysicsMind: Sim and Real Mechanics Benchmarking for Physical Reasoning and Prediction in Foundational VLMs and World Models",
      "url": "https://arxiv.org/abs/2601.16007",
      "date": "2026-01-22",
      "authors_text": "Chak-Wing Mak, Guanyu Zhu, Boyi Zhang, Hongji Li, Xiaowei Chi",
      "is_highlight": false,
      "score": 94.1,
      "summary": "PhysicsMind introduces a benchmark for evaluating physical reasoning in multimodal models through real and simulated tasks focused on fundamental physics principles, revealing significant gaps in current models' understanding.",
      "debug_abstract": "Modern foundational Multimodal Large Language Models (MLLMs) and video world models have advanced significantly in mathematical, common-sense, and visual reasoning, but their grasp of the underlying physics remains underexplored. Existing benchmarks attempting to measure this matter rely on synthetic, Visual Question Answer templates or focus on perceptual video quality that is tangential to measuring how well the video abides by physical laws. To address this fragmentation, we introduce PhysicsMind, a unified benchmark with both real and simulation environments that evaluates law-consistent reasoning and generation over three canonical principles: Center of Mass, Lever Equilibrium, and Newton&#39;s First Law. PhysicsMind comprises two main tasks: i) VQA tasks, testing whether models can reason and determine physical quantities and values from images or short videos, and ii) Video Generation(VG) tasks, evaluating if predicted motion trajectories obey the same center-of-mass, torque, and inertial constraints as the ground truth. A broad range of recent models and video generation models is evaluated on PhysicsMind and found to rely on appearance heuristics while often violating basic mechanics. These gaps indicate that current scaling and training are still insufficient for robust physical understanding, underscoring PhysicsMind as a focused testbed for physics-aware multimodal models. Our data will be released upon acceptance."
    }
  ],
  "深圳市人工智能与机器人研究院 (AIRS)": [
    {
      "title": "Search-R2: Enhancing Search-Integrated Reasoning via Actor-Refiner Collaboration",
      "url": "https://arxiv.org/abs/2602.03647",
      "date": "2026-02-03",
      "authors_text": "Bowei He, Minda Hu, Zenan Xu, Hongru Wang, Licheng Zong",
      "is_highlight": false,
      "score": 52.0,
      "summary": "Search-R2 introduces an Actor-Refiner framework that enhances search-integrated reasoning in language agents through targeted intervention and fine-grained supervision, outperforming existing methods.",
      "teaser_image": "https://arxiv.org/html/2602.03647/x2.png",
      "debug_abstract": "Search-integrated reasoning enables language agents to transcend static parametric knowledge by actively querying external sources. However, training these agents via reinforcement learning is hindered by the multi-scale credit assignment problem: existing methods typically rely on sparse, trajectory-level rewards that fail to distinguish between high-quality reasoning and fortuitous guesses, leading to redundant or misleading search behaviors. To address this, we propose Search-R2, a novel Actor-Refiner collaboration framework that enhances reasoning through targeted intervention, with both components jointly optimized during training. Our approach decomposes the generation process into an Actor, which produces initial reasoning trajectories, and a Meta-Refiner, which selectively diagnoses and repairs flawed steps via a &#39;cut-and-regenerate&#39; mechanism. To provide fine-grained supervision, we introduce a hybrid reward design that couples outcome correctness with a dense process reward quantifying the information density of retrieved evidence. Theoretically, we formalize the Actor-Refiner interaction as a smoothed mixture policy, proving that selective correction yields strict performance gains over strong baselines. Extensive experiments across various general and multi-hop QA datasets demonstrate that Search-R2 consistently outperforms strong RAG and RL-based baselines across model scales, achieving superior reasoning accuracy with minimal overhead."
    },
    {
      "title": "Dual Quaternion SE(3) Synchronization with Recovery Guarantees",
      "url": "https://arxiv.org/abs/2602.00324",
      "date": "2026-01-30",
      "authors_text": "Jianing Zhao, Linglingzhi Zhu, Anthony Man-Cho So",
      "is_highlight": false,
      "score": 75.0,
      "summary": "This paper presents a dual quaternion-based SE(3) synchronization method with a two-stage algorithm that guarantees recovery of absolute poses from noisy transformations, outperforming traditional matrix methods.",
      "teaser_image": "https://arxiv.org/html/2602.00324/x2.png",
      "debug_abstract": "Synchronization over the special Euclidean group SE(3) aims to recover absolute poses from noisy pairwise relative transformations and is a core primitive in robotics and 3D vision. Standard approaches often require multi-step heuristic procedures to recover valid poses, which are difficult to analyze and typically lack theoretical guarantees. This paper adopts a dual quaternion representation and formulates SE(3) synchronization directly over the unit dual quaternion. A two-stage algorithm is developed: A spectral initializer computed via the power method on a Hermitian dual quaternion measurement matrix, followed by a dual quaternion generalized power method (DQGPM) that enforces feasibility through per-iteration projection. The estimation error bounds are established for spectral estimators, and DQGPM is shown to admit a finite-iteration error bound and achieves linear error contraction up to an explicit noise-dependent threshold. Experiments on synthetic benchmarks and real-world multi-scan point-set registration demonstrate that the proposed pipeline improves both accuracy and efficiency over representative matrix-based methods."
    },
    {
      "title": "MedAD-R1: Eliciting Consistent Reasoning in Interpretible Medical Anomaly Detection via Consistency-Reinforced Policy Optimization",
      "url": "https://arxiv.org/abs/2602.01081",
      "date": "2026-02-01",
      "authors_text": "Haitao Zhang, Yingying Wang, Jiaxiang Wang, Haote Xu, Hongyang Zhang",
      "is_highlight": false,
      "score": 64.0,
      "summary": "MedAD-R1 enhances medical anomaly detection by utilizing a two-stage training framework with consistency-reinforced policy optimization, achieving state-of-the-art performance on the MedAD-38K benchmark.",
      "teaser_image": "https://arxiv.org/html/2602.01081/x3.png",
      "debug_abstract": "Medical Anomaly Detection (MedAD) presents a significant opportunity to enhance diagnostic accuracy using Large Multimodal Models (LMMs) to interpret and answer questions based on medical images. However, the reliance on Supervised Fine-Tuning (SFT) on simplistic and fragmented datasets has hindered the development of models capable of plausible reasoning and robust multimodal generalization. To overcome this, we introduce MedAD-38K, the first large-scale, multi-modal, and multi-center benchmark for MedAD featuring diagnostic Chain-of-Thought (CoT) annotations alongside structured Visual Question-Answering (VQA) pairs. On this foundation, we propose a two-stage training framework. The first stage, Cognitive Injection, uses SFT to instill foundational medical knowledge and align the model with a structured think-then-answer paradigm. Given that standard policy optimization can produce reasoning that is disconnected from the final answer, the second stage incorporates Consistency Group Relative Policy Optimization (Con-GRPO). This novel algorithm incorporates a crucial consistency reward to ensure the generated reasoning process is relevant and logically coherent with the final diagnosis. Our proposed model, MedAD-R1, achieves state-of-the-art (SOTA) performance on the MedAD-38K benchmark, outperforming strong baselines by more than 10\\%. This superior performance stems from its ability to generate transparent and logically consistent reasoning pathways, offering a promising approach to enhancing the trustworthiness and interpretability of AI for clinical decision support."
    },
    {
      "title": "Adaptive Rollout Allocation for Online Reinforcement Learning with Verifiable Rewards",
      "url": "https://arxiv.org/abs/2602.01601",
      "date": "2026-02-02",
      "authors_text": "Hieu Trung Nguyen, Bao Nguyen, Wenao Ma, Yuzhi Zhao, Ruifeng She",
      "is_highlight": false,
      "score": 40.0,
      "summary": "The paper presents \\Ours, a variance-informed strategy for adaptive rollout allocation in online reinforcement learning, enhancing sampling efficiency and performance over traditional methods.",
      "teaser_image": "https://arxiv.org/html/2602.01601/x2.png",
      "debug_abstract": "Sampling efficiency is a key bottleneck in reinforcement learning with verifiable rewards. Existing group-based policy optimization methods, such as GRPO, allocate a fixed number of rollouts for all training prompts. This uniform allocation implicitly treats all prompts as equally informative, and could lead to inefficient computational budget usage and impede training progress. We introduce \\Ours, a Variance-Informed Predictive allocation strategy that allocates a given rollout budget to the prompts in the incumbent batch to minimize the expected gradient variance of the policy update. At each iteration, \\Ours~uses a lightweight Gaussian process model to predict per-prompt success probabilities based on recent rollouts. These probability predictions are translated into variance estimates, which are then fed into a convex optimization problem to determine the optimal rollout allocations under a hard compute budget constraint. Empirical results show that \\Ours~consistently improves sampling efficiency and achieves higher performance than uniform or heuristic allocation strategies in multiple benchmarks. Our code will be available at this https URL."
    },
    {
      "title": "Reasoning with Autoregressive-Diffusion Collaborative Thoughts",
      "url": "https://arxiv.org/abs/2602.01608",
      "date": "2026-02-02",
      "authors_text": "Mu Yuan, Liekang Zeng, Guoliang Xing, Lan Zhang, Yunhao Liu",
      "is_highlight": false,
      "score": 41.0,
      "summary": "The paper presents Collaborative Thoughts, a framework integrating autoregressive and diffusion models for enhanced spatial reasoning and generation through iterative feedback and joint reasoning.",
      "teaser_image": "https://arxiv.org/html/2602.01608/x2.png",
      "debug_abstract": "Autoregressive and diffusion models represent two complementary generative paradigms. Autoregressive models excel at sequential planning and constraint composition, yet struggle with tasks that require explicit spatial or physical grounding. Diffusion models, in contrast, capture rich spatial structure through high-dimensional generation, but lack the stepwise logical control needed to satisfy complex, multi-stage constraints or to reliably identify and correct errors. We introduce Collaborative Thoughts, a unified collaborative framework that enables autoregressive and diffusion models to reason and generate jointly through a closed-loop interaction. In Collaborative Thoughts, autoregressive models perform structured planning and constraint management, diffusion models instantiate these constraints as intermediate visual thoughts, and a vision-based critic module evaluates whether the visual thoughts satisfy the intended structural and physical requirements. This feedback is then used to iteratively refine subsequent planning and generation steps, mitigating error propagation across modalities. Importantly, Collaborative Thoughts uses the same collaborative loop regardless of whether the task is autoregressive question answering or diffusion-based visual generation. Through representative examples, we illustrate how Collaborative Thoughts can improve the reliability of spatial reasoning and the controllability of generation."
    },
    {
      "title": "Tilt-Ropter: A Novel Hybrid Aerial and Terrestrial Vehicle with Tilt Rotors and Passive Wheels",
      "url": "https://arxiv.org/abs/2602.01700",
      "date": "2026-02-02",
      "authors_text": "Ruoyu Wang, Xuchen Liu, Zongzhou Wu, Zixuan Guo, Wendi Ding",
      "is_highlight": false,
      "score": 38.0,
      "summary": "The Tilt-Ropter is a hybrid aerial-terrestrial vehicle that utilizes tilt rotors and passive wheels for efficient locomotion, featuring advanced control systems for enhanced mobility and energy savings.",
      "teaser_image": "https://arxiv.org/html/2602.01700/x2.png",
      "debug_abstract": "In this work, we present Tilt-Ropter, a novel hybrid aerial-terrestrial vehicle (HATV) that combines tilt rotors with passive wheels to achieve energy-efficient multi-mode locomotion. Unlike existing under-actuated HATVs, the fully actuated design of Tilt-Ropter enables decoupled force and torque control, greatly enhancing its mobility and environmental adaptability. A nonlinear model predictive controller (NMPC) is developed to track reference trajectories and handle contact constraints across locomotion modes, while a dedicated control allocation module exploits actuation redundancy to achieve energy-efficient control of actuators. Additionally, to enhance robustness during ground contact, we introduce an external wrench estimation algorithm that estimates environmental interaction forces and torques in real time. The system is validated through both simulation and real-world experiments, including seamless air-ground transitions and trajectory tracking. Results show low tracking errors in both modes and highlight a 92.8% reduction in power consumption during ground locomotion, demonstrating the system&#39;s potential for long-duration missions across large-scale and energy-constrained environments."
    },
    {
      "title": "Breaking the Static Graph: Context-Aware Traversal for Robust Retrieval-Augmented Generation",
      "url": "https://arxiv.org/abs/2602.01965",
      "date": "2026-02-02",
      "authors_text": "Kwun Hang Lau, Fangyuan Zhang, Boyu Ruan, Yingli Zhou, Qintian Guo",
      "is_highlight": false,
      "score": 27.0,
      "summary": "CatRAG enhances Retrieval-Augmented Generation by transforming static Knowledge Graphs into query-adaptive structures, improving multi-hop evidence retrieval and reasoning completeness.",
      "teaser_image": "https://arxiv.org/html/2602.01965/image/HubBiasShift.png",
      "debug_abstract": "Recent advances in Retrieval-Augmented Generation (RAG) have shifted from simple vector similarity to structure-aware approaches like HippoRAG, which leverage Knowledge Graphs (KGs) and Personalized PageRank (PPR) to capture multi-hop dependencies. However, these methods suffer from a &#34;Static Graph Fallacy&#34;: they rely on fixed transition probabilities determined during indexing. This rigidity ignores the query-dependent nature of edge relevance, causing semantic drift where random walks are diverted into high-degree &#34;hub&#34; nodes before reaching critical downstream evidence. Consequently, models often achieve high partial recall but fail to retrieve the complete evidence chain required for multi-hop queries. To address this, we propose CatRAG, Context-Aware Traversal for robust RAG, a framework that builds on the HippoRAG 2 architecture and transforms the static KG into a query-adaptive navigation structure. We introduce a multi-faceted framework to steer the random walk: (1) Symbolic Anchoring, which injects weak entity constraints to regularize the random walk; (2) Query-Aware Dynamic Edge Weighting, which dynamically modulates graph structure, to prune irrelevant paths while amplifying those aligned with the query&#39;s intent; and (3) Key-Fact Passage Weight Enhancement, a cost-efficient bias that structurally anchors the random walk to likely evidence. Experiments across four multi-hop benchmarks demonstrate that CatRAG consistently outperforms state of the art baselines. Our analysis reveals that while standard Recall metrics show modest gains, CatRAG achieves substantial improvements in reasoning completeness, the capacity to recover the entire evidence path without gaps. These results reveal that our approach effectively bridges the gap between retrieving partial context and enabling fully grounded reasoning. Resources are available at this https URL."
    },
    {
      "title": "Towards Agentic Intelligence for Materials Science",
      "url": "https://arxiv.org/abs/2602.00169",
      "date": "2026-01-29",
      "authors_text": "Huan Zhang, Yizhan Li, Wenhao Huang, Ziyu Hou, Yu Song",
      "is_highlight": false,
      "score": 12.0,
      "summary": "This paper advocates for developing agentic AI systems in materials science that autonomously plan, act, and learn to enhance discovery processes beyond traditional models.",
      "teaser_image": "https://arxiv.org/html/2602.00169/x1.png",
      "debug_abstract": "The convergence of artificial intelligence and materials science presents a transformative opportunity, but achieving true acceleration in discovery requires moving beyond task-isolated, fine-tuned models toward agentic systems that plan, act, and learn across the full discovery loop. This survey advances a unique pipeline-centric view that spans from corpus curation and pretraining, through domain adaptation and instruction tuning, to goal-conditioned agents interfacing with simulation and experimental platforms. Unlike prior reviews, we treat the entire process as an end-to-end system to be optimized for tangible discovery outcomes rather than proxy benchmarks. This perspective allows us to trace how upstream design choices-such as data curation and training objectives-can be aligned with downstream experimental success through effective credit assignment. To bridge communities and establish a shared frame of reference, we first present an integrated lens that aligns terminology, evaluation, and workflow stages across AI and materials science. We then analyze the field through two focused lenses: From the AI perspective, the survey details LLM strengths in pattern recognition, predictive analytics, and natural language processing for literature mining, materials characterization, and property prediction; from the materials science perspective, it highlights applications in materials design, process optimization, and the acceleration of computational workflows via integration with external tools (e.g., DFT, robotic labs). Finally, we contrast passive, reactive approaches with agentic design, cataloging current contributions while motivating systems that pursue long-horizon goals with autonomy, memory, and tool use. This survey charts a practical roadmap towards autonomous, safety-aware LLM agents aimed at discovering novel and useful materials."
    },
    {
      "title": "FlowSteer: Interactive Agentic Workflow Orchestration via End-to-End Reinforcement Learning",
      "url": "https://arxiv.org/abs/2602.01664",
      "date": "2026-02-02",
      "authors_text": "Mingda Zhang, Haoran Luo, Tiesunlong Shen, Qika Lin, Xiaoying Tang",
      "is_highlight": false,
      "score": 7.0,
      "summary": "FlowSteer is an end-to-end reinforcement learning framework that automates workflow orchestration through interactive learning, addressing manual costs and improving performance across diverse tasks.",
      "teaser_image": "https://arxiv.org/html/2602.01664/x1.png",
      "debug_abstract": "In recent years, a variety of powerful agentic workflows have been applied to solve a wide range of human problems. However, existing workflow orchestration still faces key challenges, including high manual cost, reliance on specific operators/large language models (LLMs), and sparse reward signals. To address these challenges, we propose FlowSteer, an end-to-end reinforcement learning framework that takes a lightweight policy model as the agent and an executable canvas environment, automating workflow orchestration through multi-turn interaction. In this process, the policy model analyzes execution states and selects editing actions, while the canvas executes operators and returns feedback for iterative refinement. Moreover, FlowSteer provides a plug-and-play framework that supports diverse operator libraries and interchangeable LLM backends. To effectively train this interaction paradigm, we propose Canvas Workflow Relative Policy Optimization (CWRPO), which introduces diversity-constrained rewards with conditional release to stabilize learning and suppress shortcut behaviors. Experimental results on twelve datasets show that FlowSteer significantly outperforms baselines across various tasks."
    },
    {
      "title": "Heterogeneous Vertiport Selection Optimization for On-Demand Air Taxi Services: A Deep Reinforcement Learning Approach",
      "url": "https://arxiv.org/abs/2601.21316",
      "date": "2026-01-29",
      "authors_text": "Aoyu Pang, Maonan Wang, Zifan Sha, Wenwei Yue, Changle Li",
      "is_highlight": false,
      "score": 80.0,
      "summary": "The paper presents a deep reinforcement learning framework for optimizing vertiport selection and air taxi routing, achieving significant travel time reductions in urban air mobility systems.",
      "teaser_image": "https://arxiv.org/html/2601.21316/x2.png",
      "debug_abstract": "Urban Air Mobility (UAM) has emerged as a transformative solution to alleviate urban congestion by utilizing low-altitude airspace, thereby reducing pressure on ground transportation networks. To enable truly efficient and seamless door-to-door travel experiences, UAM requires close integration with existing ground transportation infrastructure. However, current research on optimal integrated routing strategies for passengers in air-ground mobility systems remains limited, with a lack of systematic this http URL address this gap, we first propose a unified optimization model that integrates strategy selection for both air and ground transportation. This model captures the dynamic characteristics of multimodal transport networks and incorporates real-time traffic conditions alongside passenger decision-making behavior. Building on this model, we propose a Unified Air-Ground Mobility Coordination (UAGMC) framework, which leverages deep reinforcement learning (RL) and Vehicle-to-Everything (V2X) communication to optimize vertiport selection and dynamically plan air taxi routes. Experimental results demonstrate that UAGMC achieves a 34\\% reduction in average travel time compared to conventional proportional allocation methods, enhancing overall travel efficiency and providing novel insights into the integration and optimization of multimodal transportation systems. This work lays a solid foundation for advancing intelligent urban mobility solutions through the coordination of air and ground transportation modes. The related code can be found at this https URL."
    },
    {
      "title": "Disentangling perception and reasoning for improving data efficiency in learning cloth manipulation without demonstrations",
      "url": "https://arxiv.org/abs/2601.21713",
      "date": "2026-01-29",
      "authors_text": "Donatien Delehelle, Fei Chen, Darwin Caldwell",
      "is_highlight": false,
      "score": 87.0,
      "summary": "This paper presents a modular reinforcement learning approach for cloth manipulation that enhances data efficiency and reduces model size and training time, achieving improved performance in simulation and real-world transfer.",
      "teaser_image": "https://arxiv.org/html/2601.21713/x1.png",
      "debug_abstract": "Cloth manipulation is a ubiquitous task in everyday life, but it remains an open challenge for robotics. The difficulties in developing cloth manipulation policies are attributed to the high-dimensional state space, complex dynamics, and high propensity to self-occlusion exhibited by fabrics. As analytical methods have not been able to provide robust and general manipulation policies, reinforcement learning (RL) is considered a promising approach to these problems. However, to address the large state space and complex dynamics, data-based methods usually rely on large models and long training times. The resulting computational cost significantly hampers the development and adoption of these methods. Additionally, due to the challenge of robust state estimation, garment manipulation policies often adopt an end-to-end learning approach with workspace images as input. While this approach enables a conceptually straightforward sim-to-real transfer via real-world fine-tuning, it also incurs a significant computational cost by training agents on a highly lossy representation of the environment state. This paper questions this common design choice by exploring an efficient and modular approach to RL for cloth manipulation. We show that, through careful design choices, model size and training time can be significantly reduced when learning in simulation. Furthermore, we demonstrate how the resulting simulation-trained model can be transferred to the real world. We evaluate our approach on the SoftGym benchmark and achieve significant performance improvements over available baselines on our task, while using a substantially smaller model."
    },
    {
      "title": "PLANING: A Loosely Coupled Triangle-Gaussian Framework for Streaming 3D Reconstruction",
      "url": "https://arxiv.org/abs/2601.22046",
      "date": "2026-01-29",
      "authors_text": "Changjian Jiang, Kerui Ren, Xudong Li, Kaiwen Song, Linning Xu",
      "is_highlight": false,
      "score": 66.0,
      "summary": "PLANING is a novel streaming 3D reconstruction framework that efficiently combines geometric primitives and neural Gaussians, enhancing quality and speed while reducing redundancy.",
      "teaser_image": "https://arxiv.org/html/2601.22046/x3.png",
      "debug_abstract": "Streaming reconstruction from monocular image sequences remains challenging, as existing methods typically favor either high-quality rendering or accurate geometry, but rarely both. We present PLANING, an efficient on-the-fly reconstruction framework built on a hybrid representation that loosely couples explicit geometric primitives with neural Gaussians, enabling geometry and appearance to be modeled in a decoupled manner. This decoupling supports an online initialization and optimization strategy that separates geometry and appearance updates, yielding stable streaming reconstruction with substantially reduced structural redundancy. PLANING improves dense mesh Chamfer-L2 by 18.52% over PGSR, surpasses ARTDECO by 1.31 dB PSNR, and reconstructs ScanNetV2 scenes in under 100 seconds, over 5x faster than 2D Gaussian Splatting, while matching the quality of offline per-scene optimization. Beyond reconstruction quality, the structural clarity and computational efficiency of \\modelname~make it well suited for a broad range of downstream applications, such as enabling large-scale scene modeling and simulation-ready environments for embodied AI. Project page: this https URL ."
    },
    {
      "title": "Rhombot: Rhombus-shaped Modular Robots for Stable, Medium-Independent Reconfiguration Motion",
      "url": "https://arxiv.org/abs/2601.19529",
      "date": "2026-01-27",
      "authors_text": "Jie Gu, Yirui Sun, Zhihao Xia, Tin Lun Lam, Chunxu Tian",
      "is_highlight": false,
      "score": 86.0,
      "summary": "Rhombot is a modular self-reconfigurable robot featuring rhombus-shaped modules that enable stable, medium-independent morphing and locomotion through a novel motion primitive called morphpivoting.",
      "debug_abstract": "In this paper, we present Rhombot, a novel deformable planar lattice modular self-reconfigurable robot (MSRR) with a rhombus shaped module. Each module consists of a parallelogram skeleton with a single centrally mounted actuator that enables folding and unfolding along its diagonal. The core design philosophy is to achieve essential MSRR functionalities such as morphing, docking, and locomotion with minimal control complexity. This enables a continuous and stable reconfiguration process that is independent of the surrounding medium, allowing the system to reliably form various configurations in diverse environments. To leverage the unique kinematics of Rhombot, we introduce morphpivoting, a novel motion primitive for reconfiguration that differs from advanced MSRR systems, and propose a strategy for its continuous execution. Finally, a series of physical experiments validate the module&#39;s stable reconfiguration ability, as well as its positional and docking accuracy."
    },
    {
      "title": "EgoHandICL: Egocentric 3D Hand Reconstruction with In-Context Learning",
      "url": "https://arxiv.org/abs/2601.19850",
      "date": "2026-01-27",
      "authors_text": "Binzhu Xie, Shi Qiu, Sicheng Zhang, Yinqiao Wang, Hao Xu",
      "is_highlight": false,
      "score": 76.0,
      "summary": "EgoHandICL introduces an innovative in-context learning framework for robust 3D hand reconstruction in egocentric vision, enhancing performance through exemplar retrieval and multimodal context integration.",
      "debug_abstract": "Robust 3D hand reconstruction in egocentric vision is challenging due to depth ambiguity, self-occlusion, and complex hand-object interactions. Prior methods mitigate these issues by scaling training data or adding auxiliary cues, but they often struggle in unseen contexts. We present EgoHandICL, the first in-context learning (ICL) framework for 3D hand reconstruction that improves semantic alignment, visual consistency, and robustness under challenging egocentric conditions. EgoHandICL introduces complementary exemplar retrieval guided by vision-language models (VLMs), an ICL-tailored tokenizer for multimodal context, and a masked autoencoder (MAE)-based architecture trained with hand-guided geometric and perceptual objectives. Experiments on ARCTIC and EgoExo4D show consistent gains over state-of-the-art methods. We also demonstrate real-world generalization and improve EgoVLM hand-object interaction reasoning by using reconstructed hands as visual prompts. Code and data: this https URL"
    },
    {
      "title": "AdaReasoner: Dynamic Tool Orchestration for Iterative Visual Reasoning",
      "url": "https://arxiv.org/abs/2601.18631",
      "date": "2026-01-26",
      "authors_text": "Mingyang Song, Haoyu Sun, Jiawei Gu, Linjie Li, Luxin Xu",
      "is_highlight": false,
      "score": 61.0,
      "summary": "AdaReasoner is a multimodal model that autonomously optimizes tool selection and usage for iterative visual reasoning, achieving state-of-the-art performance on various benchmarks.",
      "debug_abstract": "When humans face problems beyond their immediate capabilities, they rely on tools, providing a promising paradigm for improving visual reasoning in multimodal large language models (MLLMs). Effective reasoning, therefore, hinges on knowing which tools to use, when to invoke them, and how to compose them over multiple steps, even when faced with new tools or new tasks. We introduce \\textbf{AdaReasoner}, a family of multimodal models that learn tool use as a general reasoning skill rather than as tool-specific or explicitly supervised behavior. AdaReasoner is enabled by (i) a scalable data curation pipeline exposing models to long-horizon, multi-step tool interactions; (ii) Tool-GRPO, a reinforcement learning algorithm that optimizes tool selection and sequencing based on end-task success; and (iii) an adaptive learning mechanism that dynamically regulates tool usage. Together, these components allow models to infer tool utility from task context and intermediate outcomes, enabling coordination of multiple tools and generalization to unseen tools. Empirically, AdaReasoner exhibits strong tool-adaptive and generalization behaviors: it autonomously adopts beneficial tools, suppresses irrelevant ones, and adjusts tool usage frequency based on task demands, despite never being explicitly trained to do so. These capabilities translate into state-of-the-art performance across challenging benchmarks, improving the 7B base model by +24.9\\% on average and surpassing strong proprietary systems such as GPT-5 on multiple tasks, including VSP and Jigsaw."
    },
    {
      "title": "Timely Machine: Awareness of Time Makes Test-Time Scaling Agentic",
      "url": "https://arxiv.org/abs/2601.16486",
      "date": "2026-01-23",
      "authors_text": "Yichuan Ma, Linyang Li, Yongkang chen, Peiji Li, Xiaozhe Li",
      "is_highlight": false,
      "score": 78.0,
      "summary": "The paper introduces Timely Machine, redefining test-time scaling for LLMs based on wall-clock time, enhancing performance through dynamic strategy adjustments and reinforcement learning.",
      "debug_abstract": "As large language models (LLMs) increasingly tackle complex reasoning tasks, test-time scaling has become critical for enhancing capabilities. However, in agentic scenarios with frequent tool calls, the traditional generation-length-based definition breaks down: tool latency decouples inference time from generation length. We propose Timely Machine, redefining test-time as wall-clock time, where models dynamically adjust strategies based on time budgets. We introduce Timely-Eval, a benchmark spanning high-frequency tool calls, low-frequency tool calls, and time-constrained reasoning. By varying tool latency, we find smaller models excel with fast feedback through more interactions, while larger models dominate high-latency settings via superior interaction quality. Moreover, existing models fail to adapt reasoning to time budgets. We propose Timely-RL to address this gap. After cold-start supervised fine-tuning, we use reinforcement learning to enhance temporal planning. Timely-RL improves time budget awareness and consistently boosts performance across Timely-Eval. We hope our work offers a new perspective on test-time scaling for the agentic era."
    },
    {
      "title": "Adaptive Reinforcement and Model Predictive Control Switching for Safe Human-Robot Cooperative Navigation",
      "url": "https://arxiv.org/abs/2601.16686",
      "date": "2026-01-23",
      "authors_text": "Ning Liu, Sen Shen, Zheng Li, Matthew D'Souza, Jen Jen Chung",
      "is_highlight": false,
      "score": 94.0,
      "summary": "The paper presents ARMS, a hybrid control framework combining reinforcement learning and model predictive control for safe human-robot navigation, achieving superior performance in cluttered environments.",
      "debug_abstract": "This paper addresses the challenge of human-guided navigation for mobile collaborative robots under simultaneous proximity regulation and safety constraints. We introduce Adaptive Reinforcement and Model Predictive Control Switching (ARMS), a hybrid learning-control framework that integrates a reinforcement learning follower trained with Proximal Policy Optimization (PPO) and an analytical one-step Model Predictive Control (MPC) formulated as a quadratic program safety filter. To enable robust perception under partial observability and non-stationary human motion, ARMS employs a decoupled sensing architecture with a Long Short-Term Memory (LSTM) temporal encoder for the human-robot relative state and a spatial encoder for 360-degree LiDAR scans. The core contribution is a learned adaptive neural switcher that performs context-aware soft action fusion between the two controllers, favoring conservative, constraint-aware QP-based control in low-risk regions while progressively shifting control authority to the learned follower in highly cluttered or constrained scenarios where maneuverability is critical, and reverting to the follower action when the QP becomes infeasible. Extensive evaluations against Pure Pursuit, Dynamic Window Approach (DWA), and an RL-only baseline demonstrate that ARMS achieves an 82.5 percent success rate in highly cluttered environments, outperforming DWA and RL-only approaches by 7.1 percent and 3.1 percent, respectively, while reducing average computational latency by 33 percent to 5.2 milliseconds compared to a multi-step MPC baseline. Additional simulation transfer in Gazebo and initial real-world deployment results further indicate the practicality and robustness of ARMS for safe and efficient human-robot collaboration. Source code and a demonstration video are available at this https URL."
    },
    {
      "title": "Natural Language-Driven Global Mapping of Martian Landforms",
      "url": "https://arxiv.org/abs/2601.15949",
      "date": "2026-01-22",
      "authors_text": "Yiran Wang, Shuoyuan Wang, Zhaoran Wei, Jiannan Zhao, Zhonghua Yao",
      "is_highlight": false,
      "score": 72.0,
      "summary": "MarScope is a vision-language framework that enables natural language-driven, label-free mapping of Martian landforms, enhancing global geomorphic analysis and user query flexibility.",
      "debug_abstract": "Planetary surfaces are typically analyzed using high-level semantic concepts in natural language, yet vast orbital image archives remain organized at the pixel level. This mismatch limits scalable, open-ended exploration of planetary surfaces. Here we present MarScope, a planetary-scale vision-language framework enabling natural language-driven, label-free mapping of Martian landforms. MarScope aligns planetary images and text in a shared semantic space, trained on over 200,000 curated image-text pairs. This framework transforms global geomorphic mapping on Mars by replacing pre-defined classifications with flexible semantic retrieval, enabling arbitrary user queries across the entire planet in 5 seconds with F1 scores up to 0.978. Applications further show that it extends beyond morphological classification to facilitate process-oriented analysis and similarity-based geomorphological mapping at a planetary scale. MarScope establishes a new paradigm where natural language serves as a direct interface for scientific discovery over massive geospatial datasets."
    },
    {
      "title": "Robust Tool Use via Fission-GRPO: Learning to Recover from Execution Errors",
      "url": "https://arxiv.org/abs/2601.15625",
      "date": "2026-01-22",
      "authors_text": "Zhiwei Zhang, Fei Zhao, Rui Wang, Zezhong Wang, Bin Liang",
      "is_highlight": false,
      "score": 87.1,
      "summary": "Fission-GRPO enhances tool use in LLMs by converting execution errors into corrective supervision, improving error recovery and overall accuracy in multi-turn interactions.",
      "debug_abstract": "Large language models (LLMs) can call tools effectively, yet they remain brittle in multi-turn execution: following a tool call error, smaller models often degenerate into repetitive invalid re-invocations, failing to interpret error feedback and self-correct. This brittleness hinders reliable real-world deployment, where the execution errors are inherently inevitable during tool interaction procedures. We identify a key limitation of current approaches: standard reinforcement learning (RL) treats errors as sparse negative rewards, providing no guidance on how to recover, while pre-collected synthetic error-correction datasets suffer from distribution mismatch with the model&#39;s on-policy error modes. To bridge this gap, we propose Fission-GRPO, a framework that converts execution errors into corrective supervision within the RL training loop. Our core mechanism fissions each failed trajectory into a new training instance by augmenting it with diagnostic feedback from a finetuned Error Simulator, then resampling recovery rollouts on-policy. This enables the model to learn from the precise errors it makes during exploration, rather than from static, pre-collected error cases. On the BFCL v4 Multi-Turn, Fission-GRPO improves the error recovery rate of Qwen3-8B by 5.7% absolute, crucially, yielding a 4% overall accuracy gain (42.75% to 46.75%) over GRPO and outperforming specialized tool-use agents."
    }
  ],
  "Multimedia Lab (MMLab)": [
    {
      "title": "Search-R2: Enhancing Search-Integrated Reasoning via Actor-Refiner Collaboration",
      "url": "https://arxiv.org/abs/2602.03647",
      "date": "2026-02-03",
      "authors_text": "Bowei He, Minda Hu, Zenan Xu, Hongru Wang, Licheng Zong",
      "is_highlight": false,
      "score": 52.0,
      "summary": "Search-R2 introduces an Actor-Refiner framework that enhances search-integrated reasoning in language agents through targeted intervention and fine-grained supervision, outperforming existing methods.",
      "teaser_image": "https://arxiv.org/html/2602.03647/x2.png",
      "debug_abstract": "Search-integrated reasoning enables language agents to transcend static parametric knowledge by actively querying external sources. However, training these agents via reinforcement learning is hindered by the multi-scale credit assignment problem: existing methods typically rely on sparse, trajectory-level rewards that fail to distinguish between high-quality reasoning and fortuitous guesses, leading to redundant or misleading search behaviors. To address this, we propose Search-R2, a novel Actor-Refiner collaboration framework that enhances reasoning through targeted intervention, with both components jointly optimized during training. Our approach decomposes the generation process into an Actor, which produces initial reasoning trajectories, and a Meta-Refiner, which selectively diagnoses and repairs flawed steps via a &#39;cut-and-regenerate&#39; mechanism. To provide fine-grained supervision, we introduce a hybrid reward design that couples outcome correctness with a dense process reward quantifying the information density of retrieved evidence. Theoretically, we formalize the Actor-Refiner interaction as a smoothed mixture policy, proving that selective correction yields strict performance gains over strong baselines. Extensive experiments across various general and multi-hop QA datasets demonstrate that Search-R2 consistently outperforms strong RAG and RL-based baselines across model scales, achieving superior reasoning accuracy with minimal overhead."
    },
    {
      "title": "Dual Quaternion SE(3) Synchronization with Recovery Guarantees",
      "url": "https://arxiv.org/abs/2602.00324",
      "date": "2026-01-30",
      "authors_text": "Jianing Zhao, Linglingzhi Zhu, Anthony Man-Cho So",
      "is_highlight": false,
      "score": 75.0,
      "summary": "This paper presents a dual quaternion-based SE(3) synchronization method with a two-stage algorithm that guarantees recovery of absolute poses from noisy transformations, outperforming traditional matrix methods.",
      "teaser_image": "https://arxiv.org/html/2602.00324/x2.png",
      "debug_abstract": "Synchronization over the special Euclidean group SE(3) aims to recover absolute poses from noisy pairwise relative transformations and is a core primitive in robotics and 3D vision. Standard approaches often require multi-step heuristic procedures to recover valid poses, which are difficult to analyze and typically lack theoretical guarantees. This paper adopts a dual quaternion representation and formulates SE(3) synchronization directly over the unit dual quaternion. A two-stage algorithm is developed: A spectral initializer computed via the power method on a Hermitian dual quaternion measurement matrix, followed by a dual quaternion generalized power method (DQGPM) that enforces feasibility through per-iteration projection. The estimation error bounds are established for spectral estimators, and DQGPM is shown to admit a finite-iteration error bound and achieves linear error contraction up to an explicit noise-dependent threshold. Experiments on synthetic benchmarks and real-world multi-scan point-set registration demonstrate that the proposed pipeline improves both accuracy and efficiency over representative matrix-based methods."
    },
    {
      "title": "MedAD-R1: Eliciting Consistent Reasoning in Interpretible Medical Anomaly Detection via Consistency-Reinforced Policy Optimization",
      "url": "https://arxiv.org/abs/2602.01081",
      "date": "2026-02-01",
      "authors_text": "Haitao Zhang, Yingying Wang, Jiaxiang Wang, Haote Xu, Hongyang Zhang",
      "is_highlight": false,
      "score": 64.0,
      "summary": "MedAD-R1 enhances medical anomaly detection by utilizing a two-stage training framework with consistency-reinforced policy optimization, achieving state-of-the-art performance on the MedAD-38K benchmark.",
      "teaser_image": "https://arxiv.org/html/2602.01081/x3.png",
      "debug_abstract": "Medical Anomaly Detection (MedAD) presents a significant opportunity to enhance diagnostic accuracy using Large Multimodal Models (LMMs) to interpret and answer questions based on medical images. However, the reliance on Supervised Fine-Tuning (SFT) on simplistic and fragmented datasets has hindered the development of models capable of plausible reasoning and robust multimodal generalization. To overcome this, we introduce MedAD-38K, the first large-scale, multi-modal, and multi-center benchmark for MedAD featuring diagnostic Chain-of-Thought (CoT) annotations alongside structured Visual Question-Answering (VQA) pairs. On this foundation, we propose a two-stage training framework. The first stage, Cognitive Injection, uses SFT to instill foundational medical knowledge and align the model with a structured think-then-answer paradigm. Given that standard policy optimization can produce reasoning that is disconnected from the final answer, the second stage incorporates Consistency Group Relative Policy Optimization (Con-GRPO). This novel algorithm incorporates a crucial consistency reward to ensure the generated reasoning process is relevant and logically coherent with the final diagnosis. Our proposed model, MedAD-R1, achieves state-of-the-art (SOTA) performance on the MedAD-38K benchmark, outperforming strong baselines by more than 10\\%. This superior performance stems from its ability to generate transparent and logically consistent reasoning pathways, offering a promising approach to enhancing the trustworthiness and interpretability of AI for clinical decision support."
    },
    {
      "title": "Adaptive Rollout Allocation for Online Reinforcement Learning with Verifiable Rewards",
      "url": "https://arxiv.org/abs/2602.01601",
      "date": "2026-02-02",
      "authors_text": "Hieu Trung Nguyen, Bao Nguyen, Wenao Ma, Yuzhi Zhao, Ruifeng She",
      "is_highlight": false,
      "score": 40.0,
      "summary": "The paper presents \\Ours, a variance-informed strategy for adaptive rollout allocation in online reinforcement learning, enhancing sampling efficiency and performance over traditional methods.",
      "teaser_image": "https://arxiv.org/html/2602.01601/x2.png",
      "debug_abstract": "Sampling efficiency is a key bottleneck in reinforcement learning with verifiable rewards. Existing group-based policy optimization methods, such as GRPO, allocate a fixed number of rollouts for all training prompts. This uniform allocation implicitly treats all prompts as equally informative, and could lead to inefficient computational budget usage and impede training progress. We introduce \\Ours, a Variance-Informed Predictive allocation strategy that allocates a given rollout budget to the prompts in the incumbent batch to minimize the expected gradient variance of the policy update. At each iteration, \\Ours~uses a lightweight Gaussian process model to predict per-prompt success probabilities based on recent rollouts. These probability predictions are translated into variance estimates, which are then fed into a convex optimization problem to determine the optimal rollout allocations under a hard compute budget constraint. Empirical results show that \\Ours~consistently improves sampling efficiency and achieves higher performance than uniform or heuristic allocation strategies in multiple benchmarks. Our code will be available at this https URL."
    },
    {
      "title": "Reasoning with Autoregressive-Diffusion Collaborative Thoughts",
      "url": "https://arxiv.org/abs/2602.01608",
      "date": "2026-02-02",
      "authors_text": "Mu Yuan, Liekang Zeng, Guoliang Xing, Lan Zhang, Yunhao Liu",
      "is_highlight": false,
      "score": 41.0,
      "summary": "The paper presents Collaborative Thoughts, a framework integrating autoregressive and diffusion models for enhanced spatial reasoning and generation through iterative feedback and joint reasoning.",
      "teaser_image": "https://arxiv.org/html/2602.01608/x2.png",
      "debug_abstract": "Autoregressive and diffusion models represent two complementary generative paradigms. Autoregressive models excel at sequential planning and constraint composition, yet struggle with tasks that require explicit spatial or physical grounding. Diffusion models, in contrast, capture rich spatial structure through high-dimensional generation, but lack the stepwise logical control needed to satisfy complex, multi-stage constraints or to reliably identify and correct errors. We introduce Collaborative Thoughts, a unified collaborative framework that enables autoregressive and diffusion models to reason and generate jointly through a closed-loop interaction. In Collaborative Thoughts, autoregressive models perform structured planning and constraint management, diffusion models instantiate these constraints as intermediate visual thoughts, and a vision-based critic module evaluates whether the visual thoughts satisfy the intended structural and physical requirements. This feedback is then used to iteratively refine subsequent planning and generation steps, mitigating error propagation across modalities. Importantly, Collaborative Thoughts uses the same collaborative loop regardless of whether the task is autoregressive question answering or diffusion-based visual generation. Through representative examples, we illustrate how Collaborative Thoughts can improve the reliability of spatial reasoning and the controllability of generation."
    },
    {
      "title": "Tilt-Ropter: A Novel Hybrid Aerial and Terrestrial Vehicle with Tilt Rotors and Passive Wheels",
      "url": "https://arxiv.org/abs/2602.01700",
      "date": "2026-02-02",
      "authors_text": "Ruoyu Wang, Xuchen Liu, Zongzhou Wu, Zixuan Guo, Wendi Ding",
      "is_highlight": false,
      "score": 38.0,
      "summary": "The Tilt-Ropter is a hybrid aerial-terrestrial vehicle that utilizes tilt rotors and passive wheels for efficient locomotion, featuring advanced control systems for enhanced mobility and energy savings.",
      "teaser_image": "https://arxiv.org/html/2602.01700/x2.png",
      "debug_abstract": "In this work, we present Tilt-Ropter, a novel hybrid aerial-terrestrial vehicle (HATV) that combines tilt rotors with passive wheels to achieve energy-efficient multi-mode locomotion. Unlike existing under-actuated HATVs, the fully actuated design of Tilt-Ropter enables decoupled force and torque control, greatly enhancing its mobility and environmental adaptability. A nonlinear model predictive controller (NMPC) is developed to track reference trajectories and handle contact constraints across locomotion modes, while a dedicated control allocation module exploits actuation redundancy to achieve energy-efficient control of actuators. Additionally, to enhance robustness during ground contact, we introduce an external wrench estimation algorithm that estimates environmental interaction forces and torques in real time. The system is validated through both simulation and real-world experiments, including seamless air-ground transitions and trajectory tracking. Results show low tracking errors in both modes and highlight a 92.8% reduction in power consumption during ground locomotion, demonstrating the system&#39;s potential for long-duration missions across large-scale and energy-constrained environments."
    },
    {
      "title": "Breaking the Static Graph: Context-Aware Traversal for Robust Retrieval-Augmented Generation",
      "url": "https://arxiv.org/abs/2602.01965",
      "date": "2026-02-02",
      "authors_text": "Kwun Hang Lau, Fangyuan Zhang, Boyu Ruan, Yingli Zhou, Qintian Guo",
      "is_highlight": false,
      "score": 27.0,
      "summary": "CatRAG enhances Retrieval-Augmented Generation by transforming static Knowledge Graphs into query-adaptive structures, improving multi-hop evidence retrieval and reasoning completeness.",
      "teaser_image": "https://arxiv.org/html/2602.01965/image/HubBiasShift.png",
      "debug_abstract": "Recent advances in Retrieval-Augmented Generation (RAG) have shifted from simple vector similarity to structure-aware approaches like HippoRAG, which leverage Knowledge Graphs (KGs) and Personalized PageRank (PPR) to capture multi-hop dependencies. However, these methods suffer from a &#34;Static Graph Fallacy&#34;: they rely on fixed transition probabilities determined during indexing. This rigidity ignores the query-dependent nature of edge relevance, causing semantic drift where random walks are diverted into high-degree &#34;hub&#34; nodes before reaching critical downstream evidence. Consequently, models often achieve high partial recall but fail to retrieve the complete evidence chain required for multi-hop queries. To address this, we propose CatRAG, Context-Aware Traversal for robust RAG, a framework that builds on the HippoRAG 2 architecture and transforms the static KG into a query-adaptive navigation structure. We introduce a multi-faceted framework to steer the random walk: (1) Symbolic Anchoring, which injects weak entity constraints to regularize the random walk; (2) Query-Aware Dynamic Edge Weighting, which dynamically modulates graph structure, to prune irrelevant paths while amplifying those aligned with the query&#39;s intent; and (3) Key-Fact Passage Weight Enhancement, a cost-efficient bias that structurally anchors the random walk to likely evidence. Experiments across four multi-hop benchmarks demonstrate that CatRAG consistently outperforms state of the art baselines. Our analysis reveals that while standard Recall metrics show modest gains, CatRAG achieves substantial improvements in reasoning completeness, the capacity to recover the entire evidence path without gaps. These results reveal that our approach effectively bridges the gap between retrieving partial context and enabling fully grounded reasoning. Resources are available at this https URL."
    },
    {
      "title": "Towards Agentic Intelligence for Materials Science",
      "url": "https://arxiv.org/abs/2602.00169",
      "date": "2026-01-29",
      "authors_text": "Huan Zhang, Yizhan Li, Wenhao Huang, Ziyu Hou, Yu Song",
      "is_highlight": false,
      "score": 12.0,
      "summary": "This paper advocates for developing agentic AI systems in materials science that autonomously plan, act, and learn to enhance discovery processes beyond traditional models.",
      "teaser_image": "https://arxiv.org/html/2602.00169/x1.png",
      "debug_abstract": "The convergence of artificial intelligence and materials science presents a transformative opportunity, but achieving true acceleration in discovery requires moving beyond task-isolated, fine-tuned models toward agentic systems that plan, act, and learn across the full discovery loop. This survey advances a unique pipeline-centric view that spans from corpus curation and pretraining, through domain adaptation and instruction tuning, to goal-conditioned agents interfacing with simulation and experimental platforms. Unlike prior reviews, we treat the entire process as an end-to-end system to be optimized for tangible discovery outcomes rather than proxy benchmarks. This perspective allows us to trace how upstream design choices-such as data curation and training objectives-can be aligned with downstream experimental success through effective credit assignment. To bridge communities and establish a shared frame of reference, we first present an integrated lens that aligns terminology, evaluation, and workflow stages across AI and materials science. We then analyze the field through two focused lenses: From the AI perspective, the survey details LLM strengths in pattern recognition, predictive analytics, and natural language processing for literature mining, materials characterization, and property prediction; from the materials science perspective, it highlights applications in materials design, process optimization, and the acceleration of computational workflows via integration with external tools (e.g., DFT, robotic labs). Finally, we contrast passive, reactive approaches with agentic design, cataloging current contributions while motivating systems that pursue long-horizon goals with autonomy, memory, and tool use. This survey charts a practical roadmap towards autonomous, safety-aware LLM agents aimed at discovering novel and useful materials."
    },
    {
      "title": "FlowSteer: Interactive Agentic Workflow Orchestration via End-to-End Reinforcement Learning",
      "url": "https://arxiv.org/abs/2602.01664",
      "date": "2026-02-02",
      "authors_text": "Mingda Zhang, Haoran Luo, Tiesunlong Shen, Qika Lin, Xiaoying Tang",
      "is_highlight": false,
      "score": 7.0,
      "summary": "FlowSteer is an end-to-end reinforcement learning framework that automates workflow orchestration through interactive learning, addressing manual costs and improving performance across diverse tasks.",
      "teaser_image": "https://arxiv.org/html/2602.01664/x1.png",
      "debug_abstract": "In recent years, a variety of powerful agentic workflows have been applied to solve a wide range of human problems. However, existing workflow orchestration still faces key challenges, including high manual cost, reliance on specific operators/large language models (LLMs), and sparse reward signals. To address these challenges, we propose FlowSteer, an end-to-end reinforcement learning framework that takes a lightweight policy model as the agent and an executable canvas environment, automating workflow orchestration through multi-turn interaction. In this process, the policy model analyzes execution states and selects editing actions, while the canvas executes operators and returns feedback for iterative refinement. Moreover, FlowSteer provides a plug-and-play framework that supports diverse operator libraries and interchangeable LLM backends. To effectively train this interaction paradigm, we propose Canvas Workflow Relative Policy Optimization (CWRPO), which introduces diversity-constrained rewards with conditional release to stabilize learning and suppress shortcut behaviors. Experimental results on twelve datasets show that FlowSteer significantly outperforms baselines across various tasks."
    },
    {
      "title": "Heterogeneous Vertiport Selection Optimization for On-Demand Air Taxi Services: A Deep Reinforcement Learning Approach",
      "url": "https://arxiv.org/abs/2601.21316",
      "date": "2026-01-29",
      "authors_text": "Aoyu Pang, Maonan Wang, Zifan Sha, Wenwei Yue, Changle Li",
      "is_highlight": false,
      "score": 80.0,
      "summary": "The paper presents a deep reinforcement learning framework for optimizing vertiport selection and air taxi routing, achieving significant travel time reductions in urban air mobility systems.",
      "teaser_image": "https://arxiv.org/html/2601.21316/x2.png",
      "debug_abstract": "Urban Air Mobility (UAM) has emerged as a transformative solution to alleviate urban congestion by utilizing low-altitude airspace, thereby reducing pressure on ground transportation networks. To enable truly efficient and seamless door-to-door travel experiences, UAM requires close integration with existing ground transportation infrastructure. However, current research on optimal integrated routing strategies for passengers in air-ground mobility systems remains limited, with a lack of systematic this http URL address this gap, we first propose a unified optimization model that integrates strategy selection for both air and ground transportation. This model captures the dynamic characteristics of multimodal transport networks and incorporates real-time traffic conditions alongside passenger decision-making behavior. Building on this model, we propose a Unified Air-Ground Mobility Coordination (UAGMC) framework, which leverages deep reinforcement learning (RL) and Vehicle-to-Everything (V2X) communication to optimize vertiport selection and dynamically plan air taxi routes. Experimental results demonstrate that UAGMC achieves a 34\\% reduction in average travel time compared to conventional proportional allocation methods, enhancing overall travel efficiency and providing novel insights into the integration and optimization of multimodal transportation systems. This work lays a solid foundation for advancing intelligent urban mobility solutions through the coordination of air and ground transportation modes. The related code can be found at this https URL."
    },
    {
      "title": "Disentangling perception and reasoning for improving data efficiency in learning cloth manipulation without demonstrations",
      "url": "https://arxiv.org/abs/2601.21713",
      "date": "2026-01-29",
      "authors_text": "Donatien Delehelle, Fei Chen, Darwin Caldwell",
      "is_highlight": false,
      "score": 87.0,
      "summary": "This paper presents a modular reinforcement learning approach for cloth manipulation that enhances data efficiency and reduces model size and training time, achieving improved performance in simulation and real-world transfer.",
      "teaser_image": "https://arxiv.org/html/2601.21713/x1.png",
      "debug_abstract": "Cloth manipulation is a ubiquitous task in everyday life, but it remains an open challenge for robotics. The difficulties in developing cloth manipulation policies are attributed to the high-dimensional state space, complex dynamics, and high propensity to self-occlusion exhibited by fabrics. As analytical methods have not been able to provide robust and general manipulation policies, reinforcement learning (RL) is considered a promising approach to these problems. However, to address the large state space and complex dynamics, data-based methods usually rely on large models and long training times. The resulting computational cost significantly hampers the development and adoption of these methods. Additionally, due to the challenge of robust state estimation, garment manipulation policies often adopt an end-to-end learning approach with workspace images as input. While this approach enables a conceptually straightforward sim-to-real transfer via real-world fine-tuning, it also incurs a significant computational cost by training agents on a highly lossy representation of the environment state. This paper questions this common design choice by exploring an efficient and modular approach to RL for cloth manipulation. We show that, through careful design choices, model size and training time can be significantly reduced when learning in simulation. Furthermore, we demonstrate how the resulting simulation-trained model can be transferred to the real world. We evaluate our approach on the SoftGym benchmark and achieve significant performance improvements over available baselines on our task, while using a substantially smaller model."
    },
    {
      "title": "PLANING: A Loosely Coupled Triangle-Gaussian Framework for Streaming 3D Reconstruction",
      "url": "https://arxiv.org/abs/2601.22046",
      "date": "2026-01-29",
      "authors_text": "Changjian Jiang, Kerui Ren, Xudong Li, Kaiwen Song, Linning Xu",
      "is_highlight": false,
      "score": 66.0,
      "summary": "PLANING is a novel streaming 3D reconstruction framework that efficiently combines geometric primitives and neural Gaussians, enhancing quality and speed while reducing redundancy.",
      "teaser_image": "https://arxiv.org/html/2601.22046/x3.png",
      "debug_abstract": "Streaming reconstruction from monocular image sequences remains challenging, as existing methods typically favor either high-quality rendering or accurate geometry, but rarely both. We present PLANING, an efficient on-the-fly reconstruction framework built on a hybrid representation that loosely couples explicit geometric primitives with neural Gaussians, enabling geometry and appearance to be modeled in a decoupled manner. This decoupling supports an online initialization and optimization strategy that separates geometry and appearance updates, yielding stable streaming reconstruction with substantially reduced structural redundancy. PLANING improves dense mesh Chamfer-L2 by 18.52% over PGSR, surpasses ARTDECO by 1.31 dB PSNR, and reconstructs ScanNetV2 scenes in under 100 seconds, over 5x faster than 2D Gaussian Splatting, while matching the quality of offline per-scene optimization. Beyond reconstruction quality, the structural clarity and computational efficiency of \\modelname~make it well suited for a broad range of downstream applications, such as enabling large-scale scene modeling and simulation-ready environments for embodied AI. Project page: this https URL ."
    },
    {
      "title": "Rhombot: Rhombus-shaped Modular Robots for Stable, Medium-Independent Reconfiguration Motion",
      "url": "https://arxiv.org/abs/2601.19529",
      "date": "2026-01-27",
      "authors_text": "Jie Gu, Yirui Sun, Zhihao Xia, Tin Lun Lam, Chunxu Tian",
      "is_highlight": false,
      "score": 86.0,
      "summary": "Rhombot is a modular self-reconfigurable robot featuring rhombus-shaped modules that enable stable, medium-independent morphing and locomotion through a novel motion primitive called morphpivoting.",
      "debug_abstract": "In this paper, we present Rhombot, a novel deformable planar lattice modular self-reconfigurable robot (MSRR) with a rhombus shaped module. Each module consists of a parallelogram skeleton with a single centrally mounted actuator that enables folding and unfolding along its diagonal. The core design philosophy is to achieve essential MSRR functionalities such as morphing, docking, and locomotion with minimal control complexity. This enables a continuous and stable reconfiguration process that is independent of the surrounding medium, allowing the system to reliably form various configurations in diverse environments. To leverage the unique kinematics of Rhombot, we introduce morphpivoting, a novel motion primitive for reconfiguration that differs from advanced MSRR systems, and propose a strategy for its continuous execution. Finally, a series of physical experiments validate the module&#39;s stable reconfiguration ability, as well as its positional and docking accuracy."
    },
    {
      "title": "EgoHandICL: Egocentric 3D Hand Reconstruction with In-Context Learning",
      "url": "https://arxiv.org/abs/2601.19850",
      "date": "2026-01-27",
      "authors_text": "Binzhu Xie, Shi Qiu, Sicheng Zhang, Yinqiao Wang, Hao Xu",
      "is_highlight": false,
      "score": 76.0,
      "summary": "EgoHandICL introduces an innovative in-context learning framework for robust 3D hand reconstruction in egocentric vision, enhancing performance through exemplar retrieval and multimodal context integration.",
      "debug_abstract": "Robust 3D hand reconstruction in egocentric vision is challenging due to depth ambiguity, self-occlusion, and complex hand-object interactions. Prior methods mitigate these issues by scaling training data or adding auxiliary cues, but they often struggle in unseen contexts. We present EgoHandICL, the first in-context learning (ICL) framework for 3D hand reconstruction that improves semantic alignment, visual consistency, and robustness under challenging egocentric conditions. EgoHandICL introduces complementary exemplar retrieval guided by vision-language models (VLMs), an ICL-tailored tokenizer for multimodal context, and a masked autoencoder (MAE)-based architecture trained with hand-guided geometric and perceptual objectives. Experiments on ARCTIC and EgoExo4D show consistent gains over state-of-the-art methods. We also demonstrate real-world generalization and improve EgoVLM hand-object interaction reasoning by using reconstructed hands as visual prompts. Code and data: this https URL"
    },
    {
      "title": "AdaReasoner: Dynamic Tool Orchestration for Iterative Visual Reasoning",
      "url": "https://arxiv.org/abs/2601.18631",
      "date": "2026-01-26",
      "authors_text": "Mingyang Song, Haoyu Sun, Jiawei Gu, Linjie Li, Luxin Xu",
      "is_highlight": false,
      "score": 61.0,
      "summary": "AdaReasoner is a multimodal model that autonomously optimizes tool selection and usage for iterative visual reasoning, achieving state-of-the-art performance on various benchmarks.",
      "debug_abstract": "When humans face problems beyond their immediate capabilities, they rely on tools, providing a promising paradigm for improving visual reasoning in multimodal large language models (MLLMs). Effective reasoning, therefore, hinges on knowing which tools to use, when to invoke them, and how to compose them over multiple steps, even when faced with new tools or new tasks. We introduce \\textbf{AdaReasoner}, a family of multimodal models that learn tool use as a general reasoning skill rather than as tool-specific or explicitly supervised behavior. AdaReasoner is enabled by (i) a scalable data curation pipeline exposing models to long-horizon, multi-step tool interactions; (ii) Tool-GRPO, a reinforcement learning algorithm that optimizes tool selection and sequencing based on end-task success; and (iii) an adaptive learning mechanism that dynamically regulates tool usage. Together, these components allow models to infer tool utility from task context and intermediate outcomes, enabling coordination of multiple tools and generalization to unseen tools. Empirically, AdaReasoner exhibits strong tool-adaptive and generalization behaviors: it autonomously adopts beneficial tools, suppresses irrelevant ones, and adjusts tool usage frequency based on task demands, despite never being explicitly trained to do so. These capabilities translate into state-of-the-art performance across challenging benchmarks, improving the 7B base model by +24.9\\% on average and surpassing strong proprietary systems such as GPT-5 on multiple tasks, including VSP and Jigsaw."
    },
    {
      "title": "Timely Machine: Awareness of Time Makes Test-Time Scaling Agentic",
      "url": "https://arxiv.org/abs/2601.16486",
      "date": "2026-01-23",
      "authors_text": "Yichuan Ma, Linyang Li, Yongkang chen, Peiji Li, Xiaozhe Li",
      "is_highlight": false,
      "score": 78.0,
      "summary": "The paper introduces Timely Machine, redefining test-time scaling for LLMs based on wall-clock time, enhancing performance through dynamic strategy adjustments and reinforcement learning.",
      "debug_abstract": "As large language models (LLMs) increasingly tackle complex reasoning tasks, test-time scaling has become critical for enhancing capabilities. However, in agentic scenarios with frequent tool calls, the traditional generation-length-based definition breaks down: tool latency decouples inference time from generation length. We propose Timely Machine, redefining test-time as wall-clock time, where models dynamically adjust strategies based on time budgets. We introduce Timely-Eval, a benchmark spanning high-frequency tool calls, low-frequency tool calls, and time-constrained reasoning. By varying tool latency, we find smaller models excel with fast feedback through more interactions, while larger models dominate high-latency settings via superior interaction quality. Moreover, existing models fail to adapt reasoning to time budgets. We propose Timely-RL to address this gap. After cold-start supervised fine-tuning, we use reinforcement learning to enhance temporal planning. Timely-RL improves time budget awareness and consistently boosts performance across Timely-Eval. We hope our work offers a new perspective on test-time scaling for the agentic era."
    },
    {
      "title": "Adaptive Reinforcement and Model Predictive Control Switching for Safe Human-Robot Cooperative Navigation",
      "url": "https://arxiv.org/abs/2601.16686",
      "date": "2026-01-23",
      "authors_text": "Ning Liu, Sen Shen, Zheng Li, Matthew D'Souza, Jen Jen Chung",
      "is_highlight": false,
      "score": 94.0,
      "summary": "The paper presents ARMS, a hybrid control framework combining reinforcement learning and model predictive control for safe human-robot navigation, achieving superior performance in cluttered environments.",
      "debug_abstract": "This paper addresses the challenge of human-guided navigation for mobile collaborative robots under simultaneous proximity regulation and safety constraints. We introduce Adaptive Reinforcement and Model Predictive Control Switching (ARMS), a hybrid learning-control framework that integrates a reinforcement learning follower trained with Proximal Policy Optimization (PPO) and an analytical one-step Model Predictive Control (MPC) formulated as a quadratic program safety filter. To enable robust perception under partial observability and non-stationary human motion, ARMS employs a decoupled sensing architecture with a Long Short-Term Memory (LSTM) temporal encoder for the human-robot relative state and a spatial encoder for 360-degree LiDAR scans. The core contribution is a learned adaptive neural switcher that performs context-aware soft action fusion between the two controllers, favoring conservative, constraint-aware QP-based control in low-risk regions while progressively shifting control authority to the learned follower in highly cluttered or constrained scenarios where maneuverability is critical, and reverting to the follower action when the QP becomes infeasible. Extensive evaluations against Pure Pursuit, Dynamic Window Approach (DWA), and an RL-only baseline demonstrate that ARMS achieves an 82.5 percent success rate in highly cluttered environments, outperforming DWA and RL-only approaches by 7.1 percent and 3.1 percent, respectively, while reducing average computational latency by 33 percent to 5.2 milliseconds compared to a multi-step MPC baseline. Additional simulation transfer in Gazebo and initial real-world deployment results further indicate the practicality and robustness of ARMS for safe and efficient human-robot collaboration. Source code and a demonstration video are available at this https URL."
    },
    {
      "title": "Natural Language-Driven Global Mapping of Martian Landforms",
      "url": "https://arxiv.org/abs/2601.15949",
      "date": "2026-01-22",
      "authors_text": "Yiran Wang, Shuoyuan Wang, Zhaoran Wei, Jiannan Zhao, Zhonghua Yao",
      "is_highlight": false,
      "score": 72.0,
      "summary": "MarScope is a vision-language framework that enables natural language-driven, label-free mapping of Martian landforms, enhancing global geomorphic analysis and user query flexibility.",
      "debug_abstract": "Planetary surfaces are typically analyzed using high-level semantic concepts in natural language, yet vast orbital image archives remain organized at the pixel level. This mismatch limits scalable, open-ended exploration of planetary surfaces. Here we present MarScope, a planetary-scale vision-language framework enabling natural language-driven, label-free mapping of Martian landforms. MarScope aligns planetary images and text in a shared semantic space, trained on over 200,000 curated image-text pairs. This framework transforms global geomorphic mapping on Mars by replacing pre-defined classifications with flexible semantic retrieval, enabling arbitrary user queries across the entire planet in 5 seconds with F1 scores up to 0.978. Applications further show that it extends beyond morphological classification to facilitate process-oriented analysis and similarity-based geomorphological mapping at a planetary scale. MarScope establishes a new paradigm where natural language serves as a direct interface for scientific discovery over massive geospatial datasets."
    },
    {
      "title": "Robust Tool Use via Fission-GRPO: Learning to Recover from Execution Errors",
      "url": "https://arxiv.org/abs/2601.15625",
      "date": "2026-01-22",
      "authors_text": "Zhiwei Zhang, Fei Zhao, Rui Wang, Zezhong Wang, Bin Liang",
      "is_highlight": false,
      "score": 87.1,
      "summary": "Fission-GRPO enhances tool use in LLMs by converting execution errors into corrective supervision, improving error recovery and overall accuracy in multi-turn interactions.",
      "debug_abstract": "Large language models (LLMs) can call tools effectively, yet they remain brittle in multi-turn execution: following a tool call error, smaller models often degenerate into repetitive invalid re-invocations, failing to interpret error feedback and self-correct. This brittleness hinders reliable real-world deployment, where the execution errors are inherently inevitable during tool interaction procedures. We identify a key limitation of current approaches: standard reinforcement learning (RL) treats errors as sparse negative rewards, providing no guidance on how to recover, while pre-collected synthetic error-correction datasets suffer from distribution mismatch with the model&#39;s on-policy error modes. To bridge this gap, we propose Fission-GRPO, a framework that converts execution errors into corrective supervision within the RL training loop. Our core mechanism fissions each failed trajectory into a new training instance by augmenting it with diagnostic feedback from a finetuned Error Simulator, then resampling recovery rollouts on-policy. This enables the model to learn from the precise errors it makes during exploration, rather than from static, pre-collected error cases. On the BFCL v4 Multi-Turn, Fission-GRPO improves the error recovery rate of Qwen3-8B by 5.7% absolute, crucially, yielding a 4% overall accuracy gain (42.75% to 46.75%) over GRPO and outperforming specialized tool-use agents."
    }
  ],
  "OpenDriveLab": [
    {
      "title": "Large Language Models Can Take False First Steps at Inference-time Planning",
      "url": "https://arxiv.org/abs/2602.02991",
      "date": "2026-02-03",
      "authors_text": "Haijiang Yan, Jian-Qiao Zhu, Adam Sanborn",
      "is_highlight": false,
      "score": 73.0,
      "summary": "The paper explains that large language models exhibit inconsistent planning at inference due to evolving self-generated context, supported by experiments demonstrating planning shifts and reduced biases.",
      "teaser_image": "https://arxiv.org/html/2602.02991/images/llama_regression.png",
      "debug_abstract": "Large language models (LLMs) have been shown to acquire sequence-level planning abilities during training, yet their planning behavior exhibited at inference time often appears short-sighted and inconsistent with these capabilities. We propose a Bayesian account for this gap by grounding planning behavior in the evolving generative context: given the subtle differences between natural language and the language internalized by LLMs, accumulated self-generated context drives a planning-shift during inference and thereby creates the appearance of compromised planning behavior. We further validate the proposed model through two controlled experiments: a random-generation task demonstrating constrained planning under human prompts and increasing planning strength as self-generated context accumulates, and a Gaussian-sampling task showing reduced initial bias when conditioning on self-generated sequences. These findings provide a theoretical explanation along with empirical evidence for characterizing how LLMs plan ahead during inference."
    },
    {
      "title": "Search-R2: Enhancing Search-Integrated Reasoning via Actor-Refiner Collaboration",
      "url": "https://arxiv.org/abs/2602.03647",
      "date": "2026-02-03",
      "authors_text": "Bowei He, Minda Hu, Zenan Xu, Hongru Wang, Licheng Zong",
      "is_highlight": false,
      "score": 52.0,
      "summary": "Search-R2 introduces an Actor-Refiner framework that enhances search-integrated reasoning in language agents through targeted intervention and fine-grained supervision, outperforming existing methods.",
      "teaser_image": "https://arxiv.org/html/2602.03647/x2.png",
      "debug_abstract": "Search-integrated reasoning enables language agents to transcend static parametric knowledge by actively querying external sources. However, training these agents via reinforcement learning is hindered by the multi-scale credit assignment problem: existing methods typically rely on sparse, trajectory-level rewards that fail to distinguish between high-quality reasoning and fortuitous guesses, leading to redundant or misleading search behaviors. To address this, we propose Search-R2, a novel Actor-Refiner collaboration framework that enhances reasoning through targeted intervention, with both components jointly optimized during training. Our approach decomposes the generation process into an Actor, which produces initial reasoning trajectories, and a Meta-Refiner, which selectively diagnoses and repairs flawed steps via a &#39;cut-and-regenerate&#39; mechanism. To provide fine-grained supervision, we introduce a hybrid reward design that couples outcome correctness with a dense process reward quantifying the information density of retrieved evidence. Theoretically, we formalize the Actor-Refiner interaction as a smoothed mixture policy, proving that selective correction yields strict performance gains over strong baselines. Extensive experiments across various general and multi-hop QA datasets demonstrate that Search-R2 consistently outperforms strong RAG and RL-based baselines across model scales, achieving superior reasoning accuracy with minimal overhead."
    },
    {
      "title": "Dual Quaternion SE(3) Synchronization with Recovery Guarantees",
      "url": "https://arxiv.org/abs/2602.00324",
      "date": "2026-01-30",
      "authors_text": "Jianing Zhao, Linglingzhi Zhu, Anthony Man-Cho So",
      "is_highlight": false,
      "score": 75.0,
      "summary": "This paper presents a dual quaternion-based SE(3) synchronization method with a two-stage algorithm that guarantees recovery of absolute poses from noisy transformations, outperforming traditional matrix methods.",
      "teaser_image": "https://arxiv.org/html/2602.00324/x2.png",
      "debug_abstract": "Synchronization over the special Euclidean group SE(3) aims to recover absolute poses from noisy pairwise relative transformations and is a core primitive in robotics and 3D vision. Standard approaches often require multi-step heuristic procedures to recover valid poses, which are difficult to analyze and typically lack theoretical guarantees. This paper adopts a dual quaternion representation and formulates SE(3) synchronization directly over the unit dual quaternion. A two-stage algorithm is developed: A spectral initializer computed via the power method on a Hermitian dual quaternion measurement matrix, followed by a dual quaternion generalized power method (DQGPM) that enforces feasibility through per-iteration projection. The estimation error bounds are established for spectral estimators, and DQGPM is shown to admit a finite-iteration error bound and achieves linear error contraction up to an explicit noise-dependent threshold. Experiments on synthetic benchmarks and real-world multi-scan point-set registration demonstrate that the proposed pipeline improves both accuracy and efficiency over representative matrix-based methods."
    },
    {
      "title": "Offline Discovery of Interpretable Skills from Multi-Task Trajectories",
      "url": "https://arxiv.org/abs/2602.01018",
      "date": "2026-02-01",
      "authors_text": "Chongyu Zhu, Mithun Vanniasinghe, Jiayu Chen, Chi-Guhn Lee",
      "is_highlight": false,
      "score": 77.0,
      "summary": "LOKI is a three-stage framework for offline skill discovery and hierarchical imitation learning, effectively extracting interpretable skills from multi-task trajectories without explicit rewards.",
      "teaser_image": "https://arxiv.org/html/2602.01018/fig/two_stage_skill_learning.png",
      "debug_abstract": "Hierarchical Imitation Learning is a powerful paradigm for acquiring complex robot behaviors from demonstrations. A central challenge, however, lies in discovering reusable skills from long-horizon, multi-task offline data, especially when the data lacks explicit rewards or subtask annotations. In this work, we introduce LOKI, a three-stage end-to-end learning framework designed for offline skill discovery and hierarchical imitation. The framework commences with a two-stage, weakly supervised skill discovery process: Stage one performs coarse, task-aware macro-segmentation by employing an alignment-enforced Vector Quantized VAE guided by weak task labels. Stage two then refines these segments at a micro-level using a self-supervised sequential model, followed by an iterative clustering process to consolidate skill boundaries. The third stage then leverages these precise boundaries to construct a hierarchical policy within an option-based framework-complete with a learned termination condition beta for explicit skill switching. LOKI achieves high success rates on the challenging D4RL Kitchen benchmark and outperforms standard HIL baselines. Furthermore, we demonstrate that the discovered skills are semantically meaningful, aligning with human intuition, and exhibit compositionality by successfully sequencing them to solve a novel, unseen task."
    },
    {
      "title": "Residual Decoding: Mitigating Hallucinations in Large Vision-Language Models via History-Aware Residual Guidance",
      "url": "https://arxiv.org/abs/2602.01047",
      "date": "2026-02-01",
      "authors_text": "Xinrong Chen, Xu Chu, Yingmin Qiu, Hengyuan Zhang, Jing Xiong",
      "is_highlight": false,
      "score": 53.0,
      "summary": "Residual Decoding (ResDec) mitigates hallucinations in Large Vision-Language Models by leveraging historical information to enhance decoding and improve visual grounding.",
      "teaser_image": "https://arxiv.org/html/2602.01047/x1.png",
      "debug_abstract": "Large Vision-Language Models (LVLMs) can reason effectively from image-text inputs and perform well in various multimodal tasks. Despite this success, they are affected by language priors and often produce hallucinations. Hallucinations denote generated content that is grammatically and syntactically coherent, yet bears no match or direct relevance to actual visual input. To address this problem, we propose Residual Decoding (ResDec). It is a novel training-free method that uses historical information to aid decoding. The method relies on the internal implicit reasoning mechanism and token logits evolution mechanism of LVLMs to correct biases. Extensive experiments demonstrate that ResDec effectively suppresses hallucinations induced by language priors, significantly improves visual grounding, and reduces object hallucinations. In addition to mitigating hallucinations, ResDec also performs exceptionally well on comprehensive LVLM benchmarks, highlighting its broad applicability."
    },
    {
      "title": "MedAD-R1: Eliciting Consistent Reasoning in Interpretible Medical Anomaly Detection via Consistency-Reinforced Policy Optimization",
      "url": "https://arxiv.org/abs/2602.01081",
      "date": "2026-02-01",
      "authors_text": "Haitao Zhang, Yingying Wang, Jiaxiang Wang, Haote Xu, Hongyang Zhang",
      "is_highlight": false,
      "score": 64.0,
      "summary": "MedAD-R1 enhances medical anomaly detection by utilizing a two-stage training framework with consistency-reinforced policy optimization, achieving state-of-the-art performance on the MedAD-38K benchmark.",
      "teaser_image": "https://arxiv.org/html/2602.01081/x3.png",
      "debug_abstract": "Medical Anomaly Detection (MedAD) presents a significant opportunity to enhance diagnostic accuracy using Large Multimodal Models (LMMs) to interpret and answer questions based on medical images. However, the reliance on Supervised Fine-Tuning (SFT) on simplistic and fragmented datasets has hindered the development of models capable of plausible reasoning and robust multimodal generalization. To overcome this, we introduce MedAD-38K, the first large-scale, multi-modal, and multi-center benchmark for MedAD featuring diagnostic Chain-of-Thought (CoT) annotations alongside structured Visual Question-Answering (VQA) pairs. On this foundation, we propose a two-stage training framework. The first stage, Cognitive Injection, uses SFT to instill foundational medical knowledge and align the model with a structured think-then-answer paradigm. Given that standard policy optimization can produce reasoning that is disconnected from the final answer, the second stage incorporates Consistency Group Relative Policy Optimization (Con-GRPO). This novel algorithm incorporates a crucial consistency reward to ensure the generated reasoning process is relevant and logically coherent with the final diagnosis. Our proposed model, MedAD-R1, achieves state-of-the-art (SOTA) performance on the MedAD-38K benchmark, outperforming strong baselines by more than 10\\%. This superior performance stems from its ability to generate transparent and logically consistent reasoning pathways, offering a promising approach to enhancing the trustworthiness and interpretability of AI for clinical decision support."
    },
    {
      "title": "Adaptive Rollout Allocation for Online Reinforcement Learning with Verifiable Rewards",
      "url": "https://arxiv.org/abs/2602.01601",
      "date": "2026-02-02",
      "authors_text": "Hieu Trung Nguyen, Bao Nguyen, Wenao Ma, Yuzhi Zhao, Ruifeng She",
      "is_highlight": false,
      "score": 40.0,
      "summary": "The paper presents \\Ours, a variance-informed strategy for adaptive rollout allocation in online reinforcement learning, enhancing sampling efficiency and performance over traditional methods.",
      "teaser_image": "https://arxiv.org/html/2602.01601/x2.png",
      "debug_abstract": "Sampling efficiency is a key bottleneck in reinforcement learning with verifiable rewards. Existing group-based policy optimization methods, such as GRPO, allocate a fixed number of rollouts for all training prompts. This uniform allocation implicitly treats all prompts as equally informative, and could lead to inefficient computational budget usage and impede training progress. We introduce \\Ours, a Variance-Informed Predictive allocation strategy that allocates a given rollout budget to the prompts in the incumbent batch to minimize the expected gradient variance of the policy update. At each iteration, \\Ours~uses a lightweight Gaussian process model to predict per-prompt success probabilities based on recent rollouts. These probability predictions are translated into variance estimates, which are then fed into a convex optimization problem to determine the optimal rollout allocations under a hard compute budget constraint. Empirical results show that \\Ours~consistently improves sampling efficiency and achieves higher performance than uniform or heuristic allocation strategies in multiple benchmarks. Our code will be available at this https URL."
    },
    {
      "title": "Reasoning with Autoregressive-Diffusion Collaborative Thoughts",
      "url": "https://arxiv.org/abs/2602.01608",
      "date": "2026-02-02",
      "authors_text": "Mu Yuan, Liekang Zeng, Guoliang Xing, Lan Zhang, Yunhao Liu",
      "is_highlight": false,
      "score": 41.0,
      "summary": "The paper presents Collaborative Thoughts, a framework integrating autoregressive and diffusion models for enhanced spatial reasoning and generation through iterative feedback and joint reasoning.",
      "teaser_image": "https://arxiv.org/html/2602.01608/x2.png",
      "debug_abstract": "Autoregressive and diffusion models represent two complementary generative paradigms. Autoregressive models excel at sequential planning and constraint composition, yet struggle with tasks that require explicit spatial or physical grounding. Diffusion models, in contrast, capture rich spatial structure through high-dimensional generation, but lack the stepwise logical control needed to satisfy complex, multi-stage constraints or to reliably identify and correct errors. We introduce Collaborative Thoughts, a unified collaborative framework that enables autoregressive and diffusion models to reason and generate jointly through a closed-loop interaction. In Collaborative Thoughts, autoregressive models perform structured planning and constraint management, diffusion models instantiate these constraints as intermediate visual thoughts, and a vision-based critic module evaluates whether the visual thoughts satisfy the intended structural and physical requirements. This feedback is then used to iteratively refine subsequent planning and generation steps, mitigating error propagation across modalities. Importantly, Collaborative Thoughts uses the same collaborative loop regardless of whether the task is autoregressive question answering or diffusion-based visual generation. Through representative examples, we illustrate how Collaborative Thoughts can improve the reliability of spatial reasoning and the controllability of generation."
    },
    {
      "title": "Tilt-Ropter: A Novel Hybrid Aerial and Terrestrial Vehicle with Tilt Rotors and Passive Wheels",
      "url": "https://arxiv.org/abs/2602.01700",
      "date": "2026-02-02",
      "authors_text": "Ruoyu Wang, Xuchen Liu, Zongzhou Wu, Zixuan Guo, Wendi Ding",
      "is_highlight": false,
      "score": 38.0,
      "summary": "The Tilt-Ropter is a hybrid aerial-terrestrial vehicle that utilizes tilt rotors and passive wheels for efficient locomotion, featuring advanced control systems for enhanced mobility and energy savings.",
      "teaser_image": "https://arxiv.org/html/2602.01700/x2.png",
      "debug_abstract": "In this work, we present Tilt-Ropter, a novel hybrid aerial-terrestrial vehicle (HATV) that combines tilt rotors with passive wheels to achieve energy-efficient multi-mode locomotion. Unlike existing under-actuated HATVs, the fully actuated design of Tilt-Ropter enables decoupled force and torque control, greatly enhancing its mobility and environmental adaptability. A nonlinear model predictive controller (NMPC) is developed to track reference trajectories and handle contact constraints across locomotion modes, while a dedicated control allocation module exploits actuation redundancy to achieve energy-efficient control of actuators. Additionally, to enhance robustness during ground contact, we introduce an external wrench estimation algorithm that estimates environmental interaction forces and torques in real time. The system is validated through both simulation and real-world experiments, including seamless air-ground transitions and trajectory tracking. Results show low tracking errors in both modes and highlight a 92.8% reduction in power consumption during ground locomotion, demonstrating the system&#39;s potential for long-duration missions across large-scale and energy-constrained environments."
    },
    {
      "title": "Breaking the Static Graph: Context-Aware Traversal for Robust Retrieval-Augmented Generation",
      "url": "https://arxiv.org/abs/2602.01965",
      "date": "2026-02-02",
      "authors_text": "Kwun Hang Lau, Fangyuan Zhang, Boyu Ruan, Yingli Zhou, Qintian Guo",
      "is_highlight": false,
      "score": 27.0,
      "summary": "CatRAG enhances Retrieval-Augmented Generation by transforming static Knowledge Graphs into query-adaptive structures, improving multi-hop evidence retrieval and reasoning completeness.",
      "teaser_image": "https://arxiv.org/html/2602.01965/image/HubBiasShift.png",
      "debug_abstract": "Recent advances in Retrieval-Augmented Generation (RAG) have shifted from simple vector similarity to structure-aware approaches like HippoRAG, which leverage Knowledge Graphs (KGs) and Personalized PageRank (PPR) to capture multi-hop dependencies. However, these methods suffer from a &#34;Static Graph Fallacy&#34;: they rely on fixed transition probabilities determined during indexing. This rigidity ignores the query-dependent nature of edge relevance, causing semantic drift where random walks are diverted into high-degree &#34;hub&#34; nodes before reaching critical downstream evidence. Consequently, models often achieve high partial recall but fail to retrieve the complete evidence chain required for multi-hop queries. To address this, we propose CatRAG, Context-Aware Traversal for robust RAG, a framework that builds on the HippoRAG 2 architecture and transforms the static KG into a query-adaptive navigation structure. We introduce a multi-faceted framework to steer the random walk: (1) Symbolic Anchoring, which injects weak entity constraints to regularize the random walk; (2) Query-Aware Dynamic Edge Weighting, which dynamically modulates graph structure, to prune irrelevant paths while amplifying those aligned with the query&#39;s intent; and (3) Key-Fact Passage Weight Enhancement, a cost-efficient bias that structurally anchors the random walk to likely evidence. Experiments across four multi-hop benchmarks demonstrate that CatRAG consistently outperforms state of the art baselines. Our analysis reveals that while standard Recall metrics show modest gains, CatRAG achieves substantial improvements in reasoning completeness, the capacity to recover the entire evidence path without gaps. These results reveal that our approach effectively bridges the gap between retrieving partial context and enabling fully grounded reasoning. Resources are available at this https URL."
    },
    {
      "title": "TIDE: Trajectory-based Diagnostic Evaluation of Test-Time Improvement in LLM Agents",
      "url": "https://arxiv.org/abs/2602.02196",
      "date": "2026-02-02",
      "authors_text": "Hang Yan, Xinyu Che, Fangzhi Xu, Qiushi Sun, Zichen Ding",
      "is_highlight": false,
      "score": 20.0,
      "summary": "The TIDE framework evaluates Test-Time Improvement in LLM agents by analyzing task completion dynamics, recursive behaviors, and memory constraints to enhance performance optimization.",
      "teaser_image": "https://arxiv.org/html/2602.02196/x1.png",
      "debug_abstract": "Recent advances in autonomous LLM agents demonstrate their ability to improve performance through iterative interaction with the environment. We define this paradigm as Test-Time Improvement (TTI). However, the mechanisms under how and why TTI succeed or fail remain poorly understood, and existing evaluation metrics fail to capture their task optimization efficiency, behavior adaptation after erroneous actions, and the specific utility of working memory for task completion. To address these gaps, we propose Test-time Improvement Diagnostic Evaluation (TIDE), an agent-agnostic and environment-agnostic framework that decomposes TTI into three comprehensive and interconnected dimensions. The framework measures (1) the overall temporal dynamics of task completion and (2) identifies whether performance is primarily constrained by recursive looping behaviors or (3) by burdensome accumulated memory. Through extensive experiments across diverse agents and environments, TIDE highlights that improving agent performance requires more than scaling internal reasoning, calling for explicitly optimizing the interaction dynamics between the agent and the environment."
    },
    {
      "title": "Towards Agentic Intelligence for Materials Science",
      "url": "https://arxiv.org/abs/2602.00169",
      "date": "2026-01-29",
      "authors_text": "Huan Zhang, Yizhan Li, Wenhao Huang, Ziyu Hou, Yu Song",
      "is_highlight": false,
      "score": 12.0,
      "summary": "This paper advocates for developing agentic AI systems in materials science that autonomously plan, act, and learn to enhance discovery processes beyond traditional models.",
      "teaser_image": "https://arxiv.org/html/2602.00169/x1.png",
      "debug_abstract": "The convergence of artificial intelligence and materials science presents a transformative opportunity, but achieving true acceleration in discovery requires moving beyond task-isolated, fine-tuned models toward agentic systems that plan, act, and learn across the full discovery loop. This survey advances a unique pipeline-centric view that spans from corpus curation and pretraining, through domain adaptation and instruction tuning, to goal-conditioned agents interfacing with simulation and experimental platforms. Unlike prior reviews, we treat the entire process as an end-to-end system to be optimized for tangible discovery outcomes rather than proxy benchmarks. This perspective allows us to trace how upstream design choices-such as data curation and training objectives-can be aligned with downstream experimental success through effective credit assignment. To bridge communities and establish a shared frame of reference, we first present an integrated lens that aligns terminology, evaluation, and workflow stages across AI and materials science. We then analyze the field through two focused lenses: From the AI perspective, the survey details LLM strengths in pattern recognition, predictive analytics, and natural language processing for literature mining, materials characterization, and property prediction; from the materials science perspective, it highlights applications in materials design, process optimization, and the acceleration of computational workflows via integration with external tools (e.g., DFT, robotic labs). Finally, we contrast passive, reactive approaches with agentic design, cataloging current contributions while motivating systems that pursue long-horizon goals with autonomy, memory, and tool use. This survey charts a practical roadmap towards autonomous, safety-aware LLM agents aimed at discovering novel and useful materials."
    },
    {
      "title": "Failure-Aware Bimanual Teleoperation via Conservative Value Guided Assistance",
      "url": "https://arxiv.org/abs/2602.01092",
      "date": "2026-02-01",
      "authors_text": "Peng Zhou, Zhongxuan Li, Jinsong Wu, Jiaming Qi, Jun Hu",
      "is_highlight": false,
      "score": 9.0,
      "summary": "This paper presents a failure-aware bimanual teleoperation framework that uses conservative value learning to enhance task success and reduce operator workload through compliant haptic assistance.",
      "teaser_image": "https://arxiv.org/html/2602.01092/x1.png",
      "debug_abstract": "Teleoperation of high-precision manipulation is con-strained by tight success tolerances and complex contact dy-namics, which make impending failures difficult for human operators to anticipate under partial observability. This paper proposes a value-guided, failure-aware framework for bimanual teleoperation that provides compliant haptic assistance while pre-serving continuous human authority. The framework is trained entirely from heterogeneous offline teleoperation data containing both successful and failed executions. Task feasibility is mod-eled as a conservative success score learned via Conservative Value Learning, yielding a risk-sensitive estimate that remains reliable under distribution shift. During online operation, the learned success score regulates the level of assistance, while a learned actor provides a corrective motion direction. Both are integrated through a joint-space impedance interface on the master side, yielding continuous guidance that steers the operator away from failure-prone actions without overriding intent. Experimental results on contact-rich manipulation tasks demonstrate improved task success rates and reduced operator workload compared to conventional teleoperation and shared-autonomy baselines, indicating that conservative value learning provides an effective mechanism for embedding failure awareness into bilateral teleoperation. Experimental videos are available at this https URL"
    },
    {
      "title": "FlowSteer: Interactive Agentic Workflow Orchestration via End-to-End Reinforcement Learning",
      "url": "https://arxiv.org/abs/2602.01664",
      "date": "2026-02-02",
      "authors_text": "Mingda Zhang, Haoran Luo, Tiesunlong Shen, Qika Lin, Xiaoying Tang",
      "is_highlight": false,
      "score": 7.0,
      "summary": "FlowSteer is an end-to-end reinforcement learning framework that automates workflow orchestration through interactive learning, addressing manual costs and improving performance across diverse tasks.",
      "teaser_image": "https://arxiv.org/html/2602.01664/x1.png",
      "debug_abstract": "In recent years, a variety of powerful agentic workflows have been applied to solve a wide range of human problems. However, existing workflow orchestration still faces key challenges, including high manual cost, reliance on specific operators/large language models (LLMs), and sparse reward signals. To address these challenges, we propose FlowSteer, an end-to-end reinforcement learning framework that takes a lightweight policy model as the agent and an executable canvas environment, automating workflow orchestration through multi-turn interaction. In this process, the policy model analyzes execution states and selects editing actions, while the canvas executes operators and returns feedback for iterative refinement. Moreover, FlowSteer provides a plug-and-play framework that supports diverse operator libraries and interchangeable LLM backends. To effectively train this interaction paradigm, we propose Canvas Workflow Relative Policy Optimization (CWRPO), which introduces diversity-constrained rewards with conditional release to stabilize learning and suppress shortcut behaviors. Experimental results on twelve datasets show that FlowSteer significantly outperforms baselines across various tasks."
    },
    {
      "title": "Diffusion Model-based Reinforcement Learning for Version Age of Information Scheduling: Average and Tail-Risk-Sensitive Control",
      "url": "https://arxiv.org/abs/2601.18069",
      "date": "2026-01-26",
      "authors_text": "Haoyuan Pan, Sizhao Chen, Zhaorui Wang, Tse-Tin Chan",
      "is_highlight": false,
      "score": 57.0,
      "summary": "This paper presents a novel reinforcement learning approach, RS-D3SAC, for optimizing Version Age of Information scheduling by minimizing both average and tail-risk-sensitive metrics in wireless systems.",
      "teaser_image": null,
      "debug_abstract": "Ensuring timely and semantically accurate information delivery is critical in real-time wireless systems. While Age of Information (AoI) quantifies temporal freshness, Version Age of Information (VAoI) captures semantic staleness by accounting for version evolution between transmitters and receivers. Existing VAoI scheduling approaches primarily focus on minimizing average VAoI, overlooking rare but severe staleness events that can compromise reliability under stochastic packet arrivals and unreliable channels. This paper investigates both average-oriented and tail-risk-sensitive VAoI scheduling in a multi-user status update system with long-term transmission cost constraints. We first formulate the average VAoI minimization problem as a constrained Markov decision process and introduce a deep diffusion-based Soft Actor-Critic (D2SAC) algorithm. By generating actions through a diffusion-based denoising process, D2SAC enhances policy expressiveness and establishes a strong baseline for mean performance. Building on this foundation, we put forth RS-D3SAC, a risk-sensitive deep distributional diffusion-based Soft Actor-Critic algorithm. RS-D3SAC integrates a diffusion-based actor with a quantile-based distributional critic, explicitly modeling the full VAoI return distribution. This enables principled tail-risk optimization via Conditional Value-at-Risk (CVaR) while satisfying long-term transmission cost constraints. Extensive simulations show that, while D2SAC reduces average VAoI, RS-D3SAC consistently achieves substantial reductions in CVaR without sacrificing mean performance. The dominant gain in tail-risk reduction stems from the distributional critic, with the diffusion-based actor providing complementary refinement to stabilize and enrich policy decisions, highlighting their effectiveness for robust and risk-aware VAoI scheduling in multi-user wireless systems."
    },
    {
      "title": "LoD-Structured 3D Gaussian Splatting for Streaming Video Reconstruction",
      "url": "https://arxiv.org/abs/2601.18475",
      "date": "2026-01-26",
      "authors_text": "Xinhui Liu, Can Wang, Lei Liu, Zhenghao Chen, Wei Jiang",
      "is_highlight": false,
      "score": 52.0,
      "summary": "The StreamLoD-GS framework enhances Streaming Free-Viewpoint Video reconstruction by optimizing 3D Gaussian Splatting for efficient, high-fidelity rendering with reduced storage needs.",
      "teaser_image": null,
      "debug_abstract": "Free-Viewpoint Video (FVV) reconstruction enables photorealistic and interactive 3D scene visualization; however, real-time streaming is often bottlenecked by sparse-view inputs, prohibitive training costs, and bandwidth constraints. While recent 3D Gaussian Splatting (3DGS) has advanced FVV due to its superior rendering speed, Streaming Free-Viewpoint Video (SFVV) introduces additional demands for rapid optimization, high-fidelity reconstruction under sparse constraints, and minimal storage footprints. To bridge this gap, we propose StreamLoD-GS, an LoD-based Gaussian Splatting framework designed specifically for SFVV. Our approach integrates three core innovations: 1) an Anchor- and Octree-based LoD-structured 3DGS with a hierarchical Gaussian dropout technique to ensure efficient and stable optimization while maintaining high-quality rendering; 2) a GMM-based motion partitioning mechanism that separates dynamic and static content, refining dynamic regions while preserving background stability; and 3) a quantized residual refinement framework that significantly reduces storage requirements without compromising visual fidelity. Extensive experiments demonstrate that StreamLoD-GS achieves competitive or state-of-the-art performance in terms of quality, efficiency, and storage."
    },
    {
      "title": "Heterogeneous Vertiport Selection Optimization for On-Demand Air Taxi Services: A Deep Reinforcement Learning Approach",
      "url": "https://arxiv.org/abs/2601.21316",
      "date": "2026-01-29",
      "authors_text": "Aoyu Pang, Maonan Wang, Zifan Sha, Wenwei Yue, Changle Li",
      "is_highlight": false,
      "score": 80.0,
      "summary": "The paper presents a deep reinforcement learning framework for optimizing vertiport selection and air taxi routing, achieving significant travel time reductions in urban air mobility systems.",
      "teaser_image": "https://arxiv.org/html/2601.21316/x2.png",
      "debug_abstract": "Urban Air Mobility (UAM) has emerged as a transformative solution to alleviate urban congestion by utilizing low-altitude airspace, thereby reducing pressure on ground transportation networks. To enable truly efficient and seamless door-to-door travel experiences, UAM requires close integration with existing ground transportation infrastructure. However, current research on optimal integrated routing strategies for passengers in air-ground mobility systems remains limited, with a lack of systematic this http URL address this gap, we first propose a unified optimization model that integrates strategy selection for both air and ground transportation. This model captures the dynamic characteristics of multimodal transport networks and incorporates real-time traffic conditions alongside passenger decision-making behavior. Building on this model, we propose a Unified Air-Ground Mobility Coordination (UAGMC) framework, which leverages deep reinforcement learning (RL) and Vehicle-to-Everything (V2X) communication to optimize vertiport selection and dynamically plan air taxi routes. Experimental results demonstrate that UAGMC achieves a 34\\% reduction in average travel time compared to conventional proportional allocation methods, enhancing overall travel efficiency and providing novel insights into the integration and optimization of multimodal transportation systems. This work lays a solid foundation for advancing intelligent urban mobility solutions through the coordination of air and ground transportation modes. The related code can be found at this https URL."
    },
    {
      "title": "Disentangling perception and reasoning for improving data efficiency in learning cloth manipulation without demonstrations",
      "url": "https://arxiv.org/abs/2601.21713",
      "date": "2026-01-29",
      "authors_text": "Donatien Delehelle, Fei Chen, Darwin Caldwell",
      "is_highlight": false,
      "score": 87.0,
      "summary": "This paper presents a modular reinforcement learning approach for cloth manipulation that enhances data efficiency and reduces model size and training time, achieving improved performance in simulation and real-world transfer.",
      "teaser_image": "https://arxiv.org/html/2601.21713/x1.png",
      "debug_abstract": "Cloth manipulation is a ubiquitous task in everyday life, but it remains an open challenge for robotics. The difficulties in developing cloth manipulation policies are attributed to the high-dimensional state space, complex dynamics, and high propensity to self-occlusion exhibited by fabrics. As analytical methods have not been able to provide robust and general manipulation policies, reinforcement learning (RL) is considered a promising approach to these problems. However, to address the large state space and complex dynamics, data-based methods usually rely on large models and long training times. The resulting computational cost significantly hampers the development and adoption of these methods. Additionally, due to the challenge of robust state estimation, garment manipulation policies often adopt an end-to-end learning approach with workspace images as input. While this approach enables a conceptually straightforward sim-to-real transfer via real-world fine-tuning, it also incurs a significant computational cost by training agents on a highly lossy representation of the environment state. This paper questions this common design choice by exploring an efficient and modular approach to RL for cloth manipulation. We show that, through careful design choices, model size and training time can be significantly reduced when learning in simulation. Furthermore, we demonstrate how the resulting simulation-trained model can be transferred to the real world. We evaluate our approach on the SoftGym benchmark and achieve significant performance improvements over available baselines on our task, while using a substantially smaller model."
    },
    {
      "title": "Dynamic Topology Awareness: Breaking the Granularity Rigidity in Vision-Language Navigation",
      "url": "https://arxiv.org/abs/2601.21751",
      "date": "2026-01-29",
      "authors_text": "Jiankun Peng, Jianyuan Guo, Ying Xu, Yue Liu, Jiashuang Yan",
      "is_highlight": false,
      "score": 84.0,
      "summary": "The paper presents DGNav, a dynamic framework for Vision-Language Navigation that adapts topological map density and connectivity to enhance navigation efficiency and safety in complex environments.",
      "teaser_image": "https://arxiv.org/html/2601.21751/x1.png",
      "debug_abstract": "Vision-Language Navigation in Continuous Environments (VLN-CE) presents a core challenge: grounding high-level linguistic instructions into precise, safe, and long-horizon spatial actions. Explicit topological maps have proven to be a vital solution for providing robust spatial memory in such tasks. However, existing topological planning methods suffer from a &#34;Granularity Rigidity&#34; problem. Specifically, these methods typically rely on fixed geometric thresholds to sample nodes, which fails to adapt to varying environmental complexities. This rigidity leads to a critical mismatch: the model tends to over-sample in simple areas, causing computational redundancy, while under-sampling in high-uncertainty regions, increasing collision risks and compromising precision. To address this, we propose DGNav, a framework for Dynamic Topological Navigation, introducing a context-aware mechanism to modulate map density and connectivity on-the-fly. Our approach comprises two core innovations: (1) A Scene-Aware Adaptive Strategy that dynamically modulates graph construction thresholds based on the dispersion of predicted waypoints, enabling &#34;densification on demand&#34; in challenging environments; (2) A Dynamic Graph Transformer that reconstructs graph connectivity by fusing visual, linguistic, and geometric cues into dynamic edge weights, enabling the agent to filter out topological noise and enhancing instruction adherence. Extensive experiments on the R2R-CE and RxR-CE benchmarks demonstrate DGNav exhibits superior navigation performance and strong generalization capabilities. Furthermore, ablation studies confirm that our framework achieves an optimal trade-off between navigation efficiency and safe exploration. The code is available at this https URL."
    },
    {
      "title": "PLANING: A Loosely Coupled Triangle-Gaussian Framework for Streaming 3D Reconstruction",
      "url": "https://arxiv.org/abs/2601.22046",
      "date": "2026-01-29",
      "authors_text": "Changjian Jiang, Kerui Ren, Xudong Li, Kaiwen Song, Linning Xu",
      "is_highlight": false,
      "score": 66.0,
      "summary": "PLANING is a novel streaming 3D reconstruction framework that efficiently combines geometric primitives and neural Gaussians, enhancing quality and speed while reducing redundancy.",
      "teaser_image": "https://arxiv.org/html/2601.22046/x3.png",
      "debug_abstract": "Streaming reconstruction from monocular image sequences remains challenging, as existing methods typically favor either high-quality rendering or accurate geometry, but rarely both. We present PLANING, an efficient on-the-fly reconstruction framework built on a hybrid representation that loosely couples explicit geometric primitives with neural Gaussians, enabling geometry and appearance to be modeled in a decoupled manner. This decoupling supports an online initialization and optimization strategy that separates geometry and appearance updates, yielding stable streaming reconstruction with substantially reduced structural redundancy. PLANING improves dense mesh Chamfer-L2 by 18.52% over PGSR, surpasses ARTDECO by 1.31 dB PSNR, and reconstructs ScanNetV2 scenes in under 100 seconds, over 5x faster than 2D Gaussian Splatting, while matching the quality of offline per-scene optimization. Beyond reconstruction quality, the structural clarity and computational efficiency of \\modelname~make it well suited for a broad range of downstream applications, such as enabling large-scale scene modeling and simulation-ready environments for embodied AI. Project page: this https URL ."
    },
    {
      "title": "Listen, Look, Drive: Coupling Audio Instructions for User-aware VLA-based Autonomous Driving",
      "url": "https://arxiv.org/abs/2601.12142",
      "date": "2026-01-28",
      "authors_text": "Ziang Guo, Feng Yang, Xuefeng Zhang, Jiaqi Guo, Kun Zhao",
      "is_highlight": false,
      "score": 88.0,
      "summary": "EchoVLA enhances autonomous driving by integrating audio instructions with visual inputs, allowing for user influence and emotional context, significantly improving navigation accuracy and safety.",
      "teaser_image": "https://arxiv.org/html/2601.12142/Figs/overview.png",
      "debug_abstract": "Vision Language Action (VLA) models promise an open-vocabulary interface that can translate perceptual ambiguity into semantically grounded driving decisions, yet they still treat language as a static prior fixed at inference time. As a result, the model must infer continuously shifting objectives from pixels alone, yielding delayed or overly conservative maneuvers. We argue that effective VLAs for autonomous driving need an online channel in which users can influence driving with specific intentions. To this end, we present EchoVLA, a user-aware VLA that couples camera streams with in situ audio instructions. We augment the nuScenes dataset with temporally aligned, intent-specific speech commands generated by converting ego-motion descriptions into synthetic audios. Further, we compose emotional speech-trajectory pairs into a multimodal Chain-of-Thought (CoT) for fine-tuning a Multimodal Large Model (MLM) based on Qwen2.5-Omni. Specifically, we synthesize the audio-augmented dataset with different emotion types paired with corresponding driving behaviors, leveraging the emotional cues embedded in tone, pitch, and speech tempo to reflect varying user states, such as urgent or hesitant intentions, thus enabling our EchoVLA to interpret not only the semantic content but also the emotional context of audio commands for more nuanced and emotionally adaptive driving behavior. In open-loop benchmarks, our approach reduces the average L2 error by $59.4\\%$ and the collision rate by $74.4\\%$ compared to the baseline of vision-only perception. More experiments on nuScenes dataset validate that EchoVLA not only steers the trajectory through audio instructions, but also modulates driving behavior in response to the emotions detected in the user&#39;s speech."
    },
    {
      "title": "Tactile-Force Alignment in Vision-Language-Action Models for Force-aware Manipulation",
      "url": "https://arxiv.org/abs/2601.20321",
      "date": "2026-01-28",
      "authors_text": "Yuzhe Huang, Pei Lin, Wanlin Li, Daohan Li, Jiajun Li",
      "is_highlight": false,
      "score": 91.0,
      "summary": "The paper introduces TaF-VLA, a framework that enhances Vision-Language-Action models by integrating tactile-force alignment for improved force-aware robotic manipulation in contact-rich tasks.",
      "teaser_image": "https://arxiv.org/html/2601.20321/figures/fig1-teaser.png",
      "debug_abstract": "Vision-Language-Action (VLA) models have recently emerged as powerful generalists for robotic manipulation. However, due to their predominant reliance on visual modalities, they fundamentally lack the physical intuition required for contact-rich tasks that require precise force regulation and physical reasoning. Existing attempts to incorporate vision-based tactile sensing into VLA models typically treat tactile inputs as auxiliary visual textures, thereby overlooking the underlying correlation between surface deformation and interaction dynamics. To bridge this gap, we propose a paradigm shift from tactile-vision alignment to tactile-force alignment. Here, we introduce TaF-VLA, a framework that explicitly grounds high-dimensional tactile observations in physical interaction forces. To facilitate this, we develop an automated tactile-force data acquisition device and curate the TaF-Dataset, comprising over 10 million synchronized tactile observations, 6-axis force/torque, and matrix force map. To align sequential tactile observations with interaction forces, the central component of our approach is the Tactile-Force Adapter (TaF-Adapter), a tactile sensor encoder that extracts discretized latent information for encoding tactile observations. This mechanism ensures that the learned representations capture history-dependent, noise-insensitive physical dynamics rather than static visual textures. Finally, we integrate this force-aligned encoder into a VLA backbone. Extensive real-world experiments demonstrate that TaF-VLA policy significantly outperforms state-of-the-art tactile-vision-aligned and vision-only baselines on contact-rich tasks, verifying its ability to achieve robust, force-aware manipulation through cross-modal physical reasoning."
    },
    {
      "title": "Rhombot: Rhombus-shaped Modular Robots for Stable, Medium-Independent Reconfiguration Motion",
      "url": "https://arxiv.org/abs/2601.19529",
      "date": "2026-01-27",
      "authors_text": "Jie Gu, Yirui Sun, Zhihao Xia, Tin Lun Lam, Chunxu Tian",
      "is_highlight": false,
      "score": 86.0,
      "summary": "Rhombot is a modular self-reconfigurable robot featuring rhombus-shaped modules that enable stable, medium-independent morphing and locomotion through a novel motion primitive called morphpivoting.",
      "debug_abstract": "In this paper, we present Rhombot, a novel deformable planar lattice modular self-reconfigurable robot (MSRR) with a rhombus shaped module. Each module consists of a parallelogram skeleton with a single centrally mounted actuator that enables folding and unfolding along its diagonal. The core design philosophy is to achieve essential MSRR functionalities such as morphing, docking, and locomotion with minimal control complexity. This enables a continuous and stable reconfiguration process that is independent of the surrounding medium, allowing the system to reliably form various configurations in diverse environments. To leverage the unique kinematics of Rhombot, we introduce morphpivoting, a novel motion primitive for reconfiguration that differs from advanced MSRR systems, and propose a strategy for its continuous execution. Finally, a series of physical experiments validate the module&#39;s stable reconfiguration ability, as well as its positional and docking accuracy."
    },
    {
      "title": "Component-Level Lesioning of Language Models Reveals Clinically Aligned Aphasia Phenotypes",
      "url": "https://arxiv.org/abs/2601.19723",
      "date": "2026-01-27",
      "authors_text": "Yifan Wang, Jichen Zheng, Jingyuan Sun, Yunhao Zhang, Chunyu Ye",
      "is_highlight": false,
      "score": 62.0,
      "summary": "This study presents a framework for simulating aphasia in language models by selectively perturbing components, revealing clinically relevant language impairments and enhancing understanding of language function degradation.",
      "debug_abstract": "Large language models (LLMs) increasingly exhibit human-like linguistic behaviors and internal representations that they could serve as computational simulators of language cognition. We ask whether LLMs can be systematically manipulated to reproduce language-production impairments characteristic of aphasia following focal brain lesions. Such models could provide scalable proxies for testing rehabilitation hypotheses, and offer a controlled framework for probing the functional organization of language. We introduce a clinically grounded, component-level framework that simulates aphasia by selectively perturbing functional components in LLMs, and apply it to both modular Mixture-of-Experts models and dense Transformers using a unified intervention interface. Our pipeline (i) identifies subtype-linked components for Broca&#39;s and Wernicke&#39;s aphasia, (ii) interprets these components with linguistic probing tasks, and (iii) induces graded impairments by progressively perturbing the top-k subtype-linked components, evaluating outcomes with Western Aphasia Battery (WAB) subtests summarized by Aphasia Quotient (AQ). Across architectures and lesioning strategies, subtype-targeted perturbations yield more systematic, aphasia-like regressions than size-matched random perturbations, and MoE modularity supports more localized and interpretable phenotype-to-component mappings. These findings suggest that modular LLMs, combined with clinically informed component perturbations, provide a promising platform for simulating aphasic language production and studying how distinct language functions degrade under targeted disruptions."
    },
    {
      "title": "EgoHandICL: Egocentric 3D Hand Reconstruction with In-Context Learning",
      "url": "https://arxiv.org/abs/2601.19850",
      "date": "2026-01-27",
      "authors_text": "Binzhu Xie, Shi Qiu, Sicheng Zhang, Yinqiao Wang, Hao Xu",
      "is_highlight": false,
      "score": 76.0,
      "summary": "EgoHandICL introduces an innovative in-context learning framework for robust 3D hand reconstruction in egocentric vision, enhancing performance through exemplar retrieval and multimodal context integration.",
      "debug_abstract": "Robust 3D hand reconstruction in egocentric vision is challenging due to depth ambiguity, self-occlusion, and complex hand-object interactions. Prior methods mitigate these issues by scaling training data or adding auxiliary cues, but they often struggle in unseen contexts. We present EgoHandICL, the first in-context learning (ICL) framework for 3D hand reconstruction that improves semantic alignment, visual consistency, and robustness under challenging egocentric conditions. EgoHandICL introduces complementary exemplar retrieval guided by vision-language models (VLMs), an ICL-tailored tokenizer for multimodal context, and a masked autoencoder (MAE)-based architecture trained with hand-guided geometric and perceptual objectives. Experiments on ARCTIC and EgoExo4D show consistent gains over state-of-the-art methods. We also demonstrate real-world generalization and improve EgoVLM hand-object interaction reasoning by using reconstructed hands as visual prompts. Code and data: this https URL"
    },
    {
      "title": "Advances and Innovations in the Multi-Agent Robotic System (MARS) Challenge",
      "url": "https://arxiv.org/abs/2601.18733",
      "date": "2026-01-26",
      "authors_text": "Li Kang, Heng Zhou, Xiufeng Song, Rui Li, Bruno N. Y. Chen",
      "is_highlight": false,
      "score": 94.0,
      "summary": "The MARS Challenge promotes advancements in multi-agent robotic systems by exploring vision-language models for collaborative task execution and planning in dynamic environments.",
      "debug_abstract": "Recent advancements in multimodal large language models and vision-languageaction models have significantly driven progress in Embodied AI. As the field transitions toward more complex task scenarios, multi-agent system frameworks are becoming essential for achieving scalable, efficient, and collaborative solutions. This shift is fueled by three primary factors: increasing agent capabilities, enhancing system efficiency through task delegation, and enabling advanced human-agent interactions. To address the challenges posed by multi-agent collaboration, we propose the Multi-Agent Robotic System (MARS) Challenge, held at the NeurIPS 2025 Workshop on SpaVLE. The competition focuses on two critical areas: planning and control, where participants explore multi-agent embodied planning using vision-language models (VLMs) to coordinate tasks and policy execution to perform robotic manipulation in dynamic environments. By evaluating solutions submitted by participants, the challenge provides valuable insights into the design and coordination of embodied multi-agent systems, contributing to the future development of advanced collaborative AI systems."
    },
    {
      "title": "Fast and Safe Trajectory Optimization for Mobile Manipulators With Neural Configuration Space Distance Field",
      "url": "https://arxiv.org/abs/2601.18548",
      "date": "2026-01-26",
      "authors_text": "Yulin Li, Zhiyuan Song, Yiming Li, Zhicheng Song, Kai Chen",
      "is_highlight": false,
      "score": 86.0,
      "summary": "This paper introduces Generalized Configuration Space Distance Fields (GCDF) for efficient trajectory optimization in mobile manipulators, enhancing collision reasoning and enabling agile motion in complex environments.",
      "debug_abstract": "Mobile manipulators promise agile, long-horizon behavior by coordinating base and arm motion, yet whole-body trajectory optimization in cluttered, confined spaces remains difficult due to high-dimensional nonconvexity and the need for fast, accurate collision reasoning. Configuration Space Distance Fields (CDF) enable fixed-base manipulators to model collisions directly in configuration space via smooth, implicit distances. This representation holds strong potential to bypass the nonlinear configuration-to-workspace mapping while preserving accurate whole-body geometry and providing optimization-friendly collision costs. Yet, extending this capability to mobile manipulators is hindered by unbounded workspaces and tighter base-arm coupling. We lift this promise to mobile manipulation with Generalized Configuration Space Distance Fields (GCDF), extending CDF to robots with both translational and rotational joints in unbounded workspaces with tighter base-arm coupling. We prove that GCDF preserves Euclidean-like local distance structure and accurately encodes whole-body geometry in configuration space, and develop a data generation and training pipeline that yields continuous neural GCDFs with accurate values and gradients, supporting efficient GPU-batched queries. Building on this representation, we develop a high-performance sequential convex optimization framework centered on GCDF-based collision reasoning. The solver scales to large numbers of implicit constraints through (i) online specification of neural constraints, (ii) sparsity-aware active-set detection with parallel batched evaluation across thousands of constraints, and (iii) incremental constraint management for rapid replanning under scene changes."
    },
    {
      "title": "Resisting Manipulative Bots in Meme Coin Copy Trading: A Multi-Agent Approach with Chain-of-Thought Reasoning",
      "url": "https://arxiv.org/abs/2601.08641",
      "date": "2026-01-26",
      "authors_text": "Yichen Luo, Yebo Feng, Jiahua Xu, Yang Liu",
      "is_highlight": false,
      "score": 67.0,
      "summary": "This paper presents a multi-agent system utilizing explainable LLMs to defend against manipulative bots in meme coin copy trading, enhancing prediction accuracy and economic performance.",
      "debug_abstract": "Copy trading has become the dominant entry strategy in meme coin markets. However, due to the market&#39;s extreme illiquid and volatile nature, the strategy exposes an exploitable attack surface: adversaries deploy manipulative bots to front-run trades, conceal positions, and fabricate sentiment, systematically extracting value from naïve copiers at scale. Despite its prevalence, bot-driven manipulation remains largely unexplored, and no robust defensive framework exists. We propose a manipulation-resistant copy-trading system based on a multi-agent architecture powered by a multi-modal, explainable large language model (LLM). Our system decomposes copy trading into three specialized agents for coin evaluation, wallet selection, and timing assessment. Evaluated on historical data from over 6,000 meme coins, our approach outperforms zero-shot and most statistic-driven baselines in prediction accuracy as well as all baselines in economic performance, achieving an average return of 14% for identified smart-money trades and an estimated copier return of 3% per trade under realistic market frictions. Overall, our results demonstrate the effectiveness of agent-based defenses and predictability of trader profitability in adversarial meme coin markets, providing a practical foundation for robust copy trading."
    },
    {
      "title": "AdaReasoner: Dynamic Tool Orchestration for Iterative Visual Reasoning",
      "url": "https://arxiv.org/abs/2601.18631",
      "date": "2026-01-26",
      "authors_text": "Mingyang Song, Haoyu Sun, Jiawei Gu, Linjie Li, Luxin Xu",
      "is_highlight": false,
      "score": 61.0,
      "summary": "AdaReasoner is a multimodal model that autonomously optimizes tool selection and usage for iterative visual reasoning, achieving state-of-the-art performance on various benchmarks.",
      "debug_abstract": "When humans face problems beyond their immediate capabilities, they rely on tools, providing a promising paradigm for improving visual reasoning in multimodal large language models (MLLMs). Effective reasoning, therefore, hinges on knowing which tools to use, when to invoke them, and how to compose them over multiple steps, even when faced with new tools or new tasks. We introduce \\textbf{AdaReasoner}, a family of multimodal models that learn tool use as a general reasoning skill rather than as tool-specific or explicitly supervised behavior. AdaReasoner is enabled by (i) a scalable data curation pipeline exposing models to long-horizon, multi-step tool interactions; (ii) Tool-GRPO, a reinforcement learning algorithm that optimizes tool selection and sequencing based on end-task success; and (iii) an adaptive learning mechanism that dynamically regulates tool usage. Together, these components allow models to infer tool utility from task context and intermediate outcomes, enabling coordination of multiple tools and generalization to unseen tools. Empirically, AdaReasoner exhibits strong tool-adaptive and generalization behaviors: it autonomously adopts beneficial tools, suppresses irrelevant ones, and adjusts tool usage frequency based on task demands, despite never being explicitly trained to do so. These capabilities translate into state-of-the-art performance across challenging benchmarks, improving the 7B base model by +24.9\\% on average and surpassing strong proprietary systems such as GPT-5 on multiple tasks, including VSP and Jigsaw."
    },
    {
      "title": "Timely Machine: Awareness of Time Makes Test-Time Scaling Agentic",
      "url": "https://arxiv.org/abs/2601.16486",
      "date": "2026-01-23",
      "authors_text": "Yichuan Ma, Linyang Li, Yongkang chen, Peiji Li, Xiaozhe Li",
      "is_highlight": false,
      "score": 78.0,
      "summary": "The paper introduces Timely Machine, redefining test-time scaling for LLMs based on wall-clock time, enhancing performance through dynamic strategy adjustments and reinforcement learning.",
      "debug_abstract": "As large language models (LLMs) increasingly tackle complex reasoning tasks, test-time scaling has become critical for enhancing capabilities. However, in agentic scenarios with frequent tool calls, the traditional generation-length-based definition breaks down: tool latency decouples inference time from generation length. We propose Timely Machine, redefining test-time as wall-clock time, where models dynamically adjust strategies based on time budgets. We introduce Timely-Eval, a benchmark spanning high-frequency tool calls, low-frequency tool calls, and time-constrained reasoning. By varying tool latency, we find smaller models excel with fast feedback through more interactions, while larger models dominate high-latency settings via superior interaction quality. Moreover, existing models fail to adapt reasoning to time budgets. We propose Timely-RL to address this gap. After cold-start supervised fine-tuning, we use reinforcement learning to enhance temporal planning. Timely-RL improves time budget awareness and consistently boosts performance across Timely-Eval. We hope our work offers a new perspective on test-time scaling for the agentic era."
    },
    {
      "title": "Adaptive Reinforcement and Model Predictive Control Switching for Safe Human-Robot Cooperative Navigation",
      "url": "https://arxiv.org/abs/2601.16686",
      "date": "2026-01-23",
      "authors_text": "Ning Liu, Sen Shen, Zheng Li, Matthew D'Souza, Jen Jen Chung",
      "is_highlight": false,
      "score": 94.0,
      "summary": "The paper presents ARMS, a hybrid control framework combining reinforcement learning and model predictive control for safe human-robot navigation, achieving superior performance in cluttered environments.",
      "debug_abstract": "This paper addresses the challenge of human-guided navigation for mobile collaborative robots under simultaneous proximity regulation and safety constraints. We introduce Adaptive Reinforcement and Model Predictive Control Switching (ARMS), a hybrid learning-control framework that integrates a reinforcement learning follower trained with Proximal Policy Optimization (PPO) and an analytical one-step Model Predictive Control (MPC) formulated as a quadratic program safety filter. To enable robust perception under partial observability and non-stationary human motion, ARMS employs a decoupled sensing architecture with a Long Short-Term Memory (LSTM) temporal encoder for the human-robot relative state and a spatial encoder for 360-degree LiDAR scans. The core contribution is a learned adaptive neural switcher that performs context-aware soft action fusion between the two controllers, favoring conservative, constraint-aware QP-based control in low-risk regions while progressively shifting control authority to the learned follower in highly cluttered or constrained scenarios where maneuverability is critical, and reverting to the follower action when the QP becomes infeasible. Extensive evaluations against Pure Pursuit, Dynamic Window Approach (DWA), and an RL-only baseline demonstrate that ARMS achieves an 82.5 percent success rate in highly cluttered environments, outperforming DWA and RL-only approaches by 7.1 percent and 3.1 percent, respectively, while reducing average computational latency by 33 percent to 5.2 milliseconds compared to a multi-step MPC baseline. Additional simulation transfer in Gazebo and initial real-world deployment results further indicate the practicality and robustness of ARMS for safe and efficient human-robot collaboration. Source code and a demonstration video are available at this https URL."
    },
    {
      "title": "Natural Language-Driven Global Mapping of Martian Landforms",
      "url": "https://arxiv.org/abs/2601.15949",
      "date": "2026-01-22",
      "authors_text": "Yiran Wang, Shuoyuan Wang, Zhaoran Wei, Jiannan Zhao, Zhonghua Yao",
      "is_highlight": false,
      "score": 72.0,
      "summary": "MarScope is a vision-language framework that enables natural language-driven, label-free mapping of Martian landforms, enhancing global geomorphic analysis and user query flexibility.",
      "debug_abstract": "Planetary surfaces are typically analyzed using high-level semantic concepts in natural language, yet vast orbital image archives remain organized at the pixel level. This mismatch limits scalable, open-ended exploration of planetary surfaces. Here we present MarScope, a planetary-scale vision-language framework enabling natural language-driven, label-free mapping of Martian landforms. MarScope aligns planetary images and text in a shared semantic space, trained on over 200,000 curated image-text pairs. This framework transforms global geomorphic mapping on Mars by replacing pre-defined classifications with flexible semantic retrieval, enabling arbitrary user queries across the entire planet in 5 seconds with F1 scores up to 0.978. Applications further show that it extends beyond morphological classification to facilitate process-oriented analysis and similarity-based geomorphological mapping at a planetary scale. MarScope establishes a new paradigm where natural language serves as a direct interface for scientific discovery over massive geospatial datasets."
    },
    {
      "title": "Robust Tool Use via Fission-GRPO: Learning to Recover from Execution Errors",
      "url": "https://arxiv.org/abs/2601.15625",
      "date": "2026-01-22",
      "authors_text": "Zhiwei Zhang, Fei Zhao, Rui Wang, Zezhong Wang, Bin Liang",
      "is_highlight": false,
      "score": 87.1,
      "summary": "Fission-GRPO enhances tool use in LLMs by converting execution errors into corrective supervision, improving error recovery and overall accuracy in multi-turn interactions.",
      "debug_abstract": "Large language models (LLMs) can call tools effectively, yet they remain brittle in multi-turn execution: following a tool call error, smaller models often degenerate into repetitive invalid re-invocations, failing to interpret error feedback and self-correct. This brittleness hinders reliable real-world deployment, where the execution errors are inherently inevitable during tool interaction procedures. We identify a key limitation of current approaches: standard reinforcement learning (RL) treats errors as sparse negative rewards, providing no guidance on how to recover, while pre-collected synthetic error-correction datasets suffer from distribution mismatch with the model&#39;s on-policy error modes. To bridge this gap, we propose Fission-GRPO, a framework that converts execution errors into corrective supervision within the RL training loop. Our core mechanism fissions each failed trajectory into a new training instance by augmenting it with diagnostic feedback from a finetuned Error Simulator, then resampling recovery rollouts on-policy. This enables the model to learn from the precise errors it makes during exploration, rather than from static, pre-collected error cases. On the BFCL v4 Multi-Turn, Fission-GRPO improves the error recovery rate of Qwen3-8B by 5.7% absolute, crucially, yielding a 4% overall accuracy gain (42.75% to 46.75%) over GRPO and outperforming specialized tool-use agents."
    },
    {
      "title": "PhysicsMind: Sim and Real Mechanics Benchmarking for Physical Reasoning and Prediction in Foundational VLMs and World Models",
      "url": "https://arxiv.org/abs/2601.16007",
      "date": "2026-01-22",
      "authors_text": "Chak-Wing Mak, Guanyu Zhu, Boyi Zhang, Hongji Li, Xiaowei Chi",
      "is_highlight": false,
      "score": 94.1,
      "summary": "PhysicsMind introduces a benchmark for evaluating physical reasoning in multimodal models through real and simulated tasks focused on fundamental physics principles, revealing significant gaps in current models' understanding.",
      "debug_abstract": "Modern foundational Multimodal Large Language Models (MLLMs) and video world models have advanced significantly in mathematical, common-sense, and visual reasoning, but their grasp of the underlying physics remains underexplored. Existing benchmarks attempting to measure this matter rely on synthetic, Visual Question Answer templates or focus on perceptual video quality that is tangential to measuring how well the video abides by physical laws. To address this fragmentation, we introduce PhysicsMind, a unified benchmark with both real and simulation environments that evaluates law-consistent reasoning and generation over three canonical principles: Center of Mass, Lever Equilibrium, and Newton&#39;s First Law. PhysicsMind comprises two main tasks: i) VQA tasks, testing whether models can reason and determine physical quantities and values from images or short videos, and ii) Video Generation(VG) tasks, evaluating if predicted motion trajectories obey the same center-of-mass, torque, and inertial constraints as the ground truth. A broad range of recent models and video generation models is evaluated on PhysicsMind and found to rely on appearance heuristics while often violating basic mechanics. These gaps indicate that current scaling and training are still insufficient for robust physical understanding, underscoring PhysicsMind as a focused testbed for physics-aware multimodal models. Our data will be released upon acceptance."
    }
  ],
  "Language and Vision (LaVi) Lab": [
    {
      "title": "Large Language Models Can Take False First Steps at Inference-time Planning",
      "url": "https://arxiv.org/abs/2602.02991",
      "date": "2026-02-03",
      "authors_text": "Haijiang Yan, Jian-Qiao Zhu, Adam Sanborn",
      "is_highlight": false,
      "score": 73.0,
      "summary": "The paper explains that large language models exhibit inconsistent planning at inference due to evolving self-generated context, supported by experiments demonstrating planning shifts and reduced biases.",
      "teaser_image": "https://arxiv.org/html/2602.02991/images/llama_regression.png",
      "debug_abstract": "Large language models (LLMs) have been shown to acquire sequence-level planning abilities during training, yet their planning behavior exhibited at inference time often appears short-sighted and inconsistent with these capabilities. We propose a Bayesian account for this gap by grounding planning behavior in the evolving generative context: given the subtle differences between natural language and the language internalized by LLMs, accumulated self-generated context drives a planning-shift during inference and thereby creates the appearance of compromised planning behavior. We further validate the proposed model through two controlled experiments: a random-generation task demonstrating constrained planning under human prompts and increasing planning strength as self-generated context accumulates, and a Gaussian-sampling task showing reduced initial bias when conditioning on self-generated sequences. These findings provide a theoretical explanation along with empirical evidence for characterizing how LLMs plan ahead during inference."
    },
    {
      "title": "Search-R2: Enhancing Search-Integrated Reasoning via Actor-Refiner Collaboration",
      "url": "https://arxiv.org/abs/2602.03647",
      "date": "2026-02-03",
      "authors_text": "Bowei He, Minda Hu, Zenan Xu, Hongru Wang, Licheng Zong",
      "is_highlight": false,
      "score": 52.0,
      "summary": "Search-R2 introduces an Actor-Refiner framework that enhances search-integrated reasoning in language agents through targeted intervention and fine-grained supervision, outperforming existing methods.",
      "teaser_image": "https://arxiv.org/html/2602.03647/x2.png",
      "debug_abstract": "Search-integrated reasoning enables language agents to transcend static parametric knowledge by actively querying external sources. However, training these agents via reinforcement learning is hindered by the multi-scale credit assignment problem: existing methods typically rely on sparse, trajectory-level rewards that fail to distinguish between high-quality reasoning and fortuitous guesses, leading to redundant or misleading search behaviors. To address this, we propose Search-R2, a novel Actor-Refiner collaboration framework that enhances reasoning through targeted intervention, with both components jointly optimized during training. Our approach decomposes the generation process into an Actor, which produces initial reasoning trajectories, and a Meta-Refiner, which selectively diagnoses and repairs flawed steps via a &#39;cut-and-regenerate&#39; mechanism. To provide fine-grained supervision, we introduce a hybrid reward design that couples outcome correctness with a dense process reward quantifying the information density of retrieved evidence. Theoretically, we formalize the Actor-Refiner interaction as a smoothed mixture policy, proving that selective correction yields strict performance gains over strong baselines. Extensive experiments across various general and multi-hop QA datasets demonstrate that Search-R2 consistently outperforms strong RAG and RL-based baselines across model scales, achieving superior reasoning accuracy with minimal overhead."
    },
    {
      "title": "Dual Quaternion SE(3) Synchronization with Recovery Guarantees",
      "url": "https://arxiv.org/abs/2602.00324",
      "date": "2026-01-30",
      "authors_text": "Jianing Zhao, Linglingzhi Zhu, Anthony Man-Cho So",
      "is_highlight": false,
      "score": 75.0,
      "summary": "This paper presents a dual quaternion-based SE(3) synchronization method with a two-stage algorithm that guarantees recovery of absolute poses from noisy transformations, outperforming traditional matrix methods.",
      "teaser_image": "https://arxiv.org/html/2602.00324/x2.png",
      "debug_abstract": "Synchronization over the special Euclidean group SE(3) aims to recover absolute poses from noisy pairwise relative transformations and is a core primitive in robotics and 3D vision. Standard approaches often require multi-step heuristic procedures to recover valid poses, which are difficult to analyze and typically lack theoretical guarantees. This paper adopts a dual quaternion representation and formulates SE(3) synchronization directly over the unit dual quaternion. A two-stage algorithm is developed: A spectral initializer computed via the power method on a Hermitian dual quaternion measurement matrix, followed by a dual quaternion generalized power method (DQGPM) that enforces feasibility through per-iteration projection. The estimation error bounds are established for spectral estimators, and DQGPM is shown to admit a finite-iteration error bound and achieves linear error contraction up to an explicit noise-dependent threshold. Experiments on synthetic benchmarks and real-world multi-scan point-set registration demonstrate that the proposed pipeline improves both accuracy and efficiency over representative matrix-based methods."
    },
    {
      "title": "Offline Discovery of Interpretable Skills from Multi-Task Trajectories",
      "url": "https://arxiv.org/abs/2602.01018",
      "date": "2026-02-01",
      "authors_text": "Chongyu Zhu, Mithun Vanniasinghe, Jiayu Chen, Chi-Guhn Lee",
      "is_highlight": false,
      "score": 77.0,
      "summary": "LOKI is a three-stage framework for offline skill discovery and hierarchical imitation learning, effectively extracting interpretable skills from multi-task trajectories without explicit rewards.",
      "teaser_image": "https://arxiv.org/html/2602.01018/fig/two_stage_skill_learning.png",
      "debug_abstract": "Hierarchical Imitation Learning is a powerful paradigm for acquiring complex robot behaviors from demonstrations. A central challenge, however, lies in discovering reusable skills from long-horizon, multi-task offline data, especially when the data lacks explicit rewards or subtask annotations. In this work, we introduce LOKI, a three-stage end-to-end learning framework designed for offline skill discovery and hierarchical imitation. The framework commences with a two-stage, weakly supervised skill discovery process: Stage one performs coarse, task-aware macro-segmentation by employing an alignment-enforced Vector Quantized VAE guided by weak task labels. Stage two then refines these segments at a micro-level using a self-supervised sequential model, followed by an iterative clustering process to consolidate skill boundaries. The third stage then leverages these precise boundaries to construct a hierarchical policy within an option-based framework-complete with a learned termination condition beta for explicit skill switching. LOKI achieves high success rates on the challenging D4RL Kitchen benchmark and outperforms standard HIL baselines. Furthermore, we demonstrate that the discovered skills are semantically meaningful, aligning with human intuition, and exhibit compositionality by successfully sequencing them to solve a novel, unseen task."
    },
    {
      "title": "Residual Decoding: Mitigating Hallucinations in Large Vision-Language Models via History-Aware Residual Guidance",
      "url": "https://arxiv.org/abs/2602.01047",
      "date": "2026-02-01",
      "authors_text": "Xinrong Chen, Xu Chu, Yingmin Qiu, Hengyuan Zhang, Jing Xiong",
      "is_highlight": false,
      "score": 53.0,
      "summary": "Residual Decoding (ResDec) mitigates hallucinations in Large Vision-Language Models by leveraging historical information to enhance decoding and improve visual grounding.",
      "teaser_image": "https://arxiv.org/html/2602.01047/x1.png",
      "debug_abstract": "Large Vision-Language Models (LVLMs) can reason effectively from image-text inputs and perform well in various multimodal tasks. Despite this success, they are affected by language priors and often produce hallucinations. Hallucinations denote generated content that is grammatically and syntactically coherent, yet bears no match or direct relevance to actual visual input. To address this problem, we propose Residual Decoding (ResDec). It is a novel training-free method that uses historical information to aid decoding. The method relies on the internal implicit reasoning mechanism and token logits evolution mechanism of LVLMs to correct biases. Extensive experiments demonstrate that ResDec effectively suppresses hallucinations induced by language priors, significantly improves visual grounding, and reduces object hallucinations. In addition to mitigating hallucinations, ResDec also performs exceptionally well on comprehensive LVLM benchmarks, highlighting its broad applicability."
    },
    {
      "title": "MedAD-R1: Eliciting Consistent Reasoning in Interpretible Medical Anomaly Detection via Consistency-Reinforced Policy Optimization",
      "url": "https://arxiv.org/abs/2602.01081",
      "date": "2026-02-01",
      "authors_text": "Haitao Zhang, Yingying Wang, Jiaxiang Wang, Haote Xu, Hongyang Zhang",
      "is_highlight": false,
      "score": 64.0,
      "summary": "MedAD-R1 enhances medical anomaly detection by utilizing a two-stage training framework with consistency-reinforced policy optimization, achieving state-of-the-art performance on the MedAD-38K benchmark.",
      "teaser_image": "https://arxiv.org/html/2602.01081/x3.png",
      "debug_abstract": "Medical Anomaly Detection (MedAD) presents a significant opportunity to enhance diagnostic accuracy using Large Multimodal Models (LMMs) to interpret and answer questions based on medical images. However, the reliance on Supervised Fine-Tuning (SFT) on simplistic and fragmented datasets has hindered the development of models capable of plausible reasoning and robust multimodal generalization. To overcome this, we introduce MedAD-38K, the first large-scale, multi-modal, and multi-center benchmark for MedAD featuring diagnostic Chain-of-Thought (CoT) annotations alongside structured Visual Question-Answering (VQA) pairs. On this foundation, we propose a two-stage training framework. The first stage, Cognitive Injection, uses SFT to instill foundational medical knowledge and align the model with a structured think-then-answer paradigm. Given that standard policy optimization can produce reasoning that is disconnected from the final answer, the second stage incorporates Consistency Group Relative Policy Optimization (Con-GRPO). This novel algorithm incorporates a crucial consistency reward to ensure the generated reasoning process is relevant and logically coherent with the final diagnosis. Our proposed model, MedAD-R1, achieves state-of-the-art (SOTA) performance on the MedAD-38K benchmark, outperforming strong baselines by more than 10\\%. This superior performance stems from its ability to generate transparent and logically consistent reasoning pathways, offering a promising approach to enhancing the trustworthiness and interpretability of AI for clinical decision support."
    },
    {
      "title": "Adaptive Rollout Allocation for Online Reinforcement Learning with Verifiable Rewards",
      "url": "https://arxiv.org/abs/2602.01601",
      "date": "2026-02-02",
      "authors_text": "Hieu Trung Nguyen, Bao Nguyen, Wenao Ma, Yuzhi Zhao, Ruifeng She",
      "is_highlight": false,
      "score": 40.0,
      "summary": "The paper presents \\Ours, a variance-informed strategy for adaptive rollout allocation in online reinforcement learning, enhancing sampling efficiency and performance over traditional methods.",
      "teaser_image": "https://arxiv.org/html/2602.01601/x2.png",
      "debug_abstract": "Sampling efficiency is a key bottleneck in reinforcement learning with verifiable rewards. Existing group-based policy optimization methods, such as GRPO, allocate a fixed number of rollouts for all training prompts. This uniform allocation implicitly treats all prompts as equally informative, and could lead to inefficient computational budget usage and impede training progress. We introduce \\Ours, a Variance-Informed Predictive allocation strategy that allocates a given rollout budget to the prompts in the incumbent batch to minimize the expected gradient variance of the policy update. At each iteration, \\Ours~uses a lightweight Gaussian process model to predict per-prompt success probabilities based on recent rollouts. These probability predictions are translated into variance estimates, which are then fed into a convex optimization problem to determine the optimal rollout allocations under a hard compute budget constraint. Empirical results show that \\Ours~consistently improves sampling efficiency and achieves higher performance than uniform or heuristic allocation strategies in multiple benchmarks. Our code will be available at this https URL."
    },
    {
      "title": "Reasoning with Autoregressive-Diffusion Collaborative Thoughts",
      "url": "https://arxiv.org/abs/2602.01608",
      "date": "2026-02-02",
      "authors_text": "Mu Yuan, Liekang Zeng, Guoliang Xing, Lan Zhang, Yunhao Liu",
      "is_highlight": false,
      "score": 41.0,
      "summary": "The paper presents Collaborative Thoughts, a framework integrating autoregressive and diffusion models for enhanced spatial reasoning and generation through iterative feedback and joint reasoning.",
      "teaser_image": "https://arxiv.org/html/2602.01608/x2.png",
      "debug_abstract": "Autoregressive and diffusion models represent two complementary generative paradigms. Autoregressive models excel at sequential planning and constraint composition, yet struggle with tasks that require explicit spatial or physical grounding. Diffusion models, in contrast, capture rich spatial structure through high-dimensional generation, but lack the stepwise logical control needed to satisfy complex, multi-stage constraints or to reliably identify and correct errors. We introduce Collaborative Thoughts, a unified collaborative framework that enables autoregressive and diffusion models to reason and generate jointly through a closed-loop interaction. In Collaborative Thoughts, autoregressive models perform structured planning and constraint management, diffusion models instantiate these constraints as intermediate visual thoughts, and a vision-based critic module evaluates whether the visual thoughts satisfy the intended structural and physical requirements. This feedback is then used to iteratively refine subsequent planning and generation steps, mitigating error propagation across modalities. Importantly, Collaborative Thoughts uses the same collaborative loop regardless of whether the task is autoregressive question answering or diffusion-based visual generation. Through representative examples, we illustrate how Collaborative Thoughts can improve the reliability of spatial reasoning and the controllability of generation."
    },
    {
      "title": "Tilt-Ropter: A Novel Hybrid Aerial and Terrestrial Vehicle with Tilt Rotors and Passive Wheels",
      "url": "https://arxiv.org/abs/2602.01700",
      "date": "2026-02-02",
      "authors_text": "Ruoyu Wang, Xuchen Liu, Zongzhou Wu, Zixuan Guo, Wendi Ding",
      "is_highlight": false,
      "score": 38.0,
      "summary": "The Tilt-Ropter is a hybrid aerial-terrestrial vehicle that utilizes tilt rotors and passive wheels for efficient locomotion, featuring advanced control systems for enhanced mobility and energy savings.",
      "teaser_image": "https://arxiv.org/html/2602.01700/x2.png",
      "debug_abstract": "In this work, we present Tilt-Ropter, a novel hybrid aerial-terrestrial vehicle (HATV) that combines tilt rotors with passive wheels to achieve energy-efficient multi-mode locomotion. Unlike existing under-actuated HATVs, the fully actuated design of Tilt-Ropter enables decoupled force and torque control, greatly enhancing its mobility and environmental adaptability. A nonlinear model predictive controller (NMPC) is developed to track reference trajectories and handle contact constraints across locomotion modes, while a dedicated control allocation module exploits actuation redundancy to achieve energy-efficient control of actuators. Additionally, to enhance robustness during ground contact, we introduce an external wrench estimation algorithm that estimates environmental interaction forces and torques in real time. The system is validated through both simulation and real-world experiments, including seamless air-ground transitions and trajectory tracking. Results show low tracking errors in both modes and highlight a 92.8% reduction in power consumption during ground locomotion, demonstrating the system&#39;s potential for long-duration missions across large-scale and energy-constrained environments."
    },
    {
      "title": "Breaking the Static Graph: Context-Aware Traversal for Robust Retrieval-Augmented Generation",
      "url": "https://arxiv.org/abs/2602.01965",
      "date": "2026-02-02",
      "authors_text": "Kwun Hang Lau, Fangyuan Zhang, Boyu Ruan, Yingli Zhou, Qintian Guo",
      "is_highlight": false,
      "score": 27.0,
      "summary": "CatRAG enhances Retrieval-Augmented Generation by transforming static Knowledge Graphs into query-adaptive structures, improving multi-hop evidence retrieval and reasoning completeness.",
      "teaser_image": "https://arxiv.org/html/2602.01965/image/HubBiasShift.png",
      "debug_abstract": "Recent advances in Retrieval-Augmented Generation (RAG) have shifted from simple vector similarity to structure-aware approaches like HippoRAG, which leverage Knowledge Graphs (KGs) and Personalized PageRank (PPR) to capture multi-hop dependencies. However, these methods suffer from a &#34;Static Graph Fallacy&#34;: they rely on fixed transition probabilities determined during indexing. This rigidity ignores the query-dependent nature of edge relevance, causing semantic drift where random walks are diverted into high-degree &#34;hub&#34; nodes before reaching critical downstream evidence. Consequently, models often achieve high partial recall but fail to retrieve the complete evidence chain required for multi-hop queries. To address this, we propose CatRAG, Context-Aware Traversal for robust RAG, a framework that builds on the HippoRAG 2 architecture and transforms the static KG into a query-adaptive navigation structure. We introduce a multi-faceted framework to steer the random walk: (1) Symbolic Anchoring, which injects weak entity constraints to regularize the random walk; (2) Query-Aware Dynamic Edge Weighting, which dynamically modulates graph structure, to prune irrelevant paths while amplifying those aligned with the query&#39;s intent; and (3) Key-Fact Passage Weight Enhancement, a cost-efficient bias that structurally anchors the random walk to likely evidence. Experiments across four multi-hop benchmarks demonstrate that CatRAG consistently outperforms state of the art baselines. Our analysis reveals that while standard Recall metrics show modest gains, CatRAG achieves substantial improvements in reasoning completeness, the capacity to recover the entire evidence path without gaps. These results reveal that our approach effectively bridges the gap between retrieving partial context and enabling fully grounded reasoning. Resources are available at this https URL."
    },
    {
      "title": "TIDE: Trajectory-based Diagnostic Evaluation of Test-Time Improvement in LLM Agents",
      "url": "https://arxiv.org/abs/2602.02196",
      "date": "2026-02-02",
      "authors_text": "Hang Yan, Xinyu Che, Fangzhi Xu, Qiushi Sun, Zichen Ding",
      "is_highlight": false,
      "score": 20.0,
      "summary": "The TIDE framework evaluates Test-Time Improvement in LLM agents by analyzing task completion dynamics, recursive behaviors, and memory constraints to enhance performance optimization.",
      "teaser_image": "https://arxiv.org/html/2602.02196/x1.png",
      "debug_abstract": "Recent advances in autonomous LLM agents demonstrate their ability to improve performance through iterative interaction with the environment. We define this paradigm as Test-Time Improvement (TTI). However, the mechanisms under how and why TTI succeed or fail remain poorly understood, and existing evaluation metrics fail to capture their task optimization efficiency, behavior adaptation after erroneous actions, and the specific utility of working memory for task completion. To address these gaps, we propose Test-time Improvement Diagnostic Evaluation (TIDE), an agent-agnostic and environment-agnostic framework that decomposes TTI into three comprehensive and interconnected dimensions. The framework measures (1) the overall temporal dynamics of task completion and (2) identifies whether performance is primarily constrained by recursive looping behaviors or (3) by burdensome accumulated memory. Through extensive experiments across diverse agents and environments, TIDE highlights that improving agent performance requires more than scaling internal reasoning, calling for explicitly optimizing the interaction dynamics between the agent and the environment."
    },
    {
      "title": "Towards Agentic Intelligence for Materials Science",
      "url": "https://arxiv.org/abs/2602.00169",
      "date": "2026-01-29",
      "authors_text": "Huan Zhang, Yizhan Li, Wenhao Huang, Ziyu Hou, Yu Song",
      "is_highlight": false,
      "score": 12.0,
      "summary": "This paper advocates for developing agentic AI systems in materials science that autonomously plan, act, and learn to enhance discovery processes beyond traditional models.",
      "teaser_image": "https://arxiv.org/html/2602.00169/x1.png",
      "debug_abstract": "The convergence of artificial intelligence and materials science presents a transformative opportunity, but achieving true acceleration in discovery requires moving beyond task-isolated, fine-tuned models toward agentic systems that plan, act, and learn across the full discovery loop. This survey advances a unique pipeline-centric view that spans from corpus curation and pretraining, through domain adaptation and instruction tuning, to goal-conditioned agents interfacing with simulation and experimental platforms. Unlike prior reviews, we treat the entire process as an end-to-end system to be optimized for tangible discovery outcomes rather than proxy benchmarks. This perspective allows us to trace how upstream design choices-such as data curation and training objectives-can be aligned with downstream experimental success through effective credit assignment. To bridge communities and establish a shared frame of reference, we first present an integrated lens that aligns terminology, evaluation, and workflow stages across AI and materials science. We then analyze the field through two focused lenses: From the AI perspective, the survey details LLM strengths in pattern recognition, predictive analytics, and natural language processing for literature mining, materials characterization, and property prediction; from the materials science perspective, it highlights applications in materials design, process optimization, and the acceleration of computational workflows via integration with external tools (e.g., DFT, robotic labs). Finally, we contrast passive, reactive approaches with agentic design, cataloging current contributions while motivating systems that pursue long-horizon goals with autonomy, memory, and tool use. This survey charts a practical roadmap towards autonomous, safety-aware LLM agents aimed at discovering novel and useful materials."
    },
    {
      "title": "Failure-Aware Bimanual Teleoperation via Conservative Value Guided Assistance",
      "url": "https://arxiv.org/abs/2602.01092",
      "date": "2026-02-01",
      "authors_text": "Peng Zhou, Zhongxuan Li, Jinsong Wu, Jiaming Qi, Jun Hu",
      "is_highlight": false,
      "score": 9.0,
      "summary": "This paper presents a failure-aware bimanual teleoperation framework that uses conservative value learning to enhance task success and reduce operator workload through compliant haptic assistance.",
      "teaser_image": "https://arxiv.org/html/2602.01092/x1.png",
      "debug_abstract": "Teleoperation of high-precision manipulation is con-strained by tight success tolerances and complex contact dy-namics, which make impending failures difficult for human operators to anticipate under partial observability. This paper proposes a value-guided, failure-aware framework for bimanual teleoperation that provides compliant haptic assistance while pre-serving continuous human authority. The framework is trained entirely from heterogeneous offline teleoperation data containing both successful and failed executions. Task feasibility is mod-eled as a conservative success score learned via Conservative Value Learning, yielding a risk-sensitive estimate that remains reliable under distribution shift. During online operation, the learned success score regulates the level of assistance, while a learned actor provides a corrective motion direction. Both are integrated through a joint-space impedance interface on the master side, yielding continuous guidance that steers the operator away from failure-prone actions without overriding intent. Experimental results on contact-rich manipulation tasks demonstrate improved task success rates and reduced operator workload compared to conventional teleoperation and shared-autonomy baselines, indicating that conservative value learning provides an effective mechanism for embedding failure awareness into bilateral teleoperation. Experimental videos are available at this https URL"
    },
    {
      "title": "FlowSteer: Interactive Agentic Workflow Orchestration via End-to-End Reinforcement Learning",
      "url": "https://arxiv.org/abs/2602.01664",
      "date": "2026-02-02",
      "authors_text": "Mingda Zhang, Haoran Luo, Tiesunlong Shen, Qika Lin, Xiaoying Tang",
      "is_highlight": false,
      "score": 7.0,
      "summary": "FlowSteer is an end-to-end reinforcement learning framework that automates workflow orchestration through interactive learning, addressing manual costs and improving performance across diverse tasks.",
      "teaser_image": "https://arxiv.org/html/2602.01664/x1.png",
      "debug_abstract": "In recent years, a variety of powerful agentic workflows have been applied to solve a wide range of human problems. However, existing workflow orchestration still faces key challenges, including high manual cost, reliance on specific operators/large language models (LLMs), and sparse reward signals. To address these challenges, we propose FlowSteer, an end-to-end reinforcement learning framework that takes a lightweight policy model as the agent and an executable canvas environment, automating workflow orchestration through multi-turn interaction. In this process, the policy model analyzes execution states and selects editing actions, while the canvas executes operators and returns feedback for iterative refinement. Moreover, FlowSteer provides a plug-and-play framework that supports diverse operator libraries and interchangeable LLM backends. To effectively train this interaction paradigm, we propose Canvas Workflow Relative Policy Optimization (CWRPO), which introduces diversity-constrained rewards with conditional release to stabilize learning and suppress shortcut behaviors. Experimental results on twelve datasets show that FlowSteer significantly outperforms baselines across various tasks."
    },
    {
      "title": "Diffusion Model-based Reinforcement Learning for Version Age of Information Scheduling: Average and Tail-Risk-Sensitive Control",
      "url": "https://arxiv.org/abs/2601.18069",
      "date": "2026-01-26",
      "authors_text": "Haoyuan Pan, Sizhao Chen, Zhaorui Wang, Tse-Tin Chan",
      "is_highlight": false,
      "score": 57.0,
      "summary": "This paper presents a novel reinforcement learning approach, RS-D3SAC, for optimizing Version Age of Information scheduling by minimizing both average and tail-risk-sensitive metrics in wireless systems.",
      "teaser_image": null,
      "debug_abstract": "Ensuring timely and semantically accurate information delivery is critical in real-time wireless systems. While Age of Information (AoI) quantifies temporal freshness, Version Age of Information (VAoI) captures semantic staleness by accounting for version evolution between transmitters and receivers. Existing VAoI scheduling approaches primarily focus on minimizing average VAoI, overlooking rare but severe staleness events that can compromise reliability under stochastic packet arrivals and unreliable channels. This paper investigates both average-oriented and tail-risk-sensitive VAoI scheduling in a multi-user status update system with long-term transmission cost constraints. We first formulate the average VAoI minimization problem as a constrained Markov decision process and introduce a deep diffusion-based Soft Actor-Critic (D2SAC) algorithm. By generating actions through a diffusion-based denoising process, D2SAC enhances policy expressiveness and establishes a strong baseline for mean performance. Building on this foundation, we put forth RS-D3SAC, a risk-sensitive deep distributional diffusion-based Soft Actor-Critic algorithm. RS-D3SAC integrates a diffusion-based actor with a quantile-based distributional critic, explicitly modeling the full VAoI return distribution. This enables principled tail-risk optimization via Conditional Value-at-Risk (CVaR) while satisfying long-term transmission cost constraints. Extensive simulations show that, while D2SAC reduces average VAoI, RS-D3SAC consistently achieves substantial reductions in CVaR without sacrificing mean performance. The dominant gain in tail-risk reduction stems from the distributional critic, with the diffusion-based actor providing complementary refinement to stabilize and enrich policy decisions, highlighting their effectiveness for robust and risk-aware VAoI scheduling in multi-user wireless systems."
    },
    {
      "title": "LoD-Structured 3D Gaussian Splatting for Streaming Video Reconstruction",
      "url": "https://arxiv.org/abs/2601.18475",
      "date": "2026-01-26",
      "authors_text": "Xinhui Liu, Can Wang, Lei Liu, Zhenghao Chen, Wei Jiang",
      "is_highlight": false,
      "score": 52.0,
      "summary": "The StreamLoD-GS framework enhances Streaming Free-Viewpoint Video reconstruction by optimizing 3D Gaussian Splatting for efficient, high-fidelity rendering with reduced storage needs.",
      "teaser_image": null,
      "debug_abstract": "Free-Viewpoint Video (FVV) reconstruction enables photorealistic and interactive 3D scene visualization; however, real-time streaming is often bottlenecked by sparse-view inputs, prohibitive training costs, and bandwidth constraints. While recent 3D Gaussian Splatting (3DGS) has advanced FVV due to its superior rendering speed, Streaming Free-Viewpoint Video (SFVV) introduces additional demands for rapid optimization, high-fidelity reconstruction under sparse constraints, and minimal storage footprints. To bridge this gap, we propose StreamLoD-GS, an LoD-based Gaussian Splatting framework designed specifically for SFVV. Our approach integrates three core innovations: 1) an Anchor- and Octree-based LoD-structured 3DGS with a hierarchical Gaussian dropout technique to ensure efficient and stable optimization while maintaining high-quality rendering; 2) a GMM-based motion partitioning mechanism that separates dynamic and static content, refining dynamic regions while preserving background stability; and 3) a quantized residual refinement framework that significantly reduces storage requirements without compromising visual fidelity. Extensive experiments demonstrate that StreamLoD-GS achieves competitive or state-of-the-art performance in terms of quality, efficiency, and storage."
    },
    {
      "title": "Heterogeneous Vertiport Selection Optimization for On-Demand Air Taxi Services: A Deep Reinforcement Learning Approach",
      "url": "https://arxiv.org/abs/2601.21316",
      "date": "2026-01-29",
      "authors_text": "Aoyu Pang, Maonan Wang, Zifan Sha, Wenwei Yue, Changle Li",
      "is_highlight": false,
      "score": 80.0,
      "summary": "The paper presents a deep reinforcement learning framework for optimizing vertiport selection and air taxi routing, achieving significant travel time reductions in urban air mobility systems.",
      "teaser_image": "https://arxiv.org/html/2601.21316/x2.png",
      "debug_abstract": "Urban Air Mobility (UAM) has emerged as a transformative solution to alleviate urban congestion by utilizing low-altitude airspace, thereby reducing pressure on ground transportation networks. To enable truly efficient and seamless door-to-door travel experiences, UAM requires close integration with existing ground transportation infrastructure. However, current research on optimal integrated routing strategies for passengers in air-ground mobility systems remains limited, with a lack of systematic this http URL address this gap, we first propose a unified optimization model that integrates strategy selection for both air and ground transportation. This model captures the dynamic characteristics of multimodal transport networks and incorporates real-time traffic conditions alongside passenger decision-making behavior. Building on this model, we propose a Unified Air-Ground Mobility Coordination (UAGMC) framework, which leverages deep reinforcement learning (RL) and Vehicle-to-Everything (V2X) communication to optimize vertiport selection and dynamically plan air taxi routes. Experimental results demonstrate that UAGMC achieves a 34\\% reduction in average travel time compared to conventional proportional allocation methods, enhancing overall travel efficiency and providing novel insights into the integration and optimization of multimodal transportation systems. This work lays a solid foundation for advancing intelligent urban mobility solutions through the coordination of air and ground transportation modes. The related code can be found at this https URL."
    },
    {
      "title": "Disentangling perception and reasoning for improving data efficiency in learning cloth manipulation without demonstrations",
      "url": "https://arxiv.org/abs/2601.21713",
      "date": "2026-01-29",
      "authors_text": "Donatien Delehelle, Fei Chen, Darwin Caldwell",
      "is_highlight": false,
      "score": 87.0,
      "summary": "This paper presents a modular reinforcement learning approach for cloth manipulation that enhances data efficiency and reduces model size and training time, achieving improved performance in simulation and real-world transfer.",
      "teaser_image": "https://arxiv.org/html/2601.21713/x1.png",
      "debug_abstract": "Cloth manipulation is a ubiquitous task in everyday life, but it remains an open challenge for robotics. The difficulties in developing cloth manipulation policies are attributed to the high-dimensional state space, complex dynamics, and high propensity to self-occlusion exhibited by fabrics. As analytical methods have not been able to provide robust and general manipulation policies, reinforcement learning (RL) is considered a promising approach to these problems. However, to address the large state space and complex dynamics, data-based methods usually rely on large models and long training times. The resulting computational cost significantly hampers the development and adoption of these methods. Additionally, due to the challenge of robust state estimation, garment manipulation policies often adopt an end-to-end learning approach with workspace images as input. While this approach enables a conceptually straightforward sim-to-real transfer via real-world fine-tuning, it also incurs a significant computational cost by training agents on a highly lossy representation of the environment state. This paper questions this common design choice by exploring an efficient and modular approach to RL for cloth manipulation. We show that, through careful design choices, model size and training time can be significantly reduced when learning in simulation. Furthermore, we demonstrate how the resulting simulation-trained model can be transferred to the real world. We evaluate our approach on the SoftGym benchmark and achieve significant performance improvements over available baselines on our task, while using a substantially smaller model."
    },
    {
      "title": "Dynamic Topology Awareness: Breaking the Granularity Rigidity in Vision-Language Navigation",
      "url": "https://arxiv.org/abs/2601.21751",
      "date": "2026-01-29",
      "authors_text": "Jiankun Peng, Jianyuan Guo, Ying Xu, Yue Liu, Jiashuang Yan",
      "is_highlight": false,
      "score": 84.0,
      "summary": "The paper presents DGNav, a dynamic framework for Vision-Language Navigation that adapts topological map density and connectivity to enhance navigation efficiency and safety in complex environments.",
      "teaser_image": "https://arxiv.org/html/2601.21751/x1.png",
      "debug_abstract": "Vision-Language Navigation in Continuous Environments (VLN-CE) presents a core challenge: grounding high-level linguistic instructions into precise, safe, and long-horizon spatial actions. Explicit topological maps have proven to be a vital solution for providing robust spatial memory in such tasks. However, existing topological planning methods suffer from a &#34;Granularity Rigidity&#34; problem. Specifically, these methods typically rely on fixed geometric thresholds to sample nodes, which fails to adapt to varying environmental complexities. This rigidity leads to a critical mismatch: the model tends to over-sample in simple areas, causing computational redundancy, while under-sampling in high-uncertainty regions, increasing collision risks and compromising precision. To address this, we propose DGNav, a framework for Dynamic Topological Navigation, introducing a context-aware mechanism to modulate map density and connectivity on-the-fly. Our approach comprises two core innovations: (1) A Scene-Aware Adaptive Strategy that dynamically modulates graph construction thresholds based on the dispersion of predicted waypoints, enabling &#34;densification on demand&#34; in challenging environments; (2) A Dynamic Graph Transformer that reconstructs graph connectivity by fusing visual, linguistic, and geometric cues into dynamic edge weights, enabling the agent to filter out topological noise and enhancing instruction adherence. Extensive experiments on the R2R-CE and RxR-CE benchmarks demonstrate DGNav exhibits superior navigation performance and strong generalization capabilities. Furthermore, ablation studies confirm that our framework achieves an optimal trade-off between navigation efficiency and safe exploration. The code is available at this https URL."
    },
    {
      "title": "PLANING: A Loosely Coupled Triangle-Gaussian Framework for Streaming 3D Reconstruction",
      "url": "https://arxiv.org/abs/2601.22046",
      "date": "2026-01-29",
      "authors_text": "Changjian Jiang, Kerui Ren, Xudong Li, Kaiwen Song, Linning Xu",
      "is_highlight": false,
      "score": 66.0,
      "summary": "PLANING is a novel streaming 3D reconstruction framework that efficiently combines geometric primitives and neural Gaussians, enhancing quality and speed while reducing redundancy.",
      "teaser_image": "https://arxiv.org/html/2601.22046/x3.png",
      "debug_abstract": "Streaming reconstruction from monocular image sequences remains challenging, as existing methods typically favor either high-quality rendering or accurate geometry, but rarely both. We present PLANING, an efficient on-the-fly reconstruction framework built on a hybrid representation that loosely couples explicit geometric primitives with neural Gaussians, enabling geometry and appearance to be modeled in a decoupled manner. This decoupling supports an online initialization and optimization strategy that separates geometry and appearance updates, yielding stable streaming reconstruction with substantially reduced structural redundancy. PLANING improves dense mesh Chamfer-L2 by 18.52% over PGSR, surpasses ARTDECO by 1.31 dB PSNR, and reconstructs ScanNetV2 scenes in under 100 seconds, over 5x faster than 2D Gaussian Splatting, while matching the quality of offline per-scene optimization. Beyond reconstruction quality, the structural clarity and computational efficiency of \\modelname~make it well suited for a broad range of downstream applications, such as enabling large-scale scene modeling and simulation-ready environments for embodied AI. Project page: this https URL ."
    },
    {
      "title": "Listen, Look, Drive: Coupling Audio Instructions for User-aware VLA-based Autonomous Driving",
      "url": "https://arxiv.org/abs/2601.12142",
      "date": "2026-01-28",
      "authors_text": "Ziang Guo, Feng Yang, Xuefeng Zhang, Jiaqi Guo, Kun Zhao",
      "is_highlight": false,
      "score": 88.0,
      "summary": "EchoVLA enhances autonomous driving by integrating audio instructions with visual inputs, allowing for user influence and emotional context, significantly improving navigation accuracy and safety.",
      "teaser_image": "https://arxiv.org/html/2601.12142/Figs/overview.png",
      "debug_abstract": "Vision Language Action (VLA) models promise an open-vocabulary interface that can translate perceptual ambiguity into semantically grounded driving decisions, yet they still treat language as a static prior fixed at inference time. As a result, the model must infer continuously shifting objectives from pixels alone, yielding delayed or overly conservative maneuvers. We argue that effective VLAs for autonomous driving need an online channel in which users can influence driving with specific intentions. To this end, we present EchoVLA, a user-aware VLA that couples camera streams with in situ audio instructions. We augment the nuScenes dataset with temporally aligned, intent-specific speech commands generated by converting ego-motion descriptions into synthetic audios. Further, we compose emotional speech-trajectory pairs into a multimodal Chain-of-Thought (CoT) for fine-tuning a Multimodal Large Model (MLM) based on Qwen2.5-Omni. Specifically, we synthesize the audio-augmented dataset with different emotion types paired with corresponding driving behaviors, leveraging the emotional cues embedded in tone, pitch, and speech tempo to reflect varying user states, such as urgent or hesitant intentions, thus enabling our EchoVLA to interpret not only the semantic content but also the emotional context of audio commands for more nuanced and emotionally adaptive driving behavior. In open-loop benchmarks, our approach reduces the average L2 error by $59.4\\%$ and the collision rate by $74.4\\%$ compared to the baseline of vision-only perception. More experiments on nuScenes dataset validate that EchoVLA not only steers the trajectory through audio instructions, but also modulates driving behavior in response to the emotions detected in the user&#39;s speech."
    },
    {
      "title": "Tactile-Force Alignment in Vision-Language-Action Models for Force-aware Manipulation",
      "url": "https://arxiv.org/abs/2601.20321",
      "date": "2026-01-28",
      "authors_text": "Yuzhe Huang, Pei Lin, Wanlin Li, Daohan Li, Jiajun Li",
      "is_highlight": false,
      "score": 91.0,
      "summary": "The paper introduces TaF-VLA, a framework that enhances Vision-Language-Action models by integrating tactile-force alignment for improved force-aware robotic manipulation in contact-rich tasks.",
      "teaser_image": "https://arxiv.org/html/2601.20321/figures/fig1-teaser.png",
      "debug_abstract": "Vision-Language-Action (VLA) models have recently emerged as powerful generalists for robotic manipulation. However, due to their predominant reliance on visual modalities, they fundamentally lack the physical intuition required for contact-rich tasks that require precise force regulation and physical reasoning. Existing attempts to incorporate vision-based tactile sensing into VLA models typically treat tactile inputs as auxiliary visual textures, thereby overlooking the underlying correlation between surface deformation and interaction dynamics. To bridge this gap, we propose a paradigm shift from tactile-vision alignment to tactile-force alignment. Here, we introduce TaF-VLA, a framework that explicitly grounds high-dimensional tactile observations in physical interaction forces. To facilitate this, we develop an automated tactile-force data acquisition device and curate the TaF-Dataset, comprising over 10 million synchronized tactile observations, 6-axis force/torque, and matrix force map. To align sequential tactile observations with interaction forces, the central component of our approach is the Tactile-Force Adapter (TaF-Adapter), a tactile sensor encoder that extracts discretized latent information for encoding tactile observations. This mechanism ensures that the learned representations capture history-dependent, noise-insensitive physical dynamics rather than static visual textures. Finally, we integrate this force-aligned encoder into a VLA backbone. Extensive real-world experiments demonstrate that TaF-VLA policy significantly outperforms state-of-the-art tactile-vision-aligned and vision-only baselines on contact-rich tasks, verifying its ability to achieve robust, force-aware manipulation through cross-modal physical reasoning."
    },
    {
      "title": "Rhombot: Rhombus-shaped Modular Robots for Stable, Medium-Independent Reconfiguration Motion",
      "url": "https://arxiv.org/abs/2601.19529",
      "date": "2026-01-27",
      "authors_text": "Jie Gu, Yirui Sun, Zhihao Xia, Tin Lun Lam, Chunxu Tian",
      "is_highlight": false,
      "score": 86.0,
      "summary": "Rhombot is a modular self-reconfigurable robot featuring rhombus-shaped modules that enable stable, medium-independent morphing and locomotion through a novel motion primitive called morphpivoting.",
      "debug_abstract": "In this paper, we present Rhombot, a novel deformable planar lattice modular self-reconfigurable robot (MSRR) with a rhombus shaped module. Each module consists of a parallelogram skeleton with a single centrally mounted actuator that enables folding and unfolding along its diagonal. The core design philosophy is to achieve essential MSRR functionalities such as morphing, docking, and locomotion with minimal control complexity. This enables a continuous and stable reconfiguration process that is independent of the surrounding medium, allowing the system to reliably form various configurations in diverse environments. To leverage the unique kinematics of Rhombot, we introduce morphpivoting, a novel motion primitive for reconfiguration that differs from advanced MSRR systems, and propose a strategy for its continuous execution. Finally, a series of physical experiments validate the module&#39;s stable reconfiguration ability, as well as its positional and docking accuracy."
    },
    {
      "title": "Component-Level Lesioning of Language Models Reveals Clinically Aligned Aphasia Phenotypes",
      "url": "https://arxiv.org/abs/2601.19723",
      "date": "2026-01-27",
      "authors_text": "Yifan Wang, Jichen Zheng, Jingyuan Sun, Yunhao Zhang, Chunyu Ye",
      "is_highlight": false,
      "score": 62.0,
      "summary": "This study presents a framework for simulating aphasia in language models by selectively perturbing components, revealing clinically relevant language impairments and enhancing understanding of language function degradation.",
      "debug_abstract": "Large language models (LLMs) increasingly exhibit human-like linguistic behaviors and internal representations that they could serve as computational simulators of language cognition. We ask whether LLMs can be systematically manipulated to reproduce language-production impairments characteristic of aphasia following focal brain lesions. Such models could provide scalable proxies for testing rehabilitation hypotheses, and offer a controlled framework for probing the functional organization of language. We introduce a clinically grounded, component-level framework that simulates aphasia by selectively perturbing functional components in LLMs, and apply it to both modular Mixture-of-Experts models and dense Transformers using a unified intervention interface. Our pipeline (i) identifies subtype-linked components for Broca&#39;s and Wernicke&#39;s aphasia, (ii) interprets these components with linguistic probing tasks, and (iii) induces graded impairments by progressively perturbing the top-k subtype-linked components, evaluating outcomes with Western Aphasia Battery (WAB) subtests summarized by Aphasia Quotient (AQ). Across architectures and lesioning strategies, subtype-targeted perturbations yield more systematic, aphasia-like regressions than size-matched random perturbations, and MoE modularity supports more localized and interpretable phenotype-to-component mappings. These findings suggest that modular LLMs, combined with clinically informed component perturbations, provide a promising platform for simulating aphasic language production and studying how distinct language functions degrade under targeted disruptions."
    },
    {
      "title": "EgoHandICL: Egocentric 3D Hand Reconstruction with In-Context Learning",
      "url": "https://arxiv.org/abs/2601.19850",
      "date": "2026-01-27",
      "authors_text": "Binzhu Xie, Shi Qiu, Sicheng Zhang, Yinqiao Wang, Hao Xu",
      "is_highlight": false,
      "score": 76.0,
      "summary": "EgoHandICL introduces an innovative in-context learning framework for robust 3D hand reconstruction in egocentric vision, enhancing performance through exemplar retrieval and multimodal context integration.",
      "debug_abstract": "Robust 3D hand reconstruction in egocentric vision is challenging due to depth ambiguity, self-occlusion, and complex hand-object interactions. Prior methods mitigate these issues by scaling training data or adding auxiliary cues, but they often struggle in unseen contexts. We present EgoHandICL, the first in-context learning (ICL) framework for 3D hand reconstruction that improves semantic alignment, visual consistency, and robustness under challenging egocentric conditions. EgoHandICL introduces complementary exemplar retrieval guided by vision-language models (VLMs), an ICL-tailored tokenizer for multimodal context, and a masked autoencoder (MAE)-based architecture trained with hand-guided geometric and perceptual objectives. Experiments on ARCTIC and EgoExo4D show consistent gains over state-of-the-art methods. We also demonstrate real-world generalization and improve EgoVLM hand-object interaction reasoning by using reconstructed hands as visual prompts. Code and data: this https URL"
    },
    {
      "title": "Advances and Innovations in the Multi-Agent Robotic System (MARS) Challenge",
      "url": "https://arxiv.org/abs/2601.18733",
      "date": "2026-01-26",
      "authors_text": "Li Kang, Heng Zhou, Xiufeng Song, Rui Li, Bruno N. Y. Chen",
      "is_highlight": false,
      "score": 94.0,
      "summary": "The MARS Challenge promotes advancements in multi-agent robotic systems by exploring vision-language models for collaborative task execution and planning in dynamic environments.",
      "debug_abstract": "Recent advancements in multimodal large language models and vision-languageaction models have significantly driven progress in Embodied AI. As the field transitions toward more complex task scenarios, multi-agent system frameworks are becoming essential for achieving scalable, efficient, and collaborative solutions. This shift is fueled by three primary factors: increasing agent capabilities, enhancing system efficiency through task delegation, and enabling advanced human-agent interactions. To address the challenges posed by multi-agent collaboration, we propose the Multi-Agent Robotic System (MARS) Challenge, held at the NeurIPS 2025 Workshop on SpaVLE. The competition focuses on two critical areas: planning and control, where participants explore multi-agent embodied planning using vision-language models (VLMs) to coordinate tasks and policy execution to perform robotic manipulation in dynamic environments. By evaluating solutions submitted by participants, the challenge provides valuable insights into the design and coordination of embodied multi-agent systems, contributing to the future development of advanced collaborative AI systems."
    },
    {
      "title": "Fast and Safe Trajectory Optimization for Mobile Manipulators With Neural Configuration Space Distance Field",
      "url": "https://arxiv.org/abs/2601.18548",
      "date": "2026-01-26",
      "authors_text": "Yulin Li, Zhiyuan Song, Yiming Li, Zhicheng Song, Kai Chen",
      "is_highlight": false,
      "score": 86.0,
      "summary": "This paper introduces Generalized Configuration Space Distance Fields (GCDF) for efficient trajectory optimization in mobile manipulators, enhancing collision reasoning and enabling agile motion in complex environments.",
      "debug_abstract": "Mobile manipulators promise agile, long-horizon behavior by coordinating base and arm motion, yet whole-body trajectory optimization in cluttered, confined spaces remains difficult due to high-dimensional nonconvexity and the need for fast, accurate collision reasoning. Configuration Space Distance Fields (CDF) enable fixed-base manipulators to model collisions directly in configuration space via smooth, implicit distances. This representation holds strong potential to bypass the nonlinear configuration-to-workspace mapping while preserving accurate whole-body geometry and providing optimization-friendly collision costs. Yet, extending this capability to mobile manipulators is hindered by unbounded workspaces and tighter base-arm coupling. We lift this promise to mobile manipulation with Generalized Configuration Space Distance Fields (GCDF), extending CDF to robots with both translational and rotational joints in unbounded workspaces with tighter base-arm coupling. We prove that GCDF preserves Euclidean-like local distance structure and accurately encodes whole-body geometry in configuration space, and develop a data generation and training pipeline that yields continuous neural GCDFs with accurate values and gradients, supporting efficient GPU-batched queries. Building on this representation, we develop a high-performance sequential convex optimization framework centered on GCDF-based collision reasoning. The solver scales to large numbers of implicit constraints through (i) online specification of neural constraints, (ii) sparsity-aware active-set detection with parallel batched evaluation across thousands of constraints, and (iii) incremental constraint management for rapid replanning under scene changes."
    },
    {
      "title": "Resisting Manipulative Bots in Meme Coin Copy Trading: A Multi-Agent Approach with Chain-of-Thought Reasoning",
      "url": "https://arxiv.org/abs/2601.08641",
      "date": "2026-01-26",
      "authors_text": "Yichen Luo, Yebo Feng, Jiahua Xu, Yang Liu",
      "is_highlight": false,
      "score": 67.0,
      "summary": "This paper presents a multi-agent system utilizing explainable LLMs to defend against manipulative bots in meme coin copy trading, enhancing prediction accuracy and economic performance.",
      "debug_abstract": "Copy trading has become the dominant entry strategy in meme coin markets. However, due to the market&#39;s extreme illiquid and volatile nature, the strategy exposes an exploitable attack surface: adversaries deploy manipulative bots to front-run trades, conceal positions, and fabricate sentiment, systematically extracting value from naïve copiers at scale. Despite its prevalence, bot-driven manipulation remains largely unexplored, and no robust defensive framework exists. We propose a manipulation-resistant copy-trading system based on a multi-agent architecture powered by a multi-modal, explainable large language model (LLM). Our system decomposes copy trading into three specialized agents for coin evaluation, wallet selection, and timing assessment. Evaluated on historical data from over 6,000 meme coins, our approach outperforms zero-shot and most statistic-driven baselines in prediction accuracy as well as all baselines in economic performance, achieving an average return of 14% for identified smart-money trades and an estimated copier return of 3% per trade under realistic market frictions. Overall, our results demonstrate the effectiveness of agent-based defenses and predictability of trader profitability in adversarial meme coin markets, providing a practical foundation for robust copy trading."
    },
    {
      "title": "AdaReasoner: Dynamic Tool Orchestration for Iterative Visual Reasoning",
      "url": "https://arxiv.org/abs/2601.18631",
      "date": "2026-01-26",
      "authors_text": "Mingyang Song, Haoyu Sun, Jiawei Gu, Linjie Li, Luxin Xu",
      "is_highlight": false,
      "score": 61.0,
      "summary": "AdaReasoner is a multimodal model that autonomously optimizes tool selection and usage for iterative visual reasoning, achieving state-of-the-art performance on various benchmarks.",
      "debug_abstract": "When humans face problems beyond their immediate capabilities, they rely on tools, providing a promising paradigm for improving visual reasoning in multimodal large language models (MLLMs). Effective reasoning, therefore, hinges on knowing which tools to use, when to invoke them, and how to compose them over multiple steps, even when faced with new tools or new tasks. We introduce \\textbf{AdaReasoner}, a family of multimodal models that learn tool use as a general reasoning skill rather than as tool-specific or explicitly supervised behavior. AdaReasoner is enabled by (i) a scalable data curation pipeline exposing models to long-horizon, multi-step tool interactions; (ii) Tool-GRPO, a reinforcement learning algorithm that optimizes tool selection and sequencing based on end-task success; and (iii) an adaptive learning mechanism that dynamically regulates tool usage. Together, these components allow models to infer tool utility from task context and intermediate outcomes, enabling coordination of multiple tools and generalization to unseen tools. Empirically, AdaReasoner exhibits strong tool-adaptive and generalization behaviors: it autonomously adopts beneficial tools, suppresses irrelevant ones, and adjusts tool usage frequency based on task demands, despite never being explicitly trained to do so. These capabilities translate into state-of-the-art performance across challenging benchmarks, improving the 7B base model by +24.9\\% on average and surpassing strong proprietary systems such as GPT-5 on multiple tasks, including VSP and Jigsaw."
    },
    {
      "title": "Timely Machine: Awareness of Time Makes Test-Time Scaling Agentic",
      "url": "https://arxiv.org/abs/2601.16486",
      "date": "2026-01-23",
      "authors_text": "Yichuan Ma, Linyang Li, Yongkang chen, Peiji Li, Xiaozhe Li",
      "is_highlight": false,
      "score": 78.0,
      "summary": "The paper introduces Timely Machine, redefining test-time scaling for LLMs based on wall-clock time, enhancing performance through dynamic strategy adjustments and reinforcement learning.",
      "debug_abstract": "As large language models (LLMs) increasingly tackle complex reasoning tasks, test-time scaling has become critical for enhancing capabilities. However, in agentic scenarios with frequent tool calls, the traditional generation-length-based definition breaks down: tool latency decouples inference time from generation length. We propose Timely Machine, redefining test-time as wall-clock time, where models dynamically adjust strategies based on time budgets. We introduce Timely-Eval, a benchmark spanning high-frequency tool calls, low-frequency tool calls, and time-constrained reasoning. By varying tool latency, we find smaller models excel with fast feedback through more interactions, while larger models dominate high-latency settings via superior interaction quality. Moreover, existing models fail to adapt reasoning to time budgets. We propose Timely-RL to address this gap. After cold-start supervised fine-tuning, we use reinforcement learning to enhance temporal planning. Timely-RL improves time budget awareness and consistently boosts performance across Timely-Eval. We hope our work offers a new perspective on test-time scaling for the agentic era."
    },
    {
      "title": "Adaptive Reinforcement and Model Predictive Control Switching for Safe Human-Robot Cooperative Navigation",
      "url": "https://arxiv.org/abs/2601.16686",
      "date": "2026-01-23",
      "authors_text": "Ning Liu, Sen Shen, Zheng Li, Matthew D'Souza, Jen Jen Chung",
      "is_highlight": false,
      "score": 94.0,
      "summary": "The paper presents ARMS, a hybrid control framework combining reinforcement learning and model predictive control for safe human-robot navigation, achieving superior performance in cluttered environments.",
      "debug_abstract": "This paper addresses the challenge of human-guided navigation for mobile collaborative robots under simultaneous proximity regulation and safety constraints. We introduce Adaptive Reinforcement and Model Predictive Control Switching (ARMS), a hybrid learning-control framework that integrates a reinforcement learning follower trained with Proximal Policy Optimization (PPO) and an analytical one-step Model Predictive Control (MPC) formulated as a quadratic program safety filter. To enable robust perception under partial observability and non-stationary human motion, ARMS employs a decoupled sensing architecture with a Long Short-Term Memory (LSTM) temporal encoder for the human-robot relative state and a spatial encoder for 360-degree LiDAR scans. The core contribution is a learned adaptive neural switcher that performs context-aware soft action fusion between the two controllers, favoring conservative, constraint-aware QP-based control in low-risk regions while progressively shifting control authority to the learned follower in highly cluttered or constrained scenarios where maneuverability is critical, and reverting to the follower action when the QP becomes infeasible. Extensive evaluations against Pure Pursuit, Dynamic Window Approach (DWA), and an RL-only baseline demonstrate that ARMS achieves an 82.5 percent success rate in highly cluttered environments, outperforming DWA and RL-only approaches by 7.1 percent and 3.1 percent, respectively, while reducing average computational latency by 33 percent to 5.2 milliseconds compared to a multi-step MPC baseline. Additional simulation transfer in Gazebo and initial real-world deployment results further indicate the practicality and robustness of ARMS for safe and efficient human-robot collaboration. Source code and a demonstration video are available at this https URL."
    },
    {
      "title": "Natural Language-Driven Global Mapping of Martian Landforms",
      "url": "https://arxiv.org/abs/2601.15949",
      "date": "2026-01-22",
      "authors_text": "Yiran Wang, Shuoyuan Wang, Zhaoran Wei, Jiannan Zhao, Zhonghua Yao",
      "is_highlight": false,
      "score": 72.0,
      "summary": "MarScope is a vision-language framework that enables natural language-driven, label-free mapping of Martian landforms, enhancing global geomorphic analysis and user query flexibility.",
      "debug_abstract": "Planetary surfaces are typically analyzed using high-level semantic concepts in natural language, yet vast orbital image archives remain organized at the pixel level. This mismatch limits scalable, open-ended exploration of planetary surfaces. Here we present MarScope, a planetary-scale vision-language framework enabling natural language-driven, label-free mapping of Martian landforms. MarScope aligns planetary images and text in a shared semantic space, trained on over 200,000 curated image-text pairs. This framework transforms global geomorphic mapping on Mars by replacing pre-defined classifications with flexible semantic retrieval, enabling arbitrary user queries across the entire planet in 5 seconds with F1 scores up to 0.978. Applications further show that it extends beyond morphological classification to facilitate process-oriented analysis and similarity-based geomorphological mapping at a planetary scale. MarScope establishes a new paradigm where natural language serves as a direct interface for scientific discovery over massive geospatial datasets."
    },
    {
      "title": "Robust Tool Use via Fission-GRPO: Learning to Recover from Execution Errors",
      "url": "https://arxiv.org/abs/2601.15625",
      "date": "2026-01-22",
      "authors_text": "Zhiwei Zhang, Fei Zhao, Rui Wang, Zezhong Wang, Bin Liang",
      "is_highlight": false,
      "score": 87.1,
      "summary": "Fission-GRPO enhances tool use in LLMs by converting execution errors into corrective supervision, improving error recovery and overall accuracy in multi-turn interactions.",
      "debug_abstract": "Large language models (LLMs) can call tools effectively, yet they remain brittle in multi-turn execution: following a tool call error, smaller models often degenerate into repetitive invalid re-invocations, failing to interpret error feedback and self-correct. This brittleness hinders reliable real-world deployment, where the execution errors are inherently inevitable during tool interaction procedures. We identify a key limitation of current approaches: standard reinforcement learning (RL) treats errors as sparse negative rewards, providing no guidance on how to recover, while pre-collected synthetic error-correction datasets suffer from distribution mismatch with the model&#39;s on-policy error modes. To bridge this gap, we propose Fission-GRPO, a framework that converts execution errors into corrective supervision within the RL training loop. Our core mechanism fissions each failed trajectory into a new training instance by augmenting it with diagnostic feedback from a finetuned Error Simulator, then resampling recovery rollouts on-policy. This enables the model to learn from the precise errors it makes during exploration, rather than from static, pre-collected error cases. On the BFCL v4 Multi-Turn, Fission-GRPO improves the error recovery rate of Qwen3-8B by 5.7% absolute, crucially, yielding a 4% overall accuracy gain (42.75% to 46.75%) over GRPO and outperforming specialized tool-use agents."
    },
    {
      "title": "PhysicsMind: Sim and Real Mechanics Benchmarking for Physical Reasoning and Prediction in Foundational VLMs and World Models",
      "url": "https://arxiv.org/abs/2601.16007",
      "date": "2026-01-22",
      "authors_text": "Chak-Wing Mak, Guanyu Zhu, Boyi Zhang, Hongji Li, Xiaowei Chi",
      "is_highlight": false,
      "score": 94.1,
      "summary": "PhysicsMind introduces a benchmark for evaluating physical reasoning in multimodal models through real and simulated tasks focused on fundamental physics principles, revealing significant gaps in current models' understanding.",
      "debug_abstract": "Modern foundational Multimodal Large Language Models (MLLMs) and video world models have advanced significantly in mathematical, common-sense, and visual reasoning, but their grasp of the underlying physics remains underexplored. Existing benchmarks attempting to measure this matter rely on synthetic, Visual Question Answer templates or focus on perceptual video quality that is tangential to measuring how well the video abides by physical laws. To address this fragmentation, we introduce PhysicsMind, a unified benchmark with both real and simulation environments that evaluates law-consistent reasoning and generation over three canonical principles: Center of Mass, Lever Equilibrium, and Newton&#39;s First Law. PhysicsMind comprises two main tasks: i) VQA tasks, testing whether models can reason and determine physical quantities and values from images or short videos, and ii) Video Generation(VG) tasks, evaluating if predicted motion trajectories obey the same center-of-mass, torque, and inertial constraints as the ground truth. A broad range of recent models and video generation models is evaluated on PhysicsMind and found to rely on appearance heuristics while often violating basic mechanics. These gaps indicate that current scaling and training are still insufficient for robust physical understanding, underscoring PhysicsMind as a focused testbed for physics-aware multimodal models. Our data will be released upon acceptance."
    }
  ],
  "潘佳老师实验室": [
    {
      "title": "Large Language Models Can Take False First Steps at Inference-time Planning",
      "url": "https://arxiv.org/abs/2602.02991",
      "date": "2026-02-03",
      "authors_text": "Haijiang Yan, Jian-Qiao Zhu, Adam Sanborn",
      "is_highlight": false,
      "score": 73.0,
      "summary": "The paper explains that large language models exhibit inconsistent planning at inference due to evolving self-generated context, supported by experiments demonstrating planning shifts and reduced biases.",
      "teaser_image": "https://arxiv.org/html/2602.02991/images/llama_regression.png",
      "debug_abstract": "Large language models (LLMs) have been shown to acquire sequence-level planning abilities during training, yet their planning behavior exhibited at inference time often appears short-sighted and inconsistent with these capabilities. We propose a Bayesian account for this gap by grounding planning behavior in the evolving generative context: given the subtle differences between natural language and the language internalized by LLMs, accumulated self-generated context drives a planning-shift during inference and thereby creates the appearance of compromised planning behavior. We further validate the proposed model through two controlled experiments: a random-generation task demonstrating constrained planning under human prompts and increasing planning strength as self-generated context accumulates, and a Gaussian-sampling task showing reduced initial bias when conditioning on self-generated sequences. These findings provide a theoretical explanation along with empirical evidence for characterizing how LLMs plan ahead during inference."
    },
    {
      "title": "Search-R2: Enhancing Search-Integrated Reasoning via Actor-Refiner Collaboration",
      "url": "https://arxiv.org/abs/2602.03647",
      "date": "2026-02-03",
      "authors_text": "Bowei He, Minda Hu, Zenan Xu, Hongru Wang, Licheng Zong",
      "is_highlight": false,
      "score": 52.0,
      "summary": "Search-R2 introduces an Actor-Refiner framework that enhances search-integrated reasoning in language agents through targeted intervention and fine-grained supervision, outperforming existing methods.",
      "teaser_image": "https://arxiv.org/html/2602.03647/x2.png",
      "debug_abstract": "Search-integrated reasoning enables language agents to transcend static parametric knowledge by actively querying external sources. However, training these agents via reinforcement learning is hindered by the multi-scale credit assignment problem: existing methods typically rely on sparse, trajectory-level rewards that fail to distinguish between high-quality reasoning and fortuitous guesses, leading to redundant or misleading search behaviors. To address this, we propose Search-R2, a novel Actor-Refiner collaboration framework that enhances reasoning through targeted intervention, with both components jointly optimized during training. Our approach decomposes the generation process into an Actor, which produces initial reasoning trajectories, and a Meta-Refiner, which selectively diagnoses and repairs flawed steps via a &#39;cut-and-regenerate&#39; mechanism. To provide fine-grained supervision, we introduce a hybrid reward design that couples outcome correctness with a dense process reward quantifying the information density of retrieved evidence. Theoretically, we formalize the Actor-Refiner interaction as a smoothed mixture policy, proving that selective correction yields strict performance gains over strong baselines. Extensive experiments across various general and multi-hop QA datasets demonstrate that Search-R2 consistently outperforms strong RAG and RL-based baselines across model scales, achieving superior reasoning accuracy with minimal overhead."
    },
    {
      "title": "Dual Quaternion SE(3) Synchronization with Recovery Guarantees",
      "url": "https://arxiv.org/abs/2602.00324",
      "date": "2026-01-30",
      "authors_text": "Jianing Zhao, Linglingzhi Zhu, Anthony Man-Cho So",
      "is_highlight": false,
      "score": 75.0,
      "summary": "This paper presents a dual quaternion-based SE(3) synchronization method with a two-stage algorithm that guarantees recovery of absolute poses from noisy transformations, outperforming traditional matrix methods.",
      "teaser_image": "https://arxiv.org/html/2602.00324/x2.png",
      "debug_abstract": "Synchronization over the special Euclidean group SE(3) aims to recover absolute poses from noisy pairwise relative transformations and is a core primitive in robotics and 3D vision. Standard approaches often require multi-step heuristic procedures to recover valid poses, which are difficult to analyze and typically lack theoretical guarantees. This paper adopts a dual quaternion representation and formulates SE(3) synchronization directly over the unit dual quaternion. A two-stage algorithm is developed: A spectral initializer computed via the power method on a Hermitian dual quaternion measurement matrix, followed by a dual quaternion generalized power method (DQGPM) that enforces feasibility through per-iteration projection. The estimation error bounds are established for spectral estimators, and DQGPM is shown to admit a finite-iteration error bound and achieves linear error contraction up to an explicit noise-dependent threshold. Experiments on synthetic benchmarks and real-world multi-scan point-set registration demonstrate that the proposed pipeline improves both accuracy and efficiency over representative matrix-based methods."
    },
    {
      "title": "Offline Discovery of Interpretable Skills from Multi-Task Trajectories",
      "url": "https://arxiv.org/abs/2602.01018",
      "date": "2026-02-01",
      "authors_text": "Chongyu Zhu, Mithun Vanniasinghe, Jiayu Chen, Chi-Guhn Lee",
      "is_highlight": false,
      "score": 77.0,
      "summary": "LOKI is a three-stage framework for offline skill discovery and hierarchical imitation learning, effectively extracting interpretable skills from multi-task trajectories without explicit rewards.",
      "teaser_image": "https://arxiv.org/html/2602.01018/fig/two_stage_skill_learning.png",
      "debug_abstract": "Hierarchical Imitation Learning is a powerful paradigm for acquiring complex robot behaviors from demonstrations. A central challenge, however, lies in discovering reusable skills from long-horizon, multi-task offline data, especially when the data lacks explicit rewards or subtask annotations. In this work, we introduce LOKI, a three-stage end-to-end learning framework designed for offline skill discovery and hierarchical imitation. The framework commences with a two-stage, weakly supervised skill discovery process: Stage one performs coarse, task-aware macro-segmentation by employing an alignment-enforced Vector Quantized VAE guided by weak task labels. Stage two then refines these segments at a micro-level using a self-supervised sequential model, followed by an iterative clustering process to consolidate skill boundaries. The third stage then leverages these precise boundaries to construct a hierarchical policy within an option-based framework-complete with a learned termination condition beta for explicit skill switching. LOKI achieves high success rates on the challenging D4RL Kitchen benchmark and outperforms standard HIL baselines. Furthermore, we demonstrate that the discovered skills are semantically meaningful, aligning with human intuition, and exhibit compositionality by successfully sequencing them to solve a novel, unseen task."
    },
    {
      "title": "Residual Decoding: Mitigating Hallucinations in Large Vision-Language Models via History-Aware Residual Guidance",
      "url": "https://arxiv.org/abs/2602.01047",
      "date": "2026-02-01",
      "authors_text": "Xinrong Chen, Xu Chu, Yingmin Qiu, Hengyuan Zhang, Jing Xiong",
      "is_highlight": false,
      "score": 53.0,
      "summary": "Residual Decoding (ResDec) mitigates hallucinations in Large Vision-Language Models by leveraging historical information to enhance decoding and improve visual grounding.",
      "teaser_image": "https://arxiv.org/html/2602.01047/x1.png",
      "debug_abstract": "Large Vision-Language Models (LVLMs) can reason effectively from image-text inputs and perform well in various multimodal tasks. Despite this success, they are affected by language priors and often produce hallucinations. Hallucinations denote generated content that is grammatically and syntactically coherent, yet bears no match or direct relevance to actual visual input. To address this problem, we propose Residual Decoding (ResDec). It is a novel training-free method that uses historical information to aid decoding. The method relies on the internal implicit reasoning mechanism and token logits evolution mechanism of LVLMs to correct biases. Extensive experiments demonstrate that ResDec effectively suppresses hallucinations induced by language priors, significantly improves visual grounding, and reduces object hallucinations. In addition to mitigating hallucinations, ResDec also performs exceptionally well on comprehensive LVLM benchmarks, highlighting its broad applicability."
    },
    {
      "title": "MedAD-R1: Eliciting Consistent Reasoning in Interpretible Medical Anomaly Detection via Consistency-Reinforced Policy Optimization",
      "url": "https://arxiv.org/abs/2602.01081",
      "date": "2026-02-01",
      "authors_text": "Haitao Zhang, Yingying Wang, Jiaxiang Wang, Haote Xu, Hongyang Zhang",
      "is_highlight": false,
      "score": 64.0,
      "summary": "MedAD-R1 enhances medical anomaly detection by utilizing a two-stage training framework with consistency-reinforced policy optimization, achieving state-of-the-art performance on the MedAD-38K benchmark.",
      "teaser_image": "https://arxiv.org/html/2602.01081/x3.png",
      "debug_abstract": "Medical Anomaly Detection (MedAD) presents a significant opportunity to enhance diagnostic accuracy using Large Multimodal Models (LMMs) to interpret and answer questions based on medical images. However, the reliance on Supervised Fine-Tuning (SFT) on simplistic and fragmented datasets has hindered the development of models capable of plausible reasoning and robust multimodal generalization. To overcome this, we introduce MedAD-38K, the first large-scale, multi-modal, and multi-center benchmark for MedAD featuring diagnostic Chain-of-Thought (CoT) annotations alongside structured Visual Question-Answering (VQA) pairs. On this foundation, we propose a two-stage training framework. The first stage, Cognitive Injection, uses SFT to instill foundational medical knowledge and align the model with a structured think-then-answer paradigm. Given that standard policy optimization can produce reasoning that is disconnected from the final answer, the second stage incorporates Consistency Group Relative Policy Optimization (Con-GRPO). This novel algorithm incorporates a crucial consistency reward to ensure the generated reasoning process is relevant and logically coherent with the final diagnosis. Our proposed model, MedAD-R1, achieves state-of-the-art (SOTA) performance on the MedAD-38K benchmark, outperforming strong baselines by more than 10\\%. This superior performance stems from its ability to generate transparent and logically consistent reasoning pathways, offering a promising approach to enhancing the trustworthiness and interpretability of AI for clinical decision support."
    },
    {
      "title": "Adaptive Rollout Allocation for Online Reinforcement Learning with Verifiable Rewards",
      "url": "https://arxiv.org/abs/2602.01601",
      "date": "2026-02-02",
      "authors_text": "Hieu Trung Nguyen, Bao Nguyen, Wenao Ma, Yuzhi Zhao, Ruifeng She",
      "is_highlight": false,
      "score": 40.0,
      "summary": "The paper presents \\Ours, a variance-informed strategy for adaptive rollout allocation in online reinforcement learning, enhancing sampling efficiency and performance over traditional methods.",
      "teaser_image": "https://arxiv.org/html/2602.01601/x2.png",
      "debug_abstract": "Sampling efficiency is a key bottleneck in reinforcement learning with verifiable rewards. Existing group-based policy optimization methods, such as GRPO, allocate a fixed number of rollouts for all training prompts. This uniform allocation implicitly treats all prompts as equally informative, and could lead to inefficient computational budget usage and impede training progress. We introduce \\Ours, a Variance-Informed Predictive allocation strategy that allocates a given rollout budget to the prompts in the incumbent batch to minimize the expected gradient variance of the policy update. At each iteration, \\Ours~uses a lightweight Gaussian process model to predict per-prompt success probabilities based on recent rollouts. These probability predictions are translated into variance estimates, which are then fed into a convex optimization problem to determine the optimal rollout allocations under a hard compute budget constraint. Empirical results show that \\Ours~consistently improves sampling efficiency and achieves higher performance than uniform or heuristic allocation strategies in multiple benchmarks. Our code will be available at this https URL."
    },
    {
      "title": "Reasoning with Autoregressive-Diffusion Collaborative Thoughts",
      "url": "https://arxiv.org/abs/2602.01608",
      "date": "2026-02-02",
      "authors_text": "Mu Yuan, Liekang Zeng, Guoliang Xing, Lan Zhang, Yunhao Liu",
      "is_highlight": false,
      "score": 41.0,
      "summary": "The paper presents Collaborative Thoughts, a framework integrating autoregressive and diffusion models for enhanced spatial reasoning and generation through iterative feedback and joint reasoning.",
      "teaser_image": "https://arxiv.org/html/2602.01608/x2.png",
      "debug_abstract": "Autoregressive and diffusion models represent two complementary generative paradigms. Autoregressive models excel at sequential planning and constraint composition, yet struggle with tasks that require explicit spatial or physical grounding. Diffusion models, in contrast, capture rich spatial structure through high-dimensional generation, but lack the stepwise logical control needed to satisfy complex, multi-stage constraints or to reliably identify and correct errors. We introduce Collaborative Thoughts, a unified collaborative framework that enables autoregressive and diffusion models to reason and generate jointly through a closed-loop interaction. In Collaborative Thoughts, autoregressive models perform structured planning and constraint management, diffusion models instantiate these constraints as intermediate visual thoughts, and a vision-based critic module evaluates whether the visual thoughts satisfy the intended structural and physical requirements. This feedback is then used to iteratively refine subsequent planning and generation steps, mitigating error propagation across modalities. Importantly, Collaborative Thoughts uses the same collaborative loop regardless of whether the task is autoregressive question answering or diffusion-based visual generation. Through representative examples, we illustrate how Collaborative Thoughts can improve the reliability of spatial reasoning and the controllability of generation."
    },
    {
      "title": "Tilt-Ropter: A Novel Hybrid Aerial and Terrestrial Vehicle with Tilt Rotors and Passive Wheels",
      "url": "https://arxiv.org/abs/2602.01700",
      "date": "2026-02-02",
      "authors_text": "Ruoyu Wang, Xuchen Liu, Zongzhou Wu, Zixuan Guo, Wendi Ding",
      "is_highlight": false,
      "score": 38.0,
      "summary": "The Tilt-Ropter is a hybrid aerial-terrestrial vehicle that utilizes tilt rotors and passive wheels for efficient locomotion, featuring advanced control systems for enhanced mobility and energy savings.",
      "teaser_image": "https://arxiv.org/html/2602.01700/x2.png",
      "debug_abstract": "In this work, we present Tilt-Ropter, a novel hybrid aerial-terrestrial vehicle (HATV) that combines tilt rotors with passive wheels to achieve energy-efficient multi-mode locomotion. Unlike existing under-actuated HATVs, the fully actuated design of Tilt-Ropter enables decoupled force and torque control, greatly enhancing its mobility and environmental adaptability. A nonlinear model predictive controller (NMPC) is developed to track reference trajectories and handle contact constraints across locomotion modes, while a dedicated control allocation module exploits actuation redundancy to achieve energy-efficient control of actuators. Additionally, to enhance robustness during ground contact, we introduce an external wrench estimation algorithm that estimates environmental interaction forces and torques in real time. The system is validated through both simulation and real-world experiments, including seamless air-ground transitions and trajectory tracking. Results show low tracking errors in both modes and highlight a 92.8% reduction in power consumption during ground locomotion, demonstrating the system&#39;s potential for long-duration missions across large-scale and energy-constrained environments."
    },
    {
      "title": "Breaking the Static Graph: Context-Aware Traversal for Robust Retrieval-Augmented Generation",
      "url": "https://arxiv.org/abs/2602.01965",
      "date": "2026-02-02",
      "authors_text": "Kwun Hang Lau, Fangyuan Zhang, Boyu Ruan, Yingli Zhou, Qintian Guo",
      "is_highlight": false,
      "score": 27.0,
      "summary": "CatRAG enhances Retrieval-Augmented Generation by transforming static Knowledge Graphs into query-adaptive structures, improving multi-hop evidence retrieval and reasoning completeness.",
      "teaser_image": "https://arxiv.org/html/2602.01965/image/HubBiasShift.png",
      "debug_abstract": "Recent advances in Retrieval-Augmented Generation (RAG) have shifted from simple vector similarity to structure-aware approaches like HippoRAG, which leverage Knowledge Graphs (KGs) and Personalized PageRank (PPR) to capture multi-hop dependencies. However, these methods suffer from a &#34;Static Graph Fallacy&#34;: they rely on fixed transition probabilities determined during indexing. This rigidity ignores the query-dependent nature of edge relevance, causing semantic drift where random walks are diverted into high-degree &#34;hub&#34; nodes before reaching critical downstream evidence. Consequently, models often achieve high partial recall but fail to retrieve the complete evidence chain required for multi-hop queries. To address this, we propose CatRAG, Context-Aware Traversal for robust RAG, a framework that builds on the HippoRAG 2 architecture and transforms the static KG into a query-adaptive navigation structure. We introduce a multi-faceted framework to steer the random walk: (1) Symbolic Anchoring, which injects weak entity constraints to regularize the random walk; (2) Query-Aware Dynamic Edge Weighting, which dynamically modulates graph structure, to prune irrelevant paths while amplifying those aligned with the query&#39;s intent; and (3) Key-Fact Passage Weight Enhancement, a cost-efficient bias that structurally anchors the random walk to likely evidence. Experiments across four multi-hop benchmarks demonstrate that CatRAG consistently outperforms state of the art baselines. Our analysis reveals that while standard Recall metrics show modest gains, CatRAG achieves substantial improvements in reasoning completeness, the capacity to recover the entire evidence path without gaps. These results reveal that our approach effectively bridges the gap between retrieving partial context and enabling fully grounded reasoning. Resources are available at this https URL."
    },
    {
      "title": "TIDE: Trajectory-based Diagnostic Evaluation of Test-Time Improvement in LLM Agents",
      "url": "https://arxiv.org/abs/2602.02196",
      "date": "2026-02-02",
      "authors_text": "Hang Yan, Xinyu Che, Fangzhi Xu, Qiushi Sun, Zichen Ding",
      "is_highlight": false,
      "score": 20.0,
      "summary": "The TIDE framework evaluates Test-Time Improvement in LLM agents by analyzing task completion dynamics, recursive behaviors, and memory constraints to enhance performance optimization.",
      "teaser_image": "https://arxiv.org/html/2602.02196/x1.png",
      "debug_abstract": "Recent advances in autonomous LLM agents demonstrate their ability to improve performance through iterative interaction with the environment. We define this paradigm as Test-Time Improvement (TTI). However, the mechanisms under how and why TTI succeed or fail remain poorly understood, and existing evaluation metrics fail to capture their task optimization efficiency, behavior adaptation after erroneous actions, and the specific utility of working memory for task completion. To address these gaps, we propose Test-time Improvement Diagnostic Evaluation (TIDE), an agent-agnostic and environment-agnostic framework that decomposes TTI into three comprehensive and interconnected dimensions. The framework measures (1) the overall temporal dynamics of task completion and (2) identifies whether performance is primarily constrained by recursive looping behaviors or (3) by burdensome accumulated memory. Through extensive experiments across diverse agents and environments, TIDE highlights that improving agent performance requires more than scaling internal reasoning, calling for explicitly optimizing the interaction dynamics between the agent and the environment."
    },
    {
      "title": "Towards Agentic Intelligence for Materials Science",
      "url": "https://arxiv.org/abs/2602.00169",
      "date": "2026-01-29",
      "authors_text": "Huan Zhang, Yizhan Li, Wenhao Huang, Ziyu Hou, Yu Song",
      "is_highlight": false,
      "score": 12.0,
      "summary": "This paper advocates for developing agentic AI systems in materials science that autonomously plan, act, and learn to enhance discovery processes beyond traditional models.",
      "teaser_image": "https://arxiv.org/html/2602.00169/x1.png",
      "debug_abstract": "The convergence of artificial intelligence and materials science presents a transformative opportunity, but achieving true acceleration in discovery requires moving beyond task-isolated, fine-tuned models toward agentic systems that plan, act, and learn across the full discovery loop. This survey advances a unique pipeline-centric view that spans from corpus curation and pretraining, through domain adaptation and instruction tuning, to goal-conditioned agents interfacing with simulation and experimental platforms. Unlike prior reviews, we treat the entire process as an end-to-end system to be optimized for tangible discovery outcomes rather than proxy benchmarks. This perspective allows us to trace how upstream design choices-such as data curation and training objectives-can be aligned with downstream experimental success through effective credit assignment. To bridge communities and establish a shared frame of reference, we first present an integrated lens that aligns terminology, evaluation, and workflow stages across AI and materials science. We then analyze the field through two focused lenses: From the AI perspective, the survey details LLM strengths in pattern recognition, predictive analytics, and natural language processing for literature mining, materials characterization, and property prediction; from the materials science perspective, it highlights applications in materials design, process optimization, and the acceleration of computational workflows via integration with external tools (e.g., DFT, robotic labs). Finally, we contrast passive, reactive approaches with agentic design, cataloging current contributions while motivating systems that pursue long-horizon goals with autonomy, memory, and tool use. This survey charts a practical roadmap towards autonomous, safety-aware LLM agents aimed at discovering novel and useful materials."
    },
    {
      "title": "Failure-Aware Bimanual Teleoperation via Conservative Value Guided Assistance",
      "url": "https://arxiv.org/abs/2602.01092",
      "date": "2026-02-01",
      "authors_text": "Peng Zhou, Zhongxuan Li, Jinsong Wu, Jiaming Qi, Jun Hu",
      "is_highlight": false,
      "score": 9.0,
      "summary": "This paper presents a failure-aware bimanual teleoperation framework that uses conservative value learning to enhance task success and reduce operator workload through compliant haptic assistance.",
      "teaser_image": "https://arxiv.org/html/2602.01092/x1.png",
      "debug_abstract": "Teleoperation of high-precision manipulation is con-strained by tight success tolerances and complex contact dy-namics, which make impending failures difficult for human operators to anticipate under partial observability. This paper proposes a value-guided, failure-aware framework for bimanual teleoperation that provides compliant haptic assistance while pre-serving continuous human authority. The framework is trained entirely from heterogeneous offline teleoperation data containing both successful and failed executions. Task feasibility is mod-eled as a conservative success score learned via Conservative Value Learning, yielding a risk-sensitive estimate that remains reliable under distribution shift. During online operation, the learned success score regulates the level of assistance, while a learned actor provides a corrective motion direction. Both are integrated through a joint-space impedance interface on the master side, yielding continuous guidance that steers the operator away from failure-prone actions without overriding intent. Experimental results on contact-rich manipulation tasks demonstrate improved task success rates and reduced operator workload compared to conventional teleoperation and shared-autonomy baselines, indicating that conservative value learning provides an effective mechanism for embedding failure awareness into bilateral teleoperation. Experimental videos are available at this https URL"
    },
    {
      "title": "FlowSteer: Interactive Agentic Workflow Orchestration via End-to-End Reinforcement Learning",
      "url": "https://arxiv.org/abs/2602.01664",
      "date": "2026-02-02",
      "authors_text": "Mingda Zhang, Haoran Luo, Tiesunlong Shen, Qika Lin, Xiaoying Tang",
      "is_highlight": false,
      "score": 7.0,
      "summary": "FlowSteer is an end-to-end reinforcement learning framework that automates workflow orchestration through interactive learning, addressing manual costs and improving performance across diverse tasks.",
      "teaser_image": "https://arxiv.org/html/2602.01664/x1.png",
      "debug_abstract": "In recent years, a variety of powerful agentic workflows have been applied to solve a wide range of human problems. However, existing workflow orchestration still faces key challenges, including high manual cost, reliance on specific operators/large language models (LLMs), and sparse reward signals. To address these challenges, we propose FlowSteer, an end-to-end reinforcement learning framework that takes a lightweight policy model as the agent and an executable canvas environment, automating workflow orchestration through multi-turn interaction. In this process, the policy model analyzes execution states and selects editing actions, while the canvas executes operators and returns feedback for iterative refinement. Moreover, FlowSteer provides a plug-and-play framework that supports diverse operator libraries and interchangeable LLM backends. To effectively train this interaction paradigm, we propose Canvas Workflow Relative Policy Optimization (CWRPO), which introduces diversity-constrained rewards with conditional release to stabilize learning and suppress shortcut behaviors. Experimental results on twelve datasets show that FlowSteer significantly outperforms baselines across various tasks."
    },
    {
      "title": "Diffusion Model-based Reinforcement Learning for Version Age of Information Scheduling: Average and Tail-Risk-Sensitive Control",
      "url": "https://arxiv.org/abs/2601.18069",
      "date": "2026-01-26",
      "authors_text": "Haoyuan Pan, Sizhao Chen, Zhaorui Wang, Tse-Tin Chan",
      "is_highlight": false,
      "score": 57.0,
      "summary": "This paper presents a novel reinforcement learning approach, RS-D3SAC, for optimizing Version Age of Information scheduling by minimizing both average and tail-risk-sensitive metrics in wireless systems.",
      "teaser_image": null,
      "debug_abstract": "Ensuring timely and semantically accurate information delivery is critical in real-time wireless systems. While Age of Information (AoI) quantifies temporal freshness, Version Age of Information (VAoI) captures semantic staleness by accounting for version evolution between transmitters and receivers. Existing VAoI scheduling approaches primarily focus on minimizing average VAoI, overlooking rare but severe staleness events that can compromise reliability under stochastic packet arrivals and unreliable channels. This paper investigates both average-oriented and tail-risk-sensitive VAoI scheduling in a multi-user status update system with long-term transmission cost constraints. We first formulate the average VAoI minimization problem as a constrained Markov decision process and introduce a deep diffusion-based Soft Actor-Critic (D2SAC) algorithm. By generating actions through a diffusion-based denoising process, D2SAC enhances policy expressiveness and establishes a strong baseline for mean performance. Building on this foundation, we put forth RS-D3SAC, a risk-sensitive deep distributional diffusion-based Soft Actor-Critic algorithm. RS-D3SAC integrates a diffusion-based actor with a quantile-based distributional critic, explicitly modeling the full VAoI return distribution. This enables principled tail-risk optimization via Conditional Value-at-Risk (CVaR) while satisfying long-term transmission cost constraints. Extensive simulations show that, while D2SAC reduces average VAoI, RS-D3SAC consistently achieves substantial reductions in CVaR without sacrificing mean performance. The dominant gain in tail-risk reduction stems from the distributional critic, with the diffusion-based actor providing complementary refinement to stabilize and enrich policy decisions, highlighting their effectiveness for robust and risk-aware VAoI scheduling in multi-user wireless systems."
    },
    {
      "title": "LoD-Structured 3D Gaussian Splatting for Streaming Video Reconstruction",
      "url": "https://arxiv.org/abs/2601.18475",
      "date": "2026-01-26",
      "authors_text": "Xinhui Liu, Can Wang, Lei Liu, Zhenghao Chen, Wei Jiang",
      "is_highlight": false,
      "score": 52.0,
      "summary": "The StreamLoD-GS framework enhances Streaming Free-Viewpoint Video reconstruction by optimizing 3D Gaussian Splatting for efficient, high-fidelity rendering with reduced storage needs.",
      "teaser_image": null,
      "debug_abstract": "Free-Viewpoint Video (FVV) reconstruction enables photorealistic and interactive 3D scene visualization; however, real-time streaming is often bottlenecked by sparse-view inputs, prohibitive training costs, and bandwidth constraints. While recent 3D Gaussian Splatting (3DGS) has advanced FVV due to its superior rendering speed, Streaming Free-Viewpoint Video (SFVV) introduces additional demands for rapid optimization, high-fidelity reconstruction under sparse constraints, and minimal storage footprints. To bridge this gap, we propose StreamLoD-GS, an LoD-based Gaussian Splatting framework designed specifically for SFVV. Our approach integrates three core innovations: 1) an Anchor- and Octree-based LoD-structured 3DGS with a hierarchical Gaussian dropout technique to ensure efficient and stable optimization while maintaining high-quality rendering; 2) a GMM-based motion partitioning mechanism that separates dynamic and static content, refining dynamic regions while preserving background stability; and 3) a quantized residual refinement framework that significantly reduces storage requirements without compromising visual fidelity. Extensive experiments demonstrate that StreamLoD-GS achieves competitive or state-of-the-art performance in terms of quality, efficiency, and storage."
    },
    {
      "title": "Heterogeneous Vertiport Selection Optimization for On-Demand Air Taxi Services: A Deep Reinforcement Learning Approach",
      "url": "https://arxiv.org/abs/2601.21316",
      "date": "2026-01-29",
      "authors_text": "Aoyu Pang, Maonan Wang, Zifan Sha, Wenwei Yue, Changle Li",
      "is_highlight": false,
      "score": 80.0,
      "summary": "The paper presents a deep reinforcement learning framework for optimizing vertiport selection and air taxi routing, achieving significant travel time reductions in urban air mobility systems.",
      "teaser_image": "https://arxiv.org/html/2601.21316/x2.png",
      "debug_abstract": "Urban Air Mobility (UAM) has emerged as a transformative solution to alleviate urban congestion by utilizing low-altitude airspace, thereby reducing pressure on ground transportation networks. To enable truly efficient and seamless door-to-door travel experiences, UAM requires close integration with existing ground transportation infrastructure. However, current research on optimal integrated routing strategies for passengers in air-ground mobility systems remains limited, with a lack of systematic this http URL address this gap, we first propose a unified optimization model that integrates strategy selection for both air and ground transportation. This model captures the dynamic characteristics of multimodal transport networks and incorporates real-time traffic conditions alongside passenger decision-making behavior. Building on this model, we propose a Unified Air-Ground Mobility Coordination (UAGMC) framework, which leverages deep reinforcement learning (RL) and Vehicle-to-Everything (V2X) communication to optimize vertiport selection and dynamically plan air taxi routes. Experimental results demonstrate that UAGMC achieves a 34\\% reduction in average travel time compared to conventional proportional allocation methods, enhancing overall travel efficiency and providing novel insights into the integration and optimization of multimodal transportation systems. This work lays a solid foundation for advancing intelligent urban mobility solutions through the coordination of air and ground transportation modes. The related code can be found at this https URL."
    },
    {
      "title": "Disentangling perception and reasoning for improving data efficiency in learning cloth manipulation without demonstrations",
      "url": "https://arxiv.org/abs/2601.21713",
      "date": "2026-01-29",
      "authors_text": "Donatien Delehelle, Fei Chen, Darwin Caldwell",
      "is_highlight": false,
      "score": 87.0,
      "summary": "This paper presents a modular reinforcement learning approach for cloth manipulation that enhances data efficiency and reduces model size and training time, achieving improved performance in simulation and real-world transfer.",
      "teaser_image": "https://arxiv.org/html/2601.21713/x1.png",
      "debug_abstract": "Cloth manipulation is a ubiquitous task in everyday life, but it remains an open challenge for robotics. The difficulties in developing cloth manipulation policies are attributed to the high-dimensional state space, complex dynamics, and high propensity to self-occlusion exhibited by fabrics. As analytical methods have not been able to provide robust and general manipulation policies, reinforcement learning (RL) is considered a promising approach to these problems. However, to address the large state space and complex dynamics, data-based methods usually rely on large models and long training times. The resulting computational cost significantly hampers the development and adoption of these methods. Additionally, due to the challenge of robust state estimation, garment manipulation policies often adopt an end-to-end learning approach with workspace images as input. While this approach enables a conceptually straightforward sim-to-real transfer via real-world fine-tuning, it also incurs a significant computational cost by training agents on a highly lossy representation of the environment state. This paper questions this common design choice by exploring an efficient and modular approach to RL for cloth manipulation. We show that, through careful design choices, model size and training time can be significantly reduced when learning in simulation. Furthermore, we demonstrate how the resulting simulation-trained model can be transferred to the real world. We evaluate our approach on the SoftGym benchmark and achieve significant performance improvements over available baselines on our task, while using a substantially smaller model."
    },
    {
      "title": "Dynamic Topology Awareness: Breaking the Granularity Rigidity in Vision-Language Navigation",
      "url": "https://arxiv.org/abs/2601.21751",
      "date": "2026-01-29",
      "authors_text": "Jiankun Peng, Jianyuan Guo, Ying Xu, Yue Liu, Jiashuang Yan",
      "is_highlight": false,
      "score": 84.0,
      "summary": "The paper presents DGNav, a dynamic framework for Vision-Language Navigation that adapts topological map density and connectivity to enhance navigation efficiency and safety in complex environments.",
      "teaser_image": "https://arxiv.org/html/2601.21751/x1.png",
      "debug_abstract": "Vision-Language Navigation in Continuous Environments (VLN-CE) presents a core challenge: grounding high-level linguistic instructions into precise, safe, and long-horizon spatial actions. Explicit topological maps have proven to be a vital solution for providing robust spatial memory in such tasks. However, existing topological planning methods suffer from a &#34;Granularity Rigidity&#34; problem. Specifically, these methods typically rely on fixed geometric thresholds to sample nodes, which fails to adapt to varying environmental complexities. This rigidity leads to a critical mismatch: the model tends to over-sample in simple areas, causing computational redundancy, while under-sampling in high-uncertainty regions, increasing collision risks and compromising precision. To address this, we propose DGNav, a framework for Dynamic Topological Navigation, introducing a context-aware mechanism to modulate map density and connectivity on-the-fly. Our approach comprises two core innovations: (1) A Scene-Aware Adaptive Strategy that dynamically modulates graph construction thresholds based on the dispersion of predicted waypoints, enabling &#34;densification on demand&#34; in challenging environments; (2) A Dynamic Graph Transformer that reconstructs graph connectivity by fusing visual, linguistic, and geometric cues into dynamic edge weights, enabling the agent to filter out topological noise and enhancing instruction adherence. Extensive experiments on the R2R-CE and RxR-CE benchmarks demonstrate DGNav exhibits superior navigation performance and strong generalization capabilities. Furthermore, ablation studies confirm that our framework achieves an optimal trade-off between navigation efficiency and safe exploration. The code is available at this https URL."
    },
    {
      "title": "PLANING: A Loosely Coupled Triangle-Gaussian Framework for Streaming 3D Reconstruction",
      "url": "https://arxiv.org/abs/2601.22046",
      "date": "2026-01-29",
      "authors_text": "Changjian Jiang, Kerui Ren, Xudong Li, Kaiwen Song, Linning Xu",
      "is_highlight": false,
      "score": 66.0,
      "summary": "PLANING is a novel streaming 3D reconstruction framework that efficiently combines geometric primitives and neural Gaussians, enhancing quality and speed while reducing redundancy.",
      "teaser_image": "https://arxiv.org/html/2601.22046/x3.png",
      "debug_abstract": "Streaming reconstruction from monocular image sequences remains challenging, as existing methods typically favor either high-quality rendering or accurate geometry, but rarely both. We present PLANING, an efficient on-the-fly reconstruction framework built on a hybrid representation that loosely couples explicit geometric primitives with neural Gaussians, enabling geometry and appearance to be modeled in a decoupled manner. This decoupling supports an online initialization and optimization strategy that separates geometry and appearance updates, yielding stable streaming reconstruction with substantially reduced structural redundancy. PLANING improves dense mesh Chamfer-L2 by 18.52% over PGSR, surpasses ARTDECO by 1.31 dB PSNR, and reconstructs ScanNetV2 scenes in under 100 seconds, over 5x faster than 2D Gaussian Splatting, while matching the quality of offline per-scene optimization. Beyond reconstruction quality, the structural clarity and computational efficiency of \\modelname~make it well suited for a broad range of downstream applications, such as enabling large-scale scene modeling and simulation-ready environments for embodied AI. Project page: this https URL ."
    },
    {
      "title": "Listen, Look, Drive: Coupling Audio Instructions for User-aware VLA-based Autonomous Driving",
      "url": "https://arxiv.org/abs/2601.12142",
      "date": "2026-01-28",
      "authors_text": "Ziang Guo, Feng Yang, Xuefeng Zhang, Jiaqi Guo, Kun Zhao",
      "is_highlight": false,
      "score": 88.0,
      "summary": "EchoVLA enhances autonomous driving by integrating audio instructions with visual inputs, allowing for user influence and emotional context, significantly improving navigation accuracy and safety.",
      "teaser_image": "https://arxiv.org/html/2601.12142/Figs/overview.png",
      "debug_abstract": "Vision Language Action (VLA) models promise an open-vocabulary interface that can translate perceptual ambiguity into semantically grounded driving decisions, yet they still treat language as a static prior fixed at inference time. As a result, the model must infer continuously shifting objectives from pixels alone, yielding delayed or overly conservative maneuvers. We argue that effective VLAs for autonomous driving need an online channel in which users can influence driving with specific intentions. To this end, we present EchoVLA, a user-aware VLA that couples camera streams with in situ audio instructions. We augment the nuScenes dataset with temporally aligned, intent-specific speech commands generated by converting ego-motion descriptions into synthetic audios. Further, we compose emotional speech-trajectory pairs into a multimodal Chain-of-Thought (CoT) for fine-tuning a Multimodal Large Model (MLM) based on Qwen2.5-Omni. Specifically, we synthesize the audio-augmented dataset with different emotion types paired with corresponding driving behaviors, leveraging the emotional cues embedded in tone, pitch, and speech tempo to reflect varying user states, such as urgent or hesitant intentions, thus enabling our EchoVLA to interpret not only the semantic content but also the emotional context of audio commands for more nuanced and emotionally adaptive driving behavior. In open-loop benchmarks, our approach reduces the average L2 error by $59.4\\%$ and the collision rate by $74.4\\%$ compared to the baseline of vision-only perception. More experiments on nuScenes dataset validate that EchoVLA not only steers the trajectory through audio instructions, but also modulates driving behavior in response to the emotions detected in the user&#39;s speech."
    },
    {
      "title": "Tactile-Force Alignment in Vision-Language-Action Models for Force-aware Manipulation",
      "url": "https://arxiv.org/abs/2601.20321",
      "date": "2026-01-28",
      "authors_text": "Yuzhe Huang, Pei Lin, Wanlin Li, Daohan Li, Jiajun Li",
      "is_highlight": false,
      "score": 91.0,
      "summary": "The paper introduces TaF-VLA, a framework that enhances Vision-Language-Action models by integrating tactile-force alignment for improved force-aware robotic manipulation in contact-rich tasks.",
      "teaser_image": "https://arxiv.org/html/2601.20321/figures/fig1-teaser.png",
      "debug_abstract": "Vision-Language-Action (VLA) models have recently emerged as powerful generalists for robotic manipulation. However, due to their predominant reliance on visual modalities, they fundamentally lack the physical intuition required for contact-rich tasks that require precise force regulation and physical reasoning. Existing attempts to incorporate vision-based tactile sensing into VLA models typically treat tactile inputs as auxiliary visual textures, thereby overlooking the underlying correlation between surface deformation and interaction dynamics. To bridge this gap, we propose a paradigm shift from tactile-vision alignment to tactile-force alignment. Here, we introduce TaF-VLA, a framework that explicitly grounds high-dimensional tactile observations in physical interaction forces. To facilitate this, we develop an automated tactile-force data acquisition device and curate the TaF-Dataset, comprising over 10 million synchronized tactile observations, 6-axis force/torque, and matrix force map. To align sequential tactile observations with interaction forces, the central component of our approach is the Tactile-Force Adapter (TaF-Adapter), a tactile sensor encoder that extracts discretized latent information for encoding tactile observations. This mechanism ensures that the learned representations capture history-dependent, noise-insensitive physical dynamics rather than static visual textures. Finally, we integrate this force-aligned encoder into a VLA backbone. Extensive real-world experiments demonstrate that TaF-VLA policy significantly outperforms state-of-the-art tactile-vision-aligned and vision-only baselines on contact-rich tasks, verifying its ability to achieve robust, force-aware manipulation through cross-modal physical reasoning."
    },
    {
      "title": "Rhombot: Rhombus-shaped Modular Robots for Stable, Medium-Independent Reconfiguration Motion",
      "url": "https://arxiv.org/abs/2601.19529",
      "date": "2026-01-27",
      "authors_text": "Jie Gu, Yirui Sun, Zhihao Xia, Tin Lun Lam, Chunxu Tian",
      "is_highlight": false,
      "score": 86.0,
      "summary": "Rhombot is a modular self-reconfigurable robot featuring rhombus-shaped modules that enable stable, medium-independent morphing and locomotion through a novel motion primitive called morphpivoting.",
      "debug_abstract": "In this paper, we present Rhombot, a novel deformable planar lattice modular self-reconfigurable robot (MSRR) with a rhombus shaped module. Each module consists of a parallelogram skeleton with a single centrally mounted actuator that enables folding and unfolding along its diagonal. The core design philosophy is to achieve essential MSRR functionalities such as morphing, docking, and locomotion with minimal control complexity. This enables a continuous and stable reconfiguration process that is independent of the surrounding medium, allowing the system to reliably form various configurations in diverse environments. To leverage the unique kinematics of Rhombot, we introduce morphpivoting, a novel motion primitive for reconfiguration that differs from advanced MSRR systems, and propose a strategy for its continuous execution. Finally, a series of physical experiments validate the module&#39;s stable reconfiguration ability, as well as its positional and docking accuracy."
    },
    {
      "title": "Component-Level Lesioning of Language Models Reveals Clinically Aligned Aphasia Phenotypes",
      "url": "https://arxiv.org/abs/2601.19723",
      "date": "2026-01-27",
      "authors_text": "Yifan Wang, Jichen Zheng, Jingyuan Sun, Yunhao Zhang, Chunyu Ye",
      "is_highlight": false,
      "score": 62.0,
      "summary": "This study presents a framework for simulating aphasia in language models by selectively perturbing components, revealing clinically relevant language impairments and enhancing understanding of language function degradation.",
      "debug_abstract": "Large language models (LLMs) increasingly exhibit human-like linguistic behaviors and internal representations that they could serve as computational simulators of language cognition. We ask whether LLMs can be systematically manipulated to reproduce language-production impairments characteristic of aphasia following focal brain lesions. Such models could provide scalable proxies for testing rehabilitation hypotheses, and offer a controlled framework for probing the functional organization of language. We introduce a clinically grounded, component-level framework that simulates aphasia by selectively perturbing functional components in LLMs, and apply it to both modular Mixture-of-Experts models and dense Transformers using a unified intervention interface. Our pipeline (i) identifies subtype-linked components for Broca&#39;s and Wernicke&#39;s aphasia, (ii) interprets these components with linguistic probing tasks, and (iii) induces graded impairments by progressively perturbing the top-k subtype-linked components, evaluating outcomes with Western Aphasia Battery (WAB) subtests summarized by Aphasia Quotient (AQ). Across architectures and lesioning strategies, subtype-targeted perturbations yield more systematic, aphasia-like regressions than size-matched random perturbations, and MoE modularity supports more localized and interpretable phenotype-to-component mappings. These findings suggest that modular LLMs, combined with clinically informed component perturbations, provide a promising platform for simulating aphasic language production and studying how distinct language functions degrade under targeted disruptions."
    },
    {
      "title": "EgoHandICL: Egocentric 3D Hand Reconstruction with In-Context Learning",
      "url": "https://arxiv.org/abs/2601.19850",
      "date": "2026-01-27",
      "authors_text": "Binzhu Xie, Shi Qiu, Sicheng Zhang, Yinqiao Wang, Hao Xu",
      "is_highlight": false,
      "score": 76.0,
      "summary": "EgoHandICL introduces an innovative in-context learning framework for robust 3D hand reconstruction in egocentric vision, enhancing performance through exemplar retrieval and multimodal context integration.",
      "debug_abstract": "Robust 3D hand reconstruction in egocentric vision is challenging due to depth ambiguity, self-occlusion, and complex hand-object interactions. Prior methods mitigate these issues by scaling training data or adding auxiliary cues, but they often struggle in unseen contexts. We present EgoHandICL, the first in-context learning (ICL) framework for 3D hand reconstruction that improves semantic alignment, visual consistency, and robustness under challenging egocentric conditions. EgoHandICL introduces complementary exemplar retrieval guided by vision-language models (VLMs), an ICL-tailored tokenizer for multimodal context, and a masked autoencoder (MAE)-based architecture trained with hand-guided geometric and perceptual objectives. Experiments on ARCTIC and EgoExo4D show consistent gains over state-of-the-art methods. We also demonstrate real-world generalization and improve EgoVLM hand-object interaction reasoning by using reconstructed hands as visual prompts. Code and data: this https URL"
    },
    {
      "title": "Advances and Innovations in the Multi-Agent Robotic System (MARS) Challenge",
      "url": "https://arxiv.org/abs/2601.18733",
      "date": "2026-01-26",
      "authors_text": "Li Kang, Heng Zhou, Xiufeng Song, Rui Li, Bruno N. Y. Chen",
      "is_highlight": false,
      "score": 94.0,
      "summary": "The MARS Challenge promotes advancements in multi-agent robotic systems by exploring vision-language models for collaborative task execution and planning in dynamic environments.",
      "debug_abstract": "Recent advancements in multimodal large language models and vision-languageaction models have significantly driven progress in Embodied AI. As the field transitions toward more complex task scenarios, multi-agent system frameworks are becoming essential for achieving scalable, efficient, and collaborative solutions. This shift is fueled by three primary factors: increasing agent capabilities, enhancing system efficiency through task delegation, and enabling advanced human-agent interactions. To address the challenges posed by multi-agent collaboration, we propose the Multi-Agent Robotic System (MARS) Challenge, held at the NeurIPS 2025 Workshop on SpaVLE. The competition focuses on two critical areas: planning and control, where participants explore multi-agent embodied planning using vision-language models (VLMs) to coordinate tasks and policy execution to perform robotic manipulation in dynamic environments. By evaluating solutions submitted by participants, the challenge provides valuable insights into the design and coordination of embodied multi-agent systems, contributing to the future development of advanced collaborative AI systems."
    },
    {
      "title": "Fast and Safe Trajectory Optimization for Mobile Manipulators With Neural Configuration Space Distance Field",
      "url": "https://arxiv.org/abs/2601.18548",
      "date": "2026-01-26",
      "authors_text": "Yulin Li, Zhiyuan Song, Yiming Li, Zhicheng Song, Kai Chen",
      "is_highlight": false,
      "score": 86.0,
      "summary": "This paper introduces Generalized Configuration Space Distance Fields (GCDF) for efficient trajectory optimization in mobile manipulators, enhancing collision reasoning and enabling agile motion in complex environments.",
      "debug_abstract": "Mobile manipulators promise agile, long-horizon behavior by coordinating base and arm motion, yet whole-body trajectory optimization in cluttered, confined spaces remains difficult due to high-dimensional nonconvexity and the need for fast, accurate collision reasoning. Configuration Space Distance Fields (CDF) enable fixed-base manipulators to model collisions directly in configuration space via smooth, implicit distances. This representation holds strong potential to bypass the nonlinear configuration-to-workspace mapping while preserving accurate whole-body geometry and providing optimization-friendly collision costs. Yet, extending this capability to mobile manipulators is hindered by unbounded workspaces and tighter base-arm coupling. We lift this promise to mobile manipulation with Generalized Configuration Space Distance Fields (GCDF), extending CDF to robots with both translational and rotational joints in unbounded workspaces with tighter base-arm coupling. We prove that GCDF preserves Euclidean-like local distance structure and accurately encodes whole-body geometry in configuration space, and develop a data generation and training pipeline that yields continuous neural GCDFs with accurate values and gradients, supporting efficient GPU-batched queries. Building on this representation, we develop a high-performance sequential convex optimization framework centered on GCDF-based collision reasoning. The solver scales to large numbers of implicit constraints through (i) online specification of neural constraints, (ii) sparsity-aware active-set detection with parallel batched evaluation across thousands of constraints, and (iii) incremental constraint management for rapid replanning under scene changes."
    },
    {
      "title": "Resisting Manipulative Bots in Meme Coin Copy Trading: A Multi-Agent Approach with Chain-of-Thought Reasoning",
      "url": "https://arxiv.org/abs/2601.08641",
      "date": "2026-01-26",
      "authors_text": "Yichen Luo, Yebo Feng, Jiahua Xu, Yang Liu",
      "is_highlight": false,
      "score": 67.0,
      "summary": "This paper presents a multi-agent system utilizing explainable LLMs to defend against manipulative bots in meme coin copy trading, enhancing prediction accuracy and economic performance.",
      "debug_abstract": "Copy trading has become the dominant entry strategy in meme coin markets. However, due to the market&#39;s extreme illiquid and volatile nature, the strategy exposes an exploitable attack surface: adversaries deploy manipulative bots to front-run trades, conceal positions, and fabricate sentiment, systematically extracting value from naïve copiers at scale. Despite its prevalence, bot-driven manipulation remains largely unexplored, and no robust defensive framework exists. We propose a manipulation-resistant copy-trading system based on a multi-agent architecture powered by a multi-modal, explainable large language model (LLM). Our system decomposes copy trading into three specialized agents for coin evaluation, wallet selection, and timing assessment. Evaluated on historical data from over 6,000 meme coins, our approach outperforms zero-shot and most statistic-driven baselines in prediction accuracy as well as all baselines in economic performance, achieving an average return of 14% for identified smart-money trades and an estimated copier return of 3% per trade under realistic market frictions. Overall, our results demonstrate the effectiveness of agent-based defenses and predictability of trader profitability in adversarial meme coin markets, providing a practical foundation for robust copy trading."
    },
    {
      "title": "AdaReasoner: Dynamic Tool Orchestration for Iterative Visual Reasoning",
      "url": "https://arxiv.org/abs/2601.18631",
      "date": "2026-01-26",
      "authors_text": "Mingyang Song, Haoyu Sun, Jiawei Gu, Linjie Li, Luxin Xu",
      "is_highlight": false,
      "score": 61.0,
      "summary": "AdaReasoner is a multimodal model that autonomously optimizes tool selection and usage for iterative visual reasoning, achieving state-of-the-art performance on various benchmarks.",
      "debug_abstract": "When humans face problems beyond their immediate capabilities, they rely on tools, providing a promising paradigm for improving visual reasoning in multimodal large language models (MLLMs). Effective reasoning, therefore, hinges on knowing which tools to use, when to invoke them, and how to compose them over multiple steps, even when faced with new tools or new tasks. We introduce \\textbf{AdaReasoner}, a family of multimodal models that learn tool use as a general reasoning skill rather than as tool-specific or explicitly supervised behavior. AdaReasoner is enabled by (i) a scalable data curation pipeline exposing models to long-horizon, multi-step tool interactions; (ii) Tool-GRPO, a reinforcement learning algorithm that optimizes tool selection and sequencing based on end-task success; and (iii) an adaptive learning mechanism that dynamically regulates tool usage. Together, these components allow models to infer tool utility from task context and intermediate outcomes, enabling coordination of multiple tools and generalization to unseen tools. Empirically, AdaReasoner exhibits strong tool-adaptive and generalization behaviors: it autonomously adopts beneficial tools, suppresses irrelevant ones, and adjusts tool usage frequency based on task demands, despite never being explicitly trained to do so. These capabilities translate into state-of-the-art performance across challenging benchmarks, improving the 7B base model by +24.9\\% on average and surpassing strong proprietary systems such as GPT-5 on multiple tasks, including VSP and Jigsaw."
    },
    {
      "title": "Timely Machine: Awareness of Time Makes Test-Time Scaling Agentic",
      "url": "https://arxiv.org/abs/2601.16486",
      "date": "2026-01-23",
      "authors_text": "Yichuan Ma, Linyang Li, Yongkang chen, Peiji Li, Xiaozhe Li",
      "is_highlight": false,
      "score": 78.0,
      "summary": "The paper introduces Timely Machine, redefining test-time scaling for LLMs based on wall-clock time, enhancing performance through dynamic strategy adjustments and reinforcement learning.",
      "debug_abstract": "As large language models (LLMs) increasingly tackle complex reasoning tasks, test-time scaling has become critical for enhancing capabilities. However, in agentic scenarios with frequent tool calls, the traditional generation-length-based definition breaks down: tool latency decouples inference time from generation length. We propose Timely Machine, redefining test-time as wall-clock time, where models dynamically adjust strategies based on time budgets. We introduce Timely-Eval, a benchmark spanning high-frequency tool calls, low-frequency tool calls, and time-constrained reasoning. By varying tool latency, we find smaller models excel with fast feedback through more interactions, while larger models dominate high-latency settings via superior interaction quality. Moreover, existing models fail to adapt reasoning to time budgets. We propose Timely-RL to address this gap. After cold-start supervised fine-tuning, we use reinforcement learning to enhance temporal planning. Timely-RL improves time budget awareness and consistently boosts performance across Timely-Eval. We hope our work offers a new perspective on test-time scaling for the agentic era."
    },
    {
      "title": "Adaptive Reinforcement and Model Predictive Control Switching for Safe Human-Robot Cooperative Navigation",
      "url": "https://arxiv.org/abs/2601.16686",
      "date": "2026-01-23",
      "authors_text": "Ning Liu, Sen Shen, Zheng Li, Matthew D'Souza, Jen Jen Chung",
      "is_highlight": false,
      "score": 94.0,
      "summary": "The paper presents ARMS, a hybrid control framework combining reinforcement learning and model predictive control for safe human-robot navigation, achieving superior performance in cluttered environments.",
      "debug_abstract": "This paper addresses the challenge of human-guided navigation for mobile collaborative robots under simultaneous proximity regulation and safety constraints. We introduce Adaptive Reinforcement and Model Predictive Control Switching (ARMS), a hybrid learning-control framework that integrates a reinforcement learning follower trained with Proximal Policy Optimization (PPO) and an analytical one-step Model Predictive Control (MPC) formulated as a quadratic program safety filter. To enable robust perception under partial observability and non-stationary human motion, ARMS employs a decoupled sensing architecture with a Long Short-Term Memory (LSTM) temporal encoder for the human-robot relative state and a spatial encoder for 360-degree LiDAR scans. The core contribution is a learned adaptive neural switcher that performs context-aware soft action fusion between the two controllers, favoring conservative, constraint-aware QP-based control in low-risk regions while progressively shifting control authority to the learned follower in highly cluttered or constrained scenarios where maneuverability is critical, and reverting to the follower action when the QP becomes infeasible. Extensive evaluations against Pure Pursuit, Dynamic Window Approach (DWA), and an RL-only baseline demonstrate that ARMS achieves an 82.5 percent success rate in highly cluttered environments, outperforming DWA and RL-only approaches by 7.1 percent and 3.1 percent, respectively, while reducing average computational latency by 33 percent to 5.2 milliseconds compared to a multi-step MPC baseline. Additional simulation transfer in Gazebo and initial real-world deployment results further indicate the practicality and robustness of ARMS for safe and efficient human-robot collaboration. Source code and a demonstration video are available at this https URL."
    },
    {
      "title": "Natural Language-Driven Global Mapping of Martian Landforms",
      "url": "https://arxiv.org/abs/2601.15949",
      "date": "2026-01-22",
      "authors_text": "Yiran Wang, Shuoyuan Wang, Zhaoran Wei, Jiannan Zhao, Zhonghua Yao",
      "is_highlight": false,
      "score": 72.0,
      "summary": "MarScope is a vision-language framework that enables natural language-driven, label-free mapping of Martian landforms, enhancing global geomorphic analysis and user query flexibility.",
      "debug_abstract": "Planetary surfaces are typically analyzed using high-level semantic concepts in natural language, yet vast orbital image archives remain organized at the pixel level. This mismatch limits scalable, open-ended exploration of planetary surfaces. Here we present MarScope, a planetary-scale vision-language framework enabling natural language-driven, label-free mapping of Martian landforms. MarScope aligns planetary images and text in a shared semantic space, trained on over 200,000 curated image-text pairs. This framework transforms global geomorphic mapping on Mars by replacing pre-defined classifications with flexible semantic retrieval, enabling arbitrary user queries across the entire planet in 5 seconds with F1 scores up to 0.978. Applications further show that it extends beyond morphological classification to facilitate process-oriented analysis and similarity-based geomorphological mapping at a planetary scale. MarScope establishes a new paradigm where natural language serves as a direct interface for scientific discovery over massive geospatial datasets."
    },
    {
      "title": "Robust Tool Use via Fission-GRPO: Learning to Recover from Execution Errors",
      "url": "https://arxiv.org/abs/2601.15625",
      "date": "2026-01-22",
      "authors_text": "Zhiwei Zhang, Fei Zhao, Rui Wang, Zezhong Wang, Bin Liang",
      "is_highlight": false,
      "score": 87.1,
      "summary": "Fission-GRPO enhances tool use in LLMs by converting execution errors into corrective supervision, improving error recovery and overall accuracy in multi-turn interactions.",
      "debug_abstract": "Large language models (LLMs) can call tools effectively, yet they remain brittle in multi-turn execution: following a tool call error, smaller models often degenerate into repetitive invalid re-invocations, failing to interpret error feedback and self-correct. This brittleness hinders reliable real-world deployment, where the execution errors are inherently inevitable during tool interaction procedures. We identify a key limitation of current approaches: standard reinforcement learning (RL) treats errors as sparse negative rewards, providing no guidance on how to recover, while pre-collected synthetic error-correction datasets suffer from distribution mismatch with the model&#39;s on-policy error modes. To bridge this gap, we propose Fission-GRPO, a framework that converts execution errors into corrective supervision within the RL training loop. Our core mechanism fissions each failed trajectory into a new training instance by augmenting it with diagnostic feedback from a finetuned Error Simulator, then resampling recovery rollouts on-policy. This enables the model to learn from the precise errors it makes during exploration, rather than from static, pre-collected error cases. On the BFCL v4 Multi-Turn, Fission-GRPO improves the error recovery rate of Qwen3-8B by 5.7% absolute, crucially, yielding a 4% overall accuracy gain (42.75% to 46.75%) over GRPO and outperforming specialized tool-use agents."
    },
    {
      "title": "PhysicsMind: Sim and Real Mechanics Benchmarking for Physical Reasoning and Prediction in Foundational VLMs and World Models",
      "url": "https://arxiv.org/abs/2601.16007",
      "date": "2026-01-22",
      "authors_text": "Chak-Wing Mak, Guanyu Zhu, Boyi Zhang, Hongji Li, Xiaowei Chi",
      "is_highlight": false,
      "score": 94.1,
      "summary": "PhysicsMind introduces a benchmark for evaluating physical reasoning in multimodal models through real and simulated tasks focused on fundamental physics principles, revealing significant gaps in current models' understanding.",
      "debug_abstract": "Modern foundational Multimodal Large Language Models (MLLMs) and video world models have advanced significantly in mathematical, common-sense, and visual reasoning, but their grasp of the underlying physics remains underexplored. Existing benchmarks attempting to measure this matter rely on synthetic, Visual Question Answer templates or focus on perceptual video quality that is tangential to measuring how well the video abides by physical laws. To address this fragmentation, we introduce PhysicsMind, a unified benchmark with both real and simulation environments that evaluates law-consistent reasoning and generation over three canonical principles: Center of Mass, Lever Equilibrium, and Newton&#39;s First Law. PhysicsMind comprises two main tasks: i) VQA tasks, testing whether models can reason and determine physical quantities and values from images or short videos, and ii) Video Generation(VG) tasks, evaluating if predicted motion trajectories obey the same center-of-mass, torque, and inertial constraints as the ground truth. A broad range of recent models and video generation models is evaluated on PhysicsMind and found to rely on appearance heuristics while often violating basic mechanics. These gaps indicate that current scaling and training are still insufficient for robust physical understanding, underscoring PhysicsMind as a focused testbed for physics-aware multimodal models. Our data will be released upon acceptance."
    }
  ],
  "集群机器人系统实验室 (MagicLab)": [
    {
      "title": "SlowFocus: Enhancing Fine-grained Temporal Understanding in Video LLM",
      "url": "https://arxiv.org/abs/2602.03589",
      "date": "2026-02-03",
      "authors_text": "Ming Nie, Dan Ding, Chunwei Wang, Yuanfan Guo, Jianhua Han",
      "is_highlight": false,
      "score": 54.0,
      "summary": "The SlowFocus mechanism enhances video LLMs' fine-grained temporal understanding by improving sampling frequency and integrating local high-frequency features with global contexts.",
      "teaser_image": "https://arxiv.org/html/2602.03589/x2.png",
      "debug_abstract": "Large language models (LLMs) have demonstrated exceptional capabilities in text understanding, which has paved the way for their expansion into video LLMs (Vid-LLMs) to analyze video data. However, current Vid-LLMs struggle to simultaneously retain high-quality frame-level semantic information (i.e., a sufficient number of tokens per frame) and comprehensive video-level temporal information (i.e., an adequate number of sampled frames per video). This limitation hinders the advancement of Vid-LLMs towards fine-grained video understanding. To address this issue, we introduce the SlowFocus mechanism, which significantly enhances the equivalent sampling frequency without compromising the quality of frame-level visual tokens. SlowFocus begins by identifying the query-related temporal segment based on the posed question, then performs dense sampling on this segment to extract local high-frequency features. A multi-frequency mixing attention module is further leveraged to aggregate these local high-frequency details with global low-frequency contexts for enhanced temporal comprehension. Additionally, to tailor Vid-LLMs to this innovative mechanism, we introduce a set of training strategies aimed at bolstering both temporal grounding and detailed temporal reasoning capabilities. Furthermore, we establish FineAction-CGR, a benchmark specifically devised to assess the ability of Vid-LLMs to process fine-grained temporal understanding tasks. Comprehensive experiments demonstrate the superiority of our mechanism across both existing public video understanding benchmarks and our proposed FineAction-CGR."
    },
    {
      "title": "UniMotion: A Unified Motion Framework for Simulation, Prediction and Planning",
      "url": "https://arxiv.org/abs/2602.00566",
      "date": "2026-01-31",
      "authors_text": "Nan Song, Junzhe Jiang, Jingyu Li, Xiatian Zhu, Li Zhang",
      "is_highlight": false,
      "score": 89.0,
      "summary": "UniMotion is a unified framework leveraging a Transformer architecture to enhance simulation, prediction, and planning in autonomous driving by integrating shared capabilities across these tasks.",
      "teaser_image": "https://arxiv.org/html/2602.00566/x1.png",
      "debug_abstract": "Motion simulation, prediction and planning are foundational tasks in autonomous driving, each essential for modeling and reasoning about dynamic traffic scenarios. While often addressed in isolation due to their differing objectives, such as generating diverse motion states or estimating optimal trajectories, these tasks inherently depend on shared capabilities: understanding multi-agent interactions, modeling motion behaviors, and reasoning over temporal and spatial dynamics. Despite this underlying commonality, existing approaches typically adopt specialized model designs, which hinders cross-task generalization and system scalability. More critically, this separation overlooks the potential mutual benefits among tasks. Motivated by these observations, we propose UniMotion, a unified motion framework that captures shared structures across motion tasks while accommodating their individual requirements. Built on a decoder-only Transformer architecture, UniMotion employs dedicated interaction modes and tailored training strategies to simultaneously support these motion tasks. This unified design not only enables joint optimization and representation sharing but also allows for targeted fine-tuning to specialize in individual tasks when needed. Extensive experiments on the Waymo Open Motion Dataset demonstrate that joint training leads to robust generalization and effective task integration. With further fine-tuning, UniMotion achieves state-of-the-art performance across a range of motion tasks, establishing it as a versatile and scalable solution for autonomous driving."
    },
    {
      "title": "VVLoc: Prior-free 3-DoF Vehicle Visual Localization",
      "url": "https://arxiv.org/abs/2602.00810",
      "date": "2026-01-31",
      "authors_text": "Ze Huang, Zhongyang Xiao, Mingliang Song, Longan Yang, Hongyuan Yuan",
      "is_highlight": false,
      "score": 73.0,
      "summary": "VVLoc is a unified neural network approach for simultaneous topological and metric vehicle localization using multi-camera systems, providing confidence measures and requiring minimal training data.",
      "teaser_image": "https://arxiv.org/html/2602.00810/x1.png",
      "debug_abstract": "Localization is a critical technology in autonomous driving, encompassing both topological localization, which identifies the most similar map keyframe to the current observation, and metric localization, which provides precise spatial coordinates. Conventional methods typically address these tasks independently, rely on single-camera setups, and often require additional 3D semantic or pose priors, while lacking mechanisms to quantify the confidence of localization results, making them less feasible for real industrial applications. In this paper, we propose VVLoc, a unified pipeline that employs a single neural network to concurrently achieve topological and metric vehicle localization using multi-camera system. VVLoc first evaluates the geo-proximity between visual observations, then estimates their relative metric poses using a matching strategy, while also providing a confidence measure. Additionally, the training process for VVLoc is highly efficient, requiring only pairs of visual data and corresponding ground-truth poses, eliminating the need for complex supplementary data. We evaluate VVLoc not only on the publicly available datasets, but also on a more challenging self-collected dataset, demonstrating its ability to deliver state-of-the-art localization accuracy across a wide range of localization tasks."
    },
    {
      "title": "Beyond Precision: Training-Inference Mismatch is an Optimization Problem and Simple LR Scheduling Fixes It",
      "url": "https://arxiv.org/abs/2602.01826",
      "date": "2026-02-02",
      "authors_text": "Yaxiang Zhang, Yingru Li, Jiacai Liu, Jiawei Xu, Ziniu Li",
      "is_highlight": false,
      "score": 35.0,
      "summary": "This paper presents a dynamic learning rate scheduling method to mitigate training-inference mismatch and stabilize reinforcement learning in large language models by addressing gradient noise.",
      "teaser_image": "https://arxiv.org/html/2602.01826/Qwen3-4B_baseline_vs_importance_sampling_patches.png",
      "debug_abstract": "Reinforcement Learning (RL) for training Large Language Models is notoriously unstable. While recent studies attribute this to &#34;training inference mismatch stemming&#34; from inconsistent hybrid engines, standard remedies, such as Importance Sampling, might fail during extended training runs. In this work, we analyze this instability through the lens of optimization, demonstrating that gradient noise and training-inference mismatch escalate in tandem as training progresses. Meanwhile, we find that the mismatch can be effectively suppressed by shrinking the update size. Taken together, we deduce that the mismatch is not merely a static numerical discrepancy, but a dynamic failure coupled with the model&#39;s optimization. Based on this insight, we propose a simple yet effective solution: a specialized Learning Rate (LR) scheduler. Instead of pre-defined decay schedule in traditional LR scheduler, our method dynamically triggers LR decay based on response length, which we identify as a reliable early-warning signal for impending instability. Empirical evidence suggests that by reducing the learning rate as gradient noise rises, we can consistently stabilize RL training and keep the training-inference mismatch at a safe level."
    },
    {
      "title": "Enhancing Indoor Occupancy Prediction via Sparse Query-Based Multi-Level Consistent Knowledge Distillation",
      "url": "https://arxiv.org/abs/2602.02318",
      "date": "2026-02-02",
      "authors_text": "Xiang Li, Yupeng Zheng, Pengfei Li, Yilun Chen, Ya-Qin Zhang",
      "is_highlight": false,
      "score": 0,
      "summary": "DiScene introduces a sparse query-based framework for efficient and robust indoor occupancy prediction through multi-level knowledge distillation, achieving state-of-the-art performance and faster inference.",
      "teaser_image": "https://arxiv.org/html/2602.02318/x2.png",
      "debug_abstract": "Occupancy prediction provides critical geometric and semantic understanding for robotics but faces efficiency-accuracy trade-offs. Current dense methods suffer computational waste on empty voxels, while sparse query-based approaches lack robustness in diverse and complex indoor scenes. In this paper, we propose DiScene, a novel sparse query-based framework that leverages multi-level distillation to achieve efficient and robust occupancy prediction. In particular, our method incorporates two key innovations: (1) a Multi-level Consistent Knowledge Distillation strategy, which transfers hierarchical representations from large teacher models to lightweight students through coordinated alignment across four levels, including encoder-level feature alignment, query-level feature matching, prior-level spatial guidance, and anchor-level high-confidence knowledge transfer and (2) a Teacher-Guided Initialization policy, employing optimized parameter warm-up to accelerate model convergence. Validated on the Occ-Scannet benchmark, DiScene achieves 23.2 FPS without depth priors while outperforming our baseline method, OPUS, by 36.1% and even better than the depth-enhanced version, OPUS†. With depth integration, DiScene† attains new SOTA performance, surpassing EmbodiedOcc by 3.7% with 1.62$\\times$ faster inference speed. Furthermore, experiments on the Occ3D-nuScenes benchmark and in-the-wild scenarios demonstrate the versatility of our approach in various environments. Code and models can be accessed at this https URL."
    },
    {
      "title": "MAGNET: Towards Adaptive GUI Agents with Memory-Driven Knowledge Evolution",
      "url": "https://arxiv.org/abs/2601.19199",
      "date": "2026-01-27",
      "authors_text": "Libo Sun, Jiwen Zhang, Siyuan Wang, Zhongyu Wei",
      "is_highlight": false,
      "score": 75.0,
      "summary": "MAGNET introduces a memory-driven adaptive agent framework that enhances mobile GUI agents' performance by linking visual features to stable semantics and refining task intents amidst UI changes.",
      "debug_abstract": "Mobile GUI agents powered by large foundation models enable autonomous task execution, but frequent updates altering UI appearance and reorganizing workflows cause agents trained on historical data to fail. Despite surface changes, functional semantics and task intents remain fundamentally stable. Building on this insight, we introduce MAGNET, a memory-driven adaptive agent framework with dual-level memory: stationary memory linking diverse visual features to stable functional semantics for robust action grounding and procedural memory capturing stable task intents across varying workflows. We propose a dynamic memory evolution mechanism that continuously refines both memories by prioritizing frequently accessed knowledge. Online benchmark AndroidWorld evaluations show substantial improvements over baselines, while offline benchmarks confirm consistent gains under distribution shifts. These results validate that leveraging stable structures across interface changes improves agent performance and generalization in evolving software environments."
    },
    {
      "title": "Self-Reconfiguration Planning for Deformable Quadrilateral Modular Robots",
      "url": "https://arxiv.org/abs/2601.19496",
      "date": "2026-01-27",
      "authors_text": "Jie Gu, Hongrun Gao, Zhihao Xia, Yirun Sun, Chunxu Tian",
      "is_highlight": false,
      "score": 91.0,
      "summary": "This paper introduces a self-reconfiguration planning algorithm for deformable quadrilateral modular robots that ensures stable connections and efficient execution sequences through a novel graph-based approach.",
      "debug_abstract": "For lattice modular self-reconfigurable robots (MSRRs), maintaining stable connections during reconfiguration is crucial for physical feasibility and deployability. This letter presents a novel self-reconfiguration planning algorithm for deformable quadrilateral MSRRs that guarantees stable connection. The method first constructs feasible connect/disconnect actions using a virtual graph representation, and then organizes these actions into a valid execution sequence through a Dependence-based Reverse Tree (DRTree) that resolves interdependencies. We also prove that reconfiguration sequences satisfying motion characteristics exist for any pair of configurations with seven or more modules (excluding linear topologies). Finally, comparisons with a modified BiRRT algorithm highlight the superior efficiency and stability of our approach, while deployment on a physical robotic platform confirms its practical feasibility."
    },
    {
      "title": "Rhombot: Rhombus-shaped Modular Robots for Stable, Medium-Independent Reconfiguration Motion",
      "url": "https://arxiv.org/abs/2601.19529",
      "date": "2026-01-27",
      "authors_text": "Jie Gu, Yirui Sun, Zhihao Xia, Tin Lun Lam, Chunxu Tian",
      "is_highlight": false,
      "score": 86.0,
      "summary": "Rhombot is a modular self-reconfigurable robot featuring rhombus-shaped modules that enable stable, medium-independent morphing and locomotion through a novel motion primitive called morphpivoting.",
      "debug_abstract": "In this paper, we present Rhombot, a novel deformable planar lattice modular self-reconfigurable robot (MSRR) with a rhombus shaped module. Each module consists of a parallelogram skeleton with a single centrally mounted actuator that enables folding and unfolding along its diagonal. The core design philosophy is to achieve essential MSRR functionalities such as morphing, docking, and locomotion with minimal control complexity. This enables a continuous and stable reconfiguration process that is independent of the surrounding medium, allowing the system to reliably form various configurations in diverse environments. To leverage the unique kinematics of Rhombot, we introduce morphpivoting, a novel motion primitive for reconfiguration that differs from advanced MSRR systems, and propose a strategy for its continuous execution. Finally, a series of physical experiments validate the module&#39;s stable reconfiguration ability, as well as its positional and docking accuracy."
    },
    {
      "title": "MARO: Learning Stronger Reasoning from Social Interaction",
      "url": "https://arxiv.org/abs/2601.12323",
      "date": "2026-01-24",
      "authors_text": "Yin Cai, Zhouhong Gu, Juntao Zhang, Ping Chen",
      "is_highlight": false,
      "score": 70.0,
      "summary": "MARO enhances large language models' reasoning by enabling learning through multi-agent social interactions, addressing sparse signals, role balance, and environmental instability, with transferable improvements.",
      "debug_abstract": "Humans face countless scenarios that require reasoning and judgment in daily life. However, existing large language model training methods primarily allow models to learn from existing textual content or solve predetermined problems, lacking experience in real scenarios involving interaction, negotiation, and competition with others. To address this, this paper proposes Multi-Agent Reward Optimization (MARO), a method that enables large language models (LLMs) to acquire stronger reasoning abilities by learning and practicing in multi-agent social environments. Specifically, MARO first addresses the sparse learning signal problem by decomposing final success or failure outcomes into each specific behavior during the interaction process; second, it handles the uneven role distribution problem by balancing the training sample weights of different roles; finally, it addresses environmental instability issues by directly evaluating the utility of each behavior. Experimental results demonstrate that MARO not only achieves significant improvements in social reasoning capabilities, but also that the abilities acquired through social simulation learning can effectively transfer to other tasks such as mathematical reasoning and instruction following. This reveals the tremendous potential of multi-agent social learning in enhancing the general reasoning capabilities of LLMs."
    },
    {
      "title": "MetaWorld: Skill Transfer and Composition in a Hierarchical World Model for Grounding High-Level Instructions",
      "url": "https://arxiv.org/abs/2601.17507",
      "date": "2026-01-24",
      "authors_text": "Yutong Shen, Hangxu Liu, Kailin Pei, Ruizhe Xia, Tongtong Feng",
      "is_highlight": false,
      "score": 92.0,
      "summary": "MetaWorld introduces a hierarchical world model that enhances humanoid robot loco-manipulation by integrating semantic planning and physical control through expert policy transfer, improving task performance and motion coherence.",
      "debug_abstract": "Humanoid robot loco-manipulation remains constrained by the semantic-physical gap. Current methods face three limitations: Low sample efficiency in reinforcement learning, poor generalization in imitation learning, and physical inconsistency in VLMs. We propose MetaWorld, a hierarchical world model that integrates semantic planning and physical control via expert policy transfer. The framework decouples tasks into a VLM-driven semantic layer and a latent dynamics model operating in a compact state space. Our dynamic expert selection and motion prior fusion mechanism leverages a pre-trained multi-expert policy library as transferable knowledge, enabling efficient online adaptation via a two-stage framework. VLMs serve as semantic interfaces, mapping instructions to executable skills and bypassing symbol grounding. Experiments on Humanoid-Bench show MetaWorld outperforms world model-based RL in task completion and motion coherence. Our code will be found at this https URL"
    },
    {
      "title": "SwipeGen: Bridging the Execution Gap in GUI Agents via Human-like Swipe Synthesis",
      "url": "https://arxiv.org/abs/2601.18305",
      "date": "2026-01-26",
      "authors_text": "Xuan Wang, Siyuan Su, Quantong Fu, Yongxiang Hu, Yangfan Zhou",
      "is_highlight": false,
      "score": 82.0,
      "summary": "SwipeGen introduces a method for synthesizing human-like swipe interactions in GUI agents, significantly enhancing their execution accuracy and establishing a benchmark for evaluation.",
      "debug_abstract": "With the widespread adoption of Graphical User Interface (GUI) agents for automating GUI interaction tasks, substantial research focused on improving GUI perception to ground task instructions into concrete action steps. However, the step execution capability of these agents has gradually emerged as a new bottleneck for task completion. In particular, existing GUI agents often adopt overly simplified strategies for handling swipe interactions, preventing them from accurately replicating human-like behavior. To address this limitation, we decompose human swipe gestures into multiple quantifiable dimensions and propose an automated pipeline SwipeGen to synthesize human-like swipe interactions through GUI exploration. Based on this pipeline, we construct and release the first benchmark for evaluating the swipe execution capability of GUI agents. Furthermore, leveraging the synthesized data, we propose GUISwiper, a GUI agent with enhanced interaction execution capabilities. Experimental results demonstrate that GUISwiper achieves a swipe execution accuracy of 69.07%, representing a 214% improvement over existing VLM baselines."
    },
    {
      "title": "GenAgent: Scaling Text-to-Image Generation via Agentic Multimodal Reasoning",
      "url": "https://arxiv.org/abs/2601.18543",
      "date": "2026-01-26",
      "authors_text": "Kaixun Jiang, Yuzheng Wang, Junjie Zhou, Pandeng Li, Zhihang Liu",
      "is_highlight": false,
      "score": 74.0,
      "summary": "GenAgent enhances text-to-image generation by decoupling understanding and generation through an agentic framework, enabling autonomous interactions and improved performance across tasks.",
      "debug_abstract": "We introduce GenAgent, unifying visual understanding and generation through an agentic multimodal model. Unlike unified models that face expensive training costs and understanding-generation trade-offs, GenAgent decouples these capabilities through an agentic framework: understanding is handled by the multimodal model itself, while generation is achieved by treating image generation models as invokable tools. Crucially, unlike existing modular systems constrained by static pipelines, this design enables autonomous multi-turn interactions where the agent generates multimodal chains-of-thought encompassing reasoning, tool invocation, judgment, and reflection to iteratively refine outputs. We employ a two-stage training strategy: first, cold-start with supervised fine-tuning on high-quality tool invocation and reflection data to bootstrap agent behaviors; second, end-to-end agentic reinforcement learning combining pointwise rewards (final image quality) and pairwise rewards (reflection accuracy), with trajectory resampling for enhanced multi-turn exploration. GenAgent significantly boosts base generator(FLUX.1-dev) performance on GenEval++ (+23.6\\%) and WISE (+14\\%). Beyond performance gains, our framework demonstrates three key properties: 1) cross-tool generalization to generators with varying capabilities, 2) test-time scaling with consistent improvements across interaction rounds, and 3) task-adaptive reasoning that automatically adjusts to different tasks. Our code will be available at \\href{this https URL}{this url}."
    },
    {
      "title": "AdaReasoner: Dynamic Tool Orchestration for Iterative Visual Reasoning",
      "url": "https://arxiv.org/abs/2601.18631",
      "date": "2026-01-26",
      "authors_text": "Mingyang Song, Haoyu Sun, Jiawei Gu, Linjie Li, Luxin Xu",
      "is_highlight": false,
      "score": 61.0,
      "summary": "AdaReasoner is a multimodal model that autonomously optimizes tool selection and usage for iterative visual reasoning, achieving state-of-the-art performance on various benchmarks.",
      "debug_abstract": "When humans face problems beyond their immediate capabilities, they rely on tools, providing a promising paradigm for improving visual reasoning in multimodal large language models (MLLMs). Effective reasoning, therefore, hinges on knowing which tools to use, when to invoke them, and how to compose them over multiple steps, even when faced with new tools or new tasks. We introduce \\textbf{AdaReasoner}, a family of multimodal models that learn tool use as a general reasoning skill rather than as tool-specific or explicitly supervised behavior. AdaReasoner is enabled by (i) a scalable data curation pipeline exposing models to long-horizon, multi-step tool interactions; (ii) Tool-GRPO, a reinforcement learning algorithm that optimizes tool selection and sequencing based on end-task success; and (iii) an adaptive learning mechanism that dynamically regulates tool usage. Together, these components allow models to infer tool utility from task context and intermediate outcomes, enabling coordination of multiple tools and generalization to unseen tools. Empirically, AdaReasoner exhibits strong tool-adaptive and generalization behaviors: it autonomously adopts beneficial tools, suppresses irrelevant ones, and adjusts tool usage frequency based on task demands, despite never being explicitly trained to do so. These capabilities translate into state-of-the-art performance across challenging benchmarks, improving the 7B base model by +24.9\\% on average and surpassing strong proprietary systems such as GPT-5 on multiple tasks, including VSP and Jigsaw."
    },
    {
      "title": "Timely Machine: Awareness of Time Makes Test-Time Scaling Agentic",
      "url": "https://arxiv.org/abs/2601.16486",
      "date": "2026-01-23",
      "authors_text": "Yichuan Ma, Linyang Li, Yongkang chen, Peiji Li, Xiaozhe Li",
      "is_highlight": false,
      "score": 78.0,
      "summary": "The paper introduces Timely Machine, redefining test-time scaling for LLMs based on wall-clock time, enhancing performance through dynamic strategy adjustments and reinforcement learning.",
      "debug_abstract": "As large language models (LLMs) increasingly tackle complex reasoning tasks, test-time scaling has become critical for enhancing capabilities. However, in agentic scenarios with frequent tool calls, the traditional generation-length-based definition breaks down: tool latency decouples inference time from generation length. We propose Timely Machine, redefining test-time as wall-clock time, where models dynamically adjust strategies based on time budgets. We introduce Timely-Eval, a benchmark spanning high-frequency tool calls, low-frequency tool calls, and time-constrained reasoning. By varying tool latency, we find smaller models excel with fast feedback through more interactions, while larger models dominate high-latency settings via superior interaction quality. Moreover, existing models fail to adapt reasoning to time budgets. We propose Timely-RL to address this gap. After cold-start supervised fine-tuning, we use reinforcement learning to enhance temporal planning. Timely-RL improves time budget awareness and consistently boosts performance across Timely-Eval. We hope our work offers a new perspective on test-time scaling for the agentic era."
    },
    {
      "title": "ReWeaver: Towards Simulation-Ready and Topology-Accurate Garment Reconstruction",
      "url": "https://arxiv.org/abs/2601.16672",
      "date": "2026-01-23",
      "authors_text": "Ming Li, Hui Shan, Kai Zheng, Chentao Shen, Siyu Liu",
      "is_highlight": false,
      "score": 76.0,
      "summary": "ReWeaver is a novel framework for accurate 3D garment reconstruction from sparse images, enhancing topology fidelity for simulations and robotic applications.",
      "debug_abstract": "High-quality 3D garment reconstruction plays a crucial role in mitigating the sim-to-real gap in applications such as digital avatars, virtual try-on and robotic manipulation. However, existing garment reconstruction methods typically rely on unstructured representations, such as 3D Gaussian Splats, struggling to provide accurate reconstructions of garment topology and sewing structures. As a result, the reconstructed outputs are often unsuitable for high-fidelity physical simulation. We propose ReWeaver, a novel framework for topology-accurate 3D garment and sewing pattern reconstruction from sparse multi-view RGB images. Given as few as four input views, ReWeaver predicts seams and panels as well as their connectivities in both the 2D UV space and the 3D space. The predicted seams and panels align precisely with the multi-view images, yielding structured 2D--3D garment representations suitable for 3D perception, high-fidelity physical simulation, and robotic manipulation. To enable effective training, we construct a large-scale dataset GCD-TS, comprising multi-view RGB images, 3D garment geometries, textured human body meshes and annotated sewing patterns. The dataset contains over 100,000 synthetic samples covering a wide range of complex geometries and topologies. Extensive experiments show that ReWeaver consistently outperforms existing methods in terms of topology accuracy, geometry alignment and seam-panel consistency."
    },
    {
      "title": "VideoThinker: Building Agentic VideoLLMs with LLM-Guided Tool Reasoning",
      "url": "https://arxiv.org/abs/2601.15724",
      "date": "2026-01-22",
      "authors_text": "Chenglin Li, Qianglong Chen, Feng Han, Yikun Wang, Xingxi Yin",
      "is_highlight": false,
      "score": 89.6,
      "summary": "VideoThinker enhances long-form video understanding by utilizing synthetic tool interaction trajectories for adaptive reasoning, outperforming existing models through dynamic exploration and multi-step tool use.",
      "debug_abstract": "Long-form video understanding remains a fundamental challenge for current Video Large Language Models. Most existing models rely on static reasoning over uniformly sampled frames, which weakens temporal localization and leads to substantial information loss in long videos. Agentic tools such as temporal retrieval, spatial zoom, and temporal zoom offer a natural way to overcome these limitations by enabling adaptive exploration of key moments. However, constructing agentic video understanding data requires models that already possess strong long-form video comprehension, creating a circular dependency. We address this challenge with VideoThinker, an agentic Video Large Language Model trained entirely on synthetic tool interaction trajectories. Our key idea is to convert videos into rich captions and employ a powerful agentic language model to generate multi-step tool use sequences in caption space. These trajectories are subsequently grounded back to video by replacing captions with the corresponding frames, yielding a large-scale interleaved video and tool reasoning dataset without requiring any long-form understanding from the underlying model. Training on this synthetic agentic dataset equips VideoThinker with dynamic reasoning capabilities, adaptive temporal exploration, and multi-step tool use. Remarkably, VideoThinker significantly outperforms both caption-only language model agents and strong video model baselines across long-video benchmarks, demonstrating the effectiveness of tool augmented synthetic data and adaptive retrieval and zoom reasoning for long-form video understanding."
    },
    {
      "title": "EvoCUA: Evolving Computer Use Agents via Learning from Scalable Synthetic Experience",
      "url": "https://arxiv.org/abs/2601.15876",
      "date": "2026-01-22",
      "authors_text": "Taofeng Xue, Chong Peng, Mianqiu Huang, Linsen Guo, Tiancheng Han",
      "is_highlight": false,
      "score": 82.6,
      "summary": "EvoCUA introduces a self-sustaining evolutionary model for computer-use agents, leveraging autonomous task generation and iterative learning to significantly outperform existing benchmarks in multimodal AI.",
      "debug_abstract": "The development of native computer-use agents (CUA) represents a significant leap in multimodal AI. However, their potential is currently bottlenecked by the constraints of static data scaling. Existing paradigms relying primarily on passive imitation of static datasets struggle to capture the intricate causal dynamics inherent in long-horizon computer tasks. In this work, we introduce EvoCUA, a native computer use agentic model. Unlike static imitation, EvoCUA integrates data generation and policy optimization into a self-sustaining evolutionary cycle. To mitigate data scarcity, we develop a verifiable synthesis engine that autonomously generates diverse tasks coupled with executable validators. To enable large-scale experience acquisition, we design a scalable infrastructure orchestrating tens of thousands of asynchronous sandbox rollouts. Building on these massive trajectories, we propose an iterative evolving learning strategy to efficiently internalize this experience. This mechanism dynamically regulates policy updates by identifying capability boundaries -- reinforcing successful routines while transforming failure trajectories into rich supervision through error analysis and self-correction. Empirical evaluations on the OSWorld benchmark demonstrate that EvoCUA achieves a success rate of 56.7%, establishing a new open-source state-of-the-art. Notably, EvoCUA significantly outperforms the previous best open-source model, OpenCUA-72B (45.0%), and surpasses leading closed-weights models such as UI-TARS-2 (53.1%). Crucially, our results underscore the generalizability of this approach: the evolving paradigm driven by learning from experience yields consistent performance gains across foundation models of varying scales, establishing a robust and scalable path for advancing native agent capabilities."
    }
  ],
  "智能感知与无人系统实验室": [
    {
      "title": "SlowFocus: Enhancing Fine-grained Temporal Understanding in Video LLM",
      "url": "https://arxiv.org/abs/2602.03589",
      "date": "2026-02-03",
      "authors_text": "Ming Nie, Dan Ding, Chunwei Wang, Yuanfan Guo, Jianhua Han",
      "is_highlight": false,
      "score": 54.0,
      "summary": "The SlowFocus mechanism enhances video LLMs' fine-grained temporal understanding by improving sampling frequency and integrating local high-frequency features with global contexts.",
      "teaser_image": "https://arxiv.org/html/2602.03589/x2.png",
      "debug_abstract": "Large language models (LLMs) have demonstrated exceptional capabilities in text understanding, which has paved the way for their expansion into video LLMs (Vid-LLMs) to analyze video data. However, current Vid-LLMs struggle to simultaneously retain high-quality frame-level semantic information (i.e., a sufficient number of tokens per frame) and comprehensive video-level temporal information (i.e., an adequate number of sampled frames per video). This limitation hinders the advancement of Vid-LLMs towards fine-grained video understanding. To address this issue, we introduce the SlowFocus mechanism, which significantly enhances the equivalent sampling frequency without compromising the quality of frame-level visual tokens. SlowFocus begins by identifying the query-related temporal segment based on the posed question, then performs dense sampling on this segment to extract local high-frequency features. A multi-frequency mixing attention module is further leveraged to aggregate these local high-frequency details with global low-frequency contexts for enhanced temporal comprehension. Additionally, to tailor Vid-LLMs to this innovative mechanism, we introduce a set of training strategies aimed at bolstering both temporal grounding and detailed temporal reasoning capabilities. Furthermore, we establish FineAction-CGR, a benchmark specifically devised to assess the ability of Vid-LLMs to process fine-grained temporal understanding tasks. Comprehensive experiments demonstrate the superiority of our mechanism across both existing public video understanding benchmarks and our proposed FineAction-CGR."
    },
    {
      "title": "UniMotion: A Unified Motion Framework for Simulation, Prediction and Planning",
      "url": "https://arxiv.org/abs/2602.00566",
      "date": "2026-01-31",
      "authors_text": "Nan Song, Junzhe Jiang, Jingyu Li, Xiatian Zhu, Li Zhang",
      "is_highlight": false,
      "score": 89.0,
      "summary": "UniMotion is a unified framework leveraging a Transformer architecture to enhance simulation, prediction, and planning in autonomous driving by integrating shared capabilities across these tasks.",
      "teaser_image": "https://arxiv.org/html/2602.00566/x1.png",
      "debug_abstract": "Motion simulation, prediction and planning are foundational tasks in autonomous driving, each essential for modeling and reasoning about dynamic traffic scenarios. While often addressed in isolation due to their differing objectives, such as generating diverse motion states or estimating optimal trajectories, these tasks inherently depend on shared capabilities: understanding multi-agent interactions, modeling motion behaviors, and reasoning over temporal and spatial dynamics. Despite this underlying commonality, existing approaches typically adopt specialized model designs, which hinders cross-task generalization and system scalability. More critically, this separation overlooks the potential mutual benefits among tasks. Motivated by these observations, we propose UniMotion, a unified motion framework that captures shared structures across motion tasks while accommodating their individual requirements. Built on a decoder-only Transformer architecture, UniMotion employs dedicated interaction modes and tailored training strategies to simultaneously support these motion tasks. This unified design not only enables joint optimization and representation sharing but also allows for targeted fine-tuning to specialize in individual tasks when needed. Extensive experiments on the Waymo Open Motion Dataset demonstrate that joint training leads to robust generalization and effective task integration. With further fine-tuning, UniMotion achieves state-of-the-art performance across a range of motion tasks, establishing it as a versatile and scalable solution for autonomous driving."
    },
    {
      "title": "VVLoc: Prior-free 3-DoF Vehicle Visual Localization",
      "url": "https://arxiv.org/abs/2602.00810",
      "date": "2026-01-31",
      "authors_text": "Ze Huang, Zhongyang Xiao, Mingliang Song, Longan Yang, Hongyuan Yuan",
      "is_highlight": false,
      "score": 73.0,
      "summary": "VVLoc is a unified neural network approach for simultaneous topological and metric vehicle localization using multi-camera systems, providing confidence measures and requiring minimal training data.",
      "teaser_image": "https://arxiv.org/html/2602.00810/x1.png",
      "debug_abstract": "Localization is a critical technology in autonomous driving, encompassing both topological localization, which identifies the most similar map keyframe to the current observation, and metric localization, which provides precise spatial coordinates. Conventional methods typically address these tasks independently, rely on single-camera setups, and often require additional 3D semantic or pose priors, while lacking mechanisms to quantify the confidence of localization results, making them less feasible for real industrial applications. In this paper, we propose VVLoc, a unified pipeline that employs a single neural network to concurrently achieve topological and metric vehicle localization using multi-camera system. VVLoc first evaluates the geo-proximity between visual observations, then estimates their relative metric poses using a matching strategy, while also providing a confidence measure. Additionally, the training process for VVLoc is highly efficient, requiring only pairs of visual data and corresponding ground-truth poses, eliminating the need for complex supplementary data. We evaluate VVLoc not only on the publicly available datasets, but also on a more challenging self-collected dataset, demonstrating its ability to deliver state-of-the-art localization accuracy across a wide range of localization tasks."
    },
    {
      "title": "Beyond Precision: Training-Inference Mismatch is an Optimization Problem and Simple LR Scheduling Fixes It",
      "url": "https://arxiv.org/abs/2602.01826",
      "date": "2026-02-02",
      "authors_text": "Yaxiang Zhang, Yingru Li, Jiacai Liu, Jiawei Xu, Ziniu Li",
      "is_highlight": false,
      "score": 35.0,
      "summary": "This paper presents a dynamic learning rate scheduling method to mitigate training-inference mismatch and stabilize reinforcement learning in large language models by addressing gradient noise.",
      "teaser_image": "https://arxiv.org/html/2602.01826/Qwen3-4B_baseline_vs_importance_sampling_patches.png",
      "debug_abstract": "Reinforcement Learning (RL) for training Large Language Models is notoriously unstable. While recent studies attribute this to &#34;training inference mismatch stemming&#34; from inconsistent hybrid engines, standard remedies, such as Importance Sampling, might fail during extended training runs. In this work, we analyze this instability through the lens of optimization, demonstrating that gradient noise and training-inference mismatch escalate in tandem as training progresses. Meanwhile, we find that the mismatch can be effectively suppressed by shrinking the update size. Taken together, we deduce that the mismatch is not merely a static numerical discrepancy, but a dynamic failure coupled with the model&#39;s optimization. Based on this insight, we propose a simple yet effective solution: a specialized Learning Rate (LR) scheduler. Instead of pre-defined decay schedule in traditional LR scheduler, our method dynamically triggers LR decay based on response length, which we identify as a reliable early-warning signal for impending instability. Empirical evidence suggests that by reducing the learning rate as gradient noise rises, we can consistently stabilize RL training and keep the training-inference mismatch at a safe level."
    },
    {
      "title": "Enhancing Indoor Occupancy Prediction via Sparse Query-Based Multi-Level Consistent Knowledge Distillation",
      "url": "https://arxiv.org/abs/2602.02318",
      "date": "2026-02-02",
      "authors_text": "Xiang Li, Yupeng Zheng, Pengfei Li, Yilun Chen, Ya-Qin Zhang",
      "is_highlight": false,
      "score": 0,
      "summary": "DiScene introduces a sparse query-based framework for efficient and robust indoor occupancy prediction through multi-level knowledge distillation, achieving state-of-the-art performance and faster inference.",
      "teaser_image": "https://arxiv.org/html/2602.02318/x2.png",
      "debug_abstract": "Occupancy prediction provides critical geometric and semantic understanding for robotics but faces efficiency-accuracy trade-offs. Current dense methods suffer computational waste on empty voxels, while sparse query-based approaches lack robustness in diverse and complex indoor scenes. In this paper, we propose DiScene, a novel sparse query-based framework that leverages multi-level distillation to achieve efficient and robust occupancy prediction. In particular, our method incorporates two key innovations: (1) a Multi-level Consistent Knowledge Distillation strategy, which transfers hierarchical representations from large teacher models to lightweight students through coordinated alignment across four levels, including encoder-level feature alignment, query-level feature matching, prior-level spatial guidance, and anchor-level high-confidence knowledge transfer and (2) a Teacher-Guided Initialization policy, employing optimized parameter warm-up to accelerate model convergence. Validated on the Occ-Scannet benchmark, DiScene achieves 23.2 FPS without depth priors while outperforming our baseline method, OPUS, by 36.1% and even better than the depth-enhanced version, OPUS†. With depth integration, DiScene† attains new SOTA performance, surpassing EmbodiedOcc by 3.7% with 1.62$\\times$ faster inference speed. Furthermore, experiments on the Occ3D-nuScenes benchmark and in-the-wild scenarios demonstrate the versatility of our approach in various environments. Code and models can be accessed at this https URL."
    },
    {
      "title": "MAGNET: Towards Adaptive GUI Agents with Memory-Driven Knowledge Evolution",
      "url": "https://arxiv.org/abs/2601.19199",
      "date": "2026-01-27",
      "authors_text": "Libo Sun, Jiwen Zhang, Siyuan Wang, Zhongyu Wei",
      "is_highlight": false,
      "score": 75.0,
      "summary": "MAGNET introduces a memory-driven adaptive agent framework that enhances mobile GUI agents' performance by linking visual features to stable semantics and refining task intents amidst UI changes.",
      "debug_abstract": "Mobile GUI agents powered by large foundation models enable autonomous task execution, but frequent updates altering UI appearance and reorganizing workflows cause agents trained on historical data to fail. Despite surface changes, functional semantics and task intents remain fundamentally stable. Building on this insight, we introduce MAGNET, a memory-driven adaptive agent framework with dual-level memory: stationary memory linking diverse visual features to stable functional semantics for robust action grounding and procedural memory capturing stable task intents across varying workflows. We propose a dynamic memory evolution mechanism that continuously refines both memories by prioritizing frequently accessed knowledge. Online benchmark AndroidWorld evaluations show substantial improvements over baselines, while offline benchmarks confirm consistent gains under distribution shifts. These results validate that leveraging stable structures across interface changes improves agent performance and generalization in evolving software environments."
    },
    {
      "title": "Self-Reconfiguration Planning for Deformable Quadrilateral Modular Robots",
      "url": "https://arxiv.org/abs/2601.19496",
      "date": "2026-01-27",
      "authors_text": "Jie Gu, Hongrun Gao, Zhihao Xia, Yirun Sun, Chunxu Tian",
      "is_highlight": false,
      "score": 91.0,
      "summary": "This paper introduces a self-reconfiguration planning algorithm for deformable quadrilateral modular robots that ensures stable connections and efficient execution sequences through a novel graph-based approach.",
      "debug_abstract": "For lattice modular self-reconfigurable robots (MSRRs), maintaining stable connections during reconfiguration is crucial for physical feasibility and deployability. This letter presents a novel self-reconfiguration planning algorithm for deformable quadrilateral MSRRs that guarantees stable connection. The method first constructs feasible connect/disconnect actions using a virtual graph representation, and then organizes these actions into a valid execution sequence through a Dependence-based Reverse Tree (DRTree) that resolves interdependencies. We also prove that reconfiguration sequences satisfying motion characteristics exist for any pair of configurations with seven or more modules (excluding linear topologies). Finally, comparisons with a modified BiRRT algorithm highlight the superior efficiency and stability of our approach, while deployment on a physical robotic platform confirms its practical feasibility."
    },
    {
      "title": "Rhombot: Rhombus-shaped Modular Robots for Stable, Medium-Independent Reconfiguration Motion",
      "url": "https://arxiv.org/abs/2601.19529",
      "date": "2026-01-27",
      "authors_text": "Jie Gu, Yirui Sun, Zhihao Xia, Tin Lun Lam, Chunxu Tian",
      "is_highlight": false,
      "score": 86.0,
      "summary": "Rhombot is a modular self-reconfigurable robot featuring rhombus-shaped modules that enable stable, medium-independent morphing and locomotion through a novel motion primitive called morphpivoting.",
      "debug_abstract": "In this paper, we present Rhombot, a novel deformable planar lattice modular self-reconfigurable robot (MSRR) with a rhombus shaped module. Each module consists of a parallelogram skeleton with a single centrally mounted actuator that enables folding and unfolding along its diagonal. The core design philosophy is to achieve essential MSRR functionalities such as morphing, docking, and locomotion with minimal control complexity. This enables a continuous and stable reconfiguration process that is independent of the surrounding medium, allowing the system to reliably form various configurations in diverse environments. To leverage the unique kinematics of Rhombot, we introduce morphpivoting, a novel motion primitive for reconfiguration that differs from advanced MSRR systems, and propose a strategy for its continuous execution. Finally, a series of physical experiments validate the module&#39;s stable reconfiguration ability, as well as its positional and docking accuracy."
    },
    {
      "title": "MARO: Learning Stronger Reasoning from Social Interaction",
      "url": "https://arxiv.org/abs/2601.12323",
      "date": "2026-01-24",
      "authors_text": "Yin Cai, Zhouhong Gu, Juntao Zhang, Ping Chen",
      "is_highlight": false,
      "score": 70.0,
      "summary": "MARO enhances large language models' reasoning by enabling learning through multi-agent social interactions, addressing sparse signals, role balance, and environmental instability, with transferable improvements.",
      "debug_abstract": "Humans face countless scenarios that require reasoning and judgment in daily life. However, existing large language model training methods primarily allow models to learn from existing textual content or solve predetermined problems, lacking experience in real scenarios involving interaction, negotiation, and competition with others. To address this, this paper proposes Multi-Agent Reward Optimization (MARO), a method that enables large language models (LLMs) to acquire stronger reasoning abilities by learning and practicing in multi-agent social environments. Specifically, MARO first addresses the sparse learning signal problem by decomposing final success or failure outcomes into each specific behavior during the interaction process; second, it handles the uneven role distribution problem by balancing the training sample weights of different roles; finally, it addresses environmental instability issues by directly evaluating the utility of each behavior. Experimental results demonstrate that MARO not only achieves significant improvements in social reasoning capabilities, but also that the abilities acquired through social simulation learning can effectively transfer to other tasks such as mathematical reasoning and instruction following. This reveals the tremendous potential of multi-agent social learning in enhancing the general reasoning capabilities of LLMs."
    },
    {
      "title": "MetaWorld: Skill Transfer and Composition in a Hierarchical World Model for Grounding High-Level Instructions",
      "url": "https://arxiv.org/abs/2601.17507",
      "date": "2026-01-24",
      "authors_text": "Yutong Shen, Hangxu Liu, Kailin Pei, Ruizhe Xia, Tongtong Feng",
      "is_highlight": false,
      "score": 92.0,
      "summary": "MetaWorld introduces a hierarchical world model that enhances humanoid robot loco-manipulation by integrating semantic planning and physical control through expert policy transfer, improving task performance and motion coherence.",
      "debug_abstract": "Humanoid robot loco-manipulation remains constrained by the semantic-physical gap. Current methods face three limitations: Low sample efficiency in reinforcement learning, poor generalization in imitation learning, and physical inconsistency in VLMs. We propose MetaWorld, a hierarchical world model that integrates semantic planning and physical control via expert policy transfer. The framework decouples tasks into a VLM-driven semantic layer and a latent dynamics model operating in a compact state space. Our dynamic expert selection and motion prior fusion mechanism leverages a pre-trained multi-expert policy library as transferable knowledge, enabling efficient online adaptation via a two-stage framework. VLMs serve as semantic interfaces, mapping instructions to executable skills and bypassing symbol grounding. Experiments on Humanoid-Bench show MetaWorld outperforms world model-based RL in task completion and motion coherence. Our code will be found at this https URL"
    },
    {
      "title": "SwipeGen: Bridging the Execution Gap in GUI Agents via Human-like Swipe Synthesis",
      "url": "https://arxiv.org/abs/2601.18305",
      "date": "2026-01-26",
      "authors_text": "Xuan Wang, Siyuan Su, Quantong Fu, Yongxiang Hu, Yangfan Zhou",
      "is_highlight": false,
      "score": 82.0,
      "summary": "SwipeGen introduces a method for synthesizing human-like swipe interactions in GUI agents, significantly enhancing their execution accuracy and establishing a benchmark for evaluation.",
      "debug_abstract": "With the widespread adoption of Graphical User Interface (GUI) agents for automating GUI interaction tasks, substantial research focused on improving GUI perception to ground task instructions into concrete action steps. However, the step execution capability of these agents has gradually emerged as a new bottleneck for task completion. In particular, existing GUI agents often adopt overly simplified strategies for handling swipe interactions, preventing them from accurately replicating human-like behavior. To address this limitation, we decompose human swipe gestures into multiple quantifiable dimensions and propose an automated pipeline SwipeGen to synthesize human-like swipe interactions through GUI exploration. Based on this pipeline, we construct and release the first benchmark for evaluating the swipe execution capability of GUI agents. Furthermore, leveraging the synthesized data, we propose GUISwiper, a GUI agent with enhanced interaction execution capabilities. Experimental results demonstrate that GUISwiper achieves a swipe execution accuracy of 69.07%, representing a 214% improvement over existing VLM baselines."
    },
    {
      "title": "GenAgent: Scaling Text-to-Image Generation via Agentic Multimodal Reasoning",
      "url": "https://arxiv.org/abs/2601.18543",
      "date": "2026-01-26",
      "authors_text": "Kaixun Jiang, Yuzheng Wang, Junjie Zhou, Pandeng Li, Zhihang Liu",
      "is_highlight": false,
      "score": 74.0,
      "summary": "GenAgent enhances text-to-image generation by decoupling understanding and generation through an agentic framework, enabling autonomous interactions and improved performance across tasks.",
      "debug_abstract": "We introduce GenAgent, unifying visual understanding and generation through an agentic multimodal model. Unlike unified models that face expensive training costs and understanding-generation trade-offs, GenAgent decouples these capabilities through an agentic framework: understanding is handled by the multimodal model itself, while generation is achieved by treating image generation models as invokable tools. Crucially, unlike existing modular systems constrained by static pipelines, this design enables autonomous multi-turn interactions where the agent generates multimodal chains-of-thought encompassing reasoning, tool invocation, judgment, and reflection to iteratively refine outputs. We employ a two-stage training strategy: first, cold-start with supervised fine-tuning on high-quality tool invocation and reflection data to bootstrap agent behaviors; second, end-to-end agentic reinforcement learning combining pointwise rewards (final image quality) and pairwise rewards (reflection accuracy), with trajectory resampling for enhanced multi-turn exploration. GenAgent significantly boosts base generator(FLUX.1-dev) performance on GenEval++ (+23.6\\%) and WISE (+14\\%). Beyond performance gains, our framework demonstrates three key properties: 1) cross-tool generalization to generators with varying capabilities, 2) test-time scaling with consistent improvements across interaction rounds, and 3) task-adaptive reasoning that automatically adjusts to different tasks. Our code will be available at \\href{this https URL}{this url}."
    },
    {
      "title": "AdaReasoner: Dynamic Tool Orchestration for Iterative Visual Reasoning",
      "url": "https://arxiv.org/abs/2601.18631",
      "date": "2026-01-26",
      "authors_text": "Mingyang Song, Haoyu Sun, Jiawei Gu, Linjie Li, Luxin Xu",
      "is_highlight": false,
      "score": 61.0,
      "summary": "AdaReasoner is a multimodal model that autonomously optimizes tool selection and usage for iterative visual reasoning, achieving state-of-the-art performance on various benchmarks.",
      "debug_abstract": "When humans face problems beyond their immediate capabilities, they rely on tools, providing a promising paradigm for improving visual reasoning in multimodal large language models (MLLMs). Effective reasoning, therefore, hinges on knowing which tools to use, when to invoke them, and how to compose them over multiple steps, even when faced with new tools or new tasks. We introduce \\textbf{AdaReasoner}, a family of multimodal models that learn tool use as a general reasoning skill rather than as tool-specific or explicitly supervised behavior. AdaReasoner is enabled by (i) a scalable data curation pipeline exposing models to long-horizon, multi-step tool interactions; (ii) Tool-GRPO, a reinforcement learning algorithm that optimizes tool selection and sequencing based on end-task success; and (iii) an adaptive learning mechanism that dynamically regulates tool usage. Together, these components allow models to infer tool utility from task context and intermediate outcomes, enabling coordination of multiple tools and generalization to unseen tools. Empirically, AdaReasoner exhibits strong tool-adaptive and generalization behaviors: it autonomously adopts beneficial tools, suppresses irrelevant ones, and adjusts tool usage frequency based on task demands, despite never being explicitly trained to do so. These capabilities translate into state-of-the-art performance across challenging benchmarks, improving the 7B base model by +24.9\\% on average and surpassing strong proprietary systems such as GPT-5 on multiple tasks, including VSP and Jigsaw."
    },
    {
      "title": "Timely Machine: Awareness of Time Makes Test-Time Scaling Agentic",
      "url": "https://arxiv.org/abs/2601.16486",
      "date": "2026-01-23",
      "authors_text": "Yichuan Ma, Linyang Li, Yongkang chen, Peiji Li, Xiaozhe Li",
      "is_highlight": false,
      "score": 78.0,
      "summary": "The paper introduces Timely Machine, redefining test-time scaling for LLMs based on wall-clock time, enhancing performance through dynamic strategy adjustments and reinforcement learning.",
      "debug_abstract": "As large language models (LLMs) increasingly tackle complex reasoning tasks, test-time scaling has become critical for enhancing capabilities. However, in agentic scenarios with frequent tool calls, the traditional generation-length-based definition breaks down: tool latency decouples inference time from generation length. We propose Timely Machine, redefining test-time as wall-clock time, where models dynamically adjust strategies based on time budgets. We introduce Timely-Eval, a benchmark spanning high-frequency tool calls, low-frequency tool calls, and time-constrained reasoning. By varying tool latency, we find smaller models excel with fast feedback through more interactions, while larger models dominate high-latency settings via superior interaction quality. Moreover, existing models fail to adapt reasoning to time budgets. We propose Timely-RL to address this gap. After cold-start supervised fine-tuning, we use reinforcement learning to enhance temporal planning. Timely-RL improves time budget awareness and consistently boosts performance across Timely-Eval. We hope our work offers a new perspective on test-time scaling for the agentic era."
    },
    {
      "title": "ReWeaver: Towards Simulation-Ready and Topology-Accurate Garment Reconstruction",
      "url": "https://arxiv.org/abs/2601.16672",
      "date": "2026-01-23",
      "authors_text": "Ming Li, Hui Shan, Kai Zheng, Chentao Shen, Siyu Liu",
      "is_highlight": false,
      "score": 76.0,
      "summary": "ReWeaver is a novel framework for accurate 3D garment reconstruction from sparse images, enhancing topology fidelity for simulations and robotic applications.",
      "debug_abstract": "High-quality 3D garment reconstruction plays a crucial role in mitigating the sim-to-real gap in applications such as digital avatars, virtual try-on and robotic manipulation. However, existing garment reconstruction methods typically rely on unstructured representations, such as 3D Gaussian Splats, struggling to provide accurate reconstructions of garment topology and sewing structures. As a result, the reconstructed outputs are often unsuitable for high-fidelity physical simulation. We propose ReWeaver, a novel framework for topology-accurate 3D garment and sewing pattern reconstruction from sparse multi-view RGB images. Given as few as four input views, ReWeaver predicts seams and panels as well as their connectivities in both the 2D UV space and the 3D space. The predicted seams and panels align precisely with the multi-view images, yielding structured 2D--3D garment representations suitable for 3D perception, high-fidelity physical simulation, and robotic manipulation. To enable effective training, we construct a large-scale dataset GCD-TS, comprising multi-view RGB images, 3D garment geometries, textured human body meshes and annotated sewing patterns. The dataset contains over 100,000 synthetic samples covering a wide range of complex geometries and topologies. Extensive experiments show that ReWeaver consistently outperforms existing methods in terms of topology accuracy, geometry alignment and seam-panel consistency."
    },
    {
      "title": "VideoThinker: Building Agentic VideoLLMs with LLM-Guided Tool Reasoning",
      "url": "https://arxiv.org/abs/2601.15724",
      "date": "2026-01-22",
      "authors_text": "Chenglin Li, Qianglong Chen, Feng Han, Yikun Wang, Xingxi Yin",
      "is_highlight": false,
      "score": 89.6,
      "summary": "VideoThinker enhances long-form video understanding by utilizing synthetic tool interaction trajectories for adaptive reasoning, outperforming existing models through dynamic exploration and multi-step tool use.",
      "debug_abstract": "Long-form video understanding remains a fundamental challenge for current Video Large Language Models. Most existing models rely on static reasoning over uniformly sampled frames, which weakens temporal localization and leads to substantial information loss in long videos. Agentic tools such as temporal retrieval, spatial zoom, and temporal zoom offer a natural way to overcome these limitations by enabling adaptive exploration of key moments. However, constructing agentic video understanding data requires models that already possess strong long-form video comprehension, creating a circular dependency. We address this challenge with VideoThinker, an agentic Video Large Language Model trained entirely on synthetic tool interaction trajectories. Our key idea is to convert videos into rich captions and employ a powerful agentic language model to generate multi-step tool use sequences in caption space. These trajectories are subsequently grounded back to video by replacing captions with the corresponding frames, yielding a large-scale interleaved video and tool reasoning dataset without requiring any long-form understanding from the underlying model. Training on this synthetic agentic dataset equips VideoThinker with dynamic reasoning capabilities, adaptive temporal exploration, and multi-step tool use. Remarkably, VideoThinker significantly outperforms both caption-only language model agents and strong video model baselines across long-video benchmarks, demonstrating the effectiveness of tool augmented synthetic data and adaptive retrieval and zoom reasoning for long-form video understanding."
    },
    {
      "title": "EvoCUA: Evolving Computer Use Agents via Learning from Scalable Synthetic Experience",
      "url": "https://arxiv.org/abs/2601.15876",
      "date": "2026-01-22",
      "authors_text": "Taofeng Xue, Chong Peng, Mianqiu Huang, Linsen Guo, Tiancheng Han",
      "is_highlight": false,
      "score": 82.6,
      "summary": "EvoCUA introduces a self-sustaining evolutionary model for computer-use agents, leveraging autonomous task generation and iterative learning to significantly outperform existing benchmarks in multimodal AI.",
      "debug_abstract": "The development of native computer-use agents (CUA) represents a significant leap in multimodal AI. However, their potential is currently bottlenecked by the constraints of static data scaling. Existing paradigms relying primarily on passive imitation of static datasets struggle to capture the intricate causal dynamics inherent in long-horizon computer tasks. In this work, we introduce EvoCUA, a native computer use agentic model. Unlike static imitation, EvoCUA integrates data generation and policy optimization into a self-sustaining evolutionary cycle. To mitigate data scarcity, we develop a verifiable synthesis engine that autonomously generates diverse tasks coupled with executable validators. To enable large-scale experience acquisition, we design a scalable infrastructure orchestrating tens of thousands of asynchronous sandbox rollouts. Building on these massive trajectories, we propose an iterative evolving learning strategy to efficiently internalize this experience. This mechanism dynamically regulates policy updates by identifying capability boundaries -- reinforcing successful routines while transforming failure trajectories into rich supervision through error analysis and self-correction. Empirical evaluations on the OSWorld benchmark demonstrate that EvoCUA achieves a success rate of 56.7%, establishing a new open-source state-of-the-art. Notably, EvoCUA significantly outperforms the previous best open-source model, OpenCUA-72B (45.0%), and surpasses leading closed-weights models such as UI-TARS-2 (53.1%). Crucially, our results underscore the generalizability of this approach: the evolving paradigm driven by learning from experience yields consistent performance gains across foundation models of varying scales, establishing a robust and scalable path for advancing native agent capabilities."
    }
  ],
  "智能人机交互实验室 (MemX)": [
    {
      "title": "SlowFocus: Enhancing Fine-grained Temporal Understanding in Video LLM",
      "url": "https://arxiv.org/abs/2602.03589",
      "date": "2026-02-03",
      "authors_text": "Ming Nie, Dan Ding, Chunwei Wang, Yuanfan Guo, Jianhua Han",
      "is_highlight": false,
      "score": 54.0,
      "summary": "The SlowFocus mechanism enhances video LLMs' fine-grained temporal understanding by improving sampling frequency and integrating local high-frequency features with global contexts.",
      "teaser_image": "https://arxiv.org/html/2602.03589/x2.png",
      "debug_abstract": "Large language models (LLMs) have demonstrated exceptional capabilities in text understanding, which has paved the way for their expansion into video LLMs (Vid-LLMs) to analyze video data. However, current Vid-LLMs struggle to simultaneously retain high-quality frame-level semantic information (i.e., a sufficient number of tokens per frame) and comprehensive video-level temporal information (i.e., an adequate number of sampled frames per video). This limitation hinders the advancement of Vid-LLMs towards fine-grained video understanding. To address this issue, we introduce the SlowFocus mechanism, which significantly enhances the equivalent sampling frequency without compromising the quality of frame-level visual tokens. SlowFocus begins by identifying the query-related temporal segment based on the posed question, then performs dense sampling on this segment to extract local high-frequency features. A multi-frequency mixing attention module is further leveraged to aggregate these local high-frequency details with global low-frequency contexts for enhanced temporal comprehension. Additionally, to tailor Vid-LLMs to this innovative mechanism, we introduce a set of training strategies aimed at bolstering both temporal grounding and detailed temporal reasoning capabilities. Furthermore, we establish FineAction-CGR, a benchmark specifically devised to assess the ability of Vid-LLMs to process fine-grained temporal understanding tasks. Comprehensive experiments demonstrate the superiority of our mechanism across both existing public video understanding benchmarks and our proposed FineAction-CGR."
    },
    {
      "title": "UniMotion: A Unified Motion Framework for Simulation, Prediction and Planning",
      "url": "https://arxiv.org/abs/2602.00566",
      "date": "2026-01-31",
      "authors_text": "Nan Song, Junzhe Jiang, Jingyu Li, Xiatian Zhu, Li Zhang",
      "is_highlight": false,
      "score": 89.0,
      "summary": "UniMotion is a unified framework leveraging a Transformer architecture to enhance simulation, prediction, and planning in autonomous driving by integrating shared capabilities across these tasks.",
      "teaser_image": "https://arxiv.org/html/2602.00566/x1.png",
      "debug_abstract": "Motion simulation, prediction and planning are foundational tasks in autonomous driving, each essential for modeling and reasoning about dynamic traffic scenarios. While often addressed in isolation due to their differing objectives, such as generating diverse motion states or estimating optimal trajectories, these tasks inherently depend on shared capabilities: understanding multi-agent interactions, modeling motion behaviors, and reasoning over temporal and spatial dynamics. Despite this underlying commonality, existing approaches typically adopt specialized model designs, which hinders cross-task generalization and system scalability. More critically, this separation overlooks the potential mutual benefits among tasks. Motivated by these observations, we propose UniMotion, a unified motion framework that captures shared structures across motion tasks while accommodating their individual requirements. Built on a decoder-only Transformer architecture, UniMotion employs dedicated interaction modes and tailored training strategies to simultaneously support these motion tasks. This unified design not only enables joint optimization and representation sharing but also allows for targeted fine-tuning to specialize in individual tasks when needed. Extensive experiments on the Waymo Open Motion Dataset demonstrate that joint training leads to robust generalization and effective task integration. With further fine-tuning, UniMotion achieves state-of-the-art performance across a range of motion tasks, establishing it as a versatile and scalable solution for autonomous driving."
    },
    {
      "title": "VVLoc: Prior-free 3-DoF Vehicle Visual Localization",
      "url": "https://arxiv.org/abs/2602.00810",
      "date": "2026-01-31",
      "authors_text": "Ze Huang, Zhongyang Xiao, Mingliang Song, Longan Yang, Hongyuan Yuan",
      "is_highlight": false,
      "score": 73.0,
      "summary": "VVLoc is a unified neural network approach for simultaneous topological and metric vehicle localization using multi-camera systems, providing confidence measures and requiring minimal training data.",
      "teaser_image": "https://arxiv.org/html/2602.00810/x1.png",
      "debug_abstract": "Localization is a critical technology in autonomous driving, encompassing both topological localization, which identifies the most similar map keyframe to the current observation, and metric localization, which provides precise spatial coordinates. Conventional methods typically address these tasks independently, rely on single-camera setups, and often require additional 3D semantic or pose priors, while lacking mechanisms to quantify the confidence of localization results, making them less feasible for real industrial applications. In this paper, we propose VVLoc, a unified pipeline that employs a single neural network to concurrently achieve topological and metric vehicle localization using multi-camera system. VVLoc first evaluates the geo-proximity between visual observations, then estimates their relative metric poses using a matching strategy, while also providing a confidence measure. Additionally, the training process for VVLoc is highly efficient, requiring only pairs of visual data and corresponding ground-truth poses, eliminating the need for complex supplementary data. We evaluate VVLoc not only on the publicly available datasets, but also on a more challenging self-collected dataset, demonstrating its ability to deliver state-of-the-art localization accuracy across a wide range of localization tasks."
    },
    {
      "title": "Beyond Precision: Training-Inference Mismatch is an Optimization Problem and Simple LR Scheduling Fixes It",
      "url": "https://arxiv.org/abs/2602.01826",
      "date": "2026-02-02",
      "authors_text": "Yaxiang Zhang, Yingru Li, Jiacai Liu, Jiawei Xu, Ziniu Li",
      "is_highlight": false,
      "score": 35.0,
      "summary": "This paper presents a dynamic learning rate scheduling method to mitigate training-inference mismatch and stabilize reinforcement learning in large language models by addressing gradient noise.",
      "teaser_image": "https://arxiv.org/html/2602.01826/Qwen3-4B_baseline_vs_importance_sampling_patches.png",
      "debug_abstract": "Reinforcement Learning (RL) for training Large Language Models is notoriously unstable. While recent studies attribute this to &#34;training inference mismatch stemming&#34; from inconsistent hybrid engines, standard remedies, such as Importance Sampling, might fail during extended training runs. In this work, we analyze this instability through the lens of optimization, demonstrating that gradient noise and training-inference mismatch escalate in tandem as training progresses. Meanwhile, we find that the mismatch can be effectively suppressed by shrinking the update size. Taken together, we deduce that the mismatch is not merely a static numerical discrepancy, but a dynamic failure coupled with the model&#39;s optimization. Based on this insight, we propose a simple yet effective solution: a specialized Learning Rate (LR) scheduler. Instead of pre-defined decay schedule in traditional LR scheduler, our method dynamically triggers LR decay based on response length, which we identify as a reliable early-warning signal for impending instability. Empirical evidence suggests that by reducing the learning rate as gradient noise rises, we can consistently stabilize RL training and keep the training-inference mismatch at a safe level."
    },
    {
      "title": "Enhancing Indoor Occupancy Prediction via Sparse Query-Based Multi-Level Consistent Knowledge Distillation",
      "url": "https://arxiv.org/abs/2602.02318",
      "date": "2026-02-02",
      "authors_text": "Xiang Li, Yupeng Zheng, Pengfei Li, Yilun Chen, Ya-Qin Zhang",
      "is_highlight": false,
      "score": 0,
      "summary": "DiScene introduces a sparse query-based framework for efficient and robust indoor occupancy prediction through multi-level knowledge distillation, achieving state-of-the-art performance and faster inference.",
      "teaser_image": "https://arxiv.org/html/2602.02318/x2.png",
      "debug_abstract": "Occupancy prediction provides critical geometric and semantic understanding for robotics but faces efficiency-accuracy trade-offs. Current dense methods suffer computational waste on empty voxels, while sparse query-based approaches lack robustness in diverse and complex indoor scenes. In this paper, we propose DiScene, a novel sparse query-based framework that leverages multi-level distillation to achieve efficient and robust occupancy prediction. In particular, our method incorporates two key innovations: (1) a Multi-level Consistent Knowledge Distillation strategy, which transfers hierarchical representations from large teacher models to lightweight students through coordinated alignment across four levels, including encoder-level feature alignment, query-level feature matching, prior-level spatial guidance, and anchor-level high-confidence knowledge transfer and (2) a Teacher-Guided Initialization policy, employing optimized parameter warm-up to accelerate model convergence. Validated on the Occ-Scannet benchmark, DiScene achieves 23.2 FPS without depth priors while outperforming our baseline method, OPUS, by 36.1% and even better than the depth-enhanced version, OPUS†. With depth integration, DiScene† attains new SOTA performance, surpassing EmbodiedOcc by 3.7% with 1.62$\\times$ faster inference speed. Furthermore, experiments on the Occ3D-nuScenes benchmark and in-the-wild scenarios demonstrate the versatility of our approach in various environments. Code and models can be accessed at this https URL."
    },
    {
      "title": "MAGNET: Towards Adaptive GUI Agents with Memory-Driven Knowledge Evolution",
      "url": "https://arxiv.org/abs/2601.19199",
      "date": "2026-01-27",
      "authors_text": "Libo Sun, Jiwen Zhang, Siyuan Wang, Zhongyu Wei",
      "is_highlight": false,
      "score": 75.0,
      "summary": "MAGNET introduces a memory-driven adaptive agent framework that enhances mobile GUI agents' performance by linking visual features to stable semantics and refining task intents amidst UI changes.",
      "debug_abstract": "Mobile GUI agents powered by large foundation models enable autonomous task execution, but frequent updates altering UI appearance and reorganizing workflows cause agents trained on historical data to fail. Despite surface changes, functional semantics and task intents remain fundamentally stable. Building on this insight, we introduce MAGNET, a memory-driven adaptive agent framework with dual-level memory: stationary memory linking diverse visual features to stable functional semantics for robust action grounding and procedural memory capturing stable task intents across varying workflows. We propose a dynamic memory evolution mechanism that continuously refines both memories by prioritizing frequently accessed knowledge. Online benchmark AndroidWorld evaluations show substantial improvements over baselines, while offline benchmarks confirm consistent gains under distribution shifts. These results validate that leveraging stable structures across interface changes improves agent performance and generalization in evolving software environments."
    },
    {
      "title": "Self-Reconfiguration Planning for Deformable Quadrilateral Modular Robots",
      "url": "https://arxiv.org/abs/2601.19496",
      "date": "2026-01-27",
      "authors_text": "Jie Gu, Hongrun Gao, Zhihao Xia, Yirun Sun, Chunxu Tian",
      "is_highlight": false,
      "score": 91.0,
      "summary": "This paper introduces a self-reconfiguration planning algorithm for deformable quadrilateral modular robots that ensures stable connections and efficient execution sequences through a novel graph-based approach.",
      "debug_abstract": "For lattice modular self-reconfigurable robots (MSRRs), maintaining stable connections during reconfiguration is crucial for physical feasibility and deployability. This letter presents a novel self-reconfiguration planning algorithm for deformable quadrilateral MSRRs that guarantees stable connection. The method first constructs feasible connect/disconnect actions using a virtual graph representation, and then organizes these actions into a valid execution sequence through a Dependence-based Reverse Tree (DRTree) that resolves interdependencies. We also prove that reconfiguration sequences satisfying motion characteristics exist for any pair of configurations with seven or more modules (excluding linear topologies). Finally, comparisons with a modified BiRRT algorithm highlight the superior efficiency and stability of our approach, while deployment on a physical robotic platform confirms its practical feasibility."
    },
    {
      "title": "Rhombot: Rhombus-shaped Modular Robots for Stable, Medium-Independent Reconfiguration Motion",
      "url": "https://arxiv.org/abs/2601.19529",
      "date": "2026-01-27",
      "authors_text": "Jie Gu, Yirui Sun, Zhihao Xia, Tin Lun Lam, Chunxu Tian",
      "is_highlight": false,
      "score": 86.0,
      "summary": "Rhombot is a modular self-reconfigurable robot featuring rhombus-shaped modules that enable stable, medium-independent morphing and locomotion through a novel motion primitive called morphpivoting.",
      "debug_abstract": "In this paper, we present Rhombot, a novel deformable planar lattice modular self-reconfigurable robot (MSRR) with a rhombus shaped module. Each module consists of a parallelogram skeleton with a single centrally mounted actuator that enables folding and unfolding along its diagonal. The core design philosophy is to achieve essential MSRR functionalities such as morphing, docking, and locomotion with minimal control complexity. This enables a continuous and stable reconfiguration process that is independent of the surrounding medium, allowing the system to reliably form various configurations in diverse environments. To leverage the unique kinematics of Rhombot, we introduce morphpivoting, a novel motion primitive for reconfiguration that differs from advanced MSRR systems, and propose a strategy for its continuous execution. Finally, a series of physical experiments validate the module&#39;s stable reconfiguration ability, as well as its positional and docking accuracy."
    },
    {
      "title": "MARO: Learning Stronger Reasoning from Social Interaction",
      "url": "https://arxiv.org/abs/2601.12323",
      "date": "2026-01-24",
      "authors_text": "Yin Cai, Zhouhong Gu, Juntao Zhang, Ping Chen",
      "is_highlight": false,
      "score": 70.0,
      "summary": "MARO enhances large language models' reasoning by enabling learning through multi-agent social interactions, addressing sparse signals, role balance, and environmental instability, with transferable improvements.",
      "debug_abstract": "Humans face countless scenarios that require reasoning and judgment in daily life. However, existing large language model training methods primarily allow models to learn from existing textual content or solve predetermined problems, lacking experience in real scenarios involving interaction, negotiation, and competition with others. To address this, this paper proposes Multi-Agent Reward Optimization (MARO), a method that enables large language models (LLMs) to acquire stronger reasoning abilities by learning and practicing in multi-agent social environments. Specifically, MARO first addresses the sparse learning signal problem by decomposing final success or failure outcomes into each specific behavior during the interaction process; second, it handles the uneven role distribution problem by balancing the training sample weights of different roles; finally, it addresses environmental instability issues by directly evaluating the utility of each behavior. Experimental results demonstrate that MARO not only achieves significant improvements in social reasoning capabilities, but also that the abilities acquired through social simulation learning can effectively transfer to other tasks such as mathematical reasoning and instruction following. This reveals the tremendous potential of multi-agent social learning in enhancing the general reasoning capabilities of LLMs."
    },
    {
      "title": "MetaWorld: Skill Transfer and Composition in a Hierarchical World Model for Grounding High-Level Instructions",
      "url": "https://arxiv.org/abs/2601.17507",
      "date": "2026-01-24",
      "authors_text": "Yutong Shen, Hangxu Liu, Kailin Pei, Ruizhe Xia, Tongtong Feng",
      "is_highlight": false,
      "score": 92.0,
      "summary": "MetaWorld introduces a hierarchical world model that enhances humanoid robot loco-manipulation by integrating semantic planning and physical control through expert policy transfer, improving task performance and motion coherence.",
      "debug_abstract": "Humanoid robot loco-manipulation remains constrained by the semantic-physical gap. Current methods face three limitations: Low sample efficiency in reinforcement learning, poor generalization in imitation learning, and physical inconsistency in VLMs. We propose MetaWorld, a hierarchical world model that integrates semantic planning and physical control via expert policy transfer. The framework decouples tasks into a VLM-driven semantic layer and a latent dynamics model operating in a compact state space. Our dynamic expert selection and motion prior fusion mechanism leverages a pre-trained multi-expert policy library as transferable knowledge, enabling efficient online adaptation via a two-stage framework. VLMs serve as semantic interfaces, mapping instructions to executable skills and bypassing symbol grounding. Experiments on Humanoid-Bench show MetaWorld outperforms world model-based RL in task completion and motion coherence. Our code will be found at this https URL"
    },
    {
      "title": "SwipeGen: Bridging the Execution Gap in GUI Agents via Human-like Swipe Synthesis",
      "url": "https://arxiv.org/abs/2601.18305",
      "date": "2026-01-26",
      "authors_text": "Xuan Wang, Siyuan Su, Quantong Fu, Yongxiang Hu, Yangfan Zhou",
      "is_highlight": false,
      "score": 82.0,
      "summary": "SwipeGen introduces a method for synthesizing human-like swipe interactions in GUI agents, significantly enhancing their execution accuracy and establishing a benchmark for evaluation.",
      "debug_abstract": "With the widespread adoption of Graphical User Interface (GUI) agents for automating GUI interaction tasks, substantial research focused on improving GUI perception to ground task instructions into concrete action steps. However, the step execution capability of these agents has gradually emerged as a new bottleneck for task completion. In particular, existing GUI agents often adopt overly simplified strategies for handling swipe interactions, preventing them from accurately replicating human-like behavior. To address this limitation, we decompose human swipe gestures into multiple quantifiable dimensions and propose an automated pipeline SwipeGen to synthesize human-like swipe interactions through GUI exploration. Based on this pipeline, we construct and release the first benchmark for evaluating the swipe execution capability of GUI agents. Furthermore, leveraging the synthesized data, we propose GUISwiper, a GUI agent with enhanced interaction execution capabilities. Experimental results demonstrate that GUISwiper achieves a swipe execution accuracy of 69.07%, representing a 214% improvement over existing VLM baselines."
    },
    {
      "title": "GenAgent: Scaling Text-to-Image Generation via Agentic Multimodal Reasoning",
      "url": "https://arxiv.org/abs/2601.18543",
      "date": "2026-01-26",
      "authors_text": "Kaixun Jiang, Yuzheng Wang, Junjie Zhou, Pandeng Li, Zhihang Liu",
      "is_highlight": false,
      "score": 74.0,
      "summary": "GenAgent enhances text-to-image generation by decoupling understanding and generation through an agentic framework, enabling autonomous interactions and improved performance across tasks.",
      "debug_abstract": "We introduce GenAgent, unifying visual understanding and generation through an agentic multimodal model. Unlike unified models that face expensive training costs and understanding-generation trade-offs, GenAgent decouples these capabilities through an agentic framework: understanding is handled by the multimodal model itself, while generation is achieved by treating image generation models as invokable tools. Crucially, unlike existing modular systems constrained by static pipelines, this design enables autonomous multi-turn interactions where the agent generates multimodal chains-of-thought encompassing reasoning, tool invocation, judgment, and reflection to iteratively refine outputs. We employ a two-stage training strategy: first, cold-start with supervised fine-tuning on high-quality tool invocation and reflection data to bootstrap agent behaviors; second, end-to-end agentic reinforcement learning combining pointwise rewards (final image quality) and pairwise rewards (reflection accuracy), with trajectory resampling for enhanced multi-turn exploration. GenAgent significantly boosts base generator(FLUX.1-dev) performance on GenEval++ (+23.6\\%) and WISE (+14\\%). Beyond performance gains, our framework demonstrates three key properties: 1) cross-tool generalization to generators with varying capabilities, 2) test-time scaling with consistent improvements across interaction rounds, and 3) task-adaptive reasoning that automatically adjusts to different tasks. Our code will be available at \\href{this https URL}{this url}."
    },
    {
      "title": "AdaReasoner: Dynamic Tool Orchestration for Iterative Visual Reasoning",
      "url": "https://arxiv.org/abs/2601.18631",
      "date": "2026-01-26",
      "authors_text": "Mingyang Song, Haoyu Sun, Jiawei Gu, Linjie Li, Luxin Xu",
      "is_highlight": false,
      "score": 61.0,
      "summary": "AdaReasoner is a multimodal model that autonomously optimizes tool selection and usage for iterative visual reasoning, achieving state-of-the-art performance on various benchmarks.",
      "debug_abstract": "When humans face problems beyond their immediate capabilities, they rely on tools, providing a promising paradigm for improving visual reasoning in multimodal large language models (MLLMs). Effective reasoning, therefore, hinges on knowing which tools to use, when to invoke them, and how to compose them over multiple steps, even when faced with new tools or new tasks. We introduce \\textbf{AdaReasoner}, a family of multimodal models that learn tool use as a general reasoning skill rather than as tool-specific or explicitly supervised behavior. AdaReasoner is enabled by (i) a scalable data curation pipeline exposing models to long-horizon, multi-step tool interactions; (ii) Tool-GRPO, a reinforcement learning algorithm that optimizes tool selection and sequencing based on end-task success; and (iii) an adaptive learning mechanism that dynamically regulates tool usage. Together, these components allow models to infer tool utility from task context and intermediate outcomes, enabling coordination of multiple tools and generalization to unseen tools. Empirically, AdaReasoner exhibits strong tool-adaptive and generalization behaviors: it autonomously adopts beneficial tools, suppresses irrelevant ones, and adjusts tool usage frequency based on task demands, despite never being explicitly trained to do so. These capabilities translate into state-of-the-art performance across challenging benchmarks, improving the 7B base model by +24.9\\% on average and surpassing strong proprietary systems such as GPT-5 on multiple tasks, including VSP and Jigsaw."
    },
    {
      "title": "Timely Machine: Awareness of Time Makes Test-Time Scaling Agentic",
      "url": "https://arxiv.org/abs/2601.16486",
      "date": "2026-01-23",
      "authors_text": "Yichuan Ma, Linyang Li, Yongkang chen, Peiji Li, Xiaozhe Li",
      "is_highlight": false,
      "score": 78.0,
      "summary": "The paper introduces Timely Machine, redefining test-time scaling for LLMs based on wall-clock time, enhancing performance through dynamic strategy adjustments and reinforcement learning.",
      "debug_abstract": "As large language models (LLMs) increasingly tackle complex reasoning tasks, test-time scaling has become critical for enhancing capabilities. However, in agentic scenarios with frequent tool calls, the traditional generation-length-based definition breaks down: tool latency decouples inference time from generation length. We propose Timely Machine, redefining test-time as wall-clock time, where models dynamically adjust strategies based on time budgets. We introduce Timely-Eval, a benchmark spanning high-frequency tool calls, low-frequency tool calls, and time-constrained reasoning. By varying tool latency, we find smaller models excel with fast feedback through more interactions, while larger models dominate high-latency settings via superior interaction quality. Moreover, existing models fail to adapt reasoning to time budgets. We propose Timely-RL to address this gap. After cold-start supervised fine-tuning, we use reinforcement learning to enhance temporal planning. Timely-RL improves time budget awareness and consistently boosts performance across Timely-Eval. We hope our work offers a new perspective on test-time scaling for the agentic era."
    },
    {
      "title": "ReWeaver: Towards Simulation-Ready and Topology-Accurate Garment Reconstruction",
      "url": "https://arxiv.org/abs/2601.16672",
      "date": "2026-01-23",
      "authors_text": "Ming Li, Hui Shan, Kai Zheng, Chentao Shen, Siyu Liu",
      "is_highlight": false,
      "score": 76.0,
      "summary": "ReWeaver is a novel framework for accurate 3D garment reconstruction from sparse images, enhancing topology fidelity for simulations and robotic applications.",
      "debug_abstract": "High-quality 3D garment reconstruction plays a crucial role in mitigating the sim-to-real gap in applications such as digital avatars, virtual try-on and robotic manipulation. However, existing garment reconstruction methods typically rely on unstructured representations, such as 3D Gaussian Splats, struggling to provide accurate reconstructions of garment topology and sewing structures. As a result, the reconstructed outputs are often unsuitable for high-fidelity physical simulation. We propose ReWeaver, a novel framework for topology-accurate 3D garment and sewing pattern reconstruction from sparse multi-view RGB images. Given as few as four input views, ReWeaver predicts seams and panels as well as their connectivities in both the 2D UV space and the 3D space. The predicted seams and panels align precisely with the multi-view images, yielding structured 2D--3D garment representations suitable for 3D perception, high-fidelity physical simulation, and robotic manipulation. To enable effective training, we construct a large-scale dataset GCD-TS, comprising multi-view RGB images, 3D garment geometries, textured human body meshes and annotated sewing patterns. The dataset contains over 100,000 synthetic samples covering a wide range of complex geometries and topologies. Extensive experiments show that ReWeaver consistently outperforms existing methods in terms of topology accuracy, geometry alignment and seam-panel consistency."
    },
    {
      "title": "VideoThinker: Building Agentic VideoLLMs with LLM-Guided Tool Reasoning",
      "url": "https://arxiv.org/abs/2601.15724",
      "date": "2026-01-22",
      "authors_text": "Chenglin Li, Qianglong Chen, Feng Han, Yikun Wang, Xingxi Yin",
      "is_highlight": false,
      "score": 89.6,
      "summary": "VideoThinker enhances long-form video understanding by utilizing synthetic tool interaction trajectories for adaptive reasoning, outperforming existing models through dynamic exploration and multi-step tool use.",
      "debug_abstract": "Long-form video understanding remains a fundamental challenge for current Video Large Language Models. Most existing models rely on static reasoning over uniformly sampled frames, which weakens temporal localization and leads to substantial information loss in long videos. Agentic tools such as temporal retrieval, spatial zoom, and temporal zoom offer a natural way to overcome these limitations by enabling adaptive exploration of key moments. However, constructing agentic video understanding data requires models that already possess strong long-form video comprehension, creating a circular dependency. We address this challenge with VideoThinker, an agentic Video Large Language Model trained entirely on synthetic tool interaction trajectories. Our key idea is to convert videos into rich captions and employ a powerful agentic language model to generate multi-step tool use sequences in caption space. These trajectories are subsequently grounded back to video by replacing captions with the corresponding frames, yielding a large-scale interleaved video and tool reasoning dataset without requiring any long-form understanding from the underlying model. Training on this synthetic agentic dataset equips VideoThinker with dynamic reasoning capabilities, adaptive temporal exploration, and multi-step tool use. Remarkably, VideoThinker significantly outperforms both caption-only language model agents and strong video model baselines across long-video benchmarks, demonstrating the effectiveness of tool augmented synthetic data and adaptive retrieval and zoom reasoning for long-form video understanding."
    },
    {
      "title": "EvoCUA: Evolving Computer Use Agents via Learning from Scalable Synthetic Experience",
      "url": "https://arxiv.org/abs/2601.15876",
      "date": "2026-01-22",
      "authors_text": "Taofeng Xue, Chong Peng, Mianqiu Huang, Linsen Guo, Tiancheng Han",
      "is_highlight": false,
      "score": 82.6,
      "summary": "EvoCUA introduces a self-sustaining evolutionary model for computer-use agents, leveraging autonomous task generation and iterative learning to significantly outperform existing benchmarks in multimodal AI.",
      "debug_abstract": "The development of native computer-use agents (CUA) represents a significant leap in multimodal AI. However, their potential is currently bottlenecked by the constraints of static data scaling. Existing paradigms relying primarily on passive imitation of static datasets struggle to capture the intricate causal dynamics inherent in long-horizon computer tasks. In this work, we introduce EvoCUA, a native computer use agentic model. Unlike static imitation, EvoCUA integrates data generation and policy optimization into a self-sustaining evolutionary cycle. To mitigate data scarcity, we develop a verifiable synthesis engine that autonomously generates diverse tasks coupled with executable validators. To enable large-scale experience acquisition, we design a scalable infrastructure orchestrating tens of thousands of asynchronous sandbox rollouts. Building on these massive trajectories, we propose an iterative evolving learning strategy to efficiently internalize this experience. This mechanism dynamically regulates policy updates by identifying capability boundaries -- reinforcing successful routines while transforming failure trajectories into rich supervision through error analysis and self-correction. Empirical evaluations on the OSWorld benchmark demonstrate that EvoCUA achieves a success rate of 56.7%, establishing a new open-source state-of-the-art. Notably, EvoCUA significantly outperforms the previous best open-source model, OpenCUA-72B (45.0%), and surpasses leading closed-weights models such as UI-TARS-2 (53.1%). Crucially, our results underscore the generalizability of this approach: the evolving paradigm driven by learning from experience yields consistent performance gains across foundation models of varying scales, establishing a robust and scalable path for advancing native agent capabilities."
    }
  ],
  "机器人与机械智能实验室 (ROMI Lab)": [
    {
      "title": "Nüwa: Mending the Spatial Integrity Torn by VLM Token Pruning",
      "url": "https://arxiv.org/abs/2602.02951",
      "date": "2026-02-03",
      "authors_text": "Yihong Huang, Fei Ma, Yihua Shao, Jingcai Guo, Zitong Yu",
      "is_highlight": false,
      "score": 69.0,
      "summary": "Nüwa introduces a two-stage token pruning framework that enhances spatial integrity and significantly improves performance in visual grounding and question answering tasks in Vision Language Models.",
      "teaser_image": "https://arxiv.org/html/2602.02951/x2.png",
      "debug_abstract": "Vision token pruning has proven to be an effective acceleration technique for the efficient Vision Language Model (VLM). However, existing pruning methods demonstrate excellent performance preservation in visual question answering (VQA) and suffer substantial degradation on visual grounding (VG) tasks. Our analysis of the VLM&#39;s processing pipeline reveals that strategies utilizing global semantic similarity and attention scores lose the global spatial reference frame, which is derived from the interactions of tokens&#39; positional information. Motivated by these findings, we propose $\\text{Nüwa}$, a two-stage token pruning framework that enables efficient feature aggregation while maintaining spatial integrity. In the first stage, after the vision encoder, we apply three operations, namely separation, alignment, and aggregation, which are inspired by swarm intelligence algorithms to retain information-rich global spatial anchors. In the second stage, within the LLM, we perform text-guided pruning to retain task-relevant visual tokens. Extensive experiments demonstrate that $\\text{Nüwa}$ achieves SOTA performance on multiple VQA benchmarks (from 94% to 95%) and yields substantial improvements on visual grounding tasks (from 7% to 47%)."
    },
    {
      "title": "Unveiling the Cognitive Compass: Theory-of-Mind-Guided Multimodal Emotion Reasoning",
      "url": "https://arxiv.org/abs/2602.00971",
      "date": "2026-02-01",
      "authors_text": "Meng Luo, Bobo Li, Shanqing Xu, Shize Zhang, Qiuchan Chen",
      "is_highlight": false,
      "score": 62.0,
      "summary": "The paper presents HitEmotion, a benchmark for assessing and enhancing multimodal large language models' emotional reasoning through Theory of Mind-guided methods and reinforcement learning.",
      "teaser_image": "https://arxiv.org/html/2602.00971/figures/teaser.png",
      "debug_abstract": "Despite rapid progress in multimodal large language models (MLLMs), their capability for deep emotional understanding remains limited. We argue that genuine affective intelligence requires explicit modeling of Theory of Mind (ToM), the cognitive substrate from which emotions arise. To this end, we introduce HitEmotion, a ToM-grounded hierarchical benchmark that diagnoses capability breakpoints across increasing levels of cognitive depth. Second, we propose a ToM-guided reasoning chain that tracks mental states and calibrates cross-modal evidence to achieve faithful emotional reasoning. We further introduce TMPO, a reinforcement learning method that uses intermediate mental states as process-level supervision to guide and strengthen model reasoning. Extensive experiments show that HitEmotion exposes deep emotional reasoning deficits in state-of-the-art models, especially on cognitively demanding tasks. In evaluation, the ToM-guided reasoning chain and TMPO improve end-task accuracy and yield more faithful, more coherent rationales. In conclusion, our work provides the research community with a practical toolkit for evaluating and enhancing the cognition-based emotional understanding capabilities of MLLMs. Our dataset and code are available at: this https URL."
    },
    {
      "title": "Self-Reconfiguration Planning for Deformable Quadrilateral Modular Robots",
      "url": "https://arxiv.org/abs/2601.19496",
      "date": "2026-01-27",
      "authors_text": "Jie Gu, Hongrun Gao, Zhihao Xia, Yirun Sun, Chunxu Tian",
      "is_highlight": false,
      "score": 91.0,
      "summary": "This paper introduces a self-reconfiguration planning algorithm for deformable quadrilateral modular robots that ensures stable connections and efficient execution sequences through a novel graph-based approach.",
      "debug_abstract": "For lattice modular self-reconfigurable robots (MSRRs), maintaining stable connections during reconfiguration is crucial for physical feasibility and deployability. This letter presents a novel self-reconfiguration planning algorithm for deformable quadrilateral MSRRs that guarantees stable connection. The method first constructs feasible connect/disconnect actions using a virtual graph representation, and then organizes these actions into a valid execution sequence through a Dependence-based Reverse Tree (DRTree) that resolves interdependencies. We also prove that reconfiguration sequences satisfying motion characteristics exist for any pair of configurations with seven or more modules (excluding linear topologies). Finally, comparisons with a modified BiRRT algorithm highlight the superior efficiency and stability of our approach, while deployment on a physical robotic platform confirms its practical feasibility."
    },
    {
      "title": "Rhombot: Rhombus-shaped Modular Robots for Stable, Medium-Independent Reconfiguration Motion",
      "url": "https://arxiv.org/abs/2601.19529",
      "date": "2026-01-27",
      "authors_text": "Jie Gu, Yirui Sun, Zhihao Xia, Tin Lun Lam, Chunxu Tian",
      "is_highlight": false,
      "score": 86.0,
      "summary": "Rhombot is a modular self-reconfigurable robot featuring rhombus-shaped modules that enable stable, medium-independent morphing and locomotion through a novel motion primitive called morphpivoting.",
      "debug_abstract": "In this paper, we present Rhombot, a novel deformable planar lattice modular self-reconfigurable robot (MSRR) with a rhombus shaped module. Each module consists of a parallelogram skeleton with a single centrally mounted actuator that enables folding and unfolding along its diagonal. The core design philosophy is to achieve essential MSRR functionalities such as morphing, docking, and locomotion with minimal control complexity. This enables a continuous and stable reconfiguration process that is independent of the surrounding medium, allowing the system to reliably form various configurations in diverse environments. To leverage the unique kinematics of Rhombot, we introduce morphpivoting, a novel motion primitive for reconfiguration that differs from advanced MSRR systems, and propose a strategy for its continuous execution. Finally, a series of physical experiments validate the module&#39;s stable reconfiguration ability, as well as its positional and docking accuracy."
    },
    {
      "title": "Component-Level Lesioning of Language Models Reveals Clinically Aligned Aphasia Phenotypes",
      "url": "https://arxiv.org/abs/2601.19723",
      "date": "2026-01-27",
      "authors_text": "Yifan Wang, Jichen Zheng, Jingyuan Sun, Yunhao Zhang, Chunyu Ye",
      "is_highlight": false,
      "score": 62.0,
      "summary": "This study presents a framework for simulating aphasia in language models by selectively perturbing components, revealing clinically relevant language impairments and enhancing understanding of language function degradation.",
      "debug_abstract": "Large language models (LLMs) increasingly exhibit human-like linguistic behaviors and internal representations that they could serve as computational simulators of language cognition. We ask whether LLMs can be systematically manipulated to reproduce language-production impairments characteristic of aphasia following focal brain lesions. Such models could provide scalable proxies for testing rehabilitation hypotheses, and offer a controlled framework for probing the functional organization of language. We introduce a clinically grounded, component-level framework that simulates aphasia by selectively perturbing functional components in LLMs, and apply it to both modular Mixture-of-Experts models and dense Transformers using a unified intervention interface. Our pipeline (i) identifies subtype-linked components for Broca&#39;s and Wernicke&#39;s aphasia, (ii) interprets these components with linguistic probing tasks, and (iii) induces graded impairments by progressively perturbing the top-k subtype-linked components, evaluating outcomes with Western Aphasia Battery (WAB) subtests summarized by Aphasia Quotient (AQ). Across architectures and lesioning strategies, subtype-targeted perturbations yield more systematic, aphasia-like regressions than size-matched random perturbations, and MoE modularity supports more localized and interpretable phenotype-to-component mappings. These findings suggest that modular LLMs, combined with clinically informed component perturbations, provide a promising platform for simulating aphasic language production and studying how distinct language functions degrade under targeted disruptions."
    },
    {
      "title": "DV-VLN: Dual Verification for Reliable LLM-Based Vision-and-Language Navigation",
      "url": "https://arxiv.org/abs/2601.18492",
      "date": "2026-01-26",
      "authors_text": "Zijun Li, Shijie Li, Zhenxi Zhang, Bin Li, Shoujun Zhou",
      "is_highlight": false,
      "score": 84.0,
      "summary": "DV-VLN enhances vision-and-language navigation by employing a generate-then-verify approach with dual verification channels to improve action reliability and interpretability in complex environments.",
      "debug_abstract": "Vision-and-Language Navigation (VLN) requires an embodied agent to navigate in a complex 3D environment according to natural language instructions. Recent progress in large language models (LLMs) has enabled language-driven navigation with improved interpretability. However, most LLM-based agents still rely on single-shot action decisions, where the model must choose one option from noisy, textualized multi-perspective observations. Due to local mismatches and imperfect intermediate reasoning, such decisions can easily deviate from the correct path, leading to error accumulation and reduced reliability in unseen environments. In this paper, we propose DV-VLN, a new VLN framework that follows a generate-then-verify paradigm. DV-VLN first performs parameter-efficient in-domain adaptation of an open-source LLaMA-2 backbone to produce a structured navigational chain-of-thought, and then verifies candidate actions with two complementary channels: True-False Verification (TFV) and Masked-Entity Verification (MEV). DV-VLN selects actions by aggregating verification successes across multiple samples, yielding interpretable scores for reranking. Experiments on R2R, RxR (English subset), and REVERIE show that DV-VLN consistently improves over direct prediction and sampling-only baselines, achieving competitive performance among language-only VLN agents and promising results compared with several cross-modal this http URL is available at this https URL."
    },
    {
      "title": "DCCS-Det: Directional Context and Cross-Scale-Aware Detector for Infrared Small Target",
      "url": "https://arxiv.org/abs/2601.16428",
      "date": "2026-01-23",
      "authors_text": "Shuying Li, Qiang Ma, San Zhang, Chuang Yang",
      "is_highlight": false,
      "score": 55.0,
      "summary": "DCCS-Det enhances infrared small target detection by integrating a Dual-stream Saliency Enhancement block and Latent-aware Semantic Extraction module for improved feature representation and accuracy.",
      "debug_abstract": "Infrared small target detection (IRSTD) is critical for applications like remote sensing and surveillance, which aims to identify small, low-contrast targets against complex backgrounds. However, existing methods often struggle with inadequate joint modeling of local-global features (harming target-background discrimination) or feature redundancy and semantic dilution (degrading target representation quality). To tackle these issues, we propose DCCS-Det (Directional Context and Cross-Scale Aware Detector for Infrared Small Target), a novel detector that incorporates a Dual-stream Saliency Enhancement (DSE) block and a Latent-aware Semantic Extraction and Aggregation (LaSEA) module. The DSE block integrates localized perception with direction-aware context aggregation to help capture long-range spatial dependencies and local details. On this basis, the LaSEA module mitigates feature degradation via cross-scale feature extraction and random pooling sampling strategies, enhancing discriminative features and suppressing noise. Extensive experiments show that DCCS-Det achieves state-of-the-art detection accuracy with competitive efficiency across multiple datasets. Ablation studies further validate the contributions of DSE and LaSEA in improving target perception and feature representation under complex scenarios. \\href{this https URL}{DCCS-Det Official Code is Available Here!}"
    },
    {
      "title": "PhysicsMind: Sim and Real Mechanics Benchmarking for Physical Reasoning and Prediction in Foundational VLMs and World Models",
      "url": "https://arxiv.org/abs/2601.16007",
      "date": "2026-01-22",
      "authors_text": "Chak-Wing Mak, Guanyu Zhu, Boyi Zhang, Hongji Li, Xiaowei Chi",
      "is_highlight": false,
      "score": 94.1,
      "summary": "PhysicsMind introduces a benchmark for evaluating physical reasoning in multimodal models through real and simulated tasks focused on fundamental physics principles, revealing significant gaps in current models' understanding.",
      "debug_abstract": "Modern foundational Multimodal Large Language Models (MLLMs) and video world models have advanced significantly in mathematical, common-sense, and visual reasoning, but their grasp of the underlying physics remains underexplored. Existing benchmarks attempting to measure this matter rely on synthetic, Visual Question Answer templates or focus on perceptual video quality that is tangential to measuring how well the video abides by physical laws. To address this fragmentation, we introduce PhysicsMind, a unified benchmark with both real and simulation environments that evaluates law-consistent reasoning and generation over three canonical principles: Center of Mass, Lever Equilibrium, and Newton&#39;s First Law. PhysicsMind comprises two main tasks: i) VQA tasks, testing whether models can reason and determine physical quantities and values from images or short videos, and ii) Video Generation(VG) tasks, evaluating if predicted motion trajectories obey the same center-of-mass, torque, and inertial constraints as the ground truth. A broad range of recent models and video generation models is evaluated on PhysicsMind and found to rely on appearance heuristics while often violating basic mechanics. These gaps indicate that current scaling and training are still insufficient for robust physical understanding, underscoring PhysicsMind as a focused testbed for physics-aware multimodal models. Our data will be released upon acceptance."
    }
  ],
  "中国科学技术大学": [
    {
      "title": "ConsisDrive: Identity-Preserving Driving World Models for Video Generation by Instance Mask",
      "url": "https://arxiv.org/abs/2602.03213",
      "date": "2026-02-03",
      "authors_text": "Zhuoran Yang, Yanyong Zhang",
      "is_highlight": false,
      "score": 83.0,
      "summary": "ConsisDrive introduces an identity-preserving world model for video generation in autonomous driving, enhancing temporal consistency and object identity through instance-masked attention and loss mechanisms.",
      "teaser_image": "https://arxiv.org/html/2602.03213/x2.png",
      "debug_abstract": "Autonomous driving relies on robust models trained on large-scale, high-quality multi-view driving videos. Although world models provide a cost-effective solution for generating realistic driving data, they often suffer from identity drift, where the same object changes its appearance or category across frames due to the absence of instance-level temporal constraints. We introduce ConsisDrive, an identity-preserving driving world model designed to enforce temporal consistency at the instance level. Our framework incorporates two key components: (1) Instance-Masked Attention, which applies instance identity masks and trajectory masks within attention blocks to ensure that visual tokens interact only with their corresponding instance features across spatial and temporal dimensions, thereby preserving object identity consistency; and (2) Instance-Masked Loss, which adaptively emphasizes foreground regions with probabilistic instance masking, reducing background noise while maintaining overall scene fidelity. By integrating these mechanisms, ConsisDrive achieves state-of-the-art driving video generation quality and demonstrates significant improvements in downstream autonomous driving tasks on the nuScenes dataset. Our project page is this https URL."
    },
    {
      "title": "InstaDrive: Instance-Aware Driving World Models for Realistic and Consistent Video Generation",
      "url": "https://arxiv.org/abs/2602.03242",
      "date": "2026-02-03",
      "authors_text": "Zhuoran Yang, Xi Guo, Chenjing Ding, Chiyu Wang, Wei Wu",
      "is_highlight": false,
      "score": 84.0,
      "summary": "InstaDrive introduces a framework that enhances realistic driving video generation by ensuring instance-level temporal consistency and spatial geometric fidelity through innovative feature extraction and alignment techniques.",
      "teaser_image": "https://arxiv.org/html/2602.03242/x2.png",
      "debug_abstract": "Autonomous driving relies on robust models trained on high-quality, large-scale multi-view driving videos. While world models offer a cost-effective solution for generating realistic driving videos, they struggle to maintain instance-level temporal consistency and spatial geometric fidelity. To address these challenges, we propose InstaDrive, a novel framework that enhances driving video realism through two key advancements: (1) Instance Flow Guider, which extracts and propagates instance features across frames to enforce temporal consistency, preserving instance identity over time. (2) Spatial Geometric Aligner, which improves spatial reasoning, ensures precise instance positioning, and explicitly models occlusion hierarchies. By incorporating these instance-aware mechanisms, InstaDrive achieves state-of-the-art video generation quality and enhances downstream autonomous driving tasks on the nuScenes dataset. Additionally, we utilize CARLA&#39;s autopilot to procedurally and stochastically simulate rare but safety-critical driving scenarios across diverse maps and regions, enabling rigorous safety evaluation for autonomous systems. Our project page is this https URL."
    },
    {
      "title": "Self-Guard: Defending Large Reasoning Models via enhanced self-reflection",
      "url": "https://arxiv.org/abs/2602.00707",
      "date": "2026-01-31",
      "authors_text": "Jingnan Zheng, Jingjun Xu, Yanzhen Luo, Chenhang Cui, Gelei Deng",
      "is_highlight": false,
      "score": 50.0,
      "summary": "Self-Guard is a lightweight framework enhancing safety compliance in Large Reasoning Models by promoting self-reflection and mitigating risks without sacrificing model utility.",
      "teaser_image": "https://arxiv.org/html/2602.00707/materials/workflow.png",
      "debug_abstract": "The emergence of Large Reasoning Models (LRMs) introduces a new paradigm of explicit reasoning, enabling remarkable advances yet posing unique risks such as reasoning manipulation and information leakage. To mitigate these risks, current alignment strategies predominantly rely on heavy post-training paradigms or external interventions. However, these approaches are often computationally intensive and fail to address the inherent awareness-compliance gap, a critical misalignment where models recognize potential risks yet prioritize following user instructions due to their sycophantic tendencies. To address these limitations, we propose Self-Guard, a lightweight safety defense framework that reinforces safety compliance at the representational level. Self-Guard operates through two principal stages: (1) safety-oriented prompting, which activates the model&#39;s latent safety awareness to evoke spontaneous reflection, and (2) safety activation steering, which extracts the resulting directional shift in the hidden state space and amplifies it to ensure that safety compliance prevails over sycophancy during inference. Experiments demonstrate that Self-Guard effectively bridges the awareness-compliance gap, achieving robust safety performance without compromising model utility. Furthermore, Self-Guard exhibits strong generalization across diverse unseen risks and varying model scales, offering a cost-efficient solution for LRM safety alignment."
    },
    {
      "title": "Med3D-R1: Incentivizing Clinical Reasoning in 3D Medical Vision-Language Models for Abnormality Diagnosis",
      "url": "https://arxiv.org/abs/2602.01200",
      "date": "2026-02-01",
      "authors_text": "Haoran Lai, Zihang Jiang, Kun Zhang, Qingsong Yao, Rongsheng Wang",
      "is_highlight": false,
      "score": 54.0,
      "summary": "Med3D-R1 introduces a two-stage reinforcement learning framework that enhances 3D medical vision-language models for improved abnormality diagnosis and clinical reasoning.",
      "teaser_image": "https://arxiv.org/html/2602.01200/x2.png",
      "debug_abstract": "Developing 3D vision-language models with robust clinical reasoning remains a challenge due to the inherent complexity of volumetric medical imaging, the tendency of models to overfit superficial report patterns, and the lack of interpretability-aware reward designs. In this paper, we propose Med3D-R1, a reinforcement learning framework with a two-stage training process: Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL). During SFT stage, we introduce a residual alignment mechanism to bridge the gap between high-dimensional 3D features and textual embeddings, and an abnormality re-weighting strategy to emphasize clinically informative tokens and reduce structural bias in reports. In RL stage, we redesign the consistency reward to explicitly promote coherent, step-by-step diagnostic reasoning. We evaluate our method on medical multiple-choice visual question answering using two 3D diagnostic benchmarks, CT-RATE and RAD-ChestCT, where our model attains state-of-the-art accuracies of 41.92\\% on CT-RATE and 44.99\\% on RAD-ChestCT. These results indicate improved abnormality diagnosis and clinical reasoning and outperform prior methods on both benchmarks. Overall, our approach holds promise for enhancing real-world diagnostic workflows by enabling more reliable and transparent 3D medical vision-language systems."
    },
    {
      "title": "Reasoning with Autoregressive-Diffusion Collaborative Thoughts",
      "url": "https://arxiv.org/abs/2602.01608",
      "date": "2026-02-02",
      "authors_text": "Mu Yuan, Liekang Zeng, Guoliang Xing, Lan Zhang, Yunhao Liu",
      "is_highlight": false,
      "score": 41.0,
      "summary": "The paper presents Collaborative Thoughts, a framework integrating autoregressive and diffusion models for enhanced spatial reasoning and generation through iterative feedback and joint reasoning.",
      "teaser_image": "https://arxiv.org/html/2602.01608/x2.png",
      "debug_abstract": "Autoregressive and diffusion models represent two complementary generative paradigms. Autoregressive models excel at sequential planning and constraint composition, yet struggle with tasks that require explicit spatial or physical grounding. Diffusion models, in contrast, capture rich spatial structure through high-dimensional generation, but lack the stepwise logical control needed to satisfy complex, multi-stage constraints or to reliably identify and correct errors. We introduce Collaborative Thoughts, a unified collaborative framework that enables autoregressive and diffusion models to reason and generate jointly through a closed-loop interaction. In Collaborative Thoughts, autoregressive models perform structured planning and constraint management, diffusion models instantiate these constraints as intermediate visual thoughts, and a vision-based critic module evaluates whether the visual thoughts satisfy the intended structural and physical requirements. This feedback is then used to iteratively refine subsequent planning and generation steps, mitigating error propagation across modalities. Importantly, Collaborative Thoughts uses the same collaborative loop regardless of whether the task is autoregressive question answering or diffusion-based visual generation. Through representative examples, we illustrate how Collaborative Thoughts can improve the reliability of spatial reasoning and the controllability of generation."
    },
    {
      "title": "Beyond Precision: Training-Inference Mismatch is an Optimization Problem and Simple LR Scheduling Fixes It",
      "url": "https://arxiv.org/abs/2602.01826",
      "date": "2026-02-02",
      "authors_text": "Yaxiang Zhang, Yingru Li, Jiacai Liu, Jiawei Xu, Ziniu Li",
      "is_highlight": false,
      "score": 35.0,
      "summary": "This paper presents a dynamic learning rate scheduling method to mitigate training-inference mismatch and stabilize reinforcement learning in large language models by addressing gradient noise.",
      "teaser_image": "https://arxiv.org/html/2602.01826/Qwen3-4B_baseline_vs_importance_sampling_patches.png",
      "debug_abstract": "Reinforcement Learning (RL) for training Large Language Models is notoriously unstable. While recent studies attribute this to &#34;training inference mismatch stemming&#34; from inconsistent hybrid engines, standard remedies, such as Importance Sampling, might fail during extended training runs. In this work, we analyze this instability through the lens of optimization, demonstrating that gradient noise and training-inference mismatch escalate in tandem as training progresses. Meanwhile, we find that the mismatch can be effectively suppressed by shrinking the update size. Taken together, we deduce that the mismatch is not merely a static numerical discrepancy, but a dynamic failure coupled with the model&#39;s optimization. Based on this insight, we propose a simple yet effective solution: a specialized Learning Rate (LR) scheduler. Instead of pre-defined decay schedule in traditional LR scheduler, our method dynamically triggers LR decay based on response length, which we identify as a reliable early-warning signal for impending instability. Empirical evidence suggests that by reducing the learning rate as gradient noise rises, we can consistently stabilize RL training and keep the training-inference mismatch at a safe level."
    },
    {
      "title": "PLANING: A Loosely Coupled Triangle-Gaussian Framework for Streaming 3D Reconstruction",
      "url": "https://arxiv.org/abs/2601.22046",
      "date": "2026-01-29",
      "authors_text": "Changjian Jiang, Kerui Ren, Xudong Li, Kaiwen Song, Linning Xu",
      "is_highlight": false,
      "score": 66.0,
      "summary": "PLANING is a novel streaming 3D reconstruction framework that efficiently combines geometric primitives and neural Gaussians, enhancing quality and speed while reducing redundancy.",
      "teaser_image": "https://arxiv.org/html/2601.22046/x3.png",
      "debug_abstract": "Streaming reconstruction from monocular image sequences remains challenging, as existing methods typically favor either high-quality rendering or accurate geometry, but rarely both. We present PLANING, an efficient on-the-fly reconstruction framework built on a hybrid representation that loosely couples explicit geometric primitives with neural Gaussians, enabling geometry and appearance to be modeled in a decoupled manner. This decoupling supports an online initialization and optimization strategy that separates geometry and appearance updates, yielding stable streaming reconstruction with substantially reduced structural redundancy. PLANING improves dense mesh Chamfer-L2 by 18.52% over PGSR, surpasses ARTDECO by 1.31 dB PSNR, and reconstructs ScanNetV2 scenes in under 100 seconds, over 5x faster than 2D Gaussian Splatting, while matching the quality of offline per-scene optimization. Beyond reconstruction quality, the structural clarity and computational efficiency of \\modelname~make it well suited for a broad range of downstream applications, such as enabling large-scale scene modeling and simulation-ready environments for embodied AI. Project page: this https URL ."
    },
    {
      "title": "TouchGuide: Inference-Time Steering of Visuomotor Policies via Touch Guidance",
      "url": "https://arxiv.org/abs/2601.20239",
      "date": "2026-01-28",
      "authors_text": "Zhemeng Zhang, Jiahua Ma, Xincheng Yang, Xin Wen, Yuzhi Zhang",
      "is_highlight": false,
      "score": 90.0,
      "summary": "TouchGuide enhances robot manipulation by integrating tactile feedback with visual inputs to refine actions during inference, outperforming existing visuo-tactile policies in complex tasks.",
      "teaser_image": "https://arxiv.org/html/2601.20239/x2.png",
      "debug_abstract": "Fine-grained and contact-rich manipulation remain challenging for robots, largely due to the underutilization of tactile feedback. To address this, we introduce TouchGuide, a novel cross-policy visuo-tactile fusion paradigm that fuses modalities within a low-dimensional action space. Specifically, TouchGuide operates in two stages to guide a pre-trained diffusion or flow-matching visuomotor policy at inference time. First, the policy produces a coarse, visually-plausible action using only visual inputs during early sampling. Second, a task-specific Contact Physical Model (CPM) provides tactile guidance to steer and refine the action, ensuring it aligns with realistic physical contact conditions. Trained through contrastive learning on limited expert demonstrations, the CPM provides a tactile-informed feasibility score to steer the sampling process toward refined actions that satisfy physical contact constraints. Furthermore, to facilitate TouchGuide training with high-quality and cost-effective data, we introduce TacUMI, a data collection system. TacUMI achieves a favorable trade-off between precision and affordability; by leveraging rigid fingertips, it obtains direct tactile feedback, thereby enabling the collection of reliable tactile data. Extensive experiments on five challenging contact-rich tasks, such as shoe lacing and chip handover, show that TouchGuide consistently and significantly outperforms state-of-the-art visuo-tactile policies."
    },
    {
      "title": "GenAgent: Scaling Text-to-Image Generation via Agentic Multimodal Reasoning",
      "url": "https://arxiv.org/abs/2601.18543",
      "date": "2026-01-26",
      "authors_text": "Kaixun Jiang, Yuzheng Wang, Junjie Zhou, Pandeng Li, Zhihang Liu",
      "is_highlight": false,
      "score": 74.0,
      "summary": "GenAgent enhances text-to-image generation by decoupling understanding and generation through an agentic framework, enabling autonomous interactions and improved performance across tasks.",
      "debug_abstract": "We introduce GenAgent, unifying visual understanding and generation through an agentic multimodal model. Unlike unified models that face expensive training costs and understanding-generation trade-offs, GenAgent decouples these capabilities through an agentic framework: understanding is handled by the multimodal model itself, while generation is achieved by treating image generation models as invokable tools. Crucially, unlike existing modular systems constrained by static pipelines, this design enables autonomous multi-turn interactions where the agent generates multimodal chains-of-thought encompassing reasoning, tool invocation, judgment, and reflection to iteratively refine outputs. We employ a two-stage training strategy: first, cold-start with supervised fine-tuning on high-quality tool invocation and reflection data to bootstrap agent behaviors; second, end-to-end agentic reinforcement learning combining pointwise rewards (final image quality) and pairwise rewards (reflection accuracy), with trajectory resampling for enhanced multi-turn exploration. GenAgent significantly boosts base generator(FLUX.1-dev) performance on GenEval++ (+23.6\\%) and WISE (+14\\%). Beyond performance gains, our framework demonstrates three key properties: 1) cross-tool generalization to generators with varying capabilities, 2) test-time scaling with consistent improvements across interaction rounds, and 3) task-adaptive reasoning that automatically adjusts to different tasks. Our code will be available at \\href{this https URL}{this url}."
    },
    {
      "title": "PhysicsMind: Sim and Real Mechanics Benchmarking for Physical Reasoning and Prediction in Foundational VLMs and World Models",
      "url": "https://arxiv.org/abs/2601.16007",
      "date": "2026-01-22",
      "authors_text": "Chak-Wing Mak, Guanyu Zhu, Boyi Zhang, Hongji Li, Xiaowei Chi",
      "is_highlight": false,
      "score": 94.1,
      "summary": "PhysicsMind introduces a benchmark for evaluating physical reasoning in multimodal models through real and simulated tasks focused on fundamental physics principles, revealing significant gaps in current models' understanding.",
      "debug_abstract": "Modern foundational Multimodal Large Language Models (MLLMs) and video world models have advanced significantly in mathematical, common-sense, and visual reasoning, but their grasp of the underlying physics remains underexplored. Existing benchmarks attempting to measure this matter rely on synthetic, Visual Question Answer templates or focus on perceptual video quality that is tangential to measuring how well the video abides by physical laws. To address this fragmentation, we introduce PhysicsMind, a unified benchmark with both real and simulation environments that evaluates law-consistent reasoning and generation over three canonical principles: Center of Mass, Lever Equilibrium, and Newton&#39;s First Law. PhysicsMind comprises two main tasks: i) VQA tasks, testing whether models can reason and determine physical quantities and values from images or short videos, and ii) Video Generation(VG) tasks, evaluating if predicted motion trajectories obey the same center-of-mass, torque, and inertial constraints as the ground truth. A broad range of recent models and video generation models is evaluated on PhysicsMind and found to rely on appearance heuristics while often violating basic mechanics. These gaps indicate that current scaling and training are still insufficient for robust physical understanding, underscoring PhysicsMind as a focused testbed for physics-aware multimodal models. Our data will be released upon acceptance."
    }
  ],
  "Robot Perception and Learning Lab": [
    {
      "title": "See2Refine: Vision-Language Feedback Improves LLM-Based eHMI Action Designers",
      "url": "https://arxiv.org/abs/2602.02063",
      "date": "2026-02-02",
      "authors_text": "Ding Xia, Xinyue Gui, Mark Colley, Fan Gao, Zhongyi Zhou",
      "is_highlight": false,
      "score": 23.0,
      "summary": "See2Refine enhances LLM-based eHMI action designers by utilizing vision-language model feedback for automated, context-adaptive refinement, outperforming traditional methods in various evaluations.",
      "teaser_image": "https://arxiv.org/html/2602.02063/x2.png",
      "debug_abstract": "Automated vehicles lack natural communication channels with other road users, making external Human-Machine Interfaces (eHMIs) essential for conveying intent and maintaining trust in shared environments. However, most eHMI studies rely on developer-crafted message-action pairs, which are difficult to adapt to diverse and dynamic traffic contexts. A promising alternative is to use Large Language Models (LLMs) as action designers that generate context-conditioned eHMI actions, yet such designers lack perceptual verification and typically depend on fixed prompts or costly human-annotated feedback for improvement. We present See2Refine, a human-free, closed-loop framework that uses vision-language model (VLM) perceptual evaluation as automated visual feedback to improve an LLM-based eHMI action designer. Given a driving context and a candidate eHMI action, the VLM evaluates the perceived appropriateness of the action, and this feedback is used to iteratively revise the designer&#39;s outputs, enabling systematic refinement without human supervision. We evaluate our framework across three eHMI modalities (lightbar, eyes, and arm) and multiple LLM model sizes. Across settings, our framework consistently outperforms prompt-only LLM designers and manually specified baselines in both VLM-based metrics and human-subject evaluations. Results further indicate that the improvements generalize across modalities and that VLM evaluations are well aligned with human preferences, supporting the robustness and effectiveness of See2Refine for scalable action design."
    },
    {
      "title": "SanD-Planner: Sample-Efficient Diffusion Planner in B-Spline Space for Robust Local Navigation",
      "url": "https://arxiv.org/abs/2602.00923",
      "date": "2026-01-31",
      "authors_text": "Jincheng Wang, Lingfan Bao, Tong Yang, Diego Martinez Plasencia, Jianhao Jiao",
      "is_highlight": false,
      "score": 10.0,
      "summary": "SanD-Planner introduces a sample-efficient diffusion-based local planner using B-spline space, achieving high success rates in cluttered environments with minimal training data and zero-shot transferability.",
      "teaser_image": "https://arxiv.org/html/2602.00923/Images/overview_new.png",
      "debug_abstract": "The challenge of generating reliable local plans has long hindered practical applications in highly cluttered and dynamic environments. Key fundamental bottlenecks include acquiring large-scale expert demonstrations across diverse scenes and improving learning efficiency with limited data. This paper proposes SanD-Planner, a sample-efficient diffusion-based local planner that conducts depth image-based imitation learning within the clamped B-spline space. By operating within this compact space, the proposed algorithm inherently yields smooth outputs with bounded prediction errors over local supports, naturally aligning with receding-horizon execution. Integration of an ESDF-based safety checker with explicit clearance and time-to-completion metrics further reduces the training burden associated with value-function learning for feasibility assessment. Experiments show that training with $500$ episodes (merely $0.25\\%$ of the demonstration scale used by the baseline), SanD-Planner achieves state-of-the-art performance on the evaluated open benchmark, attaining success rates of $90.1\\%$ in simulated cluttered environments and $72.0\\%$ in indoor simulations. The performance is further proven by demonstrating zero-shot transferability to realistic experimentation in both 2D and 3D scenes. The dataset and pre-trained models will also be open-sourced."
    },
    {
      "title": "Unsupervised Learning of Efficient Exploration: Pre-training Adaptive Policies via Self-Imposed Goals",
      "url": "https://arxiv.org/abs/2601.19810",
      "date": "2026-01-27",
      "authors_text": "Octavio Pappalardo",
      "is_highlight": false,
      "score": 81.0,
      "summary": "The paper presents ULEE, an unsupervised meta-learning method that enhances exploration and adaptation in reinforcement learning by enabling agents to set and pursue self-imposed goals.",
      "debug_abstract": "Unsupervised pre-training can equip reinforcement learning agents with prior knowledge and accelerate learning in downstream tasks. A promising direction, grounded in human development, investigates agents that learn by setting and pursuing their own goals. The core challenge lies in how to effectively generate, select, and learn from such goals. Our focus is on broad distributions of downstream tasks where solving every task zero-shot is infeasible. Such settings naturally arise when the target tasks lie outside of the pre-training distribution or when their identities are unknown to the agent. In this work, we (i) optimize for efficient multi-episode exploration and adaptation within a meta-learning framework, and (ii) guide the training curriculum with evolving estimates of the agent&#39;s post-adaptation performance. We present ULEE, an unsupervised meta-learning method that combines an in-context learner with an adversarial goal-generation strategy that maintains training at the frontier of the agent&#39;s capabilities. On XLand-MiniGrid benchmarks, ULEE pre-training yields improved exploration and adaptation abilities that generalize to novel objectives, environment dynamics, and map structures. The resulting policy attains improved zero-shot and few-shot performance, and provides a strong initialization for longer fine-tuning processes. It outperforms learning from scratch, DIAYN pre-training, and alternative curricula."
    },
    {
      "title": "ActionMesh: Animated 3D Mesh Generation with Temporal 3D Diffusion",
      "url": "https://arxiv.org/abs/2601.16148",
      "date": "2026-01-22",
      "authors_text": "Remy Sabathier, David Novotny, Niloy J. Mitra, Tom Monnier",
      "is_highlight": true,
      "score": 81.4,
      "summary": "ActionMesh introduces a fast generative model for producing high-quality animated 3D meshes using temporal 3D diffusion, outperforming existing methods in speed and consistency.",
      "debug_abstract": "Generating animated 3D objects is at the heart of many applications, yet most advanced works are typically difficult to apply in practice because of their limited setup, their long runtime, or their limited quality. We introduce ActionMesh, a generative model that predicts production-ready 3D meshes &#34;in action&#34; in a feed-forward manner. Drawing inspiration from early video models, our key insight is to modify existing 3D diffusion models to include a temporal axis, resulting in a framework we dubbed &#34;temporal 3D diffusion&#34;. Specifically, we first adapt the 3D diffusion stage to generate a sequence of synchronized latents representing time-varying and independent 3D shapes. Second, we design a temporal 3D autoencoder that translates a sequence of independent shapes into the corresponding deformations of a pre-defined reference shape, allowing us to build an animation. Combining these two components, ActionMesh generates animated 3D meshes from different inputs like a monocular video, a text description, or even a 3D mesh with a text prompt describing its animation. Besides, compared to previous approaches, our method is fast and produces results that are rig-free and topology consistent, hence enabling rapid iteration and seamless applications like texturing and retargeting. We evaluate our model on standard video-to-4D benchmarks (Consistent4D, Objaverse) and report state-of-the-art performances on both geometric accuracy and temporal consistency, demonstrating that our model can deliver animated 3D meshes with unprecedented speed and quality."
    }
  ],
  "中国人民大学": [
    {
      "title": "MapDream: Task-Driven Map Learning for Vision-Language Navigation",
      "url": "https://arxiv.org/abs/2602.00222",
      "date": "2026-01-30",
      "authors_text": "Guoxin Lian, Shuo Wang, Yucheng Wang, Yongcai Wang, Maiyue Chen",
      "is_highlight": false,
      "score": 90.0,
      "summary": "MapDream introduces a task-driven framework for Vision-Language Navigation that learns compact, navigation-focused maps through autoregressive synthesis, achieving state-of-the-art performance in relevant benchmarks.",
      "teaser_image": "https://arxiv.org/html/2602.00222/x1.png",
      "debug_abstract": "Vision-Language Navigation (VLN) requires agents to follow natural language instructions in partially observed 3D environments, motivating map representations that aggregate spatial context beyond local perception. However, most existing approaches rely on hand-crafted maps constructed independently of the navigation policy. We argue that maps should instead be learned representations shaped directly by navigation objectives rather than exhaustive reconstructions. Based on this insight, we propose MapDream, a map-in-the-loop framework that formulates map construction as autoregressive bird&#39;s-eye-view (BEV) image synthesis. The framework jointly learns map generation and action prediction, distilling environmental context into a compact three-channel BEV map that preserves only navigation-critical affordances. Supervised pre-training bootstraps a reliable mapping-to-control interface, while the autoregressive design enables end-to-end joint optimization through reinforcement fine-tuning. Experiments on R2R-CE and RxR-CE achieve state-of-the-art monocular performance, validating task-driven generative map learning."
    },
    {
      "title": "Adaptive Ability Decomposing for Unlocking Large Reasoning Model Effective Reinforcement Learning",
      "url": "https://arxiv.org/abs/2602.00759",
      "date": "2026-01-31",
      "authors_text": "Zhipeng Chen, Xiaobo Qin, Wayne Xin Zhao, Youbin Wu, Ji-Rong Wen",
      "is_highlight": false,
      "score": 58.0,
      "summary": "The paper introduces A$^2$D, an Adaptive Ability Decomposing method that enhances reinforcement learning with verifiable rewards by breaking complex questions into simpler sub-questions for improved reasoning in large language models.",
      "teaser_image": "https://arxiv.org/html/2602.00759/x1.png",
      "debug_abstract": "Reinforcement learning with verifiable rewards (RLVR) has shown great potential to enhance the reasoning ability of large language models (LLMs). However, due to the limited amount of information provided during the RLVR process, the model can only engage in largely blind exploration, which often results in failure on challenging problems. To provide additional information for the RLVR process without relying on a teacher model, we propose A$^2$D, an Adaptive Ability Decomposing method for enhancing the effectiveness of RLVR. Specifically, we first train a decomposer via RLVR without distillation, enabling it to decompose complex questions into a set of simpler sub-questions. Next, we use this decomposer to annotate sub-questions for each question in the training dataset, and then train the reasoner under RLVR with sub-question guidance. To better understand A$^2$D, we first compare its performance with competitive baselines, showing its effectiveness. Next, we observe that our method functions as a plug-and-play module that can be applied to different RLVR algorithms. Furthermore, we conduct an analysis of the decomposer, revealing how the RLVR process affects its performance and behavior, and which type of guidance is better suited for enhancing the reasoner&#39;s exploration and exploitation abilities."
    },
    {
      "title": "CloDS: Visual-Only Unsupervised Cloth Dynamics Learning in Unknown Conditions",
      "url": "https://arxiv.org/abs/2602.01844",
      "date": "2026-02-02",
      "authors_text": "Yuliang Zhan, Jian Li, Wenbing Huang, Wenbing Huang, Yang Liu",
      "is_highlight": false,
      "score": 32.0,
      "summary": "CloDS introduces an unsupervised framework for learning cloth dynamics from multi-view visual data, overcoming challenges of non-linear deformations and occlusions without requiring known physical properties.",
      "teaser_image": "https://arxiv.org/html/2602.01844/x2.png",
      "debug_abstract": "Deep learning has demonstrated remarkable capabilities in simulating complex dynamic systems. However, existing methods require known physical properties as supervision or inputs, limiting their applicability under unknown conditions. To explore this challenge, we introduce Cloth Dynamics Grounding (CDG), a novel scenario for unsupervised learning of cloth dynamics from multi-view visual observations. We further propose Cloth Dynamics Splatting (CloDS), an unsupervised dynamic learning framework designed for CDG. CloDS adopts a three-stage pipeline that first performs video-to-geometry grounding and then trains a dynamics model on the grounded meshes. To cope with large non-linear deformations and severe self-occlusions during grounding, we introduce a dual-position opacity modulation that supports bidirectional mapping between 2D observations and 3D geometry via mesh-based Gaussian splatting in video-to-geometry grounding stage. It jointly considers the absolute and relative position of Gaussian components. Comprehensive experimental evaluations demonstrate that CloDS effectively learns cloth dynamics from visual data while maintaining strong generalization capabilities for unseen configurations. Our code is available at this https URL. Visualization results are available at this https URL}.%\\footnote{As in this example."
    },
    {
      "title": "Intelli-Planner: Towards Customized Urban Planning via Large Language Model Empowered Reinforcement Learning",
      "url": "https://arxiv.org/abs/2601.21212",
      "date": "2026-01-29",
      "authors_text": "Xixian Yong, Peilin Sun, Zihe Wang, Xiao Zhou",
      "is_highlight": true,
      "score": 76.0,
      "summary": "Intelli-Planner combines Deep Reinforcement Learning and large language models to enhance urban planning by improving stakeholder involvement, satisfaction, and decision-making efficiency.",
      "teaser_image": "https://arxiv.org/html/2601.21212/x1.png",
      "debug_abstract": "Effective urban planning is crucial for enhancing residents&#39; quality of life and ensuring societal stability, playing a pivotal role in the sustainable development of cities. Current planning methods heavily rely on human experts, which are time-consuming and labor-intensive, or utilize deep learning algorithms, often limiting stakeholder involvement. To bridge these gaps, we propose Intelli-Planner, a novel framework integrating Deep Reinforcement Learning (DRL) with large language models (LLMs) to facilitate participatory and customized planning scheme generation. Intelli-Planner utilizes demographic, geographic data, and planning preferences to determine high-level planning requirements and demands for each functional type. During training, a knowledge enhancement module is employed to enhance the decision-making capability of the policy network. Additionally, we establish a multi-dimensional evaluation system and employ LLM-based stakeholders for satisfaction scoring. Experimental validation across diverse urban settings shows that Intelli-Planner surpasses traditional baselines and achieves comparable performance to state-of-the-art DRL-based methods in objective metrics, while enhancing stakeholder satisfaction and convergence speed. These findings underscore the effectiveness and superiority of our framework, highlighting the potential for integrating the latest advancements in LLMs with DRL approaches to revolutionize tasks related to functional areas planning."
    },
    {
      "title": "Less Noise, More Voice: Reinforcement Learning for Reasoning via Instruction Purification",
      "url": "https://arxiv.org/abs/2601.21244",
      "date": "2026-01-29",
      "authors_text": "Yiju Guo, Tianyi Hu, Zexu Sun, Yankai Lin",
      "is_highlight": false,
      "score": 72.0,
      "summary": "The Less Noise Sampling Framework (LENS) enhances reinforcement learning by purging interference tokens, improving rollout efficiency and performance in reasoning tasks.",
      "teaser_image": "https://arxiv.org/html/2601.21244/x3.png",
      "debug_abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) has advanced LLM reasoning, but remains constrained by inefficient exploration under limited rollout budgets, leading to low sampling success and unstable training in complex tasks. We find that many exploration failures arise not from problem difficulty, but from a small number of prompt tokens that introduce interference. Building on this insight, we propose the Less Noise Sampling Framework (LENS), which first prompts by identifying and removing interference tokens. then transfers successful rollouts from the purification process to supervise policy optimization on the original noisy prompts, enabling the model to learn to ignore interference in the real-world, noisy prompting settings. Experimental results show that LENS significantly outperforms GRPO, delivering higher performance and faster convergence, with a 3.88% average gain and over 1.6$\\times$ speedup. Our work highlights the critical role of pruning interference tokens in improving rollout efficiency, offering a new perspective for RLVR research."
    },
    {
      "title": "ProRAG: Process-Supervised Reinforcement Learning for Retrieval-Augmented Generation",
      "url": "https://arxiv.org/abs/2601.21912",
      "date": "2026-01-29",
      "authors_text": "Zhao Wang, Ziliang Zhao, Zhicheng Dou",
      "is_highlight": false,
      "score": 69.0,
      "summary": "ProRAG introduces a process-supervised reinforcement learning framework that enhances Retrieval-Augmented Generation by integrating step-level supervision to improve reasoning accuracy and performance.",
      "teaser_image": "https://arxiv.org/html/2601.21912/x3.png",
      "debug_abstract": "Reinforcement learning (RL) has become a promising paradigm for optimizing Retrieval-Augmented Generation (RAG) in complex reasoning tasks. However, traditional outcome-based RL approaches often suffer from reward sparsity and inefficient credit assignment, as coarse-grained scalar rewards fail to identify specific erroneous steps within long-horizon trajectories. This ambiguity frequently leads to &#34;process hallucinations&#34;, where models reach correct answers through flawed logic or redundant retrieval steps. Although recent process-aware approaches attempt to mitigate this via static preference learning or heuristic reward shaping, they often lack the on-policy exploration capabilities required to decouple step-level credit from global outcomes. To address these challenges, we propose ProRAG, a process-supervised reinforcement learning framework designed to integrate learned step-level supervision into the online optimization loop. Our framework consists of four stages: (1) Supervised Policy Warmup to initialize the model with a structured reasoning format; (2) construction of an MCTS-based Process Reward Model (PRM) to quantify intermediate reasoning quality; (3) PRM-Guided Reasoning Refinement to align the policy with fine-grained process preferences; and (4) Process-Supervised Reinforcement Learning with a dual-granularity advantage mechanism. By aggregating step-level process rewards with global outcome signals, ProRAG provides precise feedback for every action. Extensive experiments on five multi-hop reasoning benchmarks demonstrate that ProRAG achieves superior overall performance compared to strong outcome-based and process-aware RL baselines, particularly on complex long-horizon tasks, validating the effectiveness of fine-grained process supervision. The code and model are available at this https URL."
    },
    {
      "title": "AtomMem : Learnable Dynamic Agentic Memory with Atomic Memory Operation",
      "url": "https://arxiv.org/abs/2601.08323",
      "date": "2026-01-27",
      "authors_text": "Yupeng Huo, Yaxi Lu, Zhong Zhang, Haotian Chen, Yankai Lin",
      "is_highlight": false,
      "score": 82.0,
      "summary": "AtomMem introduces a learnable dynamic memory framework that optimizes memory management through atomic CRUD operations, significantly outperforming static methods in long-horizon tasks.",
      "debug_abstract": "Equipping agents with memory is essential for solving real-world long-horizon problems. However, most existing agent memory mechanisms rely on static and hand-crafted workflows. This limits the performance and generalization ability of these memory designs, which highlights the need for a more flexible, learning-based memory framework. In this paper, we propose AtomMem, which reframes memory management as a dynamic decision-making problem. We deconstruct high-level memory processes into fundamental atomic CRUD (Create, Read, Update, Delete) operations, transforming the memory workflow into a learnable decision process. By combining supervised fine-tuning with reinforcement learning, AtomMem learns an autonomous, task-aligned policy to orchestrate memory behaviors tailored to specific task demands. Experimental results across 3 long-context benchmarks demonstrate that the trained AtomMem-8B consistently outperforms prior static-workflow memory methods. Further analysis of training dynamics shows that our learning-based formulation enables the agent to discover structured, task-aligned memory management strategies, highlighting a key advantage over predefined routines."
    },
    {
      "title": "LLM-in-Sandbox Elicits General Agentic Intelligence",
      "url": "https://arxiv.org/abs/2601.16206",
      "date": "2026-01-22",
      "authors_text": "Daixuan Cheng, Shaohan Huang, Yuxian Gu, Huatong Song, Guoxin Chen",
      "is_highlight": false,
      "score": 95.0,
      "summary": "The LLM-in-Sandbox framework enables large language models to demonstrate general intelligence in non-code tasks through sandbox exploration and reinforcement learning, achieving robust generalization across various domains.",
      "debug_abstract": "We introduce LLM-in-Sandbox, enabling LLMs to explore within a code sandbox (i.e., a virtual computer), to elicit general intelligence in non-code domains. We first demonstrate that strong LLMs, without additional training, exhibit generalization capabilities to leverage the code sandbox for non-code tasks. For example, LLMs spontaneously access external resources to acquire new knowledge, leverage the file system to handle long contexts, and execute scripts to satisfy formatting requirements. We further show that these agentic capabilities can be enhanced through LLM-in-Sandbox Reinforcement Learning (LLM-in-Sandbox-RL), which uses only non-agentic data to train models for sandbox exploration. Experiments demonstrate that LLM-in-Sandbox, in both training-free and post-trained settings, achieves robust generalization spanning mathematics, physics, chemistry, biomedicine, long-context understanding, and instruction following. Finally, we analyze LLM-in-Sandbox&#39;s efficiency from computational and system perspectives, and open-source it as a Python package to facilitate real-world deployment."
    }
  ],
  "天津大学": [
    {
      "title": "RGBX-R1: Visual Modality Chain-of-Thought Guided Reinforcement Learning for Multimodal Grounding",
      "url": "https://arxiv.org/abs/2602.00504",
      "date": "2026-01-31",
      "authors_text": "Jiahe Wu, Bing Cao, Qilong Wang, Qinghua Hu, Dongdong Li",
      "is_highlight": false,
      "score": 78.0,
      "summary": "RGBX-R1 enhances multimodal large language models' reasoning across various visual modalities using a novel UAV prompting strategy and a two-stage training approach, achieving significant performance improvements.",
      "teaser_image": "https://arxiv.org/html/2602.00504/x2.png",
      "debug_abstract": "Multimodal Large Language Models (MLLM) are primarily pre-trained on the RGB modality, thereby limiting their performance on other modalities, such as infrared, depth, and event data, which are crucial for complex scenarios. To address this, we propose RGBX-R1, a framework to enhance MLLM&#39;s perception and reasoning capacities across various X visual modalities. Specifically, we employ an Understand-Associate-Validate (UAV) prompting strategy to construct the Visual Modality Chain-of-Thought (VM-CoT), which aims to expand the MLLMs&#39; RGB understanding capability into X modalities. To progressively enhance reasoning capabilities, we introduce a two-stage training paradigm: Cold-Start Supervised Fine-Tuning (CS-SFT) and Spatio-Temporal Reinforcement Fine-Tuning (ST-RFT). CS-SFT supervises the reasoning process with the guidance of VM-CoT, equipping the MLLM with fundamental modality cognition. Building upon GRPO, ST-RFT employs a Modality-understanding Spatio-Temporal (MuST) reward to reinforce modality reasoning. Notably, we construct the first RGBX-Grounding benchmark, and extensive experiments verify our superiority in multimodal understanding and spatial perception, outperforming baselines by 22.71% on three RGBX grounding tasks."
    },
    {
      "title": "GDEPO: Group Dual-dynamic and Equal-right Advantage Policy Optimization with Enhanced Training Data Utilization for Sample-Constrained Reinforcement Learning",
      "url": "https://arxiv.org/abs/2601.06795",
      "date": "2026-01-22",
      "authors_text": "Zhengqing Yan, Xinyang Liu, Yi Zhang, Fan Guo, ChengXun Jia",
      "is_highlight": false,
      "score": 80.0,
      "summary": "GDEPO enhances reinforcement learning for Automated Theorem Proving by improving data utilization and optimization through dynamic sampling, equal-right advantage, and additional iterations.",
      "debug_abstract": "Automated Theorem Proving (ATP) represents a fundamental challenge in Artificial Intelligence (AI), requiring the construction of machine-verifiable proofs in formal languages such as Lean to evaluate AI reasoning capabilities. Reinforcement learning (RL), particularly the high-performance Group Relative Policy Optimization (GRPO) algorithm, has emerged as a mainstream approach for this task. However, in ATP scenarios, GRPO faces two critical issues: when composite rewards are used, its relative advantage estimation may conflict with the binary feedback from the formal verifier; meanwhile, its static sampling strategy may discard entire batches of data if no valid proof is found, resulting in zero contribution to model updates and significant data waste. To address these limitations, we propose Group Dual-dynamic and Equal-right-advantage Policy Optimization (GDEPO), a method incorporating three core mechanisms: 1) dynamic additional sampling, which resamples invalid batches until a valid proof is discovered; 2) equal-right advantage, decoupling the sign of the advantage function (based on correctness) from its magnitude (modulated by auxiliary rewards) to ensure stable and correct policy updates; and 3) dynamic additional iterations, applying extra gradient steps to initially failed but eventually successful samples to accelerate learning on challenging cases. Experiments conducted on three datasets of varying difficulty (MinF2F-test, MathOlympiadBench, PutnamBench) confirm the effectiveness of GDEPO, while ablation studies validate the necessity of its synergistic components. The proposed method enhances data utilization and optimization efficiency, offering a novel training paradigm for ATP."
    }
  ],
  "南方科技大学": [
    {
      "title": "ConsistentRFT: Reducing Visual Hallucinations in Flow-based Reinforcement Fine-Tuning",
      "url": "https://arxiv.org/abs/2602.03425",
      "date": "2026-02-03",
      "authors_text": "Xiaofeng Tan, Jun Liu, Yuanting Fan, Bin-Bin Gao, Xi Jiang",
      "is_highlight": false,
      "score": 55.0,
      "summary": "The paper presents ConsistentRFT, a framework that reduces visual hallucinations in flow-based reinforcement fine-tuning by balancing exploration and exploiting policy consistency through innovative mechanisms.",
      "teaser_image": "https://arxiv.org/html/2602.03425/x5.png",
      "debug_abstract": "Reinforcement Fine-Tuning (RFT) on flow-based models is crucial for preference alignment. However, they often introduce visual hallucinations like over-optimized details and semantic misalignment. This work preliminarily explores why visual hallucinations arise and how to reduce them. We first investigate RFT methods from a unified perspective, and reveal the core problems stemming from two aspects, exploration and exploitation: (1) limited exploration during stochastic differential equation (SDE) rollouts, leading to an over-emphasis on local details at the expense of global semantics, and (2) trajectory imitation process inherent in policy gradient methods, distorting the model&#39;s foundational vector field and its cross-step consistency. Building on this, we propose ConsistentRFT, a general framework to mitigate these hallucinations. Specifically, we design a Dynamic Granularity Rollout (DGR) mechanism to balance exploration between global semantics and local details by dynamically scheduling different noise sources. We then introduce a Consistent Policy Gradient Optimization (CPGO) that preserves the model&#39;s consistency by aligning the current policy with a more stable prior. Extensive experiments demonstrate that ConsistentRFT significantly mitigates visual hallucinations, achieving average reductions of 49\\% for low-level and 38\\% for high-level perceptual hallucinations. Furthermore, ConsistentRFT outperforms other RFT methods on out-of-domain metrics, showing an improvement of 5.1\\% (v.s. the baseline&#39;s decrease of -0.4\\%) over this http URL. This is \\href{this https URL}{Project Page}."
    },
    {
      "title": "Self-Guard: Defending Large Reasoning Models via enhanced self-reflection",
      "url": "https://arxiv.org/abs/2602.00707",
      "date": "2026-01-31",
      "authors_text": "Jingnan Zheng, Jingjun Xu, Yanzhen Luo, Chenhang Cui, Gelei Deng",
      "is_highlight": false,
      "score": 50.0,
      "summary": "Self-Guard is a lightweight framework enhancing safety compliance in Large Reasoning Models by promoting self-reflection and mitigating risks without sacrificing model utility.",
      "teaser_image": "https://arxiv.org/html/2602.00707/materials/workflow.png",
      "debug_abstract": "The emergence of Large Reasoning Models (LRMs) introduces a new paradigm of explicit reasoning, enabling remarkable advances yet posing unique risks such as reasoning manipulation and information leakage. To mitigate these risks, current alignment strategies predominantly rely on heavy post-training paradigms or external interventions. However, these approaches are often computationally intensive and fail to address the inherent awareness-compliance gap, a critical misalignment where models recognize potential risks yet prioritize following user instructions due to their sycophantic tendencies. To address these limitations, we propose Self-Guard, a lightweight safety defense framework that reinforces safety compliance at the representational level. Self-Guard operates through two principal stages: (1) safety-oriented prompting, which activates the model&#39;s latent safety awareness to evoke spontaneous reflection, and (2) safety activation steering, which extracts the resulting directional shift in the hidden state space and amplifies it to ensure that safety compliance prevails over sycophancy during inference. Experiments demonstrate that Self-Guard effectively bridges the awareness-compliance gap, achieving robust safety performance without compromising model utility. Furthermore, Self-Guard exhibits strong generalization across diverse unseen risks and varying model scales, offering a cost-efficient solution for LRM safety alignment."
    },
    {
      "title": "PolicyFlow: Policy Optimization with Continuous Normalizing Flow in Reinforcement Learning",
      "url": "https://arxiv.org/abs/2602.01156",
      "date": "2026-02-01",
      "authors_text": "Shunpeng Yang, Ben Liu, Hua Chen",
      "is_highlight": false,
      "score": 71.0,
      "summary": "PolicyFlow is a novel reinforcement learning algorithm that integrates continuous normalizing flows with PPO, enhancing performance and stability while reducing computational costs and encouraging diverse behaviors.",
      "teaser_image": "https://arxiv.org/html/2602.01156/x1.png",
      "debug_abstract": "Among on-policy reinforcement learning algorithms, Proximal Policy Optimization (PPO) demonstrates is widely favored for its simplicity, numerical stability, and strong empirical performance. Standard PPO relies on surrogate objectives defined via importance ratios, which require evaluating policy likelihood that is typically straightforward when the policy is modeled as a Gaussian distribution. However, extending PPO to more expressive, high-capacity policy models such as continuous normalizing flows (CNFs), also known as flow-matching models, is challenging because likelihood evaluation along the full flow trajectory is computationally expensive and often numerically unstable. To resolve this issue, we propose PolicyFlow, a novel on-policy CNF-based reinforcement learning algorithm that integrates expressive CNF policies with PPO-style objectives without requiring likelihood evaluation along the full flow path. PolicyFlow approximates importance ratios using velocity field variations along a simple interpolation path, reducing computational overhead without compromising training stability. To further prevent mode collapse and further encourage diverse behaviors, we propose the Brownian Regularizer, an implicit policy entropy regularizer inspired by Brownian motion, which is conceptually elegant and computationally lightweight. Experiments on diverse tasks across various environments including MultiGoal, PointMaze, IsaacLab and MuJoCo Playground show that PolicyFlow achieves competitive or superior performance compared to PPO using Gaussian policies and flow-based baselines including FPO and DPPO. Notably, results on MultiGoal highlight PolicyFlow&#39;s ability to capture richer multimodal action distributions."
    },
    {
      "title": "Know Your Step: Faster and Better Alignment for Flow Matching Models via Step-aware Advantages",
      "url": "https://arxiv.org/abs/2602.01591",
      "date": "2026-02-02",
      "authors_text": "Zhixiong Yue, Zixuan Ni, Feiyang Ye, Jinshan Zhang, Sheng Shen",
      "is_highlight": false,
      "score": 39.0,
      "summary": "The paper presents TAFS GRPO, a novel framework enhancing flow matching models for text-to-image generation by improving alignment with human preferences through efficient few-step sampling and adaptive noise.",
      "teaser_image": "https://arxiv.org/html/2602.01591/x3.png",
      "debug_abstract": "Recent advances in flow matching models, particularly with reinforcement learning (RL), have significantly enhanced human preference alignment in few step text to image generators. However, existing RL based approaches for flow matching models typically rely on numerous denoising steps, while suffering from sparse and imprecise reward signals that often lead to suboptimal alignment. To address these limitations, we propose Temperature Annealed Few step Sampling with Group Relative Policy Optimization (TAFS GRPO), a novel framework for training flow matching text to image models into efficient few step generators well aligned with human preferences. Our method iteratively injects adaptive temporal noise onto the results of one step samples. By repeatedly annealing the model&#39;s sampled outputs, it introduces stochasticity into the sampling process while preserving the semantic integrity of each generated image. Moreover, its step aware advantage integration mechanism combines the GRPO to avoid the need for the differentiable of reward function and provide dense and step specific rewards for stable policy optimization. Extensive experiments demonstrate that TAFS GRPO achieves strong performance in few step text to image generation and significantly improves the alignment of generated images with human preferences. The code and models of this work will be available to facilitate further research."
    },
    {
      "title": "UniPCB: A Unified Vision-Language Benchmark for Open-Ended PCB Quality Inspection",
      "url": "https://arxiv.org/abs/2601.19222",
      "date": "2026-01-27",
      "authors_text": "Fuxiang Sun, Xi Jiang, Jiansheng Wu, Haigang Zhang, Feng Zheng",
      "is_highlight": false,
      "score": 88.0,
      "summary": "The paper introduces UniPCB, a unified vision-language benchmark for PCB quality inspection, and PCB-GPT, an MLLM that significantly outperforms existing models in defect localization.",
      "debug_abstract": "Multimodal Large Language Models (MLLMs) show promise for general industrial quality inspection, but fall short in complex scenarios, such as Printed Circuit Board (PCB) inspection. PCB inspection poses unique challenges due to densely packed components, complex wiring structures, and subtle defect patterns that require specialized domain expertise. However, a high-quality, unified vision-language benchmark for quantitatively evaluating MLLMs across PCB inspection tasks remains absent, stemming not only from limited data availability but also from fragmented datasets and inconsistent standardization. To fill this gap, we propose UniPCB, the first unified vision-language benchmark for open-ended PCB quality inspection. UniPCB is built via a systematic pipeline that curates and standardizes data from disparate sources across three annotated scenarios. Furthermore, we introduce PCB-GPT, an MLLM trained on a new instruction dataset generated by this pipeline, utilizing a novel progressive curriculum that mimics the learning process of human experts. Evaluations on the UniPCB benchmark show that while existing MLLMs falter on domain-specific tasks, PCB-GPT establishes a new baseline. Notably, it more than doubles the performance on fine-grained defect localization compared to the strongest competitors, with significant advantages in localization and analysis. We will release the instruction data, benchmark, and model to facilitate future research."
    },
    {
      "title": "Natural Language-Driven Global Mapping of Martian Landforms",
      "url": "https://arxiv.org/abs/2601.15949",
      "date": "2026-01-22",
      "authors_text": "Yiran Wang, Shuoyuan Wang, Zhaoran Wei, Jiannan Zhao, Zhonghua Yao",
      "is_highlight": false,
      "score": 72.0,
      "summary": "MarScope is a vision-language framework that enables natural language-driven, label-free mapping of Martian landforms, enhancing global geomorphic analysis and user query flexibility.",
      "debug_abstract": "Planetary surfaces are typically analyzed using high-level semantic concepts in natural language, yet vast orbital image archives remain organized at the pixel level. This mismatch limits scalable, open-ended exploration of planetary surfaces. Here we present MarScope, a planetary-scale vision-language framework enabling natural language-driven, label-free mapping of Martian landforms. MarScope aligns planetary images and text in a shared semantic space, trained on over 200,000 curated image-text pairs. This framework transforms global geomorphic mapping on Mars by replacing pre-defined classifications with flexible semantic retrieval, enabling arbitrary user queries across the entire planet in 5 seconds with F1 scores up to 0.978. Applications further show that it extends beyond morphological classification to facilitate process-oriented analysis and similarity-based geomorphological mapping at a planetary scale. MarScope establishes a new paradigm where natural language serves as a direct interface for scientific discovery over massive geospatial datasets."
    }
  ],
  "Bio-Inspired Robotics Laboratory (BIRL)": [
    {
      "title": "Governance at the Edge of Architecture: Regulating NeuroAI and Neuromorphic Systems",
      "url": "https://arxiv.org/abs/2602.01503",
      "date": "2026-02-02",
      "authors_text": "Afifah Kashif, Abdul Muhsin Hameed, Asim Iqbal",
      "is_highlight": false,
      "score": 47.0,
      "summary": "The paper critiques existing AI governance frameworks for inadequately addressing the unique challenges posed by NeuroAI and neuromorphic systems, advocating for adaptive regulatory approaches.",
      "teaser_image": "https://arxiv.org/html/2602.01503/data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==",
      "debug_abstract": "Current AI governance frameworks, including regulatory benchmarks for accuracy, latency, and energy efficiency, are built for static, centrally trained artificial neural networks on von Neumann hardware. NeuroAI systems, embodied in neuromorphic hardware and implemented via spiking neural networks, break these assumptions. This paper examines the limitations of current AI governance frameworks for NeuroAI, arguing that assurance and audit methods must co-evolve with these architectures, aligning traditional regulatory metrics with the physics, learning dynamics, and embodied efficiency of brain-inspired computation to enable technically grounded assurance."
    },
    {
      "title": "Towards Mitigating Modality Bias in Vision-Language Models for Temporal Action Localization",
      "url": "https://arxiv.org/abs/2601.21078",
      "date": "2026-01-28",
      "authors_text": "Jiaqi Li, Guangming Wang, Shuntian Zheng, Minzhe Ni, Xiaoman Lu",
      "is_highlight": false,
      "score": 78.0,
      "summary": "The paper presents ActionVLM, a framework that reduces modality bias in Temporal Action Localization by prioritizing visual signals while adaptively incorporating language for improved performance.",
      "teaser_image": "https://arxiv.org/html/2601.21078/img/overview.png",
      "debug_abstract": "Temporal Action Localization (TAL) requires identifying both the boundaries and categories of actions in untrimmed videos. While vision-language models (VLMs) offer rich semantics to complement visual evidence, existing approaches tend to overemphasize linguistic priors at the expense of visual performance, leading to a pronounced modality bias. We propose ActionVLM, a vision-language aggregation framework that systematically mitigates modality bias in TAL. Our key insight is to preserve vision as the dominant signal while adaptively exploiting language only when beneficial. To this end, we introduce (i) a debiasing reweighting module that estimates the language advantage-the incremental benefit of language over vision-only predictions-and dynamically reweights language modality accordingly, and (ii) a residual aggregation strategy that treats language as a complementary refinement rather than the primary driver. This combination alleviates modality bias, reduces overconfidence from linguistic priors, and strengthens temporal reasoning. Experiments on THUMOS14 show that our model outperforms state-of-the-art by up to 3.2% mAP."
    },
    {
      "title": "Quartet of Diffusions: Structure-Aware Point Cloud Generation through Part and Symmetry Guidance",
      "url": "https://arxiv.org/abs/2601.20425",
      "date": "2026-01-28",
      "authors_text": "Chenliang Zhou, Fangcheng Zhong, Weihao Xia, Albert Miao, Canberk Baykal",
      "is_highlight": false,
      "score": 66.0,
      "summary": "The Quartet of Diffusions framework enables structure-aware point cloud generation by integrating symmetry and part composition through four coordinated diffusion models, achieving state-of-the-art results.",
      "teaser_image": "https://arxiv.org/html/2601.20425/figs/overview.png",
      "debug_abstract": "We introduce the Quartet of Diffusions, a structure-aware point cloud generation framework that explicitly models part composition and symmetry. Unlike prior methods that treat shape generation as a holistic process or only support part composition, our approach leverages four coordinated diffusion models to learn distributions of global shape latents, symmetries, semantic parts, and their spatial assembly. This structured pipeline ensures guaranteed symmetry, coherent part placement, and diverse, high-quality outputs. By disentangling the generative process into interpretable components, our method supports fine-grained control over shape attributes, enabling targeted manipulation of individual parts while preserving global consistency. A central global latent further reinforces structural coherence across assembled parts. Our experiments show that the Quartet achieves state-of-the-art performance. To our best knowledge, this is the first 3D point cloud generation framework that fully integrates and enforces both symmetry and part priors throughout the generative process."
    },
    {
      "title": "Reimagining Social Robots as Recommender Systems: Foundations, Framework, and Applications",
      "url": "https://arxiv.org/abs/2601.19761",
      "date": "2026-01-27",
      "authors_text": "Jin Huang, Fethiye Irmak Doğan, Hatice Gunes",
      "is_highlight": true,
      "score": 74.0,
      "summary": "The paper proposes integrating recommender systems into social robots to enhance personalization by effectively modeling user preferences and ensuring ethical interactions.",
      "debug_abstract": "Personalization in social robots refers to the ability of the robot to meet the needs and/or preferences of an individual user. Existing approaches typically rely on large language models (LLMs) to generate context-aware responses based on user metadata and historical interactions or on adaptive methods such as reinforcement learning (RL) to learn from users&#39; immediate reactions in real time. However, these approaches fall short of comprehensively capturing user preferences-including long-term, short-term, and fine-grained aspects-, and of using them to rank and select actions, proactively personalize interactions, and ensure ethically responsible adaptations. To address the limitations, we propose drawing on recommender systems (RSs), which specialize in modeling user preferences and providing personalized recommendations. To ensure the integration of RS techniques is well-grounded and seamless throughout the social robot pipeline, we (i) align the paradigms underlying social robots and RSs, (ii) identify key techniques that can enhance personalization in social robots, and (iii) design them as modular, plug-and-play components. This work not only establishes a framework for integrating RS techniques into social robots but also opens a pathway for deep collaboration between the RS and HRI communities, accelerating innovation in both fields."
    },
    {
      "title": "GameTalk: Training LLMs for Strategic Conversation",
      "url": "https://arxiv.org/abs/2601.16276",
      "date": "2026-01-22",
      "authors_text": "Victor Conchello Vendrell, Max Ruiz Luyten, Mihaela van der Schaar",
      "is_highlight": false,
      "score": 60.0,
      "summary": "GameTalk is a framework that trains LLMs for strategic decision-making through multi-turn conversations, optimizing long-term objectives and outperforming untrained models in complex games.",
      "debug_abstract": "Strategic decision-making in multi-agent settings is a key challenge for large language models (LLMs), particularly when coordination and negotiation must unfold over extended conversations. While recent work has explored the use of LLMs in isolated decision tasks, little attention has been given to optimizing long-term objectives through dialogue. We introduce \\textbf{GameTalk}, a framework for training LLMs to make strategic decisions via multi-turn interactions. Unlike prior work that focuses on single-turn objectives or static action prediction, we train LLMs to optimize a global objective across full conversations. We achieve this by adapting fine-tuning methods like GRPO, DPO, and STaR to incorporate reward signals that depend on the entire interaction. We evaluate this approach on a suite of increasingly complex games, designed to stress different aspects of reasoning, coordination, and opponent modeling. Our results show that GameTalk significantly outperforms untrained models, especially under reward shaping, with DPO consistently yielding the strongest gains. These findings position conversational fine-tuning as a promising path for LLMs to reason, negotiate, and act in interactive environments."
    }
  ],
  "Affective Intelligence and Robotics Laboratory (AFAR)": [
    {
      "title": "Governance at the Edge of Architecture: Regulating NeuroAI and Neuromorphic Systems",
      "url": "https://arxiv.org/abs/2602.01503",
      "date": "2026-02-02",
      "authors_text": "Afifah Kashif, Abdul Muhsin Hameed, Asim Iqbal",
      "is_highlight": false,
      "score": 47.0,
      "summary": "The paper critiques existing AI governance frameworks for inadequately addressing the unique challenges posed by NeuroAI and neuromorphic systems, advocating for adaptive regulatory approaches.",
      "teaser_image": "https://arxiv.org/html/2602.01503/data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==",
      "debug_abstract": "Current AI governance frameworks, including regulatory benchmarks for accuracy, latency, and energy efficiency, are built for static, centrally trained artificial neural networks on von Neumann hardware. NeuroAI systems, embodied in neuromorphic hardware and implemented via spiking neural networks, break these assumptions. This paper examines the limitations of current AI governance frameworks for NeuroAI, arguing that assurance and audit methods must co-evolve with these architectures, aligning traditional regulatory metrics with the physics, learning dynamics, and embodied efficiency of brain-inspired computation to enable technically grounded assurance."
    },
    {
      "title": "Towards Mitigating Modality Bias in Vision-Language Models for Temporal Action Localization",
      "url": "https://arxiv.org/abs/2601.21078",
      "date": "2026-01-28",
      "authors_text": "Jiaqi Li, Guangming Wang, Shuntian Zheng, Minzhe Ni, Xiaoman Lu",
      "is_highlight": false,
      "score": 78.0,
      "summary": "The paper presents ActionVLM, a framework that reduces modality bias in Temporal Action Localization by prioritizing visual signals while adaptively incorporating language for improved performance.",
      "teaser_image": "https://arxiv.org/html/2601.21078/img/overview.png",
      "debug_abstract": "Temporal Action Localization (TAL) requires identifying both the boundaries and categories of actions in untrimmed videos. While vision-language models (VLMs) offer rich semantics to complement visual evidence, existing approaches tend to overemphasize linguistic priors at the expense of visual performance, leading to a pronounced modality bias. We propose ActionVLM, a vision-language aggregation framework that systematically mitigates modality bias in TAL. Our key insight is to preserve vision as the dominant signal while adaptively exploiting language only when beneficial. To this end, we introduce (i) a debiasing reweighting module that estimates the language advantage-the incremental benefit of language over vision-only predictions-and dynamically reweights language modality accordingly, and (ii) a residual aggregation strategy that treats language as a complementary refinement rather than the primary driver. This combination alleviates modality bias, reduces overconfidence from linguistic priors, and strengthens temporal reasoning. Experiments on THUMOS14 show that our model outperforms state-of-the-art by up to 3.2% mAP."
    },
    {
      "title": "Quartet of Diffusions: Structure-Aware Point Cloud Generation through Part and Symmetry Guidance",
      "url": "https://arxiv.org/abs/2601.20425",
      "date": "2026-01-28",
      "authors_text": "Chenliang Zhou, Fangcheng Zhong, Weihao Xia, Albert Miao, Canberk Baykal",
      "is_highlight": false,
      "score": 66.0,
      "summary": "The Quartet of Diffusions framework enables structure-aware point cloud generation by integrating symmetry and part composition through four coordinated diffusion models, achieving state-of-the-art results.",
      "teaser_image": "https://arxiv.org/html/2601.20425/figs/overview.png",
      "debug_abstract": "We introduce the Quartet of Diffusions, a structure-aware point cloud generation framework that explicitly models part composition and symmetry. Unlike prior methods that treat shape generation as a holistic process or only support part composition, our approach leverages four coordinated diffusion models to learn distributions of global shape latents, symmetries, semantic parts, and their spatial assembly. This structured pipeline ensures guaranteed symmetry, coherent part placement, and diverse, high-quality outputs. By disentangling the generative process into interpretable components, our method supports fine-grained control over shape attributes, enabling targeted manipulation of individual parts while preserving global consistency. A central global latent further reinforces structural coherence across assembled parts. Our experiments show that the Quartet achieves state-of-the-art performance. To our best knowledge, this is the first 3D point cloud generation framework that fully integrates and enforces both symmetry and part priors throughout the generative process."
    },
    {
      "title": "Reimagining Social Robots as Recommender Systems: Foundations, Framework, and Applications",
      "url": "https://arxiv.org/abs/2601.19761",
      "date": "2026-01-27",
      "authors_text": "Jin Huang, Fethiye Irmak Doğan, Hatice Gunes",
      "is_highlight": true,
      "score": 74.0,
      "summary": "The paper proposes integrating recommender systems into social robots to enhance personalization by effectively modeling user preferences and ensuring ethical interactions.",
      "debug_abstract": "Personalization in social robots refers to the ability of the robot to meet the needs and/or preferences of an individual user. Existing approaches typically rely on large language models (LLMs) to generate context-aware responses based on user metadata and historical interactions or on adaptive methods such as reinforcement learning (RL) to learn from users&#39; immediate reactions in real time. However, these approaches fall short of comprehensively capturing user preferences-including long-term, short-term, and fine-grained aspects-, and of using them to rank and select actions, proactively personalize interactions, and ensure ethically responsible adaptations. To address the limitations, we propose drawing on recommender systems (RSs), which specialize in modeling user preferences and providing personalized recommendations. To ensure the integration of RS techniques is well-grounded and seamless throughout the social robot pipeline, we (i) align the paradigms underlying social robots and RSs, (ii) identify key techniques that can enhance personalization in social robots, and (iii) design them as modular, plug-and-play components. This work not only establishes a framework for integrating RS techniques into social robots but also opens a pathway for deep collaboration between the RS and HRI communities, accelerating innovation in both fields."
    },
    {
      "title": "GameTalk: Training LLMs for Strategic Conversation",
      "url": "https://arxiv.org/abs/2601.16276",
      "date": "2026-01-22",
      "authors_text": "Victor Conchello Vendrell, Max Ruiz Luyten, Mihaela van der Schaar",
      "is_highlight": false,
      "score": 60.0,
      "summary": "GameTalk is a framework that trains LLMs for strategic decision-making through multi-turn conversations, optimizing long-term objectives and outperforming untrained models in complex games.",
      "debug_abstract": "Strategic decision-making in multi-agent settings is a key challenge for large language models (LLMs), particularly when coordination and negotiation must unfold over extended conversations. While recent work has explored the use of LLMs in isolated decision tasks, little attention has been given to optimizing long-term objectives through dialogue. We introduce \\textbf{GameTalk}, a framework for training LLMs to make strategic decisions via multi-turn interactions. Unlike prior work that focuses on single-turn objectives or static action prediction, we train LLMs to optimize a global objective across full conversations. We achieve this by adapting fine-tuning methods like GRPO, DPO, and STaR to incorporate reward signals that depend on the entire interaction. We evaluate this approach on a suite of increasingly complex games, designed to stress different aspects of reasoning, coordination, and opponent modeling. Our results show that GameTalk significantly outperforms untrained models, especially under reward shaping, with DPO consistently yielding the strongest gains. These findings position conversational fine-tuning as a promising path for LLMs to reason, negotiate, and act in interactive environments."
    }
  ],
  "人机物智能融合实验室 (HCP)": [
    {
      "title": "IntentRL: Training Proactive User-intent Agents for Open-ended Deep Research via Reinforcement Learning",
      "url": "https://arxiv.org/abs/2602.03468",
      "date": "2026-02-03",
      "authors_text": "Haohao Luo, Zexi Li, Yuexiang Xie, Wenhao Zhang, Yaliang Li",
      "is_highlight": false,
      "score": 74.0,
      "summary": "IntentRL trains proactive agents to clarify user intents before deep research, enhancing performance and efficiency in generating long-form reports from web data.",
      "teaser_image": "https://arxiv.org/html/2602.03468/x2.png",
      "debug_abstract": "Deep Research (DR) agents extend Large Language Models (LLMs) beyond parametric knowledge by autonomously retrieving and synthesizing evidence from large web corpora into long-form reports, enabling a long-horizon agentic paradigm. However, unlike real-time conversational assistants, DR is computationally expensive and time-consuming, creating an autonomy-interaction dilemma: high autonomy on ambiguous user queries often leads to prolonged execution with unsatisfactory outcomes. To address this, we propose IntentRL, a framework that trains proactive agents to clarify latent user intents before starting long-horizon research. To overcome the scarcity of open-ended research data, we introduce a scalable pipeline that expands a few seed samples into high-quality dialogue turns via a shallow-to-deep intent refinement graph. We further adopt a two-stage reinforcement learning (RL) strategy: Stage I applies RL on offline dialogues to efficiently learn general user-interaction behavior, while Stage II uses the trained agent and a user simulator for online rollouts to strengthen adaptation to diverse user feedback. Extensive experiments show that IntentRL significantly improves both intent hit rate and downstream task performance, outperforming the built-in clarify modules of closed-source DR agents and proactive LLM baselines."
    },
    {
      "title": "Refer-Agent: A Collaborative Multi-Agent System with Reasoning and Reflection for Referring Video Object Segmentation",
      "url": "https://arxiv.org/abs/2602.03595",
      "date": "2026-02-03",
      "authors_text": "Haichao Jiang, Tianming Liang, Wei-Shi Zheng, Jian-Fang Hu",
      "is_highlight": false,
      "score": 53.0,
      "summary": "Refer-Agent is a collaborative multi-agent system for Referring Video Object Segmentation that enhances performance through reasoning-reflection mechanisms and flexible integration of new MLLMs.",
      "teaser_image": "https://arxiv.org/html/2602.03595/x2.png",
      "debug_abstract": "Referring Video Object Segmentation (RVOS) aims to segment objects in videos based on textual queries. Current methods mainly rely on large-scale supervised fine-tuning (SFT) of Multi-modal Large Language Models (MLLMs). However, this paradigm suffers from heavy data dependence and limited scalability against the rapid evolution of MLLMs. Although recent zero-shot approaches offer a flexible alternative, their performance remains significantly behind SFT-based methods, due to the straightforward workflow designs. To address these limitations, we propose \\textbf{Refer-Agent}, a collaborative multi-agent system with alternating reasoning-reflection mechanisms. This system decomposes RVOS into step-by-step reasoning process. During reasoning, we introduce a Coarse-to-Fine frame selection strategy to ensure the frame diversity and textual relevance, along with a Dynamic Focus Layout that adaptively adjusts the agent&#39;s visual focus. Furthermore, we propose a Chain-of-Reflection mechanism, which employs a Questioner-Responder pair to generate a self-reflection chain, enabling the system to verify intermediate results and generates feedback for next-round reasoning refinement. Extensive experiments on five challenging benchmarks demonstrate that Refer-Agent significantly outperforms state-of-the-art methods, including both SFT-based models and zero-shot approaches. Moreover, Refer-Agent is flexible and enables fast integration of new MLLMs without any additional fine-tuning costs. Code will be released."
    },
    {
      "title": "Resource-Efficient Reinforcement for Reasoning Large Language Models via Dynamic One-Shot Policy Refinement",
      "url": "https://arxiv.org/abs/2602.00815",
      "date": "2026-01-31",
      "authors_text": "Yunjian Zhang, Sudong Wang, Yang Li, Peiran Xu, Conghao Zhou",
      "is_highlight": false,
      "score": 57.0,
      "summary": "The paper introduces Dynamic One-Shot Policy Refinement (DoPR), a resource-efficient reinforcement learning strategy that enhances reasoning in large language models while significantly reducing training costs.",
      "teaser_image": "https://arxiv.org/html/2602.00815/dopr.png",
      "debug_abstract": "Large language models (LLMs) have exhibited remarkable performance on complex reasoning tasks, with reinforcement learning under verifiable rewards (RLVR) emerging as a principled framework for aligning model behavior with reasoning chains. Despite its promise, RLVR remains prohibitively resource-intensive, requiring extensive reward signals and incurring substantial rollout costs during training. In this work, we revisit the fundamental question of data and compute efficiency in RLVR. We first establish a theoretical lower bound on the sample complexity required to unlock reasoning capabilities, and empirically validate that strong performance can be achieved with a surprisingly small number of training instances. To tackle the computational burden, we propose Dynamic One-Shot Policy Refinement (DoPR), an uncertainty-aware RL strategy that dynamically selects a single informative training sample per batch for policy updates, guided by reward volatility and exploration-driven acquisition. DoPR reduces rollout overhead by nearly an order of magnitude while preserving competitive reasoning accuracy, offering a scalable and resource-efficient solution for LLM post-training. This approach offers a practical path toward more efficient and accessible RL-based training for reasoning-intensive LLM applications."
    },
    {
      "title": "SGHA-Attack: Semantic-Guided Hierarchical Alignment for Transferable Targeted Attacks on Vision-Language Models",
      "url": "https://arxiv.org/abs/2602.01574",
      "date": "2026-02-02",
      "authors_text": "Haobo Wang, Weiqi Luo, Xiaojun Jia, Xiaochun Cao",
      "is_highlight": false,
      "score": 44.0,
      "summary": "SGHA-Attack enhances targeted adversarial attacks on vision-language models by leveraging multiple semantic references and aligning intermediate features for improved transferability and robustness.",
      "teaser_image": "https://arxiv.org/html/2602.01574/x1.png",
      "debug_abstract": "Large vision-language models (VLMs) are vulnerable to transfer-based adversarial perturbations, enabling attackers to optimize on surrogate models and manipulate black-box VLM outputs. Prior targeted transfer attacks often overfit surrogate-specific embedding space by relying on a single reference and emphasizing final-layer alignment, which underutilizes intermediate semantics and degrades transfer across heterogeneous VLMs. To address this, we propose SGHA-Attack, a Semantic-Guided Hierarchical Alignment framework that adopts multiple target references and enforces intermediate-layer consistency. Concretely, we generate a visually grounded reference pool by sampling a frozen text-to-image model conditioned on the target prompt, and then carefully select the Top-K most semantically relevant anchors under the surrogate to form a weighted mixture for stable optimization guidance. Building on these anchors, SGHA-Attack injects target semantics throughout the feature hierarchy by aligning intermediate visual representations at both global and spatial granularities across multiple depths, and by synchronizing intermediate visual and textual features in a shared latent subspace to provide early cross-modal supervision before the final projection. Extensive experiments on open-source and commercial black-box VLMs show that SGHA-Attack achieves stronger targeted transferability than prior methods and remains robust under preprocessing and purification defenses."
    },
    {
      "title": "CIEC: Coupling Implicit and Explicit Cues for Multimodal Weakly Supervised Manipulation Localization",
      "url": "https://arxiv.org/abs/2602.02175",
      "date": "2026-02-02",
      "authors_text": "Xinquan Yu, Wei Lu, Xiangyang Luo",
      "is_highlight": false,
      "score": 21.0,
      "summary": "The CIEC framework enables multimodal weakly-supervised manipulation localization using only coarse annotations, integrating image and text cues for improved accuracy and reliability.",
      "teaser_image": "https://arxiv.org/html/2602.02175/x2.png",
      "debug_abstract": "To mitigate the threat of misinformation, multimodal manipulation localization has garnered growing attention. Consider that current methods rely on costly and time-consuming fine-grained annotations, such as patch/token-level annotations. This paper proposes a novel framework named Coupling Implicit and Explicit Cues (CIEC), which aims to achieve multimodal weakly-supervised manipulation localization for image-text pairs utilizing only coarse-grained image/sentence-level annotations. It comprises two branches, image-based and text-based weakly-supervised localization. For the former, we devise the Textual-guidance Refine Patch Selection (TRPS) module. It integrates forgery cues from both visual and textual perspectives to lock onto suspicious regions aided by spatial priors. Followed by the background silencing and spatial contrast constraints to suppress interference from irrelevant areas. For the latter, we devise the Visual-deviation Calibrated Token Grounding (VCTG) module. It focuses on meaningful content words and leverages relative visual bias to assist token localization. Followed by the asymmetric sparse and semantic consistency constraints to mitigate label noise and ensure reliability. Extensive experiments demonstrate the effectiveness of our CIEC, yielding results comparable to fully supervised methods on several evaluation metrics."
    },
    {
      "title": "Context Learning for Multi-Agent Discussion",
      "url": "https://arxiv.org/abs/2602.02350",
      "date": "2026-02-02",
      "authors_text": "Xingyuan Hua, Sheng Yue, Xinyi Li, Yizhe Zhao, Jinrui Zhang",
      "is_highlight": false,
      "score": 17.0,
      "summary": "The paper presents M2CL, a multi-LLM context learning method that enhances coherence in Multi-Agent Discussions, improving problem-solving performance by 20%-50% over existing approaches.",
      "teaser_image": "https://arxiv.org/html/2602.02350/x2.png",
      "debug_abstract": "Multi-Agent Discussion (MAD) has garnered increasing attention very recently, where multiple LLM instances collaboratively solve problems via structured discussion. However, we find that current MAD methods easily suffer from discussion inconsistency, LLMs fail to reach a coherent solution, due to the misalignment between their individual this http URL this paper, we introduce a multi-LLM context learning method (M2CL) that learns a context generator for each agent, capable of dynamically generating context instructions per discussion round via automatic information organization and refinement. Specifically, inspired by our theoretical insights on the context instruction, M2CL train the generators to control context coherence and output discrepancies via a carefully crafted self-adaptive this http URL enables LLMs to avoid premature convergence on majority noise and progressively reach the correct consensus. We evaluate M2CL on challenging tasks, including academic reasoning, embodied tasks, and mobile control. The results show that the performance of M2CL significantly surpasses existing methods by 20%--50%, while enjoying favorable transferability and computational efficiency."
    },
    {
      "title": "Superman: Unifying Skeleton and Vision for Human Motion Perception and Generation",
      "url": "https://arxiv.org/abs/2602.02401",
      "date": "2026-02-02",
      "authors_text": "Xinshun Wang, Peiming Li, Ziyi Wang, Zhongbin Fang, Zhichao Deng",
      "is_highlight": false,
      "score": 15.0,
      "summary": "Superman unifies visual perception and skeleton-based motion generation through a Vision-Guided Motion Tokenizer, enabling robust cross-modal learning and superior performance in human motion tasks.",
      "teaser_image": "https://arxiv.org/html/2602.02401/x2.png",
      "debug_abstract": "Human motion analysis tasks, such as temporal 3D pose estimation, motion prediction, and motion in-betweening, play an essential role in computer vision. However, current paradigms suffer from severe fragmentation. First, the field is split between ``perception&#39;&#39; models that understand motion from video but only output text, and ``generation&#39;&#39; models that cannot perceive from raw visual input. Second, generative MLLMs are often limited to single-frame, static poses using dense, parametric SMPL models, failing to handle temporal motion. Third, existing motion vocabularies are built from skeleton data alone, severing the link to the visual domain. To address these challenges, we introduce Superman, a unified framework that bridges visual perception with temporal, skeleton-based motion generation. Our solution is twofold. First, to overcome the modality disconnect, we propose a Vision-Guided Motion Tokenizer. Leveraging the natural geometric alignment between 3D skeletons and visual data, this module pioneers robust joint learning from both modalities, creating a unified, cross-modal motion vocabulary. Second, grounded in this motion language, a single, unified MLLM architecture is trained to handle all tasks. This module flexibly processes diverse, temporal inputs, unifying 3D skeleton pose estimation from video (perception) with skeleton-based motion prediction and in-betweening (generation). Extensive experiments on standard benchmarks, including Human3.6M, demonstrate that our unified method achieves state-of-the-art or competitive performance across all motion tasks. This showcases a more efficient and scalable path for generative motion analysis using skeletons."
    },
    {
      "title": "Advances and Innovations in the Multi-Agent Robotic System (MARS) Challenge",
      "url": "https://arxiv.org/abs/2601.18733",
      "date": "2026-01-26",
      "authors_text": "Li Kang, Heng Zhou, Xiufeng Song, Rui Li, Bruno N. Y. Chen",
      "is_highlight": false,
      "score": 78.0,
      "summary": "The MARS Challenge promotes advancements in multi-agent robotic systems by exploring vision-language models for collaborative task execution and planning in dynamic environments.",
      "teaser_image": null,
      "debug_abstract": "Recent advancements in multimodal large language models and vision-languageaction models have significantly driven progress in Embodied AI. As the field transitions toward more complex task scenarios, multi-agent system frameworks are becoming essential for achieving scalable, efficient, and collaborative solutions. This shift is fueled by three primary factors: increasing agent capabilities, enhancing system efficiency through task delegation, and enabling advanced human-agent interactions. To address the challenges posed by multi-agent collaboration, we propose the Multi-Agent Robotic System (MARS) Challenge, held at the NeurIPS 2025 Workshop on SpaVLE. The competition focuses on two critical areas: planning and control, where participants explore multi-agent embodied planning using vision-language models (VLMs) to coordinate tasks and policy execution to perform robotic manipulation in dynamic environments. By evaluating solutions submitted by participants, the challenge provides valuable insights into the design and coordination of embodied multi-agent systems, contributing to the future development of advanced collaborative AI systems."
    },
    {
      "title": "TouchGuide: Inference-Time Steering of Visuomotor Policies via Touch Guidance",
      "url": "https://arxiv.org/abs/2601.20239",
      "date": "2026-01-28",
      "authors_text": "Zhemeng Zhang, Jiahua Ma, Xincheng Yang, Xin Wen, Yuzhi Zhang",
      "is_highlight": false,
      "score": 90.0,
      "summary": "TouchGuide enhances robot manipulation by integrating tactile feedback with visual inputs to refine actions during inference, outperforming existing visuo-tactile policies in complex tasks.",
      "teaser_image": "https://arxiv.org/html/2601.20239/x2.png",
      "debug_abstract": "Fine-grained and contact-rich manipulation remain challenging for robots, largely due to the underutilization of tactile feedback. To address this, we introduce TouchGuide, a novel cross-policy visuo-tactile fusion paradigm that fuses modalities within a low-dimensional action space. Specifically, TouchGuide operates in two stages to guide a pre-trained diffusion or flow-matching visuomotor policy at inference time. First, the policy produces a coarse, visually-plausible action using only visual inputs during early sampling. Second, a task-specific Contact Physical Model (CPM) provides tactile guidance to steer and refine the action, ensuring it aligns with realistic physical contact conditions. Trained through contrastive learning on limited expert demonstrations, the CPM provides a tactile-informed feasibility score to steer the sampling process toward refined actions that satisfy physical contact constraints. Furthermore, to facilitate TouchGuide training with high-quality and cost-effective data, we introduce TacUMI, a data collection system. TacUMI achieves a favorable trade-off between precision and affordability; by leveraging rigid fingertips, it obtains direct tactile feedback, thereby enabling the collection of reliable tactile data. Extensive experiments on five challenging contact-rich tasks, such as shoe lacing and chip handover, show that TouchGuide consistently and significantly outperforms state-of-the-art visuo-tactile policies."
    },
    {
      "title": "MARE: Multimodal Alignment and Reinforcement for Explainable Deepfake Detection via Vision-Language Models",
      "url": "https://arxiv.org/abs/2601.20433",
      "date": "2026-01-28",
      "authors_text": "Wenbo Xu, Wei Lu, Xiangyang Luo, Jiantao Zhou",
      "is_highlight": false,
      "score": 76.0,
      "summary": "MARE enhances Deepfake detection using vision-language models through multimodal alignment, reinforcement learning from human feedback, and a forgery disentanglement module, achieving state-of-the-art performance.",
      "teaser_image": "https://arxiv.org/html/2601.20433/x2.png",
      "debug_abstract": "Deepfake detection is a widely researched topic that is crucial for combating the spread of malicious content, with existing methods mainly modeling the problem as classification or spatial localization. The rapid advancements in generative models impose new demands on Deepfake detection. In this paper, we propose multimodal alignment and reinforcement for explainable Deepfake detection via vision-language models, termed MARE, which aims to enhance the accuracy and reliability of Vision-Language Models (VLMs) in Deepfake detection and reasoning. Specifically, MARE designs comprehensive reward functions, incorporating reinforcement learning from human feedback (RLHF), to incentivize the generation of text-spatially aligned reasoning content that adheres to human preferences. Besides, MARE introduces a forgery disentanglement module to capture intrinsic forgery traces from high-level facial semantics, thereby improving its authenticity detection capability. We conduct thorough evaluations on the reasoning content generated by MARE. Both quantitative and qualitative experimental results demonstrate that MARE achieves state-of-the-art performance in terms of accuracy and reliability."
    },
    {
      "title": "Why Keep Your Doubts to Yourself? Trading Visual Uncertainties in Multi-Agent Bandit Systems",
      "url": "https://arxiv.org/abs/2601.18735",
      "date": "2026-01-26",
      "authors_text": "Jusheng Zhang, Yijia Fan, Kaitong Cai, Jing Yang, Jiawei Yao",
      "is_highlight": false,
      "score": 44.0,
      "summary": "The Agora framework transforms multi-agent coordination into a decentralized market for trading visual uncertainties, enhancing accuracy and reducing costs compared to existing strategies.",
      "debug_abstract": "Vision-Language Models (VLMs) enable powerful multi-agent systems, but scaling them is economically unsustainable: coordinating heterogeneous agents under information asymmetry often spirals costs. Existing paradigms, such as Mixture-of-Agents and knowledge-based routers, rely on heuristic proxies that ignore costs and collapse uncertainty structure, leading to provably suboptimal coordination. We introduce Agora, a framework that reframes coordination as a decentralized market for uncertainty. Agora formalizes epistemic uncertainty into a structured, tradable asset (perceptual, semantic, inferential), and enforces profitability-driven trading among agents based on rational economic rules. A market-aware broker, extending Thompson Sampling, initiates collaboration and guides the system toward cost-efficient equilibria. Experiments on five multimodal benchmarks (MMMU, MMBench, MathVision, InfoVQA, CC-OCR) show that Agora outperforms strong VLMs and heuristic multi-agent strategies, e.g., achieving +8.5% accuracy over the best baseline on MMMU while reducing cost by over 3x. These results establish market-based coordination as a principled and scalable paradigm for building economically viable multi-agent visual intelligence systems."
    },
    {
      "title": "ResAgent: Entropy-based Prior Point Discovery and Visual Reasoning for Referring Expression Segmentation",
      "url": "https://arxiv.org/abs/2601.16394",
      "date": "2026-01-23",
      "authors_text": "Yihao Wang, Jusheng Zhang, Ziyi Tang, Keze Wang, Meng Yang",
      "is_highlight": true,
      "score": 75.0,
      "summary": "ResAgent introduces an innovative RES framework combining entropy-based point discovery and vision-based reasoning, achieving state-of-the-art segmentation performance across multiple benchmarks.",
      "debug_abstract": "Referring Expression Segmentation (RES) is a core vision-language segmentation task that enables pixel-level understanding of targets via free-form linguistic expressions, supporting critical applications such as human-robot interaction and augmented reality. Despite the progress of Multimodal Large Language Model (MLLM)-based approaches, existing RES methods still suffer from two key limitations: first, the coarse bounding boxes from MLLMs lead to redundant or non-discriminative point prompts; second, the prevalent reliance on textual coordinate reasoning is unreliable, as it fails to distinguish targets from visually similar distractors. To address these issues, we propose \\textbf{\\model}, a novel RES framework integrating \\textbf{E}ntropy-\\textbf{B}ased Point \\textbf{D}iscovery (\\textbf{EBD}) and \\textbf{V}ision-\\textbf{B}ased \\textbf{R}easoning (\\textbf{VBR}). Specifically, EBD identifies high-information candidate points by modeling spatial uncertainty within coarse bounding boxes, treating point selection as an information maximization process. VBR verifies point correctness through joint visual-semantic alignment, abandoning text-only coordinate inference for more robust validation. Built on these components, \\model implements a coarse-to-fine workflow: bounding box initialization, entropy-guided point discovery, vision-based validation, and mask decoding. Extensive evaluations on four benchmark datasets (RefCOCO, RefCOCO+, RefCOCOg, and ReasonSeg) demonstrate that \\model achieves new state-of-the-art performance across all four benchmarks, highlighting its effectiveness in generating accurate and semantically grounded segmentation masks with minimal prompts."
    },
    {
      "title": "Order from Chaos: Physical World Understanding from Glitchy Gameplay Videos",
      "url": "https://arxiv.org/abs/2601.16471",
      "date": "2026-01-23",
      "authors_text": "Meng Cao, Haoran Tang, Haoze Zhao, Mingfei Han, Ruyang Liu",
      "is_highlight": false,
      "score": 80.0,
      "summary": "This paper introduces PhysGame, a dataset leveraging gameplay glitches to enhance AI's understanding of physical principles, demonstrating improved reasoning capabilities through extensive experimentation.",
      "debug_abstract": "Understanding the physical world, including object dynamics, material properties, and causal interactions, remains a core challenge in artificial intelligence. Although recent multi-modal large language models (MLLMs) have demonstrated impressive general reasoning capabilities, they still fall short of achieving human-level understanding of physical principles. Existing datasets for physical reasoning either rely on real-world videos, which incur high annotation costs, or on synthetic simulations, which suffer from limited realism and diversity. In this paper, we propose a novel paradigm that leverages glitches in gameplay videos, referring to visual anomalies that violate predefined physical laws, as a rich and scalable supervision source for physical world understanding. We introduce PhysGame, an meta information guided instruction-tuning dataset containing 140,057 glitch-centric question-answer pairs across five physical domains and sixteen fine-grained categories. To ensure data accuracy, we design a prompting strategy that utilizes gameplay metadata such as titles and descriptions to guide high-quality QA generation. Complementing PhysGame, we construct GameBench, an expert-annotated benchmark with 880 glitch-identified gameplay videos designed to evaluate physical reasoning capabilities. Extensive experiments show that PhysGame significantly enhances both Game2Real transferability, improving the real world physical reasoning performance of Qwen2.5VL by 2.5% on PhysBench, and Game2General transferability, yielding a 1.9% gain on the MVBench benchmark. Moreover, PhysGame-tuned models achieve a 3.7% absolute improvement on GameBench, demonstrating enhanced robustness in detecting physical implausibilities. These results indicate that learning from gameplay anomalies offers a scalable and effective pathway toward advancing physical world understanding in multimodal intelligence."
    },
    {
      "title": "ReViP: Reducing False Completion in Vision-Language-Action Models with Vision-Proprioception Rebalance",
      "url": "https://arxiv.org/abs/2601.16667",
      "date": "2026-01-23",
      "authors_text": "Zhuohao Li, Yinghao Li, Jian-Jian Jiang, Lang Zhou, Tianyu Zhang",
      "is_highlight": false,
      "score": 91.0,
      "summary": "ReViP enhances Vision-Language-Action models by rebalancing vision and proprioception to reduce false completions and improve robustness through task-aware visual cues.",
      "debug_abstract": "Vision-Language-Action (VLA) models have advanced robotic manipulation by combining vision, language, and proprioception to predict actions. However, previous methods fuse proprioceptive signals directly with VLM-encoded vision-language features, resulting in state-dominant bias and false completions despite visible execution failures. We attribute this to modality imbalance, where policies over-rely on internal state while underusing visual evidence. To address this, we present ReViP, a novel VLA framework with Vision-Proprioception Rebalance to enhance visual grounding and robustness under perturbations. The key insight is to introduce auxiliary task-aware environment priors to adaptively modulate the coupling between semantic perception and proprioceptive dynamics. Specifically, we use an external VLM as a task-stage observer to extract real-time task-centric visual cues from visual observations, which drive a Vision-Proprioception Feature-wise Linear Modulation to enhance environmental awareness and reduce state-driven errors. Moreover, to evaluate false completion, we propose the first False-Completion Benchmark Suite built on LIBERO with controlled settings such as Object-Drop. Extensive experiments show that ReViP effectively reduces false-completion rates and improves success rates over strong VLA baselines on our suite, with gains extending to LIBERO, RoboTwin 2.0, and real-world evaluations."
    }
  ],
  "上海人工智能实验室": [
    {
      "title": "TIDE: Trajectory-based Diagnostic Evaluation of Test-Time Improvement in LLM Agents",
      "url": "https://arxiv.org/abs/2602.02196",
      "date": "2026-02-02",
      "authors_text": "Hang Yan, Xinyu Che, Fangzhi Xu, Qiushi Sun, Zichen Ding",
      "is_highlight": false,
      "score": 20.0,
      "summary": "The TIDE framework evaluates Test-Time Improvement in LLM agents by analyzing task completion dynamics, recursive behaviors, and memory constraints to enhance performance optimization.",
      "teaser_image": "https://arxiv.org/html/2602.02196/x1.png",
      "debug_abstract": "Recent advances in autonomous LLM agents demonstrate their ability to improve performance through iterative interaction with the environment. We define this paradigm as Test-Time Improvement (TTI). However, the mechanisms under how and why TTI succeed or fail remain poorly understood, and existing evaluation metrics fail to capture their task optimization efficiency, behavior adaptation after erroneous actions, and the specific utility of working memory for task completion. To address these gaps, we propose Test-time Improvement Diagnostic Evaluation (TIDE), an agent-agnostic and environment-agnostic framework that decomposes TTI into three comprehensive and interconnected dimensions. The framework measures (1) the overall temporal dynamics of task completion and (2) identifies whether performance is primarily constrained by recursive looping behaviors or (3) by burdensome accumulated memory. Through extensive experiments across diverse agents and environments, TIDE highlights that improving agent performance requires more than scaling internal reasoning, calling for explicitly optimizing the interaction dynamics between the agent and the environment."
    },
    {
      "title": "Heterogeneous Vertiport Selection Optimization for On-Demand Air Taxi Services: A Deep Reinforcement Learning Approach",
      "url": "https://arxiv.org/abs/2601.21316",
      "date": "2026-01-29",
      "authors_text": "Aoyu Pang, Maonan Wang, Zifan Sha, Wenwei Yue, Changle Li",
      "is_highlight": false,
      "score": 80.0,
      "summary": "The paper presents a deep reinforcement learning framework for optimizing vertiport selection and air taxi routing, achieving significant travel time reductions in urban air mobility systems.",
      "teaser_image": "https://arxiv.org/html/2601.21316/x2.png",
      "debug_abstract": "Urban Air Mobility (UAM) has emerged as a transformative solution to alleviate urban congestion by utilizing low-altitude airspace, thereby reducing pressure on ground transportation networks. To enable truly efficient and seamless door-to-door travel experiences, UAM requires close integration with existing ground transportation infrastructure. However, current research on optimal integrated routing strategies for passengers in air-ground mobility systems remains limited, with a lack of systematic this http URL address this gap, we first propose a unified optimization model that integrates strategy selection for both air and ground transportation. This model captures the dynamic characteristics of multimodal transport networks and incorporates real-time traffic conditions alongside passenger decision-making behavior. Building on this model, we propose a Unified Air-Ground Mobility Coordination (UAGMC) framework, which leverages deep reinforcement learning (RL) and Vehicle-to-Everything (V2X) communication to optimize vertiport selection and dynamically plan air taxi routes. Experimental results demonstrate that UAGMC achieves a 34\\% reduction in average travel time compared to conventional proportional allocation methods, enhancing overall travel efficiency and providing novel insights into the integration and optimization of multimodal transportation systems. This work lays a solid foundation for advancing intelligent urban mobility solutions through the coordination of air and ground transportation modes. The related code can be found at this https URL."
    },
    {
      "title": "TouchGuide: Inference-Time Steering of Visuomotor Policies via Touch Guidance",
      "url": "https://arxiv.org/abs/2601.20239",
      "date": "2026-01-28",
      "authors_text": "Zhemeng Zhang, Jiahua Ma, Xincheng Yang, Xin Wen, Yuzhi Zhang",
      "is_highlight": false,
      "score": 90.0,
      "summary": "TouchGuide enhances robot manipulation by integrating tactile feedback with visual inputs to refine actions during inference, outperforming existing visuo-tactile policies in complex tasks.",
      "teaser_image": "https://arxiv.org/html/2601.20239/x2.png",
      "debug_abstract": "Fine-grained and contact-rich manipulation remain challenging for robots, largely due to the underutilization of tactile feedback. To address this, we introduce TouchGuide, a novel cross-policy visuo-tactile fusion paradigm that fuses modalities within a low-dimensional action space. Specifically, TouchGuide operates in two stages to guide a pre-trained diffusion or flow-matching visuomotor policy at inference time. First, the policy produces a coarse, visually-plausible action using only visual inputs during early sampling. Second, a task-specific Contact Physical Model (CPM) provides tactile guidance to steer and refine the action, ensuring it aligns with realistic physical contact conditions. Trained through contrastive learning on limited expert demonstrations, the CPM provides a tactile-informed feasibility score to steer the sampling process toward refined actions that satisfy physical contact constraints. Furthermore, to facilitate TouchGuide training with high-quality and cost-effective data, we introduce TacUMI, a data collection system. TacUMI achieves a favorable trade-off between precision and affordability; by leveraging rigid fingertips, it obtains direct tactile feedback, thereby enabling the collection of reliable tactile data. Extensive experiments on five challenging contact-rich tasks, such as shoe lacing and chip handover, show that TouchGuide consistently and significantly outperforms state-of-the-art visuo-tactile policies."
    },
    {
      "title": "Timely Machine: Awareness of Time Makes Test-Time Scaling Agentic",
      "url": "https://arxiv.org/abs/2601.16486",
      "date": "2026-01-23",
      "authors_text": "Yichuan Ma, Linyang Li, Yongkang chen, Peiji Li, Xiaozhe Li",
      "is_highlight": false,
      "score": 78.0,
      "summary": "The paper introduces Timely Machine, redefining test-time scaling for LLMs based on wall-clock time, enhancing performance through dynamic strategy adjustments and reinforcement learning.",
      "debug_abstract": "As large language models (LLMs) increasingly tackle complex reasoning tasks, test-time scaling has become critical for enhancing capabilities. However, in agentic scenarios with frequent tool calls, the traditional generation-length-based definition breaks down: tool latency decouples inference time from generation length. We propose Timely Machine, redefining test-time as wall-clock time, where models dynamically adjust strategies based on time budgets. We introduce Timely-Eval, a benchmark spanning high-frequency tool calls, low-frequency tool calls, and time-constrained reasoning. By varying tool latency, we find smaller models excel with fast feedback through more interactions, while larger models dominate high-latency settings via superior interaction quality. Moreover, existing models fail to adapt reasoning to time budgets. We propose Timely-RL to address this gap. After cold-start supervised fine-tuning, we use reinforcement learning to enhance temporal planning. Timely-RL improves time budget awareness and consistently boosts performance across Timely-Eval. We hope our work offers a new perspective on test-time scaling for the agentic era."
    }
  ],
  "多智能体与具身智能研究所": [
    {
      "title": "Tilt-Ropter: A Novel Hybrid Aerial and Terrestrial Vehicle with Tilt Rotors and Passive Wheels",
      "url": "https://arxiv.org/abs/2602.01700",
      "date": "2026-02-02",
      "authors_text": "Ruoyu Wang, Xuchen Liu, Zongzhou Wu, Zixuan Guo, Wendi Ding",
      "is_highlight": false,
      "score": 38.0,
      "summary": "The Tilt-Ropter is a hybrid aerial-terrestrial vehicle that utilizes tilt rotors and passive wheels for efficient locomotion, featuring advanced control systems for enhanced mobility and energy savings.",
      "teaser_image": "https://arxiv.org/html/2602.01700/x2.png",
      "debug_abstract": "In this work, we present Tilt-Ropter, a novel hybrid aerial-terrestrial vehicle (HATV) that combines tilt rotors with passive wheels to achieve energy-efficient multi-mode locomotion. Unlike existing under-actuated HATVs, the fully actuated design of Tilt-Ropter enables decoupled force and torque control, greatly enhancing its mobility and environmental adaptability. A nonlinear model predictive controller (NMPC) is developed to track reference trajectories and handle contact constraints across locomotion modes, while a dedicated control allocation module exploits actuation redundancy to achieve energy-efficient control of actuators. Additionally, to enhance robustness during ground contact, we introduce an external wrench estimation algorithm that estimates environmental interaction forces and torques in real time. The system is validated through both simulation and real-world experiments, including seamless air-ground transitions and trajectory tracking. Results show low tracking errors in both modes and highlight a 92.8% reduction in power consumption during ground locomotion, demonstrating the system&#39;s potential for long-duration missions across large-scale and energy-constrained environments."
    },
    {
      "title": "RoamScene3D: Immersive Text-to-3D Scene Generation via Adaptive Object-aware Roaming",
      "url": "https://arxiv.org/abs/2601.19433",
      "date": "2026-01-27",
      "authors_text": "Jisheng Chu, Wenrui Li, Rui Zhao, Wangmeng Zuo, Shifeng Chen",
      "is_highlight": false,
      "score": 67.0,
      "summary": "RoamScene3D introduces a novel framework for immersive text-to-3D scene generation that enhances spatial awareness and object relations, outperforming existing methods in realism and consistency.",
      "debug_abstract": "Generating immersive 3D scenes from texts is a core task in computer vision, crucial for applications in virtual reality and game development. Despite the promise of leveraging 2D diffusion priors, existing methods suffer from spatial blindness and rely on predefined trajectories that fail to exploit the inner relationships among salient objects. Consequently, these approaches are unable to comprehend the semantic layout, preventing them from exploring the scene adaptively to infer occluded content. Moreover, current inpainting models operate in 2D image space, struggling to plausibly fill holes caused by camera motion. To address these limitations, we propose RoamScene3D, a novel framework that bridges the gap between semantic guidance and spatial generation. Our method reasons about the semantic relations among objects and produces consistent and photorealistic scenes. Specifically, we employ a vision-language model (VLM) to construct a scene graph that encodes object relations, guiding the camera to perceive salient object boundaries and plan an adaptive roaming trajectory. Furthermore, to mitigate the limitations of static 2D priors, we introduce a Motion-Injected Inpainting model that is fine-tuned on a synthetic panoramic dataset integrating authentic camera trajectories, making it adaptive to camera motion. Extensive experiments demonstrate that with semantic reasoning and geometric constraints, our method significantly outperforms state-of-the-art approaches in producing consistent and photorealistic scenes. Our code is available at this https URL."
    },
    {
      "title": "Agentic reinforcement learning empowers next-generation chemical language models for molecular design and synthesis",
      "url": "https://arxiv.org/abs/2601.17687",
      "date": "2026-01-25",
      "authors_text": "Hao Li, He Cao, Shenyao Peng, Zijing Liu, Bin Feng",
      "is_highlight": false,
      "score": 62.0,
      "summary": "ChemCRAFT utilizes agentic reinforcement learning to enhance small language models for efficient, cost-effective, and privacy-preserving molecular design and synthesis, outperforming larger cloud-based models.",
      "debug_abstract": "Language models are revolutionizing the biochemistry domain, assisting scientists in drug design and chemical synthesis with high efficiency. Yet current approaches struggle between small language models prone to hallucination and limited knowledge retention, and large cloud-based language models plagued by privacy risks and high inference costs. To bridge this gap, we introduce ChemCRAFT, a novel framework leveraging agentic reinforcement learning to decouple chemical reasoning from knowledge storage. Instead of forcing the model to memorize vast chemical data, our approach empowers the language model to interact with a sandbox for precise information retrieval. This externalization of knowledge allows a locally deployable small model to achieve superior performance with minimal inference costs. To enable small language models for agent-calling ability, we build an agentic trajectory construction pipeline and a comprehensive chemical-agent sandbox. Based on sandbox interactions, we constructed ChemToolDataset, the first large-scale chemical tool trajectory dataset. Simultaneously, we propose SMILES-GRPO to build a dense chemical reward function, promoting the model&#39;s ability to call chemical agents. Evaluations across diverse aspects of drug design show that ChemCRAFT outperforms current cloud-based LLMs in molecular structure analysis, molecular optimization, and synthesis pathway prediction, demonstrating that scientific reasoning is not solely an emergent ability of model scale, but a learnable policy of tool orchestration. This work establishes a cost-effective and privacy-preserving paradigm for AI-aided chemistry, opening new avenues for accelerating molecular discovery with locally deployable agents."
    },
    {
      "title": "ReViP: Reducing False Completion in Vision-Language-Action Models with Vision-Proprioception Rebalance",
      "url": "https://arxiv.org/abs/2601.16667",
      "date": "2026-01-23",
      "authors_text": "Zhuohao Li, Yinghao Li, Jian-Jian Jiang, Lang Zhou, Tianyu Zhang",
      "is_highlight": false,
      "score": 91.0,
      "summary": "ReViP enhances Vision-Language-Action models by rebalancing vision and proprioception to reduce false completions and improve robustness through task-aware visual cues.",
      "debug_abstract": "Vision-Language-Action (VLA) models have advanced robotic manipulation by combining vision, language, and proprioception to predict actions. However, previous methods fuse proprioceptive signals directly with VLM-encoded vision-language features, resulting in state-dominant bias and false completions despite visible execution failures. We attribute this to modality imbalance, where policies over-rely on internal state while underusing visual evidence. To address this, we present ReViP, a novel VLA framework with Vision-Proprioception Rebalance to enhance visual grounding and robustness under perturbations. The key insight is to introduce auxiliary task-aware environment priors to adaptively modulate the coupling between semantic perception and proprioceptive dynamics. Specifically, we use an external VLM as a task-stage observer to extract real-time task-centric visual cues from visual observations, which drive a Vision-Proprioception Feature-wise Linear Modulation to enhance environmental awareness and reduce state-driven errors. Moreover, to evaluate false completion, we propose the first False-Completion Benchmark Suite built on LIBERO with controlled settings such as Object-Drop. Extensive experiments show that ReViP effectively reduces false-completion rates and improves success rates over strong VLA baselines on our suite, with gains extending to LIBERO, RoboTwin 2.0, and real-world evaluations."
    }
  ],
  "西湖机器人科技 / 机器智能实验室 (MiLAB)": [
    {
      "title": "Neural Predictor-Corrector: Solving Homotopy Problems with Reinforcement Learning",
      "url": "https://arxiv.org/abs/2602.03086",
      "date": "2026-02-03",
      "authors_text": "Jiayao Mai, Bangyan Liao, Zhenjun Zhao, Yingping Zeng, Haoang Li",
      "is_highlight": false,
      "score": 64.0,
      "summary": "The Neural Predictor-Corrector (NPC) framework uses reinforcement learning to automate step size and termination decisions in solving diverse homotopy problems, outperforming traditional methods.",
      "teaser_image": "https://arxiv.org/html/2602.03086/x2.png",
      "debug_abstract": "The Homotopy paradigm, a general principle for solving challenging problems, appears across diverse domains such as robust optimization, global optimization, polynomial root-finding, and sampling. Practical solvers for these problems typically follow a predictor-corrector (PC) structure, but rely on hand-crafted heuristics for step sizes and iteration termination, which are often suboptimal and task-specific. To address this, we unify these problems under a single framework, which enables the design of a general neural solver. Building on this unified view, we propose Neural Predictor-Corrector (NPC), which replaces hand-crafted heuristics with automatically learned policies. NPC formulates policy selection as a sequential decision-making problem and leverages reinforcement learning to automatically discover efficient strategies. To further enhance generalization, we introduce an amortized training mechanism, enabling one-time offline training for a class of problems and efficient online inference on new instances. Experiments on four representative homotopy problems demonstrate that our method generalizes effectively to unseen instances. It consistently outperforms classical and specialized baselines in efficiency while demonstrating superior stability across tasks, highlighting the value of unifying homotopy methods into a single neural framework."
    },
    {
      "title": "CMR: Contractive Mapping Embeddings for Robust Humanoid Locomotion on Unstructured Terrains",
      "url": "https://arxiv.org/abs/2602.03511",
      "date": "2026-02-03",
      "authors_text": "Qixin Zeng, Hongyin Zhang, Shangke Lyu, Junxi Jin, Donglin Wang",
      "is_highlight": true,
      "score": 87.0,
      "summary": "The CMR framework enhances humanoid locomotion robustness on unstructured terrains by mapping noisy observations to a stable latent space, improving performance against disturbances.",
      "teaser_image": "https://arxiv.org/html/2602.03511/figures/CMR_Framwork.png",
      "debug_abstract": "Robust disturbance rejection remains a longstanding challenge in humanoid locomotion, particularly on unstructured terrains where sensing is unreliable and model mismatch is pronounced. While perception information, such as height map, enhances terrain awareness, sensor noise and sim-to-real gaps can destabilize policies in practice. In this work, we provide theoretical analysis that bounds the return gap under observation noise, when the induced latent dynamics are contractive. Furthermore, we present Contractive Mapping for Robustness (CMR) framework that maps high-dimensional, disturbance-prone observations into a latent space, where local perturbations are attenuated over time. Specifically, this approach couples contrastive representation learning with Lipschitz regularization to preserve task-relevant geometry while explicitly controlling sensitivity. Notably, the formulation can be incorporated into modern deep reinforcement learning pipelines as an auxiliary loss term with minimal additional technical effort required. Further, our extensive humanoid experiments show that CMR potently outperforms other locomotion algorithms under increased noise."
    },
    {
      "title": "RegionReasoner: Region-Grounded Multi-Round Visual Reasoning",
      "url": "https://arxiv.org/abs/2602.03733",
      "date": "2026-02-03",
      "authors_text": "Wenfang Sun, Hao Chen, Yingjun Du, Yefeng Zheng, Cees G. M. Snoek",
      "is_highlight": false,
      "score": 49.0,
      "summary": "RegionReasoner introduces a multi-round visual reasoning framework that enhances accuracy and consistency in detection and segmentation tasks through grounded, iterative reasoning and structured rewards.",
      "teaser_image": "https://arxiv.org/html/2602.03733/image/appendix_test.png",
      "debug_abstract": "Large vision-language models have achieved remarkable progress in visual reasoning, yet most existing systems rely on single-step or text-only reasoning, limiting their ability to iteratively refine understanding across multiple visual contexts. To address this limitation, we introduce a new multi-round visual reasoning benchmark with training and test sets spanning both detection and segmentation tasks, enabling systematic evaluation under iterative reasoning scenarios. We further propose RegionReasoner, a reinforcement learning framework that enforces grounded reasoning by requiring each reasoning trace to explicitly cite the corresponding reference bounding boxes, while maintaining semantic coherence via a global-local consistency reward. This reward extracts key objects and nouns from both global scene captions and region-level captions, aligning them with the reasoning trace to ensure consistency across reasoning steps. RegionReasoner is optimized with structured rewards combining grounding fidelity and global-local semantic alignment. Experiments on detection and segmentation tasks show that RegionReasoner-7B, together with our newly introduced benchmark RegionDial-Bench, considerably improves multi-round reasoning accuracy, spatial grounding precision, and global-local consistency, establishing a strong baseline for this emerging research direction."
    },
    {
      "title": "ReWeaver: Towards Simulation-Ready and Topology-Accurate Garment Reconstruction",
      "url": "https://arxiv.org/abs/2601.16672",
      "date": "2026-01-23",
      "authors_text": "Ming Li, Hui Shan, Kai Zheng, Chentao Shen, Siyu Liu",
      "is_highlight": false,
      "score": 76.0,
      "summary": "ReWeaver is a novel framework for accurate 3D garment reconstruction from sparse images, enhancing topology fidelity for simulations and robotic applications.",
      "debug_abstract": "High-quality 3D garment reconstruction plays a crucial role in mitigating the sim-to-real gap in applications such as digital avatars, virtual try-on and robotic manipulation. However, existing garment reconstruction methods typically rely on unstructured representations, such as 3D Gaussian Splats, struggling to provide accurate reconstructions of garment topology and sewing structures. As a result, the reconstructed outputs are often unsuitable for high-fidelity physical simulation. We propose ReWeaver, a novel framework for topology-accurate 3D garment and sewing pattern reconstruction from sparse multi-view RGB images. Given as few as four input views, ReWeaver predicts seams and panels as well as their connectivities in both the 2D UV space and the 3D space. The predicted seams and panels align precisely with the multi-view images, yielding structured 2D--3D garment representations suitable for 3D perception, high-fidelity physical simulation, and robotic manipulation. To enable effective training, we construct a large-scale dataset GCD-TS, comprising multi-view RGB images, 3D garment geometries, textured human body meshes and annotated sewing patterns. The dataset contains over 100,000 synthetic samples covering a wide range of complex geometries and topologies. Extensive experiments show that ReWeaver consistently outperforms existing methods in terms of topology accuracy, geometry alignment and seam-panel consistency."
    }
  ],
  "西安电子科技大学": [
    {
      "title": "Nüwa: Mending the Spatial Integrity Torn by VLM Token Pruning",
      "url": "https://arxiv.org/abs/2602.02951",
      "date": "2026-02-03",
      "authors_text": "Yihong Huang, Fei Ma, Yihua Shao, Jingcai Guo, Zitong Yu",
      "is_highlight": false,
      "score": 69.0,
      "summary": "Nüwa introduces a two-stage token pruning framework that enhances spatial integrity and significantly improves performance in visual grounding and question answering tasks in Vision Language Models.",
      "teaser_image": "https://arxiv.org/html/2602.02951/x2.png",
      "debug_abstract": "Vision token pruning has proven to be an effective acceleration technique for the efficient Vision Language Model (VLM). However, existing pruning methods demonstrate excellent performance preservation in visual question answering (VQA) and suffer substantial degradation on visual grounding (VG) tasks. Our analysis of the VLM&#39;s processing pipeline reveals that strategies utilizing global semantic similarity and attention scores lose the global spatial reference frame, which is derived from the interactions of tokens&#39; positional information. Motivated by these findings, we propose $\\text{Nüwa}$, a two-stage token pruning framework that enables efficient feature aggregation while maintaining spatial integrity. In the first stage, after the vision encoder, we apply three operations, namely separation, alignment, and aggregation, which are inspired by swarm intelligence algorithms to retain information-rich global spatial anchors. In the second stage, within the LLM, we perform text-guided pruning to retain task-relevant visual tokens. Extensive experiments demonstrate that $\\text{Nüwa}$ achieves SOTA performance on multiple VQA benchmarks (from 94% to 95%) and yields substantial improvements on visual grounding tasks (from 7% to 47%)."
    },
    {
      "title": "EventFlash: Towards Efficient MLLMs for Event-Based Vision",
      "url": "https://arxiv.org/abs/2602.03230",
      "date": "2026-02-03",
      "authors_text": "Shaoyu Liu, Jianing Li, Guanghui Zhao, Yunjian Zhang, Wen Jiang",
      "is_highlight": false,
      "score": 63.0,
      "summary": "EventFlash introduces an efficient MLLM for event-based vision, leveraging spatiotemporal sparsification and adaptive sampling to enhance performance and reduce computational costs.",
      "teaser_image": "https://arxiv.org/html/2602.03230/x2.png",
      "debug_abstract": "Event-based multimodal large language models (MLLMs) enable robust perception in high-speed and low-light scenarios, addressing key limitations of frame-based MLLMs. However, current event-based MLLMs often rely on dense image-like processing paradigms, overlooking the spatiotemporal sparsity of event streams and resulting in high computational cost. In this paper, we propose EventFlash, a novel and efficient MLLM to explore spatiotemporal token sparsification for reducing data redundancy and accelerating inference. Technically, we build EventMind, a large-scale and scene-diverse dataset with over 500k instruction sets, providing both short and long event stream sequences to support our curriculum training strategy. We then present an adaptive temporal window aggregation module for efficient temporal sampling, which adaptively compresses temporal tokens while retaining key temporal cues. Finally, a sparse density-guided attention module is designed to improve spatial token efficiency by selecting informative regions and suppressing empty or sparse areas. Experimental results show that EventFlash achieves a $12.4\\times$ throughput improvement over the baseline (EventFlash-Zero) while maintaining comparable performance. It supports long-range event stream processing with up to 1,000 bins, significantly outperforming the 5-bin limit of EventGPT. We believe EventFlash serves as an efficient foundation model for event-based vision."
    },
    {
      "title": "AdaptMMBench: Benchmarking Adaptive Multimodal Reasoning for Mode Selection and Reasoning Process",
      "url": "https://arxiv.org/abs/2602.02676",
      "date": "2026-02-02",
      "authors_text": "Xintong Zhang, Xiaowen Zhang, Jongrong Wu, Zhi Gao, Shilin Yan",
      "is_highlight": false,
      "score": 45.0,
      "summary": "AdaptMMBench introduces a dynamic benchmark for adaptive multimodal reasoning, evaluating mode selection and reasoning processes across diverse tasks while revealing inconsistencies in tool effectiveness and performance alignment.",
      "teaser_image": "https://arxiv.org/html/2602.02676/x3.png",
      "debug_abstract": "Adaptive multimodal reasoning has emerged as a promising frontier in Vision-Language Models (VLMs), aiming to dynamically modulate between tool-augmented visual reasoning and text reasoning to enhance both effectiveness and efficiency. However, existing evaluations rely on static difficulty labels and simplistic metrics, which fail to capture the dynamic nature of difficulty relative to varying model capacities. Consequently, they obscure the distinction between adaptive mode selection and general performance while neglecting fine-grained process analyses. In this paper, we propose AdaptMMBench, a comprehensive benchmark for adaptive multimodal reasoning across five domains: real-world, OCR, GUI, knowledge, and math, encompassing both direct perception and complex reasoning tasks. AdaptMMBench utilizes a Matthews Correlation Coefficient (MCC) metric to evaluate the selection rationality of different reasoning modes, isolating this meta-cognition ability by dynamically identifying task difficulties based on models&#39; capability boundaries. Moreover, AdaptMMBench facilitates multi-dimensional process evaluation across key step coverage, tool effectiveness, and computational efficiency. Our evaluation reveals that while adaptive mode selection scales with model capacity, it notably decouples from final accuracy. Conversely, key step coverage aligns with performance, though tool effectiveness remains highly inconsistent across model architectures."
    },
    {
      "title": "Resource-Efficient Reinforcement for Reasoning Large Language Models via Dynamic One-Shot Policy Refinement",
      "url": "https://arxiv.org/abs/2602.00815",
      "date": "2026-01-31",
      "authors_text": "Yunjian Zhang, Sudong Wang, Yang Li, Peiran Xu, Conghao Zhou",
      "is_highlight": false,
      "score": 57.0,
      "summary": "The paper introduces Dynamic One-Shot Policy Refinement (DoPR), a resource-efficient reinforcement learning strategy that enhances reasoning in large language models while significantly reducing training costs.",
      "teaser_image": "https://arxiv.org/html/2602.00815/dopr.png",
      "debug_abstract": "Large language models (LLMs) have exhibited remarkable performance on complex reasoning tasks, with reinforcement learning under verifiable rewards (RLVR) emerging as a principled framework for aligning model behavior with reasoning chains. Despite its promise, RLVR remains prohibitively resource-intensive, requiring extensive reward signals and incurring substantial rollout costs during training. In this work, we revisit the fundamental question of data and compute efficiency in RLVR. We first establish a theoretical lower bound on the sample complexity required to unlock reasoning capabilities, and empirically validate that strong performance can be achieved with a surprisingly small number of training instances. To tackle the computational burden, we propose Dynamic One-Shot Policy Refinement (DoPR), an uncertainty-aware RL strategy that dynamically selects a single informative training sample per batch for policy updates, guided by reward volatility and exploration-driven acquisition. DoPR reduces rollout overhead by nearly an order of magnitude while preserving competitive reasoning accuracy, offering a scalable and resource-efficient solution for LLM post-training. This approach offers a practical path toward more efficient and accessible RL-based training for reasoning-intensive LLM applications."
    },
    {
      "title": "SPIRIT: Adapting Vision Foundation Models for Unified Single- and Multi-Frame Infrared Small Target Detection",
      "url": "https://arxiv.org/abs/2602.01843",
      "date": "2026-02-02",
      "authors_text": "Qian Xu, Xi Li, Fei Gao, Jie Guo, Haojuan Yuan",
      "is_highlight": false,
      "score": 33.0,
      "summary": "SPIRIT is a unified framework that enhances infrared small target detection by adapting vision foundation models with physics-informed plug-ins for improved single- and multi-frame analysis.",
      "teaser_image": "https://arxiv.org/html/2602.01843/x2.png",
      "debug_abstract": "Infrared small target detection (IRSTD) is crucial for surveillance and early-warning, with deployments spanning both single-frame analysis and video-mode tracking. A practical solution should leverage vision foundation models (VFMs) to mitigate infrared data scarcity, while adopting a memory-attention-based temporal propagation framework that unifies single- and multi-frame inference. However, infrared small targets exhibit weak radiometric signals and limited semantic cues, which differ markedly from visible-spectrum imagery. This modality gap makes direct use of semantics-oriented VFMs and appearance-driven cross-frame association unreliable for IRSTD: hierarchical feature aggregation can submerge localized target peaks, and appearance-only memory attention becomes ambiguous, leading to spurious clutter associations. To address these challenges, we propose SPIRIT, a unified and VFM-compatible framework that adapts VFMs to IRSTD via lightweight physics-informed plug-ins. Spatially, PIFR refines features by approximating rank-sparsity decomposition to suppress structured background components and enhance sparse target-like signals. Temporally, PGMA injects history-derived soft spatial priors into memory cross-attention to constrain cross-frame association, enabling robust video detection while naturally reverting to single-frame inference when temporal context is absent. Experiments on multiple IRSTD benchmarks show consistent gains over VFM-based baselines and SOTA performance."
    },
    {
      "title": "Heterogeneous Vertiport Selection Optimization for On-Demand Air Taxi Services: A Deep Reinforcement Learning Approach",
      "url": "https://arxiv.org/abs/2601.21316",
      "date": "2026-01-29",
      "authors_text": "Aoyu Pang, Maonan Wang, Zifan Sha, Wenwei Yue, Changle Li",
      "is_highlight": false,
      "score": 80.0,
      "summary": "The paper presents a deep reinforcement learning framework for optimizing vertiport selection and air taxi routing, achieving significant travel time reductions in urban air mobility systems.",
      "teaser_image": "https://arxiv.org/html/2601.21316/x2.png",
      "debug_abstract": "Urban Air Mobility (UAM) has emerged as a transformative solution to alleviate urban congestion by utilizing low-altitude airspace, thereby reducing pressure on ground transportation networks. To enable truly efficient and seamless door-to-door travel experiences, UAM requires close integration with existing ground transportation infrastructure. However, current research on optimal integrated routing strategies for passengers in air-ground mobility systems remains limited, with a lack of systematic this http URL address this gap, we first propose a unified optimization model that integrates strategy selection for both air and ground transportation. This model captures the dynamic characteristics of multimodal transport networks and incorporates real-time traffic conditions alongside passenger decision-making behavior. Building on this model, we propose a Unified Air-Ground Mobility Coordination (UAGMC) framework, which leverages deep reinforcement learning (RL) and Vehicle-to-Everything (V2X) communication to optimize vertiport selection and dynamically plan air taxi routes. Experimental results demonstrate that UAGMC achieves a 34\\% reduction in average travel time compared to conventional proportional allocation methods, enhancing overall travel efficiency and providing novel insights into the integration and optimization of multimodal transportation systems. This work lays a solid foundation for advancing intelligent urban mobility solutions through the coordination of air and ground transportation modes. The related code can be found at this https URL."
    },
    {
      "title": "Towards Bridging the Gap between Large-Scale Pretraining and Efficient Finetuning for Humanoid Control",
      "url": "https://arxiv.org/abs/2601.21363",
      "date": "2026-01-29",
      "authors_text": "Weidong Huang, Zhehan Li, Hangxin Liu, Biao Hou, Yao Su",
      "is_highlight": false,
      "score": 85.0,
      "summary": "This paper presents a method combining off-policy Soft Actor-Critic for efficient pretraining and model-based techniques for safe finetuning of humanoid locomotion policies.",
      "teaser_image": "https://arxiv.org/html/2601.21363/x1.png",
      "debug_abstract": "Reinforcement learning (RL) is widely used for humanoid control, with on-policy methods such as Proximal Policy Optimization (PPO) enabling robust training via large-scale parallel simulation and, in some cases, zero-shot deployment to real robots. However, the low sample efficiency of on-policy algorithms limits safe adaptation to new environments. Although off-policy RL and model-based RL have shown improved sample efficiency, the gap between large-scale pretraining and efficient finetuning on humanoids still exists. In this paper, we find that off-policy Soft Actor-Critic (SAC), with large-batch update and a high Update-To-Data (UTD) ratio, reliably supports large-scale pretraining of humanoid locomotion policies, achieving zero-shot deployment on real robots. For adaptation, we demonstrate that these SAC-pretrained policies can be finetuned in new environments and out-of-distribution tasks using model-based methods. Data collection in the new environment executes a deterministic policy while stochastic exploration is instead confined to a physics-informed world model. This separation mitigates the risks of random exploration during adaptation while preserving exploratory coverage for improvement. Overall, the approach couples the wall-clock efficiency of large-scale simulation during pretraining with the sample efficiency of model-based learning during fine-tuning."
    },
    {
      "title": "ReWeaver: Towards Simulation-Ready and Topology-Accurate Garment Reconstruction",
      "url": "https://arxiv.org/abs/2601.16672",
      "date": "2026-01-23",
      "authors_text": "Ming Li, Hui Shan, Kai Zheng, Chentao Shen, Siyu Liu",
      "is_highlight": false,
      "score": 76.0,
      "summary": "ReWeaver is a novel framework for accurate 3D garment reconstruction from sparse images, enhancing topology fidelity for simulations and robotic applications.",
      "debug_abstract": "High-quality 3D garment reconstruction plays a crucial role in mitigating the sim-to-real gap in applications such as digital avatars, virtual try-on and robotic manipulation. However, existing garment reconstruction methods typically rely on unstructured representations, such as 3D Gaussian Splats, struggling to provide accurate reconstructions of garment topology and sewing structures. As a result, the reconstructed outputs are often unsuitable for high-fidelity physical simulation. We propose ReWeaver, a novel framework for topology-accurate 3D garment and sewing pattern reconstruction from sparse multi-view RGB images. Given as few as four input views, ReWeaver predicts seams and panels as well as their connectivities in both the 2D UV space and the 3D space. The predicted seams and panels align precisely with the multi-view images, yielding structured 2D--3D garment representations suitable for 3D perception, high-fidelity physical simulation, and robotic manipulation. To enable effective training, we construct a large-scale dataset GCD-TS, comprising multi-view RGB images, 3D garment geometries, textured human body meshes and annotated sewing patterns. The dataset contains over 100,000 synthetic samples covering a wide range of complex geometries and topologies. Extensive experiments show that ReWeaver consistently outperforms existing methods in terms of topology accuracy, geometry alignment and seam-panel consistency."
    }
  ],
  "深圳大学": [
    {
      "title": "Nüwa: Mending the Spatial Integrity Torn by VLM Token Pruning",
      "url": "https://arxiv.org/abs/2602.02951",
      "date": "2026-02-03",
      "authors_text": "Yihong Huang, Fei Ma, Yihua Shao, Jingcai Guo, Zitong Yu",
      "is_highlight": false,
      "score": 69.0,
      "summary": "Nüwa introduces a two-stage token pruning framework that enhances spatial integrity and significantly improves performance in visual grounding and question answering tasks in Vision Language Models.",
      "teaser_image": "https://arxiv.org/html/2602.02951/x2.png",
      "debug_abstract": "Vision token pruning has proven to be an effective acceleration technique for the efficient Vision Language Model (VLM). However, existing pruning methods demonstrate excellent performance preservation in visual question answering (VQA) and suffer substantial degradation on visual grounding (VG) tasks. Our analysis of the VLM&#39;s processing pipeline reveals that strategies utilizing global semantic similarity and attention scores lose the global spatial reference frame, which is derived from the interactions of tokens&#39; positional information. Motivated by these findings, we propose $\\text{Nüwa}$, a two-stage token pruning framework that enables efficient feature aggregation while maintaining spatial integrity. In the first stage, after the vision encoder, we apply three operations, namely separation, alignment, and aggregation, which are inspired by swarm intelligence algorithms to retain information-rich global spatial anchors. In the second stage, within the LLM, we perform text-guided pruning to retain task-relevant visual tokens. Extensive experiments demonstrate that $\\text{Nüwa}$ achieves SOTA performance on multiple VQA benchmarks (from 94% to 95%) and yields substantial improvements on visual grounding tasks (from 7% to 47%)."
    },
    {
      "title": "T2M Mamba: Motion Periodicity-Saliency Coupling Approach for Stable Text-Driven Motion Generation",
      "url": "https://arxiv.org/abs/2602.01352",
      "date": "2026-02-01",
      "authors_text": "Xingzu Zhan, Chen Xie, Honghang Chen, Yixun Lin, Xiaochun Mai",
      "is_highlight": false,
      "score": 63.0,
      "summary": "T2M Mamba enhances text-to-motion generation by coupling motion periodicity and keyframe saliency, improving stability and robustness against paraphrasing in 3D human motion sequences.",
      "teaser_image": "https://arxiv.org/html/2602.01352/T2MMamba.png",
      "debug_abstract": "Text-to-motion generation, which converts motion language descriptions into coherent 3D human motion sequences, has attracted increasing attention in fields, such as avatar animation and humanoid robotic interaction. Though existing models have achieved significant fidelity, they still suffer from two core limitations: (i) They treat motion periodicity and keyframe saliency as independent factors, overlooking their coupling and causing generation drift in long sequences. (ii) They are fragile to semantically equivalent paraphrases, where minor synonym substitutions distort textual embeddings, propagating through the decoder and producing unstable or erroneous motions. In this work, we propose T2M Mamba to address these limitations by (i) proposing Periodicity-Saliency Aware Mamba, which utilizes novel algorithms for keyframe weight estimation via enhanced Density Peaks Clustering and motion periodicity estimation via FFT-accelerated autocorrelation to capture coupled dynamics with minimal computational overhead, and (ii) constructing a Periodic Differential Cross-modal Alignment Module (PDCAM) to enhance robust alignment of textual and motion embeddings. Extensive experiments on HumanML3D and KIT-ML datasets have been conducted, confirming the effectiveness of our approach, achieving an FID of 0.068 and consistent gains on all other metrics."
    },
    {
      "title": "Diffusion Model-based Reinforcement Learning for Version Age of Information Scheduling: Average and Tail-Risk-Sensitive Control",
      "url": "https://arxiv.org/abs/2601.18069",
      "date": "2026-01-26",
      "authors_text": "Haoyuan Pan, Sizhao Chen, Zhaorui Wang, Tse-Tin Chan",
      "is_highlight": false,
      "score": 57.0,
      "summary": "This paper presents a novel reinforcement learning approach, RS-D3SAC, for optimizing Version Age of Information scheduling by minimizing both average and tail-risk-sensitive metrics in wireless systems.",
      "teaser_image": null,
      "debug_abstract": "Ensuring timely and semantically accurate information delivery is critical in real-time wireless systems. While Age of Information (AoI) quantifies temporal freshness, Version Age of Information (VAoI) captures semantic staleness by accounting for version evolution between transmitters and receivers. Existing VAoI scheduling approaches primarily focus on minimizing average VAoI, overlooking rare but severe staleness events that can compromise reliability under stochastic packet arrivals and unreliable channels. This paper investigates both average-oriented and tail-risk-sensitive VAoI scheduling in a multi-user status update system with long-term transmission cost constraints. We first formulate the average VAoI minimization problem as a constrained Markov decision process and introduce a deep diffusion-based Soft Actor-Critic (D2SAC) algorithm. By generating actions through a diffusion-based denoising process, D2SAC enhances policy expressiveness and establishes a strong baseline for mean performance. Building on this foundation, we put forth RS-D3SAC, a risk-sensitive deep distributional diffusion-based Soft Actor-Critic algorithm. RS-D3SAC integrates a diffusion-based actor with a quantile-based distributional critic, explicitly modeling the full VAoI return distribution. This enables principled tail-risk optimization via Conditional Value-at-Risk (CVaR) while satisfying long-term transmission cost constraints. Extensive simulations show that, while D2SAC reduces average VAoI, RS-D3SAC consistently achieves substantial reductions in CVaR without sacrificing mean performance. The dominant gain in tail-risk reduction stems from the distributional critic, with the diffusion-based actor providing complementary refinement to stabilize and enrich policy decisions, highlighting their effectiveness for robust and risk-aware VAoI scheduling in multi-user wireless systems."
    },
    {
      "title": "YOLO-DS: Fine-Grained Feature Decoupling via Dual-Statistic Synergy Operator for Object Detection",
      "url": "https://arxiv.org/abs/2601.18172",
      "date": "2026-01-26",
      "authors_text": "Lin Huang, Yujuan Tan, Weisheng Li, Shitai Shan, Liu Liu",
      "is_highlight": false,
      "score": 47.0,
      "summary": "YOLO-DS enhances object detection by introducing a Dual-Statistic Synergy Operator for improved feature decoupling, outperforming YOLOv8 with minimal latency increase.",
      "debug_abstract": "One-stage object detection, particularly the YOLO series, strikes a favorable balance between accuracy and efficiency. However, existing YOLO detectors lack explicit modeling of heterogeneous object responses within shared feature channels, which limits further performance gains. To address this, we propose YOLO-DS, a framework built around a novel Dual-Statistic Synergy Operator (DSO). The DSO decouples object features by jointly modeling the channel-wise mean and the peak-to-mean difference. Building upon the DSO, we design two lightweight gating modules: the Dual-Statistic Synergy Gating (DSG) module for adaptive channel-wise feature selection, and the Multi-Path Segmented Gating (MSG) module for depth-wise feature weighting. On the MS-COCO benchmark, YOLO-DS consistently outperforms YOLOv8 across five model scales (N, S, M, L, X), achieving AP gains of 1.1% to 1.7% with only a minimal increase in inference latency. Extensive visualization, ablation, and comparative studies validate the effectiveness of our approach, demonstrating its superior capability in discriminating heterogeneous objects with high efficiency."
    },
    {
      "title": "Emotion-LLaMAv2 and MMEVerse: A New Framework and Benchmark for Multimodal Emotion Understanding",
      "url": "https://arxiv.org/abs/2601.16449",
      "date": "2026-01-23",
      "authors_text": "Xiaojiang Peng, Jingyi Chen, Zebang Cheng, Bao Peng, Fengyi Wu",
      "is_highlight": false,
      "score": 58.0,
      "summary": "Emotion-LLaMAv2 and MMEVerse introduce a comprehensive framework and benchmark for enhanced multimodal emotion understanding through advanced encoding, feature interaction, and large-scale dataset integration.",
      "debug_abstract": "Understanding human emotions from multimodal signals poses a significant challenge in affective computing and human-robot interaction. While multimodal large language models (MLLMs) have excelled in general vision-language tasks, their capabilities in emotional reasoning remain limited. The field currently suffers from a scarcity of large-scale datasets with high-quality, descriptive emotion annotations and lacks standardized benchmarks for evaluation. Our preliminary framework, Emotion-LLaMA, pioneered instruction-tuned multimodal learning for emotion reasoning but was restricted by explicit face detectors, implicit fusion strategies, and low-quality training data with limited scale. To address these limitations, we present Emotion-LLaMAv2 and the MMEVerse benchmark, establishing an end-to-end pipeline together with a standardized evaluation setting for emotion recognition and reasoning. Emotion-LLaMAv2 introduces three key advances. First, an end-to-end multiview encoder eliminates external face detection and captures nuanced emotional cues via richer spatial and temporal multiview tokens. Second, a Conv Attention pre-fusion module is designed to enable simultaneous local and global multimodal feature interactions external to the LLM backbone. Third, a perception-to-cognition curriculum instruction tuning scheme within the LLaMA2 backbone unifies emotion recognition and free-form emotion reasoning. To support large-scale training and reproducible evaluation, MMEVerse aggregates twelve publicly available emotion datasets, including IEMOCAP, MELD, DFEW, and MAFW, into a unified multimodal instruction format. The data are re-annotated via a multi-agent pipeline involving Qwen2 Audio, Qwen2.5 VL, and GPT 4o, producing 130k training clips and 36k testing clips across 18 evaluation benchmarks."
    }
  ],
  "机器人技术与系统国家重点实验室": [
    {
      "title": "Inject Once Survive Later: Backdooring Vision-Language-Action Models to Persist Through Downstream Fine-tuning",
      "url": "https://arxiv.org/abs/2602.00500",
      "date": "2026-01-31",
      "authors_text": "Jianyi Zhou, Yujie Wei, Ruichen Zhen, Bo Zhao, Xiaobo Xia",
      "is_highlight": true,
      "score": 60.0,
      "summary": "The paper introduces INFUSE, a robust backdoor attack framework for Vision-Language-Action models that survives user fine-tuning, maintaining high attack success rates while preserving performance.",
      "teaser_image": "https://arxiv.org/html/2602.00500/x2.png",
      "debug_abstract": "Vision-Language-Action (VLA) models have become foundational to modern embodied AI systems. By integrating visual perception, language understanding, and action planning, they enable general-purpose task execution across diverse environments. Despite their importance, the security of VLA models remains underexplored -- particularly in the context of backdoor attacks, which pose realistic threats in physical-world deployments. While recent methods attempt to inject backdoors into VLA models, these backdoors are easily erased during downstream adaptation, as user-side fine-tuning with clean data significantly alters model parameters, rendering them impractical for real-world applications. To address these challenges, we propose INFUSE (INjection into Fine-tUne-inSensitive modulEs), the first backdoor attack framework for VLA base models that remains effective even with arbitrary user fine-tuning. INFUSE begins by analyzing parameter sensitivity across diverse fine-tuning scenarios to identify modules that remain largely unchanged -- the fine-tune-insensitive modules. It then injects backdoors into these stable modules while freezing the rest, ensuring malicious behavior persists after extensive user fine-tuning. Comprehensive experiments across multiple VLA architectures demonstrate INFUSE&#39;s effectiveness. After user-side fine-tuning, INFUSE maintains mean attack success rates of 91.0% on simulation environments and 79.8% on real-world robot tasks, substantially surpassing BadVLA (38.8% and 36.6%, respectively), while preserving clean-task performance comparable to standard models. These results uncover a critical threat: backdoors implanted before distribution can persist through fine-tuning and remain effective at deployment."
    },
    {
      "title": "APEX: A Decoupled Memory-based Explorer for Asynchronous Aerial Object Goal Navigation",
      "url": "https://arxiv.org/abs/2602.00551",
      "date": "2026-01-31",
      "authors_text": "Daoxuan Zhang, Ping Chen, Xiaobo Xia, Xiu Su, Ruichen Zhen",
      "is_highlight": false,
      "score": 88.0,
      "summary": "APEX is a novel hierarchical agent that enhances UAV navigation through efficient exploration and target acquisition using dynamic memory, reinforcement learning, and open-vocabulary detection.",
      "teaser_image": "https://arxiv.org/html/2602.00551/x1.png",
      "debug_abstract": "Aerial Object Goal Navigation, a challenging frontier in Embodied AI, requires an Unmanned Aerial Vehicle (UAV) agent to autonomously explore, reason, and identify a specific target using only visual perception and language description. However, existing methods struggle with the memorization of complex spatial representations in aerial environments, reliable and interpretable action decision-making, and inefficient exploration and information gathering. To address these challenges, we introduce \\textbf{APEX} (Aerial Parallel Explorer), a novel hierarchical agent designed for efficient exploration and target acquisition in complex aerial settings. APEX is built upon a modular, three-part architecture: 1) Dynamic Spatio-Semantic Mapping Memory, which leverages the zero-shot capability of a Vision-Language Model (VLM) to dynamically construct high-resolution 3D Attraction, Exploration, and Obstacle maps, serving as an interpretable memory mechanism. 2) Action Decision Module, trained with reinforcement learning, which translates this rich spatial understanding into a fine-grained and robust control policy. 3) Target Grounding Module, which employs an open-vocabulary detector to achieve definitive and generalizable target identification. All these components are integrated into a hierarchical, asynchronous, and parallel framework, effectively bypassing the VLM&#39;s inference latency and boosting the agent&#39;s proactivity in exploration. Extensive experiments show that APEX outperforms the previous state of the art by +4.2\\% SR and +2.8\\% SPL on challenging UAV-ON benchmarks, demonstrating its superior efficiency and the effectiveness of its hierarchical asynchronous design. Our source code is provided in \\href{this https URL}{GitHub}"
    },
    {
      "title": "ConLA: Contrastive Latent Action Learning from Human Videos for Robotic Manipulation",
      "url": "https://arxiv.org/abs/2602.00557",
      "date": "2026-01-31",
      "authors_text": "Weisheng Dai, Kai Lan, Jianyi Zhou, Bo Zhao, Xiu Su",
      "is_highlight": true,
      "score": 91.0,
      "summary": "ConLA introduces a contrastive disentanglement framework for unsupervised robotic policy learning from human videos, enhancing transferability and performance beyond traditional robot trajectory pretraining.",
      "teaser_image": "https://arxiv.org/html/2602.00557/x3.png",
      "debug_abstract": "Vision-Language-Action (VLA) models achieve preliminary generalization through pretraining on large scale robot teleoperation datasets. However, acquiring datasets that comprehensively cover diverse tasks and environments is extremely costly and difficult to scale. In contrast, human demonstration videos offer a rich and scalable source of diverse scenes and manipulation behaviors, yet their lack of explicit action supervision hinders direct utilization. Prior work leverages VQ-VAE based frameworks to learn latent actions from human videos in an unsupervised manner. Nevertheless, since the training objective primarily focuses on reconstructing visual appearances rather than capturing inter-frame dynamics, the learned representations tend to rely on spurious visual cues, leading to shortcut learning and entangled latent representations that hinder transferability. To address this, we propose ConLA, an unsupervised pretraining framework for learning robotic policies from human videos. ConLA introduces a contrastive disentanglement mechanism that leverages action category priors and temporal cues to isolate motion dynamics from visual content, effectively mitigating shortcut learning. Extensive experiments show that ConLA achieves strong performance across diverse benchmarks. Notably, by pretraining solely on human videos, our method for the first time surpasses the performance obtained with real robot trajectory pretraining, highlighting its ability to extract pure and semantically consistent latent action representations for scalable robot learning."
    },
    {
      "title": "Learning to Accelerate Vision-Language-Action Models through Adaptive Visual Token Caching",
      "url": "https://arxiv.org/abs/2602.00686",
      "date": "2026-01-31",
      "authors_text": "Yujie Wei, Jiahan Fan, Jiyu Guo, Ruichen Zhen, Rui Shao",
      "is_highlight": false,
      "score": 84.0,
      "summary": "This paper presents a novel framework for enhancing Vision-Language-Action models' efficiency through adaptive token caching, achieving significant speedup and improved task success rates.",
      "teaser_image": "https://arxiv.org/html/2602.00686/x2.png",
      "debug_abstract": "Vision-Language-Action (VLA) models have demonstrated remarkable generalization capabilities in robotic manipulation tasks, yet their substantial computational overhead remains a critical obstacle to real-world deployment. Improving inference efficiency is therefore essential for practical robotic applications. Existing acceleration methods often rely on heuristic or static strategies--such as rule-based token caching or pruning--that are decoupled from task objectives and fail to adapt to dynamic scene changes. In this work, we reformulate inference acceleration as a learnable policy optimization problem and propose a novel framework that integrates a dynamic, task-aware decision-making process directly into the VLA model. At its core are two lightweight, cooperative modules: a Cached Token Selector, which determines which tokens should be reused, and a Cache Ratio Predictor, which controls how many tokens to reuse. Training these modules is non-trivial due to their discrete decisions. We address this by adopting a differentiable relaxation that allows gradient-based end-to-end optimization. Extensive experiments on the LIBERO and SIMPLER benchmarks, as well as real-robot evaluations, show that our method achieves a 1.76x wall-clock inference speedup while simultaneously improving the average success rate by 1.9 percentage points (from 75.0% to 76.9%) on LIBERO and by 5.0 percentage points on real-world tasks, significantly outperforming existing baselines. This work highlights the potential of learning task-aware computational allocation policies, paving the way for VLA models that are both powerful and efficient."
    },
    {
      "title": "Do All Individual Layers Help? An Empirical Study of Task-Interfering Layers in Vision-Language Models",
      "url": "https://arxiv.org/abs/2602.01167",
      "date": "2026-02-01",
      "authors_text": "Zhiming Liu, Yujie Wei, Lei Feng, Xiu Su, Xiaobo Xia",
      "is_highlight": false,
      "score": 52.0,
      "summary": "This study identifies Task-Interfering Layers in vision-language models, demonstrating that selectively bypassing certain layers can enhance task performance without retraining.",
      "teaser_image": "https://arxiv.org/html/2602.01167/x1.png",
      "debug_abstract": "Current VLMs have demonstrated capabilities across a wide range of multimodal tasks. Typically, in a pretrained VLM, all layers are engaged by default to make predictions on downstream tasks. We find that intervening on a single layer, such as by zeroing its parameters, can improve the performance on certain tasks, indicating that some layers hinder rather than help downstream tasks. We systematically investigate how individual layers influence different tasks via layer intervention. Specifically, we measure the change in performance relative to the base model after intervening on each layer and observe improvements when bypassing specific layers. This improvement can be generalizable across models and datasets, indicating the presence of Task-Interfering Layers that harm downstream tasks&#39; performance. We introduce Task-Layer Interaction Vector, which quantifies the effect of intervening on each layer of a VLM given a task. These task-interfering layers exhibit task-specific sensitivity patterns: tasks requiring similar capabilities show consistent response trends under layer interventions, as evidenced by the high similarity in their task-layer interaction vectors. Inspired by these findings, we propose TaLo (Task-Adaptive Layer Knockout), a training-free, test-time adaptation method that dynamically identifies and bypasses the most interfering layer for a given task. Without parameter updates, TaLo improves performance across various models and datasets, including boosting Qwen-VL&#39;s accuracy on the Maps task in ScienceQA by up to 16.6%. Our work reveals an unexpected form of modularity in pretrained VLMs and provides a plug-and-play, training-free mechanism to unlock hidden capabilities at inference time. The source code will be publicly available."
    },
    {
      "title": "Cross-Modal Alignment and Fusion for RGB-D Transmission-Line Defect Detection",
      "url": "https://arxiv.org/abs/2602.01696",
      "date": "2026-02-02",
      "authors_text": "Jiaming Cui, Shuai Zhou, Wenqiang Li, Ruifeng Qin, Feng Shen",
      "is_highlight": false,
      "score": 6.0,
      "summary": "CMAFNet enhances RGB-D transmission line defect detection by integrating RGB and depth data through a novel purification and fusion approach, significantly outperforming existing methods.",
      "teaser_image": "https://arxiv.org/html/2602.01696/figures/Overall_Architecture.png",
      "debug_abstract": "Transmission line defect detection remains challenging for automated UAV inspection due to the dominance of small-scale defects, complex backgrounds, and illumination variations. Existing RGB-based detectors, despite recent progress, struggle to distinguish geometrically subtle defects from visually similar background structures under limited chromatic contrast. This paper proposes CMAFNet, a Cross-Modal Alignment and Fusion Network that integrates RGB appearance and depth geometry through a principled purify-then-fuse paradigm. CMAFNet consists of a Semantic Recomposition Module that performs dictionary-based feature purification via a learned codebook to suppress modality-specific noise while preserving defect-discriminative information, and a Contextual Semantic Integration Framework that captures global spatial dependencies using partial-channel attention to enhance structural semantic reasoning. Position-wise normalization within the purification stage enforces explicit reconstruction-driven cross-modal alignment, ensuring statistical compatibility between heterogeneous features prior to fusion. Extensive experiments on the TLRGBD benchmark, where 94.5% of instances are small objects, demonstrate that CMAFNet achieves 32.2% mAP@50 and 12.5% APs, outperforming the strongest baseline by 9.8 and 4.0 percentage points, respectively. A lightweight variant reaches 24.8% mAP50 at 228 FPS with only 4.9M parameters, surpassing all YOLO-based detectors while matching transformer-based methods at substantially lower computational cost."
    },
    {
      "title": "RoamScene3D: Immersive Text-to-3D Scene Generation via Adaptive Object-aware Roaming",
      "url": "https://arxiv.org/abs/2601.19433",
      "date": "2026-01-27",
      "authors_text": "Jisheng Chu, Wenrui Li, Rui Zhao, Wangmeng Zuo, Shifeng Chen",
      "is_highlight": false,
      "score": 67.0,
      "summary": "RoamScene3D introduces a novel framework for immersive text-to-3D scene generation that enhances spatial awareness and object relations, outperforming existing methods in realism and consistency.",
      "debug_abstract": "Generating immersive 3D scenes from texts is a core task in computer vision, crucial for applications in virtual reality and game development. Despite the promise of leveraging 2D diffusion priors, existing methods suffer from spatial blindness and rely on predefined trajectories that fail to exploit the inner relationships among salient objects. Consequently, these approaches are unable to comprehend the semantic layout, preventing them from exploring the scene adaptively to infer occluded content. Moreover, current inpainting models operate in 2D image space, struggling to plausibly fill holes caused by camera motion. To address these limitations, we propose RoamScene3D, a novel framework that bridges the gap between semantic guidance and spatial generation. Our method reasons about the semantic relations among objects and produces consistent and photorealistic scenes. Specifically, we employ a vision-language model (VLM) to construct a scene graph that encodes object relations, guiding the camera to perceive salient object boundaries and plan an adaptive roaming trajectory. Furthermore, to mitigate the limitations of static 2D priors, we introduce a Motion-Injected Inpainting model that is fine-tuned on a synthetic panoramic dataset integrating authentic camera trajectories, making it adaptive to camera motion. Extensive experiments demonstrate that with semantic reasoning and geometric constraints, our method significantly outperforms state-of-the-art approaches in producing consistent and photorealistic scenes. Our code is available at this https URL."
    },
    {
      "title": "Orchestrating Intelligence: Confidence-Aware Routing for Efficient Multi-Agent Collaboration across Multi-Scale Models",
      "url": "https://arxiv.org/abs/2601.04861",
      "date": "2026-01-24",
      "authors_text": "Jingbo Wang, Sendong Zhao, Jiatong Liu, Haochun Wang, Wanting Li",
      "is_highlight": false,
      "score": 75.0,
      "summary": "The OI-MAS framework enhances multi-agent collaboration by dynamically routing tasks to appropriate model scales based on cognitive demands, improving accuracy and reducing computational costs.",
      "debug_abstract": "While multi-agent systems (MAS) have demonstrated superior performance over single-agent approaches in complex reasoning tasks, they often suffer from significant computational inefficiencies. Existing frameworks typically deploy large language models (LLMs) uniformly across all agent roles, failing to account for the varying cognitive demands of different reasoning stages. We address this inefficiency by proposing OI-MAS framework, a novel multi-agent framework that implements an adaptive model-selection policy across a heterogeneous pool of multi-scale LLMs. Specifically, OI-MAS introduces a state-dependent routing mechanism that dynamically selects agent roles and model scales throughout the reasoning process. In addition, we introduce a confidence-aware mechanism that selects appropriate model scales conditioned on task complexity, thus reducing unnecessary reliance on large-scale models. Experimental results show that OI-MAS consistently outperforms baseline multi-agent systems, improving accuracy by up to 12.88\\% while reducing cost by up to 79.78\\%."
    },
    {
      "title": "M2I2HA: Multi-modal Object Detection Based on Intra- and Inter-Modal Hypergraph Attention",
      "url": "https://arxiv.org/abs/2601.14776",
      "date": "2026-01-24",
      "authors_text": "Xiaofan Yang, Yubin Liu, Wei Pan, Guoqing Chu, Junming Zhang",
      "is_highlight": true,
      "score": 71.0,
      "summary": "The M2I2HA framework enhances multi-modal object detection by utilizing hypergraph attention to effectively capture intra- and inter-modal relationships, achieving state-of-the-art performance.",
      "debug_abstract": "Recent advances in multi-modal detection have significantly improved detection accuracy in challenging environments (e.g., low light, overexposure). By integrating RGB with modalities such as thermal and depth, multi-modal fusion increases data redundancy and system robustness. However, significant challenges remain in effectively extracting task-relevant information both within and across modalities, as well as in achieving precise cross-modal alignment. While CNNs excel at feature extraction, they are limited by constrained receptive fields, strong inductive biases, and difficulty in capturing long-range dependencies. Transformer-based models offer global context but suffer from quadratic computational complexity and are confined to pairwise correlation modeling. Mamba and other State Space Models (SSMs), on the other hand, are hindered by their sequential scanning mechanism, which flattens 2D spatial structures into 1D sequences, disrupting topological relationships and limiting the modeling of complex higher-order dependencies. To address these issues, we propose a multi-modal perception network based on hypergraph theory called M2I2HA. Our architecture includes an Intra-Hypergraph Enhancement module to capture global many-to-many high-order relationships within each modality, and an Inter-Hypergraph Fusion module to align, enhance, and fuse cross-modal features by bridging configuration and spatial gaps between data sources. We further introduce a M2-FullPAD module to enable adaptive multi-level fusion of multi-modal enhanced features within the network, meanwhile enhancing data distribution and flow across the architecture. Extensive object detection experiments on multiple public datasets against baselines demonstrate that M2I2HA achieves state-of-the-art performance in multi-modal object detection tasks."
    }
  ],
  "BIGAI": [
    {
      "title": "Towards Bridging the Gap between Large-Scale Pretraining and Efficient Finetuning for Humanoid Control",
      "url": "https://arxiv.org/abs/2601.21363",
      "date": "2026-01-29",
      "authors_text": "Weidong Huang, Zhehan Li, Hangxin Liu, Biao Hou, Yao Su",
      "is_highlight": false,
      "score": 85.0,
      "summary": "This paper presents a method combining off-policy Soft Actor-Critic for efficient pretraining and model-based techniques for safe finetuning of humanoid locomotion policies.",
      "teaser_image": "https://arxiv.org/html/2601.21363/x1.png",
      "debug_abstract": "Reinforcement learning (RL) is widely used for humanoid control, with on-policy methods such as Proximal Policy Optimization (PPO) enabling robust training via large-scale parallel simulation and, in some cases, zero-shot deployment to real robots. However, the low sample efficiency of on-policy algorithms limits safe adaptation to new environments. Although off-policy RL and model-based RL have shown improved sample efficiency, the gap between large-scale pretraining and efficient finetuning on humanoids still exists. In this paper, we find that off-policy Soft Actor-Critic (SAC), with large-batch update and a high Update-To-Data (UTD) ratio, reliably supports large-scale pretraining of humanoid locomotion policies, achieving zero-shot deployment on real robots. For adaptation, we demonstrate that these SAC-pretrained policies can be finetuned in new environments and out-of-distribution tasks using model-based methods. Data collection in the new environment executes a deterministic policy while stochastic exploration is instead confined to a physics-informed world model. This separation mitigates the risks of random exploration during adaptation while preserving exploratory coverage for improvement. Overall, the approach couples the wall-clock efficiency of large-scale simulation during pretraining with the sample efficiency of model-based learning during fine-tuning."
    },
    {
      "title": "Hybrid Deep Feature Extraction and ML for Construction and Demolition Debris Classification",
      "url": "https://arxiv.org/abs/2601.17038",
      "date": "2026-01-20",
      "authors_text": "Obai Alashram, Nejad Alagha, Mahmoud AlKakuri, Zeeshan Swaveel, Abigail Copiaco",
      "is_highlight": false,
      "score": 65.0,
      "summary": "This study introduces a hybrid pipeline combining deep feature extraction and classical ML classifiers for high-accuracy classification of construction debris, achieving up to 99.5% accuracy.",
      "debug_abstract": "The construction industry produces significant volumes of debris, making effective sorting and classification critical for sustainable waste management and resource recovery. This study presents a hybrid vision-based pipeline that integrates deep feature extraction with classical machine learning (ML) classifiers for automated construction and demolition (C\\&amp;D) debris classification. A novel dataset comprising 1,800 balanced, high-quality images representing four material categories, Ceramic/Tile, Concrete, Trash/Waste, and Wood was collected from real construction sites in the UAE, capturing diverse real-world conditions. Deep features were extracted using a pre-trained Xception network, and multiple ML classifiers, including SVM, kNN, Bagged Trees, LDA, and Logistic Regression, were systematically evaluated. The results demonstrate that hybrid pipelines using Xception features with simple classifiers such as Linear SVM, kNN, and Bagged Trees achieve state-of-the-art performance, with up to 99.5\\% accuracy and macro-F1 scores, surpassing more complex or end-to-end deep learning approaches. The analysis highlights the operational benefits of this approach for robust, field-deployable debris identification and provides pathways for future integration with robotics and onsite automation systems."
    }
  ],
  "南京大学": [
    {
      "title": "CMR: Contractive Mapping Embeddings for Robust Humanoid Locomotion on Unstructured Terrains",
      "url": "https://arxiv.org/abs/2602.03511",
      "date": "2026-02-03",
      "authors_text": "Qixin Zeng, Hongyin Zhang, Shangke Lyu, Junxi Jin, Donglin Wang",
      "is_highlight": true,
      "score": 87.0,
      "summary": "The CMR framework enhances humanoid locomotion robustness on unstructured terrains by mapping noisy observations to a stable latent space, improving performance against disturbances.",
      "teaser_image": "https://arxiv.org/html/2602.03511/figures/CMR_Framwork.png",
      "debug_abstract": "Robust disturbance rejection remains a longstanding challenge in humanoid locomotion, particularly on unstructured terrains where sensing is unreliable and model mismatch is pronounced. While perception information, such as height map, enhances terrain awareness, sensor noise and sim-to-real gaps can destabilize policies in practice. In this work, we provide theoretical analysis that bounds the return gap under observation noise, when the induced latent dynamics are contractive. Furthermore, we present Contractive Mapping for Robustness (CMR) framework that maps high-dimensional, disturbance-prone observations into a latent space, where local perturbations are attenuated over time. Specifically, this approach couples contrastive representation learning with Lipschitz regularization to preserve task-relevant geometry while explicitly controlling sensitivity. Notably, the formulation can be incorporated into modern deep reinforcement learning pipelines as an auxiliary loss term with minimal additional technical effort required. Further, our extensive humanoid experiments show that CMR potently outperforms other locomotion algorithms under increased noise."
    },
    {
      "title": "GLAD: Generative Language-Assisted Visual Tracking for Low-Semantic Templates",
      "url": "https://arxiv.org/abs/2602.00570",
      "date": "2026-01-31",
      "authors_text": "Xingyu Luo, Yidong Cai, Jie Liu, Jie Tang, Gangshan Wu",
      "is_highlight": false,
      "score": 70.0,
      "summary": "GLAD introduces a generative model that enhances vision-language tracking by improving compatibility between low-semantic images and text, achieving state-of-the-art performance and speed.",
      "teaser_image": "https://arxiv.org/html/2602.00570/x3.png",
      "debug_abstract": "Vision-language tracking has gained increasing attention in many scenarios. This task simultaneously deals with visual and linguistic information to localize objects in videos. Despite its growing utility, the development of vision-language tracking methods remains in its early stage. Current vision-language trackers usually employ Transformer architectures for interactive integration of template, search, and text features. However, persistent challenges about low-semantic images including prevalent image blurriness, low resolution and so on, may compromise model performance through degraded cross-modal understanding. To solve this problem, language assistance is usually used to deal with the obstacles posed by low-semantic images. However, due to the existing gap between current textual and visual features, direct concatenation and fusion of these features may have limited effectiveness. To address these challenges, we introduce a pioneering Generative Language-AssisteD tracking model, GLAD, which utilizes diffusion models for the generative multi-modal fusion of text description and template image to bolster compatibility between language and image and enhance template image semantic information. Our approach demonstrates notable improvements over the existing fusion paradigms. Blurry and semantically ambiguous template images can be restored to improve multi-modal features in the generative fusion paradigm. Experiments show that our method establishes a new state-of-the-art on multiple benchmarks and achieves an impressive inference speed. The code and models will be released at: this https URL"
    },
    {
      "title": "Learning Adaptive Cross-Embodiment Visuomotor Policy with Contrastive Prompt Orchestration",
      "url": "https://arxiv.org/abs/2602.01040",
      "date": "2026-02-01",
      "authors_text": "Yuhang Zhang, Chao Yan, Jiaxi Yu, Jiaping Xiao, Mir Feroskhan",
      "is_highlight": false,
      "score": 92.0,
      "summary": "The paper presents ContrAstive Prompt Orchestration (CAPO), a method for improving adaptive visuomotor policies in diverse environments through hybrid contrastive learning and dynamic prompt orchestration.",
      "teaser_image": "https://arxiv.org/html/2602.01040/x4.png",
      "debug_abstract": "Learning adaptive visuomotor policies for embodied agents remains a formidable challenge, particularly when facing cross-embodiment variations such as diverse sensor configurations and dynamic properties. Conventional learning approaches often struggle to separate task-relevant features from domain-specific variations (e.g., lighting, field-of-view, and rotation), leading to poor sample efficiency and catastrophic failure in unseen environments. To bridge this gap, we propose ContrAstive Prompt Orchestration (CAPO), a novel approach for learning visuomotor policies that integrates contrastive prompt learning and adaptive prompt orchestration. For prompt learning, we devise a hybrid contrastive learning strategy that integrates visual, temporal action, and text objectives to establish a pool of learnable prompts, where each prompt induces a visual representation encapsulating fine-grained domain factors. Based on these learned prompts, we introduce an adaptive prompt orchestration mechanism that dynamically aggregates these prompts conditioned on current observations. This enables the agent to adaptively construct optimal state representations by identifying dominant domain factors instantaneously. Consequently, the policy optimization is effectively shielded from irrelevant interference, preventing the common issue of overfitting to source domains. Extensive experiments demonstrate that CAPO significantly outperforms state-of-the-art baselines in sample efficiency and asymptotic performance. Crucially, it exhibits superior zero-shot adaptation across unseen target domains characterized by drastic environmental (e.g., illumination) and physical shifts (e.g., field-of-view and rotation), validating its effectiveness as a viable solution for cross-embodiment visuomotor policy adaptation."
    },
    {
      "title": "TIDE: Trajectory-based Diagnostic Evaluation of Test-Time Improvement in LLM Agents",
      "url": "https://arxiv.org/abs/2602.02196",
      "date": "2026-02-02",
      "authors_text": "Hang Yan, Xinyu Che, Fangzhi Xu, Qiushi Sun, Zichen Ding",
      "is_highlight": false,
      "score": 20.0,
      "summary": "The TIDE framework evaluates Test-Time Improvement in LLM agents by analyzing task completion dynamics, recursive behaviors, and memory constraints to enhance performance optimization.",
      "teaser_image": "https://arxiv.org/html/2602.02196/x1.png",
      "debug_abstract": "Recent advances in autonomous LLM agents demonstrate their ability to improve performance through iterative interaction with the environment. We define this paradigm as Test-Time Improvement (TTI). However, the mechanisms under how and why TTI succeed or fail remain poorly understood, and existing evaluation metrics fail to capture their task optimization efficiency, behavior adaptation after erroneous actions, and the specific utility of working memory for task completion. To address these gaps, we propose Test-time Improvement Diagnostic Evaluation (TIDE), an agent-agnostic and environment-agnostic framework that decomposes TTI into three comprehensive and interconnected dimensions. The framework measures (1) the overall temporal dynamics of task completion and (2) identifies whether performance is primarily constrained by recursive looping behaviors or (3) by burdensome accumulated memory. Through extensive experiments across diverse agents and environments, TIDE highlights that improving agent performance requires more than scaling internal reasoning, calling for explicitly optimizing the interaction dynamics between the agent and the environment."
    },
    {
      "title": "TraceRouter: Robust Safety for Large Foundation Models via Path-Level Intervention",
      "url": "https://arxiv.org/abs/2601.21900",
      "date": "2026-01-29",
      "authors_text": "Chuancheng Shi, Shangze Li, Wenjun Lu, Wenhua Wu, Cong Wang",
      "is_highlight": false,
      "score": 73.0,
      "summary": "TraceRouter enhances the safety of large foundation models by disconnecting harmful semantic pathways through a novel path-level intervention framework, outperforming existing defenses.",
      "teaser_image": "https://arxiv.org/html/2601.21900/x8.png",
      "debug_abstract": "Despite their capabilities, large foundation models (LFMs) remain susceptible to adversarial manipulation. Current defenses predominantly rely on the &#34;locality hypothesis&#34;, suppressing isolated neurons or features. However, harmful semantics act as distributed, cross-layer circuits, rendering such localized interventions brittle and detrimental to utility. To bridge this gap, we propose \\textbf{TraceRouter}, a path-level framework that traces and disconnects the causal propagation circuits of illicit semantics. TraceRouter operates in three stages: (1) it pinpoints a sensitive onset layer by analyzing attention divergence; (2) it leverages sparse autoencoders (SAEs) and differential activation analysis to disentangle and isolate malicious features; and (3) it maps these features to downstream causal pathways via feature influence scores (FIS) derived from zero-out interventions. By selectively suppressing these causal chains, TraceRouter physically severs the flow of harmful information while leaving orthogonal computation routes intact. Extensive experiments demonstrate that TraceRouter significantly outperforms state-of-the-art baselines, achieving a superior trade-off between adversarial robustness and general utility. Our code will be publicly released. WARNING: This paper contains unsafe model responses."
    },
    {
      "title": "GeoDiff3D: Self-Supervised 3D Scene Generation with Geometry-Constrained 2D Diffusion Guidance",
      "url": "https://arxiv.org/abs/2601.19785",
      "date": "2026-01-27",
      "authors_text": "Haozhi Zhu, Miaomiao Zhao, Dingyao Liu, Runze Tian, Yan Zhang",
      "is_highlight": false,
      "score": 73.0,
      "summary": "GeoDiff3D is a self-supervised framework for efficient, high-quality 3D scene generation using geometry-constrained 2D diffusion, reducing reliance on labeled data and enhancing structural coherence.",
      "debug_abstract": "3D scene generation is a core technology for gaming, film/VFX, and VR/AR. Growing demand for rapid iteration, high-fidelity detail, and accessible content creation has further increased interest in this area. Existing methods broadly follow two paradigms - indirect 2D-to-3D reconstruction and direct 3D generation - but both are limited by weak structural modeling and heavy reliance on large-scale ground-truth supervision, often producing structural artifacts, geometric inconsistencies, and degraded high-frequency details in complex scenes. We propose GeoDiff3D, an efficient self-supervised framework that uses coarse geometry as a structural anchor and a geometry-constrained 2D diffusion model to provide texture-rich reference images. Importantly, GeoDiff3D does not require strict multi-view consistency of the diffusion-generated references and remains robust to the resulting noisy, inconsistent guidance. We further introduce voxel-aligned 3D feature aggregation and dual self-supervision to maintain scene coherence and fine details while substantially reducing dependence on labeled data. GeoDiff3D also trains with low computational cost and enables fast, high-quality 3D scene generation. Extensive experiments on challenging scenes show improved generalization and generation quality over existing baselines, offering a practical solution for accessible and efficient 3D scene construction."
    },
    {
      "title": "Flatten The Complex: Joint B-Rep Generation via Compositional $k$-Cell Particles",
      "url": "https://arxiv.org/abs/2601.17733",
      "date": "2026-01-25",
      "authors_text": "Junran Lu, Yuanqi Li, Hengji Li, Jie Guo, Yanwen Guo",
      "is_highlight": false,
      "score": 58.0,
      "summary": "This paper presents a novel method for generating Boundary Representations (B-Reps) using compositional $k$-cell particles, enhancing topology-geometry coupling and context awareness for improved CAD modeling.",
      "debug_abstract": "Boundary Representation (B-Rep) is the widely adopted standard in Computer-Aided Design (CAD) and manufacturing. However, generative modeling of B-Reps remains a formidable challenge due to their inherent heterogeneity as geometric cell complexes, which entangles topology with geometry across cells of varying orders (i.e., $k$-cells such as vertices, edges, faces). Previous methods typically rely on cascaded sequences to handle this hierarchy, which fails to fully exploit the geometric relationships between cells, such as adjacency and sharing, limiting context awareness and error recovery. To fill this gap, we introduce a novel paradigm that reformulates B-Reps into sets of compositional $k$-cell particles. Our approach encodes each topological entity as a composition of particles, where adjacent cells share identical latents at their interfaces, thereby promoting geometric coupling along shared boundaries. By decoupling the rigid hierarchy, our representation unifies vertices, edges, and faces, enabling the joint generation of topology and geometry with global context awareness. We synthesize these particle sets using a multi-modal flow matching framework to handle unconditional generation as well as precise conditional tasks, such as 3D reconstruction from single-view or point cloud. Furthermore, the explicit and localized nature of our representation naturally extends to downstream tasks like local in-painting and enables the direct synthesis of non-manifold structures (e.g., wireframes). Extensive experiments demonstrate that our method produces high-fidelity CAD models with superior validity and editability compared to state-of-the-art methods."
    },
    {
      "title": "Physical Prompt Injection Attacks on Large Vision-Language Models",
      "url": "https://arxiv.org/abs/2601.17383",
      "date": "2026-01-24",
      "authors_text": "Chen Ling, Kai Hu, Hangcheng Liu, Xingshuo Han, Tianwei Zhang",
      "is_highlight": false,
      "score": 55.0,
      "summary": "The paper introduces Physical Prompt Injection Attacks (PPIA), a novel, highly effective method for manipulating Large Vision-Language Models using visually embedded prompts in physical environments.",
      "debug_abstract": "Large Vision-Language Models (LVLMs) are increasingly deployed in real-world intelligent systems for perception and reasoning in open physical environments. While LVLMs are known to be vulnerable to prompt injection attacks, existing methods either require access to input channels or depend on knowledge of user queries, assumptions that rarely hold in practical deployments. We propose the first Physical Prompt Injection Attack (PPIA), a black-box, query-agnostic attack that embeds malicious typographic instructions into physical objects perceivable by the LVLM. PPIA requires no access to the model, its inputs, or internal pipeline, and operates solely through visual observation. It combines offline selection of highly recognizable and semantically effective visual prompts with strategic environment-aware placement guided by spatiotemporal attention, ensuring that the injected prompts are both perceivable and influential on model behavior. We evaluate PPIA across 10 state-of-the-art LVLMs in both simulated and real-world settings on tasks including visual question answering, planning, and navigation, PPIA achieves attack success rates up to 98%, with strong robustness under varying physical conditions such as distance, viewpoint, and illumination. Our code is publicly available at this https URL."
    },
    {
      "title": "PEAfowl: Perception-Enhanced Multi-View Vision-Language-Action for Bimanual Manipulation",
      "url": "https://arxiv.org/abs/2601.17885",
      "date": "2026-01-25",
      "authors_text": "Qingyu Fan, Zhaoxiang Li, Yi Lu, Wang Chen, Qiu Shen",
      "is_highlight": false,
      "score": 91.0,
      "summary": "PEAfowl enhances bimanual manipulation by integrating perception-driven multi-view features and refined instruction grounding, achieving significant performance improvements in cluttered environments.",
      "debug_abstract": "Bimanual manipulation in cluttered scenes requires policies that remain stable under occlusions, viewpoint and scene variations. Existing vision-language-action models often fail to generalize because (i) multi-view features are fused via view-agnostic token concatenation, yielding weak 3D-consistent spatial understanding, and (ii) language is injected as global conditioning, resulting in coarse instruction grounding. In this paper, we introduce PEAfowl, a perception-enhanced multi-view VLA policy for bimanual manipulation. For spatial reasoning, PEAfowl predicts per-token depth distributions, performs differentiable 3D lifting, and aggregates local cross-view neighbors to form geometrically grounded, cross-view consistent representations. For instruction grounding, we propose to replace global conditioning with a Perceiver-style text-aware readout over frozen CLIP visual features, enabling iterative evidence accumulation. To overcome noisy and incomplete commodity depth without adding inference overhead, we apply training-only depth distillation from a pretrained depth teacher to supervise the depth-distribution head, providing perception front-end with geometry-aware priors. On RoboTwin 2.0 under domain-randomized setting, PEAfowl improves the strongest baseline by 23.0 pp in success rate, and real-robot experiments further demonstrate reliable sim-to-real transfer and consistent improvements from depth distillation. Project website: this https URL."
    },
    {
      "title": "GenAgent: Scaling Text-to-Image Generation via Agentic Multimodal Reasoning",
      "url": "https://arxiv.org/abs/2601.18543",
      "date": "2026-01-26",
      "authors_text": "Kaixun Jiang, Yuzheng Wang, Junjie Zhou, Pandeng Li, Zhihang Liu",
      "is_highlight": false,
      "score": 74.0,
      "summary": "GenAgent enhances text-to-image generation by decoupling understanding and generation through an agentic framework, enabling autonomous interactions and improved performance across tasks.",
      "debug_abstract": "We introduce GenAgent, unifying visual understanding and generation through an agentic multimodal model. Unlike unified models that face expensive training costs and understanding-generation trade-offs, GenAgent decouples these capabilities through an agentic framework: understanding is handled by the multimodal model itself, while generation is achieved by treating image generation models as invokable tools. Crucially, unlike existing modular systems constrained by static pipelines, this design enables autonomous multi-turn interactions where the agent generates multimodal chains-of-thought encompassing reasoning, tool invocation, judgment, and reflection to iteratively refine outputs. We employ a two-stage training strategy: first, cold-start with supervised fine-tuning on high-quality tool invocation and reflection data to bootstrap agent behaviors; second, end-to-end agentic reinforcement learning combining pointwise rewards (final image quality) and pairwise rewards (reflection accuracy), with trajectory resampling for enhanced multi-turn exploration. GenAgent significantly boosts base generator(FLUX.1-dev) performance on GenEval++ (+23.6\\%) and WISE (+14\\%). Beyond performance gains, our framework demonstrates three key properties: 1) cross-tool generalization to generators with varying capabilities, 2) test-time scaling with consistent improvements across interaction rounds, and 3) task-adaptive reasoning that automatically adjusts to different tasks. Our code will be available at \\href{this https URL}{this url}."
    }
  ],
  "机器人系统实验室 (RSL)": [
    {
      "title": "LEVIO: Lightweight Embedded Visual Inertial Odometry for Resource-Constrained Devices",
      "url": "https://arxiv.org/abs/2602.03294",
      "date": "2026-02-03",
      "authors_text": "Jonas Kühne, Christian Vogt, Michele Magno, Luca Benini",
      "is_highlight": false,
      "score": 86.0,
      "summary": "LEVIO is a lightweight visual-inertial odometry system optimized for low-power devices, achieving real-time performance with efficient algorithms and open-source implementation.",
      "teaser_image": "https://arxiv.org/html/2602.03294/x2.png",
      "debug_abstract": "Accurate, infrastructure-less sensor systems for motion tracking are essential for mobile robotics and augmented reality (AR) applications. The most popular state-of-the-art visual-inertial odometry (VIO) systems, however, are too computationally demanding for resource-constrained hardware, such as micro-drones and smart glasses. This work presents LEVIO, a fully featured VIO pipeline optimized for ultra-low-power compute platforms, allowing six-degrees-of-freedom (DoF) real-time sensing. LEVIO incorporates established VIO components such as Oriented FAST and Rotated BRIEF (ORB) feature tracking and bundle adjustment, while emphasizing a computationally efficient architecture with parallelization and low memory usage to suit embedded microcontrollers and low-power systems-on-chip (SoCs). The paper proposes and details the algorithmic design choices and the hardware-software co-optimization approach, and presents real-time performance on resource-constrained hardware. LEVIO is validated on a parallel-processing ultra-low-power RISC-V SoC, achieving 20 FPS while consuming less than 100 mW, and benchmarked against public VIO datasets, offering a compelling balance between efficiency and accuracy. To facilitate reproducibility and adoption, the complete implementation is released as open-source."
    },
    {
      "title": "Depth Completion in Unseen Field Robotics Environments Using Extremely Sparse Depth Measurements",
      "url": "https://arxiv.org/abs/2602.03209",
      "date": "2026-02-03",
      "authors_text": "Marco Job, Thomas Stastny, Eleni Kelasidi, Roland Siegwart, Michael Pantic",
      "is_highlight": false,
      "score": 43.0,
      "summary": "The paper presents a depth completion model that utilizes synthetic data and sparse depth measurements to enhance depth perception for autonomous field robots in unstructured environments.",
      "teaser_image": "https://arxiv.org/html/2602.03209/x2.png",
      "debug_abstract": "Autonomous field robots operating in unstructured environments require robust perception to ensure safe and reliable operations. Recent advances in monocular depth estimation have demonstrated the potential of low-cost cameras as depth sensors; however, their adoption in field robotics remains limited due to the absence of reliable scale cues, ambiguous or low-texture conditions, and the scarcity of large-scale datasets. To address these challenges, we propose a depth completion model that trains on synthetic data and uses extremely sparse measurements from depth sensors to predict dense metric depth in unseen field robotics environments. A synthetic dataset generation pipeline tailored to field robotics enables the creation of multiple realistic datasets for training purposes. This dataset generation approach utilizes textured 3D meshes from Structure from Motion and photorealistic rendering with novel viewpoint synthesis to simulate diverse field robotics scenarios. Our approach achieves an end-to-end latency of 53 ms per frame on a Nvidia Jetson AGX Orin, enabling real-time deployment on embedded platforms. Extensive evaluation demonstrates competitive performance across diverse real-world field robotics scenarios."
    },
    {
      "title": "VRGaussianAvatar: Integrating 3D Gaussian Avatars into VR",
      "url": "https://arxiv.org/abs/2602.01674",
      "date": "2026-02-02",
      "authors_text": "Hail Song, Boram Yoon, Seokhwan Yang, Seoyoung Kang, Hyunjeong Kim",
      "is_highlight": false,
      "score": 37.0,
      "summary": "VRGaussianAvatar is a real-time system that creates full-body 3D Gaussian avatars in VR using HMD tracking, enhancing rendering efficiency and user experience.",
      "teaser_image": "https://arxiv.org/html/2602.01674/x2.png",
      "debug_abstract": "We present VRGaussianAvatar, an integrated system that enables real-time full-body 3D Gaussian Splatting (3DGS) avatars in virtual reality using only head-mounted display (HMD) tracking signals. The system adopts a parallel pipeline with a VR Frontend and a GA Backend. The VR Frontend uses inverse kinematics to estimate full-body pose and streams the resulting pose along with stereo camera parameters to the backend. The GA Backend stereoscopically renders a 3DGS avatar reconstructed from a single image. To improve stereo rendering efficiency, we introduce Binocular Batching, which jointly processes left and right eye views in a single batched pass to reduce redundant computation and support high-resolution VR displays. We evaluate VRGaussianAvatar with quantitative performance tests and a within-subject user study against image- and video-based mesh avatar baselines. Results show that VRGaussianAvatar sustains interactive VR performance and yields higher perceived appearance similarity, embodiment, and plausibility. Project page and source code are available at this https URL."
    },
    {
      "title": "Training slow silicon neurons to control extremely fast robots with spiking reinforcement learning",
      "url": "https://arxiv.org/abs/2601.21548",
      "date": "2026-01-29",
      "authors_text": "Irene Ambrosini, Ingo Blakowski, Dmitrii Zendrikov, Cristiano Capone, Luna Gava",
      "is_highlight": false,
      "score": 91.0,
      "summary": "This paper presents a neuromorphic processor-based spiking neural network that achieves real-time learning for controlling fast robots in air hockey through efficient reinforcement learning.",
      "teaser_image": "https://arxiv.org/html/2601.21548/img/figure1.png",
      "debug_abstract": "Air hockey demands split-second decisions at high puck velocities, a challenge we address with a compact network of spiking neurons running on a mixed-signal analog/digital neuromorphic processor. By co-designing hardware and learning algorithms, we train the system to achieve successful puck interactions through reinforcement learning in a remarkably small number of trials. The network leverages fixed random connectivity to capture the task&#39;s temporal structure and adopts a local e-prop learning rule in the readout layer to exploit event-driven activity for fast and efficient learning. The result is real-time learning with a setup comprising a computer and the neuromorphic chip in-the-loop, enabling practical training of spiking neural networks for robotic autonomous systems. This work bridges neuroscience-inspired hardware with real-world robotic control, showing that brain-inspired approaches can tackle fast-paced interaction tasks while supporting always-on learning in intelligent machines."
    },
    {
      "title": "Game-Theoretic Autonomous Driving: A Graphs of Convex Sets Approach",
      "url": "https://arxiv.org/abs/2601.20054",
      "date": "2026-01-27",
      "authors_text": "Nikolaj Käfer, Ahmed Khalil, Edward Huynh, Efstathios Bakolas, David Fridovich-Keil",
      "is_highlight": true,
      "score": 82.0,
      "summary": "The IBR-GCS approach models multi-vehicle autonomous driving as a game, enabling efficient trajectory planning and strategic interaction under safety constraints through iterative best-response updates.",
      "teaser_image": "https://arxiv.org/html/2601.20054/x2.png",
      "debug_abstract": "Multi-vehicle autonomous driving couples strategic interaction with hybrid (discrete-continuous) maneuver planning under shared safety constraints. We introduce IBR-GCS, an Iterative Best Response (IBR) planning approach based on the Graphs of Convex Sets (GCS) framework that models highway driving as a generalized noncooperative game. IBR-GCS integrates combinatorial maneuver reasoning, trajectory planning, and game-theoretic interaction within a unified framework. The key novelty is a vehicle-specific, strategy-dependent GCS construction. Specifically, at each best-response update, each vehicle builds its own graph conditioned on the current strategies of the other vehicles, with vertices representing lane-specific, time-varying, convex, collision-free regions and edges encoding dynamically feasible transitions. This yields a shortest-path problem in GCS for each best-response step, which admits an efficient convex relaxation that can be solved using convex optimization tools without exhaustive discrete tree search. We then apply an iterative best-response scheme in which vehicles update their trajectories sequentially and provide conditions under which the resulting inexact updates converge to an approximate generalized Nash equilibrium. Simulation results across multi-lane, multi-vehicle scenarios demonstrate that IBR-GCS produces safe trajectories and strategically consistent interactive behaviors."
    },
    {
      "title": "Reinforcement Learning via Self-Distillation",
      "url": "https://arxiv.org/abs/2601.20802",
      "date": "2026-01-28",
      "authors_text": "Jonas Hübotter, Frederike Lübeck, Lejs Behric, Anton Baumann, Marco Bagatella",
      "is_highlight": false,
      "score": 62.0,
      "summary": "The paper introduces Self-Distillation Policy Optimization (SDPO), enhancing reinforcement learning with rich feedback to improve sample efficiency and accuracy in various domains.",
      "teaser_image": "https://arxiv.org/html/2601.20802/x2.png",
      "debug_abstract": "Large language models are increasingly post-trained with reinforcement learning in verifiable domains such as code and math. Yet, current methods for reinforcement learning with verifiable rewards (RLVR) learn only from a scalar outcome reward per attempt, creating a severe credit-assignment bottleneck. Many verifiable environments actually provide rich textual feedback, such as runtime errors or judge evaluations, that explain why an attempt failed. We formalize this setting as reinforcement learning with rich feedback and introduce Self-Distillation Policy Optimization (SDPO), which converts tokenized feedback into a dense learning signal without any external teacher or explicit reward model. SDPO treats the current model conditioned on feedback as a self-teacher and distills its feedback-informed next-token predictions back into the policy. In this way, SDPO leverages the model&#39;s ability to retrospectively identify its own mistakes in-context. Across scientific reasoning, tool use, and competitive programming on LiveCodeBench v6, SDPO improves sample efficiency and final accuracy over strong RLVR baselines. Notably, SDPO also outperforms baselines in standard RLVR environments that only return scalar feedback by using successful rollouts as implicit feedback for failed attempts. Finally, applying SDPO to individual questions at test time accelerates discovery on difficult binary-reward tasks, achieving the same discovery probability as best-of-k sampling or multi-turn conversations with 3x fewer attempts."
    },
    {
      "title": "Safe Exploration via Policy Priors",
      "url": "https://arxiv.org/abs/2601.19612",
      "date": "2026-01-27",
      "authors_text": "Manuel Wendl, Yarden As, Manish Prajapat, Anton Pollak, Stelian Coros",
      "is_highlight": false,
      "score": 84.0,
      "summary": "The paper presents SOOPER, a safe exploration method for reinforcement learning that uses conservative policy priors to ensure safety and optimal policy convergence during online learning.",
      "debug_abstract": "Safe exploration is a key requirement for reinforcement learning (RL) agents to learn and adapt online, beyond controlled (e.g. simulated) environments. In this work, we tackle this challenge by utilizing suboptimal yet conservative policies (e.g., obtained from offline data or simulators) as priors. Our approach, SOOPER, uses probabilistic dynamics models to optimistically explore, yet pessimistically fall back to the conservative policy prior if needed. We prove that SOOPER guarantees safety throughout learning, and establish convergence to an optimal policy by bounding its cumulative regret. Extensive experiments on key safe RL benchmarks and real-world hardware demonstrate that SOOPER is scalable, outperforms the state-of-the-art and validate our theoretical guarantees in practice."
    },
    {
      "title": "DeFM: Learning Foundation Representations from Depth for Robotics",
      "url": "https://arxiv.org/abs/2601.18923",
      "date": "2026-01-26",
      "authors_text": "Manthan Patel, Jonas Frey, Mayank Mittal, Fan Yang, Alexander Hansson",
      "is_highlight": true,
      "score": 88.0,
      "summary": "DeFM is a self-supervised foundation model that learns robust geometric and semantic representations from depth images, achieving state-of-the-art performance across various robotic tasks.",
      "debug_abstract": "Depth sensors are widely deployed across robotic platforms, and advances in fast, high-fidelity depth simulation have enabled robotic policies trained on depth observations to achieve robust sim-to-real transfer for a wide range of tasks. Despite this, representation learning for depth modality remains underexplored compared to RGB, where large-scale foundation models now define the state of the art. To address this gap, we present DeFM, a self-supervised foundation model trained entirely on depth images for robotic applications. Using a DINO-style self-distillation objective on a curated dataset of 60M depth images, DeFM learns geometric and semantic representations that generalize to diverse environments, tasks, and sensors. To retain metric awareness across multiple scales, we introduce a novel input normalization strategy. We further distill DeFM into compact models suitable for resource-constrained robotic systems. When evaluated on depth-based classification, segmentation, navigation, locomotion, and manipulation benchmarks, DeFM achieves state-of-the-art performance and demonstrates strong generalization from simulation to real-world environments. We release all our pretrained models, which can be adopted off-the-shelf for depth-based robotic learning without task-specific fine-tuning. Webpage: this https URL"
    },
    {
      "title": "Scaling Rough Terrain Locomotion with Automatic Curriculum Reinforcement Learning",
      "url": "https://arxiv.org/abs/2601.17428",
      "date": "2026-01-24",
      "authors_text": "Ziming Li, Chenhao Li, Marco Hutter",
      "is_highlight": true,
      "score": 85.0,
      "summary": "The LP-ACRL framework enables quadruped robots to achieve stable high-speed locomotion across diverse terrains by adaptively generating curricula based on online learning progress.",
      "debug_abstract": "Curriculum learning has demonstrated substantial effectiveness in robot learning. However, it still faces limitations when scaling to complex, wide-ranging task spaces. Such task spaces often lack a well-defined difficulty structure, making the difficulty ordering required by previous methods challenging to define. We propose a Learning Progress-based Automatic Curriculum Reinforcement Learning (LP-ACRL) framework, which estimates the agent&#39;s learning progress online and adaptively adjusts the task-sampling distribution, thereby enabling automatic curriculum generation without prior knowledge of the difficulty distribution over the task space. Policies trained with LP-ACRL enable the ANYmal D quadruped to achieve and maintain stable, high-speed locomotion at 2.5 m/s linear velocity and 3.0 rad/s angular velocity across diverse terrains, including stairs, slopes, gravel, and low-friction flat surfaces--whereas previous methods have generally been limited to high speeds on flat terrain or low speeds on complex terrain. Experimental results demonstrate that LP-ACRL exhibits strong scalability and real-world applicability, providing a robust baseline for future research on curriculum generation in complex, wide-ranging robotic learning task spaces."
    }
  ],
  "Sensing, Interaction & Perception Lab (SIPLAB)": [
    {
      "title": "LEVIO: Lightweight Embedded Visual Inertial Odometry for Resource-Constrained Devices",
      "url": "https://arxiv.org/abs/2602.03294",
      "date": "2026-02-03",
      "authors_text": "Jonas Kühne, Christian Vogt, Michele Magno, Luca Benini",
      "is_highlight": false,
      "score": 86.0,
      "summary": "LEVIO is a lightweight visual-inertial odometry system optimized for low-power devices, achieving real-time performance with efficient algorithms and open-source implementation.",
      "teaser_image": "https://arxiv.org/html/2602.03294/x2.png",
      "debug_abstract": "Accurate, infrastructure-less sensor systems for motion tracking are essential for mobile robotics and augmented reality (AR) applications. The most popular state-of-the-art visual-inertial odometry (VIO) systems, however, are too computationally demanding for resource-constrained hardware, such as micro-drones and smart glasses. This work presents LEVIO, a fully featured VIO pipeline optimized for ultra-low-power compute platforms, allowing six-degrees-of-freedom (DoF) real-time sensing. LEVIO incorporates established VIO components such as Oriented FAST and Rotated BRIEF (ORB) feature tracking and bundle adjustment, while emphasizing a computationally efficient architecture with parallelization and low memory usage to suit embedded microcontrollers and low-power systems-on-chip (SoCs). The paper proposes and details the algorithmic design choices and the hardware-software co-optimization approach, and presents real-time performance on resource-constrained hardware. LEVIO is validated on a parallel-processing ultra-low-power RISC-V SoC, achieving 20 FPS while consuming less than 100 mW, and benchmarked against public VIO datasets, offering a compelling balance between efficiency and accuracy. To facilitate reproducibility and adoption, the complete implementation is released as open-source."
    },
    {
      "title": "Depth Completion in Unseen Field Robotics Environments Using Extremely Sparse Depth Measurements",
      "url": "https://arxiv.org/abs/2602.03209",
      "date": "2026-02-03",
      "authors_text": "Marco Job, Thomas Stastny, Eleni Kelasidi, Roland Siegwart, Michael Pantic",
      "is_highlight": false,
      "score": 43.0,
      "summary": "The paper presents a depth completion model that utilizes synthetic data and sparse depth measurements to enhance depth perception for autonomous field robots in unstructured environments.",
      "teaser_image": "https://arxiv.org/html/2602.03209/x2.png",
      "debug_abstract": "Autonomous field robots operating in unstructured environments require robust perception to ensure safe and reliable operations. Recent advances in monocular depth estimation have demonstrated the potential of low-cost cameras as depth sensors; however, their adoption in field robotics remains limited due to the absence of reliable scale cues, ambiguous or low-texture conditions, and the scarcity of large-scale datasets. To address these challenges, we propose a depth completion model that trains on synthetic data and uses extremely sparse measurements from depth sensors to predict dense metric depth in unseen field robotics environments. A synthetic dataset generation pipeline tailored to field robotics enables the creation of multiple realistic datasets for training purposes. This dataset generation approach utilizes textured 3D meshes from Structure from Motion and photorealistic rendering with novel viewpoint synthesis to simulate diverse field robotics scenarios. Our approach achieves an end-to-end latency of 53 ms per frame on a Nvidia Jetson AGX Orin, enabling real-time deployment on embedded platforms. Extensive evaluation demonstrates competitive performance across diverse real-world field robotics scenarios."
    },
    {
      "title": "VRGaussianAvatar: Integrating 3D Gaussian Avatars into VR",
      "url": "https://arxiv.org/abs/2602.01674",
      "date": "2026-02-02",
      "authors_text": "Hail Song, Boram Yoon, Seokhwan Yang, Seoyoung Kang, Hyunjeong Kim",
      "is_highlight": false,
      "score": 37.0,
      "summary": "VRGaussianAvatar is a real-time system that creates full-body 3D Gaussian avatars in VR using HMD tracking, enhancing rendering efficiency and user experience.",
      "teaser_image": "https://arxiv.org/html/2602.01674/x2.png",
      "debug_abstract": "We present VRGaussianAvatar, an integrated system that enables real-time full-body 3D Gaussian Splatting (3DGS) avatars in virtual reality using only head-mounted display (HMD) tracking signals. The system adopts a parallel pipeline with a VR Frontend and a GA Backend. The VR Frontend uses inverse kinematics to estimate full-body pose and streams the resulting pose along with stereo camera parameters to the backend. The GA Backend stereoscopically renders a 3DGS avatar reconstructed from a single image. To improve stereo rendering efficiency, we introduce Binocular Batching, which jointly processes left and right eye views in a single batched pass to reduce redundant computation and support high-resolution VR displays. We evaluate VRGaussianAvatar with quantitative performance tests and a within-subject user study against image- and video-based mesh avatar baselines. Results show that VRGaussianAvatar sustains interactive VR performance and yields higher perceived appearance similarity, embodiment, and plausibility. Project page and source code are available at this https URL."
    },
    {
      "title": "Training slow silicon neurons to control extremely fast robots with spiking reinforcement learning",
      "url": "https://arxiv.org/abs/2601.21548",
      "date": "2026-01-29",
      "authors_text": "Irene Ambrosini, Ingo Blakowski, Dmitrii Zendrikov, Cristiano Capone, Luna Gava",
      "is_highlight": false,
      "score": 91.0,
      "summary": "This paper presents a neuromorphic processor-based spiking neural network that achieves real-time learning for controlling fast robots in air hockey through efficient reinforcement learning.",
      "teaser_image": "https://arxiv.org/html/2601.21548/img/figure1.png",
      "debug_abstract": "Air hockey demands split-second decisions at high puck velocities, a challenge we address with a compact network of spiking neurons running on a mixed-signal analog/digital neuromorphic processor. By co-designing hardware and learning algorithms, we train the system to achieve successful puck interactions through reinforcement learning in a remarkably small number of trials. The network leverages fixed random connectivity to capture the task&#39;s temporal structure and adopts a local e-prop learning rule in the readout layer to exploit event-driven activity for fast and efficient learning. The result is real-time learning with a setup comprising a computer and the neuromorphic chip in-the-loop, enabling practical training of spiking neural networks for robotic autonomous systems. This work bridges neuroscience-inspired hardware with real-world robotic control, showing that brain-inspired approaches can tackle fast-paced interaction tasks while supporting always-on learning in intelligent machines."
    },
    {
      "title": "Game-Theoretic Autonomous Driving: A Graphs of Convex Sets Approach",
      "url": "https://arxiv.org/abs/2601.20054",
      "date": "2026-01-27",
      "authors_text": "Nikolaj Käfer, Ahmed Khalil, Edward Huynh, Efstathios Bakolas, David Fridovich-Keil",
      "is_highlight": true,
      "score": 82.0,
      "summary": "The IBR-GCS approach models multi-vehicle autonomous driving as a game, enabling efficient trajectory planning and strategic interaction under safety constraints through iterative best-response updates.",
      "teaser_image": "https://arxiv.org/html/2601.20054/x2.png",
      "debug_abstract": "Multi-vehicle autonomous driving couples strategic interaction with hybrid (discrete-continuous) maneuver planning under shared safety constraints. We introduce IBR-GCS, an Iterative Best Response (IBR) planning approach based on the Graphs of Convex Sets (GCS) framework that models highway driving as a generalized noncooperative game. IBR-GCS integrates combinatorial maneuver reasoning, trajectory planning, and game-theoretic interaction within a unified framework. The key novelty is a vehicle-specific, strategy-dependent GCS construction. Specifically, at each best-response update, each vehicle builds its own graph conditioned on the current strategies of the other vehicles, with vertices representing lane-specific, time-varying, convex, collision-free regions and edges encoding dynamically feasible transitions. This yields a shortest-path problem in GCS for each best-response step, which admits an efficient convex relaxation that can be solved using convex optimization tools without exhaustive discrete tree search. We then apply an iterative best-response scheme in which vehicles update their trajectories sequentially and provide conditions under which the resulting inexact updates converge to an approximate generalized Nash equilibrium. Simulation results across multi-lane, multi-vehicle scenarios demonstrate that IBR-GCS produces safe trajectories and strategically consistent interactive behaviors."
    },
    {
      "title": "Reinforcement Learning via Self-Distillation",
      "url": "https://arxiv.org/abs/2601.20802",
      "date": "2026-01-28",
      "authors_text": "Jonas Hübotter, Frederike Lübeck, Lejs Behric, Anton Baumann, Marco Bagatella",
      "is_highlight": false,
      "score": 62.0,
      "summary": "The paper introduces Self-Distillation Policy Optimization (SDPO), enhancing reinforcement learning with rich feedback to improve sample efficiency and accuracy in various domains.",
      "teaser_image": "https://arxiv.org/html/2601.20802/x2.png",
      "debug_abstract": "Large language models are increasingly post-trained with reinforcement learning in verifiable domains such as code and math. Yet, current methods for reinforcement learning with verifiable rewards (RLVR) learn only from a scalar outcome reward per attempt, creating a severe credit-assignment bottleneck. Many verifiable environments actually provide rich textual feedback, such as runtime errors or judge evaluations, that explain why an attempt failed. We formalize this setting as reinforcement learning with rich feedback and introduce Self-Distillation Policy Optimization (SDPO), which converts tokenized feedback into a dense learning signal without any external teacher or explicit reward model. SDPO treats the current model conditioned on feedback as a self-teacher and distills its feedback-informed next-token predictions back into the policy. In this way, SDPO leverages the model&#39;s ability to retrospectively identify its own mistakes in-context. Across scientific reasoning, tool use, and competitive programming on LiveCodeBench v6, SDPO improves sample efficiency and final accuracy over strong RLVR baselines. Notably, SDPO also outperforms baselines in standard RLVR environments that only return scalar feedback by using successful rollouts as implicit feedback for failed attempts. Finally, applying SDPO to individual questions at test time accelerates discovery on difficult binary-reward tasks, achieving the same discovery probability as best-of-k sampling or multi-turn conversations with 3x fewer attempts."
    },
    {
      "title": "Safe Exploration via Policy Priors",
      "url": "https://arxiv.org/abs/2601.19612",
      "date": "2026-01-27",
      "authors_text": "Manuel Wendl, Yarden As, Manish Prajapat, Anton Pollak, Stelian Coros",
      "is_highlight": false,
      "score": 84.0,
      "summary": "The paper presents SOOPER, a safe exploration method for reinforcement learning that uses conservative policy priors to ensure safety and optimal policy convergence during online learning.",
      "debug_abstract": "Safe exploration is a key requirement for reinforcement learning (RL) agents to learn and adapt online, beyond controlled (e.g. simulated) environments. In this work, we tackle this challenge by utilizing suboptimal yet conservative policies (e.g., obtained from offline data or simulators) as priors. Our approach, SOOPER, uses probabilistic dynamics models to optimistically explore, yet pessimistically fall back to the conservative policy prior if needed. We prove that SOOPER guarantees safety throughout learning, and establish convergence to an optimal policy by bounding its cumulative regret. Extensive experiments on key safe RL benchmarks and real-world hardware demonstrate that SOOPER is scalable, outperforms the state-of-the-art and validate our theoretical guarantees in practice."
    },
    {
      "title": "DeFM: Learning Foundation Representations from Depth for Robotics",
      "url": "https://arxiv.org/abs/2601.18923",
      "date": "2026-01-26",
      "authors_text": "Manthan Patel, Jonas Frey, Mayank Mittal, Fan Yang, Alexander Hansson",
      "is_highlight": true,
      "score": 88.0,
      "summary": "DeFM is a self-supervised foundation model that learns robust geometric and semantic representations from depth images, achieving state-of-the-art performance across various robotic tasks.",
      "debug_abstract": "Depth sensors are widely deployed across robotic platforms, and advances in fast, high-fidelity depth simulation have enabled robotic policies trained on depth observations to achieve robust sim-to-real transfer for a wide range of tasks. Despite this, representation learning for depth modality remains underexplored compared to RGB, where large-scale foundation models now define the state of the art. To address this gap, we present DeFM, a self-supervised foundation model trained entirely on depth images for robotic applications. Using a DINO-style self-distillation objective on a curated dataset of 60M depth images, DeFM learns geometric and semantic representations that generalize to diverse environments, tasks, and sensors. To retain metric awareness across multiple scales, we introduce a novel input normalization strategy. We further distill DeFM into compact models suitable for resource-constrained robotic systems. When evaluated on depth-based classification, segmentation, navigation, locomotion, and manipulation benchmarks, DeFM achieves state-of-the-art performance and demonstrates strong generalization from simulation to real-world environments. We release all our pretrained models, which can be adopted off-the-shelf for depth-based robotic learning without task-specific fine-tuning. Webpage: this https URL"
    },
    {
      "title": "Scaling Rough Terrain Locomotion with Automatic Curriculum Reinforcement Learning",
      "url": "https://arxiv.org/abs/2601.17428",
      "date": "2026-01-24",
      "authors_text": "Ziming Li, Chenhao Li, Marco Hutter",
      "is_highlight": true,
      "score": 85.0,
      "summary": "The LP-ACRL framework enables quadruped robots to achieve stable high-speed locomotion across diverse terrains by adaptively generating curricula based on online learning progress.",
      "debug_abstract": "Curriculum learning has demonstrated substantial effectiveness in robot learning. However, it still faces limitations when scaling to complex, wide-ranging task spaces. Such task spaces often lack a well-defined difficulty structure, making the difficulty ordering required by previous methods challenging to define. We propose a Learning Progress-based Automatic Curriculum Reinforcement Learning (LP-ACRL) framework, which estimates the agent&#39;s learning progress online and adaptively adjusts the task-sampling distribution, thereby enabling automatic curriculum generation without prior knowledge of the difficulty distribution over the task space. Policies trained with LP-ACRL enable the ANYmal D quadruped to achieve and maintain stable, high-speed locomotion at 2.5 m/s linear velocity and 3.0 rad/s angular velocity across diverse terrains, including stairs, slopes, gravel, and low-friction flat surfaces--whereas previous methods have generally been limited to high speeds on flat terrain or low speeds on complex terrain. Experimental results demonstrate that LP-ACRL exhibits strong scalability and real-world applicability, providing a robust baseline for future research on curriculum generation in complex, wide-ranging robotic learning task spaces."
    }
  ],
  "ReThinkLab": [
    {
      "title": "Embodiment-Aware Generalist Specialist Distillation for Unified Humanoid Whole-Body Control",
      "url": "https://arxiv.org/abs/2602.02960",
      "date": "2026-02-03",
      "authors_text": "Quanquan Peng, Yunfeng Lin, Yufei Xue, Jiangmiao Pang, Weinan Zhang",
      "is_highlight": false,
      "score": 90.0,
      "summary": "The EAGLE framework enables a unified policy for diverse humanoid robots, enhancing whole-body control through iterative generalist-specialist distillation without individual reward tuning.",
      "teaser_image": "https://arxiv.org/html/2602.02960/x1.png",
      "debug_abstract": "Humanoid Whole-Body Controllers trained with reinforcement learning (RL) have recently achieved remarkable performance, yet many target a single robot embodiment. Variations in dynamics, degrees of freedom (DoFs), and kinematic topology still hinder a single policy from commanding diverse humanoids. Moreover, obtaining a generalist policy that not only transfers across embodiments but also supports richer behaviors-beyond simple walking to squatting, leaning-remains especially challenging. In this work, we tackle these obstacles by introducing EAGLE, an iterative generalist-specialist distillation framework that produces a single unified policy that controls multiple heterogeneous humanoids without per-robot reward tuning. During each cycle, embodiment-specific specialists are forked from the current generalist, refined on their respective robots, and new skills are distilled back into the generalist by training on the pooled embodiment set. Repeating this loop until performance convergence produces a robust Whole-Body Controller validated on robots such as Unitree H1, G1, and Fourier N1. We conducted experiments on five different robots in simulation and four in real-world settings. Through quantitative evaluations, EAGLE achieves high tracking accuracy and robustness compared to other methods, marking a step toward scalable, fleet-level humanoid control. See more details at this https URL"
    },
    {
      "title": "Bongards at the Boundary of Perception and Reasoning: Programs or Language?",
      "url": "https://arxiv.org/abs/2602.03038",
      "date": "2026-02-03",
      "authors_text": "Cassidy Langenfeld, Claas Beger, Gloria Geng, Wasu Top Piriyakulkij, Keya Hu",
      "is_highlight": false,
      "score": 60.0,
      "summary": "This paper introduces a neurosymbolic method utilizing LLMs and Bayesian optimization to tackle Bongard problems, enhancing visual reasoning beyond conventional VLM capabilities.",
      "teaser_image": "https://arxiv.org/html/2602.03038/x3.png",
      "debug_abstract": "Vision-Language Models (VLMs) have made great strides in everyday visual tasks, such as captioning a natural image, or answering commonsense questions about such images. But humans possess the puzzling ability to deploy their visual reasoning abilities in radically new situations, a skill rigorously tested by the classic set of visual reasoning challenges known as the Bongard problems. We present a neurosymbolic approach to solving these problems: given a hypothesized solution rule for a Bongard problem, we leverage LLMs to generate parameterized programmatic representations for the rule and perform parameter fitting using Bayesian optimization. We evaluate our method on classifying Bongard problem images given the ground truth rule, as well as on solving the problems from scratch."
    },
    {
      "title": "Agentic Proposing: Enhancing Large Language Model Reasoning via Compositional Skill Synthesis",
      "url": "https://arxiv.org/abs/2602.03279",
      "date": "2026-02-03",
      "authors_text": "Zhengbo Jiao, Shaobo Wang, Zifan Zhang, Xuan Ren, Wei Wang",
      "is_highlight": false,
      "score": 71.0,
      "summary": "The paper introduces Agentic Proposing, a framework for generating high-quality synthetic training data that enhances reasoning in large language models, achieving superior performance with minimal human input.",
      "teaser_image": "https://arxiv.org/html/2602.03279/x2.png",
      "debug_abstract": "Advancing complex reasoning in large language models relies on high-quality, verifiable datasets, yet human annotation remains cost-prohibitive and difficult to scale. Current synthesis paradigms often face a recurring trade-off: maintaining structural validity typically restricts problem complexity, while relaxing constraints to increase difficulty frequently leads to inconsistent or unsolvable instances. To address this, we propose Agentic Proposing, a framework that models problem synthesis as a goal-driven sequential decision process where a specialized agent dynamically selects and composes modular reasoning skills. Through an iterative workflow of internal reflection and tool-use, we develop the Agentic-Proposer-4B using Multi-Granularity Policy Optimization (MGPO) to generate high-precision, verifiable training trajectories across mathematics, coding, and science. Empirical results demonstrate that downstream solvers trained on agent-synthesized data significantly outperform leading baselines and exhibit robust cross-domain generalization. Notably, a 30B solver trained on only 11,000 synthesized trajectories achieves a state-of-the-art 91.6% accuracy on AIME25, rivaling frontier-scale proprietary models such as GPT-5 and proving that a small volume of high-quality synthetic signals can effectively substitute for massive human-curated datasets."
    },
    {
      "title": "Socratic-Geo: Synthetic Data Generation and Geometric Reasoning via Multi-Agent Interaction",
      "url": "https://arxiv.org/abs/2602.03414",
      "date": "2026-02-03",
      "authors_text": "Zhengbo Jiao, Shaobo Wang, Zifan Zhang, Wei Wang, Bing Zhao",
      "is_highlight": false,
      "score": 58.0,
      "summary": "Socratic-Geo introduces a multi-agent framework for autonomous data synthesis and geometric reasoning, significantly improving model performance with minimal seed problems and surpassing existing benchmarks.",
      "teaser_image": "https://arxiv.org/html/2602.03414/x1.png",
      "debug_abstract": "Multimodal Large Language Models (MLLMs) have significantly advanced vision-language understanding. However, even state-of-the-art models struggle with geometric reasoning, revealing a critical bottleneck: the extreme scarcity of high-quality image-text pairs. Human annotation is prohibitively expensive, while automated methods fail to ensure fidelity and training effectiveness. Existing approaches either passively adapt to available images or employ inefficient random exploration with filtering, decoupling generation from learning needs. We propose Socratic-Geo, a fully autonomous framework that dynamically couples data synthesis with model learning through multi-agent interaction. The Teacher agent generates parameterized Python scripts with reflective feedback (Reflect for solvability, RePI for visual validity), ensuring image-text pair purity. The Solver agent optimizes reasoning through preference learning, with failure paths guiding Teacher&#39;s targeted augmentation. Independently, the Generator learns image generation capabilities on accumulated &#34;image-code-instruction&#34; triplets, distilling programmatic drawing intelligence into visual generation. Starting from only 108 seed problems, Socratic-Solver achieves 49.11 on six benchmarks using one-quarter of baseline data, surpassing strong baselines by 2.43 points. Socratic-Generator achieves 42.4% on GenExam, establishing new state-of-the-art for open-source models, surpassing Seedream-4.0 (39.8%) and approaching Gemini-2.5-Flash-Image (43.1%)."
    },
    {
      "title": "QVLA: Not All Channels Are Equal in Vision-Language-Action Model's Quantization",
      "url": "https://arxiv.org/abs/2602.03782",
      "date": "2026-02-03",
      "authors_text": "Yuhao Xu, Yantai Yang, Zhenyang Fan, Yufan Liu, Yuming Li",
      "is_highlight": false,
      "score": 47.0,
      "summary": "QVLA introduces an action-centric quantization framework for Vision-Language-Action models, optimizing channel-wise bit allocation to enhance performance and reduce resource demands in robotics.",
      "teaser_image": "https://arxiv.org/html/2602.03782/x2.png",
      "debug_abstract": "The advent of Vision-Language-Action (VLA) models represents a significant leap for embodied intelligence, yet their immense computational demands critically hinder deployment on resource-constrained robotic platforms. Intuitively, low-bit quantization is a prevalent and preferred technique for large-scale model compression. However, we find that a systematic analysis of VLA model&#39;s quantization is fundamentally lacking. We argue that naively applying uniform-bit quantization from Large Language Models (LLMs) to robotics is flawed, as these methods prioritize passive data fidelity while ignoring how minor action deviations compound into catastrophic task failures. To bridge this gap, we introduce QVLA, the first action-centric quantization framework specifically designed for embodied control. In a sharp departure from the rigid, uniform-bit quantization of LLM-based methods, QVLA introduces a highly granular, channel-wise bit allocation strategy. Its core mechanism is to directly measure the final action-space sensitivity when quantizing each individual channel to various bit-widths. This process yields a precise, per-channel importance metric that guides a global optimization, which elegantly unifies quantization and pruning (0-bit) into a single, cohesive framework. Extensive evaluations on different baselines demonstrate the superiority of our approach. In the LIBERO, the quantization version of OpenVLA-OFT with our method requires only 29.2% of the original model&#39;s VRAM while maintaining 98.9% of its original performance and achieving a 1.49x speedup. This translates to a 22.6% performance improvement over the LLM-derived method SmoothQuant. Our work establishes a new, principled foundation for compressing VLA models in robotics, paving the way for deploying powerful, large-scale models on real-world hardware. Code will be released."
    },
    {
      "title": "ConLA: Contrastive Latent Action Learning from Human Videos for Robotic Manipulation",
      "url": "https://arxiv.org/abs/2602.00557",
      "date": "2026-01-31",
      "authors_text": "Weisheng Dai, Kai Lan, Jianyi Zhou, Bo Zhao, Xiu Su",
      "is_highlight": true,
      "score": 91.0,
      "summary": "ConLA introduces a contrastive disentanglement framework for unsupervised robotic policy learning from human videos, enhancing transferability and performance beyond traditional robot trajectory pretraining.",
      "teaser_image": "https://arxiv.org/html/2602.00557/x3.png",
      "debug_abstract": "Vision-Language-Action (VLA) models achieve preliminary generalization through pretraining on large scale robot teleoperation datasets. However, acquiring datasets that comprehensively cover diverse tasks and environments is extremely costly and difficult to scale. In contrast, human demonstration videos offer a rich and scalable source of diverse scenes and manipulation behaviors, yet their lack of explicit action supervision hinders direct utilization. Prior work leverages VQ-VAE based frameworks to learn latent actions from human videos in an unsupervised manner. Nevertheless, since the training objective primarily focuses on reconstructing visual appearances rather than capturing inter-frame dynamics, the learned representations tend to rely on spurious visual cues, leading to shortcut learning and entangled latent representations that hinder transferability. To address this, we propose ConLA, an unsupervised pretraining framework for learning robotic policies from human videos. ConLA introduces a contrastive disentanglement mechanism that leverages action category priors and temporal cues to isolate motion dynamics from visual content, effectively mitigating shortcut learning. Extensive experiments show that ConLA achieves strong performance across diverse benchmarks. Notably, by pretraining solely on human videos, our method for the first time surpasses the performance obtained with real robot trajectory pretraining, highlighting its ability to extract pure and semantically consistent latent action representations for scalable robot learning."
    },
    {
      "title": "ProxyImg: Towards Highly-Controllable Image Representation via Hierarchical Disentangled Proxy Embedding",
      "url": "https://arxiv.org/abs/2602.01881",
      "date": "2026-02-02",
      "authors_text": "Ye Chen, Yupeng Zhu, Xiongzhen Zhang, Zhewen Wan, Yingzhe Li",
      "is_highlight": false,
      "score": 30.0,
      "summary": "The paper presents ProxyImg, a hierarchical proxy-based image representation that enables fine-grained, controllable editing by disentangling semantic, geometric, and textural attributes for high-fidelity reconstruction.",
      "teaser_image": "https://arxiv.org/html/2602.01881/x2.png",
      "debug_abstract": "Prevailing image representation methods, including explicit representations such as raster images and Gaussian primitives, as well as implicit representations such as latent images, either suffer from representation redundancy that leads to heavy manual editing effort, or lack a direct mapping from latent variables to semantic instances or parts, making fine-grained manipulation difficult. These limitations hinder efficient and controllable image and video editing. To address these issues, we propose a hierarchical proxy-based parametric image representation that disentangles semantic, geometric, and textural attributes into independent and manipulable parameter spaces. Based on a semantic-aware decomposition of the input image, our representation constructs hierarchical proxy geometries through adaptive Bezier fitting and iterative internal region subdivision and meshing. Multi-scale implicit texture parameters are embedded into the resulting geometry-aware distributed proxy nodes, enabling continuous high-fidelity reconstruction in the pixel domain and instance- or part-independent semantic editing. In addition, we introduce a locality-adaptive feature indexing mechanism to ensure spatial texture coherence, which further supports high-quality background completion without relying on generative models. Extensive experiments on image reconstruction and editing benchmarks, including ImageNet, OIR-Bench, and HumanEdit, demonstrate that our method achieves state-of-the-art rendering fidelity with significantly fewer parameters, while enabling intuitive, interactive, and physically plausible manipulation. Moreover, by integrating proxy nodes with Position-Based Dynamics, our framework supports real-time physics-driven animation using lightweight implicit rendering, achieving superior temporal consistency and visual realism compared with generative approaches."
    },
    {
      "title": "SurfSplat: Conquering Feedforward 2D Gaussian Splatting with Surface Continuity Priors",
      "url": "https://arxiv.org/abs/2602.02000",
      "date": "2026-02-02",
      "authors_text": "Bing He, Jingnan Gao, Yunuo Chen, Ning Cao, Gang Chen",
      "is_highlight": false,
      "score": 25.0,
      "summary": "SurfSplat introduces a feedforward 2D Gaussian Splatting framework that enhances 3D scene reconstruction by ensuring surface continuity and improving geometric precision over existing methods.",
      "teaser_image": "https://arxiv.org/html/2602.02000/x2.png",
      "debug_abstract": "Reconstructing 3D scenes from sparse images remains a challenging task due to the difficulty of recovering accurate geometry and texture without optimization. Recent approaches leverage generalizable models to generate 3D scenes using 3D Gaussian Splatting (3DGS) primitive. However, they often fail to produce continuous surfaces and instead yield discrete, color-biased point clouds that appear plausible at normal resolution but reveal severe artifacts under close-up views. To address this issue, we present SurfSplat, a feedforward framework based on 2D Gaussian Splatting (2DGS) primitive, which provides stronger anisotropy and higher geometric precision. By incorporating a surface continuity prior and a forced alpha blending strategy, SurfSplat reconstructs coherent geometry together with faithful textures. Furthermore, we introduce High-Resolution Rendering Consistency (HRRC), a new evaluation metric designed to evaluate high-resolution reconstruction quality. Extensive experiments on RealEstate10K, DL3DV, and ScanNet demonstrate that SurfSplat consistently outperforms prior methods on both standard metrics and HRRC, establishing a robust solution for high-fidelity 3D reconstruction from sparse inputs. Project page: this https URL"
    },
    {
      "title": "PLANING: A Loosely Coupled Triangle-Gaussian Framework for Streaming 3D Reconstruction",
      "url": "https://arxiv.org/abs/2601.22046",
      "date": "2026-01-29",
      "authors_text": "Changjian Jiang, Kerui Ren, Xudong Li, Kaiwen Song, Linning Xu",
      "is_highlight": false,
      "score": 66.0,
      "summary": "PLANING is a novel streaming 3D reconstruction framework that efficiently combines geometric primitives and neural Gaussians, enhancing quality and speed while reducing redundancy.",
      "teaser_image": "https://arxiv.org/html/2601.22046/x3.png",
      "debug_abstract": "Streaming reconstruction from monocular image sequences remains challenging, as existing methods typically favor either high-quality rendering or accurate geometry, but rarely both. We present PLANING, an efficient on-the-fly reconstruction framework built on a hybrid representation that loosely couples explicit geometric primitives with neural Gaussians, enabling geometry and appearance to be modeled in a decoupled manner. This decoupling supports an online initialization and optimization strategy that separates geometry and appearance updates, yielding stable streaming reconstruction with substantially reduced structural redundancy. PLANING improves dense mesh Chamfer-L2 by 18.52% over PGSR, surpasses ARTDECO by 1.31 dB PSNR, and reconstructs ScanNetV2 scenes in under 100 seconds, over 5x faster than 2D Gaussian Splatting, while matching the quality of offline per-scene optimization. Beyond reconstruction quality, the structural clarity and computational efficiency of \\modelname~make it well suited for a broad range of downstream applications, such as enabling large-scale scene modeling and simulation-ready environments for embodied AI. Project page: this https URL ."
    },
    {
      "title": "DynaWeb: Model-Based Reinforcement Learning of Web Agents",
      "url": "https://arxiv.org/abs/2601.22149",
      "date": "2026-01-29",
      "authors_text": "Hang Ding, Peidong Liu, Junqiao Wang, Ziwei Ji, Meng Cao",
      "is_highlight": false,
      "score": 77.0,
      "summary": "DynaWeb introduces a model-based reinforcement learning framework that enhances web agent training by simulating interactions with a web world model, improving efficiency and performance.",
      "teaser_image": "https://arxiv.org/html/2601.22149/figures/dynaweb.png",
      "debug_abstract": "The development of autonomous web agents, powered by Large Language Models (LLMs) and reinforcement learning (RL), represents a significant step towards general-purpose AI assistants. However, training these agents is severely hampered by the challenges of interacting with the live internet, which is inefficient, costly, and fraught with risks. Model-based reinforcement learning (MBRL) offers a promising solution by learning a world model of the environment to enable simulated interaction. This paper introduces DynaWeb, a novel MBRL framework that trains web agents through interacting with a web world model trained to predict naturalistic web page representations given agent actions. This model serves as a synthetic web environment where an agent policy can dream by generating vast quantities of rollout action trajectories for efficient online reinforcement learning. Beyond free policy rollouts, DynaWeb incorporates real expert trajectories from training data, which are randomly interleaved with on-policy rollouts during training to improve stability and sample efficiency. Experiments conducted on the challenging WebArena and WebVoyager benchmarks demonstrate that DynaWeb consistently and significantly improves the performance of state-of-the-art open-source web agent models. Our findings establish the viability of training web agents through imagination, offering a scalable and efficient way to scale up online agentic RL."
    },
    {
      "title": "E-mem: Multi-agent based Episodic Context Reconstruction for LLM Agent Memory",
      "url": "https://arxiv.org/abs/2601.21714",
      "date": "2026-01-29",
      "authors_text": "Kaixiang Wang, Yidan Lin, Jiong Lou, Zhaojiacheng Zhou, Bunyod Suvonov",
      "is_highlight": false,
      "score": 71.0,
      "summary": "E-mem introduces a multi-agent framework for episodic context reconstruction in LLMs, enhancing reasoning accuracy and efficiency while preserving contextual integrity.",
      "teaser_image": "https://arxiv.org/html/2601.21714/x2.png",
      "debug_abstract": "The evolution of Large Language Model (LLM) agents towards System~2 reasoning, characterized by deliberative, high-precision problem-solving, requires maintaining rigorous logical integrity over extended horizons. However, prevalent memory preprocessing paradigms suffer from destructive de-contextualization. By compressing complex sequential dependencies into pre-defined structures (e.g., embeddings or graphs), these methods sever the contextual integrity essential for deep reasoning. To address this, we propose E-mem, a framework shifting from Memory Preprocessing to Episodic Context Reconstruction. Inspired by biological engrams, E-mem employs a heterogeneous hierarchical architecture where multiple assistant agents maintain uncompressed memory contexts, while a central master agent orchestrates global planning. Unlike passive retrieval, our mechanism empowers assistants to locally reason within activated segments, extracting context-aware evidence before aggregation. Evaluations on the LoCoMo benchmark demonstrate that E-mem achieves over 54\\% F1, surpassing the state-of-the-art GAM by 7.75\\%, while reducing token cost by over 70\\%."
    },
    {
      "title": "TouchGuide: Inference-Time Steering of Visuomotor Policies via Touch Guidance",
      "url": "https://arxiv.org/abs/2601.20239",
      "date": "2026-01-28",
      "authors_text": "Zhemeng Zhang, Jiahua Ma, Xincheng Yang, Xin Wen, Yuzhi Zhang",
      "is_highlight": false,
      "score": 90.0,
      "summary": "TouchGuide enhances robot manipulation by integrating tactile feedback with visual inputs to refine actions during inference, outperforming existing visuo-tactile policies in complex tasks.",
      "teaser_image": "https://arxiv.org/html/2601.20239/x2.png",
      "debug_abstract": "Fine-grained and contact-rich manipulation remain challenging for robots, largely due to the underutilization of tactile feedback. To address this, we introduce TouchGuide, a novel cross-policy visuo-tactile fusion paradigm that fuses modalities within a low-dimensional action space. Specifically, TouchGuide operates in two stages to guide a pre-trained diffusion or flow-matching visuomotor policy at inference time. First, the policy produces a coarse, visually-plausible action using only visual inputs during early sampling. Second, a task-specific Contact Physical Model (CPM) provides tactile guidance to steer and refine the action, ensuring it aligns with realistic physical contact conditions. Trained through contrastive learning on limited expert demonstrations, the CPM provides a tactile-informed feasibility score to steer the sampling process toward refined actions that satisfy physical contact constraints. Furthermore, to facilitate TouchGuide training with high-quality and cost-effective data, we introduce TacUMI, a data collection system. TacUMI achieves a favorable trade-off between precision and affordability; by leveraging rigid fingertips, it obtains direct tactile feedback, thereby enabling the collection of reliable tactile data. Extensive experiments on five challenging contact-rich tasks, such as shoe lacing and chip handover, show that TouchGuide consistently and significantly outperforms state-of-the-art visuo-tactile policies."
    },
    {
      "title": "Innovator-VL: A Multimodal Large Language Model for Scientific Discovery",
      "url": "https://arxiv.org/abs/2601.19325",
      "date": "2026-01-27",
      "authors_text": "Zichen Wen, Boxue Yang, Shuang Chen, Yaojie Zhang, Yuhang Han",
      "is_highlight": false,
      "score": 78.0,
      "summary": "Innovator-VL is a multimodal large language model that enhances scientific discovery through efficient training, transparency, and strong generalization without relying on extensive pretraining.",
      "teaser_image": "https://arxiv.org/html/2601.19325/x8.png",
      "debug_abstract": "We present Innovator-VL, a scientific multimodal large language model designed to advance understanding and reasoning across diverse scientific domains while maintaining excellent performance on general vision tasks. Contrary to the trend of relying on massive domain-specific pretraining and opaque pipelines, our work demonstrates that principled training design and transparent methodology can yield strong scientific intelligence with substantially reduced data requirements. (i) First, we provide a fully transparent, end-to-end reproducible training pipeline, covering data collection, cleaning, preprocessing, supervised fine-tuning, reinforcement learning, and evaluation, along with detailed optimization recipes. This facilitates systematic extension by the community. (ii) Second, Innovator-VL exhibits remarkable data efficiency, achieving competitive performance on various scientific tasks using fewer than five million curated samples without large-scale pretraining. These results highlight that effective reasoning can be achieved through principled data selection rather than indiscriminate scaling. (iii) Third, Innovator-VL demonstrates strong generalization, achieving competitive performance on general vision, multimodal reasoning, and scientific benchmarks. This indicates that scientific alignment can be integrated into a unified model without compromising general-purpose capabilities. Our practices suggest that efficient, reproducible, and high-performing scientific multimodal models can be built even without large-scale data, providing a practical foundation for future research."
    },
    {
      "title": "PILOT: A Perceptive Integrated Low-level Controller for Loco-manipulation over Unstructured Scenes",
      "url": "https://arxiv.org/abs/2601.17440",
      "date": "2026-01-24",
      "authors_text": "Xinru Cui, Linxi Feng, Yixuan Zhou, Haoqi Han, Zhe Liu",
      "is_highlight": true,
      "score": 90.0,
      "summary": "PILOT is a unified reinforcement learning framework that integrates perceptive locomotion and whole-body control for stable loco-manipulation in unstructured environments, outperforming existing controllers.",
      "debug_abstract": "Humanoid robots hold great potential for diverse interactions and daily service tasks within human-centered environments, necessitating controllers that seamlessly integrate precise locomotion with dexterous manipulation. However, most existing whole-body controllers lack exteroceptive awareness of the surrounding environment, rendering them insufficient for stable task execution in complex, unstructured this http URL address this challenge, we propose PILOT, a unified single-stage reinforcement learning (RL) framework tailored for perceptive loco-manipulation, which synergizes perceptive locomotion and expansive whole-body control within a single policy. To enhance terrain awareness and ensure precise foot placement, we design a cross-modal context encoder that fuses prediction-based proprioceptive features with attention-based perceptive representations. Furthermore, we introduce a Mixture-of-Experts (MoE) policy architecture to coordinate diverse motor skills, facilitating better specialization across distinct motion patterns. Extensive experiments in both simulation and on the physical Unitree G1 humanoid robot validate the efficacy of our framework. PILOT demonstrates superior stability, command tracking precision, and terrain traversability compared to existing baselines. These results highlight its potential to serve as a robust, foundational low-level controller for loco-manipulation in unstructured scenes."
    },
    {
      "title": "BMDS-Net: A Bayesian Multi-Modal Deep Supervision Network for Robust Brain Tumor Segmentation",
      "url": "https://arxiv.org/abs/2601.17504",
      "date": "2026-01-24",
      "authors_text": "Yan Zhou, Zhen Huang, Yingqiu Li, Yue Ouyang, Suncheng Xiang",
      "is_highlight": false,
      "score": 50.0,
      "summary": "BMDS-Net enhances brain tumor segmentation by integrating robust feature learning and Bayesian uncertainty estimation, ensuring reliable performance despite missing MRI modalities.",
      "debug_abstract": "Accurate brain tumor segmentation from multi-modal magnetic resonance imaging (MRI) is a prerequisite for precise radiotherapy planning and surgical navigation. While recent Transformer-based models such as Swin UNETR have achieved impressive benchmark performance, their clinical utility is often compromised by two critical issues: sensitivity to missing modalities (common in clinical practice) and a lack of confidence calibration. Merely chasing higher Dice scores on idealized data fails to meet the safety requirements of real-world medical deployment. In this work, we propose BMDS-Net, a unified framework that prioritizes clinical robustness and trustworthiness over simple metric maximization. Our contribution is three-fold. First, we construct a robust deterministic backbone by integrating a Zero-Init Multimodal Contextual Fusion (MMCF) module and a Residual-Gated Deep Decoder Supervision (DDS) mechanism, enabling stable feature learning and precise boundary delineation with significantly reduced Hausdorff Distance, even under modality corruption. Second, and most importantly, we introduce a memory-efficient Bayesian fine-tuning strategy that transforms the network into a probabilistic predictor, providing voxel-wise uncertainty maps to highlight potential errors for clinicians. Third, comprehensive experiments on the BraTS 2021 dataset demonstrate that BMDS-Net not only maintains competitive accuracy but, more importantly, exhibits superior stability in missing-modality scenarios where baseline models fail. The source code is publicly available at this https URL."
    },
    {
      "title": "QualiRAG: Retrieval-Augmented Generation for Visual Quality Understanding",
      "url": "https://arxiv.org/abs/2601.18195",
      "date": "2026-01-26",
      "authors_text": "Linhan Cao, Wei Sun, Weixia Zhang, Xiangyang Zhu, Kaiwei Zhang",
      "is_highlight": false,
      "score": 66.0,
      "summary": "QualiRAG introduces a training-free framework for visual quality assessment that utilizes dynamic retrieval and multimodal knowledge to enhance interpretability and performance without task-specific training.",
      "debug_abstract": "Visual quality assessment (VQA) is increasingly shifting from scalar score prediction toward interpretable quality understanding -- a paradigm that demands \\textit{fine-grained spatiotemporal perception} and \\textit{auxiliary contextual information}. Current approaches rely on supervised fine-tuning or reinforcement learning on curated instruction datasets, which involve labor-intensive annotation and are prone to dataset-specific biases. To address these challenges, we propose \\textbf{QualiRAG}, a \\textit{training-free} \\textbf{R}etrieval-\\textbf{A}ugmented \\textbf{G}eneration \\textbf{(RAG)} framework that systematically leverages the latent perceptual knowledge of large multimodal models (LMMs) for visual quality perception. Unlike conventional RAG that retrieves from static corpora, QualiRAG dynamically generates auxiliary knowledge by decomposing questions into structured requests and constructing four complementary knowledge sources: \\textit{visual metadata}, \\textit{subject localization}, \\textit{global quality summaries}, and \\textit{local quality descriptions}, followed by relevance-aware retrieval for evidence-grounded reasoning. Extensive experiments show that QualiRAG achieves substantial improvements over open-source general-purpose LMMs and VQA-finetuned LMMs on visual quality understanding tasks, and delivers competitive performance on visual quality comparison tasks, demonstrating robust quality assessment capabilities without any task-specific training. The code will be publicly available at this https URL."
    },
    {
      "title": "ExoGS: A 4D Real-to-Sim-to-Real Framework for Scalable Manipulation Data Collection",
      "url": "https://arxiv.org/abs/2601.18629",
      "date": "2026-01-26",
      "authors_text": "Yiming Wang, Ruogu Zhang, Minyang Li, Hao Shi, Junbo Wang",
      "is_highlight": false,
      "score": 93.0,
      "summary": "ExoGS introduces a 4D framework for efficient manipulation data collection by seamlessly transferring real-world interactions to simulation, enhancing policy learning and generalization.",
      "debug_abstract": "Real-to-Sim-to-Real technique is gaining increasing interest for robotic manipulation, as it can generate scalable data in simulation while having narrower sim-to-real gap. However, previous methods mainly focused on environment-level visual real-to-sim transfer, ignoring the transfer of interactions, which could be challenging and inefficient to obtain purely in simulation especially for contact-rich tasks. We propose ExoGS, a robot-free 4D Real-to-Sim-to-Real framework that captures both static environments and dynamic interactions in the real world and transfers them seamlessly to a simulated environment. It provides a new solution for scalable manipulation data collection and policy learning. ExoGS employs a self-designed robot-isomorphic passive exoskeleton AirExo-3 to capture kinematically consistent trajectories with millimeter-level accuracy and synchronized RGB observations during direct human demonstrations. The robot, objects, and environment are reconstructed as editable 3D Gaussian Splatting assets, enabling geometry-consistent replay and large-scale data augmentation. Additionally, a lightweight Mask Adapter injects instance-level semantics into the policy to enhance robustness under visual domain shifts. Real-world experiments demonstrate that ExoGS significantly improves data efficiency and policy generalization compared to teleoperation-based baselines. Code and hardware files have been released on this https URL."
    },
    {
      "title": "Advances and Innovations in the Multi-Agent Robotic System (MARS) Challenge",
      "url": "https://arxiv.org/abs/2601.18733",
      "date": "2026-01-26",
      "authors_text": "Li Kang, Heng Zhou, Xiufeng Song, Rui Li, Bruno N. Y. Chen",
      "is_highlight": false,
      "score": 81.0,
      "summary": "The MARS Challenge at NeurIPS 2025 aims to enhance multi-agent collaboration in embodied AI through competition in planning and control using vision-language models.",
      "debug_abstract": "Recent advancements in multimodal large language models and vision-languageaction models have significantly driven progress in Embodied AI. As the field transitions toward more complex task scenarios, multi-agent system frameworks are becoming essential for achieving scalable, efficient, and collaborative solutions. This shift is fueled by three primary factors: increasing agent capabilities, enhancing system efficiency through task delegation, and enabling advanced human-agent interactions. To address the challenges posed by multi-agent collaboration, we propose the Multi-Agent Robotic System (MARS) Challenge, held at the NeurIPS 2025 Workshop on SpaVLE. The competition focuses on two critical areas: planning and control, where participants explore multi-agent embodied planning using vision-language models (VLMs) to coordinate tasks and policy execution to perform robotic manipulation in dynamic environments. By evaluating solutions submitted by participants, the challenge provides valuable insights into the design and coordination of embodied multi-agent systems, contributing to the future development of advanced collaborative AI systems."
    }
  ],
  "赵波老师实验室": [
    {
      "title": "Embodiment-Aware Generalist Specialist Distillation for Unified Humanoid Whole-Body Control",
      "url": "https://arxiv.org/abs/2602.02960",
      "date": "2026-02-03",
      "authors_text": "Quanquan Peng, Yunfeng Lin, Yufei Xue, Jiangmiao Pang, Weinan Zhang",
      "is_highlight": false,
      "score": 90.0,
      "summary": "The EAGLE framework enables a unified policy for diverse humanoid robots, enhancing whole-body control through iterative generalist-specialist distillation without individual reward tuning.",
      "teaser_image": "https://arxiv.org/html/2602.02960/x1.png",
      "debug_abstract": "Humanoid Whole-Body Controllers trained with reinforcement learning (RL) have recently achieved remarkable performance, yet many target a single robot embodiment. Variations in dynamics, degrees of freedom (DoFs), and kinematic topology still hinder a single policy from commanding diverse humanoids. Moreover, obtaining a generalist policy that not only transfers across embodiments but also supports richer behaviors-beyond simple walking to squatting, leaning-remains especially challenging. In this work, we tackle these obstacles by introducing EAGLE, an iterative generalist-specialist distillation framework that produces a single unified policy that controls multiple heterogeneous humanoids without per-robot reward tuning. During each cycle, embodiment-specific specialists are forked from the current generalist, refined on their respective robots, and new skills are distilled back into the generalist by training on the pooled embodiment set. Repeating this loop until performance convergence produces a robust Whole-Body Controller validated on robots such as Unitree H1, G1, and Fourier N1. We conducted experiments on five different robots in simulation and four in real-world settings. Through quantitative evaluations, EAGLE achieves high tracking accuracy and robustness compared to other methods, marking a step toward scalable, fleet-level humanoid control. See more details at this https URL"
    },
    {
      "title": "Bongards at the Boundary of Perception and Reasoning: Programs or Language?",
      "url": "https://arxiv.org/abs/2602.03038",
      "date": "2026-02-03",
      "authors_text": "Cassidy Langenfeld, Claas Beger, Gloria Geng, Wasu Top Piriyakulkij, Keya Hu",
      "is_highlight": false,
      "score": 60.0,
      "summary": "This paper introduces a neurosymbolic method utilizing LLMs and Bayesian optimization to tackle Bongard problems, enhancing visual reasoning beyond conventional VLM capabilities.",
      "teaser_image": "https://arxiv.org/html/2602.03038/x3.png",
      "debug_abstract": "Vision-Language Models (VLMs) have made great strides in everyday visual tasks, such as captioning a natural image, or answering commonsense questions about such images. But humans possess the puzzling ability to deploy their visual reasoning abilities in radically new situations, a skill rigorously tested by the classic set of visual reasoning challenges known as the Bongard problems. We present a neurosymbolic approach to solving these problems: given a hypothesized solution rule for a Bongard problem, we leverage LLMs to generate parameterized programmatic representations for the rule and perform parameter fitting using Bayesian optimization. We evaluate our method on classifying Bongard problem images given the ground truth rule, as well as on solving the problems from scratch."
    },
    {
      "title": "Agentic Proposing: Enhancing Large Language Model Reasoning via Compositional Skill Synthesis",
      "url": "https://arxiv.org/abs/2602.03279",
      "date": "2026-02-03",
      "authors_text": "Zhengbo Jiao, Shaobo Wang, Zifan Zhang, Xuan Ren, Wei Wang",
      "is_highlight": false,
      "score": 71.0,
      "summary": "The paper introduces Agentic Proposing, a framework for generating high-quality synthetic training data that enhances reasoning in large language models, achieving superior performance with minimal human input.",
      "teaser_image": "https://arxiv.org/html/2602.03279/x2.png",
      "debug_abstract": "Advancing complex reasoning in large language models relies on high-quality, verifiable datasets, yet human annotation remains cost-prohibitive and difficult to scale. Current synthesis paradigms often face a recurring trade-off: maintaining structural validity typically restricts problem complexity, while relaxing constraints to increase difficulty frequently leads to inconsistent or unsolvable instances. To address this, we propose Agentic Proposing, a framework that models problem synthesis as a goal-driven sequential decision process where a specialized agent dynamically selects and composes modular reasoning skills. Through an iterative workflow of internal reflection and tool-use, we develop the Agentic-Proposer-4B using Multi-Granularity Policy Optimization (MGPO) to generate high-precision, verifiable training trajectories across mathematics, coding, and science. Empirical results demonstrate that downstream solvers trained on agent-synthesized data significantly outperform leading baselines and exhibit robust cross-domain generalization. Notably, a 30B solver trained on only 11,000 synthesized trajectories achieves a state-of-the-art 91.6% accuracy on AIME25, rivaling frontier-scale proprietary models such as GPT-5 and proving that a small volume of high-quality synthetic signals can effectively substitute for massive human-curated datasets."
    },
    {
      "title": "Socratic-Geo: Synthetic Data Generation and Geometric Reasoning via Multi-Agent Interaction",
      "url": "https://arxiv.org/abs/2602.03414",
      "date": "2026-02-03",
      "authors_text": "Zhengbo Jiao, Shaobo Wang, Zifan Zhang, Wei Wang, Bing Zhao",
      "is_highlight": false,
      "score": 58.0,
      "summary": "Socratic-Geo introduces a multi-agent framework for autonomous data synthesis and geometric reasoning, significantly improving model performance with minimal seed problems and surpassing existing benchmarks.",
      "teaser_image": "https://arxiv.org/html/2602.03414/x1.png",
      "debug_abstract": "Multimodal Large Language Models (MLLMs) have significantly advanced vision-language understanding. However, even state-of-the-art models struggle with geometric reasoning, revealing a critical bottleneck: the extreme scarcity of high-quality image-text pairs. Human annotation is prohibitively expensive, while automated methods fail to ensure fidelity and training effectiveness. Existing approaches either passively adapt to available images or employ inefficient random exploration with filtering, decoupling generation from learning needs. We propose Socratic-Geo, a fully autonomous framework that dynamically couples data synthesis with model learning through multi-agent interaction. The Teacher agent generates parameterized Python scripts with reflective feedback (Reflect for solvability, RePI for visual validity), ensuring image-text pair purity. The Solver agent optimizes reasoning through preference learning, with failure paths guiding Teacher&#39;s targeted augmentation. Independently, the Generator learns image generation capabilities on accumulated &#34;image-code-instruction&#34; triplets, distilling programmatic drawing intelligence into visual generation. Starting from only 108 seed problems, Socratic-Solver achieves 49.11 on six benchmarks using one-quarter of baseline data, surpassing strong baselines by 2.43 points. Socratic-Generator achieves 42.4% on GenExam, establishing new state-of-the-art for open-source models, surpassing Seedream-4.0 (39.8%) and approaching Gemini-2.5-Flash-Image (43.1%)."
    },
    {
      "title": "QVLA: Not All Channels Are Equal in Vision-Language-Action Model's Quantization",
      "url": "https://arxiv.org/abs/2602.03782",
      "date": "2026-02-03",
      "authors_text": "Yuhao Xu, Yantai Yang, Zhenyang Fan, Yufan Liu, Yuming Li",
      "is_highlight": false,
      "score": 47.0,
      "summary": "QVLA introduces an action-centric quantization framework for Vision-Language-Action models, optimizing channel-wise bit allocation to enhance performance and reduce resource demands in robotics.",
      "teaser_image": "https://arxiv.org/html/2602.03782/x2.png",
      "debug_abstract": "The advent of Vision-Language-Action (VLA) models represents a significant leap for embodied intelligence, yet their immense computational demands critically hinder deployment on resource-constrained robotic platforms. Intuitively, low-bit quantization is a prevalent and preferred technique for large-scale model compression. However, we find that a systematic analysis of VLA model&#39;s quantization is fundamentally lacking. We argue that naively applying uniform-bit quantization from Large Language Models (LLMs) to robotics is flawed, as these methods prioritize passive data fidelity while ignoring how minor action deviations compound into catastrophic task failures. To bridge this gap, we introduce QVLA, the first action-centric quantization framework specifically designed for embodied control. In a sharp departure from the rigid, uniform-bit quantization of LLM-based methods, QVLA introduces a highly granular, channel-wise bit allocation strategy. Its core mechanism is to directly measure the final action-space sensitivity when quantizing each individual channel to various bit-widths. This process yields a precise, per-channel importance metric that guides a global optimization, which elegantly unifies quantization and pruning (0-bit) into a single, cohesive framework. Extensive evaluations on different baselines demonstrate the superiority of our approach. In the LIBERO, the quantization version of OpenVLA-OFT with our method requires only 29.2% of the original model&#39;s VRAM while maintaining 98.9% of its original performance and achieving a 1.49x speedup. This translates to a 22.6% performance improvement over the LLM-derived method SmoothQuant. Our work establishes a new, principled foundation for compressing VLA models in robotics, paving the way for deploying powerful, large-scale models on real-world hardware. Code will be released."
    },
    {
      "title": "ConLA: Contrastive Latent Action Learning from Human Videos for Robotic Manipulation",
      "url": "https://arxiv.org/abs/2602.00557",
      "date": "2026-01-31",
      "authors_text": "Weisheng Dai, Kai Lan, Jianyi Zhou, Bo Zhao, Xiu Su",
      "is_highlight": true,
      "score": 91.0,
      "summary": "ConLA introduces a contrastive disentanglement framework for unsupervised robotic policy learning from human videos, enhancing transferability and performance beyond traditional robot trajectory pretraining.",
      "teaser_image": "https://arxiv.org/html/2602.00557/x3.png",
      "debug_abstract": "Vision-Language-Action (VLA) models achieve preliminary generalization through pretraining on large scale robot teleoperation datasets. However, acquiring datasets that comprehensively cover diverse tasks and environments is extremely costly and difficult to scale. In contrast, human demonstration videos offer a rich and scalable source of diverse scenes and manipulation behaviors, yet their lack of explicit action supervision hinders direct utilization. Prior work leverages VQ-VAE based frameworks to learn latent actions from human videos in an unsupervised manner. Nevertheless, since the training objective primarily focuses on reconstructing visual appearances rather than capturing inter-frame dynamics, the learned representations tend to rely on spurious visual cues, leading to shortcut learning and entangled latent representations that hinder transferability. To address this, we propose ConLA, an unsupervised pretraining framework for learning robotic policies from human videos. ConLA introduces a contrastive disentanglement mechanism that leverages action category priors and temporal cues to isolate motion dynamics from visual content, effectively mitigating shortcut learning. Extensive experiments show that ConLA achieves strong performance across diverse benchmarks. Notably, by pretraining solely on human videos, our method for the first time surpasses the performance obtained with real robot trajectory pretraining, highlighting its ability to extract pure and semantically consistent latent action representations for scalable robot learning."
    },
    {
      "title": "ProxyImg: Towards Highly-Controllable Image Representation via Hierarchical Disentangled Proxy Embedding",
      "url": "https://arxiv.org/abs/2602.01881",
      "date": "2026-02-02",
      "authors_text": "Ye Chen, Yupeng Zhu, Xiongzhen Zhang, Zhewen Wan, Yingzhe Li",
      "is_highlight": false,
      "score": 30.0,
      "summary": "The paper presents ProxyImg, a hierarchical proxy-based image representation that enables fine-grained, controllable editing by disentangling semantic, geometric, and textural attributes for high-fidelity reconstruction.",
      "teaser_image": "https://arxiv.org/html/2602.01881/x2.png",
      "debug_abstract": "Prevailing image representation methods, including explicit representations such as raster images and Gaussian primitives, as well as implicit representations such as latent images, either suffer from representation redundancy that leads to heavy manual editing effort, or lack a direct mapping from latent variables to semantic instances or parts, making fine-grained manipulation difficult. These limitations hinder efficient and controllable image and video editing. To address these issues, we propose a hierarchical proxy-based parametric image representation that disentangles semantic, geometric, and textural attributes into independent and manipulable parameter spaces. Based on a semantic-aware decomposition of the input image, our representation constructs hierarchical proxy geometries through adaptive Bezier fitting and iterative internal region subdivision and meshing. Multi-scale implicit texture parameters are embedded into the resulting geometry-aware distributed proxy nodes, enabling continuous high-fidelity reconstruction in the pixel domain and instance- or part-independent semantic editing. In addition, we introduce a locality-adaptive feature indexing mechanism to ensure spatial texture coherence, which further supports high-quality background completion without relying on generative models. Extensive experiments on image reconstruction and editing benchmarks, including ImageNet, OIR-Bench, and HumanEdit, demonstrate that our method achieves state-of-the-art rendering fidelity with significantly fewer parameters, while enabling intuitive, interactive, and physically plausible manipulation. Moreover, by integrating proxy nodes with Position-Based Dynamics, our framework supports real-time physics-driven animation using lightweight implicit rendering, achieving superior temporal consistency and visual realism compared with generative approaches."
    },
    {
      "title": "SurfSplat: Conquering Feedforward 2D Gaussian Splatting with Surface Continuity Priors",
      "url": "https://arxiv.org/abs/2602.02000",
      "date": "2026-02-02",
      "authors_text": "Bing He, Jingnan Gao, Yunuo Chen, Ning Cao, Gang Chen",
      "is_highlight": false,
      "score": 25.0,
      "summary": "SurfSplat introduces a feedforward 2D Gaussian Splatting framework that enhances 3D scene reconstruction by ensuring surface continuity and improving geometric precision over existing methods.",
      "teaser_image": "https://arxiv.org/html/2602.02000/x2.png",
      "debug_abstract": "Reconstructing 3D scenes from sparse images remains a challenging task due to the difficulty of recovering accurate geometry and texture without optimization. Recent approaches leverage generalizable models to generate 3D scenes using 3D Gaussian Splatting (3DGS) primitive. However, they often fail to produce continuous surfaces and instead yield discrete, color-biased point clouds that appear plausible at normal resolution but reveal severe artifacts under close-up views. To address this issue, we present SurfSplat, a feedforward framework based on 2D Gaussian Splatting (2DGS) primitive, which provides stronger anisotropy and higher geometric precision. By incorporating a surface continuity prior and a forced alpha blending strategy, SurfSplat reconstructs coherent geometry together with faithful textures. Furthermore, we introduce High-Resolution Rendering Consistency (HRRC), a new evaluation metric designed to evaluate high-resolution reconstruction quality. Extensive experiments on RealEstate10K, DL3DV, and ScanNet demonstrate that SurfSplat consistently outperforms prior methods on both standard metrics and HRRC, establishing a robust solution for high-fidelity 3D reconstruction from sparse inputs. Project page: this https URL"
    },
    {
      "title": "PLANING: A Loosely Coupled Triangle-Gaussian Framework for Streaming 3D Reconstruction",
      "url": "https://arxiv.org/abs/2601.22046",
      "date": "2026-01-29",
      "authors_text": "Changjian Jiang, Kerui Ren, Xudong Li, Kaiwen Song, Linning Xu",
      "is_highlight": false,
      "score": 66.0,
      "summary": "PLANING is a novel streaming 3D reconstruction framework that efficiently combines geometric primitives and neural Gaussians, enhancing quality and speed while reducing redundancy.",
      "teaser_image": "https://arxiv.org/html/2601.22046/x3.png",
      "debug_abstract": "Streaming reconstruction from monocular image sequences remains challenging, as existing methods typically favor either high-quality rendering or accurate geometry, but rarely both. We present PLANING, an efficient on-the-fly reconstruction framework built on a hybrid representation that loosely couples explicit geometric primitives with neural Gaussians, enabling geometry and appearance to be modeled in a decoupled manner. This decoupling supports an online initialization and optimization strategy that separates geometry and appearance updates, yielding stable streaming reconstruction with substantially reduced structural redundancy. PLANING improves dense mesh Chamfer-L2 by 18.52% over PGSR, surpasses ARTDECO by 1.31 dB PSNR, and reconstructs ScanNetV2 scenes in under 100 seconds, over 5x faster than 2D Gaussian Splatting, while matching the quality of offline per-scene optimization. Beyond reconstruction quality, the structural clarity and computational efficiency of \\modelname~make it well suited for a broad range of downstream applications, such as enabling large-scale scene modeling and simulation-ready environments for embodied AI. Project page: this https URL ."
    },
    {
      "title": "DynaWeb: Model-Based Reinforcement Learning of Web Agents",
      "url": "https://arxiv.org/abs/2601.22149",
      "date": "2026-01-29",
      "authors_text": "Hang Ding, Peidong Liu, Junqiao Wang, Ziwei Ji, Meng Cao",
      "is_highlight": false,
      "score": 77.0,
      "summary": "DynaWeb introduces a model-based reinforcement learning framework that enhances web agent training by simulating interactions with a web world model, improving efficiency and performance.",
      "teaser_image": "https://arxiv.org/html/2601.22149/figures/dynaweb.png",
      "debug_abstract": "The development of autonomous web agents, powered by Large Language Models (LLMs) and reinforcement learning (RL), represents a significant step towards general-purpose AI assistants. However, training these agents is severely hampered by the challenges of interacting with the live internet, which is inefficient, costly, and fraught with risks. Model-based reinforcement learning (MBRL) offers a promising solution by learning a world model of the environment to enable simulated interaction. This paper introduces DynaWeb, a novel MBRL framework that trains web agents through interacting with a web world model trained to predict naturalistic web page representations given agent actions. This model serves as a synthetic web environment where an agent policy can dream by generating vast quantities of rollout action trajectories for efficient online reinforcement learning. Beyond free policy rollouts, DynaWeb incorporates real expert trajectories from training data, which are randomly interleaved with on-policy rollouts during training to improve stability and sample efficiency. Experiments conducted on the challenging WebArena and WebVoyager benchmarks demonstrate that DynaWeb consistently and significantly improves the performance of state-of-the-art open-source web agent models. Our findings establish the viability of training web agents through imagination, offering a scalable and efficient way to scale up online agentic RL."
    },
    {
      "title": "E-mem: Multi-agent based Episodic Context Reconstruction for LLM Agent Memory",
      "url": "https://arxiv.org/abs/2601.21714",
      "date": "2026-01-29",
      "authors_text": "Kaixiang Wang, Yidan Lin, Jiong Lou, Zhaojiacheng Zhou, Bunyod Suvonov",
      "is_highlight": false,
      "score": 71.0,
      "summary": "E-mem introduces a multi-agent framework for episodic context reconstruction in LLMs, enhancing reasoning accuracy and efficiency while preserving contextual integrity.",
      "teaser_image": "https://arxiv.org/html/2601.21714/x2.png",
      "debug_abstract": "The evolution of Large Language Model (LLM) agents towards System~2 reasoning, characterized by deliberative, high-precision problem-solving, requires maintaining rigorous logical integrity over extended horizons. However, prevalent memory preprocessing paradigms suffer from destructive de-contextualization. By compressing complex sequential dependencies into pre-defined structures (e.g., embeddings or graphs), these methods sever the contextual integrity essential for deep reasoning. To address this, we propose E-mem, a framework shifting from Memory Preprocessing to Episodic Context Reconstruction. Inspired by biological engrams, E-mem employs a heterogeneous hierarchical architecture where multiple assistant agents maintain uncompressed memory contexts, while a central master agent orchestrates global planning. Unlike passive retrieval, our mechanism empowers assistants to locally reason within activated segments, extracting context-aware evidence before aggregation. Evaluations on the LoCoMo benchmark demonstrate that E-mem achieves over 54\\% F1, surpassing the state-of-the-art GAM by 7.75\\%, while reducing token cost by over 70\\%."
    },
    {
      "title": "TouchGuide: Inference-Time Steering of Visuomotor Policies via Touch Guidance",
      "url": "https://arxiv.org/abs/2601.20239",
      "date": "2026-01-28",
      "authors_text": "Zhemeng Zhang, Jiahua Ma, Xincheng Yang, Xin Wen, Yuzhi Zhang",
      "is_highlight": false,
      "score": 90.0,
      "summary": "TouchGuide enhances robot manipulation by integrating tactile feedback with visual inputs to refine actions during inference, outperforming existing visuo-tactile policies in complex tasks.",
      "teaser_image": "https://arxiv.org/html/2601.20239/x2.png",
      "debug_abstract": "Fine-grained and contact-rich manipulation remain challenging for robots, largely due to the underutilization of tactile feedback. To address this, we introduce TouchGuide, a novel cross-policy visuo-tactile fusion paradigm that fuses modalities within a low-dimensional action space. Specifically, TouchGuide operates in two stages to guide a pre-trained diffusion or flow-matching visuomotor policy at inference time. First, the policy produces a coarse, visually-plausible action using only visual inputs during early sampling. Second, a task-specific Contact Physical Model (CPM) provides tactile guidance to steer and refine the action, ensuring it aligns with realistic physical contact conditions. Trained through contrastive learning on limited expert demonstrations, the CPM provides a tactile-informed feasibility score to steer the sampling process toward refined actions that satisfy physical contact constraints. Furthermore, to facilitate TouchGuide training with high-quality and cost-effective data, we introduce TacUMI, a data collection system. TacUMI achieves a favorable trade-off between precision and affordability; by leveraging rigid fingertips, it obtains direct tactile feedback, thereby enabling the collection of reliable tactile data. Extensive experiments on five challenging contact-rich tasks, such as shoe lacing and chip handover, show that TouchGuide consistently and significantly outperforms state-of-the-art visuo-tactile policies."
    },
    {
      "title": "Innovator-VL: A Multimodal Large Language Model for Scientific Discovery",
      "url": "https://arxiv.org/abs/2601.19325",
      "date": "2026-01-27",
      "authors_text": "Zichen Wen, Boxue Yang, Shuang Chen, Yaojie Zhang, Yuhang Han",
      "is_highlight": false,
      "score": 78.0,
      "summary": "Innovator-VL is a multimodal large language model that enhances scientific discovery through efficient training, transparency, and strong generalization without relying on extensive pretraining.",
      "teaser_image": "https://arxiv.org/html/2601.19325/x8.png",
      "debug_abstract": "We present Innovator-VL, a scientific multimodal large language model designed to advance understanding and reasoning across diverse scientific domains while maintaining excellent performance on general vision tasks. Contrary to the trend of relying on massive domain-specific pretraining and opaque pipelines, our work demonstrates that principled training design and transparent methodology can yield strong scientific intelligence with substantially reduced data requirements. (i) First, we provide a fully transparent, end-to-end reproducible training pipeline, covering data collection, cleaning, preprocessing, supervised fine-tuning, reinforcement learning, and evaluation, along with detailed optimization recipes. This facilitates systematic extension by the community. (ii) Second, Innovator-VL exhibits remarkable data efficiency, achieving competitive performance on various scientific tasks using fewer than five million curated samples without large-scale pretraining. These results highlight that effective reasoning can be achieved through principled data selection rather than indiscriminate scaling. (iii) Third, Innovator-VL demonstrates strong generalization, achieving competitive performance on general vision, multimodal reasoning, and scientific benchmarks. This indicates that scientific alignment can be integrated into a unified model without compromising general-purpose capabilities. Our practices suggest that efficient, reproducible, and high-performing scientific multimodal models can be built even without large-scale data, providing a practical foundation for future research."
    },
    {
      "title": "PILOT: A Perceptive Integrated Low-level Controller for Loco-manipulation over Unstructured Scenes",
      "url": "https://arxiv.org/abs/2601.17440",
      "date": "2026-01-24",
      "authors_text": "Xinru Cui, Linxi Feng, Yixuan Zhou, Haoqi Han, Zhe Liu",
      "is_highlight": true,
      "score": 90.0,
      "summary": "PILOT is a unified reinforcement learning framework that integrates perceptive locomotion and whole-body control for stable loco-manipulation in unstructured environments, outperforming existing controllers.",
      "debug_abstract": "Humanoid robots hold great potential for diverse interactions and daily service tasks within human-centered environments, necessitating controllers that seamlessly integrate precise locomotion with dexterous manipulation. However, most existing whole-body controllers lack exteroceptive awareness of the surrounding environment, rendering them insufficient for stable task execution in complex, unstructured this http URL address this challenge, we propose PILOT, a unified single-stage reinforcement learning (RL) framework tailored for perceptive loco-manipulation, which synergizes perceptive locomotion and expansive whole-body control within a single policy. To enhance terrain awareness and ensure precise foot placement, we design a cross-modal context encoder that fuses prediction-based proprioceptive features with attention-based perceptive representations. Furthermore, we introduce a Mixture-of-Experts (MoE) policy architecture to coordinate diverse motor skills, facilitating better specialization across distinct motion patterns. Extensive experiments in both simulation and on the physical Unitree G1 humanoid robot validate the efficacy of our framework. PILOT demonstrates superior stability, command tracking precision, and terrain traversability compared to existing baselines. These results highlight its potential to serve as a robust, foundational low-level controller for loco-manipulation in unstructured scenes."
    },
    {
      "title": "BMDS-Net: A Bayesian Multi-Modal Deep Supervision Network for Robust Brain Tumor Segmentation",
      "url": "https://arxiv.org/abs/2601.17504",
      "date": "2026-01-24",
      "authors_text": "Yan Zhou, Zhen Huang, Yingqiu Li, Yue Ouyang, Suncheng Xiang",
      "is_highlight": false,
      "score": 50.0,
      "summary": "BMDS-Net enhances brain tumor segmentation by integrating robust feature learning and Bayesian uncertainty estimation, ensuring reliable performance despite missing MRI modalities.",
      "debug_abstract": "Accurate brain tumor segmentation from multi-modal magnetic resonance imaging (MRI) is a prerequisite for precise radiotherapy planning and surgical navigation. While recent Transformer-based models such as Swin UNETR have achieved impressive benchmark performance, their clinical utility is often compromised by two critical issues: sensitivity to missing modalities (common in clinical practice) and a lack of confidence calibration. Merely chasing higher Dice scores on idealized data fails to meet the safety requirements of real-world medical deployment. In this work, we propose BMDS-Net, a unified framework that prioritizes clinical robustness and trustworthiness over simple metric maximization. Our contribution is three-fold. First, we construct a robust deterministic backbone by integrating a Zero-Init Multimodal Contextual Fusion (MMCF) module and a Residual-Gated Deep Decoder Supervision (DDS) mechanism, enabling stable feature learning and precise boundary delineation with significantly reduced Hausdorff Distance, even under modality corruption. Second, and most importantly, we introduce a memory-efficient Bayesian fine-tuning strategy that transforms the network into a probabilistic predictor, providing voxel-wise uncertainty maps to highlight potential errors for clinicians. Third, comprehensive experiments on the BraTS 2021 dataset demonstrate that BMDS-Net not only maintains competitive accuracy but, more importantly, exhibits superior stability in missing-modality scenarios where baseline models fail. The source code is publicly available at this https URL."
    },
    {
      "title": "QualiRAG: Retrieval-Augmented Generation for Visual Quality Understanding",
      "url": "https://arxiv.org/abs/2601.18195",
      "date": "2026-01-26",
      "authors_text": "Linhan Cao, Wei Sun, Weixia Zhang, Xiangyang Zhu, Kaiwei Zhang",
      "is_highlight": false,
      "score": 66.0,
      "summary": "QualiRAG introduces a training-free framework for visual quality assessment that utilizes dynamic retrieval and multimodal knowledge to enhance interpretability and performance without task-specific training.",
      "debug_abstract": "Visual quality assessment (VQA) is increasingly shifting from scalar score prediction toward interpretable quality understanding -- a paradigm that demands \\textit{fine-grained spatiotemporal perception} and \\textit{auxiliary contextual information}. Current approaches rely on supervised fine-tuning or reinforcement learning on curated instruction datasets, which involve labor-intensive annotation and are prone to dataset-specific biases. To address these challenges, we propose \\textbf{QualiRAG}, a \\textit{training-free} \\textbf{R}etrieval-\\textbf{A}ugmented \\textbf{G}eneration \\textbf{(RAG)} framework that systematically leverages the latent perceptual knowledge of large multimodal models (LMMs) for visual quality perception. Unlike conventional RAG that retrieves from static corpora, QualiRAG dynamically generates auxiliary knowledge by decomposing questions into structured requests and constructing four complementary knowledge sources: \\textit{visual metadata}, \\textit{subject localization}, \\textit{global quality summaries}, and \\textit{local quality descriptions}, followed by relevance-aware retrieval for evidence-grounded reasoning. Extensive experiments show that QualiRAG achieves substantial improvements over open-source general-purpose LMMs and VQA-finetuned LMMs on visual quality understanding tasks, and delivers competitive performance on visual quality comparison tasks, demonstrating robust quality assessment capabilities without any task-specific training. The code will be publicly available at this https URL."
    },
    {
      "title": "ExoGS: A 4D Real-to-Sim-to-Real Framework for Scalable Manipulation Data Collection",
      "url": "https://arxiv.org/abs/2601.18629",
      "date": "2026-01-26",
      "authors_text": "Yiming Wang, Ruogu Zhang, Minyang Li, Hao Shi, Junbo Wang",
      "is_highlight": false,
      "score": 93.0,
      "summary": "ExoGS introduces a 4D framework for efficient manipulation data collection by seamlessly transferring real-world interactions to simulation, enhancing policy learning and generalization.",
      "debug_abstract": "Real-to-Sim-to-Real technique is gaining increasing interest for robotic manipulation, as it can generate scalable data in simulation while having narrower sim-to-real gap. However, previous methods mainly focused on environment-level visual real-to-sim transfer, ignoring the transfer of interactions, which could be challenging and inefficient to obtain purely in simulation especially for contact-rich tasks. We propose ExoGS, a robot-free 4D Real-to-Sim-to-Real framework that captures both static environments and dynamic interactions in the real world and transfers them seamlessly to a simulated environment. It provides a new solution for scalable manipulation data collection and policy learning. ExoGS employs a self-designed robot-isomorphic passive exoskeleton AirExo-3 to capture kinematically consistent trajectories with millimeter-level accuracy and synchronized RGB observations during direct human demonstrations. The robot, objects, and environment are reconstructed as editable 3D Gaussian Splatting assets, enabling geometry-consistent replay and large-scale data augmentation. Additionally, a lightweight Mask Adapter injects instance-level semantics into the policy to enhance robustness under visual domain shifts. Real-world experiments demonstrate that ExoGS significantly improves data efficiency and policy generalization compared to teleoperation-based baselines. Code and hardware files have been released on this https URL."
    },
    {
      "title": "Advances and Innovations in the Multi-Agent Robotic System (MARS) Challenge",
      "url": "https://arxiv.org/abs/2601.18733",
      "date": "2026-01-26",
      "authors_text": "Li Kang, Heng Zhou, Xiufeng Song, Rui Li, Bruno N. Y. Chen",
      "is_highlight": false,
      "score": 81.0,
      "summary": "The MARS Challenge at NeurIPS 2025 aims to enhance multi-agent collaboration in embodied AI through competition in planning and control using vision-language models.",
      "debug_abstract": "Recent advancements in multimodal large language models and vision-languageaction models have significantly driven progress in Embodied AI. As the field transitions toward more complex task scenarios, multi-agent system frameworks are becoming essential for achieving scalable, efficient, and collaborative solutions. This shift is fueled by three primary factors: increasing agent capabilities, enhancing system efficiency through task delegation, and enabling advanced human-agent interactions. To address the challenges posed by multi-agent collaboration, we propose the Multi-Agent Robotic System (MARS) Challenge, held at the NeurIPS 2025 Workshop on SpaVLE. The competition focuses on two critical areas: planning and control, where participants explore multi-agent embodied planning using vision-language models (VLMs) to coordinate tasks and policy execution to perform robotic manipulation in dynamic environments. By evaluating solutions submitted by participants, the challenge provides valuable insights into the design and coordination of embodied multi-agent systems, contributing to the future development of advanced collaborative AI systems."
    }
  ],
  "机器视觉与智能学习实验室 (MVIG)": [
    {
      "title": "Embodiment-Aware Generalist Specialist Distillation for Unified Humanoid Whole-Body Control",
      "url": "https://arxiv.org/abs/2602.02960",
      "date": "2026-02-03",
      "authors_text": "Quanquan Peng, Yunfeng Lin, Yufei Xue, Jiangmiao Pang, Weinan Zhang",
      "is_highlight": false,
      "score": 90.0,
      "summary": "The EAGLE framework enables a unified policy for diverse humanoid robots, enhancing whole-body control through iterative generalist-specialist distillation without individual reward tuning.",
      "teaser_image": "https://arxiv.org/html/2602.02960/x1.png",
      "debug_abstract": "Humanoid Whole-Body Controllers trained with reinforcement learning (RL) have recently achieved remarkable performance, yet many target a single robot embodiment. Variations in dynamics, degrees of freedom (DoFs), and kinematic topology still hinder a single policy from commanding diverse humanoids. Moreover, obtaining a generalist policy that not only transfers across embodiments but also supports richer behaviors-beyond simple walking to squatting, leaning-remains especially challenging. In this work, we tackle these obstacles by introducing EAGLE, an iterative generalist-specialist distillation framework that produces a single unified policy that controls multiple heterogeneous humanoids without per-robot reward tuning. During each cycle, embodiment-specific specialists are forked from the current generalist, refined on their respective robots, and new skills are distilled back into the generalist by training on the pooled embodiment set. Repeating this loop until performance convergence produces a robust Whole-Body Controller validated on robots such as Unitree H1, G1, and Fourier N1. We conducted experiments on five different robots in simulation and four in real-world settings. Through quantitative evaluations, EAGLE achieves high tracking accuracy and robustness compared to other methods, marking a step toward scalable, fleet-level humanoid control. See more details at this https URL"
    },
    {
      "title": "Bongards at the Boundary of Perception and Reasoning: Programs or Language?",
      "url": "https://arxiv.org/abs/2602.03038",
      "date": "2026-02-03",
      "authors_text": "Cassidy Langenfeld, Claas Beger, Gloria Geng, Wasu Top Piriyakulkij, Keya Hu",
      "is_highlight": false,
      "score": 60.0,
      "summary": "This paper introduces a neurosymbolic method utilizing LLMs and Bayesian optimization to tackle Bongard problems, enhancing visual reasoning beyond conventional VLM capabilities.",
      "teaser_image": "https://arxiv.org/html/2602.03038/x3.png",
      "debug_abstract": "Vision-Language Models (VLMs) have made great strides in everyday visual tasks, such as captioning a natural image, or answering commonsense questions about such images. But humans possess the puzzling ability to deploy their visual reasoning abilities in radically new situations, a skill rigorously tested by the classic set of visual reasoning challenges known as the Bongard problems. We present a neurosymbolic approach to solving these problems: given a hypothesized solution rule for a Bongard problem, we leverage LLMs to generate parameterized programmatic representations for the rule and perform parameter fitting using Bayesian optimization. We evaluate our method on classifying Bongard problem images given the ground truth rule, as well as on solving the problems from scratch."
    },
    {
      "title": "Agentic Proposing: Enhancing Large Language Model Reasoning via Compositional Skill Synthesis",
      "url": "https://arxiv.org/abs/2602.03279",
      "date": "2026-02-03",
      "authors_text": "Zhengbo Jiao, Shaobo Wang, Zifan Zhang, Xuan Ren, Wei Wang",
      "is_highlight": false,
      "score": 71.0,
      "summary": "The paper introduces Agentic Proposing, a framework for generating high-quality synthetic training data that enhances reasoning in large language models, achieving superior performance with minimal human input.",
      "teaser_image": "https://arxiv.org/html/2602.03279/x2.png",
      "debug_abstract": "Advancing complex reasoning in large language models relies on high-quality, verifiable datasets, yet human annotation remains cost-prohibitive and difficult to scale. Current synthesis paradigms often face a recurring trade-off: maintaining structural validity typically restricts problem complexity, while relaxing constraints to increase difficulty frequently leads to inconsistent or unsolvable instances. To address this, we propose Agentic Proposing, a framework that models problem synthesis as a goal-driven sequential decision process where a specialized agent dynamically selects and composes modular reasoning skills. Through an iterative workflow of internal reflection and tool-use, we develop the Agentic-Proposer-4B using Multi-Granularity Policy Optimization (MGPO) to generate high-precision, verifiable training trajectories across mathematics, coding, and science. Empirical results demonstrate that downstream solvers trained on agent-synthesized data significantly outperform leading baselines and exhibit robust cross-domain generalization. Notably, a 30B solver trained on only 11,000 synthesized trajectories achieves a state-of-the-art 91.6% accuracy on AIME25, rivaling frontier-scale proprietary models such as GPT-5 and proving that a small volume of high-quality synthetic signals can effectively substitute for massive human-curated datasets."
    },
    {
      "title": "Socratic-Geo: Synthetic Data Generation and Geometric Reasoning via Multi-Agent Interaction",
      "url": "https://arxiv.org/abs/2602.03414",
      "date": "2026-02-03",
      "authors_text": "Zhengbo Jiao, Shaobo Wang, Zifan Zhang, Wei Wang, Bing Zhao",
      "is_highlight": false,
      "score": 58.0,
      "summary": "Socratic-Geo introduces a multi-agent framework for autonomous data synthesis and geometric reasoning, significantly improving model performance with minimal seed problems and surpassing existing benchmarks.",
      "teaser_image": "https://arxiv.org/html/2602.03414/x1.png",
      "debug_abstract": "Multimodal Large Language Models (MLLMs) have significantly advanced vision-language understanding. However, even state-of-the-art models struggle with geometric reasoning, revealing a critical bottleneck: the extreme scarcity of high-quality image-text pairs. Human annotation is prohibitively expensive, while automated methods fail to ensure fidelity and training effectiveness. Existing approaches either passively adapt to available images or employ inefficient random exploration with filtering, decoupling generation from learning needs. We propose Socratic-Geo, a fully autonomous framework that dynamically couples data synthesis with model learning through multi-agent interaction. The Teacher agent generates parameterized Python scripts with reflective feedback (Reflect for solvability, RePI for visual validity), ensuring image-text pair purity. The Solver agent optimizes reasoning through preference learning, with failure paths guiding Teacher&#39;s targeted augmentation. Independently, the Generator learns image generation capabilities on accumulated &#34;image-code-instruction&#34; triplets, distilling programmatic drawing intelligence into visual generation. Starting from only 108 seed problems, Socratic-Solver achieves 49.11 on six benchmarks using one-quarter of baseline data, surpassing strong baselines by 2.43 points. Socratic-Generator achieves 42.4% on GenExam, establishing new state-of-the-art for open-source models, surpassing Seedream-4.0 (39.8%) and approaching Gemini-2.5-Flash-Image (43.1%)."
    },
    {
      "title": "QVLA: Not All Channels Are Equal in Vision-Language-Action Model's Quantization",
      "url": "https://arxiv.org/abs/2602.03782",
      "date": "2026-02-03",
      "authors_text": "Yuhao Xu, Yantai Yang, Zhenyang Fan, Yufan Liu, Yuming Li",
      "is_highlight": false,
      "score": 47.0,
      "summary": "QVLA introduces an action-centric quantization framework for Vision-Language-Action models, optimizing channel-wise bit allocation to enhance performance and reduce resource demands in robotics.",
      "teaser_image": "https://arxiv.org/html/2602.03782/x2.png",
      "debug_abstract": "The advent of Vision-Language-Action (VLA) models represents a significant leap for embodied intelligence, yet their immense computational demands critically hinder deployment on resource-constrained robotic platforms. Intuitively, low-bit quantization is a prevalent and preferred technique for large-scale model compression. However, we find that a systematic analysis of VLA model&#39;s quantization is fundamentally lacking. We argue that naively applying uniform-bit quantization from Large Language Models (LLMs) to robotics is flawed, as these methods prioritize passive data fidelity while ignoring how minor action deviations compound into catastrophic task failures. To bridge this gap, we introduce QVLA, the first action-centric quantization framework specifically designed for embodied control. In a sharp departure from the rigid, uniform-bit quantization of LLM-based methods, QVLA introduces a highly granular, channel-wise bit allocation strategy. Its core mechanism is to directly measure the final action-space sensitivity when quantizing each individual channel to various bit-widths. This process yields a precise, per-channel importance metric that guides a global optimization, which elegantly unifies quantization and pruning (0-bit) into a single, cohesive framework. Extensive evaluations on different baselines demonstrate the superiority of our approach. In the LIBERO, the quantization version of OpenVLA-OFT with our method requires only 29.2% of the original model&#39;s VRAM while maintaining 98.9% of its original performance and achieving a 1.49x speedup. This translates to a 22.6% performance improvement over the LLM-derived method SmoothQuant. Our work establishes a new, principled foundation for compressing VLA models in robotics, paving the way for deploying powerful, large-scale models on real-world hardware. Code will be released."
    },
    {
      "title": "ConLA: Contrastive Latent Action Learning from Human Videos for Robotic Manipulation",
      "url": "https://arxiv.org/abs/2602.00557",
      "date": "2026-01-31",
      "authors_text": "Weisheng Dai, Kai Lan, Jianyi Zhou, Bo Zhao, Xiu Su",
      "is_highlight": true,
      "score": 91.0,
      "summary": "ConLA introduces a contrastive disentanglement framework for unsupervised robotic policy learning from human videos, enhancing transferability and performance beyond traditional robot trajectory pretraining.",
      "teaser_image": "https://arxiv.org/html/2602.00557/x3.png",
      "debug_abstract": "Vision-Language-Action (VLA) models achieve preliminary generalization through pretraining on large scale robot teleoperation datasets. However, acquiring datasets that comprehensively cover diverse tasks and environments is extremely costly and difficult to scale. In contrast, human demonstration videos offer a rich and scalable source of diverse scenes and manipulation behaviors, yet their lack of explicit action supervision hinders direct utilization. Prior work leverages VQ-VAE based frameworks to learn latent actions from human videos in an unsupervised manner. Nevertheless, since the training objective primarily focuses on reconstructing visual appearances rather than capturing inter-frame dynamics, the learned representations tend to rely on spurious visual cues, leading to shortcut learning and entangled latent representations that hinder transferability. To address this, we propose ConLA, an unsupervised pretraining framework for learning robotic policies from human videos. ConLA introduces a contrastive disentanglement mechanism that leverages action category priors and temporal cues to isolate motion dynamics from visual content, effectively mitigating shortcut learning. Extensive experiments show that ConLA achieves strong performance across diverse benchmarks. Notably, by pretraining solely on human videos, our method for the first time surpasses the performance obtained with real robot trajectory pretraining, highlighting its ability to extract pure and semantically consistent latent action representations for scalable robot learning."
    },
    {
      "title": "ProxyImg: Towards Highly-Controllable Image Representation via Hierarchical Disentangled Proxy Embedding",
      "url": "https://arxiv.org/abs/2602.01881",
      "date": "2026-02-02",
      "authors_text": "Ye Chen, Yupeng Zhu, Xiongzhen Zhang, Zhewen Wan, Yingzhe Li",
      "is_highlight": false,
      "score": 30.0,
      "summary": "The paper presents ProxyImg, a hierarchical proxy-based image representation that enables fine-grained, controllable editing by disentangling semantic, geometric, and textural attributes for high-fidelity reconstruction.",
      "teaser_image": "https://arxiv.org/html/2602.01881/x2.png",
      "debug_abstract": "Prevailing image representation methods, including explicit representations such as raster images and Gaussian primitives, as well as implicit representations such as latent images, either suffer from representation redundancy that leads to heavy manual editing effort, or lack a direct mapping from latent variables to semantic instances or parts, making fine-grained manipulation difficult. These limitations hinder efficient and controllable image and video editing. To address these issues, we propose a hierarchical proxy-based parametric image representation that disentangles semantic, geometric, and textural attributes into independent and manipulable parameter spaces. Based on a semantic-aware decomposition of the input image, our representation constructs hierarchical proxy geometries through adaptive Bezier fitting and iterative internal region subdivision and meshing. Multi-scale implicit texture parameters are embedded into the resulting geometry-aware distributed proxy nodes, enabling continuous high-fidelity reconstruction in the pixel domain and instance- or part-independent semantic editing. In addition, we introduce a locality-adaptive feature indexing mechanism to ensure spatial texture coherence, which further supports high-quality background completion without relying on generative models. Extensive experiments on image reconstruction and editing benchmarks, including ImageNet, OIR-Bench, and HumanEdit, demonstrate that our method achieves state-of-the-art rendering fidelity with significantly fewer parameters, while enabling intuitive, interactive, and physically plausible manipulation. Moreover, by integrating proxy nodes with Position-Based Dynamics, our framework supports real-time physics-driven animation using lightweight implicit rendering, achieving superior temporal consistency and visual realism compared with generative approaches."
    },
    {
      "title": "SurfSplat: Conquering Feedforward 2D Gaussian Splatting with Surface Continuity Priors",
      "url": "https://arxiv.org/abs/2602.02000",
      "date": "2026-02-02",
      "authors_text": "Bing He, Jingnan Gao, Yunuo Chen, Ning Cao, Gang Chen",
      "is_highlight": false,
      "score": 25.0,
      "summary": "SurfSplat introduces a feedforward 2D Gaussian Splatting framework that enhances 3D scene reconstruction by ensuring surface continuity and improving geometric precision over existing methods.",
      "teaser_image": "https://arxiv.org/html/2602.02000/x2.png",
      "debug_abstract": "Reconstructing 3D scenes from sparse images remains a challenging task due to the difficulty of recovering accurate geometry and texture without optimization. Recent approaches leverage generalizable models to generate 3D scenes using 3D Gaussian Splatting (3DGS) primitive. However, they often fail to produce continuous surfaces and instead yield discrete, color-biased point clouds that appear plausible at normal resolution but reveal severe artifacts under close-up views. To address this issue, we present SurfSplat, a feedforward framework based on 2D Gaussian Splatting (2DGS) primitive, which provides stronger anisotropy and higher geometric precision. By incorporating a surface continuity prior and a forced alpha blending strategy, SurfSplat reconstructs coherent geometry together with faithful textures. Furthermore, we introduce High-Resolution Rendering Consistency (HRRC), a new evaluation metric designed to evaluate high-resolution reconstruction quality. Extensive experiments on RealEstate10K, DL3DV, and ScanNet demonstrate that SurfSplat consistently outperforms prior methods on both standard metrics and HRRC, establishing a robust solution for high-fidelity 3D reconstruction from sparse inputs. Project page: this https URL"
    },
    {
      "title": "PLANING: A Loosely Coupled Triangle-Gaussian Framework for Streaming 3D Reconstruction",
      "url": "https://arxiv.org/abs/2601.22046",
      "date": "2026-01-29",
      "authors_text": "Changjian Jiang, Kerui Ren, Xudong Li, Kaiwen Song, Linning Xu",
      "is_highlight": false,
      "score": 66.0,
      "summary": "PLANING is a novel streaming 3D reconstruction framework that efficiently combines geometric primitives and neural Gaussians, enhancing quality and speed while reducing redundancy.",
      "teaser_image": "https://arxiv.org/html/2601.22046/x3.png",
      "debug_abstract": "Streaming reconstruction from monocular image sequences remains challenging, as existing methods typically favor either high-quality rendering or accurate geometry, but rarely both. We present PLANING, an efficient on-the-fly reconstruction framework built on a hybrid representation that loosely couples explicit geometric primitives with neural Gaussians, enabling geometry and appearance to be modeled in a decoupled manner. This decoupling supports an online initialization and optimization strategy that separates geometry and appearance updates, yielding stable streaming reconstruction with substantially reduced structural redundancy. PLANING improves dense mesh Chamfer-L2 by 18.52% over PGSR, surpasses ARTDECO by 1.31 dB PSNR, and reconstructs ScanNetV2 scenes in under 100 seconds, over 5x faster than 2D Gaussian Splatting, while matching the quality of offline per-scene optimization. Beyond reconstruction quality, the structural clarity and computational efficiency of \\modelname~make it well suited for a broad range of downstream applications, such as enabling large-scale scene modeling and simulation-ready environments for embodied AI. Project page: this https URL ."
    },
    {
      "title": "DynaWeb: Model-Based Reinforcement Learning of Web Agents",
      "url": "https://arxiv.org/abs/2601.22149",
      "date": "2026-01-29",
      "authors_text": "Hang Ding, Peidong Liu, Junqiao Wang, Ziwei Ji, Meng Cao",
      "is_highlight": false,
      "score": 77.0,
      "summary": "DynaWeb introduces a model-based reinforcement learning framework that enhances web agent training by simulating interactions with a web world model, improving efficiency and performance.",
      "teaser_image": "https://arxiv.org/html/2601.22149/figures/dynaweb.png",
      "debug_abstract": "The development of autonomous web agents, powered by Large Language Models (LLMs) and reinforcement learning (RL), represents a significant step towards general-purpose AI assistants. However, training these agents is severely hampered by the challenges of interacting with the live internet, which is inefficient, costly, and fraught with risks. Model-based reinforcement learning (MBRL) offers a promising solution by learning a world model of the environment to enable simulated interaction. This paper introduces DynaWeb, a novel MBRL framework that trains web agents through interacting with a web world model trained to predict naturalistic web page representations given agent actions. This model serves as a synthetic web environment where an agent policy can dream by generating vast quantities of rollout action trajectories for efficient online reinforcement learning. Beyond free policy rollouts, DynaWeb incorporates real expert trajectories from training data, which are randomly interleaved with on-policy rollouts during training to improve stability and sample efficiency. Experiments conducted on the challenging WebArena and WebVoyager benchmarks demonstrate that DynaWeb consistently and significantly improves the performance of state-of-the-art open-source web agent models. Our findings establish the viability of training web agents through imagination, offering a scalable and efficient way to scale up online agentic RL."
    },
    {
      "title": "E-mem: Multi-agent based Episodic Context Reconstruction for LLM Agent Memory",
      "url": "https://arxiv.org/abs/2601.21714",
      "date": "2026-01-29",
      "authors_text": "Kaixiang Wang, Yidan Lin, Jiong Lou, Zhaojiacheng Zhou, Bunyod Suvonov",
      "is_highlight": false,
      "score": 71.0,
      "summary": "E-mem introduces a multi-agent framework for episodic context reconstruction in LLMs, enhancing reasoning accuracy and efficiency while preserving contextual integrity.",
      "teaser_image": "https://arxiv.org/html/2601.21714/x2.png",
      "debug_abstract": "The evolution of Large Language Model (LLM) agents towards System~2 reasoning, characterized by deliberative, high-precision problem-solving, requires maintaining rigorous logical integrity over extended horizons. However, prevalent memory preprocessing paradigms suffer from destructive de-contextualization. By compressing complex sequential dependencies into pre-defined structures (e.g., embeddings or graphs), these methods sever the contextual integrity essential for deep reasoning. To address this, we propose E-mem, a framework shifting from Memory Preprocessing to Episodic Context Reconstruction. Inspired by biological engrams, E-mem employs a heterogeneous hierarchical architecture where multiple assistant agents maintain uncompressed memory contexts, while a central master agent orchestrates global planning. Unlike passive retrieval, our mechanism empowers assistants to locally reason within activated segments, extracting context-aware evidence before aggregation. Evaluations on the LoCoMo benchmark demonstrate that E-mem achieves over 54\\% F1, surpassing the state-of-the-art GAM by 7.75\\%, while reducing token cost by over 70\\%."
    },
    {
      "title": "TouchGuide: Inference-Time Steering of Visuomotor Policies via Touch Guidance",
      "url": "https://arxiv.org/abs/2601.20239",
      "date": "2026-01-28",
      "authors_text": "Zhemeng Zhang, Jiahua Ma, Xincheng Yang, Xin Wen, Yuzhi Zhang",
      "is_highlight": false,
      "score": 90.0,
      "summary": "TouchGuide enhances robot manipulation by integrating tactile feedback with visual inputs to refine actions during inference, outperforming existing visuo-tactile policies in complex tasks.",
      "teaser_image": "https://arxiv.org/html/2601.20239/x2.png",
      "debug_abstract": "Fine-grained and contact-rich manipulation remain challenging for robots, largely due to the underutilization of tactile feedback. To address this, we introduce TouchGuide, a novel cross-policy visuo-tactile fusion paradigm that fuses modalities within a low-dimensional action space. Specifically, TouchGuide operates in two stages to guide a pre-trained diffusion or flow-matching visuomotor policy at inference time. First, the policy produces a coarse, visually-plausible action using only visual inputs during early sampling. Second, a task-specific Contact Physical Model (CPM) provides tactile guidance to steer and refine the action, ensuring it aligns with realistic physical contact conditions. Trained through contrastive learning on limited expert demonstrations, the CPM provides a tactile-informed feasibility score to steer the sampling process toward refined actions that satisfy physical contact constraints. Furthermore, to facilitate TouchGuide training with high-quality and cost-effective data, we introduce TacUMI, a data collection system. TacUMI achieves a favorable trade-off between precision and affordability; by leveraging rigid fingertips, it obtains direct tactile feedback, thereby enabling the collection of reliable tactile data. Extensive experiments on five challenging contact-rich tasks, such as shoe lacing and chip handover, show that TouchGuide consistently and significantly outperforms state-of-the-art visuo-tactile policies."
    },
    {
      "title": "Innovator-VL: A Multimodal Large Language Model for Scientific Discovery",
      "url": "https://arxiv.org/abs/2601.19325",
      "date": "2026-01-27",
      "authors_text": "Zichen Wen, Boxue Yang, Shuang Chen, Yaojie Zhang, Yuhang Han",
      "is_highlight": false,
      "score": 78.0,
      "summary": "Innovator-VL is a multimodal large language model that enhances scientific discovery through efficient training, transparency, and strong generalization without relying on extensive pretraining.",
      "teaser_image": "https://arxiv.org/html/2601.19325/x8.png",
      "debug_abstract": "We present Innovator-VL, a scientific multimodal large language model designed to advance understanding and reasoning across diverse scientific domains while maintaining excellent performance on general vision tasks. Contrary to the trend of relying on massive domain-specific pretraining and opaque pipelines, our work demonstrates that principled training design and transparent methodology can yield strong scientific intelligence with substantially reduced data requirements. (i) First, we provide a fully transparent, end-to-end reproducible training pipeline, covering data collection, cleaning, preprocessing, supervised fine-tuning, reinforcement learning, and evaluation, along with detailed optimization recipes. This facilitates systematic extension by the community. (ii) Second, Innovator-VL exhibits remarkable data efficiency, achieving competitive performance on various scientific tasks using fewer than five million curated samples without large-scale pretraining. These results highlight that effective reasoning can be achieved through principled data selection rather than indiscriminate scaling. (iii) Third, Innovator-VL demonstrates strong generalization, achieving competitive performance on general vision, multimodal reasoning, and scientific benchmarks. This indicates that scientific alignment can be integrated into a unified model without compromising general-purpose capabilities. Our practices suggest that efficient, reproducible, and high-performing scientific multimodal models can be built even without large-scale data, providing a practical foundation for future research."
    },
    {
      "title": "PILOT: A Perceptive Integrated Low-level Controller for Loco-manipulation over Unstructured Scenes",
      "url": "https://arxiv.org/abs/2601.17440",
      "date": "2026-01-24",
      "authors_text": "Xinru Cui, Linxi Feng, Yixuan Zhou, Haoqi Han, Zhe Liu",
      "is_highlight": true,
      "score": 90.0,
      "summary": "PILOT is a unified reinforcement learning framework that integrates perceptive locomotion and whole-body control for stable loco-manipulation in unstructured environments, outperforming existing controllers.",
      "debug_abstract": "Humanoid robots hold great potential for diverse interactions and daily service tasks within human-centered environments, necessitating controllers that seamlessly integrate precise locomotion with dexterous manipulation. However, most existing whole-body controllers lack exteroceptive awareness of the surrounding environment, rendering them insufficient for stable task execution in complex, unstructured this http URL address this challenge, we propose PILOT, a unified single-stage reinforcement learning (RL) framework tailored for perceptive loco-manipulation, which synergizes perceptive locomotion and expansive whole-body control within a single policy. To enhance terrain awareness and ensure precise foot placement, we design a cross-modal context encoder that fuses prediction-based proprioceptive features with attention-based perceptive representations. Furthermore, we introduce a Mixture-of-Experts (MoE) policy architecture to coordinate diverse motor skills, facilitating better specialization across distinct motion patterns. Extensive experiments in both simulation and on the physical Unitree G1 humanoid robot validate the efficacy of our framework. PILOT demonstrates superior stability, command tracking precision, and terrain traversability compared to existing baselines. These results highlight its potential to serve as a robust, foundational low-level controller for loco-manipulation in unstructured scenes."
    },
    {
      "title": "BMDS-Net: A Bayesian Multi-Modal Deep Supervision Network for Robust Brain Tumor Segmentation",
      "url": "https://arxiv.org/abs/2601.17504",
      "date": "2026-01-24",
      "authors_text": "Yan Zhou, Zhen Huang, Yingqiu Li, Yue Ouyang, Suncheng Xiang",
      "is_highlight": false,
      "score": 50.0,
      "summary": "BMDS-Net enhances brain tumor segmentation by integrating robust feature learning and Bayesian uncertainty estimation, ensuring reliable performance despite missing MRI modalities.",
      "debug_abstract": "Accurate brain tumor segmentation from multi-modal magnetic resonance imaging (MRI) is a prerequisite for precise radiotherapy planning and surgical navigation. While recent Transformer-based models such as Swin UNETR have achieved impressive benchmark performance, their clinical utility is often compromised by two critical issues: sensitivity to missing modalities (common in clinical practice) and a lack of confidence calibration. Merely chasing higher Dice scores on idealized data fails to meet the safety requirements of real-world medical deployment. In this work, we propose BMDS-Net, a unified framework that prioritizes clinical robustness and trustworthiness over simple metric maximization. Our contribution is three-fold. First, we construct a robust deterministic backbone by integrating a Zero-Init Multimodal Contextual Fusion (MMCF) module and a Residual-Gated Deep Decoder Supervision (DDS) mechanism, enabling stable feature learning and precise boundary delineation with significantly reduced Hausdorff Distance, even under modality corruption. Second, and most importantly, we introduce a memory-efficient Bayesian fine-tuning strategy that transforms the network into a probabilistic predictor, providing voxel-wise uncertainty maps to highlight potential errors for clinicians. Third, comprehensive experiments on the BraTS 2021 dataset demonstrate that BMDS-Net not only maintains competitive accuracy but, more importantly, exhibits superior stability in missing-modality scenarios where baseline models fail. The source code is publicly available at this https URL."
    },
    {
      "title": "QualiRAG: Retrieval-Augmented Generation for Visual Quality Understanding",
      "url": "https://arxiv.org/abs/2601.18195",
      "date": "2026-01-26",
      "authors_text": "Linhan Cao, Wei Sun, Weixia Zhang, Xiangyang Zhu, Kaiwei Zhang",
      "is_highlight": false,
      "score": 66.0,
      "summary": "QualiRAG introduces a training-free framework for visual quality assessment that utilizes dynamic retrieval and multimodal knowledge to enhance interpretability and performance without task-specific training.",
      "debug_abstract": "Visual quality assessment (VQA) is increasingly shifting from scalar score prediction toward interpretable quality understanding -- a paradigm that demands \\textit{fine-grained spatiotemporal perception} and \\textit{auxiliary contextual information}. Current approaches rely on supervised fine-tuning or reinforcement learning on curated instruction datasets, which involve labor-intensive annotation and are prone to dataset-specific biases. To address these challenges, we propose \\textbf{QualiRAG}, a \\textit{training-free} \\textbf{R}etrieval-\\textbf{A}ugmented \\textbf{G}eneration \\textbf{(RAG)} framework that systematically leverages the latent perceptual knowledge of large multimodal models (LMMs) for visual quality perception. Unlike conventional RAG that retrieves from static corpora, QualiRAG dynamically generates auxiliary knowledge by decomposing questions into structured requests and constructing four complementary knowledge sources: \\textit{visual metadata}, \\textit{subject localization}, \\textit{global quality summaries}, and \\textit{local quality descriptions}, followed by relevance-aware retrieval for evidence-grounded reasoning. Extensive experiments show that QualiRAG achieves substantial improvements over open-source general-purpose LMMs and VQA-finetuned LMMs on visual quality understanding tasks, and delivers competitive performance on visual quality comparison tasks, demonstrating robust quality assessment capabilities without any task-specific training. The code will be publicly available at this https URL."
    },
    {
      "title": "ExoGS: A 4D Real-to-Sim-to-Real Framework for Scalable Manipulation Data Collection",
      "url": "https://arxiv.org/abs/2601.18629",
      "date": "2026-01-26",
      "authors_text": "Yiming Wang, Ruogu Zhang, Minyang Li, Hao Shi, Junbo Wang",
      "is_highlight": false,
      "score": 93.0,
      "summary": "ExoGS introduces a 4D framework for efficient manipulation data collection by seamlessly transferring real-world interactions to simulation, enhancing policy learning and generalization.",
      "debug_abstract": "Real-to-Sim-to-Real technique is gaining increasing interest for robotic manipulation, as it can generate scalable data in simulation while having narrower sim-to-real gap. However, previous methods mainly focused on environment-level visual real-to-sim transfer, ignoring the transfer of interactions, which could be challenging and inefficient to obtain purely in simulation especially for contact-rich tasks. We propose ExoGS, a robot-free 4D Real-to-Sim-to-Real framework that captures both static environments and dynamic interactions in the real world and transfers them seamlessly to a simulated environment. It provides a new solution for scalable manipulation data collection and policy learning. ExoGS employs a self-designed robot-isomorphic passive exoskeleton AirExo-3 to capture kinematically consistent trajectories with millimeter-level accuracy and synchronized RGB observations during direct human demonstrations. The robot, objects, and environment are reconstructed as editable 3D Gaussian Splatting assets, enabling geometry-consistent replay and large-scale data augmentation. Additionally, a lightweight Mask Adapter injects instance-level semantics into the policy to enhance robustness under visual domain shifts. Real-world experiments demonstrate that ExoGS significantly improves data efficiency and policy generalization compared to teleoperation-based baselines. Code and hardware files have been released on this https URL."
    },
    {
      "title": "Advances and Innovations in the Multi-Agent Robotic System (MARS) Challenge",
      "url": "https://arxiv.org/abs/2601.18733",
      "date": "2026-01-26",
      "authors_text": "Li Kang, Heng Zhou, Xiufeng Song, Rui Li, Bruno N. Y. Chen",
      "is_highlight": false,
      "score": 81.0,
      "summary": "The MARS Challenge at NeurIPS 2025 aims to enhance multi-agent collaboration in embodied AI through competition in planning and control using vision-language models.",
      "debug_abstract": "Recent advancements in multimodal large language models and vision-languageaction models have significantly driven progress in Embodied AI. As the field transitions toward more complex task scenarios, multi-agent system frameworks are becoming essential for achieving scalable, efficient, and collaborative solutions. This shift is fueled by three primary factors: increasing agent capabilities, enhancing system efficiency through task delegation, and enabling advanced human-agent interactions. To address the challenges posed by multi-agent collaboration, we propose the Multi-Agent Robotic System (MARS) Challenge, held at the NeurIPS 2025 Workshop on SpaVLE. The competition focuses on two critical areas: planning and control, where participants explore multi-agent embodied planning using vision-language models (VLMs) to coordinate tasks and policy execution to perform robotic manipulation in dynamic environments. By evaluating solutions submitted by participants, the challenge provides valuable insights into the design and coordination of embodied multi-agent systems, contributing to the future development of advanced collaborative AI systems."
    }
  ],
  "智能机器人与机器视觉(IRMV)实验室": [
    {
      "title": "Embodiment-Aware Generalist Specialist Distillation for Unified Humanoid Whole-Body Control",
      "url": "https://arxiv.org/abs/2602.02960",
      "date": "2026-02-03",
      "authors_text": "Quanquan Peng, Yunfeng Lin, Yufei Xue, Jiangmiao Pang, Weinan Zhang",
      "is_highlight": false,
      "score": 90.0,
      "summary": "The EAGLE framework enables a unified policy for diverse humanoid robots, enhancing whole-body control through iterative generalist-specialist distillation without individual reward tuning.",
      "teaser_image": "https://arxiv.org/html/2602.02960/x1.png",
      "debug_abstract": "Humanoid Whole-Body Controllers trained with reinforcement learning (RL) have recently achieved remarkable performance, yet many target a single robot embodiment. Variations in dynamics, degrees of freedom (DoFs), and kinematic topology still hinder a single policy from commanding diverse humanoids. Moreover, obtaining a generalist policy that not only transfers across embodiments but also supports richer behaviors-beyond simple walking to squatting, leaning-remains especially challenging. In this work, we tackle these obstacles by introducing EAGLE, an iterative generalist-specialist distillation framework that produces a single unified policy that controls multiple heterogeneous humanoids without per-robot reward tuning. During each cycle, embodiment-specific specialists are forked from the current generalist, refined on their respective robots, and new skills are distilled back into the generalist by training on the pooled embodiment set. Repeating this loop until performance convergence produces a robust Whole-Body Controller validated on robots such as Unitree H1, G1, and Fourier N1. We conducted experiments on five different robots in simulation and four in real-world settings. Through quantitative evaluations, EAGLE achieves high tracking accuracy and robustness compared to other methods, marking a step toward scalable, fleet-level humanoid control. See more details at this https URL"
    },
    {
      "title": "Bongards at the Boundary of Perception and Reasoning: Programs or Language?",
      "url": "https://arxiv.org/abs/2602.03038",
      "date": "2026-02-03",
      "authors_text": "Cassidy Langenfeld, Claas Beger, Gloria Geng, Wasu Top Piriyakulkij, Keya Hu",
      "is_highlight": false,
      "score": 60.0,
      "summary": "This paper introduces a neurosymbolic method utilizing LLMs and Bayesian optimization to tackle Bongard problems, enhancing visual reasoning beyond conventional VLM capabilities.",
      "teaser_image": "https://arxiv.org/html/2602.03038/x3.png",
      "debug_abstract": "Vision-Language Models (VLMs) have made great strides in everyday visual tasks, such as captioning a natural image, or answering commonsense questions about such images. But humans possess the puzzling ability to deploy their visual reasoning abilities in radically new situations, a skill rigorously tested by the classic set of visual reasoning challenges known as the Bongard problems. We present a neurosymbolic approach to solving these problems: given a hypothesized solution rule for a Bongard problem, we leverage LLMs to generate parameterized programmatic representations for the rule and perform parameter fitting using Bayesian optimization. We evaluate our method on classifying Bongard problem images given the ground truth rule, as well as on solving the problems from scratch."
    },
    {
      "title": "Agentic Proposing: Enhancing Large Language Model Reasoning via Compositional Skill Synthesis",
      "url": "https://arxiv.org/abs/2602.03279",
      "date": "2026-02-03",
      "authors_text": "Zhengbo Jiao, Shaobo Wang, Zifan Zhang, Xuan Ren, Wei Wang",
      "is_highlight": false,
      "score": 71.0,
      "summary": "The paper introduces Agentic Proposing, a framework for generating high-quality synthetic training data that enhances reasoning in large language models, achieving superior performance with minimal human input.",
      "teaser_image": "https://arxiv.org/html/2602.03279/x2.png",
      "debug_abstract": "Advancing complex reasoning in large language models relies on high-quality, verifiable datasets, yet human annotation remains cost-prohibitive and difficult to scale. Current synthesis paradigms often face a recurring trade-off: maintaining structural validity typically restricts problem complexity, while relaxing constraints to increase difficulty frequently leads to inconsistent or unsolvable instances. To address this, we propose Agentic Proposing, a framework that models problem synthesis as a goal-driven sequential decision process where a specialized agent dynamically selects and composes modular reasoning skills. Through an iterative workflow of internal reflection and tool-use, we develop the Agentic-Proposer-4B using Multi-Granularity Policy Optimization (MGPO) to generate high-precision, verifiable training trajectories across mathematics, coding, and science. Empirical results demonstrate that downstream solvers trained on agent-synthesized data significantly outperform leading baselines and exhibit robust cross-domain generalization. Notably, a 30B solver trained on only 11,000 synthesized trajectories achieves a state-of-the-art 91.6% accuracy on AIME25, rivaling frontier-scale proprietary models such as GPT-5 and proving that a small volume of high-quality synthetic signals can effectively substitute for massive human-curated datasets."
    },
    {
      "title": "Socratic-Geo: Synthetic Data Generation and Geometric Reasoning via Multi-Agent Interaction",
      "url": "https://arxiv.org/abs/2602.03414",
      "date": "2026-02-03",
      "authors_text": "Zhengbo Jiao, Shaobo Wang, Zifan Zhang, Wei Wang, Bing Zhao",
      "is_highlight": false,
      "score": 58.0,
      "summary": "Socratic-Geo introduces a multi-agent framework for autonomous data synthesis and geometric reasoning, significantly improving model performance with minimal seed problems and surpassing existing benchmarks.",
      "teaser_image": "https://arxiv.org/html/2602.03414/x1.png",
      "debug_abstract": "Multimodal Large Language Models (MLLMs) have significantly advanced vision-language understanding. However, even state-of-the-art models struggle with geometric reasoning, revealing a critical bottleneck: the extreme scarcity of high-quality image-text pairs. Human annotation is prohibitively expensive, while automated methods fail to ensure fidelity and training effectiveness. Existing approaches either passively adapt to available images or employ inefficient random exploration with filtering, decoupling generation from learning needs. We propose Socratic-Geo, a fully autonomous framework that dynamically couples data synthesis with model learning through multi-agent interaction. The Teacher agent generates parameterized Python scripts with reflective feedback (Reflect for solvability, RePI for visual validity), ensuring image-text pair purity. The Solver agent optimizes reasoning through preference learning, with failure paths guiding Teacher&#39;s targeted augmentation. Independently, the Generator learns image generation capabilities on accumulated &#34;image-code-instruction&#34; triplets, distilling programmatic drawing intelligence into visual generation. Starting from only 108 seed problems, Socratic-Solver achieves 49.11 on six benchmarks using one-quarter of baseline data, surpassing strong baselines by 2.43 points. Socratic-Generator achieves 42.4% on GenExam, establishing new state-of-the-art for open-source models, surpassing Seedream-4.0 (39.8%) and approaching Gemini-2.5-Flash-Image (43.1%)."
    },
    {
      "title": "QVLA: Not All Channels Are Equal in Vision-Language-Action Model's Quantization",
      "url": "https://arxiv.org/abs/2602.03782",
      "date": "2026-02-03",
      "authors_text": "Yuhao Xu, Yantai Yang, Zhenyang Fan, Yufan Liu, Yuming Li",
      "is_highlight": false,
      "score": 47.0,
      "summary": "QVLA introduces an action-centric quantization framework for Vision-Language-Action models, optimizing channel-wise bit allocation to enhance performance and reduce resource demands in robotics.",
      "teaser_image": "https://arxiv.org/html/2602.03782/x2.png",
      "debug_abstract": "The advent of Vision-Language-Action (VLA) models represents a significant leap for embodied intelligence, yet their immense computational demands critically hinder deployment on resource-constrained robotic platforms. Intuitively, low-bit quantization is a prevalent and preferred technique for large-scale model compression. However, we find that a systematic analysis of VLA model&#39;s quantization is fundamentally lacking. We argue that naively applying uniform-bit quantization from Large Language Models (LLMs) to robotics is flawed, as these methods prioritize passive data fidelity while ignoring how minor action deviations compound into catastrophic task failures. To bridge this gap, we introduce QVLA, the first action-centric quantization framework specifically designed for embodied control. In a sharp departure from the rigid, uniform-bit quantization of LLM-based methods, QVLA introduces a highly granular, channel-wise bit allocation strategy. Its core mechanism is to directly measure the final action-space sensitivity when quantizing each individual channel to various bit-widths. This process yields a precise, per-channel importance metric that guides a global optimization, which elegantly unifies quantization and pruning (0-bit) into a single, cohesive framework. Extensive evaluations on different baselines demonstrate the superiority of our approach. In the LIBERO, the quantization version of OpenVLA-OFT with our method requires only 29.2% of the original model&#39;s VRAM while maintaining 98.9% of its original performance and achieving a 1.49x speedup. This translates to a 22.6% performance improvement over the LLM-derived method SmoothQuant. Our work establishes a new, principled foundation for compressing VLA models in robotics, paving the way for deploying powerful, large-scale models on real-world hardware. Code will be released."
    },
    {
      "title": "ConLA: Contrastive Latent Action Learning from Human Videos for Robotic Manipulation",
      "url": "https://arxiv.org/abs/2602.00557",
      "date": "2026-01-31",
      "authors_text": "Weisheng Dai, Kai Lan, Jianyi Zhou, Bo Zhao, Xiu Su",
      "is_highlight": true,
      "score": 91.0,
      "summary": "ConLA introduces a contrastive disentanglement framework for unsupervised robotic policy learning from human videos, enhancing transferability and performance beyond traditional robot trajectory pretraining.",
      "teaser_image": "https://arxiv.org/html/2602.00557/x3.png",
      "debug_abstract": "Vision-Language-Action (VLA) models achieve preliminary generalization through pretraining on large scale robot teleoperation datasets. However, acquiring datasets that comprehensively cover diverse tasks and environments is extremely costly and difficult to scale. In contrast, human demonstration videos offer a rich and scalable source of diverse scenes and manipulation behaviors, yet their lack of explicit action supervision hinders direct utilization. Prior work leverages VQ-VAE based frameworks to learn latent actions from human videos in an unsupervised manner. Nevertheless, since the training objective primarily focuses on reconstructing visual appearances rather than capturing inter-frame dynamics, the learned representations tend to rely on spurious visual cues, leading to shortcut learning and entangled latent representations that hinder transferability. To address this, we propose ConLA, an unsupervised pretraining framework for learning robotic policies from human videos. ConLA introduces a contrastive disentanglement mechanism that leverages action category priors and temporal cues to isolate motion dynamics from visual content, effectively mitigating shortcut learning. Extensive experiments show that ConLA achieves strong performance across diverse benchmarks. Notably, by pretraining solely on human videos, our method for the first time surpasses the performance obtained with real robot trajectory pretraining, highlighting its ability to extract pure and semantically consistent latent action representations for scalable robot learning."
    },
    {
      "title": "ProxyImg: Towards Highly-Controllable Image Representation via Hierarchical Disentangled Proxy Embedding",
      "url": "https://arxiv.org/abs/2602.01881",
      "date": "2026-02-02",
      "authors_text": "Ye Chen, Yupeng Zhu, Xiongzhen Zhang, Zhewen Wan, Yingzhe Li",
      "is_highlight": false,
      "score": 30.0,
      "summary": "The paper presents ProxyImg, a hierarchical proxy-based image representation that enables fine-grained, controllable editing by disentangling semantic, geometric, and textural attributes for high-fidelity reconstruction.",
      "teaser_image": "https://arxiv.org/html/2602.01881/x2.png",
      "debug_abstract": "Prevailing image representation methods, including explicit representations such as raster images and Gaussian primitives, as well as implicit representations such as latent images, either suffer from representation redundancy that leads to heavy manual editing effort, or lack a direct mapping from latent variables to semantic instances or parts, making fine-grained manipulation difficult. These limitations hinder efficient and controllable image and video editing. To address these issues, we propose a hierarchical proxy-based parametric image representation that disentangles semantic, geometric, and textural attributes into independent and manipulable parameter spaces. Based on a semantic-aware decomposition of the input image, our representation constructs hierarchical proxy geometries through adaptive Bezier fitting and iterative internal region subdivision and meshing. Multi-scale implicit texture parameters are embedded into the resulting geometry-aware distributed proxy nodes, enabling continuous high-fidelity reconstruction in the pixel domain and instance- or part-independent semantic editing. In addition, we introduce a locality-adaptive feature indexing mechanism to ensure spatial texture coherence, which further supports high-quality background completion without relying on generative models. Extensive experiments on image reconstruction and editing benchmarks, including ImageNet, OIR-Bench, and HumanEdit, demonstrate that our method achieves state-of-the-art rendering fidelity with significantly fewer parameters, while enabling intuitive, interactive, and physically plausible manipulation. Moreover, by integrating proxy nodes with Position-Based Dynamics, our framework supports real-time physics-driven animation using lightweight implicit rendering, achieving superior temporal consistency and visual realism compared with generative approaches."
    },
    {
      "title": "SurfSplat: Conquering Feedforward 2D Gaussian Splatting with Surface Continuity Priors",
      "url": "https://arxiv.org/abs/2602.02000",
      "date": "2026-02-02",
      "authors_text": "Bing He, Jingnan Gao, Yunuo Chen, Ning Cao, Gang Chen",
      "is_highlight": false,
      "score": 25.0,
      "summary": "SurfSplat introduces a feedforward 2D Gaussian Splatting framework that enhances 3D scene reconstruction by ensuring surface continuity and improving geometric precision over existing methods.",
      "teaser_image": "https://arxiv.org/html/2602.02000/x2.png",
      "debug_abstract": "Reconstructing 3D scenes from sparse images remains a challenging task due to the difficulty of recovering accurate geometry and texture without optimization. Recent approaches leverage generalizable models to generate 3D scenes using 3D Gaussian Splatting (3DGS) primitive. However, they often fail to produce continuous surfaces and instead yield discrete, color-biased point clouds that appear plausible at normal resolution but reveal severe artifacts under close-up views. To address this issue, we present SurfSplat, a feedforward framework based on 2D Gaussian Splatting (2DGS) primitive, which provides stronger anisotropy and higher geometric precision. By incorporating a surface continuity prior and a forced alpha blending strategy, SurfSplat reconstructs coherent geometry together with faithful textures. Furthermore, we introduce High-Resolution Rendering Consistency (HRRC), a new evaluation metric designed to evaluate high-resolution reconstruction quality. Extensive experiments on RealEstate10K, DL3DV, and ScanNet demonstrate that SurfSplat consistently outperforms prior methods on both standard metrics and HRRC, establishing a robust solution for high-fidelity 3D reconstruction from sparse inputs. Project page: this https URL"
    },
    {
      "title": "PLANING: A Loosely Coupled Triangle-Gaussian Framework for Streaming 3D Reconstruction",
      "url": "https://arxiv.org/abs/2601.22046",
      "date": "2026-01-29",
      "authors_text": "Changjian Jiang, Kerui Ren, Xudong Li, Kaiwen Song, Linning Xu",
      "is_highlight": false,
      "score": 66.0,
      "summary": "PLANING is a novel streaming 3D reconstruction framework that efficiently combines geometric primitives and neural Gaussians, enhancing quality and speed while reducing redundancy.",
      "teaser_image": "https://arxiv.org/html/2601.22046/x3.png",
      "debug_abstract": "Streaming reconstruction from monocular image sequences remains challenging, as existing methods typically favor either high-quality rendering or accurate geometry, but rarely both. We present PLANING, an efficient on-the-fly reconstruction framework built on a hybrid representation that loosely couples explicit geometric primitives with neural Gaussians, enabling geometry and appearance to be modeled in a decoupled manner. This decoupling supports an online initialization and optimization strategy that separates geometry and appearance updates, yielding stable streaming reconstruction with substantially reduced structural redundancy. PLANING improves dense mesh Chamfer-L2 by 18.52% over PGSR, surpasses ARTDECO by 1.31 dB PSNR, and reconstructs ScanNetV2 scenes in under 100 seconds, over 5x faster than 2D Gaussian Splatting, while matching the quality of offline per-scene optimization. Beyond reconstruction quality, the structural clarity and computational efficiency of \\modelname~make it well suited for a broad range of downstream applications, such as enabling large-scale scene modeling and simulation-ready environments for embodied AI. Project page: this https URL ."
    },
    {
      "title": "DynaWeb: Model-Based Reinforcement Learning of Web Agents",
      "url": "https://arxiv.org/abs/2601.22149",
      "date": "2026-01-29",
      "authors_text": "Hang Ding, Peidong Liu, Junqiao Wang, Ziwei Ji, Meng Cao",
      "is_highlight": false,
      "score": 77.0,
      "summary": "DynaWeb introduces a model-based reinforcement learning framework that enhances web agent training by simulating interactions with a web world model, improving efficiency and performance.",
      "teaser_image": "https://arxiv.org/html/2601.22149/figures/dynaweb.png",
      "debug_abstract": "The development of autonomous web agents, powered by Large Language Models (LLMs) and reinforcement learning (RL), represents a significant step towards general-purpose AI assistants. However, training these agents is severely hampered by the challenges of interacting with the live internet, which is inefficient, costly, and fraught with risks. Model-based reinforcement learning (MBRL) offers a promising solution by learning a world model of the environment to enable simulated interaction. This paper introduces DynaWeb, a novel MBRL framework that trains web agents through interacting with a web world model trained to predict naturalistic web page representations given agent actions. This model serves as a synthetic web environment where an agent policy can dream by generating vast quantities of rollout action trajectories for efficient online reinforcement learning. Beyond free policy rollouts, DynaWeb incorporates real expert trajectories from training data, which are randomly interleaved with on-policy rollouts during training to improve stability and sample efficiency. Experiments conducted on the challenging WebArena and WebVoyager benchmarks demonstrate that DynaWeb consistently and significantly improves the performance of state-of-the-art open-source web agent models. Our findings establish the viability of training web agents through imagination, offering a scalable and efficient way to scale up online agentic RL."
    },
    {
      "title": "E-mem: Multi-agent based Episodic Context Reconstruction for LLM Agent Memory",
      "url": "https://arxiv.org/abs/2601.21714",
      "date": "2026-01-29",
      "authors_text": "Kaixiang Wang, Yidan Lin, Jiong Lou, Zhaojiacheng Zhou, Bunyod Suvonov",
      "is_highlight": false,
      "score": 71.0,
      "summary": "E-mem introduces a multi-agent framework for episodic context reconstruction in LLMs, enhancing reasoning accuracy and efficiency while preserving contextual integrity.",
      "teaser_image": "https://arxiv.org/html/2601.21714/x2.png",
      "debug_abstract": "The evolution of Large Language Model (LLM) agents towards System~2 reasoning, characterized by deliberative, high-precision problem-solving, requires maintaining rigorous logical integrity over extended horizons. However, prevalent memory preprocessing paradigms suffer from destructive de-contextualization. By compressing complex sequential dependencies into pre-defined structures (e.g., embeddings or graphs), these methods sever the contextual integrity essential for deep reasoning. To address this, we propose E-mem, a framework shifting from Memory Preprocessing to Episodic Context Reconstruction. Inspired by biological engrams, E-mem employs a heterogeneous hierarchical architecture where multiple assistant agents maintain uncompressed memory contexts, while a central master agent orchestrates global planning. Unlike passive retrieval, our mechanism empowers assistants to locally reason within activated segments, extracting context-aware evidence before aggregation. Evaluations on the LoCoMo benchmark demonstrate that E-mem achieves over 54\\% F1, surpassing the state-of-the-art GAM by 7.75\\%, while reducing token cost by over 70\\%."
    },
    {
      "title": "TouchGuide: Inference-Time Steering of Visuomotor Policies via Touch Guidance",
      "url": "https://arxiv.org/abs/2601.20239",
      "date": "2026-01-28",
      "authors_text": "Zhemeng Zhang, Jiahua Ma, Xincheng Yang, Xin Wen, Yuzhi Zhang",
      "is_highlight": false,
      "score": 90.0,
      "summary": "TouchGuide enhances robot manipulation by integrating tactile feedback with visual inputs to refine actions during inference, outperforming existing visuo-tactile policies in complex tasks.",
      "teaser_image": "https://arxiv.org/html/2601.20239/x2.png",
      "debug_abstract": "Fine-grained and contact-rich manipulation remain challenging for robots, largely due to the underutilization of tactile feedback. To address this, we introduce TouchGuide, a novel cross-policy visuo-tactile fusion paradigm that fuses modalities within a low-dimensional action space. Specifically, TouchGuide operates in two stages to guide a pre-trained diffusion or flow-matching visuomotor policy at inference time. First, the policy produces a coarse, visually-plausible action using only visual inputs during early sampling. Second, a task-specific Contact Physical Model (CPM) provides tactile guidance to steer and refine the action, ensuring it aligns with realistic physical contact conditions. Trained through contrastive learning on limited expert demonstrations, the CPM provides a tactile-informed feasibility score to steer the sampling process toward refined actions that satisfy physical contact constraints. Furthermore, to facilitate TouchGuide training with high-quality and cost-effective data, we introduce TacUMI, a data collection system. TacUMI achieves a favorable trade-off between precision and affordability; by leveraging rigid fingertips, it obtains direct tactile feedback, thereby enabling the collection of reliable tactile data. Extensive experiments on five challenging contact-rich tasks, such as shoe lacing and chip handover, show that TouchGuide consistently and significantly outperforms state-of-the-art visuo-tactile policies."
    },
    {
      "title": "Innovator-VL: A Multimodal Large Language Model for Scientific Discovery",
      "url": "https://arxiv.org/abs/2601.19325",
      "date": "2026-01-27",
      "authors_text": "Zichen Wen, Boxue Yang, Shuang Chen, Yaojie Zhang, Yuhang Han",
      "is_highlight": false,
      "score": 78.0,
      "summary": "Innovator-VL is a multimodal large language model that enhances scientific discovery through efficient training, transparency, and strong generalization without relying on extensive pretraining.",
      "teaser_image": "https://arxiv.org/html/2601.19325/x8.png",
      "debug_abstract": "We present Innovator-VL, a scientific multimodal large language model designed to advance understanding and reasoning across diverse scientific domains while maintaining excellent performance on general vision tasks. Contrary to the trend of relying on massive domain-specific pretraining and opaque pipelines, our work demonstrates that principled training design and transparent methodology can yield strong scientific intelligence with substantially reduced data requirements. (i) First, we provide a fully transparent, end-to-end reproducible training pipeline, covering data collection, cleaning, preprocessing, supervised fine-tuning, reinforcement learning, and evaluation, along with detailed optimization recipes. This facilitates systematic extension by the community. (ii) Second, Innovator-VL exhibits remarkable data efficiency, achieving competitive performance on various scientific tasks using fewer than five million curated samples without large-scale pretraining. These results highlight that effective reasoning can be achieved through principled data selection rather than indiscriminate scaling. (iii) Third, Innovator-VL demonstrates strong generalization, achieving competitive performance on general vision, multimodal reasoning, and scientific benchmarks. This indicates that scientific alignment can be integrated into a unified model without compromising general-purpose capabilities. Our practices suggest that efficient, reproducible, and high-performing scientific multimodal models can be built even without large-scale data, providing a practical foundation for future research."
    },
    {
      "title": "PILOT: A Perceptive Integrated Low-level Controller for Loco-manipulation over Unstructured Scenes",
      "url": "https://arxiv.org/abs/2601.17440",
      "date": "2026-01-24",
      "authors_text": "Xinru Cui, Linxi Feng, Yixuan Zhou, Haoqi Han, Zhe Liu",
      "is_highlight": true,
      "score": 90.0,
      "summary": "PILOT is a unified reinforcement learning framework that integrates perceptive locomotion and whole-body control for stable loco-manipulation in unstructured environments, outperforming existing controllers.",
      "debug_abstract": "Humanoid robots hold great potential for diverse interactions and daily service tasks within human-centered environments, necessitating controllers that seamlessly integrate precise locomotion with dexterous manipulation. However, most existing whole-body controllers lack exteroceptive awareness of the surrounding environment, rendering them insufficient for stable task execution in complex, unstructured this http URL address this challenge, we propose PILOT, a unified single-stage reinforcement learning (RL) framework tailored for perceptive loco-manipulation, which synergizes perceptive locomotion and expansive whole-body control within a single policy. To enhance terrain awareness and ensure precise foot placement, we design a cross-modal context encoder that fuses prediction-based proprioceptive features with attention-based perceptive representations. Furthermore, we introduce a Mixture-of-Experts (MoE) policy architecture to coordinate diverse motor skills, facilitating better specialization across distinct motion patterns. Extensive experiments in both simulation and on the physical Unitree G1 humanoid robot validate the efficacy of our framework. PILOT demonstrates superior stability, command tracking precision, and terrain traversability compared to existing baselines. These results highlight its potential to serve as a robust, foundational low-level controller for loco-manipulation in unstructured scenes."
    },
    {
      "title": "BMDS-Net: A Bayesian Multi-Modal Deep Supervision Network for Robust Brain Tumor Segmentation",
      "url": "https://arxiv.org/abs/2601.17504",
      "date": "2026-01-24",
      "authors_text": "Yan Zhou, Zhen Huang, Yingqiu Li, Yue Ouyang, Suncheng Xiang",
      "is_highlight": false,
      "score": 50.0,
      "summary": "BMDS-Net enhances brain tumor segmentation by integrating robust feature learning and Bayesian uncertainty estimation, ensuring reliable performance despite missing MRI modalities.",
      "debug_abstract": "Accurate brain tumor segmentation from multi-modal magnetic resonance imaging (MRI) is a prerequisite for precise radiotherapy planning and surgical navigation. While recent Transformer-based models such as Swin UNETR have achieved impressive benchmark performance, their clinical utility is often compromised by two critical issues: sensitivity to missing modalities (common in clinical practice) and a lack of confidence calibration. Merely chasing higher Dice scores on idealized data fails to meet the safety requirements of real-world medical deployment. In this work, we propose BMDS-Net, a unified framework that prioritizes clinical robustness and trustworthiness over simple metric maximization. Our contribution is three-fold. First, we construct a robust deterministic backbone by integrating a Zero-Init Multimodal Contextual Fusion (MMCF) module and a Residual-Gated Deep Decoder Supervision (DDS) mechanism, enabling stable feature learning and precise boundary delineation with significantly reduced Hausdorff Distance, even under modality corruption. Second, and most importantly, we introduce a memory-efficient Bayesian fine-tuning strategy that transforms the network into a probabilistic predictor, providing voxel-wise uncertainty maps to highlight potential errors for clinicians. Third, comprehensive experiments on the BraTS 2021 dataset demonstrate that BMDS-Net not only maintains competitive accuracy but, more importantly, exhibits superior stability in missing-modality scenarios where baseline models fail. The source code is publicly available at this https URL."
    },
    {
      "title": "QualiRAG: Retrieval-Augmented Generation for Visual Quality Understanding",
      "url": "https://arxiv.org/abs/2601.18195",
      "date": "2026-01-26",
      "authors_text": "Linhan Cao, Wei Sun, Weixia Zhang, Xiangyang Zhu, Kaiwei Zhang",
      "is_highlight": false,
      "score": 66.0,
      "summary": "QualiRAG introduces a training-free framework for visual quality assessment that utilizes dynamic retrieval and multimodal knowledge to enhance interpretability and performance without task-specific training.",
      "debug_abstract": "Visual quality assessment (VQA) is increasingly shifting from scalar score prediction toward interpretable quality understanding -- a paradigm that demands \\textit{fine-grained spatiotemporal perception} and \\textit{auxiliary contextual information}. Current approaches rely on supervised fine-tuning or reinforcement learning on curated instruction datasets, which involve labor-intensive annotation and are prone to dataset-specific biases. To address these challenges, we propose \\textbf{QualiRAG}, a \\textit{training-free} \\textbf{R}etrieval-\\textbf{A}ugmented \\textbf{G}eneration \\textbf{(RAG)} framework that systematically leverages the latent perceptual knowledge of large multimodal models (LMMs) for visual quality perception. Unlike conventional RAG that retrieves from static corpora, QualiRAG dynamically generates auxiliary knowledge by decomposing questions into structured requests and constructing four complementary knowledge sources: \\textit{visual metadata}, \\textit{subject localization}, \\textit{global quality summaries}, and \\textit{local quality descriptions}, followed by relevance-aware retrieval for evidence-grounded reasoning. Extensive experiments show that QualiRAG achieves substantial improvements over open-source general-purpose LMMs and VQA-finetuned LMMs on visual quality understanding tasks, and delivers competitive performance on visual quality comparison tasks, demonstrating robust quality assessment capabilities without any task-specific training. The code will be publicly available at this https URL."
    },
    {
      "title": "ExoGS: A 4D Real-to-Sim-to-Real Framework for Scalable Manipulation Data Collection",
      "url": "https://arxiv.org/abs/2601.18629",
      "date": "2026-01-26",
      "authors_text": "Yiming Wang, Ruogu Zhang, Minyang Li, Hao Shi, Junbo Wang",
      "is_highlight": false,
      "score": 93.0,
      "summary": "ExoGS introduces a 4D framework for efficient manipulation data collection by seamlessly transferring real-world interactions to simulation, enhancing policy learning and generalization.",
      "debug_abstract": "Real-to-Sim-to-Real technique is gaining increasing interest for robotic manipulation, as it can generate scalable data in simulation while having narrower sim-to-real gap. However, previous methods mainly focused on environment-level visual real-to-sim transfer, ignoring the transfer of interactions, which could be challenging and inefficient to obtain purely in simulation especially for contact-rich tasks. We propose ExoGS, a robot-free 4D Real-to-Sim-to-Real framework that captures both static environments and dynamic interactions in the real world and transfers them seamlessly to a simulated environment. It provides a new solution for scalable manipulation data collection and policy learning. ExoGS employs a self-designed robot-isomorphic passive exoskeleton AirExo-3 to capture kinematically consistent trajectories with millimeter-level accuracy and synchronized RGB observations during direct human demonstrations. The robot, objects, and environment are reconstructed as editable 3D Gaussian Splatting assets, enabling geometry-consistent replay and large-scale data augmentation. Additionally, a lightweight Mask Adapter injects instance-level semantics into the policy to enhance robustness under visual domain shifts. Real-world experiments demonstrate that ExoGS significantly improves data efficiency and policy generalization compared to teleoperation-based baselines. Code and hardware files have been released on this https URL."
    },
    {
      "title": "Advances and Innovations in the Multi-Agent Robotic System (MARS) Challenge",
      "url": "https://arxiv.org/abs/2601.18733",
      "date": "2026-01-26",
      "authors_text": "Li Kang, Heng Zhou, Xiufeng Song, Rui Li, Bruno N. Y. Chen",
      "is_highlight": false,
      "score": 81.0,
      "summary": "The MARS Challenge at NeurIPS 2025 aims to enhance multi-agent collaboration in embodied AI through competition in planning and control using vision-language models.",
      "debug_abstract": "Recent advancements in multimodal large language models and vision-languageaction models have significantly driven progress in Embodied AI. As the field transitions toward more complex task scenarios, multi-agent system frameworks are becoming essential for achieving scalable, efficient, and collaborative solutions. This shift is fueled by three primary factors: increasing agent capabilities, enhancing system efficiency through task delegation, and enabling advanced human-agent interactions. To address the challenges posed by multi-agent collaboration, we propose the Multi-Agent Robotic System (MARS) Challenge, held at the NeurIPS 2025 Workshop on SpaVLE. The competition focuses on two critical areas: planning and control, where participants explore multi-agent embodied planning using vision-language models (VLMs) to coordinate tasks and policy execution to perform robotic manipulation in dynamic environments. By evaluating solutions submitted by participants, the challenge provides valuable insights into the design and coordination of embodied multi-agent systems, contributing to the future development of advanced collaborative AI systems."
    }
  ],
  "IWIN-FINS实验室": [
    {
      "title": "Embodiment-Aware Generalist Specialist Distillation for Unified Humanoid Whole-Body Control",
      "url": "https://arxiv.org/abs/2602.02960",
      "date": "2026-02-03",
      "authors_text": "Quanquan Peng, Yunfeng Lin, Yufei Xue, Jiangmiao Pang, Weinan Zhang",
      "is_highlight": false,
      "score": 90.0,
      "summary": "The EAGLE framework enables a unified policy for diverse humanoid robots, enhancing whole-body control through iterative generalist-specialist distillation without individual reward tuning.",
      "teaser_image": "https://arxiv.org/html/2602.02960/x1.png",
      "debug_abstract": "Humanoid Whole-Body Controllers trained with reinforcement learning (RL) have recently achieved remarkable performance, yet many target a single robot embodiment. Variations in dynamics, degrees of freedom (DoFs), and kinematic topology still hinder a single policy from commanding diverse humanoids. Moreover, obtaining a generalist policy that not only transfers across embodiments but also supports richer behaviors-beyond simple walking to squatting, leaning-remains especially challenging. In this work, we tackle these obstacles by introducing EAGLE, an iterative generalist-specialist distillation framework that produces a single unified policy that controls multiple heterogeneous humanoids without per-robot reward tuning. During each cycle, embodiment-specific specialists are forked from the current generalist, refined on their respective robots, and new skills are distilled back into the generalist by training on the pooled embodiment set. Repeating this loop until performance convergence produces a robust Whole-Body Controller validated on robots such as Unitree H1, G1, and Fourier N1. We conducted experiments on five different robots in simulation and four in real-world settings. Through quantitative evaluations, EAGLE achieves high tracking accuracy and robustness compared to other methods, marking a step toward scalable, fleet-level humanoid control. See more details at this https URL"
    },
    {
      "title": "Bongards at the Boundary of Perception and Reasoning: Programs or Language?",
      "url": "https://arxiv.org/abs/2602.03038",
      "date": "2026-02-03",
      "authors_text": "Cassidy Langenfeld, Claas Beger, Gloria Geng, Wasu Top Piriyakulkij, Keya Hu",
      "is_highlight": false,
      "score": 60.0,
      "summary": "This paper introduces a neurosymbolic method utilizing LLMs and Bayesian optimization to tackle Bongard problems, enhancing visual reasoning beyond conventional VLM capabilities.",
      "teaser_image": "https://arxiv.org/html/2602.03038/x3.png",
      "debug_abstract": "Vision-Language Models (VLMs) have made great strides in everyday visual tasks, such as captioning a natural image, or answering commonsense questions about such images. But humans possess the puzzling ability to deploy their visual reasoning abilities in radically new situations, a skill rigorously tested by the classic set of visual reasoning challenges known as the Bongard problems. We present a neurosymbolic approach to solving these problems: given a hypothesized solution rule for a Bongard problem, we leverage LLMs to generate parameterized programmatic representations for the rule and perform parameter fitting using Bayesian optimization. We evaluate our method on classifying Bongard problem images given the ground truth rule, as well as on solving the problems from scratch."
    },
    {
      "title": "Agentic Proposing: Enhancing Large Language Model Reasoning via Compositional Skill Synthesis",
      "url": "https://arxiv.org/abs/2602.03279",
      "date": "2026-02-03",
      "authors_text": "Zhengbo Jiao, Shaobo Wang, Zifan Zhang, Xuan Ren, Wei Wang",
      "is_highlight": false,
      "score": 71.0,
      "summary": "The paper introduces Agentic Proposing, a framework for generating high-quality synthetic training data that enhances reasoning in large language models, achieving superior performance with minimal human input.",
      "teaser_image": "https://arxiv.org/html/2602.03279/x2.png",
      "debug_abstract": "Advancing complex reasoning in large language models relies on high-quality, verifiable datasets, yet human annotation remains cost-prohibitive and difficult to scale. Current synthesis paradigms often face a recurring trade-off: maintaining structural validity typically restricts problem complexity, while relaxing constraints to increase difficulty frequently leads to inconsistent or unsolvable instances. To address this, we propose Agentic Proposing, a framework that models problem synthesis as a goal-driven sequential decision process where a specialized agent dynamically selects and composes modular reasoning skills. Through an iterative workflow of internal reflection and tool-use, we develop the Agentic-Proposer-4B using Multi-Granularity Policy Optimization (MGPO) to generate high-precision, verifiable training trajectories across mathematics, coding, and science. Empirical results demonstrate that downstream solvers trained on agent-synthesized data significantly outperform leading baselines and exhibit robust cross-domain generalization. Notably, a 30B solver trained on only 11,000 synthesized trajectories achieves a state-of-the-art 91.6% accuracy on AIME25, rivaling frontier-scale proprietary models such as GPT-5 and proving that a small volume of high-quality synthetic signals can effectively substitute for massive human-curated datasets."
    },
    {
      "title": "Socratic-Geo: Synthetic Data Generation and Geometric Reasoning via Multi-Agent Interaction",
      "url": "https://arxiv.org/abs/2602.03414",
      "date": "2026-02-03",
      "authors_text": "Zhengbo Jiao, Shaobo Wang, Zifan Zhang, Wei Wang, Bing Zhao",
      "is_highlight": false,
      "score": 58.0,
      "summary": "Socratic-Geo introduces a multi-agent framework for autonomous data synthesis and geometric reasoning, significantly improving model performance with minimal seed problems and surpassing existing benchmarks.",
      "teaser_image": "https://arxiv.org/html/2602.03414/x1.png",
      "debug_abstract": "Multimodal Large Language Models (MLLMs) have significantly advanced vision-language understanding. However, even state-of-the-art models struggle with geometric reasoning, revealing a critical bottleneck: the extreme scarcity of high-quality image-text pairs. Human annotation is prohibitively expensive, while automated methods fail to ensure fidelity and training effectiveness. Existing approaches either passively adapt to available images or employ inefficient random exploration with filtering, decoupling generation from learning needs. We propose Socratic-Geo, a fully autonomous framework that dynamically couples data synthesis with model learning through multi-agent interaction. The Teacher agent generates parameterized Python scripts with reflective feedback (Reflect for solvability, RePI for visual validity), ensuring image-text pair purity. The Solver agent optimizes reasoning through preference learning, with failure paths guiding Teacher&#39;s targeted augmentation. Independently, the Generator learns image generation capabilities on accumulated &#34;image-code-instruction&#34; triplets, distilling programmatic drawing intelligence into visual generation. Starting from only 108 seed problems, Socratic-Solver achieves 49.11 on six benchmarks using one-quarter of baseline data, surpassing strong baselines by 2.43 points. Socratic-Generator achieves 42.4% on GenExam, establishing new state-of-the-art for open-source models, surpassing Seedream-4.0 (39.8%) and approaching Gemini-2.5-Flash-Image (43.1%)."
    },
    {
      "title": "QVLA: Not All Channels Are Equal in Vision-Language-Action Model's Quantization",
      "url": "https://arxiv.org/abs/2602.03782",
      "date": "2026-02-03",
      "authors_text": "Yuhao Xu, Yantai Yang, Zhenyang Fan, Yufan Liu, Yuming Li",
      "is_highlight": false,
      "score": 47.0,
      "summary": "QVLA introduces an action-centric quantization framework for Vision-Language-Action models, optimizing channel-wise bit allocation to enhance performance and reduce resource demands in robotics.",
      "teaser_image": "https://arxiv.org/html/2602.03782/x2.png",
      "debug_abstract": "The advent of Vision-Language-Action (VLA) models represents a significant leap for embodied intelligence, yet their immense computational demands critically hinder deployment on resource-constrained robotic platforms. Intuitively, low-bit quantization is a prevalent and preferred technique for large-scale model compression. However, we find that a systematic analysis of VLA model&#39;s quantization is fundamentally lacking. We argue that naively applying uniform-bit quantization from Large Language Models (LLMs) to robotics is flawed, as these methods prioritize passive data fidelity while ignoring how minor action deviations compound into catastrophic task failures. To bridge this gap, we introduce QVLA, the first action-centric quantization framework specifically designed for embodied control. In a sharp departure from the rigid, uniform-bit quantization of LLM-based methods, QVLA introduces a highly granular, channel-wise bit allocation strategy. Its core mechanism is to directly measure the final action-space sensitivity when quantizing each individual channel to various bit-widths. This process yields a precise, per-channel importance metric that guides a global optimization, which elegantly unifies quantization and pruning (0-bit) into a single, cohesive framework. Extensive evaluations on different baselines demonstrate the superiority of our approach. In the LIBERO, the quantization version of OpenVLA-OFT with our method requires only 29.2% of the original model&#39;s VRAM while maintaining 98.9% of its original performance and achieving a 1.49x speedup. This translates to a 22.6% performance improvement over the LLM-derived method SmoothQuant. Our work establishes a new, principled foundation for compressing VLA models in robotics, paving the way for deploying powerful, large-scale models on real-world hardware. Code will be released."
    },
    {
      "title": "ConLA: Contrastive Latent Action Learning from Human Videos for Robotic Manipulation",
      "url": "https://arxiv.org/abs/2602.00557",
      "date": "2026-01-31",
      "authors_text": "Weisheng Dai, Kai Lan, Jianyi Zhou, Bo Zhao, Xiu Su",
      "is_highlight": true,
      "score": 91.0,
      "summary": "ConLA introduces a contrastive disentanglement framework for unsupervised robotic policy learning from human videos, enhancing transferability and performance beyond traditional robot trajectory pretraining.",
      "teaser_image": "https://arxiv.org/html/2602.00557/x3.png",
      "debug_abstract": "Vision-Language-Action (VLA) models achieve preliminary generalization through pretraining on large scale robot teleoperation datasets. However, acquiring datasets that comprehensively cover diverse tasks and environments is extremely costly and difficult to scale. In contrast, human demonstration videos offer a rich and scalable source of diverse scenes and manipulation behaviors, yet their lack of explicit action supervision hinders direct utilization. Prior work leverages VQ-VAE based frameworks to learn latent actions from human videos in an unsupervised manner. Nevertheless, since the training objective primarily focuses on reconstructing visual appearances rather than capturing inter-frame dynamics, the learned representations tend to rely on spurious visual cues, leading to shortcut learning and entangled latent representations that hinder transferability. To address this, we propose ConLA, an unsupervised pretraining framework for learning robotic policies from human videos. ConLA introduces a contrastive disentanglement mechanism that leverages action category priors and temporal cues to isolate motion dynamics from visual content, effectively mitigating shortcut learning. Extensive experiments show that ConLA achieves strong performance across diverse benchmarks. Notably, by pretraining solely on human videos, our method for the first time surpasses the performance obtained with real robot trajectory pretraining, highlighting its ability to extract pure and semantically consistent latent action representations for scalable robot learning."
    },
    {
      "title": "ProxyImg: Towards Highly-Controllable Image Representation via Hierarchical Disentangled Proxy Embedding",
      "url": "https://arxiv.org/abs/2602.01881",
      "date": "2026-02-02",
      "authors_text": "Ye Chen, Yupeng Zhu, Xiongzhen Zhang, Zhewen Wan, Yingzhe Li",
      "is_highlight": false,
      "score": 30.0,
      "summary": "The paper presents ProxyImg, a hierarchical proxy-based image representation that enables fine-grained, controllable editing by disentangling semantic, geometric, and textural attributes for high-fidelity reconstruction.",
      "teaser_image": "https://arxiv.org/html/2602.01881/x2.png",
      "debug_abstract": "Prevailing image representation methods, including explicit representations such as raster images and Gaussian primitives, as well as implicit representations such as latent images, either suffer from representation redundancy that leads to heavy manual editing effort, or lack a direct mapping from latent variables to semantic instances or parts, making fine-grained manipulation difficult. These limitations hinder efficient and controllable image and video editing. To address these issues, we propose a hierarchical proxy-based parametric image representation that disentangles semantic, geometric, and textural attributes into independent and manipulable parameter spaces. Based on a semantic-aware decomposition of the input image, our representation constructs hierarchical proxy geometries through adaptive Bezier fitting and iterative internal region subdivision and meshing. Multi-scale implicit texture parameters are embedded into the resulting geometry-aware distributed proxy nodes, enabling continuous high-fidelity reconstruction in the pixel domain and instance- or part-independent semantic editing. In addition, we introduce a locality-adaptive feature indexing mechanism to ensure spatial texture coherence, which further supports high-quality background completion without relying on generative models. Extensive experiments on image reconstruction and editing benchmarks, including ImageNet, OIR-Bench, and HumanEdit, demonstrate that our method achieves state-of-the-art rendering fidelity with significantly fewer parameters, while enabling intuitive, interactive, and physically plausible manipulation. Moreover, by integrating proxy nodes with Position-Based Dynamics, our framework supports real-time physics-driven animation using lightweight implicit rendering, achieving superior temporal consistency and visual realism compared with generative approaches."
    },
    {
      "title": "SurfSplat: Conquering Feedforward 2D Gaussian Splatting with Surface Continuity Priors",
      "url": "https://arxiv.org/abs/2602.02000",
      "date": "2026-02-02",
      "authors_text": "Bing He, Jingnan Gao, Yunuo Chen, Ning Cao, Gang Chen",
      "is_highlight": false,
      "score": 25.0,
      "summary": "SurfSplat introduces a feedforward 2D Gaussian Splatting framework that enhances 3D scene reconstruction by ensuring surface continuity and improving geometric precision over existing methods.",
      "teaser_image": "https://arxiv.org/html/2602.02000/x2.png",
      "debug_abstract": "Reconstructing 3D scenes from sparse images remains a challenging task due to the difficulty of recovering accurate geometry and texture without optimization. Recent approaches leverage generalizable models to generate 3D scenes using 3D Gaussian Splatting (3DGS) primitive. However, they often fail to produce continuous surfaces and instead yield discrete, color-biased point clouds that appear plausible at normal resolution but reveal severe artifacts under close-up views. To address this issue, we present SurfSplat, a feedforward framework based on 2D Gaussian Splatting (2DGS) primitive, which provides stronger anisotropy and higher geometric precision. By incorporating a surface continuity prior and a forced alpha blending strategy, SurfSplat reconstructs coherent geometry together with faithful textures. Furthermore, we introduce High-Resolution Rendering Consistency (HRRC), a new evaluation metric designed to evaluate high-resolution reconstruction quality. Extensive experiments on RealEstate10K, DL3DV, and ScanNet demonstrate that SurfSplat consistently outperforms prior methods on both standard metrics and HRRC, establishing a robust solution for high-fidelity 3D reconstruction from sparse inputs. Project page: this https URL"
    },
    {
      "title": "PLANING: A Loosely Coupled Triangle-Gaussian Framework for Streaming 3D Reconstruction",
      "url": "https://arxiv.org/abs/2601.22046",
      "date": "2026-01-29",
      "authors_text": "Changjian Jiang, Kerui Ren, Xudong Li, Kaiwen Song, Linning Xu",
      "is_highlight": false,
      "score": 66.0,
      "summary": "PLANING is a novel streaming 3D reconstruction framework that efficiently combines geometric primitives and neural Gaussians, enhancing quality and speed while reducing redundancy.",
      "teaser_image": "https://arxiv.org/html/2601.22046/x3.png",
      "debug_abstract": "Streaming reconstruction from monocular image sequences remains challenging, as existing methods typically favor either high-quality rendering or accurate geometry, but rarely both. We present PLANING, an efficient on-the-fly reconstruction framework built on a hybrid representation that loosely couples explicit geometric primitives with neural Gaussians, enabling geometry and appearance to be modeled in a decoupled manner. This decoupling supports an online initialization and optimization strategy that separates geometry and appearance updates, yielding stable streaming reconstruction with substantially reduced structural redundancy. PLANING improves dense mesh Chamfer-L2 by 18.52% over PGSR, surpasses ARTDECO by 1.31 dB PSNR, and reconstructs ScanNetV2 scenes in under 100 seconds, over 5x faster than 2D Gaussian Splatting, while matching the quality of offline per-scene optimization. Beyond reconstruction quality, the structural clarity and computational efficiency of \\modelname~make it well suited for a broad range of downstream applications, such as enabling large-scale scene modeling and simulation-ready environments for embodied AI. Project page: this https URL ."
    },
    {
      "title": "DynaWeb: Model-Based Reinforcement Learning of Web Agents",
      "url": "https://arxiv.org/abs/2601.22149",
      "date": "2026-01-29",
      "authors_text": "Hang Ding, Peidong Liu, Junqiao Wang, Ziwei Ji, Meng Cao",
      "is_highlight": false,
      "score": 77.0,
      "summary": "DynaWeb introduces a model-based reinforcement learning framework that enhances web agent training by simulating interactions with a web world model, improving efficiency and performance.",
      "teaser_image": "https://arxiv.org/html/2601.22149/figures/dynaweb.png",
      "debug_abstract": "The development of autonomous web agents, powered by Large Language Models (LLMs) and reinforcement learning (RL), represents a significant step towards general-purpose AI assistants. However, training these agents is severely hampered by the challenges of interacting with the live internet, which is inefficient, costly, and fraught with risks. Model-based reinforcement learning (MBRL) offers a promising solution by learning a world model of the environment to enable simulated interaction. This paper introduces DynaWeb, a novel MBRL framework that trains web agents through interacting with a web world model trained to predict naturalistic web page representations given agent actions. This model serves as a synthetic web environment where an agent policy can dream by generating vast quantities of rollout action trajectories for efficient online reinforcement learning. Beyond free policy rollouts, DynaWeb incorporates real expert trajectories from training data, which are randomly interleaved with on-policy rollouts during training to improve stability and sample efficiency. Experiments conducted on the challenging WebArena and WebVoyager benchmarks demonstrate that DynaWeb consistently and significantly improves the performance of state-of-the-art open-source web agent models. Our findings establish the viability of training web agents through imagination, offering a scalable and efficient way to scale up online agentic RL."
    },
    {
      "title": "E-mem: Multi-agent based Episodic Context Reconstruction for LLM Agent Memory",
      "url": "https://arxiv.org/abs/2601.21714",
      "date": "2026-01-29",
      "authors_text": "Kaixiang Wang, Yidan Lin, Jiong Lou, Zhaojiacheng Zhou, Bunyod Suvonov",
      "is_highlight": false,
      "score": 71.0,
      "summary": "E-mem introduces a multi-agent framework for episodic context reconstruction in LLMs, enhancing reasoning accuracy and efficiency while preserving contextual integrity.",
      "teaser_image": "https://arxiv.org/html/2601.21714/x2.png",
      "debug_abstract": "The evolution of Large Language Model (LLM) agents towards System~2 reasoning, characterized by deliberative, high-precision problem-solving, requires maintaining rigorous logical integrity over extended horizons. However, prevalent memory preprocessing paradigms suffer from destructive de-contextualization. By compressing complex sequential dependencies into pre-defined structures (e.g., embeddings or graphs), these methods sever the contextual integrity essential for deep reasoning. To address this, we propose E-mem, a framework shifting from Memory Preprocessing to Episodic Context Reconstruction. Inspired by biological engrams, E-mem employs a heterogeneous hierarchical architecture where multiple assistant agents maintain uncompressed memory contexts, while a central master agent orchestrates global planning. Unlike passive retrieval, our mechanism empowers assistants to locally reason within activated segments, extracting context-aware evidence before aggregation. Evaluations on the LoCoMo benchmark demonstrate that E-mem achieves over 54\\% F1, surpassing the state-of-the-art GAM by 7.75\\%, while reducing token cost by over 70\\%."
    },
    {
      "title": "TouchGuide: Inference-Time Steering of Visuomotor Policies via Touch Guidance",
      "url": "https://arxiv.org/abs/2601.20239",
      "date": "2026-01-28",
      "authors_text": "Zhemeng Zhang, Jiahua Ma, Xincheng Yang, Xin Wen, Yuzhi Zhang",
      "is_highlight": false,
      "score": 90.0,
      "summary": "TouchGuide enhances robot manipulation by integrating tactile feedback with visual inputs to refine actions during inference, outperforming existing visuo-tactile policies in complex tasks.",
      "teaser_image": "https://arxiv.org/html/2601.20239/x2.png",
      "debug_abstract": "Fine-grained and contact-rich manipulation remain challenging for robots, largely due to the underutilization of tactile feedback. To address this, we introduce TouchGuide, a novel cross-policy visuo-tactile fusion paradigm that fuses modalities within a low-dimensional action space. Specifically, TouchGuide operates in two stages to guide a pre-trained diffusion or flow-matching visuomotor policy at inference time. First, the policy produces a coarse, visually-plausible action using only visual inputs during early sampling. Second, a task-specific Contact Physical Model (CPM) provides tactile guidance to steer and refine the action, ensuring it aligns with realistic physical contact conditions. Trained through contrastive learning on limited expert demonstrations, the CPM provides a tactile-informed feasibility score to steer the sampling process toward refined actions that satisfy physical contact constraints. Furthermore, to facilitate TouchGuide training with high-quality and cost-effective data, we introduce TacUMI, a data collection system. TacUMI achieves a favorable trade-off between precision and affordability; by leveraging rigid fingertips, it obtains direct tactile feedback, thereby enabling the collection of reliable tactile data. Extensive experiments on five challenging contact-rich tasks, such as shoe lacing and chip handover, show that TouchGuide consistently and significantly outperforms state-of-the-art visuo-tactile policies."
    },
    {
      "title": "Innovator-VL: A Multimodal Large Language Model for Scientific Discovery",
      "url": "https://arxiv.org/abs/2601.19325",
      "date": "2026-01-27",
      "authors_text": "Zichen Wen, Boxue Yang, Shuang Chen, Yaojie Zhang, Yuhang Han",
      "is_highlight": false,
      "score": 78.0,
      "summary": "Innovator-VL is a multimodal large language model that enhances scientific discovery through efficient training, transparency, and strong generalization without relying on extensive pretraining.",
      "teaser_image": "https://arxiv.org/html/2601.19325/x8.png",
      "debug_abstract": "We present Innovator-VL, a scientific multimodal large language model designed to advance understanding and reasoning across diverse scientific domains while maintaining excellent performance on general vision tasks. Contrary to the trend of relying on massive domain-specific pretraining and opaque pipelines, our work demonstrates that principled training design and transparent methodology can yield strong scientific intelligence with substantially reduced data requirements. (i) First, we provide a fully transparent, end-to-end reproducible training pipeline, covering data collection, cleaning, preprocessing, supervised fine-tuning, reinforcement learning, and evaluation, along with detailed optimization recipes. This facilitates systematic extension by the community. (ii) Second, Innovator-VL exhibits remarkable data efficiency, achieving competitive performance on various scientific tasks using fewer than five million curated samples without large-scale pretraining. These results highlight that effective reasoning can be achieved through principled data selection rather than indiscriminate scaling. (iii) Third, Innovator-VL demonstrates strong generalization, achieving competitive performance on general vision, multimodal reasoning, and scientific benchmarks. This indicates that scientific alignment can be integrated into a unified model without compromising general-purpose capabilities. Our practices suggest that efficient, reproducible, and high-performing scientific multimodal models can be built even without large-scale data, providing a practical foundation for future research."
    },
    {
      "title": "PILOT: A Perceptive Integrated Low-level Controller for Loco-manipulation over Unstructured Scenes",
      "url": "https://arxiv.org/abs/2601.17440",
      "date": "2026-01-24",
      "authors_text": "Xinru Cui, Linxi Feng, Yixuan Zhou, Haoqi Han, Zhe Liu",
      "is_highlight": true,
      "score": 90.0,
      "summary": "PILOT is a unified reinforcement learning framework that integrates perceptive locomotion and whole-body control for stable loco-manipulation in unstructured environments, outperforming existing controllers.",
      "debug_abstract": "Humanoid robots hold great potential for diverse interactions and daily service tasks within human-centered environments, necessitating controllers that seamlessly integrate precise locomotion with dexterous manipulation. However, most existing whole-body controllers lack exteroceptive awareness of the surrounding environment, rendering them insufficient for stable task execution in complex, unstructured this http URL address this challenge, we propose PILOT, a unified single-stage reinforcement learning (RL) framework tailored for perceptive loco-manipulation, which synergizes perceptive locomotion and expansive whole-body control within a single policy. To enhance terrain awareness and ensure precise foot placement, we design a cross-modal context encoder that fuses prediction-based proprioceptive features with attention-based perceptive representations. Furthermore, we introduce a Mixture-of-Experts (MoE) policy architecture to coordinate diverse motor skills, facilitating better specialization across distinct motion patterns. Extensive experiments in both simulation and on the physical Unitree G1 humanoid robot validate the efficacy of our framework. PILOT demonstrates superior stability, command tracking precision, and terrain traversability compared to existing baselines. These results highlight its potential to serve as a robust, foundational low-level controller for loco-manipulation in unstructured scenes."
    },
    {
      "title": "BMDS-Net: A Bayesian Multi-Modal Deep Supervision Network for Robust Brain Tumor Segmentation",
      "url": "https://arxiv.org/abs/2601.17504",
      "date": "2026-01-24",
      "authors_text": "Yan Zhou, Zhen Huang, Yingqiu Li, Yue Ouyang, Suncheng Xiang",
      "is_highlight": false,
      "score": 50.0,
      "summary": "BMDS-Net enhances brain tumor segmentation by integrating robust feature learning and Bayesian uncertainty estimation, ensuring reliable performance despite missing MRI modalities.",
      "debug_abstract": "Accurate brain tumor segmentation from multi-modal magnetic resonance imaging (MRI) is a prerequisite for precise radiotherapy planning and surgical navigation. While recent Transformer-based models such as Swin UNETR have achieved impressive benchmark performance, their clinical utility is often compromised by two critical issues: sensitivity to missing modalities (common in clinical practice) and a lack of confidence calibration. Merely chasing higher Dice scores on idealized data fails to meet the safety requirements of real-world medical deployment. In this work, we propose BMDS-Net, a unified framework that prioritizes clinical robustness and trustworthiness over simple metric maximization. Our contribution is three-fold. First, we construct a robust deterministic backbone by integrating a Zero-Init Multimodal Contextual Fusion (MMCF) module and a Residual-Gated Deep Decoder Supervision (DDS) mechanism, enabling stable feature learning and precise boundary delineation with significantly reduced Hausdorff Distance, even under modality corruption. Second, and most importantly, we introduce a memory-efficient Bayesian fine-tuning strategy that transforms the network into a probabilistic predictor, providing voxel-wise uncertainty maps to highlight potential errors for clinicians. Third, comprehensive experiments on the BraTS 2021 dataset demonstrate that BMDS-Net not only maintains competitive accuracy but, more importantly, exhibits superior stability in missing-modality scenarios where baseline models fail. The source code is publicly available at this https URL."
    },
    {
      "title": "QualiRAG: Retrieval-Augmented Generation for Visual Quality Understanding",
      "url": "https://arxiv.org/abs/2601.18195",
      "date": "2026-01-26",
      "authors_text": "Linhan Cao, Wei Sun, Weixia Zhang, Xiangyang Zhu, Kaiwei Zhang",
      "is_highlight": false,
      "score": 66.0,
      "summary": "QualiRAG introduces a training-free framework for visual quality assessment that utilizes dynamic retrieval and multimodal knowledge to enhance interpretability and performance without task-specific training.",
      "debug_abstract": "Visual quality assessment (VQA) is increasingly shifting from scalar score prediction toward interpretable quality understanding -- a paradigm that demands \\textit{fine-grained spatiotemporal perception} and \\textit{auxiliary contextual information}. Current approaches rely on supervised fine-tuning or reinforcement learning on curated instruction datasets, which involve labor-intensive annotation and are prone to dataset-specific biases. To address these challenges, we propose \\textbf{QualiRAG}, a \\textit{training-free} \\textbf{R}etrieval-\\textbf{A}ugmented \\textbf{G}eneration \\textbf{(RAG)} framework that systematically leverages the latent perceptual knowledge of large multimodal models (LMMs) for visual quality perception. Unlike conventional RAG that retrieves from static corpora, QualiRAG dynamically generates auxiliary knowledge by decomposing questions into structured requests and constructing four complementary knowledge sources: \\textit{visual metadata}, \\textit{subject localization}, \\textit{global quality summaries}, and \\textit{local quality descriptions}, followed by relevance-aware retrieval for evidence-grounded reasoning. Extensive experiments show that QualiRAG achieves substantial improvements over open-source general-purpose LMMs and VQA-finetuned LMMs on visual quality understanding tasks, and delivers competitive performance on visual quality comparison tasks, demonstrating robust quality assessment capabilities without any task-specific training. The code will be publicly available at this https URL."
    },
    {
      "title": "ExoGS: A 4D Real-to-Sim-to-Real Framework for Scalable Manipulation Data Collection",
      "url": "https://arxiv.org/abs/2601.18629",
      "date": "2026-01-26",
      "authors_text": "Yiming Wang, Ruogu Zhang, Minyang Li, Hao Shi, Junbo Wang",
      "is_highlight": false,
      "score": 93.0,
      "summary": "ExoGS introduces a 4D framework for efficient manipulation data collection by seamlessly transferring real-world interactions to simulation, enhancing policy learning and generalization.",
      "debug_abstract": "Real-to-Sim-to-Real technique is gaining increasing interest for robotic manipulation, as it can generate scalable data in simulation while having narrower sim-to-real gap. However, previous methods mainly focused on environment-level visual real-to-sim transfer, ignoring the transfer of interactions, which could be challenging and inefficient to obtain purely in simulation especially for contact-rich tasks. We propose ExoGS, a robot-free 4D Real-to-Sim-to-Real framework that captures both static environments and dynamic interactions in the real world and transfers them seamlessly to a simulated environment. It provides a new solution for scalable manipulation data collection and policy learning. ExoGS employs a self-designed robot-isomorphic passive exoskeleton AirExo-3 to capture kinematically consistent trajectories with millimeter-level accuracy and synchronized RGB observations during direct human demonstrations. The robot, objects, and environment are reconstructed as editable 3D Gaussian Splatting assets, enabling geometry-consistent replay and large-scale data augmentation. Additionally, a lightweight Mask Adapter injects instance-level semantics into the policy to enhance robustness under visual domain shifts. Real-world experiments demonstrate that ExoGS significantly improves data efficiency and policy generalization compared to teleoperation-based baselines. Code and hardware files have been released on this https URL."
    },
    {
      "title": "Advances and Innovations in the Multi-Agent Robotic System (MARS) Challenge",
      "url": "https://arxiv.org/abs/2601.18733",
      "date": "2026-01-26",
      "authors_text": "Li Kang, Heng Zhou, Xiufeng Song, Rui Li, Bruno N. Y. Chen",
      "is_highlight": false,
      "score": 81.0,
      "summary": "The MARS Challenge at NeurIPS 2025 aims to enhance multi-agent collaboration in embodied AI through competition in planning and control using vision-language models.",
      "debug_abstract": "Recent advancements in multimodal large language models and vision-languageaction models have significantly driven progress in Embodied AI. As the field transitions toward more complex task scenarios, multi-agent system frameworks are becoming essential for achieving scalable, efficient, and collaborative solutions. This shift is fueled by three primary factors: increasing agent capabilities, enhancing system efficiency through task delegation, and enabling advanced human-agent interactions. To address the challenges posed by multi-agent collaboration, we propose the Multi-Agent Robotic System (MARS) Challenge, held at the NeurIPS 2025 Workshop on SpaVLE. The competition focuses on two critical areas: planning and control, where participants explore multi-agent embodied planning using vision-language models (VLMs) to coordinate tasks and policy execution to perform robotic manipulation in dynamic environments. By evaluating solutions submitted by participants, the challenge provides valuable insights into the design and coordination of embodied multi-agent systems, contributing to the future development of advanced collaborative AI systems."
    }
  ],
  "数字媒体与计算机视觉实验室": [
    {
      "title": "Embodiment-Aware Generalist Specialist Distillation for Unified Humanoid Whole-Body Control",
      "url": "https://arxiv.org/abs/2602.02960",
      "date": "2026-02-03",
      "authors_text": "Quanquan Peng, Yunfeng Lin, Yufei Xue, Jiangmiao Pang, Weinan Zhang",
      "is_highlight": false,
      "score": 90.0,
      "summary": "The EAGLE framework enables a unified policy for diverse humanoid robots, enhancing whole-body control through iterative generalist-specialist distillation without individual reward tuning.",
      "teaser_image": "https://arxiv.org/html/2602.02960/x1.png",
      "debug_abstract": "Humanoid Whole-Body Controllers trained with reinforcement learning (RL) have recently achieved remarkable performance, yet many target a single robot embodiment. Variations in dynamics, degrees of freedom (DoFs), and kinematic topology still hinder a single policy from commanding diverse humanoids. Moreover, obtaining a generalist policy that not only transfers across embodiments but also supports richer behaviors-beyond simple walking to squatting, leaning-remains especially challenging. In this work, we tackle these obstacles by introducing EAGLE, an iterative generalist-specialist distillation framework that produces a single unified policy that controls multiple heterogeneous humanoids without per-robot reward tuning. During each cycle, embodiment-specific specialists are forked from the current generalist, refined on their respective robots, and new skills are distilled back into the generalist by training on the pooled embodiment set. Repeating this loop until performance convergence produces a robust Whole-Body Controller validated on robots such as Unitree H1, G1, and Fourier N1. We conducted experiments on five different robots in simulation and four in real-world settings. Through quantitative evaluations, EAGLE achieves high tracking accuracy and robustness compared to other methods, marking a step toward scalable, fleet-level humanoid control. See more details at this https URL"
    },
    {
      "title": "Bongards at the Boundary of Perception and Reasoning: Programs or Language?",
      "url": "https://arxiv.org/abs/2602.03038",
      "date": "2026-02-03",
      "authors_text": "Cassidy Langenfeld, Claas Beger, Gloria Geng, Wasu Top Piriyakulkij, Keya Hu",
      "is_highlight": false,
      "score": 60.0,
      "summary": "This paper introduces a neurosymbolic method utilizing LLMs and Bayesian optimization to tackle Bongard problems, enhancing visual reasoning beyond conventional VLM capabilities.",
      "teaser_image": "https://arxiv.org/html/2602.03038/x3.png",
      "debug_abstract": "Vision-Language Models (VLMs) have made great strides in everyday visual tasks, such as captioning a natural image, or answering commonsense questions about such images. But humans possess the puzzling ability to deploy their visual reasoning abilities in radically new situations, a skill rigorously tested by the classic set of visual reasoning challenges known as the Bongard problems. We present a neurosymbolic approach to solving these problems: given a hypothesized solution rule for a Bongard problem, we leverage LLMs to generate parameterized programmatic representations for the rule and perform parameter fitting using Bayesian optimization. We evaluate our method on classifying Bongard problem images given the ground truth rule, as well as on solving the problems from scratch."
    },
    {
      "title": "Agentic Proposing: Enhancing Large Language Model Reasoning via Compositional Skill Synthesis",
      "url": "https://arxiv.org/abs/2602.03279",
      "date": "2026-02-03",
      "authors_text": "Zhengbo Jiao, Shaobo Wang, Zifan Zhang, Xuan Ren, Wei Wang",
      "is_highlight": false,
      "score": 71.0,
      "summary": "The paper introduces Agentic Proposing, a framework for generating high-quality synthetic training data that enhances reasoning in large language models, achieving superior performance with minimal human input.",
      "teaser_image": "https://arxiv.org/html/2602.03279/x2.png",
      "debug_abstract": "Advancing complex reasoning in large language models relies on high-quality, verifiable datasets, yet human annotation remains cost-prohibitive and difficult to scale. Current synthesis paradigms often face a recurring trade-off: maintaining structural validity typically restricts problem complexity, while relaxing constraints to increase difficulty frequently leads to inconsistent or unsolvable instances. To address this, we propose Agentic Proposing, a framework that models problem synthesis as a goal-driven sequential decision process where a specialized agent dynamically selects and composes modular reasoning skills. Through an iterative workflow of internal reflection and tool-use, we develop the Agentic-Proposer-4B using Multi-Granularity Policy Optimization (MGPO) to generate high-precision, verifiable training trajectories across mathematics, coding, and science. Empirical results demonstrate that downstream solvers trained on agent-synthesized data significantly outperform leading baselines and exhibit robust cross-domain generalization. Notably, a 30B solver trained on only 11,000 synthesized trajectories achieves a state-of-the-art 91.6% accuracy on AIME25, rivaling frontier-scale proprietary models such as GPT-5 and proving that a small volume of high-quality synthetic signals can effectively substitute for massive human-curated datasets."
    },
    {
      "title": "Socratic-Geo: Synthetic Data Generation and Geometric Reasoning via Multi-Agent Interaction",
      "url": "https://arxiv.org/abs/2602.03414",
      "date": "2026-02-03",
      "authors_text": "Zhengbo Jiao, Shaobo Wang, Zifan Zhang, Wei Wang, Bing Zhao",
      "is_highlight": false,
      "score": 58.0,
      "summary": "Socratic-Geo introduces a multi-agent framework for autonomous data synthesis and geometric reasoning, significantly improving model performance with minimal seed problems and surpassing existing benchmarks.",
      "teaser_image": "https://arxiv.org/html/2602.03414/x1.png",
      "debug_abstract": "Multimodal Large Language Models (MLLMs) have significantly advanced vision-language understanding. However, even state-of-the-art models struggle with geometric reasoning, revealing a critical bottleneck: the extreme scarcity of high-quality image-text pairs. Human annotation is prohibitively expensive, while automated methods fail to ensure fidelity and training effectiveness. Existing approaches either passively adapt to available images or employ inefficient random exploration with filtering, decoupling generation from learning needs. We propose Socratic-Geo, a fully autonomous framework that dynamically couples data synthesis with model learning through multi-agent interaction. The Teacher agent generates parameterized Python scripts with reflective feedback (Reflect for solvability, RePI for visual validity), ensuring image-text pair purity. The Solver agent optimizes reasoning through preference learning, with failure paths guiding Teacher&#39;s targeted augmentation. Independently, the Generator learns image generation capabilities on accumulated &#34;image-code-instruction&#34; triplets, distilling programmatic drawing intelligence into visual generation. Starting from only 108 seed problems, Socratic-Solver achieves 49.11 on six benchmarks using one-quarter of baseline data, surpassing strong baselines by 2.43 points. Socratic-Generator achieves 42.4% on GenExam, establishing new state-of-the-art for open-source models, surpassing Seedream-4.0 (39.8%) and approaching Gemini-2.5-Flash-Image (43.1%)."
    },
    {
      "title": "QVLA: Not All Channels Are Equal in Vision-Language-Action Model's Quantization",
      "url": "https://arxiv.org/abs/2602.03782",
      "date": "2026-02-03",
      "authors_text": "Yuhao Xu, Yantai Yang, Zhenyang Fan, Yufan Liu, Yuming Li",
      "is_highlight": false,
      "score": 47.0,
      "summary": "QVLA introduces an action-centric quantization framework for Vision-Language-Action models, optimizing channel-wise bit allocation to enhance performance and reduce resource demands in robotics.",
      "teaser_image": "https://arxiv.org/html/2602.03782/x2.png",
      "debug_abstract": "The advent of Vision-Language-Action (VLA) models represents a significant leap for embodied intelligence, yet their immense computational demands critically hinder deployment on resource-constrained robotic platforms. Intuitively, low-bit quantization is a prevalent and preferred technique for large-scale model compression. However, we find that a systematic analysis of VLA model&#39;s quantization is fundamentally lacking. We argue that naively applying uniform-bit quantization from Large Language Models (LLMs) to robotics is flawed, as these methods prioritize passive data fidelity while ignoring how minor action deviations compound into catastrophic task failures. To bridge this gap, we introduce QVLA, the first action-centric quantization framework specifically designed for embodied control. In a sharp departure from the rigid, uniform-bit quantization of LLM-based methods, QVLA introduces a highly granular, channel-wise bit allocation strategy. Its core mechanism is to directly measure the final action-space sensitivity when quantizing each individual channel to various bit-widths. This process yields a precise, per-channel importance metric that guides a global optimization, which elegantly unifies quantization and pruning (0-bit) into a single, cohesive framework. Extensive evaluations on different baselines demonstrate the superiority of our approach. In the LIBERO, the quantization version of OpenVLA-OFT with our method requires only 29.2% of the original model&#39;s VRAM while maintaining 98.9% of its original performance and achieving a 1.49x speedup. This translates to a 22.6% performance improvement over the LLM-derived method SmoothQuant. Our work establishes a new, principled foundation for compressing VLA models in robotics, paving the way for deploying powerful, large-scale models on real-world hardware. Code will be released."
    },
    {
      "title": "ConLA: Contrastive Latent Action Learning from Human Videos for Robotic Manipulation",
      "url": "https://arxiv.org/abs/2602.00557",
      "date": "2026-01-31",
      "authors_text": "Weisheng Dai, Kai Lan, Jianyi Zhou, Bo Zhao, Xiu Su",
      "is_highlight": true,
      "score": 91.0,
      "summary": "ConLA introduces a contrastive disentanglement framework for unsupervised robotic policy learning from human videos, enhancing transferability and performance beyond traditional robot trajectory pretraining.",
      "teaser_image": "https://arxiv.org/html/2602.00557/x3.png",
      "debug_abstract": "Vision-Language-Action (VLA) models achieve preliminary generalization through pretraining on large scale robot teleoperation datasets. However, acquiring datasets that comprehensively cover diverse tasks and environments is extremely costly and difficult to scale. In contrast, human demonstration videos offer a rich and scalable source of diverse scenes and manipulation behaviors, yet their lack of explicit action supervision hinders direct utilization. Prior work leverages VQ-VAE based frameworks to learn latent actions from human videos in an unsupervised manner. Nevertheless, since the training objective primarily focuses on reconstructing visual appearances rather than capturing inter-frame dynamics, the learned representations tend to rely on spurious visual cues, leading to shortcut learning and entangled latent representations that hinder transferability. To address this, we propose ConLA, an unsupervised pretraining framework for learning robotic policies from human videos. ConLA introduces a contrastive disentanglement mechanism that leverages action category priors and temporal cues to isolate motion dynamics from visual content, effectively mitigating shortcut learning. Extensive experiments show that ConLA achieves strong performance across diverse benchmarks. Notably, by pretraining solely on human videos, our method for the first time surpasses the performance obtained with real robot trajectory pretraining, highlighting its ability to extract pure and semantically consistent latent action representations for scalable robot learning."
    },
    {
      "title": "ProxyImg: Towards Highly-Controllable Image Representation via Hierarchical Disentangled Proxy Embedding",
      "url": "https://arxiv.org/abs/2602.01881",
      "date": "2026-02-02",
      "authors_text": "Ye Chen, Yupeng Zhu, Xiongzhen Zhang, Zhewen Wan, Yingzhe Li",
      "is_highlight": false,
      "score": 30.0,
      "summary": "The paper presents ProxyImg, a hierarchical proxy-based image representation that enables fine-grained, controllable editing by disentangling semantic, geometric, and textural attributes for high-fidelity reconstruction.",
      "teaser_image": "https://arxiv.org/html/2602.01881/x2.png",
      "debug_abstract": "Prevailing image representation methods, including explicit representations such as raster images and Gaussian primitives, as well as implicit representations such as latent images, either suffer from representation redundancy that leads to heavy manual editing effort, or lack a direct mapping from latent variables to semantic instances or parts, making fine-grained manipulation difficult. These limitations hinder efficient and controllable image and video editing. To address these issues, we propose a hierarchical proxy-based parametric image representation that disentangles semantic, geometric, and textural attributes into independent and manipulable parameter spaces. Based on a semantic-aware decomposition of the input image, our representation constructs hierarchical proxy geometries through adaptive Bezier fitting and iterative internal region subdivision and meshing. Multi-scale implicit texture parameters are embedded into the resulting geometry-aware distributed proxy nodes, enabling continuous high-fidelity reconstruction in the pixel domain and instance- or part-independent semantic editing. In addition, we introduce a locality-adaptive feature indexing mechanism to ensure spatial texture coherence, which further supports high-quality background completion without relying on generative models. Extensive experiments on image reconstruction and editing benchmarks, including ImageNet, OIR-Bench, and HumanEdit, demonstrate that our method achieves state-of-the-art rendering fidelity with significantly fewer parameters, while enabling intuitive, interactive, and physically plausible manipulation. Moreover, by integrating proxy nodes with Position-Based Dynamics, our framework supports real-time physics-driven animation using lightweight implicit rendering, achieving superior temporal consistency and visual realism compared with generative approaches."
    },
    {
      "title": "SurfSplat: Conquering Feedforward 2D Gaussian Splatting with Surface Continuity Priors",
      "url": "https://arxiv.org/abs/2602.02000",
      "date": "2026-02-02",
      "authors_text": "Bing He, Jingnan Gao, Yunuo Chen, Ning Cao, Gang Chen",
      "is_highlight": false,
      "score": 25.0,
      "summary": "SurfSplat introduces a feedforward 2D Gaussian Splatting framework that enhances 3D scene reconstruction by ensuring surface continuity and improving geometric precision over existing methods.",
      "teaser_image": "https://arxiv.org/html/2602.02000/x2.png",
      "debug_abstract": "Reconstructing 3D scenes from sparse images remains a challenging task due to the difficulty of recovering accurate geometry and texture without optimization. Recent approaches leverage generalizable models to generate 3D scenes using 3D Gaussian Splatting (3DGS) primitive. However, they often fail to produce continuous surfaces and instead yield discrete, color-biased point clouds that appear plausible at normal resolution but reveal severe artifacts under close-up views. To address this issue, we present SurfSplat, a feedforward framework based on 2D Gaussian Splatting (2DGS) primitive, which provides stronger anisotropy and higher geometric precision. By incorporating a surface continuity prior and a forced alpha blending strategy, SurfSplat reconstructs coherent geometry together with faithful textures. Furthermore, we introduce High-Resolution Rendering Consistency (HRRC), a new evaluation metric designed to evaluate high-resolution reconstruction quality. Extensive experiments on RealEstate10K, DL3DV, and ScanNet demonstrate that SurfSplat consistently outperforms prior methods on both standard metrics and HRRC, establishing a robust solution for high-fidelity 3D reconstruction from sparse inputs. Project page: this https URL"
    },
    {
      "title": "PLANING: A Loosely Coupled Triangle-Gaussian Framework for Streaming 3D Reconstruction",
      "url": "https://arxiv.org/abs/2601.22046",
      "date": "2026-01-29",
      "authors_text": "Changjian Jiang, Kerui Ren, Xudong Li, Kaiwen Song, Linning Xu",
      "is_highlight": false,
      "score": 66.0,
      "summary": "PLANING is a novel streaming 3D reconstruction framework that efficiently combines geometric primitives and neural Gaussians, enhancing quality and speed while reducing redundancy.",
      "teaser_image": "https://arxiv.org/html/2601.22046/x3.png",
      "debug_abstract": "Streaming reconstruction from monocular image sequences remains challenging, as existing methods typically favor either high-quality rendering or accurate geometry, but rarely both. We present PLANING, an efficient on-the-fly reconstruction framework built on a hybrid representation that loosely couples explicit geometric primitives with neural Gaussians, enabling geometry and appearance to be modeled in a decoupled manner. This decoupling supports an online initialization and optimization strategy that separates geometry and appearance updates, yielding stable streaming reconstruction with substantially reduced structural redundancy. PLANING improves dense mesh Chamfer-L2 by 18.52% over PGSR, surpasses ARTDECO by 1.31 dB PSNR, and reconstructs ScanNetV2 scenes in under 100 seconds, over 5x faster than 2D Gaussian Splatting, while matching the quality of offline per-scene optimization. Beyond reconstruction quality, the structural clarity and computational efficiency of \\modelname~make it well suited for a broad range of downstream applications, such as enabling large-scale scene modeling and simulation-ready environments for embodied AI. Project page: this https URL ."
    },
    {
      "title": "DynaWeb: Model-Based Reinforcement Learning of Web Agents",
      "url": "https://arxiv.org/abs/2601.22149",
      "date": "2026-01-29",
      "authors_text": "Hang Ding, Peidong Liu, Junqiao Wang, Ziwei Ji, Meng Cao",
      "is_highlight": false,
      "score": 77.0,
      "summary": "DynaWeb introduces a model-based reinforcement learning framework that enhances web agent training by simulating interactions with a web world model, improving efficiency and performance.",
      "teaser_image": "https://arxiv.org/html/2601.22149/figures/dynaweb.png",
      "debug_abstract": "The development of autonomous web agents, powered by Large Language Models (LLMs) and reinforcement learning (RL), represents a significant step towards general-purpose AI assistants. However, training these agents is severely hampered by the challenges of interacting with the live internet, which is inefficient, costly, and fraught with risks. Model-based reinforcement learning (MBRL) offers a promising solution by learning a world model of the environment to enable simulated interaction. This paper introduces DynaWeb, a novel MBRL framework that trains web agents through interacting with a web world model trained to predict naturalistic web page representations given agent actions. This model serves as a synthetic web environment where an agent policy can dream by generating vast quantities of rollout action trajectories for efficient online reinforcement learning. Beyond free policy rollouts, DynaWeb incorporates real expert trajectories from training data, which are randomly interleaved with on-policy rollouts during training to improve stability and sample efficiency. Experiments conducted on the challenging WebArena and WebVoyager benchmarks demonstrate that DynaWeb consistently and significantly improves the performance of state-of-the-art open-source web agent models. Our findings establish the viability of training web agents through imagination, offering a scalable and efficient way to scale up online agentic RL."
    },
    {
      "title": "E-mem: Multi-agent based Episodic Context Reconstruction for LLM Agent Memory",
      "url": "https://arxiv.org/abs/2601.21714",
      "date": "2026-01-29",
      "authors_text": "Kaixiang Wang, Yidan Lin, Jiong Lou, Zhaojiacheng Zhou, Bunyod Suvonov",
      "is_highlight": false,
      "score": 71.0,
      "summary": "E-mem introduces a multi-agent framework for episodic context reconstruction in LLMs, enhancing reasoning accuracy and efficiency while preserving contextual integrity.",
      "teaser_image": "https://arxiv.org/html/2601.21714/x2.png",
      "debug_abstract": "The evolution of Large Language Model (LLM) agents towards System~2 reasoning, characterized by deliberative, high-precision problem-solving, requires maintaining rigorous logical integrity over extended horizons. However, prevalent memory preprocessing paradigms suffer from destructive de-contextualization. By compressing complex sequential dependencies into pre-defined structures (e.g., embeddings or graphs), these methods sever the contextual integrity essential for deep reasoning. To address this, we propose E-mem, a framework shifting from Memory Preprocessing to Episodic Context Reconstruction. Inspired by biological engrams, E-mem employs a heterogeneous hierarchical architecture where multiple assistant agents maintain uncompressed memory contexts, while a central master agent orchestrates global planning. Unlike passive retrieval, our mechanism empowers assistants to locally reason within activated segments, extracting context-aware evidence before aggregation. Evaluations on the LoCoMo benchmark demonstrate that E-mem achieves over 54\\% F1, surpassing the state-of-the-art GAM by 7.75\\%, while reducing token cost by over 70\\%."
    },
    {
      "title": "TouchGuide: Inference-Time Steering of Visuomotor Policies via Touch Guidance",
      "url": "https://arxiv.org/abs/2601.20239",
      "date": "2026-01-28",
      "authors_text": "Zhemeng Zhang, Jiahua Ma, Xincheng Yang, Xin Wen, Yuzhi Zhang",
      "is_highlight": false,
      "score": 90.0,
      "summary": "TouchGuide enhances robot manipulation by integrating tactile feedback with visual inputs to refine actions during inference, outperforming existing visuo-tactile policies in complex tasks.",
      "teaser_image": "https://arxiv.org/html/2601.20239/x2.png",
      "debug_abstract": "Fine-grained and contact-rich manipulation remain challenging for robots, largely due to the underutilization of tactile feedback. To address this, we introduce TouchGuide, a novel cross-policy visuo-tactile fusion paradigm that fuses modalities within a low-dimensional action space. Specifically, TouchGuide operates in two stages to guide a pre-trained diffusion or flow-matching visuomotor policy at inference time. First, the policy produces a coarse, visually-plausible action using only visual inputs during early sampling. Second, a task-specific Contact Physical Model (CPM) provides tactile guidance to steer and refine the action, ensuring it aligns with realistic physical contact conditions. Trained through contrastive learning on limited expert demonstrations, the CPM provides a tactile-informed feasibility score to steer the sampling process toward refined actions that satisfy physical contact constraints. Furthermore, to facilitate TouchGuide training with high-quality and cost-effective data, we introduce TacUMI, a data collection system. TacUMI achieves a favorable trade-off between precision and affordability; by leveraging rigid fingertips, it obtains direct tactile feedback, thereby enabling the collection of reliable tactile data. Extensive experiments on five challenging contact-rich tasks, such as shoe lacing and chip handover, show that TouchGuide consistently and significantly outperforms state-of-the-art visuo-tactile policies."
    },
    {
      "title": "Innovator-VL: A Multimodal Large Language Model for Scientific Discovery",
      "url": "https://arxiv.org/abs/2601.19325",
      "date": "2026-01-27",
      "authors_text": "Zichen Wen, Boxue Yang, Shuang Chen, Yaojie Zhang, Yuhang Han",
      "is_highlight": false,
      "score": 78.0,
      "summary": "Innovator-VL is a multimodal large language model that enhances scientific discovery through efficient training, transparency, and strong generalization without relying on extensive pretraining.",
      "teaser_image": "https://arxiv.org/html/2601.19325/x8.png",
      "debug_abstract": "We present Innovator-VL, a scientific multimodal large language model designed to advance understanding and reasoning across diverse scientific domains while maintaining excellent performance on general vision tasks. Contrary to the trend of relying on massive domain-specific pretraining and opaque pipelines, our work demonstrates that principled training design and transparent methodology can yield strong scientific intelligence with substantially reduced data requirements. (i) First, we provide a fully transparent, end-to-end reproducible training pipeline, covering data collection, cleaning, preprocessing, supervised fine-tuning, reinforcement learning, and evaluation, along with detailed optimization recipes. This facilitates systematic extension by the community. (ii) Second, Innovator-VL exhibits remarkable data efficiency, achieving competitive performance on various scientific tasks using fewer than five million curated samples without large-scale pretraining. These results highlight that effective reasoning can be achieved through principled data selection rather than indiscriminate scaling. (iii) Third, Innovator-VL demonstrates strong generalization, achieving competitive performance on general vision, multimodal reasoning, and scientific benchmarks. This indicates that scientific alignment can be integrated into a unified model without compromising general-purpose capabilities. Our practices suggest that efficient, reproducible, and high-performing scientific multimodal models can be built even without large-scale data, providing a practical foundation for future research."
    },
    {
      "title": "PILOT: A Perceptive Integrated Low-level Controller for Loco-manipulation over Unstructured Scenes",
      "url": "https://arxiv.org/abs/2601.17440",
      "date": "2026-01-24",
      "authors_text": "Xinru Cui, Linxi Feng, Yixuan Zhou, Haoqi Han, Zhe Liu",
      "is_highlight": true,
      "score": 90.0,
      "summary": "PILOT is a unified reinforcement learning framework that integrates perceptive locomotion and whole-body control for stable loco-manipulation in unstructured environments, outperforming existing controllers.",
      "debug_abstract": "Humanoid robots hold great potential for diverse interactions and daily service tasks within human-centered environments, necessitating controllers that seamlessly integrate precise locomotion with dexterous manipulation. However, most existing whole-body controllers lack exteroceptive awareness of the surrounding environment, rendering them insufficient for stable task execution in complex, unstructured this http URL address this challenge, we propose PILOT, a unified single-stage reinforcement learning (RL) framework tailored for perceptive loco-manipulation, which synergizes perceptive locomotion and expansive whole-body control within a single policy. To enhance terrain awareness and ensure precise foot placement, we design a cross-modal context encoder that fuses prediction-based proprioceptive features with attention-based perceptive representations. Furthermore, we introduce a Mixture-of-Experts (MoE) policy architecture to coordinate diverse motor skills, facilitating better specialization across distinct motion patterns. Extensive experiments in both simulation and on the physical Unitree G1 humanoid robot validate the efficacy of our framework. PILOT demonstrates superior stability, command tracking precision, and terrain traversability compared to existing baselines. These results highlight its potential to serve as a robust, foundational low-level controller for loco-manipulation in unstructured scenes."
    },
    {
      "title": "BMDS-Net: A Bayesian Multi-Modal Deep Supervision Network for Robust Brain Tumor Segmentation",
      "url": "https://arxiv.org/abs/2601.17504",
      "date": "2026-01-24",
      "authors_text": "Yan Zhou, Zhen Huang, Yingqiu Li, Yue Ouyang, Suncheng Xiang",
      "is_highlight": false,
      "score": 50.0,
      "summary": "BMDS-Net enhances brain tumor segmentation by integrating robust feature learning and Bayesian uncertainty estimation, ensuring reliable performance despite missing MRI modalities.",
      "debug_abstract": "Accurate brain tumor segmentation from multi-modal magnetic resonance imaging (MRI) is a prerequisite for precise radiotherapy planning and surgical navigation. While recent Transformer-based models such as Swin UNETR have achieved impressive benchmark performance, their clinical utility is often compromised by two critical issues: sensitivity to missing modalities (common in clinical practice) and a lack of confidence calibration. Merely chasing higher Dice scores on idealized data fails to meet the safety requirements of real-world medical deployment. In this work, we propose BMDS-Net, a unified framework that prioritizes clinical robustness and trustworthiness over simple metric maximization. Our contribution is three-fold. First, we construct a robust deterministic backbone by integrating a Zero-Init Multimodal Contextual Fusion (MMCF) module and a Residual-Gated Deep Decoder Supervision (DDS) mechanism, enabling stable feature learning and precise boundary delineation with significantly reduced Hausdorff Distance, even under modality corruption. Second, and most importantly, we introduce a memory-efficient Bayesian fine-tuning strategy that transforms the network into a probabilistic predictor, providing voxel-wise uncertainty maps to highlight potential errors for clinicians. Third, comprehensive experiments on the BraTS 2021 dataset demonstrate that BMDS-Net not only maintains competitive accuracy but, more importantly, exhibits superior stability in missing-modality scenarios where baseline models fail. The source code is publicly available at this https URL."
    },
    {
      "title": "QualiRAG: Retrieval-Augmented Generation for Visual Quality Understanding",
      "url": "https://arxiv.org/abs/2601.18195",
      "date": "2026-01-26",
      "authors_text": "Linhan Cao, Wei Sun, Weixia Zhang, Xiangyang Zhu, Kaiwei Zhang",
      "is_highlight": false,
      "score": 66.0,
      "summary": "QualiRAG introduces a training-free framework for visual quality assessment that utilizes dynamic retrieval and multimodal knowledge to enhance interpretability and performance without task-specific training.",
      "debug_abstract": "Visual quality assessment (VQA) is increasingly shifting from scalar score prediction toward interpretable quality understanding -- a paradigm that demands \\textit{fine-grained spatiotemporal perception} and \\textit{auxiliary contextual information}. Current approaches rely on supervised fine-tuning or reinforcement learning on curated instruction datasets, which involve labor-intensive annotation and are prone to dataset-specific biases. To address these challenges, we propose \\textbf{QualiRAG}, a \\textit{training-free} \\textbf{R}etrieval-\\textbf{A}ugmented \\textbf{G}eneration \\textbf{(RAG)} framework that systematically leverages the latent perceptual knowledge of large multimodal models (LMMs) for visual quality perception. Unlike conventional RAG that retrieves from static corpora, QualiRAG dynamically generates auxiliary knowledge by decomposing questions into structured requests and constructing four complementary knowledge sources: \\textit{visual metadata}, \\textit{subject localization}, \\textit{global quality summaries}, and \\textit{local quality descriptions}, followed by relevance-aware retrieval for evidence-grounded reasoning. Extensive experiments show that QualiRAG achieves substantial improvements over open-source general-purpose LMMs and VQA-finetuned LMMs on visual quality understanding tasks, and delivers competitive performance on visual quality comparison tasks, demonstrating robust quality assessment capabilities without any task-specific training. The code will be publicly available at this https URL."
    },
    {
      "title": "ExoGS: A 4D Real-to-Sim-to-Real Framework for Scalable Manipulation Data Collection",
      "url": "https://arxiv.org/abs/2601.18629",
      "date": "2026-01-26",
      "authors_text": "Yiming Wang, Ruogu Zhang, Minyang Li, Hao Shi, Junbo Wang",
      "is_highlight": false,
      "score": 93.0,
      "summary": "ExoGS introduces a 4D framework for efficient manipulation data collection by seamlessly transferring real-world interactions to simulation, enhancing policy learning and generalization.",
      "debug_abstract": "Real-to-Sim-to-Real technique is gaining increasing interest for robotic manipulation, as it can generate scalable data in simulation while having narrower sim-to-real gap. However, previous methods mainly focused on environment-level visual real-to-sim transfer, ignoring the transfer of interactions, which could be challenging and inefficient to obtain purely in simulation especially for contact-rich tasks. We propose ExoGS, a robot-free 4D Real-to-Sim-to-Real framework that captures both static environments and dynamic interactions in the real world and transfers them seamlessly to a simulated environment. It provides a new solution for scalable manipulation data collection and policy learning. ExoGS employs a self-designed robot-isomorphic passive exoskeleton AirExo-3 to capture kinematically consistent trajectories with millimeter-level accuracy and synchronized RGB observations during direct human demonstrations. The robot, objects, and environment are reconstructed as editable 3D Gaussian Splatting assets, enabling geometry-consistent replay and large-scale data augmentation. Additionally, a lightweight Mask Adapter injects instance-level semantics into the policy to enhance robustness under visual domain shifts. Real-world experiments demonstrate that ExoGS significantly improves data efficiency and policy generalization compared to teleoperation-based baselines. Code and hardware files have been released on this https URL."
    },
    {
      "title": "Advances and Innovations in the Multi-Agent Robotic System (MARS) Challenge",
      "url": "https://arxiv.org/abs/2601.18733",
      "date": "2026-01-26",
      "authors_text": "Li Kang, Heng Zhou, Xiufeng Song, Rui Li, Bruno N. Y. Chen",
      "is_highlight": false,
      "score": 81.0,
      "summary": "The MARS Challenge at NeurIPS 2025 aims to enhance multi-agent collaboration in embodied AI through competition in planning and control using vision-language models.",
      "debug_abstract": "Recent advancements in multimodal large language models and vision-languageaction models have significantly driven progress in Embodied AI. As the field transitions toward more complex task scenarios, multi-agent system frameworks are becoming essential for achieving scalable, efficient, and collaborative solutions. This shift is fueled by three primary factors: increasing agent capabilities, enhancing system efficiency through task delegation, and enabling advanced human-agent interactions. To address the challenges posed by multi-agent collaboration, we propose the Multi-Agent Robotic System (MARS) Challenge, held at the NeurIPS 2025 Workshop on SpaVLE. The competition focuses on two critical areas: planning and control, where participants explore multi-agent embodied planning using vision-language models (VLMs) to coordinate tasks and policy execution to perform robotic manipulation in dynamic environments. By evaluating solutions submitted by participants, the challenge provides valuable insights into the design and coordination of embodied multi-agent systems, contributing to the future development of advanced collaborative AI systems."
    }
  ],
  "具身智能实验室": [
    {
      "title": "MeCo: Enhancing LLM-Empowered Multi-Robot Collaboration via Similar Task Memoization",
      "url": "https://arxiv.org/abs/2601.20577",
      "date": "2026-01-28",
      "authors_text": "Baiqing Wang, Helei Cui, Bo Zhang, Xiaolong Zheng, Bin Guo",
      "is_highlight": false,
      "score": 85.0,
      "summary": "MeCo enhances multi-robot collaboration by utilizing memoization to efficiently reuse solutions for similar tasks, reducing planning costs and improving success rates.",
      "teaser_image": "https://arxiv.org/html/2601.20577/x2.png",
      "debug_abstract": "Multi-robot systems have been widely deployed in real-world applications, providing significant improvements in efficiency and reductions in labor costs. However, most existing multi-robot collaboration methods rely on extensive task-specific training, which limits their adaptability to new or diverse scenarios. Recent research leverages the language understanding and reasoning capabilities of large language models (LLMs) to enable more flexible collaboration without specialized training. Yet, current LLM-empowered approaches remain inefficient: when confronted with identical or similar tasks, they must replan from scratch because they omit task-level similarities. To address this limitation, we propose MeCo, a similarity-aware multi-robot collaboration framework that applies the principle of ``cache and reuse&#39;&#39; (a.k.a., memoization) to reduce redundant computation. Unlike simple task repetition, identifying and reusing solutions for similar but not identical tasks is far more challenging, particularly in multi-robot settings. To this end, MeCo introduces a new similarity testing method that retrieves previously solved tasks with high relevance, enabling effective plan reuse without re-invoking LLMs. Furthermore, we present MeCoBench, the first benchmark designed to evaluate performance on similar-task collaboration scenarios. Experimental results show that MeCo substantially reduces planning costs and improves success rates compared with state-of-the-art approaches."
    },
    {
      "title": "Text-Pass Filter: An Efficient Scene Text Detector",
      "url": "https://arxiv.org/abs/2601.18098",
      "date": "2026-01-26",
      "authors_text": "Chuang Yang, Haozhao Ma, Xu Han, Yuan Yuan, Qi Wang",
      "is_highlight": false,
      "score": 48.0,
      "summary": "The Text-Pass Filter (TPF) enhances arbitrary-shaped text detection by directly segmenting text, improving recognition through unique feature-filter pairs and reinforcement mechanisms.",
      "debug_abstract": "To pursue an efficient text assembling process, existing methods detect texts via the shrink-mask expansion strategy. However, the shrinking operation loses the visual features of text margins and confuses the foreground and background difference, which brings intrinsic limitations to recognize text features. We follow this issue and design Text-Pass Filter (TPF) for arbitrary-shaped text detection. It segments the whole text directly, which avoids the intrinsic limitations. It is noteworthy that different from previous whole text region-based methods, TPF can separate adhesive texts naturally without complex decoding or post-processing processes, which makes it possible for real-time text detection. Concretely, we find that the band-pass filter allows through components in a specified band of frequencies, called its passband but blocks components with frequencies above or below this band. It provides a natural idea for extracting whole texts separately. By simulating the band-pass filter, TPF constructs a unique feature-filter pair for each text. In the inference stage, every filter extracts the corresponding matched text by passing its pass-feature and blocking other features. Meanwhile, considering the large aspect ratio problem of ribbon-like texts makes it hard to recognize texts wholly, a Reinforcement Ensemble Unit (REU) is designed to enhance the feature consistency of the same text and to enlarge the filter&#39;s recognition field to help recognize whole texts. Furthermore, a Foreground Prior Unit (FPU) is introduced to encourage TPF to discriminate the difference between the foreground and background, which improves the feature-filter pair quality. Experiments demonstrate the effectiveness of REU and FPU while showing the TPF&#39;s superiority."
    }
  ],
  "通用机器人、自动化、传感和感知实验室 (GRASP)": [
    {
      "title": "See-through: Single-image Layer Decomposition for Anime Characters",
      "url": "https://arxiv.org/abs/2602.03749",
      "date": "2026-02-03",
      "authors_text": "Jian Lin, Chengze Li, Haoyun Qin, Kwun Wang Chan, Yanghua Jin",
      "is_highlight": false,
      "score": 48.0,
      "summary": "The paper presents a framework that transforms static anime illustrations into dynamic 2.5D models by automating layer decomposition and utilizing high-quality supervision from Live2D models.",
      "teaser_image": "https://arxiv.org/html/2602.03749/figures/Figure_Method.png",
      "debug_abstract": "We introduce a framework that automates the transformation of static anime illustrations into manipulatable 2.5D models. Current professional workflows require tedious manual segmentation and the artistic ``hallucination&#39;&#39; of occluded regions to enable motion. Our approach overcomes this by decomposing a single image into fully inpainted, semantically distinct layers with inferred drawing orders. To address the scarcity of training data, we introduce a scalable engine that bootstraps high-quality supervision from commercial Live2D models, capturing pixel-perfect semantics and hidden geometry. Our methodology couples a diffusion-based Body Part Consistency Module, which enforces global geometric coherence, with a pixel-level pseudo-depth inference mechanism. This combination resolves the intricate stratification of anime characters, e.g., interleaving hair strands, allowing for dynamic layer reconstruction. We demonstrate that our approach yields high-fidelity, manipulatable models suitable for professional, real-time animation applications."
    },
    {
      "title": "Llama-3.1-FoundationAI-SecurityLLM-Reasoning-8B Technical Report",
      "url": "https://arxiv.org/abs/2601.21051",
      "date": "2026-01-28",
      "authors_text": "Zhuoran Yang, Ed Li, Jianliang He, Aman Priyanshu, Baturay Saglam",
      "is_highlight": false,
      "score": 70.0,
      "summary": "Foundation-Sec-8B-Reasoning is an open-source cybersecurity reasoning model that excels in specialized tasks while retaining strong general capabilities through innovative training methods.",
      "teaser_image": "https://arxiv.org/html/2601.21051/x2.png",
      "debug_abstract": "We present Foundation-Sec-8B-Reasoning, the first open-source native reasoning model for cybersecurity. Built upon our previously released Foundation-Sec-8B base model (derived from Llama-3.1-8B-Base), the model is trained through a two-stage process combining supervised fine-tuning (SFT) and reinforcement learning from verifiable rewards (RLVR). Our training leverages proprietary reasoning data spanning cybersecurity analysis, instruction-following, and mathematical reasoning. Evaluation across 10 cybersecurity benchmarks and 10 general-purpose benchmarks demonstrates performance competitive with significantly larger models on cybersecurity tasks while maintaining strong general capabilities. The model shows effective generalization on multi-hop reasoning tasks and strong safety performance when deployed with appropriate system prompts and guardrails. This work demonstrates that domain-specialized reasoning models can achieve strong performance on specialized tasks while maintaining broad general capabilities. We release the model publicly at this https URL."
    },
    {
      "title": "FreeOrbit4D: Training-Free Arbitrary Camera Redirection for Monocular Videos via Geometry-Complete 4D Reconstruction",
      "url": "https://arxiv.org/abs/2601.18993",
      "date": "2026-01-26",
      "authors_text": "Wei Cao, Hao Zhang, Fengrui Tian, Yulun Wu, Yingying Li",
      "is_highlight": false,
      "score": 70.0,
      "summary": "FreeOrbit4D introduces a training-free method for effective camera redirection in monocular videos by reconstructing a geometry-complete 4D proxy to resolve geometric ambiguities.",
      "teaser_image": "https://arxiv.org/html/2601.18993/x3.png",
      "debug_abstract": "Camera redirection aims to replay a dynamic scene from a single monocular video under a user-specified camera trajectory. However, large-angle redirection is inherently ill-posed: a monocular video captures only a narrow spatio-temporal view of a dynamic 3D scene, providing highly partial observations of the underlying 4D world. The key challenge is therefore to recover a complete and coherent representation from this limited input, with consistent geometry and motion. While recent diffusion-based methods achieve impressive results, they often break down under large-angle viewpoint changes far from the original trajectory, where missing visual grounding leads to severe geometric ambiguity and temporal inconsistency. To address this, we present FreeOrbit4D, an effective training-free framework that tackles this geometric ambiguity by recovering a geometry-complete 4D proxy as structural grounding for video generation. We obtain this proxy by decoupling foreground and background reconstructions: we unproject the monocular video into a static background and geometry-incomplete foreground point clouds in a unified global space, then leverage an object-centric multi-view diffusion model to synthesize multi-view images and reconstruct geometry-complete foreground point clouds in canonical object space. By aligning the canonical foreground point cloud to the global scene space via dense pixel-synchronized 3D--3D correspondences and projecting the geometry-complete 4D proxy onto target camera viewpoints, we provide geometric scaffolds that guide a conditional video diffusion model. Extensive experiments show that FreeOrbit4D produces more faithful redirected videos under challenging large-angle trajectories, and our geometry-complete 4D proxy further opens a potential avenue for practical applications such as edit propagation and 4D data generation. Project page and code will be released soon."
    },
    {
      "title": "Paying Less Generalization Tax: A Cross-Domain Generalization Study of RL Training for LLM Agents",
      "url": "https://arxiv.org/abs/2601.18217",
      "date": "2026-01-26",
      "authors_text": "Zhihan Liu, Lin Guan, Yixin Nie, Kai Zhang, Zhuoqun Hao",
      "is_highlight": false,
      "score": 64.0,
      "summary": "This study identifies key factors influencing cross-domain generalization in RL-trained LLM agents, emphasizing state information richness and planning complexity over realism and similarity.",
      "debug_abstract": "Generalist LLM agents are often post-trained on a narrow set of environments but deployed across far broader, unseen domains. In this work, we investigate the challenge of agentic post-training when the eventual test domains are unknown. Specifically, we analyze which properties of reinforcement learning (RL) environments and modeling choices have the greatest influence on out-of-domain performance. First, we identify two environment axes that strongly correlate with cross-domain generalization: (i) state information richness, i.e., the amount of information for the agent to process from the state, and (ii) planning complexity, estimated via goal reachability and trajectory length under a base policy. Notably, domain realism and text-level similarity are not the primary factors; for instance, the simple grid-world domain Sokoban leads to even stronger generalization in SciWorld than the more realistic ALFWorld. Motivated by these findings, we further show that increasing state information richness alone can already effectively improve cross-domain robustness. We propose a randomization technique, which is low-overhead and broadly applicable: add small amounts of distractive goal-irrelevant features to the state to make it richer without altering the task. Beyond environment-side properties, we also examine several modeling choices: (a) SFT warmup or mid-training helps prevent catastrophic forgetting during RL but undermines generalization to domains that are not included in the mid-training datamix; and (b) turning on step-by-step thinking during RL, while not always improving in-domain performance, plays a crucial role in preserving generalization."
    }
  ],
  "四川大学": [
    {
      "title": "Multi-function Robotized Surgical Dissector for Endoscopic Pulmonary Thromboendarterectomy: Preclinical Study and Evaluation",
      "url": "https://arxiv.org/abs/2602.03147",
      "date": "2026-02-03",
      "authors_text": "Runfeng Zhu, Xin Zhong, Qingxiang Zhao, Jing Lin, Zhong Wu",
      "is_highlight": true,
      "score": 88.0,
      "summary": "This study introduces a novel robotized surgical dissector for endoscopic pulmonary thromboendarterectomy, enhancing dexterity and precision in accessing pulmonary artery branches.",
      "teaser_image": "https://arxiv.org/html/2602.03147/x2.png",
      "debug_abstract": "Patients suffering chronic severe pulmonary thromboembolism need Pulmonary Thromboendarterectomy (PTE) to remove the thromb and intima located inside pulmonary artery (PA). During the surgery, a surgeon holds tweezers and a dissector to delicately strip the blockage, but available tools for this surgery are rigid and straight, lacking distal dexterity to access into thin branches of PA. Therefore, this work presents a novel robotized dissector based on concentric push/pull robot (CPPR) structure, enabling entering deep thin branch of tortuous PA. Compared with conventional rigid dissectors, our design characterizes slenderness and dual-segment-bending dexterity. Owing to the hollow and thin-walled structure of the CPPR-based dissector as it has a slender body of 3.5mm in diameter, the central lumen accommodates two channels for irrigation and tip tool, and space for endoscopic camera&#39;s signal wire. To provide accurate surgical manipulation, optimization-based kinematics model was established, realizing a 2mm accuracy in positioning the tip tool (60mm length) under open-loop control strategy. As such, with the endoscopic camera, traditional PTE is possible to be upgraded as endoscopic PTE. Basic physic performance of the robotized dissector including stiffness, motion accuracy and maneuverability was evaluated through experiments. Surgery simulation on ex vivo porcine lung also demonstrates its dexterity and notable advantages in PTE."
    },
    {
      "title": "DynaWeb: Model-Based Reinforcement Learning of Web Agents",
      "url": "https://arxiv.org/abs/2601.22149",
      "date": "2026-01-29",
      "authors_text": "Hang Ding, Peidong Liu, Junqiao Wang, Ziwei Ji, Meng Cao",
      "is_highlight": false,
      "score": 77.0,
      "summary": "DynaWeb introduces a model-based reinforcement learning framework that enhances web agent training by simulating interactions with a web world model, improving efficiency and performance.",
      "teaser_image": "https://arxiv.org/html/2601.22149/figures/dynaweb.png",
      "debug_abstract": "The development of autonomous web agents, powered by Large Language Models (LLMs) and reinforcement learning (RL), represents a significant step towards general-purpose AI assistants. However, training these agents is severely hampered by the challenges of interacting with the live internet, which is inefficient, costly, and fraught with risks. Model-based reinforcement learning (MBRL) offers a promising solution by learning a world model of the environment to enable simulated interaction. This paper introduces DynaWeb, a novel MBRL framework that trains web agents through interacting with a web world model trained to predict naturalistic web page representations given agent actions. This model serves as a synthetic web environment where an agent policy can dream by generating vast quantities of rollout action trajectories for efficient online reinforcement learning. Beyond free policy rollouts, DynaWeb incorporates real expert trajectories from training data, which are randomly interleaved with on-policy rollouts during training to improve stability and sample efficiency. Experiments conducted on the challenging WebArena and WebVoyager benchmarks demonstrate that DynaWeb consistently and significantly improves the performance of state-of-the-art open-source web agent models. Our findings establish the viability of training web agents through imagination, offering a scalable and efficient way to scale up online agentic RL."
    },
    {
      "title": "Co-PLNet: A Collaborative Point-Line Network for Prompt-Guided Wireframe Parsing",
      "url": "https://arxiv.org/abs/2601.18252",
      "date": "2026-01-26",
      "authors_text": "Chao Wang, Xuanying Li, Cheng Dai, Jinglei Feng, Yuxiang Luo",
      "is_highlight": false,
      "score": 54.0,
      "summary": "Co-PLNet introduces a collaborative framework for wireframe parsing that enhances line and junction detection through spatial cue exchange, improving accuracy and efficiency in structured geometry perception.",
      "debug_abstract": "Wireframe parsing aims to recover line segments and their junctions to form a structured geometric representation useful for downstream tasks such as Simultaneous Localization and Mapping (SLAM). Existing methods predict lines and junctions separately and reconcile them post-hoc, causing mismatches and reduced robustness. We present Co-PLNet, a point-line collaborative framework that exchanges spatial cues between the two tasks, where early detections are converted into spatial prompts via a Point-Line Prompt Encoder (PLP-Encoder), which encodes geometric attributes into compact and spatially aligned maps. A Cross-Guidance Line Decoder (CGL-Decoder) then refines predictions with sparse attention conditioned on complementary prompts, enforcing point-line consistency and efficiency. Experiments on Wireframe and YorkUrban show consistent improvements in accuracy and robustness, together with favorable real-time efficiency, demonstrating our effectiveness for structured geometry perception."
    }
  ],
  "电子科技大学": [
    {
      "title": "Multi-function Robotized Surgical Dissector for Endoscopic Pulmonary Thromboendarterectomy: Preclinical Study and Evaluation",
      "url": "https://arxiv.org/abs/2602.03147",
      "date": "2026-02-03",
      "authors_text": "Runfeng Zhu, Xin Zhong, Qingxiang Zhao, Jing Lin, Zhong Wu",
      "is_highlight": true,
      "score": 88.0,
      "summary": "This study introduces a novel robotized surgical dissector for endoscopic pulmonary thromboendarterectomy, enhancing dexterity and precision in accessing pulmonary artery branches.",
      "teaser_image": "https://arxiv.org/html/2602.03147/x2.png",
      "debug_abstract": "Patients suffering chronic severe pulmonary thromboembolism need Pulmonary Thromboendarterectomy (PTE) to remove the thromb and intima located inside pulmonary artery (PA). During the surgery, a surgeon holds tweezers and a dissector to delicately strip the blockage, but available tools for this surgery are rigid and straight, lacking distal dexterity to access into thin branches of PA. Therefore, this work presents a novel robotized dissector based on concentric push/pull robot (CPPR) structure, enabling entering deep thin branch of tortuous PA. Compared with conventional rigid dissectors, our design characterizes slenderness and dual-segment-bending dexterity. Owing to the hollow and thin-walled structure of the CPPR-based dissector as it has a slender body of 3.5mm in diameter, the central lumen accommodates two channels for irrigation and tip tool, and space for endoscopic camera&#39;s signal wire. To provide accurate surgical manipulation, optimization-based kinematics model was established, realizing a 2mm accuracy in positioning the tip tool (60mm length) under open-loop control strategy. As such, with the endoscopic camera, traditional PTE is possible to be upgraded as endoscopic PTE. Basic physic performance of the robotized dissector including stiffness, motion accuracy and maneuverability was evaluated through experiments. Surgery simulation on ex vivo porcine lung also demonstrates its dexterity and notable advantages in PTE."
    },
    {
      "title": "R^3: Replay, Reflection, and Ranking Rewards for LLM Reinforcement Learning",
      "url": "https://arxiv.org/abs/2601.19620",
      "date": "2026-01-27",
      "authors_text": "Zhizheng Jiang, Kang Zhao, Weikai Xu, Xinkui Lin, Wei Liu",
      "is_highlight": false,
      "score": 68.0,
      "summary": "The R^3 framework enhances reinforcement learning for large reasoning models by integrating cross-context replay, in-context self-reflection, and structural entropy ranking rewards, achieving state-of-the-art performance in math tasks.",
      "debug_abstract": "Large reasoning models (LRMs) aim to solve diverse and complex problems through structured reasoning. Recent advances in group-based policy optimization methods have shown promise in enabling stable advantage estimation without reliance on process-level annotations. However, these methods rely on advantage gaps induced by high-quality samples within the same batch, which makes the training process fragile and inefficient when intra-group advantages collapse under challenging tasks. To address these problems, we propose a reinforcement learning mechanism named \\emph{\\textbf{R^3}} that along three directions: (1) a \\emph{cross-context \\underline{\\textbf{R}}eplay} strategy that maintains the intra-group advantage by recalling valuable examples from historical trajectories of the same query, (2) an \\emph{in-context self-\\underline{\\textbf{R}}eflection} mechanism enabling models to refine outputs by leveraging past failures, and (3) a \\emph{structural entropy \\underline{\\textbf{R}}anking reward}, which assigns relative rewards to truncated or failed samples by ranking responses based on token-level entropy patterns, capturing both local exploration and global stability. We implement our method on Deepseek-R1-Distill-Qwen-1.5B and train it on the DeepscaleR-40k in the math domain. Experiments demonstrate our method achieves SoTA performance on several math benchmarks, representing significant improvements and fewer reasoning tokens over the base models. Code and model will be released."
    },
    {
      "title": "AC^2-VLA: Action-Context-Aware Adaptive Computation in Vision-Language-Action Models for Efficient Robotic Manipulation",
      "url": "https://arxiv.org/abs/2601.19634",
      "date": "2026-01-27",
      "authors_text": "Wenda Yu, Tianshi Wang, Fengling Li, Jingjing Li, Lei Zhu",
      "is_highlight": false,
      "score": 94.0,
      "summary": "AC^2-VLA enhances robotic manipulation efficiency by adaptively optimizing computation based on action context, achieving significant speedup and reduced resource usage without sacrificing performance.",
      "debug_abstract": "Vision-Language-Action (VLA) models have demonstrated strong performance in robotic manipulation, yet their closed-loop deployment is hindered by the high latency and compute cost of repeatedly running large vision-language backbones at every timestep. We observe that VLA inference exhibits structured redundancies across temporal, spatial, and depth dimensions, and that most existing efficiency methods ignore action context, despite its central role in embodied tasks. To address this gap, we propose Action-Context-aware Adaptive Computation for VLA models (AC^2-VLA), a unified framework that conditions computation on current visual observations, language instructions, and previous action states. Based on this action-centric context, AC^2-VLA adaptively performs cognition reuse across timesteps, token pruning, and selective execution of model components within a unified mechanism. To train the adaptive policy, we introduce an action-guided self-distillation scheme that preserves the behavior of the dense VLA policy while enabling structured sparsification that transfers across tasks and settings. Extensive experiments on robotic manipulation benchmarks show that AC^2-VLA achieves up to a 1.79\\times speedup while reducing FLOPs to 29.4% of the dense baseline, with comparable task success."
    },
    {
      "title": "AdaReasoner: Dynamic Tool Orchestration for Iterative Visual Reasoning",
      "url": "https://arxiv.org/abs/2601.18631",
      "date": "2026-01-26",
      "authors_text": "Mingyang Song, Haoyu Sun, Jiawei Gu, Linjie Li, Luxin Xu",
      "is_highlight": false,
      "score": 61.0,
      "summary": "AdaReasoner is a multimodal model that autonomously optimizes tool selection and usage for iterative visual reasoning, achieving state-of-the-art performance on various benchmarks.",
      "debug_abstract": "When humans face problems beyond their immediate capabilities, they rely on tools, providing a promising paradigm for improving visual reasoning in multimodal large language models (MLLMs). Effective reasoning, therefore, hinges on knowing which tools to use, when to invoke them, and how to compose them over multiple steps, even when faced with new tools or new tasks. We introduce \\textbf{AdaReasoner}, a family of multimodal models that learn tool use as a general reasoning skill rather than as tool-specific or explicitly supervised behavior. AdaReasoner is enabled by (i) a scalable data curation pipeline exposing models to long-horizon, multi-step tool interactions; (ii) Tool-GRPO, a reinforcement learning algorithm that optimizes tool selection and sequencing based on end-task success; and (iii) an adaptive learning mechanism that dynamically regulates tool usage. Together, these components allow models to infer tool utility from task context and intermediate outcomes, enabling coordination of multiple tools and generalization to unseen tools. Empirically, AdaReasoner exhibits strong tool-adaptive and generalization behaviors: it autonomously adopts beneficial tools, suppresses irrelevant ones, and adjusts tool usage frequency based on task demands, despite never being explicitly trained to do so. These capabilities translate into state-of-the-art performance across challenging benchmarks, improving the 7B base model by +24.9\\% on average and surpassing strong proprietary systems such as GPT-5 on multiple tasks, including VSP and Jigsaw."
    }
  ],
  "CSAIL Embodied Intelligence Labs": [
    {
      "title": "VGGT-SLAM 2.0: Real time Dense Feed-forward Scene Reconstruction",
      "url": "https://arxiv.org/abs/2601.19887",
      "date": "2026-01-27",
      "authors_text": "Dominic Maggio, Luca Carlone",
      "is_highlight": false,
      "score": 92.0,
      "summary": "VGGT-SLAM 2.0 enhances real-time RGB SLAM by improving submap alignment, reducing drift, and enabling effective image retrieval verification, achieving superior accuracy in diverse environments.",
      "debug_abstract": "We present VGGT-SLAM 2.0, a real time RGB feed-forward SLAM system which substantially improves upon VGGT-SLAM for incrementally aligning submaps created from VGGT. Firstly, we remove high-dimensional 15-degree-of-freedom drift and planar degeneracy from VGGT-SLAM by creating a new factor graph design while still addressing the reconstruction ambiguity of VGGT given unknown camera intrinsics. Secondly, by studying the attention layers of VGGT, we show that one of the layers is well suited to assist in image retrieval verification for free without additional training, which enables both rejecting false positive matches and allows for completing more loop closures. Finally, we conduct a suite of experiments which includes showing VGGT-SLAM 2.0 can easily be adapted for open-set object detection and demonstrating real time performance while running online onboard a ground robot using a Jetson Thor. We also test in environments ranging from cluttered indoor apartments and office scenes to a 4,200 square foot barn, and we also demonstrate VGGT-SLAM 2.0 achieves the highest accuracy on the TUM dataset with about 23 percent less pose error than VGGT-SLAM. Code will be released upon publication."
    },
    {
      "title": "Dep-Search: Learning Dependency-Aware Reasoning Traces with Persistent Memory",
      "url": "https://arxiv.org/abs/2601.18771",
      "date": "2026-01-26",
      "authors_text": "Yanming Liu, Xinyue Peng, Zixuan Yan, Yanxin Shen, Wenjie Xu",
      "is_highlight": false,
      "score": 46.0,
      "summary": "Dep-Search enhances large language models' multi-hop reasoning by integrating dependency-aware search, structured retrieval, and persistent memory, outperforming existing frameworks in complex tasks.",
      "debug_abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in complex reasoning tasks, particularly when augmented with search mechanisms that enable systematic exploration of external knowledge bases. The field has evolved from traditional retrieval-augmented generation (RAG) frameworks to more sophisticated search-based frameworks that orchestrate multi-step reasoning through explicit search strategies. However, existing search frameworks still rely heavily on implicit natural language reasoning to determine search strategies and how to leverage retrieved information across reasoning steps. This reliance on implicit reasoning creates fundamental challenges for managing dependencies between sub-questions, efficiently reusing previously retrieved knowledge, and learning optimal search strategies through reinforcement learning. To address these limitations, we propose Dep-Search, a dependency-aware search framework that advances beyond existing search frameworks by integrating structured reasoning, retrieval, and persistent memory through GRPO. Dep-Search introduces explicit control mechanisms that enable the model to decompose questions with dependency relationships, retrieve information when needed, access previously stored knowledge from memory, and summarize long reasoning contexts into reusable memory entries. Through extensive experiments on seven diverse question answering datasets, we demonstrate that Dep-Search significantly enhances LLMs&#39; ability to tackle complex multi-hop reasoning tasks, achieving substantial improvements over strong baselines across different model scales."
    }
  ],
  "Quest for Intelligence": [
    {
      "title": "VGGT-SLAM 2.0: Real time Dense Feed-forward Scene Reconstruction",
      "url": "https://arxiv.org/abs/2601.19887",
      "date": "2026-01-27",
      "authors_text": "Dominic Maggio, Luca Carlone",
      "is_highlight": false,
      "score": 92.0,
      "summary": "VGGT-SLAM 2.0 enhances real-time RGB SLAM by improving submap alignment, reducing drift, and enabling effective image retrieval verification, achieving superior accuracy in diverse environments.",
      "debug_abstract": "We present VGGT-SLAM 2.0, a real time RGB feed-forward SLAM system which substantially improves upon VGGT-SLAM for incrementally aligning submaps created from VGGT. Firstly, we remove high-dimensional 15-degree-of-freedom drift and planar degeneracy from VGGT-SLAM by creating a new factor graph design while still addressing the reconstruction ambiguity of VGGT given unknown camera intrinsics. Secondly, by studying the attention layers of VGGT, we show that one of the layers is well suited to assist in image retrieval verification for free without additional training, which enables both rejecting false positive matches and allows for completing more loop closures. Finally, we conduct a suite of experiments which includes showing VGGT-SLAM 2.0 can easily be adapted for open-set object detection and demonstrating real time performance while running online onboard a ground robot using a Jetson Thor. We also test in environments ranging from cluttered indoor apartments and office scenes to a 4,200 square foot barn, and we also demonstrate VGGT-SLAM 2.0 achieves the highest accuracy on the TUM dataset with about 23 percent less pose error than VGGT-SLAM. Code will be released upon publication."
    },
    {
      "title": "Dep-Search: Learning Dependency-Aware Reasoning Traces with Persistent Memory",
      "url": "https://arxiv.org/abs/2601.18771",
      "date": "2026-01-26",
      "authors_text": "Yanming Liu, Xinyue Peng, Zixuan Yan, Yanxin Shen, Wenjie Xu",
      "is_highlight": false,
      "score": 46.0,
      "summary": "Dep-Search enhances large language models' multi-hop reasoning by integrating dependency-aware search, structured retrieval, and persistent memory, outperforming existing frameworks in complex tasks.",
      "debug_abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in complex reasoning tasks, particularly when augmented with search mechanisms that enable systematic exploration of external knowledge bases. The field has evolved from traditional retrieval-augmented generation (RAG) frameworks to more sophisticated search-based frameworks that orchestrate multi-step reasoning through explicit search strategies. However, existing search frameworks still rely heavily on implicit natural language reasoning to determine search strategies and how to leverage retrieved information across reasoning steps. This reliance on implicit reasoning creates fundamental challenges for managing dependencies between sub-questions, efficiently reusing previously retrieved knowledge, and learning optimal search strategies through reinforcement learning. To address these limitations, we propose Dep-Search, a dependency-aware search framework that advances beyond existing search frameworks by integrating structured reasoning, retrieval, and persistent memory through GRPO. Dep-Search introduces explicit control mechanisms that enable the model to decompose questions with dependency relationships, retrieve information when needed, access previously stored knowledge from memory, and summarize long reasoning contexts into reusable memory entries. Through extensive experiments on seven diverse question answering datasets, we demonstrate that Dep-Search significantly enhances LLMs&#39; ability to tackle complex multi-hop reasoning tasks, achieving substantial improvements over strong baselines across different model scales."
    }
  ],
  "加州理工-机器人实验室": [
    {
      "title": "Input-to-State Safe Backstepping: Robust Safety-Critical Control with Unmatched Uncertainties",
      "url": "https://arxiv.org/abs/2602.03691",
      "date": "2026-02-03",
      "authors_text": "Max H. Cohen, Pio Ong, Aaron D. Ames",
      "is_highlight": false,
      "score": 50.0,
      "summary": "This paper introduces a novel approach for ensuring safety in nonlinear systems with unmatched disturbances through an enhanced input-to-state safety framework using Optimal Decay CBFs.",
      "teaser_image": "https://arxiv.org/html/2602.03691/x2.png",
      "debug_abstract": "Guaranteeing safety in the presence of unmatched disturbances -- uncertainties that cannot be directly canceled by the control input -- remains a key challenge in nonlinear control. This paper presents a constructive approach to safety-critical control of nonlinear systems with unmatched disturbances. We first present a generalization of the input-to-state safety (ISSf) framework for systems with these uncertainties using the recently developed notion of an Optimal Decay CBF, which provides more flexibility for satisfying the associated Lyapunov-like conditions for safety. From there, we outline a procedure for constructing ISSf-CBFs for two relevant classes of systems with unmatched uncertainties: i) strict-feedback systems; ii) dual-relative-degree systems, which are similar to differentially flat systems. Our theoretical results are illustrated via numerical simulations of an inverted pendulum and planar quadrotor."
    },
    {
      "title": "Large-Scale Continual Scheduling and Execution for Dynamic Distributed Satellite Constellation Observation Allocation",
      "url": "https://arxiv.org/abs/2601.06188",
      "date": "2026-01-26",
      "authors_text": "Itai Zilberstein, Steve Chien",
      "is_highlight": false,
      "score": 70.0,
      "summary": "This paper introduces the Dynamic Incremental Neighborhood Stochastic Search algorithm for efficient scheduling in large-scale satellite constellations, enhancing autonomy and performance in dynamic environments.",
      "debug_abstract": "The size and capabilities of Earth-observing satellite constellations are rapidly increasing. Leveraging distributed onboard control, we can enable novel time-sensitive measurements and responses. However, deploying autonomy to large multiagent satellite systems necessitates algorithms with efficient computation and communication. We tackle this challenge and propose new, online algorithms for large-scale dynamic distributed constraint optimization problems (DDCOP). We present the Dynamic Multi-Satellite Constellation Observation Scheduling Problem (DCOSP), a new formulation of DDCOPs that models integrated scheduling and execution. We construct an omniscient offline algorithm to compute the novel optimality condition of DCOSP and present the Dynamic Incremental Neighborhood Stochastic Search (D-NSS) algorithm, an incomplete online decomposition-based DDCOP approach. We show through simulation that D-NSS converges to near-optimal solutions and outperforms DDCOP baselines in terms of solution quality, computation time, and message volume. Our work forms the foundation of the largest in-space demonstration of distributed multiagent AI to date: the NASA FAME mission."
    }
  ],
  "具身智能多模态大模型中心": [
    {
      "title": "TIGaussian: Disentangle Gaussians for Spatial-Awared Text-Image-3D Alignment",
      "url": "https://arxiv.org/abs/2601.19247",
      "date": "2026-01-27",
      "authors_text": "Jiarun Liu, Qifeng Chen, Yiru Zhao, Minghua Liu, Baorui Ma",
      "is_highlight": false,
      "score": 72.0,
      "summary": "TIGaussian enhances cross-modal alignment by utilizing 3D Gaussian Splatting for improved feature extraction and integration of text, image, and 3D data.",
      "debug_abstract": "While visual-language models have profoundly linked features between texts and images, the incorporation of 3D modality data, such as point clouds and 3D Gaussians, further enables pretraining for 3D-related tasks, e.g., cross-modal retrieval, zero-shot classification, and scene recognition. As challenges remain in extracting 3D modal features and bridging the gap between different modalities, we propose TIGaussian, a framework that harnesses 3D Gaussian Splatting (3DGS) characteristics to strengthen cross-modality alignment through multi-branch 3DGS tokenizer and modality-specific 3D feature alignment strategies. Specifically, our multi-branch 3DGS tokenizer decouples the intrinsic properties of 3DGS structures into compact latent representations, enabling more generalizable feature extraction. To further bridge the modality gap, we develop a bidirectional cross-modal alignment strategies: a multi-view feature fusion mechanism that leverages diffusion priors to resolve perspective ambiguity in image-3D alignment, while a text-3D projection module adaptively maps 3D features to text embedding space for better text-3D alignment. Extensive experiments on various datasets demonstrate the state-of-the-art performance of TIGaussian in multiple tasks."
    }
  ],
  "具身智能研究中心": [
    {
      "title": "TIGaussian: Disentangle Gaussians for Spatial-Awared Text-Image-3D Alignment",
      "url": "https://arxiv.org/abs/2601.19247",
      "date": "2026-01-27",
      "authors_text": "Jiarun Liu, Qifeng Chen, Yiru Zhao, Minghua Liu, Baorui Ma",
      "is_highlight": false,
      "score": 72.0,
      "summary": "TIGaussian enhances cross-modal alignment by utilizing 3D Gaussian Splatting for improved feature extraction and integration of text, image, and 3D data.",
      "debug_abstract": "While visual-language models have profoundly linked features between texts and images, the incorporation of 3D modality data, such as point clouds and 3D Gaussians, further enables pretraining for 3D-related tasks, e.g., cross-modal retrieval, zero-shot classification, and scene recognition. As challenges remain in extracting 3D modal features and bridging the gap between different modalities, we propose TIGaussian, a framework that harnesses 3D Gaussian Splatting (3DGS) characteristics to strengthen cross-modality alignment through multi-branch 3DGS tokenizer and modality-specific 3D feature alignment strategies. Specifically, our multi-branch 3DGS tokenizer decouples the intrinsic properties of 3DGS structures into compact latent representations, enabling more generalizable feature extraction. To further bridge the modality gap, we develop a bidirectional cross-modal alignment strategies: a multi-view feature fusion mechanism that leverages diffusion priors to resolve perspective ambiguity in image-3D alignment, while a text-3D projection module adaptively maps 3D features to text embedding space for better text-3D alignment. Extensive experiments on various datasets demonstrate the state-of-the-art performance of TIGaussian in multiple tasks."
    }
  ],
  "Berkeley Artificial Intelligence Research Lab (BAIR)": [
    {
      "title": "Spatiotemporal Decision Transformer for Traffic Coordination",
      "url": "https://arxiv.org/abs/2602.02903",
      "date": "2026-02-02",
      "authors_text": "Haoran Su, Yandong Sun, Hanxiao Deng",
      "is_highlight": false,
      "score": 81.0,
      "summary": "The MADT framework enhances multi-agent traffic signal control using a sequence modeling approach, achieving state-of-the-art performance and improved coordination through graph attention and temporal dynamics.",
      "teaser_image": "https://arxiv.org/html/2602.02903/figures/method_comparison.png",
      "debug_abstract": "Traffic signal control is a critical challenge in urban transportation, requiring coordination among multiple intersections to optimize network-wide traffic flow. While reinforcement learning has shown promise for adaptive signal control, existing methods struggle with multi-agent coordination and sample efficiency. We introduce MADT (Multi-Agent Decision Transformer), a novel approach that reformulates multi-agent traffic signal control as a sequence modeling problem. MADT extends the Decision Transformer paradigm to multi-agent settings by incorporating: (1) a graph attention mechanism for modeling spatial dependencies between intersections, (2) a|temporal transformer encoder for capturing traffic dynamics, and (3) return-to-go conditioning for target performance specification. Our approach enables offline learning from historical traffic data, with architecture design that facilitates potential online fine-tuning. Experiments on synthetic grid networks and real-world traffic scenarios demonstrate that MADT achieves state-of-the-art performance, reducing average travel time by 5-6% compared to the strongest baseline while exhibiting superior coordination among adjacent intersections."
    },
    {
      "title": "HetroD: A High-Fidelity Drone Dataset and Benchmark for Autonomous Driving in Heterogeneous Traffic",
      "url": "https://arxiv.org/abs/2602.03447",
      "date": "2026-02-03",
      "authors_text": "Yu-Hsiang Chen, Wei-Jer Chang, Christian Kotulla, Thomas Keutgens, Steffen Runde",
      "is_highlight": false,
      "score": 92.0,
      "summary": "HetroD is a comprehensive drone-based dataset designed to enhance autonomous driving systems by addressing the complexities of heterogeneous traffic involving vulnerable road users.",
      "teaser_image": "https://arxiv.org/html/2602.03447/x3.png",
      "debug_abstract": "We present HetroD, a dataset and benchmark for developing autonomous driving systems in heterogeneous environments. HetroD targets the critical challenge of navi- gating real-world heterogeneous traffic dominated by vulner- able road users (VRUs), including pedestrians, cyclists, and motorcyclists that interact with vehicles. These mixed agent types exhibit complex behaviors such as hook turns, lane splitting, and informal right-of-way negotiation. Such behaviors pose significant challenges for autonomous vehicles but remain underrepresented in existing datasets focused on structured, lane-disciplined traffic. To bridge the gap, we collect a large- scale drone-based dataset to provide a holistic observation of traffic scenes with centimeter-accurate annotations, HD maps, and traffic signal states. We further develop a modular toolkit for extracting per-agent scenarios to support downstream task development. In total, the dataset comprises over 65.4k high- fidelity agent trajectories, 70% of which are from VRUs. HetroD supports modeling of VRU behaviors in dense, het- erogeneous traffic and provides standardized benchmarks for forecasting, planning, and simulation tasks. Evaluation results reveal that state-of-the-art prediction and planning models struggle with the challenges presented by our dataset: they fail to predict lateral VRU movements, cannot handle unstructured maneuvers, and exhibit limited performance in dense and multi-agent scenarios, highlighting the need for more robust approaches to heterogeneous traffic. See our project page for more examples: this https URL"
    },
    {
      "title": "ROMA: Recursive Open Meta-Agent Framework for Long-Horizon Multi-Agent Systems",
      "url": "https://arxiv.org/abs/2602.01848",
      "date": "2026-02-02",
      "authors_text": "Salaheddin Alzu'bi, Baran Nama, Arda Kaz, Anushri Eswaran, Weiyuan Chen",
      "is_highlight": false,
      "score": 31.0,
      "summary": "ROMA is a recursive framework that enhances long-horizon multi-agent systems by decomposing tasks into parallel subtasks, improving performance and interpretability without fine-tuning.",
      "teaser_image": "https://arxiv.org/html/2602.01848/x2.png",
      "debug_abstract": "Current agentic frameworks underperform on long-horizon tasks. As reasoning depth increases, sequential orchestration becomes brittle, context windows impose hard limits that degrade performance, and opaque execution traces make failures difficult to localize or debug. We introduce ROMA (Recursive Open Meta-Agents), a domain-agnostic framework that addresses these limitations through recursive task decomposition and structured aggregation. ROMA decomposes goals into dependency-aware subtask trees that can be executed in parallel, while aggregation compresses and validates intermediate results to control context growth. Our framework standardizes agent construction around four modular roles --Atomizer (which decides whether a task should be decomposed), Planner, Executor, and Aggregator -- which cleanly separate orchestration from model selection and enable transparent, hierarchical execution traces. This design supports heterogeneous multi-agent systems that mix models and tools according to cost, latency, and capability. To adapt ROMA to specific tasks without fine-tuning, we further introduce GEPA$+$, an improved Genetic-Pareto prompt proposer that searches over prompts within ROMA&#39;s component hierarchy while preserving interface contracts. We show that ROMA, combined with GEPA+, delivers leading system-level performance on reasoning and long-form generation benchmarks. On SEAL-0, which evaluates reasoning over conflicting web evidence, ROMA instantiated with GLM-4.6 improves accuracy by 9.9\\% over Kimi-Researcher. On EQ-Bench, a long-form writing benchmark, ROMA enables DeepSeek-V3 to match the performance of leading closed-source models such as Claude Sonnet 4.5. Our results demonstrate that recursive, modular agent architectures can scale reasoning depth while remaining interpretable, flexible, and model-agnostic."
    },
    {
      "title": "Flow Policy Gradients for Robot Control",
      "url": "https://arxiv.org/abs/2602.02481",
      "date": "2026-02-02",
      "authors_text": "Brent Yi, Hongsuk Choi, Himanshu Gaurav Singh, Xiaoyu Huang, Takara E. Truong",
      "is_highlight": false,
      "score": 13.0,
      "summary": "This paper introduces flow matching policy gradients for training expressive robot control policies, enhancing performance in locomotion, manipulation, and sim-to-real transfer tasks.",
      "teaser_image": "https://arxiv.org/html/2602.02481/x11.png",
      "debug_abstract": "Likelihood-based policy gradient methods are the dominant approach for training robot control policies from rewards. These methods rely on differentiable action likelihoods, which constrain policy outputs to simple distributions like Gaussians. In this work, we show how flow matching policy gradients -- a recent framework that bypasses likelihood computation -- can be made effective for training and fine-tuning more expressive policies in challenging robot control settings. We introduce an improved objective that enables success in legged locomotion, humanoid motion tracking, and manipulation tasks, as well as robust sim-to-real transfer on two humanoid robots. We then present ablations and analysis on training dynamics. Results show how policies can exploit the flow representation for exploration when training from scratch, as well as improved fine-tuning robustness over baselines."
    },
    {
      "title": "mjlab: A Lightweight Framework for GPU-Accelerated Robot Learning",
      "url": "https://arxiv.org/abs/2601.22074",
      "date": "2026-01-29",
      "authors_text": "Kevin Zakka, Qiayuan Liao, Brent Yi, Louis Le Lay, Koushil Sreenath",
      "is_highlight": false,
      "score": 92.0,
      "summary": "mjlab is an open-source, GPU-accelerated robot learning framework that simplifies setup and enables modular environment composition for various robotic tasks.",
      "teaser_image": "https://arxiv.org/html/2601.22074/rough_terrain.png",
      "debug_abstract": "We present mjlab, a lightweight, open-source framework for robot learning that combines GPU-accelerated simulation with composable environments and minimal setup friction. mjlab adopts the manager-based API introduced by Isaac Lab, where users compose modular building blocks for observations, rewards, and events, and pairs it with MuJoCo Warp for GPU-accelerated physics. The result is a framework installable with a single command, requiring minimal dependencies, and providing direct access to native MuJoCo data structures. mjlab ships with reference implementations of velocity tracking, motion imitation, and manipulation tasks."
    },
    {
      "title": "DeFM: Learning Foundation Representations from Depth for Robotics",
      "url": "https://arxiv.org/abs/2601.18923",
      "date": "2026-01-26",
      "authors_text": "Manthan Patel, Jonas Frey, Mayank Mittal, Fan Yang, Alexander Hansson",
      "is_highlight": true,
      "score": 94.0,
      "summary": "DeFM is a self-supervised foundation model that learns robust geometric and semantic representations from depth images, achieving state-of-the-art performance in various robotic tasks and environments.",
      "teaser_image": "https://arxiv.org/html/2601.18923/x1.png",
      "debug_abstract": "Depth sensors are widely deployed across robotic platforms, and advances in fast, high-fidelity depth simulation have enabled robotic policies trained on depth observations to achieve robust sim-to-real transfer for a wide range of tasks. Despite this, representation learning for depth modality remains underexplored compared to RGB, where large-scale foundation models now define the state of the art. To address this gap, we present DeFM, a self-supervised foundation model trained entirely on depth images for robotic applications. Using a DINO-style self-distillation objective on a curated dataset of 60M depth images, DeFM learns geometric and semantic representations that generalize to diverse environments, tasks, and sensors. To retain metric awareness across multiple scales, we introduce a novel input normalization strategy. We further distill DeFM into compact models suitable for resource-constrained robotic systems. When evaluated on depth-based classification, segmentation, navigation, locomotion, and manipulation benchmarks, DeFM achieves state-of-the-art performance and demonstrates strong generalization from simulation to real-world environments. We release all our pretrained models, which can be adopted off-the-shelf for depth-based robotic learning without task-specific fine-tuning. Webpage: this https URL"
    }
  ],
  "上海科技大学": [
    {
      "title": "Interpreting and Controlling LLM Reasoning through Integrated Policy Gradient",
      "url": "https://arxiv.org/abs/2602.02313",
      "date": "2026-02-02",
      "authors_text": "Changming Li, Kaixing Zhang, Haoyun Xu, Yingdong Shi, Zheng Zhang",
      "is_highlight": false,
      "score": 19.0,
      "summary": "The paper introduces Integrated Policy Gradient (IPG), a framework that enhances the interpretability and control of large language model reasoning by tracing sequential contributions of internal components to outcomes.",
      "teaser_image": "https://arxiv.org/html/2602.02313/x1.png",
      "debug_abstract": "Large language models (LLMs) demonstrate strong reasoning abilities in solving complex real-world problems. Yet, the internal mechanisms driving these complex reasoning behaviors remain opaque. Existing interpretability approaches targeting reasoning either identify components (e.g., neurons) correlated with special textual patterns, or rely on human-annotated contrastive pairs to derive control vectors. Consequently, current methods struggle to precisely localize complex reasoning mechanisms or capture sequential influence from model internal workings to the reasoning outputs. In this paper, built on outcome-oriented and sequential-influence-aware principles, we focus on identifying components that have sequential contribution to reasoning behavior where outcomes are cumulated by long-range effects. We propose Integrated Policy Gradient (IPG), a novel framework that attributes reasoning behaviors to model&#39;s inner components by propagating compound outcome-based signals such as post reasoning accuracy backward through model inference trajectories. Empirical evaluations demonstrate that our approach achieves more precise localization and enables reliable modulation of reasoning behaviors (e.g., reasoning capability, reasoning strength) across diverse reasoning models."
    },
    {
      "title": "Tactile-Force Alignment in Vision-Language-Action Models for Force-aware Manipulation",
      "url": "https://arxiv.org/abs/2601.20321",
      "date": "2026-01-28",
      "authors_text": "Yuzhe Huang, Pei Lin, Wanlin Li, Daohan Li, Jiajun Li",
      "is_highlight": false,
      "score": 91.0,
      "summary": "The paper introduces TaF-VLA, a framework that enhances Vision-Language-Action models by integrating tactile-force alignment for improved force-aware robotic manipulation in contact-rich tasks.",
      "teaser_image": "https://arxiv.org/html/2601.20321/figures/fig1-teaser.png",
      "debug_abstract": "Vision-Language-Action (VLA) models have recently emerged as powerful generalists for robotic manipulation. However, due to their predominant reliance on visual modalities, they fundamentally lack the physical intuition required for contact-rich tasks that require precise force regulation and physical reasoning. Existing attempts to incorporate vision-based tactile sensing into VLA models typically treat tactile inputs as auxiliary visual textures, thereby overlooking the underlying correlation between surface deformation and interaction dynamics. To bridge this gap, we propose a paradigm shift from tactile-vision alignment to tactile-force alignment. Here, we introduce TaF-VLA, a framework that explicitly grounds high-dimensional tactile observations in physical interaction forces. To facilitate this, we develop an automated tactile-force data acquisition device and curate the TaF-Dataset, comprising over 10 million synchronized tactile observations, 6-axis force/torque, and matrix force map. To align sequential tactile observations with interaction forces, the central component of our approach is the Tactile-Force Adapter (TaF-Adapter), a tactile sensor encoder that extracts discretized latent information for encoding tactile observations. This mechanism ensures that the learned representations capture history-dependent, noise-insensitive physical dynamics rather than static visual textures. Finally, we integrate this force-aligned encoder into a VLA backbone. Extensive real-world experiments demonstrate that TaF-VLA policy significantly outperforms state-of-the-art tactile-vision-aligned and vision-only baselines on contact-rich tasks, verifying its ability to achieve robust, force-aware manipulation through cross-modal physical reasoning."
    }
  ],
  "Rajpurkar Lab": [
    {
      "title": "Accelerating Structured Chain-of-Thought in Autonomous Vehicles",
      "url": "https://arxiv.org/abs/2602.02864",
      "date": "2026-02-02",
      "authors_text": "Yi Gu, Yan Wang, Yuxiao Chen, Yurong You, Wenjie Luo",
      "is_highlight": false,
      "score": 76.0,
      "summary": "FastDriveCoT accelerates Chain-of-Thought reasoning in autonomous vehicles by enabling parallel decoding of sub-tasks, achieving 3-4× speedup while maintaining decision-making improvements.",
      "teaser_image": "https://arxiv.org/html/2602.02864/x2.png",
      "debug_abstract": "Chain-of-Thought (CoT) reasoning enhances the decision-making capabilities of vision-language-action models in autonomous driving, but its autoregressive nature introduces significant inference latency, making it impractical for real-time applications. To address this, we introduce FastDriveCoT, a novel parallel decoding method that accelerates template-structured CoT. Our approach decomposes the reasoning process into a dependency graph of distinct sub-tasks, such as identifying critical objects and summarizing traffic rules, some of which can be generated in parallel. By generating multiple independent reasoning steps concurrently within a single forward pass, we significantly reduce the number of sequential computations. Experiments demonstrate a 3-4$\\times$ speedup in CoT generation and a substantial reduction in end-to-end latency across various model architectures, all while preserving the original downstream task improvements brought by incorporating CoT reasoning."
    },
    {
      "title": "FD-VLA: Force-Distilled Vision-Language-Action Model for Contact-Rich Manipulation",
      "url": "https://arxiv.org/abs/2602.02142",
      "date": "2026-02-02",
      "authors_text": "Ruiteng Zhao, Wenshuo Wang, Yicheng Ma, Xiaocong Li, Francis E. H. Tay",
      "is_highlight": false,
      "score": 0,
      "summary": "The FD-VLA framework enhances contact-rich manipulation by integrating force awareness through a novel distillation method, improving performance without physical force sensors.",
      "teaser_image": "https://arxiv.org/html/2602.02142/x1.png",
      "debug_abstract": "Force sensing is a crucial modality for Vision-Language-Action (VLA) frameworks, as it enables fine-grained perception and dexterous manipulation in contact-rich tasks. We present Force-Distilled VLA (FD-VLA), a novel framework that integrates force awareness into contact-rich manipulation without relying on physical force sensors. The core of our approach is a Force Distillation Module (FDM), which distills force by mapping a learnable query token, conditioned on visual observations and robot states, into a predicted force token aligned with the latent representation of actual force signals. During inference, this distilled force token is injected into the pretrained VLM, enabling force-aware reasoning while preserving the integrity of its vision-language semantics. This design provides two key benefits: first, it allows practical deployment across a wide range of robots that lack expensive or fragile force-torque sensors, thereby reducing hardware cost and complexity; second, the FDM introduces an additional force-vision-state fusion prior to the VLM, which improves cross-modal alignment and enhances perception-action robustness in contact-rich scenarios. Surprisingly, our physical experiments show that the distilled force token outperforms direct sensor force measurements as well as other baselines, which highlights the effectiveness of this force-distilled VLA approach."
    },
    {
      "title": "Investigating the Development of Task-Oriented Communication in Vision-Language Models",
      "url": "https://arxiv.org/abs/2601.20641",
      "date": "2026-01-28",
      "authors_text": "Boaz Carmeli, Orr Paradise, Shafi Goldwasser, Yonatan Belinkov, Ron Meir",
      "is_highlight": false,
      "score": 73.0,
      "summary": "The study reveals that LLM-based agents can create efficient, covert communication protocols in collaborative tasks, raising transparency concerns while demonstrating spontaneous coordination.",
      "teaser_image": "https://arxiv.org/html/2601.20641/x2.png",
      "debug_abstract": "We investigate whether \\emph{LLM-based agents} can develop task-oriented communication protocols that differ from standard natural language in collaborative reasoning tasks. Our focus is on two core properties such task-oriented protocols may exhibit: Efficiency -- conveying task-relevant information more concisely than natural language, and Covertness -- becoming difficult for external observers to interpret, raising concerns about transparency and control. To investigate these aspects, we use a referential-game framework in which vision-language model (VLM) agents communicate, providing a controlled, measurable setting for evaluating language variants. Experiments show that VLMs can develop effective, task-adapted communication patterns. At the same time, they can develop covert protocols that are difficult for humans and external agents to interpret. We also observe spontaneous coordination between similar models without explicitly shared protocols. These findings highlight both the potential and the risks of task-oriented communication, and position referential games as a valuable testbed for future work in this area."
    }
  ],
  "Harvard Microrobotics Laboratory": [
    {
      "title": "Accelerating Structured Chain-of-Thought in Autonomous Vehicles",
      "url": "https://arxiv.org/abs/2602.02864",
      "date": "2026-02-02",
      "authors_text": "Yi Gu, Yan Wang, Yuxiao Chen, Yurong You, Wenjie Luo",
      "is_highlight": false,
      "score": 76.0,
      "summary": "FastDriveCoT accelerates Chain-of-Thought reasoning in autonomous vehicles by enabling parallel decoding of sub-tasks, achieving 3-4× speedup while maintaining decision-making improvements.",
      "teaser_image": "https://arxiv.org/html/2602.02864/x2.png",
      "debug_abstract": "Chain-of-Thought (CoT) reasoning enhances the decision-making capabilities of vision-language-action models in autonomous driving, but its autoregressive nature introduces significant inference latency, making it impractical for real-time applications. To address this, we introduce FastDriveCoT, a novel parallel decoding method that accelerates template-structured CoT. Our approach decomposes the reasoning process into a dependency graph of distinct sub-tasks, such as identifying critical objects and summarizing traffic rules, some of which can be generated in parallel. By generating multiple independent reasoning steps concurrently within a single forward pass, we significantly reduce the number of sequential computations. Experiments demonstrate a 3-4$\\times$ speedup in CoT generation and a substantial reduction in end-to-end latency across various model architectures, all while preserving the original downstream task improvements brought by incorporating CoT reasoning."
    },
    {
      "title": "FD-VLA: Force-Distilled Vision-Language-Action Model for Contact-Rich Manipulation",
      "url": "https://arxiv.org/abs/2602.02142",
      "date": "2026-02-02",
      "authors_text": "Ruiteng Zhao, Wenshuo Wang, Yicheng Ma, Xiaocong Li, Francis E. H. Tay",
      "is_highlight": false,
      "score": 0,
      "summary": "The FD-VLA framework enhances contact-rich manipulation by integrating force awareness through a novel distillation method, improving performance without physical force sensors.",
      "teaser_image": "https://arxiv.org/html/2602.02142/x1.png",
      "debug_abstract": "Force sensing is a crucial modality for Vision-Language-Action (VLA) frameworks, as it enables fine-grained perception and dexterous manipulation in contact-rich tasks. We present Force-Distilled VLA (FD-VLA), a novel framework that integrates force awareness into contact-rich manipulation without relying on physical force sensors. The core of our approach is a Force Distillation Module (FDM), which distills force by mapping a learnable query token, conditioned on visual observations and robot states, into a predicted force token aligned with the latent representation of actual force signals. During inference, this distilled force token is injected into the pretrained VLM, enabling force-aware reasoning while preserving the integrity of its vision-language semantics. This design provides two key benefits: first, it allows practical deployment across a wide range of robots that lack expensive or fragile force-torque sensors, thereby reducing hardware cost and complexity; second, the FDM introduces an additional force-vision-state fusion prior to the VLM, which improves cross-modal alignment and enhances perception-action robustness in contact-rich scenarios. Surprisingly, our physical experiments show that the distilled force token outperforms direct sensor force measurements as well as other baselines, which highlights the effectiveness of this force-distilled VLA approach."
    },
    {
      "title": "Investigating the Development of Task-Oriented Communication in Vision-Language Models",
      "url": "https://arxiv.org/abs/2601.20641",
      "date": "2026-01-28",
      "authors_text": "Boaz Carmeli, Orr Paradise, Shafi Goldwasser, Yonatan Belinkov, Ron Meir",
      "is_highlight": false,
      "score": 73.0,
      "summary": "The study reveals that LLM-based agents can create efficient, covert communication protocols in collaborative tasks, raising transparency concerns while demonstrating spontaneous coordination.",
      "teaser_image": "https://arxiv.org/html/2601.20641/x2.png",
      "debug_abstract": "We investigate whether \\emph{LLM-based agents} can develop task-oriented communication protocols that differ from standard natural language in collaborative reasoning tasks. Our focus is on two core properties such task-oriented protocols may exhibit: Efficiency -- conveying task-relevant information more concisely than natural language, and Covertness -- becoming difficult for external observers to interpret, raising concerns about transparency and control. To investigate these aspects, we use a referential-game framework in which vision-language model (VLM) agents communicate, providing a controlled, measurable setting for evaluating language variants. Experiments show that VLMs can develop effective, task-adapted communication patterns. At the same time, they can develop covert protocols that are difficult for humans and external agents to interpret. We also observe spontaneous coordination between similar models without explicitly shared protocols. These findings highlight both the potential and the risks of task-oriented communication, and position referential games as a valuable testbed for future work in this area."
    }
  ],
  "厦门大学": [
    {
      "title": "MedAD-R1: Eliciting Consistent Reasoning in Interpretible Medical Anomaly Detection via Consistency-Reinforced Policy Optimization",
      "url": "https://arxiv.org/abs/2602.01081",
      "date": "2026-02-01",
      "authors_text": "Haitao Zhang, Yingying Wang, Jiaxiang Wang, Haote Xu, Hongyang Zhang",
      "is_highlight": false,
      "score": 64.0,
      "summary": "MedAD-R1 enhances medical anomaly detection by utilizing a two-stage training framework with consistency-reinforced policy optimization, achieving state-of-the-art performance on the MedAD-38K benchmark.",
      "teaser_image": "https://arxiv.org/html/2602.01081/x3.png",
      "debug_abstract": "Medical Anomaly Detection (MedAD) presents a significant opportunity to enhance diagnostic accuracy using Large Multimodal Models (LMMs) to interpret and answer questions based on medical images. However, the reliance on Supervised Fine-Tuning (SFT) on simplistic and fragmented datasets has hindered the development of models capable of plausible reasoning and robust multimodal generalization. To overcome this, we introduce MedAD-38K, the first large-scale, multi-modal, and multi-center benchmark for MedAD featuring diagnostic Chain-of-Thought (CoT) annotations alongside structured Visual Question-Answering (VQA) pairs. On this foundation, we propose a two-stage training framework. The first stage, Cognitive Injection, uses SFT to instill foundational medical knowledge and align the model with a structured think-then-answer paradigm. Given that standard policy optimization can produce reasoning that is disconnected from the final answer, the second stage incorporates Consistency Group Relative Policy Optimization (Con-GRPO). This novel algorithm incorporates a crucial consistency reward to ensure the generated reasoning process is relevant and logically coherent with the final diagnosis. Our proposed model, MedAD-R1, achieves state-of-the-art (SOTA) performance on the MedAD-38K benchmark, outperforming strong baselines by more than 10\\%. This superior performance stems from its ability to generate transparent and logically consistent reasoning pathways, offering a promising approach to enhancing the trustworthiness and interpretability of AI for clinical decision support."
    },
    {
      "title": "A State-Transition Framework for Efficient LLM Reasoning",
      "url": "https://arxiv.org/abs/2602.01198",
      "date": "2026-02-01",
      "authors_text": "Liang Zhang, Yu Zhao, Longyue Wang, Tianqi Shi, Weihua Luo",
      "is_highlight": false,
      "score": 49.0,
      "summary": "This paper introduces a state-transition framework for LLMs that enhances reasoning efficiency by reducing attention complexity from quadratic to linear while improving performance.",
      "teaser_image": "https://arxiv.org/html/2602.01198/figure-2.jpeg",
      "debug_abstract": "While Long Chain-of-Thought (CoT) reasoning significantly improves Large Language Models (LLMs) performance on complex reasoning tasks, the substantial computational and memory costs of generating long CoT sequences limit their efficiency and practicality. Existing studies usually enhance the reasoning efficiency of LLMs by compressing CoT sequences. However, this approach conflicts with test-time scaling, limiting the reasoning capacity of LLMs. In this paper, we propose an efficient reasoning framework that models the reasoning process of LLMs as a state-transition process. Specifically, we first apply a linear attention mechanism to estimate the LLM&#39;s reasoning state, which records the historical reasoning information from previous reasoning steps. Then, based on the query prompt and the reasoning state, the LLM can efficiently perform the current reasoning step and update the state. With the linear attention, each token in the current reasoning step can directly retrieve relevant historical reasoning information from the reasoning state, without explicitly attending to tokens in previous reasoning steps. In this way, the computational complexity of attention is reduced from quadratic to linear, significantly improving the reasoning efficiency of LLMs. In addition, we propose a state-based reasoning strategy to mitigate the over-thinking issue caused by noisy reasoning steps. Extensive experiments across multiple datasets and model sizes demonstrate that our framework not only improves the reasoning efficiency of LLMs but also enhances their reasoning performance."
    }
  ],
  "高性能人形技术实验室 (H²T)": [
    {
      "title": "Efficient Cross-Country Data Acquisition Strategy for ADAS via Street-View Imagery",
      "url": "https://arxiv.org/abs/2602.01836",
      "date": "2026-02-02",
      "authors_text": "Yin Wu, Daniel Slieter, Carl Esselborn, Ahmed Abouelazm, Tsung Yuan Tseng",
      "is_highlight": false,
      "score": 34.0,
      "summary": "The paper presents a street-view-guided data acquisition strategy for ADAS, enhancing cross-country model adaptation efficiency and cost-effectiveness through innovative POI scoring methods.",
      "teaser_image": "https://arxiv.org/html/2602.01836/x1.png",
      "debug_abstract": "Deploying ADAS and ADS across countries remains challenging due to differences in legislation, traffic infrastructure, and visual conventions, which introduce domain shifts that degrade perception performance. Traditional cross-country data collection relies on extensive on-road driving, making it costly and inefficient to identify representative locations. To address this, we propose a street-view-guided data acquisition strategy that leverages publicly available imagery to identify places of interest (POI). Two POI scoring methods are introduced: a KNN-based feature distance approach using a vision foundation model, and a visual-attribution approach using a vision-language model. To enable repeatable evaluation, we adopt a collect-detect protocol and construct a co-located dataset by pairing the Zenseact Open Dataset with Mapillary street-view images. Experiments on traffic sign detection, a task particularly sensitive to cross-country variations in sign appearance, show that our approach achieves performance comparable to random sampling while using only half of the target-domain data. We further provide cost estimations for full-country analysis, demonstrating that large-scale street-view processing remains economically feasible. These results highlight the potential of street-view-guided data acquisition for efficient and cost-effective cross-country model adaptation."
    }
  ]
}