实验室名,地区,Region,学校,实验室方向,简要介绍,团队重要人物,代表论文,经度,纬度,实验室主页,英文名,Institution_Keywords,Unnamed: 12,paper
具身感知与交互实验室 (EPIC Lab),中国,China,北京大学,"三维视觉感知与机器人学, 具身多模态大模型",由王鹤博士创立并领导，隶属北京大学前沿计算研究中心。研究目标是通过发展具身技能及具身多模态大模型，推进通用具身智能的实现。,王鹤,"Task-Oriented Dexterous Grasp Synthesis, NaVid, MaskClustering",116.308264,39.995304,https://pku-epic.github.io/,He Wang,Peking University,,"GraspVLA: a Grasping Foundation Model Pre-trained on Billion-scale Synthetic Action Data|https://arxiv.org/abs/2505.03233;DreamVLA: A Vision-Language-Action Model Dreamed with Comprehensive World Knowledge|https://arxiv.org/abs/2507.04447;TrackVLA++: Unleashing Reasoning and Memory Capabilities in VLA Models for Embodied Visual Tracking|https://arxiv.org/abs/2510.07134;ASGrasp: Generalizable Transparent Object Reconstruction and 6-DoF Grasp Detection from RGB-D Active Stereo Camera|https://arxiv.org/abs/2405.05648;Watch Less, Feel More: Sim-to-Real RL for Generalizable Articulated Object Manipulation via Motion Adaptation and Impedance Control|https://arxiv.org/abs/2502.14457;Text-guided Fine-Grained Video Anomaly Detection|https://arxiv.org/abs/2511.00524;RoboCOIN: An Open-Sourced Bimanual Robotic Data COllection for INtegrated Manipulation|https://arxiv.org/abs/2511.17441;Make a Donut: Hierarchical EMD-Space Planning for Zero-Shot Deformable Manipulation with Tools|https://arxiv.org/abs/2311.02787;Qwen3-Omni Technical Report|https://arxiv.org/abs/2509.17765;FetchBot: Learning Generalizable Object Fetching in Cluttered Scenes via Zero-Shot Sim2Real|https://arxiv.org/abs/2502.17894;FLUX-Makeup: High-Fidelity, Identity-Consistent, and Robust Makeup Transfer via Diffusion Transformer|https://arxiv.org/abs/2508.05069;QuadWBG: Generalizable Quadrupedal Whole-Body Grasping|https://arxiv.org/abs/2411.06782;Large-Scale Multi-Character Interaction Synthesis|https://arxiv.org/abs/2505.14087;Robust Differentiable Collision Detection for General Objects|https://arxiv.org/abs/2511.06267;Code-as-Monitor: Constraint-aware Visual Programming for Reactive and Proactive Robotic Failure Detection|https://arxiv.org/abs/2412.04455;DexNDM: Closing the Reality Gap for Dexterous In-Hand Rotation via Joint-Wise Neural Dynamics Model|https://arxiv.org/abs/2510.08556;Embodied Arena: A Comprehensive, Unified, and Evolving Evaluation Platform for Embodied AI|https://arxiv.org/abs/2509.15273;Recent Deep Learning in Crowd Behaviour Analysis: A Brief Review|https://arxiv.org/abs/2505.18401;Moderating the Generalization of Score-based Generative Model|https://arxiv.org/abs/2412.07229;Learning Extremely High Density Crowds as Active Matters|https://arxiv.org/abs/2503.12168;Interactive Rendering of Relightable and Animatable Gaussian Avatars|https://arxiv.org/abs/2407.10707;GAPartManip: A Large-scale Part-centric Dataset for Material-Agnostic Articulated Object Manipulation|https://arxiv.org/abs/2411.18276;Pedestrian Intention Prediction via Vision-Language Foundation Models|https://arxiv.org/abs/2507.04141;FoldNet: Learning Generalizable Closed-Loop Policy for Garment Folding via Keypoint-Driven Asset and Demonstration Synthesis|https://arxiv.org/abs/2505.09109;Self-Learning Hyperspectral and Multispectral Image Fusion via Adaptive Residual Guided Subspace Diffusion Model|https://arxiv.org/abs/2505.11800;UrbanVLA: A Vision-Language-Action Model for Urban Micromobility|https://arxiv.org/abs/2510.23576;Extreme Cardiac MRI Analysis under Respiratory Motion: Results of the CMRxMotion Challenge|https://arxiv.org/abs/2507.19165;OctoNav: Towards Generalist Embodied Navigation|https://arxiv.org/abs/2506.09839;PIP-Net: Pedestrian Intention Prediction in the Wild|https://arxiv.org/abs/2402.12810;Dexonomy: Synthesizing All Dexterous Grasp Types in a Grasp Taxonomy|https://arxiv.org/abs/2504.18829;MM-Nav: Multi-View VLA Model for Robust Visual Navigation via Multi-Expert Learning|https://arxiv.org/abs/2510.03142;Adversarially Robust AI-Generated Image Detection for Free: An Information Theoretic Perspective|https://arxiv.org/abs/2505.22604;RealMirror: A Comprehensive, Open-Source Vision-Language-Action Platform for Embodied AI|https://arxiv.org/abs/2509.14687;CMRxRecon2024: A Multi-Modality, Multi-View K-Space Dataset Boosting Universal Machine Learning for Accelerated Cardiac MRI|https://arxiv.org/abs/2406.19043;MobileH2R: Learning Generalizable Human to Mobile Robot Handover Exclusively from Scalable and Diverse Synthetic Data|https://arxiv.org/abs/2501.04595;DyWA: Dynamics-adaptive World Action Model for Generalizable Non-prehensile Manipulation|https://arxiv.org/abs/2503.16806;CL3R: 3D Reconstruction and Contrastive Learning for Enhanced Robotic Manipulation Representations|https://arxiv.org/abs/2507.08262;Towards Robust Probabilistic Modeling on SO(3) via Rotation Laplace Distribution|https://arxiv.org/abs/2305.10465;PC-Diffusion: Aligning Diffusion Models with Human Preferences via Preference Classifier|https://arxiv.org/abs/2511.07806;UASTrack: A Unified Adaptive Selection Framework with Modality-Customization in Single Object Tracking|https://arxiv.org/abs/2502.18220;High-fidelity 3D Object Generation from Single Image with RGBN-Volume Gaussian Reconstruction Model|https://arxiv.org/abs/2504.01512;Unleashing Humanoid Reaching Potential via Real-world-Ready Skill Space|https://arxiv.org/abs/2505.10918;Track Any Motions under Any Disturbances|https://arxiv.org/abs/2509.13833;OmniSpatial: Towards Comprehensive Spatial Reasoning Benchmark for Vision Language Models|https://arxiv.org/abs/2506.03135;AVGGT: Rethinking Global Attention for Accelerating VGGT|https://arxiv.org/abs/2512.02541;DexVLG: Dexterous Vision-Language-Grasp Model at Scale|https://arxiv.org/abs/2507.02747;TASAR: Transfer-based Attack on Skeletal Action Recognition|https://arxiv.org/abs/2409.02483;Embodied Navigation Foundation Model|https://arxiv.org/abs/2509.12129;TrackVLA: Embodied Visual Tracking in the Wild|https://arxiv.org/abs/2505.23189;RoboHanger: Learning Generalizable Robotic Hanger Insertion for Diverse Garments|https://arxiv.org/abs/2412.01083;Uni-NaVid: A Video-based Vision-Language-Action Model for Unifying Embodied Navigation Tasks|https://arxiv.org/abs/2412.06224;Learned Single-Pass Multitasking Perceptual Graphics for Immersive Displays|https://arxiv.org/abs/2408.07836;3D Student Splatting and Scooping|https://arxiv.org/abs/2503.10148;BODex: Scalable and Efficient Robotic Dexterous Grasp Synthesis Using Bilevel Optimization|https://arxiv.org/abs/2412.16490;SoFar: Language-Grounded Orientation Bridges Spatial Reasoning and Object Manipulation|https://arxiv.org/abs/2502.13143;Mimicking-Bench: A Benchmark for Generalizable Humanoid-Scene Interaction Learning via Human Mimicking|https://arxiv.org/abs/2412.17730"
智元机器人联合实验室 (PKU-Agibot),中国,China,北京大学,机器人视觉、物体操作、语义导航、具身自主决策,成立于2024年，由北大与智元机器人联合成立，探索机器人在复杂环境中的自主感知、决策和执行能力。,董豪,"OmniManip, ET-SEED, GarmentLab",116.308264,39.995304,https://agibot-world.cn/,Hao Dong,Peking University,,"AdaManip: Adaptive Articulated Object Manipulation Environments and Policy Learning|https://arxiv.org/abs/2502.11124;PartRM: Modeling Part-Level Dynamics with Large Cross-State Reconstruction Model|https://arxiv.org/abs/2503.19913;SpikeGrasp: A Benchmark for 6-DoF Grasp Pose Detection from Stereo Spike Streams|https://arxiv.org/abs/2510.10602;CheckManual: A New Challenge and Benchmark for Manual-based Appliance Manipulation|https://arxiv.org/abs/2506.09343;Fidelity-Aware Data Composition for Robust Robot Generalization|https://arxiv.org/abs/2509.24797;Adaptive Visuo-Tactile Fusion with Predictive Force Attention for Dexterous Manipulation|https://arxiv.org/abs/2505.13982;RealAppliance: Let High-fidelity Appliance Assets Controllable and Workable as Aligned Real Manuals|https://arxiv.org/abs/2512.00287;DexGarmentLab: Dexterous Garment Manipulation Environment with Generalizable Policy|https://arxiv.org/abs/2505.11032;RoboCOIN: An Open-Sourced Bimanual Robotic Data COllection for INtegrated Manipulation|https://arxiv.org/abs/2511.17441;Communication-Efficient Desire Alignment for Embodied Agent-Human Adaptation|https://arxiv.org/abs/2505.22503;GarmentLab: A Unified Simulation and Benchmark for Garment Manipulation|https://arxiv.org/abs/2411.01200;Recall and Refine: A Simple but Effective Source-free Open-set Domain Adaptation Framework|https://arxiv.org/abs/2411.12558;Unseen Visual Anomaly Generation|https://arxiv.org/abs/2406.01078;OmniManip: Towards General Robotic Manipulation via Object-Centric Interaction Primitives as Spatial Constraints|https://arxiv.org/abs/2501.03841;ClutterDexGrasp: A Sim-to-Real System for General Dexterous Grasping in Cluttered Scenes|https://arxiv.org/abs/2506.14317;GM-MoE: Low-Light Enhancement with Gated-Mechanism Mixture-of-Experts|https://arxiv.org/abs/2503.07417;BiAssemble: Learning Collaborative Affordance for Bimanual Geometric Assembly|https://arxiv.org/abs/2506.06221;ManipGPT: Is Affordance Segmentation by Large Vision Models Enough for Articulated Object Manipulation?|https://arxiv.org/abs/2412.10050;SpikeStereoNet: A Brain-Inspired Framework for Stereo Depth Estimation from Spike Streams|https://arxiv.org/abs/2505.19487;CordViP: Correspondence-based Visuomotor Policy for Dexterous Manipulation in Real-World|https://arxiv.org/abs/2502.08449;Canonical Representation and Force-Based Pretraining of 3D Tactile for Dexterous Visuo-Tactile Policy Learning|https://arxiv.org/abs/2409.17549;InternData-A1: Pioneering High-Fidelity Synthetic Data for Pre-training Generalist Policy|https://arxiv.org/abs/2511.16651;SpatialBot: Precise Spatial Understanding with Vision Language Models|https://arxiv.org/abs/2406.13642;Learning Environment-Aware Affordance for 3D Articulated Object Manipulation under Occlusions|https://arxiv.org/abs/2309.07510;CrayonRobo: Object-Centric Prompt-Driven Vision-Language-Action Model for Robotic Manipulation|https://arxiv.org/abs/2505.02166;NavSpace: How Navigation Agents Follow Spatial Intelligence Instructions|https://arxiv.org/abs/2510.08173;To Trust Or Not To Trust Your Vision-Language Model's Prediction|https://arxiv.org/abs/2505.23745;GraspGF: Learning Score-based Grasping Primitive for Human-assisting Dexterous Grasping|https://arxiv.org/abs/2309.06038;RoboVerse: Towards a Unified Platform, Dataset and Benchmark for Scalable and Generalizable Robot Learning|https://arxiv.org/abs/2504.18904;Towards Robust Multimodal Open-set Test-time Adaptation via Adaptive Entropy-aware Optimization|https://arxiv.org/abs/2501.13924;RwoR: Generating Robot Demonstrations from Human Hand Collection for Policy Learning without Robot|https://arxiv.org/abs/2507.03930;Improving Unsupervised Task-driven Models of Ventral Visual Stream via Relative Position Predictivity|https://arxiv.org/abs/2505.08316;SCANet: Correcting LEGO Assembly Errors with Self-Correct Assembly Network|https://arxiv.org/abs/2403.18195;Imagine2Act: Leveraging Object-Action Motion Consistency from Imagined Goals for Robotic Manipulation|https://arxiv.org/abs/2509.17125;Adaptive Articulated Object Manipulation On The Fly with Foundation Model Reasoning and Part Grounding|https://arxiv.org/abs/2507.18276;Extremely Simple Multimodal Outlier Synthesis for Out-of-Distribution Detection and Segmentation|https://arxiv.org/abs/2505.16985;SplatMesh: Interactive 3D Segmentation and Editing Using Mesh-Based Gaussian Splatting|https://arxiv.org/abs/2312.15856;CorrectNav: Self-Correction Flywheel Empowers Vision-Language-Action Navigation Model|https://arxiv.org/abs/2508.10416;TransDiff: Diffusion-Based Method for Manipulating Transparent Objects Using a Single RGB-D Image|https://arxiv.org/abs/2503.12779;Foundation Feature-Driven Online End-Effector Pose Estimation: A Marker-Free and Learning-Free Approach|https://arxiv.org/abs/2503.14051;DexFlyWheel: A Scalable and Self-improving Data Generation Framework for Dexterous Manipulation|https://arxiv.org/abs/2509.23829;Predictive Inverse Dynamics Models are Scalable Learners for Robotic Manipulation|https://arxiv.org/abs/2412.15109;ET-SEED: Efficient Trajectory-Level SE(3) Equivariant Diffusion Policy|https://arxiv.org/abs/2411.03990;3DWG: 3D Weakly Supervised Visual Grounding via Category and Instance-Level Alignment|https://arxiv.org/abs/2505.01809;Advances in Multimodal Adaptation and Generalization: From Traditional Approaches to Foundation Models|https://arxiv.org/abs/2501.18592;GarmentPile: Point-Level Visual Affordance Guided Retrieval and Adaptation for Cluttered Garments Manipulation|https://arxiv.org/abs/2503.09243;SimLauncher: Launching Sample-Efficient Real-world Robotic Reinforcement Learning via Simulation Pre-training|https://arxiv.org/abs/2507.04452;REGen: Multimodal Retrieval-Embedded Generation for Long-to-Short Video Editing|https://arxiv.org/abs/2505.18880;Adapting Vision-Language Models Without Labels: A Comprehensive Survey|https://arxiv.org/abs/2508.05547;SR3D: Unleashing Single-view 3D Reconstruction for Transparent and Specular Object Grasping|https://arxiv.org/abs/2505.24305"
情感与认知智能机器人实验室 (ACIR),中国,China,北京大学,情感与认知计算、精细表情识别、注意分析,以赋予机器人真正的智能和情感为发展目标，进行情感与认知计算的学术研究。,王韬,"Ground Robots, EventFormer, Diffusion Model",116.308264,39.995304,https://acir.pku.edu.cn/,Tao Wang,Peking University,,From Noisy Labels to Intrinsic Structure: A Geometric-Structural Dual-Guided Framework for Noise-Robust Medical Image Segmentation|https://arxiv.org/abs/2509.02419;Trend-Aware Supervision: On Learning Invariance for Semi-Supervised Facial Action Unit Intensity Estimation|https://arxiv.org/abs/2503.08078;StickMotion: Generating 3D Human Motions by Drawing a Stickman|https://arxiv.org/abs/2503.04829;DAM-VSR: Disentanglement of Appearance and Motion for Video Super-Resolution|https://arxiv.org/abs/2507.01012;IMM-MOT: A Novel 3D Multi-object Tracking Framework with Interacting Multiple Model Filter|https://arxiv.org/abs/2502.09672;Perceptual Taxonomy: Evaluating and Guiding Hierarchical Scene Reasoning in Vision-Language Models|https://arxiv.org/abs/2511.19526;Segmentation Guided Sparse Transformer for Under-Display Camera Image Restoration|https://arxiv.org/abs/2403.05906;JOGS: Joint Optimization of Pose Estimation and 3D Gaussian Splatting|https://arxiv.org/abs/2510.26117;Glass Surface Segmentation with an RGB-D Camera via Weighted Feature Fusion for Service Robots|https://arxiv.org/abs/2508.01639;Deep Face Restoration: A Survey|https://arxiv.org/abs/2211.02831;HiMTok: Learning Hierarchical Mask Tokens for Image Segmentation with Large Multimodal Model|https://arxiv.org/abs/2503.13026;Gemini: A Family of Highly Capable Multimodal Models|https://arxiv.org/abs/2312.11805;Open-Det: An Efficient Learning Framework for Open-Ended Detection|https://arxiv.org/abs/2505.20639;DiffBrush:Just Painting the Art by Your Hands|https://arxiv.org/abs/2502.20904;SemDP: Semantic-level Differential Privacy Protection for Face Datasets|https://arxiv.org/abs/2412.15590;Valley: Video Assistant with Large Language model Enhanced abilitY|https://arxiv.org/abs/2306.07207;CrossCheck-Bench: Diagnosing Compositional Failures in Multimodal Conflict Resolution|https://arxiv.org/abs/2511.21717;ScribbleVS: Scribble-Supervised Medical Image Segmentation via Dynamic Competitive Pseudo Label Selection|https://arxiv.org/abs/2411.10237;Adaptive Convolutional Neural Network for Image Super-resolution|https://arxiv.org/abs/2402.15704;Visual Style Prompt Learning Using Diffusion Models for Blind Face Restoration|https://arxiv.org/abs/2412.21042;Feature Point Extraction for Extra-Affine Image|https://arxiv.org/abs/2503.03479;MaterialMVP: Illumination-Invariant Material Generation via Multi-view PBR Diffusion|https://arxiv.org/abs/2503.10289;Precision Neural Network Quantization via Learnable Adaptive Modules|https://arxiv.org/abs/2504.17263;NTIRE 2025 Challenge on Cross-Domain Few-Shot Object Detection: Methods and Results|https://arxiv.org/abs/2504.10685;Dual Prompt Learning for Adapting Vision-Language Models to Downstream Image-Text Retrieval|https://arxiv.org/abs/2508.04028;Rapid morphology characterization of two-dimensional TMDs and lateral heterostructures based on deep learning|https://arxiv.org/abs/2503.00470;CorrMoE: Mixture of Experts with De-stylization Learning for Cross-Scene and Cross-Domain Correspondence Pruning|https://arxiv.org/abs/2507.11834;DiffAD: A Unified Diffusion Modeling Approach for Autonomous Driving|https://arxiv.org/abs/2503.12170;AtrousMamaba: An Atrous-Window Scanning Visual State Space Model for Remote Sensing Change Detection|https://arxiv.org/abs/2507.16172;Gaussian Herding across Pens: An Optimal Transport Perspective on Global Gaussian Reduction for 3DGS|https://arxiv.org/abs/2506.09534;ALTo: Adaptive-Length Tokenizer for Autoregressive Mask Generation|https://arxiv.org/abs/2505.16495;KPIs 2024 Challenge: Advancing Glomerular Segmentation from Patch- to Slide-Level|https://arxiv.org/abs/2502.07288;Improving Value Estimation Critically Enhances Vanilla Policy Gradient|https://arxiv.org/abs/2505.19247;PromptRR: Diffusion Models as Prompt Generators for Single Image Reflection Removal|https://arxiv.org/abs/2402.02374;Detecting Malicious Concepts Without Image Generation in AIGC|https://arxiv.org/abs/2502.08921;Data-driven RF Tomography via Cross-modal Sensing and Continual Learning|https://arxiv.org/abs/2508.11654;Innovative Quantitative Analysis for Disease Progression Assessment in Familial Cerebral Cavernous Malformations|https://arxiv.org/abs/2403.15803;Astra: Toward General-Purpose Mobile Robots via Hierarchical Multimodal Learning|https://arxiv.org/abs/2506.06205;Memory-Augmented Dual-Decoder Networks for Multi-Class Unsupervised Anomaly Detection|https://arxiv.org/abs/2504.14884
HMI Lab,中国,China,北京大学,具身智能、多模态大模型、高效推理与操作,依托两大国家级平台，推出了RoboMamba多模态大模型，在通用和机器人场景下展现卓越性能。,仉尚航,"Robo Mamba, ManipLLM, LocLLM",116.308264,39.995304,https://acir.pku.edu.cn/,Shanghang Zhang,Peking University,,"MinD: Learning A Dual-System World Model for Real-Time Planning and Implicit Risk Analysis|https://arxiv.org/abs/2506.18897;Can World Models Benefit VLMs for World Dynamics?|https://arxiv.org/abs/2510.00855;Video2Act: A Dual-System Video Diffusion Policy with Robotic Spatio-Motional Modeling|https://arxiv.org/abs/2512.03044;HumanoidVerse: A Versatile Humanoid for Vision-Language Guided Multi-Object Rearrangement|https://arxiv.org/abs/2508.16943;RoboArmGS: High-Quality Robotic Arm Splatting via Bézier Curve Refinement|https://arxiv.org/abs/2511.17961;AffordGrasp: In-Context Affordance Reasoning for Open-Vocabulary Task-Oriented Grasping in Clutter|https://arxiv.org/abs/2503.00778;UniEdit-I: Training-free Image Editing for Unified VLM via Iterative Understanding, Editing and Verifying|https://arxiv.org/abs/2508.03142;Lift3D Foundation Policy: Lifting 2D Large-Scale Pretrained Models for Robust 3D Robotic Manipulation|https://arxiv.org/abs/2411.18623;RoboBrain: A Unified Brain Model for Robotic Manipulation from Abstract to Concrete|https://arxiv.org/abs/2502.21257;EMD: Explicit Motion Modeling for High-Quality Street Gaussian Splatting|https://arxiv.org/abs/2411.15582;StreamKV: Streaming Video Question-Answering with Segment-based KV Cache Retrieval and Compression|https://arxiv.org/abs/2511.07278;From Language to Locomotion: Retargeting-free Humanoid Control via Motion Latent Guidance|https://arxiv.org/abs/2510.14952;URDF-Anything: Constructing Articulated Objects with 3D Multimodal Language Model|https://arxiv.org/abs/2511.00940;SpikePingpong: High-Frequency Spike Vision-based Robot Learning for Precise Striking in Table Tennis Game|https://arxiv.org/abs/2506.06690;MC-LLaVA: Multi-Concept Personalized Vision-Language Model|https://arxiv.org/abs/2503.18854;MC-LLaVA: Multi-Concept Personalized Vision-Language Model|https://arxiv.org/abs/2411.11706;MapNav: A Novel Memory Representation via Annotated Semantic Maps for Vision-and-Language Navigation|https://arxiv.org/abs/2502.13451;Towards a Unified Understanding of Robot Manipulation: A Comprehensive Survey|https://arxiv.org/abs/2510.10903;RoboRefer: Towards Spatial Referring with Reasoning in Vision-Language Models for Robotics|https://arxiv.org/abs/2506.04308;RoboMIND: Benchmark on Multi-embodiment Intelligence Normative Data for Robot Manipulation|https://arxiv.org/abs/2412.13877;3rd Place Solution for VisDA 2021 Challenge -- Universally Domain Adaptive Image Recognition|https://arxiv.org/abs/2110.14240;H2R: A Human-to-Robot Data Augmentation for Robot Pre-training from Videos|https://arxiv.org/abs/2505.11920;GaussianAD: Gaussian-Centric End-to-End Autonomous Driving|https://arxiv.org/abs/2412.10371;PIGEON: VLM-Driven Object Navigation via Points of Interest Selection|https://arxiv.org/abs/2511.13207;M$^{2}$Chat: Empowering VLM for Multimodal LLM Interleaved Text-Image Generation|https://arxiv.org/abs/2311.17963;MotionTrans: Human VR Data Enable Motion-Level Learning for Robotic Manipulation Policies|https://arxiv.org/abs/2509.17759;ManipDreamer: Boosting Robotic Manipulation World Model with Action Tree and Visual Guidance|https://arxiv.org/abs/2504.16464;dVLA: Diffusion Vision-Language-Action Model with Multimodal Chain-of-Thought|https://arxiv.org/abs/2509.25681;Reason-RFT: Reinforcement Fine-Tuning for Visual Reasoning of Vision Language Models|https://arxiv.org/abs/2503.20752;SparseVLM: Visual Token Sparsification for Efficient Vision-Language Model Inference|https://arxiv.org/abs/2410.04417;RoboCOIN: An Open-Sourced Bimanual Robotic Data COllection for INtegrated Manipulation|https://arxiv.org/abs/2511.17441;RoboOS: A Hierarchical Embodied Framework for Cross-Embodiment and Multi-Agent Collaboration|https://arxiv.org/abs/2505.03673;DualVLA: Building a Generalizable Embodied Agent via Partial Decoupling of Reasoning and Action|https://arxiv.org/abs/2511.22134;METIS: Multi-Source Egocentric Training for Integrated Dexterous Vision-Language-Action Model|https://arxiv.org/abs/2511.17366;CordViP: Correspondence-based Visuomotor Policy for Dexterous Manipulation in Real-World|https://arxiv.org/abs/2502.08449;Segment Any Motion in Videos|https://arxiv.org/abs/2503.22268;ManipDreamer3D : Synthesizing Plausible Robotic Manipulation Video with Occupancy-aware 3D Trajectory|https://arxiv.org/abs/2509.05314;BEVUDA++: Geometric-aware Unsupervised Domain Adaptation for Multi-View 3D Object Detection|https://arxiv.org/abs/2509.14151;EmbodiedOcc++: Boosting Embodied 3D Occupancy Prediction with Plane Regularization and Uncertainty Sampler|https://arxiv.org/abs/2504.09540;$NavA^3$: Understanding Any Instruction, Navigating Anywhere, Finding Anything|https://arxiv.org/abs/2508.04598;MMTrail: A Multimodal Trailer Video Dataset with Language and Music Descriptions|https://arxiv.org/abs/2407.20962;MLA: A Multisensory Language-Action Model for Multimodal Understanding and Forecasting in Robotic Manipulation|https://arxiv.org/abs/2509.26642;RoboBrain 2.0 Technical Report|https://arxiv.org/abs/2507.02029;OmniSAT: Compact Action Token, Faster Auto Regression|https://arxiv.org/abs/2510.09667;TIGeR: Tool-Integrated Geometric Reasoning in Vision-Language Models for Robotics|https://arxiv.org/abs/2510.07181;XR-1: Towards Versatile Vision-Language-Action Models via Learning Unified Vision-Motion Representations|https://arxiv.org/abs/2511.02776;Video-CoT: A Comprehensive Dataset for Spatiotemporal Understanding of Videos Based on Chain-of-Thought|https://arxiv.org/abs/2506.08817;WristWorld: Generating Wrist-Views via 4D World Models for Robotic Manipulation|https://arxiv.org/abs/2510.07313;MoLe-VLA: Dynamic Layer-skipping Vision Language Action Model via Mixture-of-Layers for Efficient Robot Manipulation|https://arxiv.org/abs/2503.20384;High-Quality 3D Creation from A Single Image Using Subject-Specific Knowledge Prior|https://arxiv.org/abs/2312.11535;Co$^{3}$Gesture: Towards Coherent Concurrent Co-speech 3D Gesture Generation with Interactive Diffusion|https://arxiv.org/abs/2505.01746;EVA: An Embodied World Model for Future Video Anticipation|https://arxiv.org/abs/2410.15461;A Self-Correcting Vision-Language-Action Model for Fast and Slow System Manipulation|https://arxiv.org/abs/2405.17418;RoboOS-NeXT: A Unified Memory-based Framework for Lifelong, Scalable, and Robust Multi-Robot Collaboration|https://arxiv.org/abs/2510.26536;ManualVLA: A Unified VLA Model for Chain-of-Thought Manual Generation and Robotic Manipulation|https://arxiv.org/abs/2512.02013;CrayonRobo: Object-Centric Prompt-Driven Vision-Language-Action Model for Robotic Manipulation|https://arxiv.org/abs/2505.02166;FastInit: Fast Noise Initialization for Temporally Consistent Video Generation|https://arxiv.org/abs/2506.16119;RwoR: Generating Robot Demonstrations from Human Hand Collection for Policy Learning without Robot|https://arxiv.org/abs/2507.03930;DAG-Plan: Generating Directed Acyclic Dependency Graphs for Dual-Arm Cooperative Planning|https://arxiv.org/abs/2406.09953;4D Visual Pre-training for Robot Learning|https://arxiv.org/abs/2508.17230;RoboMamba: Efficient Vision-Language-Action Model for Robotic Reasoning and Manipulation|https://arxiv.org/abs/2406.04339;BranchGRPO: Stable and Efficient GRPO with Structured Branching in Diffusion Models|https://arxiv.org/abs/2509.06040;GeoDrive: 3D Geometry-Informed Driving World Model with Precise Action Control|https://arxiv.org/abs/2505.22421;MoVE-KD: Knowledge Distillation for VLMs with Mixture of Visual Encoders|https://arxiv.org/abs/2501.01709;AC-DiT: Adaptive Coordination Diffusion Transformer for Mobile Manipulation|https://arxiv.org/abs/2507.01961;WoW: Towards a World omniscient World model Through Embodied Interaction|https://arxiv.org/abs/2509.22642;OmniIndoor3D: Comprehensive Indoor 3D Reconstruction|https://arxiv.org/abs/2505.20610;SimpleVLA-RL: Scaling VLA Training via Reinforcement Learning|https://arxiv.org/abs/2509.09674;SpikeGen: Decoupled ""Rods and Cones"" Visual Representation Processing with Latent Generative Framework|https://arxiv.org/abs/2505.18049;Robobench: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models as Embodied Brain|https://arxiv.org/abs/2510.17801;Loss-Oriented Ranking for Automated Visual Prompting in LVLMs|https://arxiv.org/abs/2506.16112;Learning from Mistakes: Iterative Prompt Relabeling for Text-to-Image Diffusion Model Training|https://arxiv.org/abs/2312.16204;HybridVLA: Collaborative Diffusion and Autoregression in a Unified Vision-Language-Action Model|https://arxiv.org/abs/2503.10631;FastDriveVLA: Efficient End-to-End Driving via Plug-and-Play Reconstruction-based Token Pruning|https://arxiv.org/abs/2507.23318;SCBench: A Sports Commentary Benchmark for Video LLMs|https://arxiv.org/abs/2412.17637;MathSticks: A Benchmark for Visual Symbolic Compositional Reasoning with Matchstick Puzzles|https://arxiv.org/abs/2510.00483;MMG-Vid: Maximizing Marginal Gains at Segment-level and Token-level for Efficient Video LLMs|https://arxiv.org/abs/2508.21044;Can We Generate Images with CoT? Let's Verify and Reinforce Image Generation Step by Step|https://arxiv.org/abs/2501.13926;Draw-and-Understand: Leveraging Visual Prompts to Enable MLLMs to Comprehend What You Want|https://arxiv.org/abs/2403.20271;Beyond Text-Visual Attention: Exploiting Visual Cues for Effective Token Pruning in VLMs|https://arxiv.org/abs/2412.01818;Research Challenges and Progress in the End-to-End V2X Cooperative Autonomous Driving Competition|https://arxiv.org/abs/2507.21610;Fast-in-Slow: A Dual-System Foundation Model Unifying Fast Manipulation within Slow Reasoning|https://arxiv.org/abs/2506.01953;Beyond Attention or Similarity: Maximizing Conditional Diversity for Token Pruning in MLLMs|https://arxiv.org/abs/2506.10967;SliceOcc: Indoor 3D Semantic Occupancy Prediction with Vertical Slice Representation|https://arxiv.org/abs/2501.16684;ChainV: Atomic Visual Hints Make Multimodal Reasoning Shorter and Better|https://arxiv.org/abs/2511.17106;CoCoGesture: Toward Coherent Co-speech 3D Gesture Generation in the Wild|https://arxiv.org/abs/2405.16874;PoseDiff: A Unified Diffusion Model Bridging Robot Pose Estimation and Video-to-Action Control|https://arxiv.org/abs/2509.24591"
BIGAI,中国,China,北京通用人工智能研究院,通用人工智能、通用智能体,致力于在通用人工智能领域开展战略性、前瞻性、基础性科技创新，创造通用智能体。,朱松纯,"首个虚拟人与场景交互动作生成框架, 双足机器人连续跳跃与稳定行走, 空中具身智能机器人CORVUS",116.326759,40.003304,https://www.bigai.ai/,Songchun Zhu,BIGAI,,"V-HUB: A Visual-Centric Humor Understanding Benchmark for Video LLMs|https://arxiv.org/abs/2509.25773;Discovering ""Words"" in Music: Unsupervised Learning of Compositional Sparse Code for Symbolic Music|https://arxiv.org/abs/2509.24603"
具身智能研究中心,中国,China,北京智源研究院,多模态具身大脑、小脑大模型、智能体、端上算力平台,在通用抓取、分级具身大模型系统、端到端具身大模型等方面取得全球领先成果。,王鹤,"GAMMA, MaskClustering, UniDexGrasp",116.326759,40.003304,https://www.baai.ac.cn/,He Wang,Beijing Academy of Artificial Intelligence,,"GraspVLA: a Grasping Foundation Model Pre-trained on Billion-scale Synthetic Action Data|https://arxiv.org/abs/2505.03233;DreamVLA: A Vision-Language-Action Model Dreamed with Comprehensive World Knowledge|https://arxiv.org/abs/2507.04447;TrackVLA++: Unleashing Reasoning and Memory Capabilities in VLA Models for Embodied Visual Tracking|https://arxiv.org/abs/2510.07134;ASGrasp: Generalizable Transparent Object Reconstruction and 6-DoF Grasp Detection from RGB-D Active Stereo Camera|https://arxiv.org/abs/2405.05648;Watch Less, Feel More: Sim-to-Real RL for Generalizable Articulated Object Manipulation via Motion Adaptation and Impedance Control|https://arxiv.org/abs/2502.14457;Text-guided Fine-Grained Video Anomaly Detection|https://arxiv.org/abs/2511.00524;RoboCOIN: An Open-Sourced Bimanual Robotic Data COllection for INtegrated Manipulation|https://arxiv.org/abs/2511.17441;Make a Donut: Hierarchical EMD-Space Planning for Zero-Shot Deformable Manipulation with Tools|https://arxiv.org/abs/2311.02787;Qwen3-Omni Technical Report|https://arxiv.org/abs/2509.17765;FetchBot: Learning Generalizable Object Fetching in Cluttered Scenes via Zero-Shot Sim2Real|https://arxiv.org/abs/2502.17894;FLUX-Makeup: High-Fidelity, Identity-Consistent, and Robust Makeup Transfer via Diffusion Transformer|https://arxiv.org/abs/2508.05069;QuadWBG: Generalizable Quadrupedal Whole-Body Grasping|https://arxiv.org/abs/2411.06782;Large-Scale Multi-Character Interaction Synthesis|https://arxiv.org/abs/2505.14087;Robust Differentiable Collision Detection for General Objects|https://arxiv.org/abs/2511.06267;Code-as-Monitor: Constraint-aware Visual Programming for Reactive and Proactive Robotic Failure Detection|https://arxiv.org/abs/2412.04455;DexNDM: Closing the Reality Gap for Dexterous In-Hand Rotation via Joint-Wise Neural Dynamics Model|https://arxiv.org/abs/2510.08556;Embodied Arena: A Comprehensive, Unified, and Evolving Evaluation Platform for Embodied AI|https://arxiv.org/abs/2509.15273;Recent Deep Learning in Crowd Behaviour Analysis: A Brief Review|https://arxiv.org/abs/2505.18401;Moderating the Generalization of Score-based Generative Model|https://arxiv.org/abs/2412.07229;Learning Extremely High Density Crowds as Active Matters|https://arxiv.org/abs/2503.12168;Interactive Rendering of Relightable and Animatable Gaussian Avatars|https://arxiv.org/abs/2407.10707;GAPartManip: A Large-scale Part-centric Dataset for Material-Agnostic Articulated Object Manipulation|https://arxiv.org/abs/2411.18276;Pedestrian Intention Prediction via Vision-Language Foundation Models|https://arxiv.org/abs/2507.04141;FoldNet: Learning Generalizable Closed-Loop Policy for Garment Folding via Keypoint-Driven Asset and Demonstration Synthesis|https://arxiv.org/abs/2505.09109;Self-Learning Hyperspectral and Multispectral Image Fusion via Adaptive Residual Guided Subspace Diffusion Model|https://arxiv.org/abs/2505.11800;UrbanVLA: A Vision-Language-Action Model for Urban Micromobility|https://arxiv.org/abs/2510.23576;Extreme Cardiac MRI Analysis under Respiratory Motion: Results of the CMRxMotion Challenge|https://arxiv.org/abs/2507.19165;OctoNav: Towards Generalist Embodied Navigation|https://arxiv.org/abs/2506.09839;PIP-Net: Pedestrian Intention Prediction in the Wild|https://arxiv.org/abs/2402.12810;Dexonomy: Synthesizing All Dexterous Grasp Types in a Grasp Taxonomy|https://arxiv.org/abs/2504.18829;MM-Nav: Multi-View VLA Model for Robust Visual Navigation via Multi-Expert Learning|https://arxiv.org/abs/2510.03142;Adversarially Robust AI-Generated Image Detection for Free: An Information Theoretic Perspective|https://arxiv.org/abs/2505.22604;RealMirror: A Comprehensive, Open-Source Vision-Language-Action Platform for Embodied AI|https://arxiv.org/abs/2509.14687;CMRxRecon2024: A Multi-Modality, Multi-View K-Space Dataset Boosting Universal Machine Learning for Accelerated Cardiac MRI|https://arxiv.org/abs/2406.19043;MobileH2R: Learning Generalizable Human to Mobile Robot Handover Exclusively from Scalable and Diverse Synthetic Data|https://arxiv.org/abs/2501.04595;DyWA: Dynamics-adaptive World Action Model for Generalizable Non-prehensile Manipulation|https://arxiv.org/abs/2503.16806;CL3R: 3D Reconstruction and Contrastive Learning for Enhanced Robotic Manipulation Representations|https://arxiv.org/abs/2507.08262;Towards Robust Probabilistic Modeling on SO(3) via Rotation Laplace Distribution|https://arxiv.org/abs/2305.10465;PC-Diffusion: Aligning Diffusion Models with Human Preferences via Preference Classifier|https://arxiv.org/abs/2511.07806;UASTrack: A Unified Adaptive Selection Framework with Modality-Customization in Single Object Tracking|https://arxiv.org/abs/2502.18220;High-fidelity 3D Object Generation from Single Image with RGBN-Volume Gaussian Reconstruction Model|https://arxiv.org/abs/2504.01512;Unleashing Humanoid Reaching Potential via Real-world-Ready Skill Space|https://arxiv.org/abs/2505.10918;Track Any Motions under Any Disturbances|https://arxiv.org/abs/2509.13833;OmniSpatial: Towards Comprehensive Spatial Reasoning Benchmark for Vision Language Models|https://arxiv.org/abs/2506.03135;AVGGT: Rethinking Global Attention for Accelerating VGGT|https://arxiv.org/abs/2512.02541;DexVLG: Dexterous Vision-Language-Grasp Model at Scale|https://arxiv.org/abs/2507.02747;TASAR: Transfer-based Attack on Skeletal Action Recognition|https://arxiv.org/abs/2409.02483;Embodied Navigation Foundation Model|https://arxiv.org/abs/2509.12129;TrackVLA: Embodied Visual Tracking in the Wild|https://arxiv.org/abs/2505.23189;RoboHanger: Learning Generalizable Robotic Hanger Insertion for Diverse Garments|https://arxiv.org/abs/2412.01083;Uni-NaVid: A Video-based Vision-Language-Action Model for Unifying Embodied Navigation Tasks|https://arxiv.org/abs/2412.06224;Learned Single-Pass Multitasking Perceptual Graphics for Immersive Displays|https://arxiv.org/abs/2408.07836;3D Student Splatting and Scooping|https://arxiv.org/abs/2503.10148;BODex: Scalable and Efficient Robotic Dexterous Grasp Synthesis Using Bilevel Optimization|https://arxiv.org/abs/2412.16490;SoFar: Language-Grounded Orientation Bridges Spatial Reasoning and Object Manipulation|https://arxiv.org/abs/2502.13143;Mimicking-Bench: A Benchmark for Generalizable Humanoid-Scene Interaction Learning via Human Mimicking|https://arxiv.org/abs/2412.17730"
具身智能多模态大模型中心,中国,China,北京智源研究院,具身智能、多模态大模型,智源研究院在具身智能和多模态技术领域的重要研究平台。,仉尚航,"Emu, vid2vid-zero, AltCLIP",116.326759,40.003304,https://www.baai.ac.cn/,Shanghang Zhang,Beijing Academy of Artificial Intelligence,,"MinD: Learning A Dual-System World Model for Real-Time Planning and Implicit Risk Analysis|https://arxiv.org/abs/2506.18897;Can World Models Benefit VLMs for World Dynamics?|https://arxiv.org/abs/2510.00855;Video2Act: A Dual-System Video Diffusion Policy with Robotic Spatio-Motional Modeling|https://arxiv.org/abs/2512.03044;HumanoidVerse: A Versatile Humanoid for Vision-Language Guided Multi-Object Rearrangement|https://arxiv.org/abs/2508.16943;RoboArmGS: High-Quality Robotic Arm Splatting via Bézier Curve Refinement|https://arxiv.org/abs/2511.17961;AffordGrasp: In-Context Affordance Reasoning for Open-Vocabulary Task-Oriented Grasping in Clutter|https://arxiv.org/abs/2503.00778;UniEdit-I: Training-free Image Editing for Unified VLM via Iterative Understanding, Editing and Verifying|https://arxiv.org/abs/2508.03142;Lift3D Foundation Policy: Lifting 2D Large-Scale Pretrained Models for Robust 3D Robotic Manipulation|https://arxiv.org/abs/2411.18623;RoboBrain: A Unified Brain Model for Robotic Manipulation from Abstract to Concrete|https://arxiv.org/abs/2502.21257;EMD: Explicit Motion Modeling for High-Quality Street Gaussian Splatting|https://arxiv.org/abs/2411.15582;StreamKV: Streaming Video Question-Answering with Segment-based KV Cache Retrieval and Compression|https://arxiv.org/abs/2511.07278;From Language to Locomotion: Retargeting-free Humanoid Control via Motion Latent Guidance|https://arxiv.org/abs/2510.14952;URDF-Anything: Constructing Articulated Objects with 3D Multimodal Language Model|https://arxiv.org/abs/2511.00940;SpikePingpong: High-Frequency Spike Vision-based Robot Learning for Precise Striking in Table Tennis Game|https://arxiv.org/abs/2506.06690;MC-LLaVA: Multi-Concept Personalized Vision-Language Model|https://arxiv.org/abs/2503.18854;MC-LLaVA: Multi-Concept Personalized Vision-Language Model|https://arxiv.org/abs/2411.11706;MapNav: A Novel Memory Representation via Annotated Semantic Maps for Vision-and-Language Navigation|https://arxiv.org/abs/2502.13451;Towards a Unified Understanding of Robot Manipulation: A Comprehensive Survey|https://arxiv.org/abs/2510.10903;RoboRefer: Towards Spatial Referring with Reasoning in Vision-Language Models for Robotics|https://arxiv.org/abs/2506.04308;RoboMIND: Benchmark on Multi-embodiment Intelligence Normative Data for Robot Manipulation|https://arxiv.org/abs/2412.13877;3rd Place Solution for VisDA 2021 Challenge -- Universally Domain Adaptive Image Recognition|https://arxiv.org/abs/2110.14240;H2R: A Human-to-Robot Data Augmentation for Robot Pre-training from Videos|https://arxiv.org/abs/2505.11920;GaussianAD: Gaussian-Centric End-to-End Autonomous Driving|https://arxiv.org/abs/2412.10371;PIGEON: VLM-Driven Object Navigation via Points of Interest Selection|https://arxiv.org/abs/2511.13207;M$^{2}$Chat: Empowering VLM for Multimodal LLM Interleaved Text-Image Generation|https://arxiv.org/abs/2311.17963;MotionTrans: Human VR Data Enable Motion-Level Learning for Robotic Manipulation Policies|https://arxiv.org/abs/2509.17759;ManipDreamer: Boosting Robotic Manipulation World Model with Action Tree and Visual Guidance|https://arxiv.org/abs/2504.16464;dVLA: Diffusion Vision-Language-Action Model with Multimodal Chain-of-Thought|https://arxiv.org/abs/2509.25681;Reason-RFT: Reinforcement Fine-Tuning for Visual Reasoning of Vision Language Models|https://arxiv.org/abs/2503.20752;SparseVLM: Visual Token Sparsification for Efficient Vision-Language Model Inference|https://arxiv.org/abs/2410.04417;RoboCOIN: An Open-Sourced Bimanual Robotic Data COllection for INtegrated Manipulation|https://arxiv.org/abs/2511.17441;RoboOS: A Hierarchical Embodied Framework for Cross-Embodiment and Multi-Agent Collaboration|https://arxiv.org/abs/2505.03673;DualVLA: Building a Generalizable Embodied Agent via Partial Decoupling of Reasoning and Action|https://arxiv.org/abs/2511.22134;METIS: Multi-Source Egocentric Training for Integrated Dexterous Vision-Language-Action Model|https://arxiv.org/abs/2511.17366;CordViP: Correspondence-based Visuomotor Policy for Dexterous Manipulation in Real-World|https://arxiv.org/abs/2502.08449;Segment Any Motion in Videos|https://arxiv.org/abs/2503.22268;ManipDreamer3D : Synthesizing Plausible Robotic Manipulation Video with Occupancy-aware 3D Trajectory|https://arxiv.org/abs/2509.05314;BEVUDA++: Geometric-aware Unsupervised Domain Adaptation for Multi-View 3D Object Detection|https://arxiv.org/abs/2509.14151;EmbodiedOcc++: Boosting Embodied 3D Occupancy Prediction with Plane Regularization and Uncertainty Sampler|https://arxiv.org/abs/2504.09540;$NavA^3$: Understanding Any Instruction, Navigating Anywhere, Finding Anything|https://arxiv.org/abs/2508.04598;MMTrail: A Multimodal Trailer Video Dataset with Language and Music Descriptions|https://arxiv.org/abs/2407.20962;MLA: A Multisensory Language-Action Model for Multimodal Understanding and Forecasting in Robotic Manipulation|https://arxiv.org/abs/2509.26642;RoboBrain 2.0 Technical Report|https://arxiv.org/abs/2507.02029;OmniSAT: Compact Action Token, Faster Auto Regression|https://arxiv.org/abs/2510.09667;TIGeR: Tool-Integrated Geometric Reasoning in Vision-Language Models for Robotics|https://arxiv.org/abs/2510.07181;XR-1: Towards Versatile Vision-Language-Action Models via Learning Unified Vision-Motion Representations|https://arxiv.org/abs/2511.02776;Video-CoT: A Comprehensive Dataset for Spatiotemporal Understanding of Videos Based on Chain-of-Thought|https://arxiv.org/abs/2506.08817;WristWorld: Generating Wrist-Views via 4D World Models for Robotic Manipulation|https://arxiv.org/abs/2510.07313;MoLe-VLA: Dynamic Layer-skipping Vision Language Action Model via Mixture-of-Layers for Efficient Robot Manipulation|https://arxiv.org/abs/2503.20384;High-Quality 3D Creation from A Single Image Using Subject-Specific Knowledge Prior|https://arxiv.org/abs/2312.11535;Co$^{3}$Gesture: Towards Coherent Concurrent Co-speech 3D Gesture Generation with Interactive Diffusion|https://arxiv.org/abs/2505.01746;EVA: An Embodied World Model for Future Video Anticipation|https://arxiv.org/abs/2410.15461;A Self-Correcting Vision-Language-Action Model for Fast and Slow System Manipulation|https://arxiv.org/abs/2405.17418;RoboOS-NeXT: A Unified Memory-based Framework for Lifelong, Scalable, and Robust Multi-Robot Collaboration|https://arxiv.org/abs/2510.26536;ManualVLA: A Unified VLA Model for Chain-of-Thought Manual Generation and Robotic Manipulation|https://arxiv.org/abs/2512.02013;CrayonRobo: Object-Centric Prompt-Driven Vision-Language-Action Model for Robotic Manipulation|https://arxiv.org/abs/2505.02166;FastInit: Fast Noise Initialization for Temporally Consistent Video Generation|https://arxiv.org/abs/2506.16119;RwoR: Generating Robot Demonstrations from Human Hand Collection for Policy Learning without Robot|https://arxiv.org/abs/2507.03930;DAG-Plan: Generating Directed Acyclic Dependency Graphs for Dual-Arm Cooperative Planning|https://arxiv.org/abs/2406.09953;4D Visual Pre-training for Robot Learning|https://arxiv.org/abs/2508.17230;RoboMamba: Efficient Vision-Language-Action Model for Robotic Reasoning and Manipulation|https://arxiv.org/abs/2406.04339;BranchGRPO: Stable and Efficient GRPO with Structured Branching in Diffusion Models|https://arxiv.org/abs/2509.06040;GeoDrive: 3D Geometry-Informed Driving World Model with Precise Action Control|https://arxiv.org/abs/2505.22421;MoVE-KD: Knowledge Distillation for VLMs with Mixture of Visual Encoders|https://arxiv.org/abs/2501.01709;AC-DiT: Adaptive Coordination Diffusion Transformer for Mobile Manipulation|https://arxiv.org/abs/2507.01961;WoW: Towards a World omniscient World model Through Embodied Interaction|https://arxiv.org/abs/2509.22642;OmniIndoor3D: Comprehensive Indoor 3D Reconstruction|https://arxiv.org/abs/2505.20610;SimpleVLA-RL: Scaling VLA Training via Reinforcement Learning|https://arxiv.org/abs/2509.09674;SpikeGen: Decoupled ""Rods and Cones"" Visual Representation Processing with Latent Generative Framework|https://arxiv.org/abs/2505.18049;Robobench: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models as Embodied Brain|https://arxiv.org/abs/2510.17801;Loss-Oriented Ranking for Automated Visual Prompting in LVLMs|https://arxiv.org/abs/2506.16112;Learning from Mistakes: Iterative Prompt Relabeling for Text-to-Image Diffusion Model Training|https://arxiv.org/abs/2312.16204;HybridVLA: Collaborative Diffusion and Autoregression in a Unified Vision-Language-Action Model|https://arxiv.org/abs/2503.10631;FastDriveVLA: Efficient End-to-End Driving via Plug-and-Play Reconstruction-based Token Pruning|https://arxiv.org/abs/2507.23318;SCBench: A Sports Commentary Benchmark for Video LLMs|https://arxiv.org/abs/2412.17637;MathSticks: A Benchmark for Visual Symbolic Compositional Reasoning with Matchstick Puzzles|https://arxiv.org/abs/2510.00483;MMG-Vid: Maximizing Marginal Gains at Segment-level and Token-level for Efficient Video LLMs|https://arxiv.org/abs/2508.21044;Can We Generate Images with CoT? Let's Verify and Reinforce Image Generation Step by Step|https://arxiv.org/abs/2501.13926;Draw-and-Understand: Leveraging Visual Prompts to Enable MLLMs to Comprehend What You Want|https://arxiv.org/abs/2403.20271;Beyond Text-Visual Attention: Exploiting Visual Cues for Effective Token Pruning in VLMs|https://arxiv.org/abs/2412.01818;Research Challenges and Progress in the End-to-End V2X Cooperative Autonomous Driving Competition|https://arxiv.org/abs/2507.21610;Fast-in-Slow: A Dual-System Foundation Model Unifying Fast Manipulation within Slow Reasoning|https://arxiv.org/abs/2506.01953;Beyond Attention or Similarity: Maximizing Conditional Diversity for Token Pruning in MLLMs|https://arxiv.org/abs/2506.10967;SliceOcc: Indoor 3D Semantic Occupancy Prediction with Vertical Slice Representation|https://arxiv.org/abs/2501.16684;ChainV: Atomic Visual Hints Make Multimodal Reasoning Shorter and Better|https://arxiv.org/abs/2511.17106;CoCoGesture: Toward Coherent Co-speech 3D Gesture Generation in the Wild|https://arxiv.org/abs/2405.16874;PoseDiff: A Unified Diffusion Model Bridging Robot Pose Estimation and Video-to-Action Control|https://arxiv.org/abs/2509.24591"
通用机器人、自动化、传感和感知实验室 (GRASP),美国,China,宾夕法尼亚大学,视觉、感知、控制系统、自动化、机器学习,成立于 1979 年，是一家一流的机器人孵化器，专注于机器人领域的基础研究。,Vijay Kumar,"POCR, Collaborative robots, Optimal Scene Graph Planning",-75.189722,39.952222,https://www.grasp.upenn.edu/,Vijay Kumar,University of Pennsylvania,,"3D Active Metric-Semantic SLAM|https://arxiv.org/abs/2309.06950;ADMM-MCBF-LCA: A Layered Control Architecture for Safe Real-Time Navigation|https://arxiv.org/abs/2503.02208;MAST: Multi-Agent Spatial Transformer for Learning to Collaborate|https://arxiv.org/abs/2509.17195;Heterogeneous Robot Collaboration in Unstructured Environments with Grounded Generative Intelligence|https://arxiv.org/abs/2510.26915;Symskill: Symbol and Skill Co-Invention for Data-Efficient and Real-Time Long-Horizon Manipulation|https://arxiv.org/abs/2510.01661;Don't Yell at Your Robot: Physical Correction as the Collaborative Interface for Language Model Powered Robots|https://arxiv.org/abs/2412.12602;TACO: Trajectory-Aware Controller Optimization for Quadrotors|https://arxiv.org/abs/2511.02060;Online Multi-Robot Coordination and Cooperation with Task Precedence Relationships|https://arxiv.org/abs/2509.15052;KoopMotion: Learning Almost Divergence Free Koopman Flow Fields for Motion Planning|https://arxiv.org/abs/2509.09074;MIGHTY: Hermite Spline-based Efficient Trajectory Planning|https://arxiv.org/abs/2511.10822;SPAR: Scalable LLM-based PDDL Domain Generation for Aerial Robotics|https://arxiv.org/abs/2509.13691;RT-GuIDE: Real-Time Gaussian Splatting for Information-Driven Exploration|https://arxiv.org/abs/2409.18122;Safe Interval Motion Planning for Quadrotors in Dynamic Environments|https://arxiv.org/abs/2409.10647;Sequence Modeling for Time-Optimal Quadrotor Trajectory Optimization with Sampling-based Robustness Analysis|https://arxiv.org/abs/2506.13915;DroneFL: Federated Learning for Multi-UAV Visual Target Tracking|https://arxiv.org/abs/2509.21523;Hierarchical LLMs In-the-Loop Optimization for Real-Time Multi-Robot Target Tracking under Unknown Hazards|https://arxiv.org/abs/2409.12274;Gaussian Splatting as a Unified Representation for Autonomy in Unstructured Environments|https://arxiv.org/abs/2505.11794;Vision Transformers for End-to-End Vision-Based Quadrotor Obstacle Avoidance|https://arxiv.org/abs/2405.10391;Deploying Foundation Model-Enabled Air and Ground Robots in the Field: Challenges and Opportunities|https://arxiv.org/abs/2505.09477;Estimating the Diameter at Breast Height of Trees in a Forest With a Single 360 Camera|https://arxiv.org/abs/2505.03093;Compositional Coordination for Multi-Robot Teams with Large Language Models|https://arxiv.org/abs/2507.16068;A Scalable Multi-Robot Framework for Decentralized and Asynchronous Perception-Action-Communication Loops|https://arxiv.org/abs/2309.10164;Quad-LCD: Layered Control Decomposition Enables Actuator-Feasible Quadrotor Trajectory Planning|https://arxiv.org/abs/2505.10228;Towards Optimizing a Convex Cover of Collision-Free Space for Trajectory Generation|https://arxiv.org/abs/2406.09631;Leveling the Playing Field: Carefully Comparing Classical and Learned Controllers for Quadrotor Trajectory Tracking|https://arxiv.org/abs/2506.17832;An Active Perception Game for Robust Information Gathering|https://arxiv.org/abs/2404.00769;Real-Time Glass Detection and Reprojection using Sensor Fusion Onboard Aerial Robots|https://arxiv.org/abs/2510.06518;SPINE: Online Semantic Planning for Missions with Incomplete Natural Language Specifications in Unstructured Environments|https://arxiv.org/abs/2410.03035;SlideSLAM: Sparse, Lightweight, Decentralized Metric-Semantic SLAM for Multi-Robot Navigation|https://arxiv.org/abs/2406.17249;Multi-Robot Target Tracking with Sensing and Communication Danger Zones|https://arxiv.org/abs/2404.07880;Leveraging Symmetry to Accelerate Learning of Trajectory Tracking Controllers for Free-Flying Robotic Systems|https://arxiv.org/abs/2409.11238;Safety Guardrails for LLM-Enabled Robots|https://arxiv.org/abs/2503.07885;A Roadmap for Climate-Relevant Robotics Research|https://arxiv.org/abs/2507.11623;Tactile Displays Driven by Projected Light|https://arxiv.org/abs/2410.05494;Air-Ground Collaboration for Language-Specified Missions in Unknown Environments|https://arxiv.org/abs/2505.09108;LPAC: Learnable Perception-Action-Communication Loops with Applications to Coverage Control|https://arxiv.org/abs/2401.04855;ATLAS Navigator: Active Task-driven LAnguage-embedded Gaussian Splatting|https://arxiv.org/abs/2502.20386;HALO: High-Altitude Language-Conditioned Monocular Aerial Exploration and Navigation|https://arxiv.org/abs/2511.17497;Resilient Multi-Robot Target Tracking with Sensing and Communication Danger Zones|https://arxiv.org/abs/2409.11230;Spatio-Temporal Metric-Semantic Mapping for Persistent Orchard Monitoring: Method and Dataset|https://arxiv.org/abs/2409.19786;PIP-LLM: Integrating PDDL-Integer Programming with LLMs for Coordinating Multi-Robot Teams Using Natural Language|https://arxiv.org/abs/2510.22784;Robotic Monitoring of Colorimetric Leaf Sensors for Precision Agriculture|https://arxiv.org/abs/2505.13916;Distilling On-device Language Models for Robot Planning with Minimal Human Intervention|https://arxiv.org/abs/2506.17486"
Multi-Scale Embodied Intelligence Lab,英国,China,帝国理工学院,多尺度具身智能、医疗机器人、多模态感知、灵巧操作,由Dandan Zhang博士领导，将机器学习与机器人技术相结合，主要针对医疗应用领域。,Dandan Zhang,"Towards the New Generation of Smart Homecare with IoHIRT, HuBotVerse, ViTacTip",-0.17636,51.49853,https://www.imperial.ac.uk/multi-scale-embodied-intelligence/,Dandan Zhang,Imperial College London,,MagicGripper: A Multimodal Sensor-Integrated Gripper for Contact-Rich Robotic Manipulation|https://arxiv.org/abs/2505.24382;Physics-Informed Machine Learning with Adaptive Grids for Optical Microrobot Depth Estimation|https://arxiv.org/abs/2509.02343;Rethinking Bimanual Robotic Manipulation: Learning with Decoupled Interaction Framework|https://arxiv.org/abs/2503.09186;Design and Benchmarking of A Multi-Modality Sensor for Robotic Manipulation with GAN-Based Cross-Modality Interpretation|https://arxiv.org/abs/2501.02303;Coarse-to-Fine Learning for Multi-Pipette Localisation in Robot-Assisted In Vivo Patch-Clamp|https://arxiv.org/abs/2504.01044;TypeTele: Releasing Dexterity in Teleoperation by Dexterous Manipulation Types|https://arxiv.org/abs/2507.01857;Interactive OT Gym: A Reinforcement Learning-Based Interactive Optical tweezer (OT)-Driven Microrobotics Simulation Platform|https://arxiv.org/abs/2505.20751;Deep Reinforcement Learning-Based Semi-Autonomous Control for Magnetic Micro-robot Navigation with Immersive Manipulation|https://arxiv.org/abs/2503.06359;Physics-Informed Machine Learning for Efficient Sim-to-Real Data Augmentation in Micro-Object Pose Estimation|https://arxiv.org/abs/2511.16494;A Dataset and Benchmarks for Deep Learning-Based Optical Microrobot Pose and Depth Perception|https://arxiv.org/abs/2505.18303;Multi-Agent Systems for Robotic Autonomy with LLMs|https://arxiv.org/abs/2505.05762;TacEva: A Performance Evaluation Framework For Vision-Based Tactile Sensors|https://arxiv.org/abs/2509.19037;SARL: Spatially-Aware Self-Supervised Representation Learning for Visuo-Tactile Perception|https://arxiv.org/abs/2512.01908;Research Challenges and Progress in the End-to-End V2X Cooperative Autonomous Driving Competition|https://arxiv.org/abs/2507.21610
智能人机交互实验室 (MemX),中国,China,复旦大学,"可穿戴AGI, Q-智能体, 具身智能",定义了大语言模型与智能可穿戴技术相融合的人机交互新范式，开发了首个智能眼镜与LLM结合的可穿戴AGI。,"尚笠, Robert P. Dick, 杨帆等","Can Large Language Models Be Good Companions?, Train Faster, Perform Better, Compositional abilities emerge multiplicatively",121.306666,31.297777,https://memx.life/#/,Li Shang; Robert P. Dick; Fan Yang,Fudan University,,"Hunyuan3D 2.0: Scaling Diffusion Models for High Resolution Textured 3D Assets Generation|https://arxiv.org/abs/2501.12202;NTIRE 2025 Challenge on Day and Night Raindrop Removal for Dual-Focused Images: Methods and Results|https://arxiv.org/abs/2504.12711;FieldWorkArena: Agentic AI Benchmark for Real Field Work Tasks|https://arxiv.org/abs/2505.19662;Solving Token Gradient Conflict in Mixture-of-Experts for Large Vision-Language Model|https://arxiv.org/abs/2406.19905;TaskGalaxy: Scaling Multi-modal Instruction Fine-tuning with Tens of Thousands Vision Task Types|https://arxiv.org/abs/2502.09925;Puppeteer: Rig and Animate Your 3D Models|https://arxiv.org/abs/2508.10898;Imperative Learning: A Self-supervised Neuro-Symbolic Learning Framework for Robot Autonomy|https://arxiv.org/abs/2406.16087;UniFucGrasp: Human-Hand-Inspired Unified Functional Grasp Annotation Strategy and Dataset for Diverse Dexterous Hands|https://arxiv.org/abs/2508.03339;Diffusing Trajectory Optimization Problems for Recovery During Multi-Finger Manipulation|https://arxiv.org/abs/2510.07030;iMOVE: Instance-Motion-Aware Video Understanding|https://arxiv.org/abs/2502.11594;iA*: Imperative Learning-based A* Search for Path Planning|https://arxiv.org/abs/2403.15870;FilmSceneDesigner: Chaining Set Design for Procedural Film Scene Generation|https://arxiv.org/abs/2511.19137;MoSAiC: Multi-Modal Multi-Label Supervision-Aware Contrastive Learning for Remote Sensing|https://arxiv.org/abs/2507.08683;iPlanner: Imperative Path Planning|https://arxiv.org/abs/2302.11434;Kwai Keye-VL 1.5 Technical Report|https://arxiv.org/abs/2509.01563;YOWO: You Only Walk Once to Jointly Map An Indoor Scene and Register Ceiling-mounted Cameras|https://arxiv.org/abs/2511.16521;VIIS: Visible and Infrared Information Synthesis for Severe Low-light Image Enhancement|https://arxiv.org/abs/2412.13655;HEIE: MLLM-Based Hierarchical Explainable AIGC Image Implausibility Evaluator|https://arxiv.org/abs/2411.17261;Magic-Boost: Boost 3D Generation with Multi-View Conditioned Diffusion|https://arxiv.org/abs/2404.06429;R1-Reward: Training Multimodal Reward Model Through Stable Reinforcement Learning|https://arxiv.org/abs/2505.02835;6D Pose Estimation on Spoons and Hands|https://arxiv.org/abs/2505.02335;Vision-R1: Evolving Human-Free Alignment in Large Vision-Language Models via Vision-Guided Reinforcement Learning|https://arxiv.org/abs/2503.18013;CoMM: A Coherent Interleaved Image-Text Dataset for Multimodal Understanding and Generation|https://arxiv.org/abs/2406.10462;MathScape: Benchmarking Multimodal Large Language Models in Real-World Mathematical Contexts|https://arxiv.org/abs/2408.07543;Detection and Geographic Localization of Natural Objects in the Wild: A Case Study on Palms|https://arxiv.org/abs/2502.13023;Personalized 3D Myocardial Infarct Geometry Reconstruction from Cine MRI with Explicit Cardiac Motion Modeling|https://arxiv.org/abs/2507.15194;Seed3D 1.0: From Images to High-Fidelity Simulation-Ready 3D Assets|https://arxiv.org/abs/2510.19944;GLM-4.5V and GLM-4.1V-Thinking: Towards Versatile Multimodal Reasoning with Scalable Reinforcement Learning|https://arxiv.org/abs/2507.01006;MM-Verify: Enhancing Multimodal Reasoning with Chain-of-Thought Verification|https://arxiv.org/abs/2502.13383;LLMI3D: MLLM-based 3D Perception from a Single 2D Image|https://arxiv.org/abs/2408.07422;Understand, Think, and Answer: Advancing Visual Reasoning with Large Multimodal Models|https://arxiv.org/abs/2505.20753;AnoF-Diff: One-Step Diffusion-Based Anomaly Detection for Forceful Tool Use|https://arxiv.org/abs/2509.15153;Exo2Ego: Exocentric Knowledge Guided MLLM for Egocentric Video Understanding|https://arxiv.org/abs/2503.09143;MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding|https://arxiv.org/abs/2512.02906;MM-RLHF: The Next Step Forward in Multimodal LLM Alignment|https://arxiv.org/abs/2502.10391;Textual and Visual Prompt Fusion for Image Editing via Step-Wise Alignment|https://arxiv.org/abs/2308.15854;ViSA: 3D-Aware Video Shading for Real-Time Upper-Body Avatar Creation|https://arxiv.org/abs/2512.07720;DSU-Net:An Improved U-Net Model Based on DINOv2 and SAM2 with Multi-scale Cross-model Feature Enhancement|https://arxiv.org/abs/2503.21187;Multi-Keypoint Affordance Representation for Functional Dexterous Grasping|https://arxiv.org/abs/2502.20018;Leveraging Neural Descriptor Fields for Learning Contact-Aware Dynamic Recovery|https://arxiv.org/abs/2510.14768;Unsupervised Discovery of Long-Term Spatiotemporal Periodic Workflows in Human Activities|https://arxiv.org/abs/2511.14945;TartanGround: A Large-Scale Dataset for Ground Robot Perception and Navigation|https://arxiv.org/abs/2505.10696;Accelerating the Evolution of Personalized Automated Lane Change through Lesson Learning|https://arxiv.org/abs/2405.07543;Integrating Learning-Based Manipulation and Physics-Based Locomotion for Whole-Body Badminton Robot Control|https://arxiv.org/abs/2504.17771;DirectSwap: Mask-Free Cross-Identity Training and Benchmarking for Expression-Consistent Video Head Swapping|https://arxiv.org/abs/2512.09417;Learning Granularity-Aware Affordances from Human-Object Interaction for Tool-Based Functional Dexterous Grasping|https://arxiv.org/abs/2407.00614;Compression then Matching: An Efficient Pre-training Paradigm for Multimodal Embedding|https://arxiv.org/abs/2511.08480;Thyme: Think Beyond Images|https://arxiv.org/abs/2508.11630;Hunyuan3D 2.5: Towards High-Fidelity 3D Assets Generation with Ultimate Details|https://arxiv.org/abs/2506.16504;Baichuan-Omni Technical Report|https://arxiv.org/abs/2410.08565;Grounding-MD: Grounded Video-language Pre-training for Open-World Moment Detection|https://arxiv.org/abs/2504.14553;VQualA 2025 Challenge on Face Image Quality Assessment: Methods and Results|https://arxiv.org/abs/2508.18445;LiveStar: Live Streaming Assistant for Real-World Online Video Understanding|https://arxiv.org/abs/2511.05299;Griffon v2: Advancing Multimodal Perception with High-Resolution Scaling and Visual-Language Co-Referring|https://arxiv.org/abs/2403.09333;StreamingCoT: A Dataset for Temporal Dynamics and Multimodal Chain-of-Thought Reasoning in Streaming VideoQA|https://arxiv.org/abs/2510.25332;Long-Tailed Distribution-Aware Router For Mixture-of-Experts in Large Vision-Language Model|https://arxiv.org/abs/2507.01351;Hunyuan3D 1.0: A Unified Framework for Text-to-3D and Image-to-3D Generation|https://arxiv.org/abs/2411.02293;MagicArticulate: Make Your 3D Models Articulation-Ready|https://arxiv.org/abs/2502.12135;Gemini: A Family of Highly Capable Multimodal Models|https://arxiv.org/abs/2312.11805;Opportunistic Osteoporosis Diagnosis via Texture-Preserving Self-Supervision, Mixture of Experts and Multi-Task Integration|https://arxiv.org/abs/2506.20282;Breaking the Stealth-Potency Trade-off in Clean-Image Backdoors with Generative Trigger Optimization|https://arxiv.org/abs/2511.07210;SVBench: A Benchmark with Temporal Multi-Turn Dialogues for Streaming Video Understanding|https://arxiv.org/abs/2502.10810;STeInFormer: Spatial-Temporal Interaction Transformer Architecture for Remote Sensing Change Detection|https://arxiv.org/abs/2412.17247;SwapAnyone: Consistent and Realistic Video Synthesis for Swapping Any Person into Any Video|https://arxiv.org/abs/2503.09154;SCB-Dataset: A Dataset for Detecting Student and Teacher Classroom Behavior|https://arxiv.org/abs/2304.02488;FOCUS: Unified Vision-Language Modeling for Interactive Editing Driven by Referential Segmentation|https://arxiv.org/abs/2506.16806;VQualA 2025 Challenge on Image Super-Resolution Generated Content Quality Assessment: Methods and Results|https://arxiv.org/abs/2509.06413;I don't Want You to Die: A Shared Responsibility Framework for Safeguarding Child-Robot Companionship|https://arxiv.org/abs/2510.26080;MExD: An Expert-Infused Diffusion Model for Whole-Slide Image Classification|https://arxiv.org/abs/2503.12401;Spatially-Enhanced Recurrent Memory for Long-Range Mapless Navigation via End-to-End Reinforcement Learning|https://arxiv.org/abs/2506.05997;InstructEngine: Instruction-driven Text-to-Image Alignment|https://arxiv.org/abs/2504.10329;Logics-Parsing Technical Report|https://arxiv.org/abs/2509.19760;What Matters in RL-Based Methods for Object-Goal Navigation? An Empirical Study and A Unified Framework|https://arxiv.org/abs/2510.01830;Kwai Keye-VL Technical Report|https://arxiv.org/abs/2507.01949;One-Shot Affordance Grounding of Deformable Objects in Egocentric Organizing Scenes|https://arxiv.org/abs/2503.01092;TIME: Temporal-Sensitive Multi-Dimensional Instruction Tuning and Robust Benchmarking for Video-LLMs|https://arxiv.org/abs/2503.09994;From Seeing to Predicting: A Vision-Language Framework for Trajectory Forecasting and Controlled Video Generation|https://arxiv.org/abs/2510.00806;From Orthomosaics to Raw UAV Imagery: Enhancing Palm Detection and Crown-Center Localization|https://arxiv.org/abs/2509.12400;Beyond the Uncanny Valley: A Mixed-Method Investigation of Anthropomorphism in Protective Responses to Robot Abuse|https://arxiv.org/abs/2510.26082;Enhancing Multi-Camera Gymnast Tracking Through Domain Knowledge Integration|https://arxiv.org/abs/2511.16532;Resource-Efficient Affordance Grounding with Complementary Depth and Semantic Prompts|https://arxiv.org/abs/2503.02600;Deblur-Avatar: Animatable Avatars from Motion-Blurred Monocular Videos|https://arxiv.org/abs/2501.13335;LLaVA-MLB: Mitigating and Leveraging Attention Bias for Training-Free Video LLMs|https://arxiv.org/abs/2503.11205;Seeing It Before It Happens: In-Generation NSFW Detection for Diffusion-Based Text-to-Image Models|https://arxiv.org/abs/2508.03006;AVO: Amortized Value Optimization for Contact Mode Switching in Multi-Finger Manipulation|https://arxiv.org/abs/2510.07548"
智能感知与无人系统实验室,中国,China,复旦大学,"机器直觉, 人机物三元融合智能, 智能感知与人机交互, 计算机视觉等",在新一代人工智能理论、智能感知、计算机视觉、智能驾驶与智慧医疗等领域开展创新研究。,"张立华, 康晓洋, 董志岩等","Robust Adaptive Ensemble Adversary Reinforcement Learning, COMB-MCM, Comparison of MI-EEG Decoding in Source to Sensor Domain",121.306666,31.297777,https://ipass.fudan.edu.cn/,Lihua Zhang; Xiaoyang Kang; Zhiyan Dong,Fudan University,,MAGI-1: Autoregressive Video Generation at Scale|https://arxiv.org/abs/2505.13211;MCCD: Multi-Agent Collaboration-based Compositional Diffusion for Complex Text-to-Image Generation|https://arxiv.org/abs/2505.02648;BloomScene: Lightweight Structured 3D Gaussian Splatting for Crossmodal Scene Generation|https://arxiv.org/abs/2501.10462;SMCD: High Realism Motion Style Transfer via Mamba-based Diffusion|https://arxiv.org/abs/2405.02844;Music-Driven Legged Robots: Synchronized Walking to Rhythmic Beats|https://arxiv.org/abs/2503.04063;VLM-based Prompts as the Optimal Assistant for Unpaired Histopathology Virtual Staining|https://arxiv.org/abs/2504.15545;Breaking Reward Collapse: Adaptive Reinforcement for Open-ended Medical Reasoning with Enhanced Semantic Discrimination|https://arxiv.org/abs/2508.12957;MSCPT: Few-shot Whole Slide Image Classification with Multi-scale and Context-focused Prompt Tuning|https://arxiv.org/abs/2408.11505;PIG: Physically-based Multi-Material Interaction with 3D Gaussians|https://arxiv.org/abs/2506.07657;RENet: Fault-Tolerant Motion Control for Quadruped Robots via Redundant Estimator Networks under Visual Collapse|https://arxiv.org/abs/2509.09283;Improving Multimodal Sentiment Analysis via Modality Optimization and Dynamic Primary Modality Selection|https://arxiv.org/abs/2511.06328;Resolving Evidence Sparsity: Agentic Context Engineering for Long-Document Understanding|https://arxiv.org/abs/2511.22850;CoMT: Chain-of-Medical-Thought Reduces Hallucination in Medical Report Generation|https://arxiv.org/abs/2406.11451;SatireDecoder: Visual Cascaded Decoupling for Enhancing Satirical Image Comprehension|https://arxiv.org/abs/2512.00582;Continuous Control of Diverse Skills in Quadruped Robots Without Complete Expert Datasets|https://arxiv.org/abs/2503.03476;PersonaAnimator: Personalized Motion Transfer from Unconstrained Videos|https://arxiv.org/abs/2508.19895;VGAT: A Cancer Survival Analysis Framework Transitioning from Generative Visual Question Answering to Genomic Reconstruction|https://arxiv.org/abs/2503.19367;2DGS-Avatar: Animatable High-fidelity Clothed Avatar via 2D Gaussian Splatting|https://arxiv.org/abs/2503.02452
集群机器人系统实验室 (MagicLab),中国,China,复旦大学,智能感知与决策规划、LLM驱动的机器人与自动驾驶,专注于机器人智能感知与决策规划，探索前沿foundation model在机器人等平台的应用。,丁文超,"O2V-Mapping, OpenAnnotate3D, DeepPointMap",121.306666,31.297777,http://www.fudanmagiclab.com/,Wenchao Ding,Fudan University,,O2V-Mapping: Online Open-Vocabulary Mapping with Neural Implicit Representation|https://arxiv.org/abs/2404.06836;Learning Occlusion-aware Decision-making from Agent Interaction via Active Perception|https://arxiv.org/abs/2409.17618;Topology-Driven Trajectory Optimization for Modelling Controllable Interactions Within Multi-Vehicle Scenario|https://arxiv.org/abs/2503.05471;Drive in Corridors: Enhancing the Safety of End-to-end Autonomous Driving via Corridor Learning and Planning|https://arxiv.org/abs/2504.07507;DynOPETs: A Versatile Benchmark for Dynamic Object Pose Estimation and Tracking in Moving Camera Scenarios|https://arxiv.org/abs/2503.19625;LiveVLM: Efficient Online Video Understanding via Streaming-Oriented KV Cache and Retrieval|https://arxiv.org/abs/2505.15269;LMPOcc: 3D Semantic Occupancy Prediction Utilizing Long-Term Memory Prior from Historical Traversals|https://arxiv.org/abs/2504.13596;VINGS-Mono: Visual-Inertial Gaussian Splatting Monocular SLAM in Large Scenes|https://arxiv.org/abs/2501.08286;SparseTem: Boosting the Efficiency of CNN-Based Video Encoders by Exploiting Temporal Continuity|https://arxiv.org/abs/2410.20790;CSMapping: Scalable Crowdsourced Semantic Mapping and Topology Inference for Autonomous Driving|https://arxiv.org/abs/2512.03510;Guiding the Inner Eye: A Framework for Hierarchical and Flexible Visual Grounded Reasoning|https://arxiv.org/abs/2511.22172
机器人技术与系统国家重点实验室,中国,China,哈尔滨工业大学,机器人智能感知与行为控制、空间机器人、灵巧手技术,研究方向之一是“机器人智能感知与行为控制”，在空间机器人和灵巧手技术方面处于国际领先水平。,"刘英想, 朱延河, 段广仁, 刘宏","足式机器人环境认知学习, 软机械手设计和控制, 压电机器人操控器",126.6425,45.742778,https://robot.hit.edu.cn/,Yingxiang Liu; Yanhe Zhu; Guangren Duan; Hong Liu,Harbin Institute of Technology,,Unify Graph Learning with Text: Unleashing LLM Potentials for Session Search|https://arxiv.org/abs/2505.14156;UST-SSM: Unified Spatio-Temporal State Space Models for Point Cloud Video Modeling|https://arxiv.org/abs/2508.14604;PANICL: Mitigating Over-Reliance on Single Prompt in Visual In-Context Learning|https://arxiv.org/abs/2509.21926;NanoHTNet: Nano Human Topology Network for Efficient 3D Human Pose Estimation|https://arxiv.org/abs/2501.15763;PDDM: Pseudo Depth Diffusion Model for RGB-PD Semantic Segmentation Based in Complex Indoor Scenes|https://arxiv.org/abs/2503.18393;Design and Control of a Perching Drone Inspired by the Prey-Capturing Mechanism of Venus Flytrap|https://arxiv.org/abs/2509.13249;Adaptive Prototype Learning for Multimodal Cancer Survival Analysis|https://arxiv.org/abs/2503.04643;Learning Mutual Excitation for Hand-to-Hand and Human-to-Human Interaction Recognition|https://arxiv.org/abs/2402.02431;PathoPainter: Augmenting Histopathology Segmentation via Tumor-aware Inpainting|https://arxiv.org/abs/2503.04634;Dual-Modal Attention-Enhanced Text-Video Retrieval with Triplet Partial Margin Contrastive Learning|https://arxiv.org/abs/2309.11082;TCPFormer: Learning Temporal Correlation with Implicit Pose Proxy for 3D Human Pose Estimation|https://arxiv.org/abs/2501.01770;Unlocking the Potential of Weakly Labeled Data: A Co-Evolutionary Learning Framework for Abnormality Detection and Report Generation|https://arxiv.org/abs/2412.13599;3D Skeleton-Based Action Recognition: A Review|https://arxiv.org/abs/2506.00915;Recognizing Actions from Robotic View for Natural Human-Robot Interaction|https://arxiv.org/abs/2507.22522;Masked Clustering Prediction for Unsupervised Point Cloud Pre-training|https://arxiv.org/abs/2508.08910;B-AVIBench: Towards Evaluating the Robustness of Large Vision-Language Model on Black-box Adversarial Visual-Instructions|https://arxiv.org/abs/2403.09346;Enhancing WSI-Based Survival Analysis with Report-Auxiliary Self-Distillation|https://arxiv.org/abs/2509.15608;Global-Local Distillation Network-Based Audio-Visual Speaker Tracking with Incomplete Modalities|https://arxiv.org/abs/2408.14585;MoCa: Modality-aware Continual Pre-training Makes Better Bidirectional Multimodal Embeddings|https://arxiv.org/abs/2506.23115;MoSa: Motion Generation with Scalable Autoregressive Modeling|https://arxiv.org/abs/2511.01200;DCIRNet: Depth Completion with Iterative Refinement for Dexterous Grasping of Transparent and Reflective Objects|https://arxiv.org/abs/2506.09491;E-InMeMo: Enhanced Prompting for Visual In-Context Learning|https://arxiv.org/abs/2504.18158;Bridge the Gap between Past and Future: Siamese Model Optimization for Context-Aware Document Ranking|https://arxiv.org/abs/2505.14180;Diffusion as Reasoning: Enhancing Object Navigation via Diffusion Model Conditioned on LLM-based Object-Room Knowledge|https://arxiv.org/abs/2410.21842;Uncertainty-Aware Testing-Time Optimization for 3D Human Pose Estimation|https://arxiv.org/abs/2402.02339;HTMNet: A Hybrid Network with Transformer-Mamba Bottleneck Multimodal Fusion for Transparent and Reflective Objects Depth Completion|https://arxiv.org/abs/2505.20904;A Microgravity Simulation Experimental Platform For Small Space Robots In Orbit|https://arxiv.org/abs/2504.18842;ERTACache: Error Rectification and Timesteps Adjustment for Efficient Diffusion|https://arxiv.org/abs/2508.21091;Neural-Network-Driven Reward Prediction as a Heuristic: Advancing Q-Learning for Mobile Robot Path Planning|https://arxiv.org/abs/2412.12650;H$_{2}$OT: Hierarchical Hourglass Tokenizer for Efficient Video Pose Transformers|https://arxiv.org/abs/2509.06956;HDCNet: A Hybrid Depth Completion Network for Grasping Transparent and Reflective Objects|https://arxiv.org/abs/2511.07081;Learning Perceptive Humanoid Locomotion over Challenging Terrain|https://arxiv.org/abs/2503.00692
Harvard Microrobotics Laboratory,英国,Europe,哈佛大学,微型机器人、软体机器人、驱动与控制,采用高度实验性的方法来开发微型机器人，并将其用作工具探索微力学、驱动、流体力学等基本问题。,Robert Wood,"Bidirectional motion of a planar fabricated piezoelectric motor, Transcriptome sequencing of seven deep marine invertebrates, Follow Anything",-71.11666,42.377,https://www.micro.seas.harvard.edu/,Robert Wood,Harvard University,,Decentralized Vision-Based Autonomous Aerial Wildlife Monitoring|https://arxiv.org/abs/2508.15038;Real-time Remote Tracking and Autonomous Planning for Whale Rendezvous using Robots|https://arxiv.org/abs/2512.05808;WiSER-X: Wireless Signals-based Efficient Decentralized Multi-Robot Exploration without Explicit Information Exchange|https://arxiv.org/abs/2412.19876
Rajpurkar Lab,英国,Europe,哈佛大学,医学人工智能、基础模型、多模态学习、生成式AI,致力于推动医学人工智能的发展，开发自适应医疗基础模型及多模态学习方法。,Pranav Rajpurkar,"Expert-level detection of pathologies from unannotated chest X-ray images, MoCo-CXR, Predicting patient decompensation",-71.11666,42.377,https://github.com/rajpurkarlab,Pranav Rajpurkar,Harvard University,,FactCheXcker: Mitigating Measurement Hallucinations in Chest X-ray Report Generation Models|https://arxiv.org/abs/2411.18672;ReXVQA: A Large-scale Visual Question Answering Benchmark for Generalist Chest X-ray Understanding|https://arxiv.org/abs/2506.04353;A Large Model for Non-invasive and Personalized Management of Breast Cancer from Multiparametric MRI|https://arxiv.org/abs/2408.12606;UniBiomed: A Universal Foundation Model for Grounded Biomedical Image Interpretation|https://arxiv.org/abs/2504.21336;An Explainable Biomedical Foundation Model via Large-Scale Concept-Enhanced Vision-Language Pre-training|https://arxiv.org/abs/2501.15579;a2z-1 for Multi-Disease Detection in Abdomen-Pelvis CT: External Validation and Performance Analysis Across 21 Conditions|https://arxiv.org/abs/2412.12629;MedVersa: A Generalist Foundation Model for Medical Image Interpretation|https://arxiv.org/abs/2405.07988;RadGame: An AI-Powered Platform for Radiology Education|https://arxiv.org/abs/2509.13270;A Synthetic Data-Driven Radiology Foundation Model for Pan-tumor Clinical Diagnosis|https://arxiv.org/abs/2502.06171;ReXGradient-160K: A Large-Scale Publicly Available Dataset of Chest Radiographs with Free-text Reports|https://arxiv.org/abs/2505.00228;FreeTumor: Large-Scale Generative Tumor Synthesis in Computed Tomography Images for Improving Tumor Recognition|https://arxiv.org/abs/2502.18519;ReXGroundingCT: A 3D Chest CT Dataset for Segmentation of Findings from Free-Text Reports|https://arxiv.org/abs/2507.22030;ColonCrafter: A Depth Estimation Model for Colonoscopy Videos Using Diffusion Priors|https://arxiv.org/abs/2509.13525;3DReasonKnee: Advancing Grounded Reasoning in Medical Vision Language Models|https://arxiv.org/abs/2510.20967
Berkeley Artificial Intelligence Research Lab (BAIR),美国,USA,加州大学伯克利分校,计算机视觉、机器学习、NLP、规划、控制、机器人,汇集了加州大学伯克利分校在人工智能多个领域的研究人员。,"Peter L. Bartlett, Jacob Steinhardt, John Canny","Trained transformers learn linear models in-context, Covert Malicious Finetuning, Interpretable Learning for Self-Driving Cars",-122.258861,37.871899,https://bair.berkeley.edu/,"Peter L. Bartlett,; Jacob Steinhardt;  John Canny",UC Berkeley,,Interpreting the Second-Order Effects of Neurons in CLIP|https://arxiv.org/abs/2406.04341
SU Lab,美国,USA,加州大学圣地亚哥分校,计算机视觉、图形学、机器人学,致力于开发先进的算法和系统，以实现对复杂三维场景的理解和交互。,Hao Su,"MANISKILL-HAB, POLICY DECORATOR, MANISKILL3",-117.234156,32.879666,https://cseweb.ucsd.edu/~haosu/,Hao Su,"University of California, San Diego",,"Seeing the Bigger Picture: 3D Latent Mapping for Mobile Manipulation Policy Learning|https://arxiv.org/abs/2510.03885;Policy Decorator: Model-Agnostic Online Refinement for Large Policy Model|https://arxiv.org/abs/2412.13630;Advances in Feed-Forward 3D Reconstruction and View Synthesis: A Survey|https://arxiv.org/abs/2507.14501;3D-Adapter: Geometry-Consistent Multi-View Diffusion for High-Quality 3D Generation|https://arxiv.org/abs/2410.18974;Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Dynamic Scenes|https://arxiv.org/abs/2404.12379;Responsive Noise-Relaying Diffusion Policy: Responsive and Efficient Visuomotor Control|https://arxiv.org/abs/2502.12724;Open X-Embodiment: Robotic Learning Datasets and RT-X Models|https://arxiv.org/abs/2310.08864;Diffusion Dynamics Models with Generative State Estimation for Cloth Manipulation|https://arxiv.org/abs/2503.11999;NeuManifold: Neural Watertight Manifold Reconstruction with Efficient and High-Quality Rendering Support|https://arxiv.org/abs/2305.17134;ManiTaskGen: A Comprehensive Task Generator for Benchmarking and Improving Vision-Language Agents on Embodied Decision-Making|https://arxiv.org/abs/2505.20726;RigAnything: Template-Free Autoregressive Rigging for Diverse 3D Assets|https://arxiv.org/abs/2502.09615;ManiSkill-HAB: A Benchmark for Low-Level Manipulation in Home Rearrangement Tasks|https://arxiv.org/abs/2412.13211;Learning Adaptive Dexterous Grasping from Single Demonstrations|https://arxiv.org/abs/2503.20208;Learning Representations of Satellite Images with Evaluations on Synoptic Weather Events|https://arxiv.org/abs/2508.06122;Passage-traversing optimal path planning with sampling-based algorithms|https://arxiv.org/abs/2506.23614;LARM: A Large Articulated-Object Reconstruction Model|https://arxiv.org/abs/2511.11563;When Should We Prefer State-to-Visual DAgger Over Visual Reinforcement Learning?|https://arxiv.org/abs/2412.13662;FreeArt3D: Training-Free Articulated Object Generation using 3D Diffusion|https://arxiv.org/abs/2510.25765;Do Vision-Language Models Have Internal World Models? Towards an Atomic Evaluation|https://arxiv.org/abs/2506.21876;ORIC: Benchmarking Object Recognition under Contextual Incongruity in Large Vision-Language Models|https://arxiv.org/abs/2509.15695;Enhancing Generalization in Vision-Language-Action Models by Preserving Pretrained Representations|https://arxiv.org/abs/2509.11417;PARTFIELD: Learning 3D Feature Fields for Part Segmentation and Beyond|https://arxiv.org/abs/2504.11451;MeshLRM: Large Reconstruction Model for High-Quality Meshes|https://arxiv.org/abs/2404.12385;Scaling Cross-Embodiment World Models for Dexterous Manipulation|https://arxiv.org/abs/2511.01177;ManiSkill3: GPU Parallelized Robotics Simulation and Rendering for Generalizable Embodied AI|https://arxiv.org/abs/2410.00425;Towards Embodiment Scaling Laws in Robot Locomotion|https://arxiv.org/abs/2505.05753;Multi-Stage Manipulation with Demonstration-Augmented Reward, Policy, and World Model Learning|https://arxiv.org/abs/2503.01837;A Recipe for Efficient Sim-to-Real Transfer in Manipulation with Online Imitation-Pretrained World Models|https://arxiv.org/abs/2510.02538;PartUV: Part-Based UV Unwrapping of 3D Meshes|https://arxiv.org/abs/2511.16659;Cross-embodied Co-design for Dexterous Hands|https://arxiv.org/abs/2512.03743;Learning Massively Multitask World Models for Continuous Control|https://arxiv.org/abs/2511.19584;Hierarchical World Models as Visual Whole-Body Humanoid Controllers|https://arxiv.org/abs/2405.18418;CogDDN: A Cognitive Demand-Driven Navigation with Decision Optimization and Dual-Process Thinking|https://arxiv.org/abs/2507.11334;LodeStar: Long-horizon Dexterity via Synthetic Data Augmentation from Human Demonstrations|https://arxiv.org/abs/2508.17547;MorphoCopter: Design, Modeling, and Control of a New Transformable Quad-Bi Copter|https://arxiv.org/abs/2506.07204"
加州理工-机器人实验室,美国,USA,加州理工学院,自主系统与技术 (CAST)、机器人自主操作,跨学科研究中心，旨在开发能够在复杂环境中自主导航和响应变化的自主机器。,Mory Gharib,"M4 Morphobot, Spectral Expansion Tree Search, Robot mobility and teaming to navigate",-118.125517,34.137447,https://robotics.caltech.edu/wiki/index.php/Robotics,Mory Gharib,California Institute of Technology,,
Bio-Inspired Robotics Laboratory (BIRL),英国,Europe,剑桥大学,生物启发机器人、机器人运动学、自修复材料、可穿戴设备,研究如何从自然中汲取灵感来改进机器人技术，由多个欧洲顶尖院校和公司合作组成。,Iida Fumiya,"Paleo-inspired robotics, State transition learning with limited data, Utilising redundancy in musculoskeletal systems",0.1218,52.2043,https://birlab.org/,Iida Fumiya,University of Cambridge,,
Affective Intelligence and Robotics Laboratory (AFAR),英国,Europe,剑桥大学,情感计算、社会信号处理、社交机器人、人机交互,研究兴趣处于计算机视觉、机器学习、多模态交互和人机交互等多个学科的交叉点。,"Hatice Gunes, Sinan Kalkan, Siyang Song","Learning Socially Appropriate Robo-waiter Behaviours, Domain-Incremental Continual Learning, Latent Generative Replay",0.1218,52.2043,https://cambridge-afar.github.io/,"Hatice Gunes;  Sinan Kalkan,; Siyang Song",University of Cambridge,,"CauSkelNet: Causal Representation Learning for Human Behaviour Analysis|https://arxiv.org/abs/2409.15564;Gender Fairness of Machine Learning Algorithms for Pain Detection|https://arxiv.org/abs/2506.11132;Who Owns The Robot?: Four Ethical and Socio-technical Questions about Wellbeing Robots in the Real World through Community Engagement|https://arxiv.org/abs/2509.02624;Participant Perceptions of a Robotic Coach Conducting Positive Psychology Exercises: A Qualitative Analysis|https://arxiv.org/abs/2209.03827;ERR@HRI 2.0 Challenge: Multimodal Detection of Errors and Failures in Human-Robot Conversations|https://arxiv.org/abs/2507.13468;D^3-Talker: Dual-Branch Decoupled Deformation Fields for Few-Shot 3D Talking Head Synthesis|https://arxiv.org/abs/2508.14449;CD-Lamba: Boosting Remote Sensing Change Detection via a Cross-Temporal Locally Adaptive State Space Model|https://arxiv.org/abs/2501.15455;Learning Personalised Human Internal Cognition from External Expressive Behaviours for Real Personality Recognition|https://arxiv.org/abs/2508.00205;LOGCAN++: Adaptive Local-global class-aware network for semantic segmentation of remote sensing imagery|https://arxiv.org/abs/2406.16502;GRACE: Generating Socially Appropriate Robot Actions Leveraging LLMs and Human Explanations|https://arxiv.org/abs/2409.16879;Beyond Vision: How Large Language Models Interpret Facial Expressions from Valence-Arousal Values|https://arxiv.org/abs/2502.06875;Robot-Led Vision Language Model Wellbeing Assessment of Children|https://arxiv.org/abs/2504.02765;REACT 2025: the Third Multiple Appropriate Facial Reaction Generation Challenge|https://arxiv.org/abs/2505.17223;Enhancing Adversarial Transferability by Balancing Exploration and Exploitation with Gradient-Guided Sampling|https://arxiv.org/abs/2511.00411;GraphAU-Pain: Graph-based Action Unit Representation for Pain Intensity Estimation|https://arxiv.org/abs/2505.19802;Machine Learning Fairness for Depression Detection using EEG Data|https://arxiv.org/abs/2501.18192;Stakeholder Perspectives on Whether and How Social Robots Can Support Mediation and Advocacy for Higher Education Students with Disabilities|https://arxiv.org/abs/2503.16499;Oral Imaging for Malocclusion Issues Assessments: OMNI Dataset, Deep Learning Baselines and Benchmarking|https://arxiv.org/abs/2505.15637;Comparing Self-Disclosure Themes and Semantics to a Human, a Robot, and a Disembodied Agent|https://arxiv.org/abs/2504.06374;What People Share With a Robot When Feeling Lonely and Stressed and How It Helps Over Time|https://arxiv.org/abs/2504.02991;DEGSTalk: Decomposed Per-Embedding Gaussian Fields for Hair-Preserving Talking Face Synthesis|https://arxiv.org/abs/2412.20148;Feature Aggregation with Latent Generative Replay for Federated Continual Learning of Socially Appropriate Robot Behaviours|https://arxiv.org/abs/2405.15773;CA-Edit: Causality-Aware Condition Adapter for High-Fidelity Local Facial Attribute Editing|https://arxiv.org/abs/2412.13565;Exploring Causality for HRI: A Case Study on Robotic Mental Well-being Coaching|https://arxiv.org/abs/2503.11684;AsyReC: A Multimodal Graph-based Framework for Spatio-Temporal Asymmetric Dyadic Relationship Classification|https://arxiv.org/abs/2504.05030;SynFER: Towards Boosting Facial Expression Recognition with Synthetic Data|https://arxiv.org/abs/2410.09865;FG 2025 TrustFAA: the First Workshop on Towards Trustworthy Facial Affect Analysis: Advancing Insights of Fairness, Explainability, and Safety (TrustFAA)|https://arxiv.org/abs/2506.05095;A Robot-Led Intervention for Emotion Regulation: From Expression to Reappraisal|https://arxiv.org/abs/2503.18243;A Longitudinal Study of Child Wellbeing Assessment via Online Interactions with a Social Robot|https://arxiv.org/abs/2404.10593;Some Optimizers are More Equal: Understanding the Role of Optimizers in Group Fairness|https://arxiv.org/abs/2504.14882;OmniResponse: Online Multimodal Conversational Response Generation in Dyadic Interactions|https://arxiv.org/abs/2505.21724;Critical Insights about Robots for Mental Wellbeing|https://arxiv.org/abs/2506.13739"
高性能人形技术实验室 (H²T),德国,Europe,卡尔斯鲁厄理工学院,仿人机器人技术、抓取和移动操纵、从人类观察中学习,研究和开发仿人机器人技术和系统，使其能在现实世界中与人类互动执行任务。,Tamim Asfour,"ARMAR-6, Safe Reinforcement Learning of Robot Trajectories, Push Effect Prediction",8.416667,49.016667,https://h2t.iar.kit.edu/index.php,Tamim Asfour,Karlsruhe Institute of Technology,,"KITchen: A Real-World Benchmark and Dataset for 6D Object Pose Estimation in Kitchen Environments|https://arxiv.org/abs/2403.16238;Force Myography based Torque Estimation in Human Knee and Ankle Joints|https://arxiv.org/abs/2409.11061;Intuitive Programming, Adaptive Task Planning, and Dynamic Role Allocation in Human-Robot Collaboration|https://arxiv.org/abs/2511.08732;Adaptive Domain Modeling with Language Models: A Multi-Agent Approach to Task Planning|https://arxiv.org/abs/2506.19592;Geometric Contact Flows: Contactomorphisms for Dynamics and Control|https://arxiv.org/abs/2506.17868;Taxonomy-aware Dynamic Motion Generation on Hyperbolic Manifolds|https://arxiv.org/abs/2509.21281;Diffusion-Based Impedance Learning for Contact-Rich Manipulation Tasks|https://arxiv.org/abs/2509.19696;CLEVER: Stream-based Active Learning for Robust Semantic Perception from Human Instructions|https://arxiv.org/abs/2507.15499;MoRe-ERL: Learning Motion Residuals using Episodic Reinforcement Learning|https://arxiv.org/abs/2508.01409;Episodic Memory Verbalization using Hierarchical Representations of Life-Long Robot Experience|https://arxiv.org/abs/2409.17702"
卡内基-机器人研究所,美国,USA,卡内基梅隆大学,机器人感知、控制、规划、智能系统,成立于 1979 年，是全球领先的机器人研究机构之一，研究涵盖多个应用领域。,Martial Hebert,"RoboAgent, Bridging Adaptivity and Safety, Core Challenges in Embodied Vision-Language Planning",-79.943056,40.443056,https://www.ri.cmu.edu/,Martial Hebert,Carnegie Mellon University,,ReferEverything: Towards Segmenting Everything We Can Speak of in Videos|https://arxiv.org/abs/2410.23287;On the Conic Complementarity of Planar Contacts|https://arxiv.org/abs/2509.25999;Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene Understanding|https://arxiv.org/abs/2409.03757
Robot Perception and Learning Lab,英国,Europe,伦敦大学学院 (UCL),关节式机器人（四足、人形等）、导航与操作、感知与运动控制,重点是为关节式机器人开发前沿算法，使它们能在不可预测的自然环境中高效导航和操作。,Dimitrios Kanoulas,"Active Sensing for Data Quality Improvement, Real-Time Metric-Semantic Mapping, Reinforcement Learning Grasping with Force Feedback",-0.134649,51.5246,https://rpl-as-ucl.github.io/,Dimitrios Kanoulas,University College London,,"Follow Everything: A Leader-Following and Obstacle Avoidance Framework with Goal-Aware Adaptation|https://arxiv.org/abs/2504.19399;Hierarchical Intention-Aware Expressive Motion Generation for Humanoid Robots|https://arxiv.org/abs/2506.01563;SDS -- See it, Do it, Sorted: Quadruped Skill Synthesis from Single Video Demonstration|https://arxiv.org/abs/2410.11571;On the Benefits of GPU Sample-Based Stochastic Predictive Controllers for Legged Locomotion|https://arxiv.org/abs/2403.11383;Unreal Robotics Lab: A High-Fidelity Robotics Simulator with Advanced Physics and Rendering|https://arxiv.org/abs/2504.14135;Exploring Adversarial Obstacle Attacks in Search-based Path Planning for Autonomous Mobile Robots|https://arxiv.org/abs/2504.06154;Watch Your STEPP: Semantic Traversability Estimation using Pose Projected Features|https://arxiv.org/abs/2501.17594;Underwater Robotic Simulators Review for Autonomous System Development|https://arxiv.org/abs/2504.06245;Learning to Recover: Dynamic Reward Shaping with Wheel-Leg Coordination for Fallen Robots|https://arxiv.org/abs/2506.05516;UniGS: Unified Geometry-Aware Gaussian Splatting for Multimodal Rendering|https://arxiv.org/abs/2510.12174;Discovery of skill switching criteria for learning agile quadruped locomotion|https://arxiv.org/abs/2502.06676;AIR-HLoc: Adaptive Retrieved Images Selection for Efficient Visual Localisation|https://arxiv.org/abs/2403.18281;Immersive Teleoperation Framework for Locomanipulation Tasks|https://arxiv.org/abs/2504.15229;The Starlink Robot: A Platform and Dataset for Mobile Satellite Communication|https://arxiv.org/abs/2506.19781"
CSAIL Embodied Intelligence Labs,美国,USA,麻省理工学院 (MIT),感知、传感、语言、学习、规划、具身智能,目标是通过研究人类智能和设计智能机器人，来理解物理世界中智能行为的本质，下设多个子实验室。,"Ted Adelson, Pulkit Agrawal, Jacob Andreas","Adaptive Language-Guided Abstraction, Towards real-time photorealistic 3D holography, Aligning Human and Robot Representations",-71.094162,42.360091,https://ei.csail.mit.edu/index.html,"Ted Adelson;  Pulkit Agrawal,; Jacob Andreas",Massachusetts Institute of Technology,,ARC Is a Vision Problem!|https://arxiv.org/abs/2511.14761;Automated Detection of Visual Attribute Reliance with a Self-Reflective Agent|https://arxiv.org/abs/2510.21704;Line of Sight: On Linear Representations in VLLMs|https://arxiv.org/abs/2506.04706;A Multimodal Automated Interpretability Agent|https://arxiv.org/abs/2404.14394
Quest for Intelligence,美国,USA,麻省理工学院 (MIT),自然智能的科学理论、计算模型、机器人/具身智能,旨在理解智能的本质——大脑如何产生智能以及如何在人工系统中复制智能，是一个跨学科的整合计划。,"Pulkit Agrawal, Cynthia Breazeal, James DiCarlo","Towards Practical Multi-object Manipulation, Adversarially trained neural representations, Diagnosis, Feedback, Adaptation",-71.094162,42.360091,https://quest.mit.edu/,Pulkit Agrawal; Cynthia Breazeal;  James DiCarlo,Massachusetts Institute of Technology,,Embodied Red Teaming for Auditing Robotic Foundation Models|https://arxiv.org/abs/2411.18676;Social Robots as Social Proxies for Fostering Connection and Empathy Towards Humanity|https://arxiv.org/abs/2502.00221;Hovering Flight of Soft-Actuated Insect-Scale Micro Aerial Vehicles using Deep Reinforcement Learning|https://arxiv.org/abs/2502.12355;DEXOP: A Device for Robotic Transfer of Dexterous Human Manipulation|https://arxiv.org/abs/2509.04441;Noninvasive precision modulation of high-level neural population activity via natural vision perturbations|https://arxiv.org/abs/2506.05633;Robot Learning with Super-Linear Scaling|https://arxiv.org/abs/2412.01770;Efficient Diffusion Transformer Policies with Mixture of Expert Denoisers for Multitask Learning|https://arxiv.org/abs/2412.12953;L-WISE: Boosting Human Visual Category Learning Through Model-Based Image Selection and Enhancement|https://arxiv.org/abs/2412.09765;Few-Shot Task Learning through Inverse Generative Modeling|https://arxiv.org/abs/2411.04987;Vision CNNs trained to estimate spatial latents learned similar ventral-stream-aligned representations|https://arxiv.org/abs/2412.09115;Bridging the Sim-to-Real Gap for Athletic Loco-Manipulation|https://arxiv.org/abs/2502.10894;ORSO: Accelerating Reward Design via Online Reward Selection and Policy Optimization|https://arxiv.org/abs/2410.13837;Large Pre-Training Datasets Don't Always Guarantee Robustness after Fine-Tuning|https://arxiv.org/abs/2410.21582;SoftMimic: Learning Compliant Whole-body Control from Examples|https://arxiv.org/abs/2510.17792;DexWrist: A Robotic Wrist for Constrained and Dynamic Manipulation|https://arxiv.org/abs/2507.01008
MARS Lab,新加坡,China,南洋理工大学 (NTU),多模态感知、具身AI、AIoT系统,研究物理人工智能，关注AI如何使物理系统（机器人、物联网等）感知、理解并与物理世界交互。,Jianfei Yang,"Diffusion Model is a Good Pose Estimator from 3D RF-Vision, Reliable Spatial-Temporal Voxels For Multi-Modal Test-Time Adaptation, MoPA",103.680374,1.3485,https://marslab.tech/,Jianfei Yang,Nanyang Technological University,,"Mask2IV: Interaction-Centric Video Generation via Mask Trajectories|https://arxiv.org/abs/2510.03135;$\mathbf{M^3A}$ Policy: Mutable Material Manipulation Augmentation Policy through Photometric Re-rendering|https://arxiv.org/abs/2512.01446;Emergence of Painting Ability via Recognition-Driven Evolution|https://arxiv.org/abs/2501.04966;Zero-Shot Open-Vocabulary Human Motion Grounding with Test-Time Training|https://arxiv.org/abs/2511.15379;X-Fi: A Modality-Invariant Foundation Model for Multimodal Human Sensing|https://arxiv.org/abs/2410.10167;RM-RL: Role-Model Reinforcement Learning for Precise Robot Manipulation|https://arxiv.org/abs/2510.15189;When Robots Should Say ""I Don't Know"": Benchmarking Abstention in Embodied Question Answering|https://arxiv.org/abs/2512.04597;Unsupervised UAV 3D Trajectories Estimation with Sparse Point Clouds|https://arxiv.org/abs/2412.12716;XTransfer: Modality-Agnostic Few-Shot Model Transfer for Human Sensing at the Edge|https://arxiv.org/abs/2506.22726;HiCoGen: Hierarchical Compositional Text-to-Image Generation in Diffusion Models via Reinforcement Learning|https://arxiv.org/abs/2511.19965;RAW-Adapter: Adapting Pre-trained Visual Model to Camera RAW Images and A Benchmark|https://arxiv.org/abs/2503.17027;REI-Bench: Can Embodied Agents Understand Vague Human Instructions in Task Planning?|https://arxiv.org/abs/2505.10872;Generative Dataset Distillation using Min-Max Diffusion Model|https://arxiv.org/abs/2503.18626;Audio Array-Based 3D UAV Trajectory Estimation with LiDAR Pseudo-Labeling|https://arxiv.org/abs/2412.12698;AV-PedAware: Self-Supervised Audio-Visual Fusion for Dynamic Pedestrian Awareness|https://arxiv.org/abs/2411.06789;HoloLLM: Multisensory Foundation Model for Language-Grounded Human Sensing and Reasoning|https://arxiv.org/abs/2505.17645;mmPred: Radar-based Human Motion Prediction in the Dark|https://arxiv.org/abs/2512.00345;Interactive Test-Time Adaptation with Reliable Spatial-Temporal Voxels for Multi-Modal Segmentation|https://arxiv.org/abs/2403.06461;QLIO: Quantized LiDAR-Inertial Odometry|https://arxiv.org/abs/2503.07949;NoisyEQA: Benchmarking Embodied Question Answering Against Noisy Queries|https://arxiv.org/abs/2412.10726"
PINE Lab,新加坡,China,南洋理工大学 (NTU),具身指令跟随、通用机器人操作生成模型、基础模型压缩、实时3D场景感知,研究方向包括让智能系统理解并执行人类指令，构建通用机器人操作的基础模型等。,Ziwei Wang,"3D Small Object Detection with Dynamic Spatial Pruning, ManiGaussian, StableLego",103.680374,1.3485,https://pine-lab-ntu.github.io/,Ziwei Wang,Nanyang Technological University,,"Queryable 3D Scene Representation: A Multi-Modal Framework for Semantic Reasoning and Robotic Task Planning|https://arxiv.org/abs/2509.20077;MoTo: A Zero-shot Plug-in Interaction-aware Navigation for General Mobile Manipulation|https://arxiv.org/abs/2509.01658;MAP-VLA: Memory-Augmented Prompting for Vision-Language-Action Model in Robotic Manipulation|https://arxiv.org/abs/2511.09516;Learning Quadrupedal Robot Locomotion for Narrow Pipe Inspection|https://arxiv.org/abs/2412.13621;A Narrative Review on Large AI Models in Lung Cancer Screening, Diagnosis, and Treatment Planning|https://arxiv.org/abs/2506.07236;GWM: Towards Scalable Gaussian World Models for Robotic Manipulation|https://arxiv.org/abs/2508.17600;Querying Autonomous Vehicle Point Clouds: Enhanced by 3D Object Counting with CounterNet|https://arxiv.org/abs/2507.19209;AnyBimanual: Transferring Unimanual Policy for General Bimanual Manipulation|https://arxiv.org/abs/2412.06779;Enhancing Certifiable Semantic Robustness via Robust Pruning of Deep Neural Networks|https://arxiv.org/abs/2510.00083;Real-time Multi-modal Object Detection and Tracking on Edge for Regulatory Compliance Monitoring|https://arxiv.org/abs/2310.03333;Learning Visual Hierarchies in Hyperbolic Space for Image Retrieval|https://arxiv.org/abs/2411.17490;StableLego: Stability Analysis of Block Stacking Assembly|https://arxiv.org/abs/2402.10711;TSP3D: Text-guided Sparse Voxel Pruning for Efficient 3D Visual Grounding|https://arxiv.org/abs/2502.10392;EfficientLLaVA:Generalizable Auto-Pruning for Large Vision-language Models|https://arxiv.org/abs/2503.15369;RESample: A Robust Data Augmentation Framework via Exploratory Sampling for Robotic Manipulation|https://arxiv.org/abs/2510.17640;RoboChemist: Long-Horizon and Safety-Compliant Robotic Chemical Experimentation|https://arxiv.org/abs/2509.08820;TA-VLA: Elucidating the Design Space of Torque-aware Vision-Language-Action Models|https://arxiv.org/abs/2509.07962;VLA-RL: Towards Masterful and General Robotic Manipulation with Scalable Reinforcement Learning|https://arxiv.org/abs/2505.18719;GaussianCAD: Robust Self-Supervised CAD Reconstruction from Three Orthographic Views Using 3D Gaussian Splatting|https://arxiv.org/abs/2503.05161;UniGoal: Towards Universal Zero-shot Goal-oriented Navigation|https://arxiv.org/abs/2503.10630;VLA-Reasoner: Empowering Vision-Language-Action Models with Reasoning via Online Monte Carlo Tree Search|https://arxiv.org/abs/2509.22643;10 Open Challenges Steering the Future of Vision-Language-Action Models|https://arxiv.org/abs/2511.05936;History-Aware Reasoning for GUI Agents|https://arxiv.org/abs/2511.09127;Governance-Ready Small Language Models for Medical Imaging: Prompting, Abstention, and PACS Integration|https://arxiv.org/abs/2508.13378;ManiGaussian++: General Robotic Bimanual Manipulation with Hierarchical Gaussian World Model|https://arxiv.org/abs/2506.19842;Q-VLM: Post-training Quantization for Large Vision-Language Models|https://arxiv.org/abs/2410.08119;iMoWM: Taming Interactive Multi-Modal World Model for Robotic Manipulation|https://arxiv.org/abs/2510.09036;MP-GUI: Modality Perception with MLLMs for GUI Understanding|https://arxiv.org/abs/2503.14021;InteractEdit: Zero-Shot Editing of Human-Object Interactions in Images|https://arxiv.org/abs/2503.09130;IGL-Nav: Incremental 3D Gaussian Localization for Image-goal Navigation|https://arxiv.org/abs/2508.00823;MoManipVLA: Transferring Vision-language-action Models for General Mobile Manipulation|https://arxiv.org/abs/2503.13446;Anyview: Generalizable Indoor 3D Object Detection with Variable Frames|https://arxiv.org/abs/2310.05346;On-the-Fly Object-aware Representative Point Selection in Point Cloud|https://arxiv.org/abs/2508.01980;SafeBimanual: Diffusion-based Trajectory Optimization for Safe Bimanual Manipulation|https://arxiv.org/abs/2508.18268;PointVDP: Learning View-Dependent Projection by Fireworks Rays for 3D Point Cloud Segmentation|https://arxiv.org/abs/2507.06618;EmbodiedSAM: Online Segment Any 3D Thing in Real Time|https://arxiv.org/abs/2408.11811;ManiCM: Real-time 3D Diffusion Policy via Consistency Model for Robotic Manipulation|https://arxiv.org/abs/2406.01586;Embodied Instruction Following in Unknown Environments|https://arxiv.org/abs/2406.11818;NORA-1.5: A Vision-Language-Action Model Trained using World Model- and Action-based Preference Rewards|https://arxiv.org/abs/2511.14659"
S-Lab for Advanced Intelligence,新加坡,China,南洋理工大学 (NTU),计算机视觉、NLP、强化学习、深度学习,2020年成立的实验室，研究前沿AI技术，具体涉及内容编辑生成、分布式学习、3D场景理解等。,"Prof. Guan Cuntai,Prof. Loy Chen Change","Unified 3D and 4D Panoptic Segmentation, Flare7K++, TOPIQ",103.680374,1.3485,https://www.ntu.edu.sg/s-lab,Cuntai Guan; Chen Change Loy,Nanyang Technological University,,"STream3R: Scalable Sequential 3D Reconstruction with Causal Transformer|https://arxiv.org/abs/2508.10893;Learning 3D Garment Animation from Trajectories of A Piece of Cloth|https://arxiv.org/abs/2501.01393;Beyond Overfitting: Doubly Adaptive Dropout for Generalizable AU Detection|https://arxiv.org/abs/2503.08974;Sigma: Semantically Informative Pre-training for Skeleton-based Sign Language Understanding|https://arxiv.org/abs/2509.21223;SeedVR2: One-Step Video Restoration via Diffusion Adversarial Post-Training|https://arxiv.org/abs/2506.05301;Enhanced Generative Structure Prior for Chinese Text Image Super-resolution|https://arxiv.org/abs/2508.07537;Half-Physics: Enabling Kinematic 3D Human Model with Physical Interactions|https://arxiv.org/abs/2507.23778;GaussianAnything: Interactive Point Cloud Flow Matching For 3D Object Generation|https://arxiv.org/abs/2411.08033;OpenUni: A Simple Baseline for Unified Multimodal Understanding and Generation|https://arxiv.org/abs/2505.23661;MEAT: Multiview Diffusion Model for Human Generation on Megapixels with Mesh Attention|https://arxiv.org/abs/2503.08664;MeViS: A Multi-Modal Dataset for Referring Motion Expression Video Segmentation|https://arxiv.org/abs/2512.10945;Next Visual Granularity Generation|https://arxiv.org/abs/2508.12811;Generative Photographic Control for Scene-Consistent Video Cinematic Editing|https://arxiv.org/abs/2511.12921;SeedVR: Seeding Infinity in Diffusion Transformer Towards Generic Video Restoration|https://arxiv.org/abs/2501.01320;Omegance: A Single Parameter for Various Granularities in Diffusion-Based Synthesis|https://arxiv.org/abs/2411.17769;Denoising as Adaptation: Noise-Space Domain Adaptation for Image Restoration|https://arxiv.org/abs/2406.18516;Trans-Adapter: A Plug-and-Play Framework for Transparent Image Inpainting|https://arxiv.org/abs/2508.01098;crossMoDA Challenge: Evolution of Cross-Modality Domain Adaptation Techniques for Vestibular Schwannoma and Cochlea Segmentation from 2021 to 2023|https://arxiv.org/abs/2506.12006;GausSim: Foreseeing Reality by Gaussian Simulator for Elastic Objects|https://arxiv.org/abs/2412.17804;Arbitrary-steps Image Super-resolution via Diffusion Inversion|https://arxiv.org/abs/2412.09013;F-LMM: Grounding Frozen Large Multimodal Models|https://arxiv.org/abs/2406.05821;Decoupled Doubly Contrastive Learning for Cross Domain Facial Action Unit Detection|https://arxiv.org/abs/2503.08977;ObjCtrl-2.5D: Training-free Object Control with Camera Poses|https://arxiv.org/abs/2412.07721;Explore In-Context Segmentation via Latent Diffusion Models|https://arxiv.org/abs/2403.09616;MARS2 2025 Challenge on Multimodal Reasoning: Datasets, Methods, Results, Discussion, and Outlook|https://arxiv.org/abs/2509.14142;MOWA: Multiple-in-One Image Warping Model|https://arxiv.org/abs/2404.10716;ObjectClear: Complete Object Removal via Object-Effect Attention|https://arxiv.org/abs/2505.22636;Controllable Human-centric Keyframe Interpolation with Generative Prior|https://arxiv.org/abs/2506.03119;Light-SQ: Structure-aware Shape Abstraction with Superquadrics for Generated Meshes|https://arxiv.org/abs/2509.24986;Generalizable Implicit Motion Modeling for Video Frame Interpolation|https://arxiv.org/abs/2407.08680;EdgeTAM: On-Device Track Anything Model|https://arxiv.org/abs/2501.07256;SMPLest-X: Ultimate Scaling for Expressive Human Pose and Shape Estimation|https://arxiv.org/abs/2501.09782;DoF-Gaussian: Controllable Depth-of-Field for 3D Gaussian Splatting|https://arxiv.org/abs/2503.00746;SA-LUT: Spatial Adaptive 4D Look-Up Table for Photorealistic Style Transfer|https://arxiv.org/abs/2506.13465;DHAGrasp: Synthesizing Affordance-Aware Dual-Hand Grasps with Text Instructions|https://arxiv.org/abs/2509.22175;Harmonizing Visual Representations for Unified Multimodal Understanding and Generation|https://arxiv.org/abs/2503.21979;3DEnhancer: Consistent Multi-View Diffusion for 3D Enhancement|https://arxiv.org/abs/2412.18565;Zero-Shot Video Translation and Editing with Frame Spatial-Temporal Correspondence|https://arxiv.org/abs/2512.03905;EdgeSAM: Prompt-In-the-Loop Distillation for SAM|https://arxiv.org/abs/2312.06660;Unveiling Deep Semantic Uncertainty Perception for Language-Anchored Multi-modal Vision-Brain Alignment|https://arxiv.org/abs/2511.04078;Thinking with Camera: A Unified Multimodal Model for Camera-Centric Understanding and Generation|https://arxiv.org/abs/2510.08673;MatAnyone: Stable Video Matting with Consistent Memory Propagation|https://arxiv.org/abs/2501.14677"
MMLab@NTU,新加坡,China,南洋理工大学 (NTU),低级别视觉、图像视频理解、创意内容创作、3D场景理解与重建,研究方向包括超分辨率、3D生成式AI、深度学习、媒体取证等多个领域。,Chen Change Loy,"Efficient Diffusion Model for Image Restoration, Talk-to-Edit, 4D Panoptic Scene Graph Generation",103.680374,1.3485,https://www.mmlab-ntu.com/,Chen Change Loy,Nanyang Technological University,,"STream3R: Scalable Sequential 3D Reconstruction with Causal Transformer|https://arxiv.org/abs/2508.10893;Learning 3D Garment Animation from Trajectories of A Piece of Cloth|https://arxiv.org/abs/2501.01393;Sigma: Semantically Informative Pre-training for Skeleton-based Sign Language Understanding|https://arxiv.org/abs/2509.21223;SeedVR2: One-Step Video Restoration via Diffusion Adversarial Post-Training|https://arxiv.org/abs/2506.05301;Enhanced Generative Structure Prior for Chinese Text Image Super-resolution|https://arxiv.org/abs/2508.07537;Half-Physics: Enabling Kinematic 3D Human Model with Physical Interactions|https://arxiv.org/abs/2507.23778;GaussianAnything: Interactive Point Cloud Flow Matching For 3D Object Generation|https://arxiv.org/abs/2411.08033;OpenUni: A Simple Baseline for Unified Multimodal Understanding and Generation|https://arxiv.org/abs/2505.23661;MEAT: Multiview Diffusion Model for Human Generation on Megapixels with Mesh Attention|https://arxiv.org/abs/2503.08664;MeViS: A Multi-Modal Dataset for Referring Motion Expression Video Segmentation|https://arxiv.org/abs/2512.10945;Next Visual Granularity Generation|https://arxiv.org/abs/2508.12811;Generative Photographic Control for Scene-Consistent Video Cinematic Editing|https://arxiv.org/abs/2511.12921;SeedVR: Seeding Infinity in Diffusion Transformer Towards Generic Video Restoration|https://arxiv.org/abs/2501.01320;Omegance: A Single Parameter for Various Granularities in Diffusion-Based Synthesis|https://arxiv.org/abs/2411.17769;Denoising as Adaptation: Noise-Space Domain Adaptation for Image Restoration|https://arxiv.org/abs/2406.18516;Trans-Adapter: A Plug-and-Play Framework for Transparent Image Inpainting|https://arxiv.org/abs/2508.01098;GausSim: Foreseeing Reality by Gaussian Simulator for Elastic Objects|https://arxiv.org/abs/2412.17804;Arbitrary-steps Image Super-resolution via Diffusion Inversion|https://arxiv.org/abs/2412.09013;F-LMM: Grounding Frozen Large Multimodal Models|https://arxiv.org/abs/2406.05821;ObjCtrl-2.5D: Training-free Object Control with Camera Poses|https://arxiv.org/abs/2412.07721;Explore In-Context Segmentation via Latent Diffusion Models|https://arxiv.org/abs/2403.09616;MARS2 2025 Challenge on Multimodal Reasoning: Datasets, Methods, Results, Discussion, and Outlook|https://arxiv.org/abs/2509.14142;MOWA: Multiple-in-One Image Warping Model|https://arxiv.org/abs/2404.10716;ObjectClear: Complete Object Removal via Object-Effect Attention|https://arxiv.org/abs/2505.22636;Controllable Human-centric Keyframe Interpolation with Generative Prior|https://arxiv.org/abs/2506.03119;Light-SQ: Structure-aware Shape Abstraction with Superquadrics for Generated Meshes|https://arxiv.org/abs/2509.24986;Generalizable Implicit Motion Modeling for Video Frame Interpolation|https://arxiv.org/abs/2407.08680;EdgeTAM: On-Device Track Anything Model|https://arxiv.org/abs/2501.07256;SMPLest-X: Ultimate Scaling for Expressive Human Pose and Shape Estimation|https://arxiv.org/abs/2501.09782;DoF-Gaussian: Controllable Depth-of-Field for 3D Gaussian Splatting|https://arxiv.org/abs/2503.00746;SA-LUT: Spatial Adaptive 4D Look-Up Table for Photorealistic Style Transfer|https://arxiv.org/abs/2506.13465;DHAGrasp: Synthesizing Affordance-Aware Dual-Hand Grasps with Text Instructions|https://arxiv.org/abs/2509.22175;Harmonizing Visual Representations for Unified Multimodal Understanding and Generation|https://arxiv.org/abs/2503.21979;3DEnhancer: Consistent Multi-View Diffusion for 3D Enhancement|https://arxiv.org/abs/2412.18565;Zero-Shot Video Translation and Editing with Frame Spatial-Temporal Correspondence|https://arxiv.org/abs/2512.03905;EdgeSAM: Prompt-In-the-Loop Distillation for SAM|https://arxiv.org/abs/2312.06660;Thinking with Camera: A Unified Multimodal Model for Camera-Centric Understanding and Generation|https://arxiv.org/abs/2510.08673;MatAnyone: Stable Video Matting with Consistent Memory Propagation|https://arxiv.org/abs/2501.14677"
MReaLLab,新加坡,China,南洋理工大学 (NTU),多模态编辑、零样本模型优化、3D内容生成、场景图生成,致力于研究结合现代深度神经网络和传统符号操作的推理算法。,Hanwang Zhang,"Towards Unified Multimodal Editing, Enhancing Zero-Shot Vision Models, Vitron",103.680374,1.3485,https://mreallab.github.io/,Hanwang Zhang,Nanyang Technological University,,"Dynamic Multimodal Prototype Learning in Vision-Language Models|https://arxiv.org/abs/2507.03657;Video-of-Thought: Step-by-Step Video Reasoning from Perception to Cognition|https://arxiv.org/abs/2501.03230;HumanEdit: A High-Quality Human-Rewarded Dataset for Instruction-based Image Editing|https://arxiv.org/abs/2412.04280;Vitron: A Unified Pixel-level Vision LLM for Understanding, Generating, Segmenting, Editing|https://arxiv.org/abs/2412.19806;SGDiff: Scene Graph Guided Diffusion Model for Image Collaborative SegCaptioning|https://arxiv.org/abs/2512.01975;Debiased Fine-Tuning for Vision-language Models by Prompt Regularization|https://arxiv.org/abs/2301.12429;Two Causal Principles for Improving Visual Dialog|https://arxiv.org/abs/1911.10496;Reasoning Physical Video Generation with Diffusion Timestep Tokens via Reinforcement Learning|https://arxiv.org/abs/2504.15932;Prompt-aligned Gradient for Prompt Tuning|https://arxiv.org/abs/2205.14865;Unsupervised Visual Chain-of-Thought Reasoning via Preference Optimization|https://arxiv.org/abs/2504.18397;Learning 4D Panoptic Scene Graph Generation from Rich 2D Visual Scene|https://arxiv.org/abs/2503.15019;Generative Multimodal Pretraining with Discrete Diffusion Timestep Tokens|https://arxiv.org/abs/2504.14666;NeuSpring: Neural Spring Fields for Reconstruction and Simulation of Deformable Objects from Videos|https://arxiv.org/abs/2511.08310;Generalized Visual Relation Detection with Diffusion Models|https://arxiv.org/abs/2504.12100;Long-Tailed Classification by Keeping the Good and Removing the Bad Momentum Causal Effect|https://arxiv.org/abs/2009.12991;VistaDPO: Video Hierarchical Spatial-Temporal Direct Preference Optimization for Large Video Models|https://arxiv.org/abs/2504.13122;On Path to Multimodal Generalist: General-Level and General-Bench|https://arxiv.org/abs/2505.04620;Adaptively Clustering Neighbor Elements for Image-Text Generation|https://arxiv.org/abs/2301.01955;Hierarchical Semantic Alignment for Image Clustering|https://arxiv.org/abs/2512.00904;Personalize Your Gaussian: Consistent 3D Scene Personalization from a Single Image|https://arxiv.org/abs/2505.14537;Aligned Contrastive Loss for Long-Tailed Recognition|https://arxiv.org/abs/2506.01071;EvolveNav: Empowering LLM-Based Vision-Language Navigation via Self-Improving Embodied Reasoning|https://arxiv.org/abs/2506.01551;Seeing World Dynamics in a Nutshell|https://arxiv.org/abs/2502.03465;Learning to Animate Images from A Few Videos to Portray Delicate Human Actions|https://arxiv.org/abs/2503.00276;Distilling Parallel Gradients for Fast ODE Solvers of Diffusion Models|https://arxiv.org/abs/2507.14797;Selftok: Discrete Visual Tokens of Autoregression, by Diffusion, and for Reasoning|https://arxiv.org/abs/2505.07538;Invariant Feature Learning for Generalized Long-Tailed Classification|https://arxiv.org/abs/2207.09504;Real-Time Motion-Controllable Autoregressive Video Diffusion|https://arxiv.org/abs/2510.08131;Corvid: Improving Multimodal Large Language Models Towards Chain-of-Thought Reasoning|https://arxiv.org/abs/2507.07424;SpriteHand: Real-Time Versatile Hand-Object Interaction with Autoregressive Video Generation|https://arxiv.org/abs/2512.01960;Class Is Invariant to Context and Vice Versa: On Learning Invariance for Out-Of-Distribution Generalization|https://arxiv.org/abs/2208.03462;Generative Distribution Distillation|https://arxiv.org/abs/2507.14503;MVGamba: Unify 3D Content Generation as State Space Sequence Modeling|https://arxiv.org/abs/2406.06367;Towards Semantic Equivalence of Tokenization in Multimodal LLM|https://arxiv.org/abs/2406.05127;Generalized Kullback-Leibler Divergence Loss|https://arxiv.org/abs/2503.08038;WEAVE: Unleashing and Benchmarking the In-context Interleaved Comprehension and Generation|https://arxiv.org/abs/2511.11434;AnyEdit: Mastering Unified High-Quality Image Editing for Any Idea|https://arxiv.org/abs/2411.15738;Generative Augmented Reality: Paradigms, Technologies, and Future Applications|https://arxiv.org/abs/2511.16783;Identifying Hard Noise in Long-Tailed Sample Distribution|https://arxiv.org/abs/2207.13378;Enhancing CLIP Robustness via Cross-Modality Alignment|https://arxiv.org/abs/2510.24038;DragNeXt: Rethinking Drag-Based Image Editing|https://arxiv.org/abs/2506.07611;Project-Probe-Aggregate: Efficient Fine-Tuning for Group Robustness|https://arxiv.org/abs/2503.09487;3D Question Answering via only 2D Vision-Language Models|https://arxiv.org/abs/2505.22143;Streaming Drag-Oriented Interactive Video Manipulation: Drag Anything, Anytime!|https://arxiv.org/abs/2510.03550;Unbiased Scene Graph Generation from Biased Training|https://arxiv.org/abs/2002.11949;Ca2-VDM: Efficient Autoregressive Video Diffusion Model with Causal Generation and Cache Sharing|https://arxiv.org/abs/2411.16375;Nautilus: Locality-aware Autoencoder for Scalable Mesh Generation|https://arxiv.org/abs/2501.14317;Co-Reinforcement Learning for Unified Multimodal Understanding and Generation|https://arxiv.org/abs/2505.17534;View-Consistent 3D Editing with Gaussian Splatting|https://arxiv.org/abs/2403.11868"
ROSE Lab,新加坡,China,南洋理工大学 (NTU),对象识别与检索、视频分析、生物识别、图像/视频取证,研究方向包括利用深度学习进行对象识别，以及人脸伪造与活体检测等生物识别技术。,"Prof Kot Chichung, Alex; Prof Tan Yap Peng","Suppress and Rebalance, Flexible-Modal Deception Detection, Semantic Deep Hiding",103.680374,1.3485,https://rose1.ntu.edu.sg/,Chichung Kot; Peng Tan Yap,Nanyang Technological University,,See What You Seek: Semantic Contextual Integration for Cloth-Changing Person Re-Identification|https://arxiv.org/abs/2412.01345
Oxford Robotics Institute (ORI),英国,Europe,牛津大学,机器学习与AI、计算机视觉、制造、感知、系统工程,研究兴趣广泛，涵盖飞行、抓取、奔跑、触觉、驾驶、探索和规划等众多机器人领域。,"Eakins Daniel, Gillespie, David","Textual explanations for automated commentary driving, Motion planning in dynamic environments, EDAMS",-1.257677,51.754843,https://ori.ox.ac.uk/,Eakins Daniel; Gillespie; David,University of Oxford,,"Clinical-ComBAT: a diffusion-weighted MRI harmonization method for clinical applications|https://arxiv.org/abs/2511.04871;FLAIR-HUB: Large-scale Multimodal Dataset for Land Cover and Crop Mapping|https://arxiv.org/abs/2506.07080;A Hierarchical, Model-Based System for High-Performance Humanoid Soccer|https://arxiv.org/abs/2512.09431;Deep Learning-Based Fetal Lung Segmentation from Diffusion-weighted MRI Images and Lung Maturity Evaluation for Fetal Growth Restriction|https://arxiv.org/abs/2507.13106;Gemini: A Family of Highly Capable Multimodal Models|https://arxiv.org/abs/2312.11805;Monitoring morphometric drift in lifelong learning segmentation of the spinal cord|https://arxiv.org/abs/2505.01364;A Generic Hybrid Framework for 2D Visual Reconstruction|https://arxiv.org/abs/2501.19325;Engineering Automotive Digital Twins on Standardized Architectures: A Case Study|https://arxiv.org/abs/2508.18662;Linear Algebraic Approaches to Neuroimaging Data Compression: A Comparative Analysis of Matrix and Tensor Decomposition Methods for High-Dimensional Medical Images|https://arxiv.org/abs/2511.18197;Yummy Operations Robot Initiative: Autonomous Cooking System Utilizing a Modular Robotic Kitchen and a Dual-Arm Proprioceptive Manipulator|https://arxiv.org/abs/2405.11094;JAM: A Comprehensive Model for Age Estimation, Verification, and Comparability|https://arxiv.org/abs/2410.04012;DivShift: Exploring Domain-Specific Distribution Shifts in Large-Scale, Volunteer-Collected Biodiversity Datasets|https://arxiv.org/abs/2410.19816;Radiance Surfaces: Optimizing Surface Representations with a 5D Radiance Field Loss|https://arxiv.org/abs/2501.18627;Low-Cost 3D printed, Biocompatible Ionic Polymer Membranes for Soft Actuators|https://arxiv.org/abs/2501.12025;DINOv3 as a Frozen Encoder for CRPS-Oriented Probabilistic Rainfall Nowcasting|https://arxiv.org/abs/2511.10894;Stochastic Preconditioning for Neural Field Optimization|https://arxiv.org/abs/2505.20473;SCIsegV2: A Universal Tool for Segmentation of Intramedullary Lesions in Spinal Cord Injury|https://arxiv.org/abs/2407.17265;ComBAT Harmonization for diffusion MRI: Challenges and Best Practices|https://arxiv.org/abs/2505.14722;RoboCAP: Robotic Classification and Precision Pouring of Diverse Liquids and Granular Media with Capacitive Sensing|https://arxiv.org/abs/2405.07423;OPFormer: Object Pose Estimation leveraging foundation model with geometric encoding|https://arxiv.org/abs/2511.12614;Can Text-to-Image Generative Models Accurately Depict Age? A Comparative Study on Synthetic Portrait Generation and Age Estimation|https://arxiv.org/abs/2502.03420;Deep Neural Encoder-Decoder Model to Relate fMRI Brain Activity with Naturalistic Stimuli|https://arxiv.org/abs/2507.12009;Redesigning Traffic Signs to Mitigate Machine-Learning Patch Attacks|https://arxiv.org/abs/2402.04660;CrossVideoMAE: Self-Supervised Image-Video Representation Learning with Masked Autoencoders|https://arxiv.org/abs/2502.07811;While recognizing actions, LMMs struggle to detect core interaction events|https://arxiv.org/abs/2511.20162;GEN3C: 3D-Informed World-Consistent Video Generation with Precise Camera Control|https://arxiv.org/abs/2503.03751"
多智能体与具身智能研究所,中国,China,鹏城实验室,"多模态感知与生成, 智能体任务规划, 多智能体协作, 具身智能体控制",重点突破智能体视角下的多模态感知、规划、协作、控制等方向，支撑多场景产业应用。,林倞,"Affordances-Oriented Planning using Foundation Models, NavCoT, MapGPT, Surfer",114.057939,22.605285,https://imaei.github.io/,Jing Lin,Peng Cheng Laboratory,,"TIMERIPPLE: Accelerating vDiTs by Understanding the Spatio-Temporal Correlations in Latent Space|https://arxiv.org/abs/2511.12035;The Quest for Generalizable Motion Generation: Data, Model, and Evaluation|https://arxiv.org/abs/2510.26794;Motion-X++: A Large-Scale Multimodal 3D Whole-body Human Motion Dataset|https://arxiv.org/abs/2501.05098;Seed3D 1.0: From Images to High-Fidelity Simulation-Ready 3D Assets|https://arxiv.org/abs/2510.19944;BinaryHPE: 3D Human Pose and Shape Estimation via Binarization|https://arxiv.org/abs/2311.14323;HumanMM: Global Human Motion Recovery from Multi-shot Videos|https://arxiv.org/abs/2503.07597;RainFusion: Adaptive Video Generation Acceleration via Multi-Dimensional Visual Redundancy|https://arxiv.org/abs/2505.21036;SkillMimic: Learning Basketball Interaction Skills from Demonstrations|https://arxiv.org/abs/2408.15270;Astraea: A Token-wise Acceleration Framework for Video Diffusion Transformers|https://arxiv.org/abs/2506.05096;Towards Fine-Grained Human Motion Video Captioning|https://arxiv.org/abs/2510.24767;SDQM: Synthetic Data Quality Metric for Object Detection Dataset Evaluation|https://arxiv.org/abs/2510.06596;Studying Various Activation Functions and Non-IID Data for Machine Learning Model Robustness|https://arxiv.org/abs/2512.04264;DPoser-X: Diffusion Model as Robust 3D Whole-body Human Pose Prior|https://arxiv.org/abs/2508.00599;ChatHuman: Chatting about 3D Humans with Tools|https://arxiv.org/abs/2405.04533"
MARS多模态学习实验室,中国,China,清华大学,"多媒体计算, 自动驾驶, 机器人, 多传感器",由赵行教授组建和指导，隶属清华大学交叉信息院。研究如何让机器像人一样通过多种感知输入进行学习、推理和交互。,赵行,"DriveVLM, A Universal Semantic-Geometric Representation for Robotic Manipulation, Occ3D, Robot Parkour Learning",116.326759,40.003304,https://group.iiis.tsinghua.edu.cn/~marslab/#/,Xing Zhao,Tsinghua University,,Iterative approach to reconstructing neural disparity fields from light-field data|https://arxiv.org/abs/2407.15380;Physics-Inspired Gaussian Kolmogorov-Arnold Networks for X-ray Scatter Correction in Cone-Beam CT|https://arxiv.org/abs/2510.24579
智能产业研究院 (AIR),中国,China,清华大学,"智慧交通, 智慧物联, 智慧医疗, 大数据智能, 智能机器人",面向第四次工业革命的国际化、智能化、产业化的研究机构，推动技术落地。,"张亚勤, 马维英, 赵峰","DecisionNCE, Instruction-Guided Visual Masking, Evolution of Future Medical AI Models, ESM All-Atom",116.326759,40.003304,https://air.tsinghua.edu.cn/,Yaqin Zhang; Weiying Ma; Feng Zhao,Tsinghua University,,V2P-Bench: Evaluating Video-Language Understanding with Visual Prompts for Better Human-Model Interaction|https://arxiv.org/abs/2503.17736;Adaptive Dropout: Unleashing Dropout across Layers for Generalizable Image Super-Resolution|https://arxiv.org/abs/2506.12738;VCR-Bench: A Comprehensive Evaluation Framework for Video Chain-of-Thought Reasoning|https://arxiv.org/abs/2504.07956;Highly Efficient Test-Time Scaling for T2I Diffusion Models with Text Embedding Perturbation|https://arxiv.org/abs/2512.03996;STAGE: Stable and Generalizable GRPO for Autoregressive Image Generation|https://arxiv.org/abs/2509.25027;Unlock Reliable Skill Inference for Quadruped Adaptive Behavior by Skill Graph|https://arxiv.org/abs/2311.06015;Navigating Image Restoration with VAR's Distribution Alignment Prior|https://arxiv.org/abs/2412.21063;AnySplat: Feed-forward 3D Gaussian Splatting from Unconstrained Views|https://arxiv.org/abs/2505.23716;DualVLA: Building a Generalizable Embodied Agent via Partial Decoupling of Reasoning and Action|https://arxiv.org/abs/2511.22134;Unleashing the Potential of the Semantic Latent Space in Diffusion Models for Image Dehazing|https://arxiv.org/abs/2509.20091;Unifying Biomedical Vision-Language Expertise: Towards a Generalist Foundation Model via Multi-CLIP Knowledge Distillation|https://arxiv.org/abs/2506.22567;ParaUni: Enhance Generation in Unified Multimodal Model with Reinforcement-driven Hierarchical Parallel Information Interaction|https://arxiv.org/abs/2512.05422;VideoMAR: Autoregressive Video Generatio with Continuous Tokens|https://arxiv.org/abs/2506.14168;CronusVLA: Towards Efficient and Robust Manipulation via Multi-Frame Vision-Language-Action Modeling|https://arxiv.org/abs/2506.19816;Temperature calibration of surface emissivities with an improved thermal image enhancement network|https://arxiv.org/abs/2506.16803;Structural and Statistical Texture Knowledge Distillation and Learning for Segmentation|https://arxiv.org/abs/2503.08043;Single-Reference Text-to-Image Manipulation with Dual Contrastive Denoising Score|https://arxiv.org/abs/2508.12718;InstructVLA: Vision-Language-Action Instruction Tuning from Understanding to Manipulation|https://arxiv.org/abs/2507.17520;Group Critical-token Policy Optimization for Autoregressive Image Generation|https://arxiv.org/abs/2509.22485;DRARL: Disengagement-Reason-Augmented Reinforcement Learning for Efficient Improvement of Autonomous Driving Policy|https://arxiv.org/abs/2506.16720;FreePCA: Integrating Consistency Information across Long-short Frames in Training-free Long Video Generation via Principal Component Analysis|https://arxiv.org/abs/2505.01172;Towards Better & Faster Autoregressive Image Generation: From the Perspective of Entropy|https://arxiv.org/abs/2510.09012;Frequency-Aware Vision-Language Multimodality Generalization Network for Remote Sensing Image Classification|https://arxiv.org/abs/2511.10774;FR-TTS: Test-Time Scaling for NTP-based Image Generation with Effective Filling-based Reward Signal|https://arxiv.org/abs/2512.00438;DualFast: Dual-Speedup Framework for Fast Sampling of Diffusion Models|https://arxiv.org/abs/2506.13058;Gaussian Variation Field Diffusion for High-fidelity Video-to-4D Synthesis|https://arxiv.org/abs/2507.23785;VRAG-RL: Empower Vision-Perception-Based RAG for Visually Rich Information Understanding via Iterative Reasoning with Reinforcement Learning|https://arxiv.org/abs/2505.22019;FreeDNA: Endowing Domain Adaptation of Diffusion-Based Dense Prediction with Training-Free Domain Noise Alignment|https://arxiv.org/abs/2506.22509;Token Painter: Training-Free Text-Guided Image Inpainting via Mask Autoregressive Models|https://arxiv.org/abs/2509.23919;ViDoRAG: Visual Document Retrieval-Augmented Generation via Dynamic Iterative Reasoning Agents|https://arxiv.org/abs/2502.18017;FEB-Cache: Frequency-Guided Exposure Bias Reduction for Enhancing Diffusion Transformer Caching|https://arxiv.org/abs/2503.07120;WaterWave: Bridging Underwater Image Enhancement into Video Streams via Wavelet-based Temporal Consistency Field|https://arxiv.org/abs/2512.05492;Extended monocular 3D imaging|https://arxiv.org/abs/2502.07403;SPA-VL: A Comprehensive Safety Preference Alignment Dataset for Vision Language Model|https://arxiv.org/abs/2406.12030;Frequency Autoregressive Image Generation with Continuous Tokens|https://arxiv.org/abs/2503.05305;InVDriver: Intra-Instance Aware Vectorized Query-Based Autonomous Driving Transformer|https://arxiv.org/abs/2502.17949;InfoScale: Unleashing Training-free Variable-scaled Image Generation via Effective Utilization of Information|https://arxiv.org/abs/2509.01421;SEA: Supervised Embedding Alignment for Token-Level Visual-Textual Integration in MLLMs|https://arxiv.org/abs/2408.11813;Fast-ARDiff: An Entropy-informed Acceleration Framework for Continuous Space Autoregressive Generation|https://arxiv.org/abs/2512.08537
智能系统与机器人实验室 (ISR Lab),中国,China,清华大学,"机器人技术, 强化学习, 大型语言模型",由陈建宇教授创立，隶属于清华大学跨学科信息科学研究所，核心目标是研发高性能、高智能的先进机器人系统。,陈建宇,"DoReMi, Decentralized Motor Skill Learning for Complex Robotic Systems, Asking Before Acting, Learning Robust, Agile, Natural Legged Locomotion Skills in the Wild",116.326759,40.003304,https://group.iiis.tsinghua.edu.cn/~isrlab/,Jianyu Chen,Tsinghua University,,Ctrl-World: A Controllable Generative World Model for Robot Manipulation|https://arxiv.org/abs/2510.10125;Video Prediction Policy: A Generalist Robot Policy with Predictive Visual Representations|https://arxiv.org/abs/2412.14803;Whleaper: A 10-DOF Flexible Bipedal Wheeled Robot|https://arxiv.org/abs/2504.21767;Learning Generalizable Robot Policy with Human Demonstration Video as a Prompt|https://arxiv.org/abs/2505.20795;Efficient and Generalized end-to-end Autonomous Driving System with Latent Deep Reinforcement Learning and Demonstrations|https://arxiv.org/abs/2401.11792;AIF-SFDA: Autonomous Information Filter-driven Source-Free Domain Adaptation for Medical Image Segmentation|https://arxiv.org/abs/2501.03074;UP-VLA: A Unified Understanding and Prediction Model for Embodied Agent|https://arxiv.org/abs/2501.18867;UniCoD: Enhancing Robot Policy via Unified Continuous and Discrete Representation Learning|https://arxiv.org/abs/2510.10642;Improving Vision-Language-Action Model with Online Reinforcement Learning|https://arxiv.org/abs/2501.16664;villa-X: Enhancing Latent Action Modeling in Vision-Language-Action Models|https://arxiv.org/abs/2507.23682;A Contact-Safe Reinforcement Learning Framework for Contact-Rich Robot Manipulation|https://arxiv.org/abs/2207.13438;HiRT: Enhancing Robotic Control with Hierarchical Robot Transformers|https://arxiv.org/abs/2410.05273
智能技术与系统实验室,中国,China,清华大学,机器人、强化学习、策略优化,隶属于清华大学计算机系，进行人工智能与机器人相关的研究。,"刘华平, 陈挺, 冯元","DGPO, Embodied multi-agent task planning from ambiguous instruction, Continual learning with recursive gradient optimization",116.326759,40.003304,http://www.thuir.cn/,Huaping Liu; Ting Chen; Yuan Feng,Tsinghua University,,"TEM^3-Learning: Time-Efficient Multimodal Multi-Task Learning for Advanced Assistive Driving|https://arxiv.org/abs/2506.18084;Isaac Lab: A GPU-Accelerated Simulation Framework for Multi-Modal Robot Learning|https://arxiv.org/abs/2511.04831;Learning To Defer To A Population With Limited Demonstrations|https://arxiv.org/abs/2510.19351;Team NYCU at Defactify4: Robust Detection and Source Identification of AI-Generated Images Using CNN and CLIP-Based Models|https://arxiv.org/abs/2503.10718;Trajectory Entropy Reinforcement Learning for Predictable and Robust Control|https://arxiv.org/abs/2505.04193;VividMed: Vision Language Model with Versatile Visual Grounding for Medicine|https://arxiv.org/abs/2410.12694;Conditional Diffusion Model for Electrical Impedance Tomography|https://arxiv.org/abs/2501.05769;ArticuBot: Learning Universal Articulated Object Manipulation Policy via Large Scale Simulation|https://arxiv.org/abs/2503.03045;3rd Workshop on Maritime Computer Vision (MaCVi) 2025: Challenge Results|https://arxiv.org/abs/2501.10343;Translating Electrocardiograms to Cardiac Magnetic Resonance Imaging Useful for Cardiac Assessment and Disease Screening: A Multi-Center Study AI for ECG to CMR Translation Study|https://arxiv.org/abs/2411.13602;AssistantX: An LLM-Powered Proactive Assistant in Collaborative Human-Populated Environment|https://arxiv.org/abs/2409.17655;CrossRay3D: Geometry and Distribution Guidance for Efficient Multimodal 3D Detection|https://arxiv.org/abs/2510.15991;PCDreamer: Point Cloud Completion Through Multi-view Diffusion Priors|https://arxiv.org/abs/2411.19036;Social Robots for People with Dementia: A Literature Review on Deception from Design to Perception|https://arxiv.org/abs/2507.00963;S2R-Bench: A Sim-to-Real Evaluation Benchmark for Autonomous Driving|https://arxiv.org/abs/2505.18631;Towards Task-Oriented Flying: Framework, Infrastructure, and Principles|https://arxiv.org/abs/2504.15129;Transforming Monolithic Foundation Models into Embodied Multi-Agent Architectures for Human-Robot Collaboration|https://arxiv.org/abs/2512.00797;Multi Source COVID-19 Detection via Kernel-Density-based Slice Sampling|https://arxiv.org/abs/2507.01564;Learning Representations of Satellite Images with Evaluations on Synoptic Weather Events|https://arxiv.org/abs/2508.06122;GRITS: A Spillage-Aware Guided Diffusion Policy for Robot Food Scooping Tasks|https://arxiv.org/abs/2510.00573;TimePre: Bridging Accuracy, Efficiency, and Stability in Probabilistic Time-Series Forecasting|https://arxiv.org/abs/2511.18539;Towards Generalist Robot Policies: What Matters in Building Vision-Language-Action Models|https://arxiv.org/abs/2412.14058;FlowDreamer: A RGB-D World Model with Flow-based Motion Representations for Robot Manipulation|https://arxiv.org/abs/2505.10075;VQualA 2025 Challenge on Face Image Quality Assessment: Methods and Results|https://arxiv.org/abs/2508.18445;Potential Field as Scene Affordance for Behavior Change-Based Visual Risk Object Identification|https://arxiv.org/abs/2409.15846;What Changed and What Could Have Changed? State-Change Counterfactuals for Procedure-Aware Video Representation Learning|https://arxiv.org/abs/2503.21055;Mitigating Cross-Modal Distraction and Ensuring Geometric Feasibility via Affordance-Guided and Self-Consistent MLLMs for Task Planning in Instruction-Following Manipulation|https://arxiv.org/abs/2503.13055;DiffIR2VR-Zero: Zero-Shot Video Restoration with Diffusion-based Image Restoration Models|https://arxiv.org/abs/2407.01519;CLOC: Contrastive Learning for Ordinal Classification with Multi-Margin N-pair Loss|https://arxiv.org/abs/2504.17813;OLMD: Orientation-aware Long-term Motion Decoupling for Continuous Sign Language Recognition|https://arxiv.org/abs/2503.08205;A High-frequency, Interaction-induced Pneumatic Oscillator Enabling Versatile Soft Robotics|https://arxiv.org/abs/2411.07588;Dino-Diffusion Modular Designs Bridge the Cross-Domain Gap in Autonomous Parking|https://arxiv.org/abs/2510.20335;Investigation of intelligent barbell squat coaching system based on computer vision and machine learning|https://arxiv.org/abs/2503.23731;UrbanIR: Large-Scale Urban Scene Inverse Rendering from a Single Video|https://arxiv.org/abs/2306.09349;OUGS: Active View Selection via Object-aware Uncertainty Estimation in 3DGS|https://arxiv.org/abs/2511.09397;MMTL-UniAD: A Unified Framework for Multimodal and Multi-Task Learning in Assistive Driving Perception|https://arxiv.org/abs/2504.02264;EgoVIS@CVPR: What Changed and What Could Have Changed? State-Change Counterfactuals for Procedure-Aware Video Representation Learning|https://arxiv.org/abs/2506.00101;Toward Real-world BEV Perception: Depth Uncertainty Estimation via Gaussian Splatting|https://arxiv.org/abs/2504.01957;DiffVQA: Video Quality Assessment Using Diffusion Feature Extractor|https://arxiv.org/abs/2505.03261;Unpaired Deblurring via Decoupled Diffusion Model|https://arxiv.org/abs/2502.01522;ATARS: An Aerial Traffic Atomic Activity Recognition and Temporal Segmentation Dataset|https://arxiv.org/abs/2503.18553;Controllable Collision Scenario Generation via Collision Pattern Prediction|https://arxiv.org/abs/2510.12206;CollabVLA: Self-Reflective Vision-Language-Action Model Dreaming Together with Human|https://arxiv.org/abs/2509.14889;Track Any Motions under Any Disturbances|https://arxiv.org/abs/2509.13833;Bridging Diffusion Models and 3D Representations: A 3D Consistent Super-Resolution Framework|https://arxiv.org/abs/2508.04090;What Foundation Models can Bring for Robot Learning in Manipulation : A Survey|https://arxiv.org/abs/2404.18201;Exploring Probabilistic Modeling Beyond Domain Generalization for Semantic Segmentation|https://arxiv.org/abs/2507.21367;A VLM-based Method for Visual Anomaly Detection in Robotic Scientific Laboratories|https://arxiv.org/abs/2506.05405;VICtoR: Learning Hierarchical Vision-Instruction Correlation Rewards for Long-horizon Manipulation|https://arxiv.org/abs/2405.16545;Task-Oriented Human Grasp Synthesis via Context- and Task-Aware Diffusers|https://arxiv.org/abs/2507.11287;RC-AutoCalib: An End-to-End Radar-Camera Automatic Calibration Network|https://arxiv.org/abs/2505.22427;Taming Domain Shift in Multi-source CT-Scan Classification via Input-Space Standardization|https://arxiv.org/abs/2507.19858;Affordance-Guided Coarse-to-Fine Exploration for Base Placement in Open-Vocabulary Mobile Manipulation|https://arxiv.org/abs/2511.06240;DESign: Dynamic Context-Aware Convolution and Efficient Subnet Regularization for Continuous Sign Language Recognition|https://arxiv.org/abs/2507.03339;Observe Then Act: Asynchronous Active Vision-Action Model for Robotic Manipulation|https://arxiv.org/abs/2409.14891"
具身智能实验室 (TEA Lab),中国,China,清华大学,触觉感知、触觉驱动的机器人操作、触觉交互界面设计,致力于通过触觉感知和交互技术来增强机器人的自主性和灵活性。,许华哲,"MENTOR, Catch It!, Learning to Manipulate Anywhere",116.326759,40.003304,https://iiis.tsinghua.edu.cn/kxyj/ktzjs/qhdxjsznsys.htm,Huazhe Xu,Tsinghua University,,"FACET: Force-Adaptive Control via Impedance Reference Tracking for Legged Robots|https://arxiv.org/abs/2505.06883;Two by Two: Learning Multi-Task Pairwise Objects Assembly for Generalizable Robot Manipulation|https://arxiv.org/abs/2504.06961;H$^3$DP: Triply-Hierarchical Diffusion Policy for Visuomotor Learning|https://arxiv.org/abs/2505.07819;ArrayBot: Reinforcement Learning for Generalizable Distributed Manipulation through Touch|https://arxiv.org/abs/2306.16857;Galaxea Open-World Dataset and G0 Dual-System VLA Model|https://arxiv.org/abs/2509.00576;DA-MMP: Learning Coordinated and Accurate Throwing with Dynamics-Aware Motion Manifold Primitives|https://arxiv.org/abs/2509.23721;DittoGym: Learning to Control Soft Shape-Shifting Robots|https://arxiv.org/abs/2401.13231;Morpheus: A Neural-driven Animatronic Face with Hybrid Actuation and Diverse Emotion Control|https://arxiv.org/abs/2507.16645;Embodied Arena: A Comprehensive, Unified, and Evolving Evaluation Platform for Embodied AI|https://arxiv.org/abs/2509.15273;RoboDuet: Learning a Cooperative Policy for Whole-body Legged Loco-Manipulation|https://arxiv.org/abs/2403.17367;Reactive Diffusion Policy: Slow-Fast Visual-Tactile Policy Learning for Contact-Rich Manipulation|https://arxiv.org/abs/2503.02881;DemoSpeedup: Accelerating Visuomotor Policies via Entropy-Guided Demonstration Acceleration|https://arxiv.org/abs/2506.05064;4D Visual Pre-training for Robot Learning|https://arxiv.org/abs/2508.17230;MoE-DP: An MoE-Enhanced Diffusion Policy for Robust Long-Horizon Robotic Manipulation with Skill Decomposition and Failure Recovery|https://arxiv.org/abs/2511.05007;DOGlove: Dexterous Manipulation with a Low-Cost Open-Source Haptic Force Feedback Glove|https://arxiv.org/abs/2502.07730;Scaling Laws in Scientific Discovery with AI and Robot Scientists|https://arxiv.org/abs/2503.22444;Fine-Tuning Hard-to-Simulate Objectives for Quadruped Locomotion: A Case Study on Total Power Saving|https://arxiv.org/abs/2502.10956;MENTOR: Mixture-of-Experts Network with Task-Oriented Perturbation for Visual Reinforcement Learning|https://arxiv.org/abs/2410.14972;DemoGen: Synthetic Demonstration Generation for Data-Efficient Visuomotor Policy Learning|https://arxiv.org/abs/2502.16932;RL-100: Performant Robotic Manipulation with Real-World Reinforcement Learning|https://arxiv.org/abs/2510.14830;HERMES: Human-to-Robot Embodied Learning from Multi-Source Motion Data for Mobile Dexterous Manipulation|https://arxiv.org/abs/2508.20085"
具身视觉与机器人实验室 (EVAR Lab),中国,China,清华大学,具身智能、计算机视觉、强化学习,隶属清华大学交叉信息院，致力于打造通用的具身智能框架。,高阳,"COPA, RLFP, ATM",116.326759,40.003304,https://iiis.tsinghua.edu.cn/kxyj/ktzjs/sjyjqrsys_VAR_.htm,Yang Gao,Tsinghua University,,"FACET: Force-Adaptive Control via Impedance Reference Tracking for Legged Robots|https://arxiv.org/abs/2505.06883;E2MPL:An Enduring and Efficient Meta Prompt Learning Framework for Few-shot Unsupervised Domain Adaptation|https://arxiv.org/abs/2407.04066;Prior Reinforce: Mastering Agile Tasks with Limited Trials|https://arxiv.org/abs/2505.21916;MotionTrans: Human VR Data Enable Motion-Level Learning for Robotic Manipulation Policies|https://arxiv.org/abs/2509.17759;RoboHiMan: A Hierarchical Evaluation Paradigm for Compositional Generalization in Long-Horizon Manipulation|https://arxiv.org/abs/2510.13149;Learning Manipulation Skills through Robot Chain-of-Thought with Sparse Failure Guidance|https://arxiv.org/abs/2405.13573;FP3: A 3D Foundation Policy for Robotic Manipulation|https://arxiv.org/abs/2503.08950;GravMAD: Grounded Spatial Value Maps Guided Action Diffusion for Generalized 3D Manipulation|https://arxiv.org/abs/2409.20154;Robust Dataset Distillation by Matching Adversarial Trajectories|https://arxiv.org/abs/2503.12069;SKIL: Semantic Keypoint Imitation Learning for Generalizable Data-efficient Manipulation|https://arxiv.org/abs/2501.14400;Video2Policy: Scaling up Manipulation Tasks in Simulation through Internet Videos|https://arxiv.org/abs/2502.09886;Stable Video Infinity: Infinite-Length Video Generation with Error Recycling|https://arxiv.org/abs/2510.09212;Towards Perfection: Building Inter-component Mutual Correction for Retinex-based Low-light Image Enhancement|https://arxiv.org/abs/2508.09009;Self-Supervised Monocular 4D Scene Reconstruction for Egocentric Videos|https://arxiv.org/abs/2411.09145;Mamba-Sea: A Mamba-based Framework with Global-to-Local Sequence Augmentation for Generalizable Medical Image Segmentation|https://arxiv.org/abs/2504.17515;Diversity-enhanced Collaborative Mamba for Semi-supervised Medical Image Segmentation|https://arxiv.org/abs/2508.13712;3D Gaussian Splatting: Survey, Technologies, Challenges, and Opportunities|https://arxiv.org/abs/2407.17418;Data Scaling Laws in Imitation Learning for Robotic Manipulation|https://arxiv.org/abs/2410.18647;GateFuseNet: An Adaptive 3D Multimodal Neuroimaging Fusion Network for Parkinson's Disease Diagnosis|https://arxiv.org/abs/2510.22507;Adapt Your Body: Mitigating Proprioception Shifts in Imitation Learning|https://arxiv.org/abs/2506.23944;Embodied Arena: A Comprehensive, Unified, and Evolving Evaluation Platform for Embodied AI|https://arxiv.org/abs/2509.15273;Unleashing the Power of Intermediate Domains for Mixed Domain Semi-Supervised Medical Image Segmentation|https://arxiv.org/abs/2505.24567;OmniTraj: Pre-Training on Heterogeneous Data for Adaptive and Zero-Shot Human Trajectory Prediction|https://arxiv.org/abs/2507.23657;OneTwoVLA: A Unified Vision-Language-Action Model with Adaptive Reasoning|https://arxiv.org/abs/2505.11917;Unified Human Localization and Trajectory Prediction with Monocular Vision|https://arxiv.org/abs/2503.03535;An Adaptor for Triggering Semi-Supervised Learning to Out-of-Box Serve Deep Image Clustering|https://arxiv.org/abs/2509.20976;Best of Sim and Real: Decoupled Visuomotor Manipulation via Learning Control in Simulation and Perception in Real|https://arxiv.org/abs/2509.25747;Balancing Multi-Target Semi-Supervised Medical Image Segmentation with Collaborative Generalist and Specialists|https://arxiv.org/abs/2504.00862;SEA: Semantic Map Prediction for Active Exploration of Uncertain Areas|https://arxiv.org/abs/2510.19766;TimeRewarder: Learning Dense Reward from Passive Videos via Frame-wise Temporal Distance|https://arxiv.org/abs/2509.26627;Do You Need Proprioceptive States in Visuomotor Policies?|https://arxiv.org/abs/2509.18644;HuB: Learning Extreme Humanoid Balance|https://arxiv.org/abs/2505.07294;Distractor-free Generalizable 3D Gaussian Splatting|https://arxiv.org/abs/2411.17605;Stitching, Fine-tuning, Re-training: A SAM-enabled Framework for Semi-supervised 3D Medical Image Segmentation|https://arxiv.org/abs/2403.11229;RoboEngine: Plug-and-Play Robot Data Augmentation with Semantic Robot Segmentation and Background Generation|https://arxiv.org/abs/2503.18738;Tactile-VLA: Unlocking Vision-Language-Action Model's Physical Knowledge for Tactile Generalization|https://arxiv.org/abs/2507.09160;KineDex: Learning Tactile-Informed Visuomotor Policies via Kinesthetic Teaching for Dexterous Manipulation|https://arxiv.org/abs/2505.01974;START: A Generalized State Space Model with Saliency-Driven Token-Aware Transformation|https://arxiv.org/abs/2410.16020;RoboHorizon: An LLM-Assisted Multi-View World Model for Long-Horizon Robotic Manipulation|https://arxiv.org/abs/2501.06605;LaViRA: Language-Vision-Robot Actions Translation for Zero-Shot Vision Language Navigation in Continuous Environments|https://arxiv.org/abs/2510.19655;Fine-Tuning Hard-to-Simulate Objectives for Quadruped Locomotion: A Case Study on Total Power Saving|https://arxiv.org/abs/2502.10956;Adapting In-Domain Few-Shot Segmentation to New Domains without Retraining|https://arxiv.org/abs/2504.21414;SEPT: Standard-Definition Map Enhanced Scene Perception and Topology Reasoning for Autonomous Driving|https://arxiv.org/abs/2505.12246;RAP: 3D Rasterization Augmented End-to-End Planning|https://arxiv.org/abs/2510.04333;Spatially anchored Tactile Awareness for Robust Dexterous Manipulation|https://arxiv.org/abs/2510.14647;VITA-VLA: Efficiently Teaching Vision-Language Models to Act via Action Expert Distillation|https://arxiv.org/abs/2510.09607;Social-Pose: Enhancing Trajectory Prediction with Human Body Pose|https://arxiv.org/abs/2507.22742;DeCo: Task Decomposition and Skill Composition for Zero-Shot Generalization in Long-Horizon 3D Manipulation|https://arxiv.org/abs/2505.00527;Macro-from-Micro Planning for High-Quality and Parallelized Autoregressive Long Video Generation|https://arxiv.org/abs/2508.03334;Intern-S1: A Scientific Multimodal Foundation Model|https://arxiv.org/abs/2508.15763;EasyInsert: A Data-Efficient and Generalizable Insertion Policy|https://arxiv.org/abs/2505.16187;Advancing Conversational Diagnostic AI with Multimodal Reasoning|https://arxiv.org/abs/2505.04653"
机器人控制实验室,中国,China,清华大学,机器人控制、类脑计算、仿人机器人步态控制,依托于清华大学自动化系，研究方向集中在机器人控制技术。,赵明国,"Multi-pathway HSN, hybrid-paradigm Tianjic chip, enhanced CP controller",116.326759,40.003304,https://www.sigs.tsinghua.edu.cn/2025/0616/c8179a275636/page.psp,Mingguo Zhao,Tsinghua University,,Booster Gym: An End-to-End Reinforcement Learning Framework for Humanoid Robot Locomotion|https://arxiv.org/abs/2506.15132;Learning Vision-Driven Reactive Soccer Skills for Humanoid Robots|https://arxiv.org/abs/2511.03996;CBMC-V3: A CNS-inspired Control Framework Towards Manipulation Agility with SNN|https://arxiv.org/abs/2511.04109;AutoOdom: Learning Auto-regressive Proprioceptive Odometry for Legged Locomotion|https://arxiv.org/abs/2511.18857;Preference-Conditioned Multi-Objective RL for Integrated Command Tracking and Force Compliance in Humanoid Locomotion|https://arxiv.org/abs/2510.10851;HiFAR: Multi-Stage Curriculum Learning for High-Dynamics Humanoid Fall Recovery|https://arxiv.org/abs/2502.20061
IWIN-FINS实验室,中国,China,上海交通大学,"移动机器人, 机器学习, 控制和优化, 开发分布式、安全智能系统",专注于为移动机器人、机器学习、控制和优化开发分布式、安全智能系统。,何建平,"Unidentifiability of System Dynamics, Observation-based Optimal Control Law Learning with LQR Reconstruction, Multi-Robot Stochastic Patrolling",121.42975,31.02067,https://iwin-fins.com/,Jianping He,Shanghai Jiao Tong University,,"Formation Maneuver Control Based on the Augmented Laplacian Method|https://arxiv.org/abs/2505.05795;Aucamp: An Underwater Camera-Based Multi-Robot Platform with Low-Cost, Distributed, and Robust Localization|https://arxiv.org/abs/2506.09876;Stochastic Trajectory Optimization for Robotic Skill Acquisition From a Suboptimal Demonstration|https://arxiv.org/abs/2408.03131;AquaGS: Fast Underwater Scene Reconstruction with SfM-Free Gaussian Splatting|https://arxiv.org/abs/2505.01799"
智能机器人与机器视觉(IRMV)实验室,中国,China,上海交通大学,"视觉伺服, 自动驾驶, 软体机器人, 强化学习控制等",旨在在具有挑战性的非结构化环境下通过视觉感知实现通用机器人自动化和机器智能。,"王贺升, 刘哲, 徐璠","Cam4DOcc, DifFlow3D, 3DSFLabelling, Cognitive Navigation for Intelligent Mobile Robots",121.42975,31.02067,https://irmv.sjtu.edu.cn/cn,Hesheng Wang; Zhe Liu; Fan Xu,Shanghai Jiao Tong University,,"Incremental Joint Learning of Depth, Pose and Implicit Scene Representation on Monocular Camera in Large-scale Scenes|https://arxiv.org/abs/2404.06050;RationalVLA: A Rational Vision-Language-Action Model with Dual System|https://arxiv.org/abs/2506.10826;LONG3R: Long Sequence Streaming 3D Reconstruction|https://arxiv.org/abs/2507.18255;Learning a General Model: Folding Clothing with Topological Dynamics|https://arxiv.org/abs/2504.20720;Physics-informed generative real-time lens-free imaging|https://arxiv.org/abs/2403.07786;Adaptive Label Correction for Robust Medical Image Segmentation with Noisy Labels|https://arxiv.org/abs/2503.12218;Zero-Shot Temporal Interaction Localization for Egocentric Videos|https://arxiv.org/abs/2506.03662;SGLoc: Semantic Localization System for Camera Pose Estimation from 3D Gaussian Splatting Representation|https://arxiv.org/abs/2507.12027;PastNet: Introducing Physical Inductive Biases for Spatio-temporal Video Prediction|https://arxiv.org/abs/2305.11421;Development of a 15-Degree-of-Freedom Bionic Hand with Cable-Driven Transmission and Distributed Actuation|https://arxiv.org/abs/2512.04399;CLIMD: A Curriculum Learning Framework for Imbalanced Multimodal Diagnosis|https://arxiv.org/abs/2508.01594;Hierarchical Corpus-View-Category Refinement for Carotid Plaque Risk Grading in Ultrasound|https://arxiv.org/abs/2506.23108;Cam4DOcc: Benchmark for Camera-Only 4D Occupancy Forecasting in Autonomous Driving Applications|https://arxiv.org/abs/2311.17663;EgoLoc: A Generalizable Solution for Temporal Interaction Localization in Egocentric Videos|https://arxiv.org/abs/2508.12349;SemAlign3D: Semantic Correspondence between RGB-Images through Aligning 3D Object-Class Representations|https://arxiv.org/abs/2503.22462;DynCIM: Dynamic Curriculum for Imbalanced Multimodal Learning|https://arxiv.org/abs/2503.06456;Trusted Unified Feature-Neighborhood Dynamics for Multi-View Classification|https://arxiv.org/abs/2409.00755;MCN-SLAM: Multi-Agent Collaborative Neural SLAM with Hybrid Implicit Neural Scene Representation|https://arxiv.org/abs/2506.18678;D2IP: Deep Dynamic Image Prior for 3D Time-sequence Pulmonary Impedance Imaging|https://arxiv.org/abs/2507.14046;MUT3R: Motion-aware Updating Transformer for Dynamic 3D Reconstruction|https://arxiv.org/abs/2512.03939;NeRFs in Robotics: A Survey|https://arxiv.org/abs/2405.01333;DIAL-GS: Dynamic Instance Aware Reconstruction for Label-free Street Scenes with 4D Gaussian Splatting|https://arxiv.org/abs/2511.06632;MovSAM: A Single-image Moving Object Segmentation Framework Based on Deep Thinking|https://arxiv.org/abs/2504.06863;D$^2$GSLAM: 4D Dynamic Gaussian Splatting SLAM|https://arxiv.org/abs/2512.09411;Wonder Wins Ways: Curiosity-Driven Exploration through Multi-Agent Contextual Calibration|https://arxiv.org/abs/2509.20648;Improving Integrated Gradient-based Transferable Adversarial Examples by Refining the Integration Path|https://arxiv.org/abs/2412.18844;MADiff: Motion-Aware Mamba Diffusion Models for Hand Trajectory Prediction on Egocentric Videos|https://arxiv.org/abs/2409.02638;QuantEIT: Ultra-Lightweight Quantum-Assisted Inference for Chest Electrical Impedance Tomography|https://arxiv.org/abs/2507.14031;MID: A Self-supervised Multimodal Iterative Denoising Framework|https://arxiv.org/abs/2511.00997;S$^2$-MLLM: Boosting Spatial Reasoning Capability of MLLMs for 3D Visual Grounding with Structural Guidance|https://arxiv.org/abs/2512.01223;Uncertainty-Guided Self-Questioning and Answering for Video-Language Alignment|https://arxiv.org/abs/2410.02768;Leveraging Semantic Graphs for Efficient and Robust LiDAR SLAM|https://arxiv.org/abs/2503.11145;Adversarial Attacks of Vision Tasks in the Past 10 Years: A Survey|https://arxiv.org/abs/2410.23687;End-to-end 2D-3D Registration between Image and LiDAR Point Cloud for Vehicle Localization|https://arxiv.org/abs/2306.11346;Visual Spatial Tuning|https://arxiv.org/abs/2511.05491;HybridTM: Combining Transformer and Mamba for 3D Semantic Segmentation|https://arxiv.org/abs/2507.18575;UniLION: Towards Unified Autonomous Driving Model with Linear Group RNNs|https://arxiv.org/abs/2511.01768;TopoLiDM: Topology-Aware LiDAR Diffusion Models for Interpretable and Realistic LiDAR Point Cloud Generation|https://arxiv.org/abs/2507.22454;Geometric-Aware Low-Light Image and Video Enhancement via Depth Guidance|https://arxiv.org/abs/2312.15855;UTTG_ A Universal Teleoperation Approach via Online Trajectory Generation|https://arxiv.org/abs/2504.19736;Novel Diffusion Models for Multimodal 3D Hand Trajectory Prediction|https://arxiv.org/abs/2504.07375;KN-LIO: Geometric Kinematics and Neural Field Coupled LiDAR-Inertial Odometry|https://arxiv.org/abs/2501.04263;MRASfM: Multi-Camera Reconstruction and Aggregation through Structure-from-Motion in Driving Scenes|https://arxiv.org/abs/2510.15467;Planning from Imagination: Episodic Simulation and Episodic Memory for Vision-and-Language Navigation|https://arxiv.org/abs/2412.01857;3D Invisible Cloak|https://arxiv.org/abs/2011.13705;VPGS-SLAM: Voxel-based Progressive 3D Gaussian SLAM in Large-Scale Scenes|https://arxiv.org/abs/2505.18992;Exploring the Role of Explicit Temporal Modeling in Multimodal Large Language Models for Video Understanding|https://arxiv.org/abs/2501.16786;Uni-Hand: Universal Hand Motion Forecasting in Egocentric Views|https://arxiv.org/abs/2511.12878;Learnable Gated Temporal Shift Module for Deep Video Inpainting|https://arxiv.org/abs/1907.01131;Diffusion Stabilizer Policy for Automated Surgical Robot Manipulations|https://arxiv.org/abs/2503.01252;LiMT: A Multi-task Liver Image Benchmark Dataset|https://arxiv.org/abs/2511.19889;Towards Autonomous Indoor Parking: A Globally Consistent Semantic SLAM System and A Semantic Localization Subsystem|https://arxiv.org/abs/2410.12169;RL-GSBridge: 3D Gaussian Splatting Based Real2Sim2Real Method for Robotic Manipulation Learning|https://arxiv.org/abs/2409.20291;ADD-SLAM: Adaptive Dynamic Dense SLAM with Gaussian Splatting|https://arxiv.org/abs/2505.19420;ERMV: Editing 4D Robotic Multi-view images to enhance embodied agents|https://arxiv.org/abs/2507.17462;FLAME: Learning to Navigate with Multimodal LLM in Urban Environments|https://arxiv.org/abs/2408.11051;A Shared Control Framework for Mobile Robots with Planning-Level Intention Prediction|https://arxiv.org/abs/2511.08912;Structure-Aware Prototype Guided Trusted Multi-View Classification|https://arxiv.org/abs/2511.21021;MAMBA4D: Efficient Long-Sequence Point Cloud Video Understanding with Disentangled Spatial-Temporal State Space Models|https://arxiv.org/abs/2405.14338;Dream to Recall: Imagination-Guided Experience Retrieval for Memory-Persistent Vision-and-Language Navigation|https://arxiv.org/abs/2510.08553;What Is The Best 3D Scene Representation for Robotics? From Geometric to Foundation Models|https://arxiv.org/abs/2512.03422;NTIRE 2025 Challenge on Low Light Image Enhancement: Methods and Results|https://arxiv.org/abs/2510.13670;BEV-GS: Feed-forward Gaussian Splatting in Bird's-Eye-View for Road Reconstruction|https://arxiv.org/abs/2504.13207;PADReg: Physics-Aware Deformable Registration Guided by Contact Force for Ultrasound Sequences|https://arxiv.org/abs/2508.08685;Breaking the Discretization Barrier of Continuous Physics Simulation Learning|https://arxiv.org/abs/2509.17955;Restoration-Oriented Video Frame Interpolation with Region-Distinguishable Priors from SAM|https://arxiv.org/abs/2312.15868;Continually Evolving Skill Knowledge in Vision Language Action Model|https://arxiv.org/abs/2511.18085;Describe, Adapt and Combine: Empowering CLIP Encoders for Open-set 3D Object Retrieval|https://arxiv.org/abs/2507.21489;Ergodic Trajectory Planning with Dynamic Sensor Footprints|https://arxiv.org/abs/2512.08661;SemGauss-SLAM: Dense Semantic Gaussian Splatting SLAM|https://arxiv.org/abs/2403.07494;Systematic Evaluation and Guidelines for Segment Anything Model in Surgical Video Analysis|https://arxiv.org/abs/2501.00525;Differential-Integral Neural Operator for Long-Term Turbulence Forecasting|https://arxiv.org/abs/2509.21196;Learning-enhanced electronic skin for tactile sensing on deformable surface based on electrical impedance tomography|https://arxiv.org/abs/2504.05987;Improving Adversarial Transferability on Vision Transformers via Forward Propagation Refinement|https://arxiv.org/abs/2503.15404;MReg: A Novel Regression Model with MoE-based Video Feature Mining for Mitral Regurgitation Diagnosis|https://arxiv.org/abs/2506.23648;Diff-IP2D: Diffusion-Based Hand-Object Interaction Prediction on Egocentric Videos|https://arxiv.org/abs/2405.04370;Seeing through Uncertainty: Robust Task-Oriented Optimization in Visual Navigation|https://arxiv.org/abs/2510.00441;3D Gaussian Splatting in Robotics: A Survey|https://arxiv.org/abs/2410.12262;FlowVLA: Visual Chain of Thought-based Motion Reasoning for Vision-Language-Action Models|https://arxiv.org/abs/2508.18269;Frequency Domain Unlocks New Perspectives for Abdominal Medical Image Segmentation|https://arxiv.org/abs/2510.11005;MobileIE: An Extremely Lightweight and Effective ConvNet for Real-Time Image Enhancement on Mobile Devices|https://arxiv.org/abs/2507.01838;UniCT Depth: Event-Image Fusion Based Monocular Depth Estimation with Convolution-Compensated ViT Dual SA Block|https://arxiv.org/abs/2507.19948;Deformable Gaussian Splatting for Efficient and High-Fidelity Reconstruction of Surgical Scenes|https://arxiv.org/abs/2501.01101;FreeDriveRF: Monocular RGB Dynamic NeRF without Poses for Autonomous Driving via Point-Level Dynamic-Static Decoupling|https://arxiv.org/abs/2505.09406"
数字媒体与计算机视觉实验室,中国,China,上海交通大学,"计算机视觉, 数字图像处理, 计算机图形, 虚拟现实",致力于计算机视觉、人工智能与计算机图形学领域前沿研究，与腾讯、商汤科技等有深度合作。,"马利庄, 卢策吾, 盛斌, 肖双九","TransVOD, Mirror Detection with the Visual Chirality Cue, Understanding Pixel-Level 2D Image Semantics With 3D Keypoint Knowledge Engine",121.42975,31.02067,https://dmcv.sjtu.edu.cn/,Lizhuang Ma; Cewu Lu; Bin Sheng; Shuangjiu Xiao,Shanghai Jiao Tong University,,"AirExo-2: Scaling up Generalizable Robotic Imitation Learning with Low-Cost Exoskeletons|https://arxiv.org/abs/2503.03081;ARMADA: Autonomous Online Failure Detection and Human Shared Control Empower Scalable Real-world Deployment and Adaptation|https://arxiv.org/abs/2510.02298;Generalizable Articulated Object Perception with Superpoints|https://arxiv.org/abs/2412.16656;AgentWorld: An Interactive Simulation Platform for Scene Construction and Mobile Robotic Manipulation|https://arxiv.org/abs/2508.07770;RealD$^2$iff: Bridging Real-World Gap in Robot Manipulation via Depth Diffusion|https://arxiv.org/abs/2511.22505;Tru-POMDP: Task Planning Under Uncertainty via Tree of Hypotheses and Open-Ended POMDPs|https://arxiv.org/abs/2506.02860;Reconstructing In-the-Wild Open-Vocabulary Human-Object Interactions|https://arxiv.org/abs/2503.15898;AnyDexGrasp: General Dexterous Grasping for Different Hands with Human-level Learning Efficiency|https://arxiv.org/abs/2502.16420;EvolveNav: Empowering LLM-Based Vision-Language Navigation via Self-Improving Embodied Reasoning|https://arxiv.org/abs/2506.01551;Articulated Object Manipulation using Online Axis Estimation with SAM2-Based Tracking|https://arxiv.org/abs/2409.16287;MOL: Joint Estimation of Micro-Expression, Optical Flow, and Landmark via Transformer-Graph-Style Convolution|https://arxiv.org/abs/2506.14511;ImplicitRDP: An End-to-End Visual-Force Diffusion Policy with Structural Slow-Fast Learning|https://arxiv.org/abs/2512.10946;CurvNet: Latent Contour Representation and Iterative Data Engine for Curvature Angle Estimation|https://arxiv.org/abs/2411.12604;Learning to Restore Multi-Degraded Images via Ingredient Decoupling and Task-Aware Path Adaptation|https://arxiv.org/abs/2511.04920;The Labyrinth of Links: Navigating the Associative Maze of Multi-modal LLMs|https://arxiv.org/abs/2410.01417;OmniCam: Unified Multimodal Video Generation via Camera Control|https://arxiv.org/abs/2504.02312;FSGlove: An Inertial-Based Hand Tracking System with Shape-Aware Calibration|https://arxiv.org/abs/2509.21242;Go to Zero: Towards Zero-shot Motion Generation with Million-scale Data|https://arxiv.org/abs/2507.07095;Towards Effective Utilization of Mixed-Quality Demonstrations in Robotic Manipulation via Segment-Level Selection and Optimization|https://arxiv.org/abs/2409.19917;ID-Sculpt: ID-aware 3D Head Generation from Single In-the-wild Portrait Image|https://arxiv.org/abs/2406.16710;Improving Autoregressive Visual Generation with Cluster-Oriented Token Prediction|https://arxiv.org/abs/2501.00880;SD-VLM: Spatial Measuring and Understanding with Depth-Encoded Vision-Language Models|https://arxiv.org/abs/2509.17664;PFDepth: Heterogeneous Pinhole-Fisheye Joint Depth Estimation via Distortion-aware Gaussian-Splatted Volumetric Fusion|https://arxiv.org/abs/2509.26008;ImDy: Human Inverse Dynamics from Imitated Observations|https://arxiv.org/abs/2410.17610;Exploring Category-level Articulated Object Pose Tracking on SE(3) Manifolds|https://arxiv.org/abs/2511.05996;MBMamba: When Memory Buffer Meets Mamba for Structure-Aware Image Deblurring|https://arxiv.org/abs/2508.12346;DORAEMON: Decentralized Ontology-aware Reliable Agent with Enhanced Memory Oriented Navigation|https://arxiv.org/abs/2505.21969;GLC++: Source-Free Universal Domain Adaptation through Global-Local Clustering and Contrastive Affinity Learning|https://arxiv.org/abs/2403.14410;Reconstructing Topology-Consistent Face Mesh by Volume Rendering from Multi-View Images|https://arxiv.org/abs/2404.05606;M$^3$-VOS: Multi-Phase, Multi-Transition, and Multi-Scenery Video Object Segmentation|https://arxiv.org/abs/2412.13803;SIME: Enhancing Policy Self-Improvement with Modal-level Exploration|https://arxiv.org/abs/2505.01396;MOS: Modeling Object-Scene Associations in Generalized Category Discovery|https://arxiv.org/abs/2503.12035;Dexterous Manipulation Based on Prior Dexterous Grasp Pose Knowledge|https://arxiv.org/abs/2412.15587;FBI: Learning Dexterous In-hand Manipulation with Dynamic Visuotactile Shortcut Policy|https://arxiv.org/abs/2508.14441;One-for-More: Continual Diffusion Model for Anomaly Detection|https://arxiv.org/abs/2502.19848;GEOcc: Geometrically Enhanced 3D Occupancy Network with Implicit-Explicit Depth Fusion and Contextual Self-Supervision|https://arxiv.org/abs/2405.10591;Open X-Embodiment: Robotic Learning Datasets and RT-X Models|https://arxiv.org/abs/2310.08864;DeformPAM: Data-Efficient Learning for Long-horizon Deformable Object Manipulation via Preference-based Action Alignment|https://arxiv.org/abs/2410.11584;Real-IAD D3: A Real-World 2D/Pseudo-3D/3D Dataset for Industrial Anomaly Detection|https://arxiv.org/abs/2504.14221;SIGMAN:Scaling 3D Human Gaussian Generation with Millions of Assets|https://arxiv.org/abs/2504.06982;Collaborative Face Experts Fusion in Video Generation: Boosting Identity Consistency Across Large Face Poses|https://arxiv.org/abs/2508.09476;UniAff: A Unified Representation of Affordances for Tool Usage and Articulation with Vision-Language Models|https://arxiv.org/abs/2409.20551;ChatGarment: Garment Estimation, Generation and Editing via Large Language Models|https://arxiv.org/abs/2412.17811;DexTOG: Learning Task-Oriented Dexterous Grasp with Language|https://arxiv.org/abs/2504.04573;Motion Before Action: Diffusing Object Motion as Manipulation Condition|https://arxiv.org/abs/2411.09658;Dynamic Reconstruction of Hand-Object Interaction with Distributed Force-aware Contact Representation|https://arxiv.org/abs/2411.09572;ForceVLA: Enhancing VLA Models with a Force-aware MoE for Contact-rich Manipulation|https://arxiv.org/abs/2505.22159;3D Gaussian Head Avatars with Expressive Dynamic Appearances by Compact Tensorial Representations|https://arxiv.org/abs/2504.14967;From Enhancement to Understanding: Build a Generalized Bridge for Low-light Vision via Semantically Consistent Unsupervised Fine-tuning|https://arxiv.org/abs/2507.08380;ForceMimic: Force-Centric Imitation Learning with Force-Motion Capture System for Contact-Rich Manipulation|https://arxiv.org/abs/2410.07554;FoAR: Force-Aware Reactive Policy for Contact-Rich Robotic Manipulation|https://arxiv.org/abs/2411.15753;EyeSeg: An Uncertainty-Aware Eye Segmentation Framework for AR/VR|https://arxiv.org/abs/2507.09649;InstanceV: Instance-Level Video Generation|https://arxiv.org/abs/2511.23146;RH20T-P: A Primitive-Level Robotic Dataset Towards Composable Generalization Agents|https://arxiv.org/abs/2403.19622;ArtGS:3D Gaussian Splatting for Interactive Visual-Physical Modeling and Manipulation of Articulated Objects|https://arxiv.org/abs/2507.02600;Physically Ground Commonsense Knowledge for Articulated Object Manipulation with Analytic Concepts|https://arxiv.org/abs/2503.23348;SuperMat: Physically Consistent PBR Material Estimation at Interactive Rates|https://arxiv.org/abs/2411.17515;Pinco: Position-induced Consistent Adapter for Diffusion Transformer in Foreground-conditioned Inpainting|https://arxiv.org/abs/2412.03812;Domain Generalization via Discrete Codebook Learning|https://arxiv.org/abs/2504.06572;Real-IAD Variety: Pushing Industrial Anomaly Detection Dataset to a Modern Era|https://arxiv.org/abs/2511.00540;Digital Gene: Learning about the Physical World through Analytic Concepts|https://arxiv.org/abs/2504.04170;SOE: Sample-Efficient Robot Policy Self-Improvement via On-Manifold Exploration|https://arxiv.org/abs/2509.19292;LidarPainter: One-Step Away From Any Lidar View To Novel Guidance|https://arxiv.org/abs/2507.12114;REArtGS++: Generalizable Articulation Reconstruction with Temporal Geometry Constraint via Planar Gaussian Splatting|https://arxiv.org/abs/2511.17059;Right-Side-Out: Learning Zero-Shot Sim-to-Real Garment Reversal|https://arxiv.org/abs/2509.15953;History-Aware Visuomotor Policy Learning via Point Tracking|https://arxiv.org/abs/2509.17141;One RL to See Them All: Visual Triple Unified Reinforcement Learning|https://arxiv.org/abs/2505.18129;Generative Classifier for Domain Generalization|https://arxiv.org/abs/2504.02272;Physically Interpretable Multi-Degradation Image Restoration via Deep Unfolding and Explainable Convolution|https://arxiv.org/abs/2511.10166;Learning Dexterous Manipulation with Quantized Hand State|https://arxiv.org/abs/2509.17450;SaRA: High-Efficient Diffusion Model Fine-tuning with Progressive Sparse Low-Rank Adaptation|https://arxiv.org/abs/2409.06633;MesaTask: Towards Task-Driven Tabletop Scene Generation via 3D Spatial Reasoning|https://arxiv.org/abs/2509.22281;StyleRWKV: High-Quality and High-Efficiency Style Transfer with RWKV-like Architecture|https://arxiv.org/abs/2412.19535;iTACO: Interactable Digital Twins of Articulated Objects from Casually Captured RGBD Videos|https://arxiv.org/abs/2506.08334;REArtGS: Reconstructing and Generating Articulated Objects via 3D Gaussian Splatting with Geometric and Motion Constraints|https://arxiv.org/abs/2503.06677;GarmentTracking: Category-Level Garment Pose Tracking|https://arxiv.org/abs/2303.13913;TrajBooster: Boosting Humanoid Whole-Body Manipulation via Trajectory-Centric Learning|https://arxiv.org/abs/2509.11839;SoftPatch+: Fully Unsupervised Anomaly Classification and Segmentation|https://arxiv.org/abs/2412.20870;BEAR: A Video Dataset For Fine-grained Behaviors Recognition Oriented with Action and Environment Factors|https://arxiv.org/abs/2503.20209;Semantic Frame Interpolation|https://arxiv.org/abs/2507.05173;Diverse Target and Contribution Scheduling for Domain Generalization|https://arxiv.org/abs/2309.16460;Micro-Expression Recognition via Fine-Grained Dynamic Perception|https://arxiv.org/abs/2509.06015;Identity-Preserving Text-to-Video Generation Guided by Simple yet Effective Spatial-Temporal Decoupled Representations|https://arxiv.org/abs/2507.04705;Executable Analytic Concepts as the Missing Link Between VLM Insight and Precise Manipulation|https://arxiv.org/abs/2510.07975;Knowledge-Driven Imitation Learning: Enabling Generalization Across Diverse Conditions|https://arxiv.org/abs/2506.21057;Kalib: Easy Hand-Eye Calibration with Reference Point Tracking|https://arxiv.org/abs/2408.10562;UniForward: Unified 3D Scene and Semantic Field Reconstruction via Feed-Forward Gaussian Splatting from Only Sparse-View Images|https://arxiv.org/abs/2506.09378;Pretraining a Unified PDDL Domain from Real-World Demonstrations for Generalizable Robot Task Planning|https://arxiv.org/abs/2507.21545;Dense Policy: Bidirectional Autoregressive Learning of Actions|https://arxiv.org/abs/2503.13217;L1 Sample Flow for Efficient Visuomotor Learning|https://arxiv.org/abs/2511.17898;Reactive Diffusion Policy: Slow-Fast Visual-Tactile Policy Learning for Contact-Rich Manipulation|https://arxiv.org/abs/2503.02881;Arti-PG: A Toolbox for Procedurally Synthesizing Large-Scale and Diverse Articulated Objects with Rich Annotations|https://arxiv.org/abs/2412.14974;PointDGMamba: Domain Generalization of Point Cloud Classification via Generalized State Space Model|https://arxiv.org/abs/2408.13574;HiGS: Hierarchical Generative Scene Framework for Multi-Step Associative Semantic Spatial Composition|https://arxiv.org/abs/2510.27148;IAR2: Improving Autoregressive Visual Generation with Semantic-Detail Associated Token Prediction|https://arxiv.org/abs/2510.06928;Interacted Object Grounding in Spatio-Temporal Human-Object Interactions|https://arxiv.org/abs/2412.19542;Homogeneous Dynamics Space for Heterogeneous Humans|https://arxiv.org/abs/2412.06146;Verb Mirage: Unveiling and Assessing Verb Concept Hallucinations in Multimodal Large Language Models|https://arxiv.org/abs/2412.04939;Novel Demonstration Generation with Gaussian Splatting Enables Robust One-Shot Manipulation|https://arxiv.org/abs/2504.13175;A$^\text{T}$A: Adaptive Transformation Agent for Text-Guided Subject-Position Variable Background Inpainting|https://arxiv.org/abs/2504.01603;PointDGRWKV: Generalizing RWKV-like Architecture to Unseen Domains for Point Cloud Classification|https://arxiv.org/abs/2508.20835;Multi-view Hand Reconstruction with a Point-Embedded Transformer|https://arxiv.org/abs/2408.10581;DrivingForward: Feed-forward 3D Gaussian Splatting for Driving Scene Reconstruction from Flexible Surround-view Input|https://arxiv.org/abs/2409.12753"
赵波老师实验室,中国,China,上海交通大学,"多模态大语言模型 (MLLM), 6D物体姿态估计, 具身AI",团队开发了轻量级多模态大语言模型Bunny-3B/4B/8B，研究方向包括数据中心人工智能等。,赵波,"Efficient Multimodal Learning from Data-centric Perspective, Omni6DPose, VISTA",121.42975,31.02067,https://mint-sjtu.github.io/,Bo Zhao,Shanghai Jiao Tong University,,"EarthSynth: Generating Informative Earth Observation with Diffusion Models|https://arxiv.org/abs/2505.12108;Mask2IV: Interaction-Centric Video Generation via Mask Trajectories|https://arxiv.org/abs/2510.03135;Video-XL-2: Towards Very Long-Video Understanding Through Task-Aware KV Sparsification|https://arxiv.org/abs/2506.19225;CardiacMamba: A Multimodal RGB-RF Fusion Framework with State Space Models for Remote Physiological Measurement|https://arxiv.org/abs/2502.13624;Evo-0: Vision-Language-Action Model with Implicit Spatial Understanding|https://arxiv.org/abs/2507.00416;Task-Aware KV Compression For Cost-Effective Long Video Understanding|https://arxiv.org/abs/2506.21184;VLA-Pruner: Temporal-Aware Dual-Level Visual Token Pruning for Efficient Vision-Language-Action Inference|https://arxiv.org/abs/2511.16449;Compensating Distribution Drifts in Class-incremental Learning of Pre-trained Vision Transformers|https://arxiv.org/abs/2511.09926;Domain Generalization for Face Anti-spoofing via Content-aware Composite Prompt Engineering|https://arxiv.org/abs/2504.04470;MomentSeeker: A Task-Oriented Benchmark For Long-Video Moment Retrieval|https://arxiv.org/abs/2502.12558;PhysLLM: Harnessing Large Language Models for Cross-Modal Remote Physiological Sensing|https://arxiv.org/abs/2505.03621;Unveiling the Ignorance of MLLMs: Seeing Clearly, Answering Incorrectly|https://arxiv.org/abs/2406.10638;MLVU: Benchmarking Multi-task Long Video Understanding|https://arxiv.org/abs/2406.04264;UtilGen: Utility-Centric Generative Data Augmentation with Dual-Level Task Adaptation|https://arxiv.org/abs/2510.24262;SpatialBot: Precise Spatial Understanding with Vision Language Models|https://arxiv.org/abs/2406.13642;BOOD: Boundary-based Out-Of-Distribution Data Generation|https://arxiv.org/abs/2508.00350;MegaPairs: Massive Data Synthesis For Universal Multimodal Retrieval|https://arxiv.org/abs/2412.14475;SEGA: A Stepwise Evolution Paradigm for Content-Aware Layout Generation with Design Prior|https://arxiv.org/abs/2510.15749;V-HUB: A Visual-Centric Humor Understanding Benchmark for Video LLMs|https://arxiv.org/abs/2509.25773;Evo-1: Lightweight Vision-Language-Action Model with Preserved Semantic Alignment|https://arxiv.org/abs/2511.04555;RoboFAC: A Comprehensive Framework for Robotic Failure Analysis and Correction|https://arxiv.org/abs/2505.12224;DD-Ranking: Rethinking the Evaluation of Dataset Distillation|https://arxiv.org/abs/2505.13300;Image Captions are Natural Prompts for Text-to-Image Models|https://arxiv.org/abs/2307.08526;PHASE-Net: Physics-Grounded Harmonic Attention System for Efficient Remote Photoplethysmography Measurement|https://arxiv.org/abs/2509.24850;Normalizing Batch Normalization for Long-Tailed Recognition|https://arxiv.org/abs/2501.03122;Touchstone Benchmark: Are We on the Right Way for Evaluating AI Algorithms for Medical Segmentation?|https://arxiv.org/abs/2411.03670;U-ARM : Ultra low-cost general teleoperation interface for robot manipulation|https://arxiv.org/abs/2509.02437;AU-LLM: Micro-Expression Action Unit Detection via Enhanced LLM-Based Feature Fusion|https://arxiv.org/abs/2507.21778;TimeScope: Towards Task-Oriented Temporal Grounding In Long Videos|https://arxiv.org/abs/2509.26360;STI-Bench: Are MLLMs Ready for Precise Spatial-Temporal World Understanding?|https://arxiv.org/abs/2503.23765;SegVol: Universal and Interactive Volumetric Medical Image Segmentation|https://arxiv.org/abs/2311.13385;Understanding Data Influence with Differential Approximation|https://arxiv.org/abs/2508.14648;Video-XL-Pro: Reconstructive Token Compression for Extremely Long Video Understanding|https://arxiv.org/abs/2503.18478;HiDream-I1: A High-Efficient Image Generative Foundation Model with Sparse Diffusion Transformer|https://arxiv.org/abs/2505.22705"
ReThinkLab,中国,China,上海交通大学,自动驾驶与机器人、图与组合优化、量子机器学习、AI4Science,致力于开发解决现实世界问题的前沿技术，探索数据驱动方法与领域知识的结合。,严骏驰,"Think2Drive, Grounding and Enhancing Grid-based Models for Neural Fields, OptCM",121.42975,31.02067,https://thinklab.sjtu.edu.cn,Junchi Yan,Shanghai Jiao Tong University,,ChartX & ChartVLM: A Versatile Benchmark and Foundation Model for Complicated Chart Reasoning|https://arxiv.org/abs/2402.12185;RoboSense: Large-scale Dataset and Benchmark for Egocentric Robot Perception and Navigation in Crowded and Unstructured Environments|https://arxiv.org/abs/2408.15503;Towards More Diverse and Challenging Pre-training for Point Cloud Learning: Self-Supervised Cross Reconstruction with Decoupled Views|https://arxiv.org/abs/2509.01250;Not All Samples Should Be Utilized Equally: Towards Understanding and Improving Dataset Distillation|https://arxiv.org/abs/2408.12483;MM-HELIX: Boosting Multimodal Long-Chain Reflective Reasoning with Holistic Platform and Adaptive Hybrid Policy Optimization|https://arxiv.org/abs/2510.08540;Interleave-VLA: Enhancing Robot Manipulation with Interleaved Image-Text Instructions|https://arxiv.org/abs/2505.02152;M-Tuning: Prompt Tuning with Mitigated Label Bias in Open-Set Scenarios|https://arxiv.org/abs/2303.05122;ProCLIP: Progressive Vision-Language Alignment via LLM-based Embedder|https://arxiv.org/abs/2510.18795;Point2RBox-v2: Rethinking Point-supervised Oriented Object Detection with Spatial Layout Among Instances|https://arxiv.org/abs/2502.04268;TerDiT: Ternary Diffusion Models with Transformers|https://arxiv.org/abs/2405.14854;GeoX: Geometric Problem Solving Through Unified Formalized Vision-Language Pre-training|https://arxiv.org/abs/2412.11863;Raw2Drive: Reinforcement Learning with Aligned World Models for End-to-End Autonomous Driving (in CARLA v2)|https://arxiv.org/abs/2505.16394;DriveTransformer: Unified Transformer for Scalable End-to-End Autonomous Driving|https://arxiv.org/abs/2503.07656;FreqPDE: Rethinking Positional Depth Embedding for Multi-View 3D Object Detection Transformers|https://arxiv.org/abs/2510.15385;LaGen: Towards Autoregressive LiDAR Scene Generation|https://arxiv.org/abs/2511.21256;PointOBB-v3: Expanding Performance Boundaries of Single Point-Supervised Oriented Object Detection|https://arxiv.org/abs/2501.13898;AgiBot World Colosseo: A Large-scale Manipulation Platform for Scalable and Intelligent Embodied Systems|https://arxiv.org/abs/2503.06669;UniMamba: Unified Spatial-Channel Representation Learning with Group-Efficient Mamba for LiDAR-based 3D Object Detection|https://arxiv.org/abs/2503.12009;SpaCE-10: A Comprehensive Benchmark for Multimodal Large Language Models in Compositional Spatial Intelligence|https://arxiv.org/abs/2506.07966;Decoupled Geometric Parameterization and its Application in Deep Homography Estimation|https://arxiv.org/abs/2505.16599;Envisioning Beyond the Pixels: Benchmarking Reasoning-Informed Visual Editing|https://arxiv.org/abs/2504.02826;Spatial Retrieval Augmented Autonomous Driving|https://arxiv.org/abs/2512.06865;Wholly-WOOD: Wholly Leveraging Diversified-quality Labels for Weakly-supervised Oriented Object Detection|https://arxiv.org/abs/2502.09471;FlatFusion: Delving into Details of Sparse Transformer-based Camera-LiDAR Fusion for Autonomous Driving|https://arxiv.org/abs/2408.06832;Co-Training Vision Language Models for Remote Sensing Multi-task Learning|https://arxiv.org/abs/2511.21272;Point2RBox-v3: Self-Bootstrapping from Point Annotations via Integrated Pseudo-Label Refinement and Utilization|https://arxiv.org/abs/2509.26281;SPOT: Scalable 3D Pre-training via Occupancy Prediction for Learning Transferable 3D Representations|https://arxiv.org/abs/2309.10527;VideoREPA: Learning Physics for Video Generation through Relational Alignment with Foundation Models|https://arxiv.org/abs/2505.23656;Player-Centric Multimodal Prompt Generation for Large Language Model Based Identity-Aware Basketball Video Captioning|https://arxiv.org/abs/2507.20163;Learning Adaptive and Temporally Causal Video Tokenization in a 1D Latent Space|https://arxiv.org/abs/2505.17011;QuEST: Low-bit Diffusion Model Quantization via Efficient Selective Finetuning|https://arxiv.org/abs/2402.03666;DiFSD: Ego-Centric Fully Sparse Paradigm with Uncertainty Denoising and Iterative Refinement for Efficient End-to-End Self-Driving|https://arxiv.org/abs/2409.09777;Rethinking Video Tokenization: A Conditioned Diffusion-based Approach|https://arxiv.org/abs/2503.03708;Calibrating Biased Distribution in VFM-derived Latent Space via Cross-Domain Geometric Consistency|https://arxiv.org/abs/2508.13518;Visionary: The World Model Carrier Built on WebGPU-Powered Gaussian Splatting Platform|https://arxiv.org/abs/2512.08478;Int2Planner: An Intention-based Multi-modal Motion Planner for Integrated Prediction and Planning|https://arxiv.org/abs/2501.12799;Repulsor: Accelerating Generative Modeling with a Contrastive Memory Bank|https://arxiv.org/abs/2512.08648;Bench2Drive-R: Turning Real World Data into Reactive Closed-Loop Autonomous Driving Benchmark by Generative Model|https://arxiv.org/abs/2412.09647;DriveVGGT: Visual Geometry Transformer for Autonomous Driving|https://arxiv.org/abs/2511.22264;Dual-Branch Center-Surrounding Contrast: Rethinking Contrastive Learning for 3D Point Clouds|https://arxiv.org/abs/2512.08673;DriveMoE: Mixture-of-Experts for Vision-Language-Action Model in End-to-End Autonomous Driving|https://arxiv.org/abs/2505.16278;CrossEarth: Geospatial Vision Foundation Model for Domain Generalizable Remote Sensing Semantic Segmentation|https://arxiv.org/abs/2410.22629
机器视觉与智能学习实验室 (MVIG),中国,China,上海交通大学,具身智能、知识驱动系统、人类活动感知与学习,致力于开发类似C-3PO和R2-D2的智能机器人，构建知识驱动的系统。,卢策吾,"HumanVLA, 时空人与物体交互中的交互对象基础, 动词幻觉",121.42975,31.02067,https://github.com/MVIG-SJTU,Cewu Lu,Shanghai Jiao Tong University,,"AirExo-2: Scaling up Generalizable Robotic Imitation Learning with Low-Cost Exoskeletons|https://arxiv.org/abs/2503.03081;Dexterous Manipulation Based on Prior Dexterous Grasp Pose Knowledge|https://arxiv.org/abs/2412.15587;ARMADA: Autonomous Online Failure Detection and Human Shared Control Empower Scalable Real-world Deployment and Adaptation|https://arxiv.org/abs/2510.02298;ArtGS:3D Gaussian Splatting for Interactive Visual-Physical Modeling and Manipulation of Articulated Objects|https://arxiv.org/abs/2507.02600;FBI: Learning Dexterous In-hand Manipulation with Dynamic Visuotactile Shortcut Policy|https://arxiv.org/abs/2508.14441;Knowledge-Driven Imitation Learning: Enabling Generalization Across Diverse Conditions|https://arxiv.org/abs/2506.21057;Physically Ground Commonsense Knowledge for Articulated Object Manipulation with Analytic Concepts|https://arxiv.org/abs/2503.23348;Executable Analytic Concepts as the Missing Link Between VLM Insight and Precise Manipulation|https://arxiv.org/abs/2510.07975;Kalib: Easy Hand-Eye Calibration with Reference Point Tracking|https://arxiv.org/abs/2408.10562;Generalizable Articulated Object Perception with Superpoints|https://arxiv.org/abs/2412.16656;Open X-Embodiment: Robotic Learning Datasets and RT-X Models|https://arxiv.org/abs/2310.08864;AgentWorld: An Interactive Simulation Platform for Scene Construction and Mobile Robotic Manipulation|https://arxiv.org/abs/2508.07770;RealD$^2$iff: Bridging Real-World Gap in Robot Manipulation via Depth Diffusion|https://arxiv.org/abs/2511.22505;DeformPAM: Data-Efficient Learning for Long-horizon Deformable Object Manipulation via Preference-based Action Alignment|https://arxiv.org/abs/2410.11584;Pretraining a Unified PDDL Domain from Real-World Demonstrations for Generalizable Robot Task Planning|https://arxiv.org/abs/2507.21545;Tru-POMDP: Task Planning Under Uncertainty via Tree of Hypotheses and Open-Ended POMDPs|https://arxiv.org/abs/2506.02860;Digital Gene: Learning about the Physical World through Analytic Concepts|https://arxiv.org/abs/2504.04170;SOE: Sample-Efficient Robot Policy Self-Improvement via On-Manifold Exploration|https://arxiv.org/abs/2509.19292;Dense Policy: Bidirectional Autoregressive Learning of Actions|https://arxiv.org/abs/2503.13217;L1 Sample Flow for Efficient Visuomotor Learning|https://arxiv.org/abs/2511.17898;REArtGS++: Generalizable Articulation Reconstruction with Temporal Geometry Constraint via Planar Gaussian Splatting|https://arxiv.org/abs/2511.17059;AnyDexGrasp: General Dexterous Grasping for Different Hands with Human-level Learning Efficiency|https://arxiv.org/abs/2502.16420;Right-Side-Out: Learning Zero-Shot Sim-to-Real Garment Reversal|https://arxiv.org/abs/2509.15953;History-Aware Visuomotor Policy Learning via Point Tracking|https://arxiv.org/abs/2509.17141;EvolveNav: Empowering LLM-Based Vision-Language Navigation via Self-Improving Embodied Reasoning|https://arxiv.org/abs/2506.01551;UniAff: A Unified Representation of Affordances for Tool Usage and Articulation with Vision-Language Models|https://arxiv.org/abs/2409.20551;Articulated Object Manipulation using Online Axis Estimation with SAM2-Based Tracking|https://arxiv.org/abs/2409.16287;Reactive Diffusion Policy: Slow-Fast Visual-Tactile Policy Learning for Contact-Rich Manipulation|https://arxiv.org/abs/2503.02881;ImplicitRDP: An End-to-End Visual-Force Diffusion Policy with Structural Slow-Fast Learning|https://arxiv.org/abs/2512.10946;Arti-PG: A Toolbox for Procedurally Synthesizing Large-Scale and Diverse Articulated Objects with Rich Annotations|https://arxiv.org/abs/2412.14974;The Labyrinth of Links: Navigating the Associative Maze of Multi-modal LLMs|https://arxiv.org/abs/2410.01417;ChatGarment: Garment Estimation, Generation and Editing via Large Language Models|https://arxiv.org/abs/2412.17811;OmniCam: Unified Multimodal Video Generation via Camera Control|https://arxiv.org/abs/2504.02312;FSGlove: An Inertial-Based Hand Tracking System with Shape-Aware Calibration|https://arxiv.org/abs/2509.21242;DexTOG: Learning Task-Oriented Dexterous Grasp with Language|https://arxiv.org/abs/2504.04573;Motion Before Action: Diffusing Object Motion as Manipulation Condition|https://arxiv.org/abs/2411.09658;Interacted Object Grounding in Spatio-Temporal Human-Object Interactions|https://arxiv.org/abs/2412.19542;Dynamic Reconstruction of Hand-Object Interaction with Distributed Force-aware Contact Representation|https://arxiv.org/abs/2411.09572;ForceVLA: Enhancing VLA Models with a Force-aware MoE for Contact-rich Manipulation|https://arxiv.org/abs/2505.22159;Learning Dexterous Manipulation with Quantized Hand State|https://arxiv.org/abs/2509.17450;Towards Effective Utilization of Mixed-Quality Demonstrations in Robotic Manipulation via Segment-Level Selection and Optimization|https://arxiv.org/abs/2409.19917;Exploring Category-level Articulated Object Pose Tracking on SE(3) Manifolds|https://arxiv.org/abs/2511.05996;ImDy: Human Inverse Dynamics from Imitated Observations|https://arxiv.org/abs/2410.17610;iTACO: Interactable Digital Twins of Articulated Objects from Casually Captured RGBD Videos|https://arxiv.org/abs/2506.08334;REArtGS: Reconstructing and Generating Articulated Objects via 3D Gaussian Splatting with Geometric and Motion Constraints|https://arxiv.org/abs/2503.06677;ForceMimic: Force-Centric Imitation Learning with Force-Motion Capture System for Contact-Rich Manipulation|https://arxiv.org/abs/2410.07554;GarmentTracking: Category-Level Garment Pose Tracking|https://arxiv.org/abs/2303.13913;GLC++: Source-Free Universal Domain Adaptation through Global-Local Clustering and Contrastive Affinity Learning|https://arxiv.org/abs/2403.14410;FoAR: Force-Aware Reactive Policy for Contact-Rich Robotic Manipulation|https://arxiv.org/abs/2411.15753;Homogeneous Dynamics Space for Heterogeneous Humans|https://arxiv.org/abs/2412.06146;Verb Mirage: Unveiling and Assessing Verb Concept Hallucinations in Multimodal Large Language Models|https://arxiv.org/abs/2412.04939;M$^3$-VOS: Multi-Phase, Multi-Transition, and Multi-Scenery Video Object Segmentation|https://arxiv.org/abs/2412.13803;TrajBooster: Boosting Humanoid Whole-Body Manipulation via Trajectory-Centric Learning|https://arxiv.org/abs/2509.11839;Novel Demonstration Generation with Gaussian Splatting Enables Robust One-Shot Manipulation|https://arxiv.org/abs/2504.13175;SIME: Enhancing Policy Self-Improvement with Modal-level Exploration|https://arxiv.org/abs/2505.01396;RH20T-P: A Primitive-Level Robotic Dataset Towards Composable Generalization Agents|https://arxiv.org/abs/2403.19622;Multi-view Hand Reconstruction with a Point-Embedded Transformer|https://arxiv.org/abs/2408.10581"
Stanford AI Lab (SAIL),美国,USA,斯坦福大学,计算机视觉、NLP、强化学习、机器人技术、计算神经科学等,研究领域涵盖人工智能的各个方面，下设多个研究小组，如NLP组和视觉与学习实验室(SVL)。,"李飞飞, Jiajun Wu, Dan Jurafsky, Percy Liang","TRANSIC, D3Fields, PhysDreamer",-122.169719,37.427474,https://ai.stanford.edu/,"Li Fei-Fei; Jiajun Wu; Dan Jurafsky, Percy Liang",Stanford University,,"Explain Before You Answer: A Survey on Compositional Visual Reasoning|https://arxiv.org/abs/2508.17298;Retargeting Matters: General Motion Retargeting for Humanoid Motion Tracking|https://arxiv.org/abs/2510.02252;Spatial Mental Modeling from Limited Views|https://arxiv.org/abs/2506.21458;Discovering Latent Graphs with GFlowNets for Diverse Conditional Image Generation|https://arxiv.org/abs/2510.22107;Category-Agnostic Neural Object Rigging|https://arxiv.org/abs/2505.20283;Phys2Real: Fusing VLM Priors with Interactive Online Adaptation for Uncertainty-Aware Sim-to-Real Manipulation|https://arxiv.org/abs/2510.11689;TWIST: Teleoperated Whole-Body Imitation System|https://arxiv.org/abs/2505.02833;Product of Experts for Visual Generation|https://arxiv.org/abs/2506.08894;Weakly-Supervised Learning of Dense Functional Correspondences|https://arxiv.org/abs/2509.03893;ZeroHSI: Zero-Shot 4D Human-Scene Interaction by Video Generation|https://arxiv.org/abs/2412.18600;Open X-Embodiment: Robotic Learning Datasets and RT-X Models|https://arxiv.org/abs/2310.08864;Lifting Motion to the 3D World via 2D Diffusion|https://arxiv.org/abs/2411.18808;CLEVRER-Humans: Describing Physical and Causal Events the Human Way|https://arxiv.org/abs/2310.03635;ENACT: Evaluating Embodied Cognition with World Modeling of Egocentric Interaction|https://arxiv.org/abs/2511.20937;The Language of Motion: Unifying Verbal and Non-verbal Language of 3D Human Motion|https://arxiv.org/abs/2412.10523;A Real-to-Sim-to-Real Approach to Robotic Manipulation with VLM-Generated Iterative Keypoint Rewards|https://arxiv.org/abs/2502.08643;Towards Fine-Grained Video Question Answering|https://arxiv.org/abs/2503.06820;CRAFT: Designing Creative and Functional 3D Objects|https://arxiv.org/abs/2412.03889;From Programs to Poses: Factored Real-World Scene Generation via Learned Program Libraries|https://arxiv.org/abs/2510.10292;MoMaGen: Generating Demonstrations under Soft and Hard Constraints for Multi-Step Bimanual Mobile Manipulation|https://arxiv.org/abs/2510.18316;Web-Scale Collection of Video Data for 4D Animal Reconstruction|https://arxiv.org/abs/2511.01169;Thinking in Space: How Multimodal Large Language Models See, Remember, and Recall Spaces|https://arxiv.org/abs/2412.14171;Cambrian-S: Towards Spatial Supersensing in Video|https://arxiv.org/abs/2511.04670;Wild2Avatar: Rendering Humans Behind Occlusions|https://arxiv.org/abs/2401.00431;DexSkin: High-Coverage Conformable Robotic Skin for Learning Contact-Rich Manipulation|https://arxiv.org/abs/2509.18830;TWIST2: Scalable, Portable, and Holistic Humanoid Data Collection System|https://arxiv.org/abs/2511.02832;T*: Re-thinking Temporal Search for Long-Form Video Understanding|https://arxiv.org/abs/2504.02259;Taming generative video models for zero-shot optical flow extraction|https://arxiv.org/abs/2507.09082;What Makes a Maze Look Like a Maze?|https://arxiv.org/abs/2409.08202;Dual encoding feature filtering generalized attention UNET for retinal vessel segmentation|https://arxiv.org/abs/2506.02312;Learning Compositional Behaviors from Demonstration and Language|https://arxiv.org/abs/2505.21981;Digital Twin Catalog: A Large-Scale Photorealistic 3D Object Digital Twin Dataset|https://arxiv.org/abs/2504.08541;Coupled Diffusion Sampling for Training-Free Multi-View Image Editing|https://arxiv.org/abs/2510.14981;WonderZoom: Multi-Scale 3D World Generation|https://arxiv.org/abs/2512.09164;Unsupervised Discovery of Object-Centric Neural Fields|https://arxiv.org/abs/2402.07376;Ctrl-VI: Controllable Video Synthesis via Variational Inference|https://arxiv.org/abs/2510.07670;Why Automate This? Exploring Correlations between Desire for Robotic Automation, Invested Time and Well-Being|https://arxiv.org/abs/2501.06348;DROID: A Large-Scale In-The-Wild Robot Manipulation Dataset|https://arxiv.org/abs/2403.12945;WorldScore: A Unified Evaluation Benchmark for World Generation|https://arxiv.org/abs/2504.00983;10 Open Challenges Steering the Future of Vision-Language-Action Models|https://arxiv.org/abs/2511.05936;LLMC+: Benchmarking Vision-Language Model Compression with a Plug-and-play Toolkit|https://arxiv.org/abs/2508.09981;Seeing the Wind from a Falling Leaf|https://arxiv.org/abs/2512.00762;Anymate: A Dataset and Baselines for Learning 3D Object Rigging|https://arxiv.org/abs/2505.06227;Repurposing 2D Diffusion Models with Gaussian Atlas for 3D Generation|https://arxiv.org/abs/2503.15877;Embodied Agent Interface: Benchmarking LLMs for Embodied Decision Making|https://arxiv.org/abs/2410.07166;Understanding Complexity in VideoQA via Visual Program Generation|https://arxiv.org/abs/2505.13429;3D-Generalist: Self-Improving Vision-Language-Action Models for Crafting 3D Worlds|https://arxiv.org/abs/2507.06484;Model-Based Policy Adaptation for Closed-Loop End-to-End Autonomous Driving|https://arxiv.org/abs/2511.21584;Flow to the Mode: Mode-Seeking Diffusion Autoencoders for State-of-the-Art Image Tokenization|https://arxiv.org/abs/2503.11056;Predicate Hierarchies Improve Few-Shot State Classification|https://arxiv.org/abs/2502.12481;FluidNexus: 3D Fluid Reconstruction and Prediction from a Single Video|https://arxiv.org/abs/2503.04720;Birth and Death of a Rose|https://arxiv.org/abs/2412.05278;VisualMimic: Visual Humanoid Loco-Manipulation via Motion Tracking and Generation|https://arxiv.org/abs/2509.20322;View-Invariant Policy Learning via Zero-Shot Novel View Synthesis|https://arxiv.org/abs/2409.03685;Generalizable Humanoid Manipulation with 3D Diffusion Policies|https://arxiv.org/abs/2410.10803;The Scene Language: Representing Scenes with Programs, Words, and Embeddings|https://arxiv.org/abs/2410.16770;BEHAVIOR Robot Suite: Streamlining Real-World Whole-Body Manipulation for Everyday Household Activities|https://arxiv.org/abs/2503.05652;Visually Descriptive Language Model for Vector Graphics Reasoning|https://arxiv.org/abs/2404.06479;WonderPlay: Dynamic 3D Scene Generation from a Single Image and Actions|https://arxiv.org/abs/2505.18151;UAD: Unsupervised Affordance Distillation for Generalization in Robotic Manipulation|https://arxiv.org/abs/2506.09284;Chain-of-Modality: Learning Manipulation Programs from Multimodal Human Videos with Vision-Language-Models|https://arxiv.org/abs/2504.13351;WonderWorld: Interactive 3D Scene Generation from a Single Image|https://arxiv.org/abs/2406.09394;X-Capture: An Open-Source Portable Device for Multi-Sensory Learning|https://arxiv.org/abs/2504.02318;Neuro-Symbolic Concepts|https://arxiv.org/abs/2505.06191;GET-USE: Learning Generalized Tool Usage for Bimanual Mobile Manipulation via Simulated Embodiment Extensions|https://arxiv.org/abs/2510.25754;Self-Supervised Learning of Motion Concepts by Optimizing Counterfactuals|https://arxiv.org/abs/2503.19953;LayoutVLM: Differentiable Optimization of 3D Layout via Vision-Language Models|https://arxiv.org/abs/2412.02193"
Robotics and Embodied Artificial Intelligence Lab (REAL),美国,USA,斯坦福大学,多机器人协作、机器人技能学习、机器人操作、导航与场景理解,专注于机器人与具身人工智能的研究。,Shuran Song,"GET-Zero, Dynamics-Guided Diffusion Model for Robot Manipulator Design, Universal Manipulation Interface",-122.169719,37.427474,https://real.stanford.edu/,Shuran Song,Stanford University,,Dynamics-Guided Diffusion Model for Sensor-less Robot Manipulator Design|https://arxiv.org/abs/2402.15038;DynaGuide: Steering Diffusion Polices with Active Dynamic Guidance|https://arxiv.org/abs/2506.13922;RoboPanoptes: The All-seeing Robot with Whole-body Dexterity|https://arxiv.org/abs/2501.05420;Robot Trains Robot: Automatic Real-World Policy Adaptation and Learning for Humanoids|https://arxiv.org/abs/2508.12252;Open X-Embodiment: Robotic Learning Datasets and RT-X Models|https://arxiv.org/abs/2310.08864;Cosmos-Reason1: From Physical Common Sense To Embodied Reasoning|https://arxiv.org/abs/2503.15558;World Simulation with Video Foundation Models for Physical AI|https://arxiv.org/abs/2511.00062;UMI-on-Air: Embodiment-Aware Guidance for Embodiment-Agnostic Visuomotor Policies|https://arxiv.org/abs/2510.02614;Compliant Residual DAgger: Improving Real-World Contact-Rich Manipulation with Human Corrections|https://arxiv.org/abs/2506.16685;Should VLMs be Pre-trained with Image Data?|https://arxiv.org/abs/2503.07603;Rectified Point Flow: Generic Point Cloud Pose Estimation|https://arxiv.org/abs/2506.05282;Adaptive Compliance Policy: Learning Approximate Compliance for Diffusion Guided Control|https://arxiv.org/abs/2410.09309;A Practical Guide for Incorporating Symmetry in Diffusion Policy|https://arxiv.org/abs/2505.13431;DexUMI: Using Human Hand as the Universal Manipulation Interface for Dexterous Manipulation|https://arxiv.org/abs/2505.21864;ToddlerBot: Open-Source ML-Compatible Humanoid Platform for Loco-Manipulation|https://arxiv.org/abs/2502.00893;TidyBot++: An Open-Source Holonomic Mobile Manipulator for Robot Learning|https://arxiv.org/abs/2412.10447;Unified Video Action Model|https://arxiv.org/abs/2503.00200;A Study of Perceived Safety for Soft Robotics in Caregiving Tasks|https://arxiv.org/abs/2503.20916;DROID: A Large-Scale In-The-Wild Robot Manipulation Dataset|https://arxiv.org/abs/2403.12945;DexMachina: Functional Retargeting for Bimanual Dexterous Manipulation|https://arxiv.org/abs/2505.24853;Geometry-aware 4D Video Generation for Robot Manipulation|https://arxiv.org/abs/2507.01099;Vision in Action: Learning Active Perception from Human Demonstrations|https://arxiv.org/abs/2506.15666;Efficient Part-level 3D Object Generation via Dual Volume Packing|https://arxiv.org/abs/2506.09980;BEHAVIOR Robot Suite: Streamlining Real-World Whole-Body Manipulation for Everyday Household Activities|https://arxiv.org/abs/2503.05652;Latent Policy Barrier: Learning Robust Visuomotor Policies by Staying In-Distribution|https://arxiv.org/abs/2508.05941
斯坦福-视觉与学习实验室 (SVL),美国,USA,斯坦福大学,计算机视觉基础、多模态融合、具身 AI 与机器人感知,世界领先的实验室之一，成果广泛应用于机器人、AR 等领域。,李飞飞,"Embodied Agent Interface, ReKep, TRANSIC",-122.169719,37.427474,https://svl.stanford.edu/,Li Fei-Fei,Stanford University,,"Spatial Mental Modeling from Limited Views|https://arxiv.org/abs/2506.21458;Discovering Latent Graphs with GFlowNets for Diverse Conditional Image Generation|https://arxiv.org/abs/2510.22107;Open X-Embodiment: Robotic Learning Datasets and RT-X Models|https://arxiv.org/abs/2310.08864;ENACT: Evaluating Embodied Cognition with World Modeling of Egocentric Interaction|https://arxiv.org/abs/2511.20937;A Real-to-Sim-to-Real Approach to Robotic Manipulation with VLM-Generated Iterative Keypoint Rewards|https://arxiv.org/abs/2502.08643;Towards Fine-Grained Video Question Answering|https://arxiv.org/abs/2503.06820;MoMaGen: Generating Demonstrations under Soft and Hard Constraints for Multi-Step Bimanual Mobile Manipulation|https://arxiv.org/abs/2510.18316;Thinking in Space: How Multimodal Large Language Models See, Remember, and Recall Spaces|https://arxiv.org/abs/2412.14171;Cambrian-S: Towards Spatial Supersensing in Video|https://arxiv.org/abs/2511.04670;Wild2Avatar: Rendering Humans Behind Occlusions|https://arxiv.org/abs/2401.00431;T*: Re-thinking Temporal Search for Long-Form Video Understanding|https://arxiv.org/abs/2504.02259;Dual encoding feature filtering generalized attention UNET for retinal vessel segmentation|https://arxiv.org/abs/2506.02312;Why Automate This? Exploring Correlations between Desire for Robotic Automation, Invested Time and Well-Being|https://arxiv.org/abs/2501.06348;WorldScore: A Unified Evaluation Benchmark for World Generation|https://arxiv.org/abs/2504.00983;Repurposing 2D Diffusion Models with Gaussian Atlas for 3D Generation|https://arxiv.org/abs/2503.15877;Embodied Agent Interface: Benchmarking LLMs for Embodied Decision Making|https://arxiv.org/abs/2410.07166;Flow to the Mode: Mode-Seeking Diffusion Autoencoders for State-of-the-Art Image Tokenization|https://arxiv.org/abs/2503.11056;BEHAVIOR Robot Suite: Streamlining Real-World Whole-Body Manipulation for Everyday Household Activities|https://arxiv.org/abs/2503.05652;UAD: Unsupervised Affordance Distillation for Generalization in Robotic Manipulation|https://arxiv.org/abs/2506.09284;Chain-of-Modality: Learning Manipulation Programs from Multimodal Human Videos with Vision-Language-Models|https://arxiv.org/abs/2504.13351;GET-USE: Learning Generalized Tool Usage for Bimanual Mobile Manipulation via Simulated Embodiment Extensions|https://arxiv.org/abs/2510.25754;The Language of Motion: Unifying Verbal and Non-verbal Language of 3D Human Motion|https://arxiv.org/abs/2412.10523"
斯坦福-人工智能实验室 (SAIL),美国,USA,斯坦福大学,机器学习、自然语言处理、计算机视觉、智能机器人,成立于 1962 年，是全球顶尖的人工智能研究机构。,Christopher Manning,"Real-Time Anomaly Detection and Reactive Planning, Position, ManiWAV",-122.169719,37.427474,https://ai.stanford.edu/,Christopher Manning,Stanford University,,"DistillKac: Few-Step Image Generation via Damped Wave Equations|https://arxiv.org/abs/2509.21513;Tversky Neural Networks: Psychologically Plausible Deep Learning with Differentiable Tversky Similarity|https://arxiv.org/abs/2506.11035;AuroraCap: Efficient, Performant Video Detailed Captioning and a New Benchmark|https://arxiv.org/abs/2410.03051"
斯坦福-机器人与具身智能实验室 (REAL),美国,USA,斯坦福大学,机器人感知、自主学习、人机交互、具身人工智能,专注于开发学习算法，让机器人通过与物理世界的交互来执行复杂任务并协助人类。,Shuran Song,"The All-Seeing Robot with Whole-body Dexterity, Diffusion Guided Control, Graph Embodiment Transformer",-122.169719,37.427474,https://real.stanford.edu/,Shuran Song,Stanford University,,Dynamics-Guided Diffusion Model for Sensor-less Robot Manipulator Design|https://arxiv.org/abs/2402.15038;DynaGuide: Steering Diffusion Polices with Active Dynamic Guidance|https://arxiv.org/abs/2506.13922;RoboPanoptes: The All-seeing Robot with Whole-body Dexterity|https://arxiv.org/abs/2501.05420;Robot Trains Robot: Automatic Real-World Policy Adaptation and Learning for Humanoids|https://arxiv.org/abs/2508.12252;Open X-Embodiment: Robotic Learning Datasets and RT-X Models|https://arxiv.org/abs/2310.08864;Cosmos-Reason1: From Physical Common Sense To Embodied Reasoning|https://arxiv.org/abs/2503.15558;World Simulation with Video Foundation Models for Physical AI|https://arxiv.org/abs/2511.00062;UMI-on-Air: Embodiment-Aware Guidance for Embodiment-Agnostic Visuomotor Policies|https://arxiv.org/abs/2510.02614;Compliant Residual DAgger: Improving Real-World Contact-Rich Manipulation with Human Corrections|https://arxiv.org/abs/2506.16685;Should VLMs be Pre-trained with Image Data?|https://arxiv.org/abs/2503.07603;Rectified Point Flow: Generic Point Cloud Pose Estimation|https://arxiv.org/abs/2506.05282;Adaptive Compliance Policy: Learning Approximate Compliance for Diffusion Guided Control|https://arxiv.org/abs/2410.09309;A Practical Guide for Incorporating Symmetry in Diffusion Policy|https://arxiv.org/abs/2505.13431;DexUMI: Using Human Hand as the Universal Manipulation Interface for Dexterous Manipulation|https://arxiv.org/abs/2505.21864;ToddlerBot: Open-Source ML-Compatible Humanoid Platform for Loco-Manipulation|https://arxiv.org/abs/2502.00893;TidyBot++: An Open-Source Holonomic Mobile Manipulator for Robot Learning|https://arxiv.org/abs/2412.10447;Unified Video Action Model|https://arxiv.org/abs/2503.00200;A Study of Perceived Safety for Soft Robotics in Caregiving Tasks|https://arxiv.org/abs/2503.20916;DROID: A Large-Scale In-The-Wild Robot Manipulation Dataset|https://arxiv.org/abs/2403.12945;DexMachina: Functional Retargeting for Bimanual Dexterous Manipulation|https://arxiv.org/abs/2505.24853;Geometry-aware 4D Video Generation for Robot Manipulation|https://arxiv.org/abs/2507.01099;Vision in Action: Learning Active Perception from Human Demonstrations|https://arxiv.org/abs/2506.15666;Efficient Part-level 3D Object Generation via Dual Volume Packing|https://arxiv.org/abs/2506.09980;BEHAVIOR Robot Suite: Streamlining Real-World Whole-Body Manipulation for Everyday Household Activities|https://arxiv.org/abs/2503.05652;Latent Policy Barrier: Learning Robust Visuomotor Policies by Staying In-Distribution|https://arxiv.org/abs/2508.05941
机器人系统实验室 (RSL),瑞士,Europe,苏黎世联邦理工学院(ETH Zürich),机械臂、足式机器人、新型驱动方法、运动与操作的控制算法,研究重点包括机械臂和足式机器人，涵盖设计、驱动、控制和优化算法。,Marco Hutter,"Learning Robust Autonomous Navigation, ANYmal parkour, Collaborative Multi-Robot Mapping",8.548,47.376,https://rsl.ethz.ch/,Marco Hutter,ETH Zurich,,"Feature-Based vs. GAN-Based Learning from Demonstrations: When and Why|https://arxiv.org/abs/2507.05906;Efficient Model-Based Reinforcement Learning for Robot Control via Online Learning|https://arxiv.org/abs/2510.18518;Kleinkram: Open Robotic Data Management|https://arxiv.org/abs/2511.20492;Parkour in the Wild: Learning a General and Extensible Agile Locomotion Policy Using Multi-expert Distillation and RL Fine-tuning|https://arxiv.org/abs/2505.11164;Isaac Lab: A GPU-Accelerated Simulation Framework for Multi-Modal Robot Learning|https://arxiv.org/abs/2511.04831;Robust Recovery Controller for a Quadrupedal Robot using Deep Reinforcement Learning|https://arxiv.org/abs/1901.07517;ExT: Towards Scalable Autonomous Excavation via Large-Scale Multi-Task Pretraining and Fine-Tuning|https://arxiv.org/abs/2509.14992;Building Forest Inventories with Autonomous Legged Robots -- System, Lessons, and Challenges Ahead|https://arxiv.org/abs/2506.20315;Multi-Domain Motion Embedding: Expressive Real-Time Mimicry for Legged Robots|https://arxiv.org/abs/2512.07673;Offline Robotic World Model: Learning Robotic Policies without a Physics Simulator|https://arxiv.org/abs/2504.16680;Collaborative Loco-Manipulation for Pick-and-Place Tasks with Dynamic Reward Curriculum|https://arxiv.org/abs/2509.13239;RSL-RL: A Learning Library for Robotics Research|https://arxiv.org/abs/2509.10771;Reinforcement Learning Control for Autonomous Hydraulic Material Handling Machines with Underactuated Tools|https://arxiv.org/abs/2410.05093;Robotic World Model: A Neural Network Simulator for Robust Policy Optimization in Robotics|https://arxiv.org/abs/2501.10100;Attention-Based Map Encoding for Learning Generalized Legged Locomotion|https://arxiv.org/abs/2506.09588;Learning Deployable Locomotion Control via Differentiable Simulation|https://arxiv.org/abs/2404.02887;Efficient Learning-Based Control of a Legged Robot in Lunar Gravity|https://arxiv.org/abs/2509.10128;Radiance Fields for Robotic Teleoperation|https://arxiv.org/abs/2407.20194;Pretraining in Actor-Critic Reinforcement Learning for Robot Locomotion|https://arxiv.org/abs/2510.12363;Towards bridging the gap: Systematic sim-to-real transfer for diverse legged robots|https://arxiv.org/abs/2509.06342;Enhancing Robotic Precision in Construction: A Modular Factor Graph-Based Framework to Deflection and Backlash Compensation Using High-Accuracy Accelerometers|https://arxiv.org/abs/2501.14280;Boxi: Design Decisions in the Context of Algorithmic Performance for Robotics|https://arxiv.org/abs/2504.18500;Motion Priors Reimagined: Adapting Flat-Terrain Skills for Complex Quadruped Mobility|https://arxiv.org/abs/2505.16084;Dynamic object goal pushing with mobile manipulators through model-free constrained reinforcement learning|https://arxiv.org/abs/2502.01546;CompSLAM: Complementary Hierarchical Multi-Modal Localization and Mapping for Robot Autonomy in Underground Environments|https://arxiv.org/abs/2505.06483;Spatially-Enhanced Recurrent Memory for Long-Range Mapless Navigation via End-to-End Reinforcement Learning|https://arxiv.org/abs/2506.05997;ForestLPR: LiDAR Place Recognition in Forests Attentioning Multiple BEV Density Images|https://arxiv.org/abs/2503.04475;Learned Perceptive Forward Dynamics Model for Safe and Platform-aware Robotic Navigation|https://arxiv.org/abs/2504.19322;Learning coordinated badminton skills for legged manipulators|https://arxiv.org/abs/2505.22974;LEVA: A high-mobility logistic vehicle with legged suspension|https://arxiv.org/abs/2503.10028;FOCI: Trajectory Optimization on Gaussian Splats|https://arxiv.org/abs/2505.08510;Diffusion Based Robust LiDAR Place Recognition|https://arxiv.org/abs/2504.12412;GraspQP: Differentiable Optimization of Force Closure for Diverse and Robust Dexterous Grasping|https://arxiv.org/abs/2508.15002;Learning Quiet Walking for a Small Home Robot|https://arxiv.org/abs/2502.10983;NaviTrace: Evaluating Embodied Navigation of Vision-Language Models|https://arxiv.org/abs/2510.26909;TartanGround: A Large-Scale Dataset for Ground Robot Perception and Navigation|https://arxiv.org/abs/2505.10696;DiskChunGS: Large-Scale 3D Gaussian SLAM Through Chunk-Based Memory Management|https://arxiv.org/abs/2511.23030;DAPPER: Discriminability-Aware Policy-to-Policy Preference-Based Reinforcement Learning for Query-Efficient Robot Skill Acquisition|https://arxiv.org/abs/2505.06357;What Matters in RL-Based Methods for Object-Goal Navigation? An Empirical Study and A Unified Framework|https://arxiv.org/abs/2510.01830;Toward Task Generalization via Memory Augmentation in Meta-Reinforcement Learning|https://arxiv.org/abs/2502.01521;iPlanner: Imperative Path Planning|https://arxiv.org/abs/2302.11434;Whole-body End-Effector Pose Tracking|https://arxiv.org/abs/2409.16048;Continuous-Time State Estimation Methods in Robotics: A Survey|https://arxiv.org/abs/2411.03951;Large Scale Robotic Material Handling: Learning, Planning, and Control|https://arxiv.org/abs/2508.09003;Robust Ladder Climbing with a Quadrupedal Robot|https://arxiv.org/abs/2409.17731;Towards Learning Boulder Excavation with Hydraulic Excavators|https://arxiv.org/abs/2509.17683;Calibrated Dynamic Modeling for Force and Payload Estimation in Hydraulic Machinery|https://arxiv.org/abs/2510.11574;CaRLi-V: Camera-RADAR-LiDAR Point-Wise 3D Velocity Estimation|https://arxiv.org/abs/2511.01383;Informed, Constrained, Aligned: A Field Analysis on Degeneracy-aware Point Cloud Registration in the Wild|https://arxiv.org/abs/2408.11809;LLM-Handover:Exploiting LLMs for Task-Oriented Robot-Human Handovers|https://arxiv.org/abs/2509.24706;Magnecko: Design and Control of a Quadrupedal Magnetic Climbing Robot|https://arxiv.org/abs/2504.13672;Risk-Guided Diffusion: Toward Deploying Robot Foundation Models in Space, Where Failure Is Not An Option|https://arxiv.org/abs/2506.17601;Multi-critic Learning for Whole-body End-effector Twist Tracking|https://arxiv.org/abs/2507.08656;Obstacle-Avoidant Leader Following with a Quadruped Robot|https://arxiv.org/abs/2410.00572;Learning Accurate Whole-body Throwing with High-frequency Residual Policy and Pullback Tube Acceleration|https://arxiv.org/abs/2506.16986;Divide, Discover, Deploy: Factorized Skill Learning with Symmetry and Style Priors|https://arxiv.org/abs/2508.19953;Holistic Fusion: Task- and Setup-Agnostic Robot Localization and State Estimation with Factor Graphs|https://arxiv.org/abs/2504.06479;DFM: Deep Fourier Mimic for Expressive Dance Motion Learning|https://arxiv.org/abs/2502.10980;Constrained Style Learning from Imperfect Demonstrations under Task Optimality|https://arxiv.org/abs/2507.09371;Experience is the Best Teacher: Grounding VLMs for Robotics through Self-Generated Memory|https://arxiv.org/abs/2507.16713"
"Sensing, Interaction & Perception Lab (SIPLAB)",瑞士,Europe,苏黎世联邦理工学院(ETH Zürich),计算交互、具身感知、移动健康、混合现实、人机交互,一个跨学科研究小组，专注于计算交互、具身感知和移动健康领域的研究。,Christian Holz,"EgoPoser, Ultra Inertial Poser, Robust Heart Rate Detection via Multi-Site Photoplethysmography",8.548,47.376,https://siplab.ethz.ch/,Christian Holz,ETH Zurich,,EgoSim: An Egocentric Multi-view Simulator and Real Dataset for Body-worn Cameras during Motion and Activity|https://arxiv.org/abs/2502.18373;egoEMOTION: Egocentric Vision and Physiological Signals for Emotion and Personality Recognition in Real-World Tasks|https://arxiv.org/abs/2510.22129;egoPPG: Heart Rate Estimation from Eye-Tracking Cameras in Egocentric Systems to Benefit Downstream Vision Tasks|https://arxiv.org/abs/2502.20879;WildPPG: A Real-World PPG Dataset of Long Continuous Recordings|https://arxiv.org/abs/2412.17540;Data-Driven Object Tracking: Integrating Modular Neural Networks into a Kalman Framework|https://arxiv.org/abs/2504.02519;Human Motion Capture from Loose and Sparse Inertial Sensors with Garment-aware Diffusion Models|https://arxiv.org/abs/2506.15290;Group Inertial Poser: Multi-Person Pose and Global Translation from Sparse Inertial Sensors and Ultra-Wideband Ranging|https://arxiv.org/abs/2510.21654;Regressor-Guided Image Editing Regulates Emotional Response to Reduce Online Engagement|https://arxiv.org/abs/2501.12289
机器人与人工智能实验室 (RAIL),中国,China,同济大学,工业机器人、自主移动机器人、2D/3D视觉、仿人机器人,国内最早从事机器人与人工智能研究的实验室之一，仿人机器人连续8年获ROBOCUP冠军。,陈启军,"复杂堆叠工件3D抓取, SCARA机器人, 轨道标记机器人",121.505835,31.281111,https://rail.tongji.edu.cn/,Qijun Chen,Tongji University,,DCPI-Depth: Explicitly Infusing Dense Correspondence Prior to Unsupervised Monocular Depth Estimation|https://arxiv.org/abs/2405.16960;NavComposer: Composing Language Instructions for Navigation Trajectories through Action-Scene-Object Modularization|https://arxiv.org/abs/2507.10894;Rethinking the Embodied Gap in Vision-and-Language Navigation: A Holistic Study of Physical and Visual Disparities|https://arxiv.org/abs/2507.13019;Realizing Text-Driven Motion Generation on NAO Robot: A Reinforcement Learning-Optimized Control Pipeline|https://arxiv.org/abs/2506.05117;Fully Exploiting Vision Foundation Model's Profound Prior Knowledge for Generalizable RGB-Depth Driving Scene Parsing|https://arxiv.org/abs/2502.06219;SparseWorld-TC: Trajectory-Conditioned Sparse Occupancy World Model|https://arxiv.org/abs/2511.22039;Bootstrapping Vision-language Models for Self-supervised Remote Physiological Measurement|https://arxiv.org/abs/2407.08507;CleanPose: Category-Level Object Pose Estimation via Causal Learning and Knowledge Distillation|https://arxiv.org/abs/2502.01312;CLASH: Collaborative Large-Small Hierarchical Framework for Continuous Vision-and-Language Navigation|https://arxiv.org/abs/2512.10360;LIX: Implicitly Infusing Spatial Geometric Prior Knowledge into Visual Semantic Segmentation for Autonomous Driving|https://arxiv.org/abs/2403.08215;A Birotation Solution for Relative Pose Problems|https://arxiv.org/abs/2505.02025;SAM-LAD: Segment Anything Model Meets Zero-Shot Logic Anomaly Detection|https://arxiv.org/abs/2406.00625;MLANet: Multi-Level Attention Network with Sub-instruction for Continuous Vision-and-Language Navigation|https://arxiv.org/abs/2303.01396;Kinematics-Aware Multi-Policy Reinforcement Learning for Force-Capable Humanoid Loco-Manipulation|https://arxiv.org/abs/2511.21169
西湖机器人科技 / 机器智能实验室 (MiLAB),中国,China,西湖大学,足式机器人、通用行为智能、深度强化学习,由王东林博士团队创办，基于西湖大学MiLAB的理论研究，研发具有通用行为智能的足式机器人。,王东林,"PiTe, Latent Diffusion Prior Enhanced Deep Unfolding for Snapshot Spectral Compressive Imaging, PathMMU",120.085586,30.18353,https://milab.westlake.edu.cn/,Donglin Wang,Westlake University,,"Cobra: Extending Mamba to Multi-Modal Large Language Model for Efficient Inference|https://arxiv.org/abs/2403.14520;VLA-RFT: Vision-Language-Action Reinforcement Fine-tuning with Verified Rewards in World Simulators|https://arxiv.org/abs/2510.00406;RationalVLA: A Rational Vision-Language-Action Model with Dual System|https://arxiv.org/abs/2506.10826;CARP: Visuomotor Policy Learning via Coarse-to-Fine Autoregressive Prediction|https://arxiv.org/abs/2412.06782;Activation-wise Propagation: A Universal Strategy to Break Timestep Constraints in Spiking Neural Networks for 3D Data Processing|https://arxiv.org/abs/2502.12791;Rethinking Target Label Conditioning in Adversarial Attacks: A 2D Tensor-Guided Generative Approach|https://arxiv.org/abs/2504.14137;Executable Analytic Concepts as the Missing Link Between VLM Insight and Precise Manipulation|https://arxiv.org/abs/2510.07975;RobustVLA: Robustness-Aware Reinforcement Post-Training for Vision-Language-Action Models|https://arxiv.org/abs/2511.01331;Unlock Reliable Skill Inference for Quadruped Adaptive Behavior by Skill Graph|https://arxiv.org/abs/2311.06015;OpenHelix: A Short Survey, Empirical Analysis, and Open-Source Dual-System VLA Model for Robotic Manipulation|https://arxiv.org/abs/2505.03912;Rethinking Latent Redundancy in Behavior Cloning: An Information Bottleneck Approach for Robot Manipulation|https://arxiv.org/abs/2502.02853;VLA-Adapter: An Effective Paradigm for Tiny-Scale Vision-Language-Action Model|https://arxiv.org/abs/2509.09372;TDMPBC: Self-Imitative Reinforcement Learning for Humanoid Robot Control|https://arxiv.org/abs/2502.17322;MoRE: Unlocking Scalability in Reinforcement Learning for Quadruped Vision-Language-Action Models|https://arxiv.org/abs/2503.08007;Balancing Signal and Variance: Adaptive Offline RL Post-Training for VLA Flow Models|https://arxiv.org/abs/2509.04063;ReconVLA: Reconstructive Vision-Language-Action Model as Effective Robot Perceiver|https://arxiv.org/abs/2508.10333;L1 Sample Flow for Efficient Visuomotor Learning|https://arxiv.org/abs/2511.17898;M2IST: Multi-Modal Interactive Side-Tuning for Efficient Referring Expression Comprehension|https://arxiv.org/abs/2407.01131;QUART-Online: Latency-Free Large Multimodal Language Model for Quadruped Robot Learning|https://arxiv.org/abs/2412.15576;VLA^2: Empowering Vision-Language-Action Models with an Agentic Framework for Unseen Concept Manipulation|https://arxiv.org/abs/2510.14902;HiF-VLA: Hindsight, Insight and Foresight through Motion Representation for Vision-Language-Action Models|https://arxiv.org/abs/2512.09928;Score and Distribution Matching Policy: Advanced Accelerated Visuomotor Policies via Matched Distillation|https://arxiv.org/abs/2412.09265;Long-VLA: Unleashing Long-Horizon Capability of Vision Language Action Model for Robot Manipulation|https://arxiv.org/abs/2508.19958;GEVRM: Goal-Expressive Video Generation Model For Robust Visual Manipulation|https://arxiv.org/abs/2502.09268;Boundary-to-Region Supervision for Offline Safe Reinforcement Learning|https://arxiv.org/abs/2509.25727;Efficient Online RL Fine Tuning with Offline Pre-trained Policy Only|https://arxiv.org/abs/2505.16856;Enhancing Adversarial Transferability via Component-Wise Transformation|https://arxiv.org/abs/2501.11901;Multi-Task Multi-Agent Reinforcement Learning via Skill Graphs|https://arxiv.org/abs/2507.06690;Towards a Unified Understanding of Robot Manipulation: A Comprehensive Survey|https://arxiv.org/abs/2510.10903;Humanoid-VLA: Towards Universal Humanoid Control with Visual Integration|https://arxiv.org/abs/2502.14795;Discovering Self-Protective Falling Policy for Humanoid Robot via Deep Reinforcement Learning|https://arxiv.org/abs/2512.01336;VLAS: Vision-Language-Action Model With Speech Instructions For Customized Robot Manipulation|https://arxiv.org/abs/2502.13508;Unicorn: Text-Only Data Synthesis for Vision Language Model Training|https://arxiv.org/abs/2503.22655;VARD: Efficient and Dense Fine-Tuning for Diffusion Models with Value-based RL|https://arxiv.org/abs/2505.15791;Learning Robotic Policy with Imagined Transition: Mitigating the Trade-off between Robustness and Optimality|https://arxiv.org/abs/2503.10484;Spatial Forcing: Implicit Spatial Representation Alignment for Vision-language-action Model|https://arxiv.org/abs/2510.12276;VCoT-Grasp: Grasp Foundation Models with Visual Chain-of-Thought Reasoning for Language-driven Grasp Generation|https://arxiv.org/abs/2510.05827;SSR: Enhancing Depth Perception in Vision-Language Models via Rationale-Guided Spatial Reasoning|https://arxiv.org/abs/2505.12448;Filter, Correlate, Compress: Training-Free Token Reduction for MLLM Acceleration|https://arxiv.org/abs/2411.17686;Dynamic Adaptive Legged Locomotion Policy via Decoupling Reaction Force Control and Gait Control|https://arxiv.org/abs/2509.13737;Integrating Trajectory Optimization and Reinforcement Learning for Quadrupedal Jumping with Terrain-Adaptive Landing|https://arxiv.org/abs/2509.12776;ReinboT: Amplifying Robot Visual-Language Manipulation with Reinforcement Learning|https://arxiv.org/abs/2505.07395;QUAR-VLA: Vision-Language-Action Model for Quadruped Robots|https://arxiv.org/abs/2312.14457;Unveiling the Potential of Vision-Language-Action Models with Open-Ended Multimodal Instructions|https://arxiv.org/abs/2505.11214;TrajBooster: Boosting Humanoid Whole-Body Manipulation via Trajectory-Centric Learning|https://arxiv.org/abs/2509.11839;Unified Diffusion VLA: Vision-Language-Action Model via Joint Discrete Denoising Diffusion Process|https://arxiv.org/abs/2511.01718;Exploring the Evolution of Physics Cognition in Video Generation: A Survey|https://arxiv.org/abs/2503.21765;Robust Online Residual Refinement via Koopman-Guided Dynamics Modeling|https://arxiv.org/abs/2509.12562;CEED-VLA: Consistency Vision-Language-Action Model with Early-Exit Decoding|https://arxiv.org/abs/2506.13725"
机器人与自动化研究中心,香港,China,香港城市大学,医疗机器人、人机交互、微/纳/生物机器人、智能自动化,研究方向包括手术机器人、社交机器人、微飞行机器人和多机器人系统等。,李文荣（中科院-城大机器人学联合实验室港方主任）,Propulsion-based High-Speed Hopping Robot with Active Leg Swings and Terrain Adaptability,114.203789,22.421639,https://www.cityu.edu.hk/cra/,Wenrong Li,City University of Hong Kong,,"Toward Reliable AR-Guided Surgical Navigation: Interactive Deformation Modeling with Data-Driven Biomechanics and Prompts|https://arxiv.org/abs/2506.08048;NPHardEval4V: Dynamic Evaluation of Large Vision-Language Models with Effects of Vision|https://arxiv.org/abs/2403.01777;Grounded Knowledge-Enhanced Medical Vision-Language Pre-training for Chest X-Ray|https://arxiv.org/abs/2404.14750;Context Consistency Learning via Sentence Removal for Semi-Supervised Video Paragraph Grounding|https://arxiv.org/abs/2506.18476;EndoPerfect: High-Accuracy Monocular Depth Estimation and 3D Reconstruction for Endoscopic Surgery via NeRF-Stereo Fusion|https://arxiv.org/abs/2410.04041;Kernel-based Unsupervised Embedding Alignment for Enhanced Visual Representation in Vision-language Models|https://arxiv.org/abs/2506.02557;SAP-Bench: Benchmarking Multimodal Large Language Models in Surgical Action Planning|https://arxiv.org/abs/2506.07196;On Robustness of Vision-Language-Action Model against Multi-Modal Perturbations|https://arxiv.org/abs/2510.00037;THCRL: Trusted Hierarchical Contrastive Representation Learning for Multi-View Clustering|https://arxiv.org/abs/2512.00368;Boomda: Balanced Multi-objective Optimization for Multimodal Domain Adaptation|https://arxiv.org/abs/2511.08152;GenTe: Generative Real-world Terrains for General Legged Robot Locomotion Control|https://arxiv.org/abs/2504.09997;Can Generalist Vision Language Models (VLMs) Rival Specialist Medical VLMs? Benchmarking and Strategic Insights|https://arxiv.org/abs/2506.17337;SurgVLM: A Large Vision-Language Model and Systematic Evaluation Benchmark for Surgical Intelligence|https://arxiv.org/abs/2506.02555;FLUX-Makeup: High-Fidelity, Identity-Consistent, and Robust Makeup Transfer via Diffusion Transformer|https://arxiv.org/abs/2508.05069;Other Vehicle Trajectories Are Also Needed: A Driving World Model Unifies Ego-Other Vehicle Trajectories in Video Latent Space|https://arxiv.org/abs/2503.09215;Endo3R: Unified Online Reconstruction from Dynamic Monocular Endoscopic Video|https://arxiv.org/abs/2504.03198;DynaMIC: Dynamic Multimodal In-Context Learning Enabled Embodied Robot Counterfactual Resistance Ability|https://arxiv.org/abs/2509.24413;UCAgents: Unidirectional Convergence for Visual Evidence Anchored Multi-Agent Medical Decision-Making|https://arxiv.org/abs/2512.02485;Systematic Evaluation and Guidelines for Segment Anything Model in Surgical Video Analysis|https://arxiv.org/abs/2501.00525;CustomVideo: Customizing Text-to-Video Generation with Multiple Subjects|https://arxiv.org/abs/2401.09962;NanoControl: A Lightweight Framework for Precise and Efficient Control in Diffusion Transformer|https://arxiv.org/abs/2508.10424;Generative Diffusion Contrastive Network for Multi-View Clustering|https://arxiv.org/abs/2509.09527;ClipGS: Clippable Gaussian Splatting for Interactive Cinematic Visualization of Volumetric Medical Data|https://arxiv.org/abs/2507.06647;CTA-Flux: Integrating Chinese Cultural Semantics into High-Quality English Text-to-Image Communities|https://arxiv.org/abs/2508.14405;Self-Learning Hyperspectral and Multispectral Image Fusion via Adaptive Residual Guided Subspace Diffusion Model|https://arxiv.org/abs/2505.11800;Extreme Cardiac MRI Analysis under Respiratory Motion: Results of the CMRxMotion Challenge|https://arxiv.org/abs/2507.19165;Trusted Mamba Contrastive Network for Multi-View Clustering|https://arxiv.org/abs/2412.16487;A Super-pixel-based Approach to the Stable Interpretation of Neural Networks|https://arxiv.org/abs/2412.14509;Diffusion Stabilizer Policy for Automated Surgical Robot Manipulations|https://arxiv.org/abs/2503.01252;ToonComposer: Streamlining Cartoon Production with Generative Post-Keyframing|https://arxiv.org/abs/2508.10881;A Learning-based Control Methodology for Transitioning VTOL UAVs|https://arxiv.org/abs/2512.03548;Learning dissection trajectories from expert surgical videos via imitation learning with equivariant diffusion|https://arxiv.org/abs/2506.04716;AnyCharV: Bootstrap Controllable Character Video Generation with Fine-to-Coarse Guidance|https://arxiv.org/abs/2502.08189;Towards High-Consistency Embodied World Model with Multi-View Trajectory Videos|https://arxiv.org/abs/2511.12882;SegSTRONG-C: Segmenting Surgical Tools Robustly On Non-adversarial Generated Corruptions -- An EndoVis'24 Challenge|https://arxiv.org/abs/2407.11906;Align-Then-stEer: Adapting the Vision-Language Action Models through Unified Latent Guidance|https://arxiv.org/abs/2509.02055;WonderVerse: Extendable 3D Scene Generation with Video Generative Models|https://arxiv.org/abs/2503.09160;Medical Large Vision Language Models with Multi-Image Visual Ability|https://arxiv.org/abs/2505.19031;Toward Robust Medical Fairness: Debiased Dual-Modal Alignment via Text-Guided Attribute-Disentangled Prompt Learning for Vision-Language Models|https://arxiv.org/abs/2508.18886;Contact Map Transfer with Conditional Diffusion Model for Generalizable Dexterous Grasp Generation|https://arxiv.org/abs/2511.01276;MoEGCL: Mixture of Ego-Graphs Contrastive Representation Learning for Multi-View Clustering|https://arxiv.org/abs/2511.05876;MagicRoad: Semantic-Aware 3D Road Surface Reconstruction via Obstacle Inpainting|https://arxiv.org/abs/2507.23340"
香港大学机械工程系机器人实验室,香港,China,香港大学,软体机器人、柔性连续体机器人、仿生机器人,主要研究软体机器人、用于微创手术的柔性机器人系统以及受自然启发的仿生机器人。,郭嘉威、岑浩璋、林海松（2022-2024科研助理教授）,"Intelligent Shape Decoding of a Soft Optical Waveguide Sensor (AIS, 2025)",114.144538,22.267821,"	https://www.mech.hku.hk/robotics 7",Jiawei Guo; Haozhang Cen; Haisong Lin,University of Hong Kong,,CROP: Contextual Region-Oriented Visual Token Pruning|https://arxiv.org/abs/2505.21233;ScaleLong: A Multi-Timescale Benchmark for Long Video Understanding|https://arxiv.org/abs/2505.23922;ToxicTextCLIP: Text-Based Poisoning and Backdoor Attacks on CLIP Pre-training|https://arxiv.org/abs/2511.00446;Kwai Keye-VL Technical Report|https://arxiv.org/abs/2507.01949
Hengshuang Zhao老师实验室,香港,China,香港大学,计算机视觉、生成式建模、自动驾驶、具身AI,研究场景理解、视觉内容创作、环境感知与决策规划，以及机器人学习和LLM应用。,赵行爽 (Hengshuang Zhao),"Zero-shot Image Editing with Reference Imitation, LARM, Pixel-GS",114.144538,22.267821,https://i.cs.hku.hk/~hszhao/more.html,Hengshuang Zhao,University of Hong Kong,,"Less is Enough: Training-Free Video Diffusion Acceleration via Runtime-Adaptive Caching|https://arxiv.org/abs/2507.02860;Train Once, Deploy Anywhere: Realize Data-Efficient Dynamic Object Manipulation|https://arxiv.org/abs/2508.14042;HaploVL: A Single-Transformer Baseline for Multi-Modal Understanding|https://arxiv.org/abs/2503.14694;GPT4Scene: Understand 3D Scenes from Videos with Vision-Language Models|https://arxiv.org/abs/2501.01428;PanDA: Towards Panoramic Depth Anything with Unlabeled Panoramas and Mobius Spatial Augmentation|https://arxiv.org/abs/2406.13378;ILLUME+: Illuminating Unified MLLM with Dual Visual Tokenization and Diffusion Refinement|https://arxiv.org/abs/2504.01934;HERMES: A Unified Self-Driving World Model for Simultaneous 3D Scene Understanding and Generation|https://arxiv.org/abs/2501.14729;PlayerOne: Egocentric World Simulator|https://arxiv.org/abs/2506.09995;Depth Anything with Any Prior|https://arxiv.org/abs/2505.10565;Concerto: Joint 2D-3D Self-Supervised Learning Emerges Spatial Representations|https://arxiv.org/abs/2510.23607;ViLLa: Video Reasoning Segmentation with Large Language Model|https://arxiv.org/abs/2407.14500;Sonata: Self-Supervised Learning of Reliable Point Representations|https://arxiv.org/abs/2503.16429;PonderV2: Pave the Way for 3D Foundation Model with A Universal Pre-training Paradigm|https://arxiv.org/abs/2310.08586;LARM: Large Auto-Regressive Model for Long-Horizon Embodied Intelligence|https://arxiv.org/abs/2405.17424;Liquid: Language Models are Scalable and Unified Multi-modal Generators|https://arxiv.org/abs/2412.04332;GPT4Point: A Unified Framework for Point-Language Understanding and Generation|https://arxiv.org/abs/2312.02980;FocalClick-XL: Towards Unified and High-quality Interactive Segmentation|https://arxiv.org/abs/2506.14686;DisCo: Towards Distinct and Coherent Visual Encapsulation in Video MLLMs|https://arxiv.org/abs/2507.10302;LiteReality: Graphics-Ready 3D Scene Reconstruction from RGB-D Scans|https://arxiv.org/abs/2507.02861;Mini-o3: Scaling Up Reasoning Patterns and Interaction Turns for Visual Search|https://arxiv.org/abs/2509.07969;EMOVA: Empowering Language Models to See, Hear and Speak with Vivid Emotions|https://arxiv.org/abs/2409.18042;DreamMask: Boosting Open-vocabulary Panoptic Segmentation with Synthetic Data|https://arxiv.org/abs/2501.02048;BOOD: Boundary-based Out-Of-Distribution Data Generation|https://arxiv.org/abs/2508.00350;DiffCamera: Arbitrary Refocusing on Images|https://arxiv.org/abs/2509.26599;ROSE: Remove Objects with Side Effects in Videos|https://arxiv.org/abs/2508.18633;MiCo: Multi-image Contrast for Reinforcement Visual Reasoning|https://arxiv.org/abs/2506.22434;FashionComposer: Compositional Fashion Image Generation|https://arxiv.org/abs/2412.14168;Seg-VAR: Image Segmentation with Visual Autoregressive Modeling|https://arxiv.org/abs/2511.12594;LayerFlow: A Unified Model for Layer-aware Video Generation|https://arxiv.org/abs/2506.04228;Wan-Move: Motion-controllable Video Generation via Latent Trajectory Guidance|https://arxiv.org/abs/2512.08765;UniMatch V2: Pushing the Limit of Semi-Supervised Semantic Segmentation|https://arxiv.org/abs/2410.10777;PhysMaster: Mastering Physical Representation for Video Generation via Reinforcement Learning|https://arxiv.org/abs/2510.13809;Orient Anything: Learning Robust Object Orientation Estimation from Rendering 3D Models|https://arxiv.org/abs/2412.18605;DiffDoctor: Diagnosing Image Diffusion Models Before Treating|https://arxiv.org/abs/2501.12382;Animate-X++: Universal Character Image Animation with Dynamic Backgrounds|https://arxiv.org/abs/2508.09454;VIP: Vision Instructed Pre-training for Robotic Manipulation|https://arxiv.org/abs/2410.07169;Generalized Decoupled Learning for Enhancing Open-Vocabulary Dense Perception|https://arxiv.org/abs/2508.11256;Empowering Large Language Models with 3D Situation Awareness|https://arxiv.org/abs/2503.23024;VideoAnydoor: High-fidelity Video Object Insertion with Precise Motion Control|https://arxiv.org/abs/2501.01427;GenSpace: Benchmarking Spatially-Aware Image Generation|https://arxiv.org/abs/2505.24870;DreamComposer++: Empowering Diffusion Models with Multi-View Conditions for 3D Content Generation|https://arxiv.org/abs/2507.02299;VisionThink: Smart and Efficient Vision Language Model via Reinforcement Learning|https://arxiv.org/abs/2507.13348;Visual Spatial Tuning|https://arxiv.org/abs/2511.05491;Efficient 3D Perception on Multi-Sweep Point Cloud with Gumbel Spatial Pruning|https://arxiv.org/abs/2411.07742;Modular Customization of Diffusion Models via Blockwise-Parameterized Low-Rank Adaptation|https://arxiv.org/abs/2503.08575;VLN-R1: Vision-Language Navigation via Reinforcement Fine-Tuning|https://arxiv.org/abs/2506.17221;UniLION: Towards Unified Autonomous Driving Model with Linear Group RNNs|https://arxiv.org/abs/2511.01768"
Language and Vision (LaVi) Lab,香港,China,香港大学,自然语言处理 (NLP) 与计算机视觉交叉领域、多模态大模型、具身AI,专注于语言与视觉的结合，探索如何让模型更好地理解和处理视觉与语言信息，并应用于具身智能。,Liwei Wang,"Multi-View Transformer for 3D Visual Grounding, Stratified Transformer for 3D Point Cloud Segmentation, Voxel Field Fusion for 3D Object Detection",114.144538,22.267821,https://lwwangcse.github.io/ ,Liwei Wang,University of Hong Kong,,"NeMo: Needle in a Montage for Video-Language Understanding|https://arxiv.org/abs/2509.24563;Defective Convolutional Networks|https://arxiv.org/abs/1911.08432;Video-3D LLM: Learning Position-Aware Video Representation for 3D Scene Understanding|https://arxiv.org/abs/2412.00493;DeformCL: Learning Deformable Centerline Representation for Vessel Extraction in 3D Medical Image|https://arxiv.org/abs/2506.05820;Diagnosing and Improving Diffusion Models by Estimating the Optimal Loss Value|https://arxiv.org/abs/2506.13763;LarvSeg: Exploring Image Classification Data For Large Vocabulary Semantic Segmentation via Category-wise Attentive Classifier|https://arxiv.org/abs/2501.06862;Rethinking Chain-of-Thought Reasoning for Videos|https://arxiv.org/abs/2512.09616;Distribution Matching Variational AutoEncoder|https://arxiv.org/abs/2512.07778;A Foundational Generative Model for Breast Ultrasound Image Analysis|https://arxiv.org/abs/2501.06869;Co-design of magnetic soft robots with large deformation and contacts via material point method and topology optimization|https://arxiv.org/abs/2503.22767;Learning from Videos for 3D World: Enhancing MLLMs with 3D Vision Geometry Priors|https://arxiv.org/abs/2505.24625;UniLiP: Adapting CLIP for Unified Multimodal Understanding, Generation and Editing|https://arxiv.org/abs/2507.23278;AIM: Adaptive Inference of Multi-Modal LLMs via Token Merging and Pruning|https://arxiv.org/abs/2412.03248;UFO: A Unified Approach to Fine-grained Visual Perception via Open-ended Language Interface|https://arxiv.org/abs/2503.01342;A Chain-of-thought Reasoning Breast Ultrasound Dataset Covering All Histopathology Categories|https://arxiv.org/abs/2509.17046;Fine-grained Spatiotemporal Grounding on Egocentric Videos|https://arxiv.org/abs/2508.00518;Efficient-VLN: A Training-Efficient Vision-Language Navigation Model|https://arxiv.org/abs/2512.10310;GraphMorph: Tubular Structure Extraction by Morphing Predicted Graphs|https://arxiv.org/abs/2502.11731"
潘佳老师实验室,香港,China,香港大学,智能算法、传感器、机器人自主化,研究智能算法、传感器和机器，以实现完全自主的机器人。,潘佳 (Jia Pan),"HHD-GP, BiKC, NetTrack, Heterogeneous Targets Trapping With Swarm Robots",114.144538,22.267821,https://cs.hku.hk/index.php/people/academic-staff/jpan,Jia Pan,University of Hong Kong,,"Lyapunov Stability Learning with Nonlinear Control via Inductive Biases|https://arxiv.org/abs/2511.01283;GeoManip: Geometric Constraints as General Interfaces for Robot Manipulation|https://arxiv.org/abs/2501.09783;DAWN: Dynamic Frame Avatar with Non-autoregressive Diffusion Framework for Talking Head Video Generation|https://arxiv.org/abs/2410.13726;Rethinking Intermediate Representation for VLM-based Robot Manipulation|https://arxiv.org/abs/2511.19315;iPad: Iterative Proposal-centric End-to-End Autonomous Driving|https://arxiv.org/abs/2505.15111;NeuPAN: Direct Point Robot Navigation with End-to-End Model-based Learning|https://arxiv.org/abs/2403.06828;CollaBot: Vision-Language Guided Simultaneous Collaborative Manipulation|https://arxiv.org/abs/2508.03526;LP-ICP: General Localizability-Aware Point Cloud Registration for Robust Localization in Extreme Unstructured Environments|https://arxiv.org/abs/2501.02580;iFlyBot-VLM Technical Report|https://arxiv.org/abs/2511.04976;Internal State Estimation in Groups via Active Information Gathering|https://arxiv.org/abs/2505.10415;A Survey: Learning Embodied Intelligence from Physical Simulators and World Models|https://arxiv.org/abs/2507.00917;Collaborative Assembly Policy Learning of a Sightless Robot|https://arxiv.org/abs/2511.03189;Col-OLHTR: A Novel Framework for Multimodal Online Handwritten Text Recognition|https://arxiv.org/abs/2502.06100;GauSS-MI: Gaussian Splatting Shannon Mutual Information for Active 3D Reconstruction|https://arxiv.org/abs/2504.21067;Binary-Gaussian: Compact and Progressive Representation for 3D Gaussian Segmentation|https://arxiv.org/abs/2512.00944;Multi-modal AI for comprehensive breast cancer prognostication|https://arxiv.org/abs/2410.21256;Aerial Vision-and-Language Navigation with Grid-based View Selection and Map Construction|https://arxiv.org/abs/2503.11091;Optimizing Efficiency of Mixed Traffic through Reinforcement Learning: A Topology-Independent Approach and Benchmark|https://arxiv.org/abs/2501.16728;A Dataset and Benchmark for Robotic Cloth Unfolding Grasp Selection: The ICRA 2024 Cloth Competition|https://arxiv.org/abs/2508.16749;iFlyBot-VLA Technical Report|https://arxiv.org/abs/2511.01914;Lattice Boltzmann Model for Learning Real-World Pixel Dynamicity|https://arxiv.org/abs/2509.16527;Generative Artificial Intelligence in Robotic Manipulation: A Survey|https://arxiv.org/abs/2503.03464;OPA-Pack: Object-Property-Aware Robotic Bin Packing|https://arxiv.org/abs/2505.13339;SViP: Sequencing Bimanual Visuomotor Policies with Object-Centric Motion Primitives|https://arxiv.org/abs/2506.18825;WonderVerse: Extendable 3D Scene Generation with Video Generative Models|https://arxiv.org/abs/2503.09160;Signage-Aware Exploration in Open World using Venue Maps|https://arxiv.org/abs/2410.10143;One-DoF Robotic Design of Overconstrained Limbs with Energy-Efficient, Self-Collision-Free Motion|https://arxiv.org/abs/2509.22002"
OpenDriveLab,香港,China,香港大学,端到端自动驾驶、具身智能,聚焦于机器人和自动驾驶领域，研究内容包括闭环视觉运动控制、世界模型构建、多智能体行为拓扑等。,"Yi Ma, Hongyang Li, Li Chen","Closed-Loop Visuomotor Control with Generative Expectation for Robotic Manipulation, DriveLM, Planning-oriented Autonomous Driving",114.144538,22.267821,https://opendrivelab.com/,Yi Ma; Hongyang Li; Li Chen,University of Hong Kong,,"Curvilinear Structure-preserving Unpaired Cross-domain Medical Image Translation|https://arxiv.org/abs/2510.19679;From Generic to Specialized: A Subspecialty Diagnostic System Powered by Self-Supervised Learning for Cervical Histopathology|https://arxiv.org/abs/2510.10196;Optimization-Guided Diffusion for Interactive Scene Generation|https://arxiv.org/abs/2512.07661;OVSeg3R: Learn Open-vocabulary Instance Segmentation from 2D via 3D Reconstruction|https://arxiv.org/abs/2509.23541;GCRayDiffusion: Pose-Free Surface Reconstruction via Geometric Consistent Ray Diffusion|https://arxiv.org/abs/2503.22349;Decoupled Diffusion Sparks Adaptive Scene Generation|https://arxiv.org/abs/2504.10485;Estimating Body and Hand Motion in an Ego-sensed World|https://arxiv.org/abs/2410.03665;CUPID: Generative 3D Reconstruction via Joint Object and Pose Modeling|https://arxiv.org/abs/2510.20776;CAD-MLLM: Unifying Multimodality-Conditioned CAD Generation With MLLM|https://arxiv.org/abs/2411.04954;Ultrasound Image Synthesis Using Generative AI for Lung Ultrasound Detection|https://arxiv.org/abs/2501.06356;LAPIS: A novel dataset for personalized image aesthetic assessment|https://arxiv.org/abs/2504.07670;Seeing from Another Perspective: Evaluating Multi-View Understanding in MLLMs|https://arxiv.org/abs/2504.15280;Test-time Correction: An Online 3D Detection System via Visual Prompting|https://arxiv.org/abs/2412.07768;Resi-VidTok: An Efficient and Decomposed Progressive Tokenization Framework for Ultra-Low-Rate and Lightweight Video Transmission|https://arxiv.org/abs/2510.25002;Language-Image Alignment with Fixed Text Encoders|https://arxiv.org/abs/2506.04209;MLLM-as-a-Judge for Image Safety without Human Labeling|https://arxiv.org/abs/2501.00192;On the Role of Individual Differences in Current Approaches to Computational Image Aesthetics|https://arxiv.org/abs/2502.20518;Improved YOLOv5s model for key components detection of power transmission lines|https://arxiv.org/abs/2502.06127;DriveLM: Driving with Graph Visual Question Answering|https://arxiv.org/abs/2312.14150;Canonical Factors for Hybrid Neural Fields|https://arxiv.org/abs/2308.15461;Scaling White-Box Transformers for Vision|https://arxiv.org/abs/2405.20299;Foreign-Object Detection in High-Voltage Transmission Line Based on Improved YOLOv8m|https://arxiv.org/abs/2502.07175;Improved YOLOv7 model for insulator defect detection|https://arxiv.org/abs/2502.07179;LanguageMPC: Large Language Models as Decision Makers for Autonomous Driving|https://arxiv.org/abs/2310.03026;ETA: Efficiency through Thinking Ahead, A Dual Approach to Self-Driving with Large Models|https://arxiv.org/abs/2506.07725;SFT Memorizes, RL Generalizes: A Comparative Study of Foundation Model Post-training|https://arxiv.org/abs/2501.17161;CRUISE: Cooperative Reconstruction and Editing in V2X Scenarios using Gaussian Splatting|https://arxiv.org/abs/2507.18473;Pseudo-Simulation for Autonomous Driving|https://arxiv.org/abs/2506.04218;PROFusion: Robust and Accurate Dense Reconstruction via Camera Pose Regression and Optimization|https://arxiv.org/abs/2509.24236;Diffusion Models Learn Low-Dimensional Distributions via Subspace Clustering|https://arxiv.org/abs/2409.02426;Spatiotemporal Learning with Context-aware Video Tubelets for Ultrasound Video Analysis|https://arxiv.org/abs/2503.17475;PANORAMA: The Rise of Omnidirectional Vision in the Embodied AI Era|https://arxiv.org/abs/2509.12989;CodePlot-CoT: Mathematical Visual Reasoning by Thinking with Code-Driven Images|https://arxiv.org/abs/2510.11718;ReSim: Reliable World Simulation for Autonomous Driving|https://arxiv.org/abs/2506.09981;Llama Learns to Direct: DirectorLLM for Human-Centric Video Generation|https://arxiv.org/abs/2412.14484;Cameras as Relative Positional Encoding|https://arxiv.org/abs/2507.10496;A Multi-Stage Optimization Framework for Deploying Learned Image Compression on FPGAs|https://arxiv.org/abs/2511.17135;Delving into Mapping Uncertainty for Mapless Trajectory Prediction|https://arxiv.org/abs/2507.18498;AgiBot World Colosseo: A Large-scale Manipulation Platform for Scalable and Intelligent Embodied Systems|https://arxiv.org/abs/2503.06669;Centaur: Robust End-to-End Autonomous Driving with Test-Time Training|https://arxiv.org/abs/2503.11650;GenDexHand: Generative Simulation for Dexterous Hands|https://arxiv.org/abs/2511.01791;Agility Meets Stability: Versatile Humanoid Control with Heterogeneous Data|https://arxiv.org/abs/2511.17373;DGGT: Feedforward 4D Reconstruction of Dynamic Driving Scenes using Unposed Images|https://arxiv.org/abs/2512.03004;From Simple to Complex Skills: The Case of In-Hand Object Reorientation|https://arxiv.org/abs/2501.05439;XVerse: Consistent Multi-Subject Control of Identity and Semantic Attributes via DiT Modulation|https://arxiv.org/abs/2506.21416;Subjective and Objective Quality Assessment of Banding Artifacts on Compressed Videos|https://arxiv.org/abs/2508.08700;UniVLA: Learning to Act Anywhere with Task-centric Latent Actions|https://arxiv.org/abs/2505.06111;FB-4D: Spatial-Temporal Coherent Dynamic 3D Content Generation with Feature Banks|https://arxiv.org/abs/2503.20784;Reinforced Refinement with Self-Aware Expansion for End-to-End Autonomous Driving|https://arxiv.org/abs/2506.09800;SceneMaker: Open-set 3D Scene Generation with Decoupled De-occlusion and Pose Estimation Model|https://arxiv.org/abs/2512.10957;SimScale: Learning to Drive via Real-World Simulation at Scale|https://arxiv.org/abs/2511.23369;Co-optimizing Physical Reconfiguration Parameters and Controllers for an Origami-inspired Reconfigurable Manipulator|https://arxiv.org/abs/2504.10474;TAPTRv3: Spatial and Temporal Context Foster Robust Tracking of Any Point in Long Video|https://arxiv.org/abs/2411.18671;Coca-Splat: Collaborative Optimization for Camera Parameters and 3D Gaussians|https://arxiv.org/abs/2504.00639;TAPTRv2: Attention-based Position Update Improves Tracking Any Point|https://arxiv.org/abs/2407.16291;Exploring bidirectional bounds for minimax-training of Energy-based models|https://arxiv.org/abs/2506.04609;Recollection from Pensieve: Novel View Synthesis via Learning from Uncalibrated Videos|https://arxiv.org/abs/2505.13440;Research Challenges and Progress in the End-to-End V2X Cooperative Autonomous Driving Competition|https://arxiv.org/abs/2507.21610;CVVNet: A Cross-Vertical-View Network for Gait Recognition|https://arxiv.org/abs/2505.01837;FreeTacMan: Robot-free Visuo-Tactile Data Collection System for Contact-rich Manipulation|https://arxiv.org/abs/2506.01941;SegDINO3D: 3D Instance Segmentation Empowered by Both Image-Level and Object-Level 2D Features|https://arxiv.org/abs/2509.16098;Towards Synergistic, Generalized, and Efficient Dual-System for Robotic Manipulation|https://arxiv.org/abs/2410.08001;Deep Hashing with Semantic Hash Centers for Image Retrieval|https://arxiv.org/abs/2507.08404;MTGS: Multi-Traversal Gaussian Splatting|https://arxiv.org/abs/2503.12552;UCS: A Universal Model for Curvilinear Structure Segmentation|https://arxiv.org/abs/2504.04034;Detect Anything 3D in the Wild|https://arxiv.org/abs/2504.07958;Communicate Less, Synthesize the Rest: Latency-aware Intent-based Generative Semantic Multicasting with Diffusion Models|https://arxiv.org/abs/2411.02334;Is Diversity All You Need for Scalable Robotic Manipulation?|https://arxiv.org/abs/2507.06219;Sim-to-Real Gentle Manipulation of Deformable and Fragile Objects with Stress-Guided Reinforcement Learning|https://arxiv.org/abs/2510.25405;Simplifying DINO via Coding Rate Regularization|https://arxiv.org/abs/2502.10385;QCS: Feature Refining from Quadruplet Cross Similarity for Facial Expression Recognition|https://arxiv.org/abs/2411.01988;Viser: Imperative, Web-based 3D Visualization in Python|https://arxiv.org/abs/2507.22885;Beyond Simple Edits: X-Planner for Complex Instruction-Based Image Editing|https://arxiv.org/abs/2507.05259;PyRoki: A Modular Toolkit for Robot Kinematic Optimization|https://arxiv.org/abs/2505.03728"
Cheng Kar-Shun Robotics Institute (CKSRI),香港,China,香港科技大学,自主飞行、海洋机器人、人形机器人、智能制造、自动驾驶等,一个多学科平台，研究领域涵盖广泛的机器人技术。,"張福民, 李澤湘, 沈劭劼","An Efficient Spatial-Temporal Trajectory Planner for Autonomous Vehicles, D(2)SLAM, FM-Fusion",113.977976,22.336398,https://ri.hkust.edu.hk/,Fumin Zhang; Zexiang Li; Shaojie Shen,Hong Kong University of Science and Technology,,"A Vision-Based Collision Sensing Method for Stable Circular Object Grasping with A Soft Gripper System|https://arxiv.org/abs/2508.05040;Foresight in Motion: Reinforcing Trajectory Prediction with Reward Heuristics|https://arxiv.org/abs/2507.12083;Advancing Multi-agent Traffic Simulation via R1-Style Reinforcement Fine-Tuning|https://arxiv.org/abs/2509.23993;GoIRL: Graph-Oriented Inverse Reinforcement Learning for Multimodal Trajectory Prediction|https://arxiv.org/abs/2506.21121;ReCamDriving: LiDAR-Free Camera-Controlled Novel Trajectory Video Generation|https://arxiv.org/abs/2512.03621;Trust-Preserved Human-Robot Shared Autonomy enabled by Bayesian Relational Event Modeling|https://arxiv.org/abs/2311.02009;SLIM: Scalable and Lightweight LiDAR Mapping in Urban Environments|https://arxiv.org/abs/2409.08681;GS-LIVO: Real-Time LiDAR, Inertial, and Visual Multi-sensor Fused Odometry with Gaussian Mapping|https://arxiv.org/abs/2501.08672;Interleaved LLM and Motion Planning for Generalized Multi-Object Collection in Large Scene Graphs|https://arxiv.org/abs/2507.15782;VIMS: A Visual-Inertial-Magnetic-Sonar SLAM System in Underwater Environments|https://arxiv.org/abs/2506.15126;Temporal and Rotational Calibration for Event-Centric Multi-Sensor Systems|https://arxiv.org/abs/2508.12564;Contact-Aware Motion Planning Among Movable Objects|https://arxiv.org/abs/2502.03317;Speak the Same Language: Global LiDAR Registration on BIM Using Pose Hough Transform|https://arxiv.org/abs/2405.03969;NuRF: Nudging the Particle Filter in Radiance Fields for Robot Visual Localization|https://arxiv.org/abs/2406.00312;SLABIM: A SLAM-BIM Coupled Dataset in HKUST Main Building|https://arxiv.org/abs/2502.16856;ST-GS: Vision-Based 3D Semantic Occupancy Prediction with Spatial-Temporal Gaussian Splatting|https://arxiv.org/abs/2509.16552;Fast and Scalable Game-Theoretic Trajectory Planning with Intentional Uncertainties|https://arxiv.org/abs/2507.12174;CaRoBio: 3D Cable Routing with a Bio-inspired Gripper Fingernail|https://arxiv.org/abs/2508.09558;FALCON: Fast Autonomous Aerial Exploration using Coverage Path Guidance|https://arxiv.org/abs/2407.00577;Autonomous Flights inside Narrow Tunnels|https://arxiv.org/abs/2505.19657;Incorporating Point Uncertainty in Radar SLAM|https://arxiv.org/abs/2402.16082;Design of a Formation Control System to Assist Human Operators in Flying a Swarm of Robotic Blimps|https://arxiv.org/abs/2505.09511;SG-Reg: Generalizable and Efficient Scene Graph Registration|https://arxiv.org/abs/2504.14440;MultiPark: Multimodal Parking Transformer with Next-Segment Prediction|https://arxiv.org/abs/2508.11537;ESVO2: Direct Visual-Inertial Odometry with Stereo Event Cameras|https://arxiv.org/abs/2410.09374;SEPT: Standard-Definition Map Enhanced Scene Perception and Topology Reasoning for Autonomous Driving|https://arxiv.org/abs/2505.12246;CoDriveVLM: VLM-Enhanced Urban Cooperative Dispatching and Motion Planning for Future Autonomous Mobility on Demand Systems|https://arxiv.org/abs/2501.06132;Metric3Dv2: A Versatile Monocular Geometric Foundation Model for Zero-shot Metric Depth and Surface Normal Estimation|https://arxiv.org/abs/2404.15506;Active Contact Engagement for Aerial Navigation in Unknown Environments with Glass|https://arxiv.org/abs/2505.00332;Semi-SMD: Semi-Supervised Metric Depth Estimation via Surrounding Cameras for Autonomous Driving|https://arxiv.org/abs/2503.19713;Control of Marine Robots in the Era of Data-Driven Intelligence|https://arxiv.org/abs/2506.21063;CSMapping: Scalable Crowdsourced Semantic Mapping and Topology Inference for Autonomous Driving|https://arxiv.org/abs/2512.03510"
机器人研究所,香港,China,香港科技大学,移动机器人、无人机、智能制造、医疗机器人等,包含多个下分实验室，如郑家纯机械人研究所、与大疆等的联合实验室，是机器人领域的研究中心。,Prof Fumin Zhang,"Dexterous Manipulation of Thin Deformable Objects, Learn to Grasp via Intention Discovery",113.977976,22.336398,https://ri.hkust.edu.hk/,Prof Fumin Zhang,Hong Kong University of Science and Technology,,
Jun MA老师实验室,香港,China,香港科技大学,机器人学、自动驾驶、运动规划与控制、优化、强化学习,研究方向主要集中在机器人与自动驾驶的运动规划、控制与优化。,马俊 (Jun MA),"Cooperative autonomous driving in urban traffic scenarios, Alternating Direction Method of Multipliers-Based Parallel Optimization",113.977976,22.336398,https://ri.hkust.edu.hk/people/jun-ma-majun,Jun MA,Hong Kong University of Science and Technology,,"Local Reactive Control for Mobile Manipulators with Whole-Body Safety in Complex Environments|https://arxiv.org/abs/2501.02815;SAMScore: A Content Structural Similarity Metric for Image Translation Evaluation|https://arxiv.org/abs/2305.15367;MedSAM2: Segment Anything in 3D Medical Images and Videos|https://arxiv.org/abs/2504.03600;TopoNav: Topological Graphs as a Key Enabler for Advanced Object Navigation|https://arxiv.org/abs/2509.01364;Label-efficient multi-organ segmentation with a diffusion model|https://arxiv.org/abs/2402.15216;LiSTAR: Ray-Centric World Models for 4D LiDAR Sequences in Autonomous Driving|https://arxiv.org/abs/2511.16049;DuLoc: Life-Long Dual-Layer Localization in Changing and Dynamic Expansive Scenarios|https://arxiv.org/abs/2507.23660;Accelerating Vision-Language-Action Model Integrated with Action Chunking via Parallel Decoding|https://arxiv.org/abs/2503.02310;VLM-E2E: Enhancing End-to-End Autonomous Driving with Multimodal Driver Attention Fusion|https://arxiv.org/abs/2502.18042;PhyBlock: A Progressive Benchmark for Physical Understanding and Planning via 3D Block Assembly|https://arxiv.org/abs/2506.08708;CurbNet: Curb Detection Framework Based on LiDAR Point Cloud Segmentation|https://arxiv.org/abs/2403.16794;On the Surprising Robustness of Sequential Convex Optimization for Contact-Implicit Motion Planning|https://arxiv.org/abs/2502.01055;Safety-Critical Traffic Simulation with Guided Latent Diffusion Model|https://arxiv.org/abs/2505.00515;AKF-LIO: LiDAR-Inertial Odometry with Gaussian Map by Adaptive Kalman Filter|https://arxiv.org/abs/2503.06891;Multi-Class Segmentation of Aortic Branches and Zones in Computed Tomography Angiography: The AortaSeg24 Challenge|https://arxiv.org/abs/2502.05330;LOVON: Legged Open-Vocabulary Object Navigator|https://arxiv.org/abs/2507.06747;LD-Scene: LLM-Guided Diffusion for Controllable Generation of Adversarial Safety-Critical Driving Scenarios|https://arxiv.org/abs/2505.11247;ManiVID-3D: Generalizable View-Invariant Reinforcement Learning for Robotic Manipulation via Disentangled 3D Representations|https://arxiv.org/abs/2509.11125;VLM-UDMC: VLM-Enhanced Unified Decision-Making and Motion Control for Urban Autonomous Driving|https://arxiv.org/abs/2507.15266;ApexNav: An Adaptive Exploration Strategy for Zero-Shot Object Navigation with Target-centric Semantic Fusion|https://arxiv.org/abs/2504.14478;DACA-Net: A Degradation-Aware Conditional Diffusion Network for Underwater Image Enhancement|https://arxiv.org/abs/2507.22501;Annotation-Free Curb Detection Leveraging Altitude Difference Image|https://arxiv.org/abs/2409.20171;MonoGlass3D: Monocular 3D Glass Detection with Plane Regression and Adaptive Feature Fusion|https://arxiv.org/abs/2509.05599;Interactive Navigation for Legged Manipulators with Learned Arm-Pushing Controller|https://arxiv.org/abs/2503.01474;LearningFlow: Automated Policy Learning Workflow for Urban Driving with Large Language Models|https://arxiv.org/abs/2501.05057;PlanScope: Learning to Plan Within Decision Scope Does Matter|https://arxiv.org/abs/2411.00476;UDMC: Unified Decision-Making and Control Framework for Urban Autonomous Driving with Motion Prediction of Traffic Participants|https://arxiv.org/abs/2501.02530;Bilevel Multi-Armed Bandit-Based Hierarchical Reinforcement Learning for Interaction-Aware Self-Driving at Unsignalized Intersections|https://arxiv.org/abs/2502.03960;FRTree Planner: Robot Navigation in Cluttered and Unknown Environments with Tree of Free Regions|https://arxiv.org/abs/2410.20230;TSCLIP: Robust CLIP Fine-Tuning for Worldwide Cross-Regional Traffic Sign Recognition|https://arxiv.org/abs/2409.15077;Safe and Non-Conservative Contingency Planning for Autonomous Vehicles via Online Learning-Based Reachable Set Barriers|https://arxiv.org/abs/2509.07464;FERMI: Flexible Radio Mapping with a Hybrid Propagation Model and Scalable Autonomous Data Collection|https://arxiv.org/abs/2504.14862;Mamba Policy: Towards Efficient 3D Diffusion Policy with Hybrid Selective State Models|https://arxiv.org/abs/2409.07163;FLORES: A Reconfigured Wheel-Legged Robot for Enhanced Steering and Adaptability|https://arxiv.org/abs/2507.22345;DECAMP: Towards Scene-Consistent Multi-Agent Motion Prediction with Disentangled Context-Aware Pre-Training|https://arxiv.org/abs/2509.10426;GDTS: Goal-Guided Diffusion Model with Tree Sampling for Multi-Modal Pedestrian Trajectory Prediction|https://arxiv.org/abs/2311.14922;SocialTraj: Two-Stage Socially-Aware Trajectory Prediction for Autonomous Driving via Conditional Diffusion Model|https://arxiv.org/abs/2509.17850;One-shot Embroidery Customization via Contrastive LoRA Modulation|https://arxiv.org/abs/2509.18948;UniGS: Unified Geometry-Aware Gaussian Splatting for Multimodal Rendering|https://arxiv.org/abs/2510.12174;CALMM-Drive: Confidence-Aware Autonomous Driving with Large Multimodal Model|https://arxiv.org/abs/2412.04209;MGTraj: Multi-Granularity Goal-Guided Human Trajectory Prediction with Recursive Refinement Network|https://arxiv.org/abs/2509.09200;Reward-Driven Automated Curriculum Learning for Interaction-Aware Self-Driving at Unsignalized Intersections|https://arxiv.org/abs/2403.13674;SEG-Parking: Towards Safe, Efficient, and Generalizable Autonomous Parking via End-to-End Offline Reinforcement Learning|https://arxiv.org/abs/2509.13956;Occlusion-Aware Consistent Model Predictive Control for Robot Navigation in Occluded Obstacle-Dense Environments|https://arxiv.org/abs/2503.04563;Safe and Real-Time Consistent Planning for Autonomous Vehicles in Partially Observed Environments via Parallel Consensus Optimization|https://arxiv.org/abs/2409.10310;FisheyeDepth: A Real Scale Self-Supervised Depth Estimation Model for Fisheye Camera|https://arxiv.org/abs/2409.15054;SAMP: Spatial Anchor-based Motion Policy for Collision-Aware Robotic Manipulators|https://arxiv.org/abs/2509.11185;Occlusion-Aware Contingency Safety-Critical Planning for Autonomous Driving|https://arxiv.org/abs/2502.06359;Embracing Bulky Objects with Humanoid Robots: Whole-Body Manipulation with Reinforcement Learning|https://arxiv.org/abs/2509.13534;Semantic-LiDAR-Inertial-Wheel Odometry Fusion for Robust Localization in Large-Scale Dynamic Environments|https://arxiv.org/abs/2509.14999;DSDrive: Distilling Large Language Model for Lightweight End-to-End Autonomous Driving with Unified Reasoning and Planning|https://arxiv.org/abs/2505.05360;How to Bridge the Gap between Modalities: Survey on Multimodal Large Language Model|https://arxiv.org/abs/2311.07594;Erase, then Redraw: A Novel Data Augmentation Approach for Free Space Detection Using Diffusion Model|https://arxiv.org/abs/2409.20164;Fast and Scalable Game-Theoretic Trajectory Planning with Intentional Uncertainties|https://arxiv.org/abs/2507.12174;RoboDexVLM: Visual Language Model-Enabled Task Planning and Motion Control for Dexterous Robot Manipulation|https://arxiv.org/abs/2503.01616;Robot Navigation in Unknown and Cluttered Workspace with Dynamical System Modulation in Starshaped Roadmap|https://arxiv.org/abs/2403.11484;DifIISR: A Diffusion Model with Gradient Guidance for Infrared Image Super-Resolution|https://arxiv.org/abs/2503.01187;Exploring the Design Space of 3D MLLMs for CT Report Generation|https://arxiv.org/abs/2506.21535;Fair Play in the Fast Lane: Integrating Sportsmanship into Autonomous Racing Systems|https://arxiv.org/abs/2503.03774;SegSTRONG-C: Segmenting Surgical Tools Robustly On Non-adversarial Generated Corruptions -- An EndoVis'24 Challenge|https://arxiv.org/abs/2407.11906;IV-Bench: A Benchmark for Image-Grounded Video Perception and Reasoning in Multimodal LLMs|https://arxiv.org/abs/2504.15415;Efficient MedSAMs: Segment Anything in Medical Images on Laptop|https://arxiv.org/abs/2412.16085;Artificial Intelligence in Breast Cancer Care: Transforming Preoperative Planning and Patient Education with 3D Reconstruction|https://arxiv.org/abs/2509.12242;Barrier-Enhanced Parallel Homotopic Trajectory Optimization for Safety-Critical Autonomous Driving|https://arxiv.org/abs/2402.10441;SLAM-Free Visual Navigation with Hierarchical Vision-Language Perception and Coarse-to-Fine Semantic Topological Planning|https://arxiv.org/abs/2509.20739;GUIDE: A Diffusion-Based Autonomous Robot Exploration Framework Using Global Graph Inference|https://arxiv.org/abs/2509.19916;NetRoller: Interfacing General and Specialized Models for End-to-End Autonomous Driving|https://arxiv.org/abs/2506.14589;nuPlan-R: A Closed-Loop Planning Benchmark for Autonomous Driving via Reactive Multi-Agent Simulation|https://arxiv.org/abs/2511.10403;CoDriveVLM: VLM-Enhanced Urban Cooperative Dispatching and Motion Planning for Future Autonomous Mobility on Demand Systems|https://arxiv.org/abs/2501.06132;Orchestrate, Generate, Reflect: A VLM-Based Multi-Agent Collaboration Framework for Automated Driving Policy Learning|https://arxiv.org/abs/2509.17042;OmniScene: Attention-Augmented Multimodal 4D Scene Understanding for Autonomous Driving|https://arxiv.org/abs/2509.19973;An Intention-driven Lane Change Framework Considering Heterogeneous Dynamic Cooperation in Mixed-traffic Environment|https://arxiv.org/abs/2509.22550;OmniReason: A Temporal-Guided Vision-Language-Action Framework for Autonomous Driving|https://arxiv.org/abs/2509.00789;Semi-SMD: Semi-Supervised Metric Depth Estimation via Surrounding Cameras for Autonomous Driving|https://arxiv.org/abs/2503.19713;CoPlanner: An Interactive Motion Planner with Contingency-Aware Diffusion for Autonomous Driving|https://arxiv.org/abs/2509.17080;MedShapeNet -- A Large-Scale Dataset of 3D Medical Shapes for Computer Vision|https://arxiv.org/abs/2308.16139"
机器人研究所 (CKSRI),香港,China,香港科技大学,自主系统、机器人、传感器、神经科学、数据分析、机器学习,作为多学科平台，支持和促进与机器人相关的研究、开发和教育。,张福民,"软机器人, 机器人操作, 人形机器人-GRACE",113.977976,22.336398,https://ri.hkust.edu.hk/,Fumin Zhang,Hong Kong University of Science and Technology,,A Vision-Based Collision Sensing Method for Stable Circular Object Grasping with A Soft Gripper System|https://arxiv.org/abs/2508.05040;Interleaved LLM and Motion Planning for Generalized Multi-Object Collection in Large Scene Graphs|https://arxiv.org/abs/2507.15782;NuRF: Nudging the Particle Filter in Radiance Fields for Robot Visual Localization|https://arxiv.org/abs/2406.00312;VIMS: A Visual-Inertial-Magnetic-Sonar SLAM System in Underwater Environments|https://arxiv.org/abs/2506.15126;SLABIM: A SLAM-BIM Coupled Dataset in HKUST Main Building|https://arxiv.org/abs/2502.16856;CaRoBio: 3D Cable Routing with a Bio-inspired Gripper Fingernail|https://arxiv.org/abs/2508.09558;Trust-Preserved Human-Robot Shared Autonomy enabled by Bayesian Relational Event Modeling|https://arxiv.org/abs/2311.02009;Control of Marine Robots in the Era of Data-Driven Intelligence|https://arxiv.org/abs/2506.21063;Speak the Same Language: Global LiDAR Registration on BIM Using Pose Hough Transform|https://arxiv.org/abs/2405.03969;Design of a Formation Control System to Assist Human Operators in Flying a Swarm of Robotic Blimps|https://arxiv.org/abs/2505.09511
Precognition Lab,中国,China,香港科技大学（广州）,具身人工智能系统,致力于构建能够有效感知、推理并与现实世界交互的人类水平的具身人工智能系统。,Junwei Liang,"Contrastive Imitation Learning for Language-guided Multi-Task Robotic Manipulation, Prioritized Semantic Learning for Zero-shot Instance Navigation",113.394231,22.637649,https://precognition.team/,Junwei Liang,Hong Kong University of Science and Technology,,GLOVER++: Unleashing the Potential of Affordance Learning from Human Behaviors for Robotic Manipulation|https://arxiv.org/abs/2505.11865;EgoTraj-Bench: Towards Robust Trajectory Prediction Under Ego-view Noisy Observations|https://arxiv.org/abs/2510.00405;End-to-End Humanoid Robot Safe and Comfortable Locomotion Policy|https://arxiv.org/abs/2508.07611;From Cognition to Precognition: A Future-Aware Framework for Social Navigation|https://arxiv.org/abs/2409.13244;From Watch to Imagine: Steering Long-horizon Manipulation via Human Demonstration and Future Envisionment|https://arxiv.org/abs/2509.22205;3EED: Ground Everything Everywhere in 3D|https://arxiv.org/abs/2511.01755;Stairway to Success: An Online Floor-Aware Zero-Shot Object-Goal Navigation Framework via LLM-Driven Coarse-to-Fine Exploration|https://arxiv.org/abs/2505.23019;ClimateIQA: A New Dataset and Benchmark to Advance Vision-Language Models in Meteorology Anomalies Analysis|https://arxiv.org/abs/2406.09838;Zero-Shot 3D Visual Grounding from Vision-Language Models|https://arxiv.org/abs/2505.22429;GaussianProperty: Integrating Physical Properties to 3D Gaussians with LMMs|https://arxiv.org/abs/2412.11258;SD-OVON: A Semantics-aware Dataset and Benchmark Generation Pipeline for Open-Vocabulary Object Navigation in Dynamic Scenes|https://arxiv.org/abs/2505.18881;Mitigating the Human-Robot Domain Discrepancy in Visual Pre-training for Robotic Manipulation|https://arxiv.org/abs/2406.14235;GLOVER: Generalizable Open-Vocabulary Affordance Reasoning for Task-Oriented Grasping|https://arxiv.org/abs/2411.12286;SeeGround: See and Ground for Zero-Shot Open-Vocabulary 3D Visual Grounding|https://arxiv.org/abs/2412.04383;Exploring the Limits of Vision-Language-Action Manipulations in Cross-task Generalization|https://arxiv.org/abs/2505.15660;Omni-Perception: Omnidirectional Collision Avoidance for Legged Locomotion in Dynamic Environments|https://arxiv.org/abs/2505.19214
范明明老师实验室,中国,China,香港科技大学（广州）,人机交互、智能无障碍、人智协同、虚拟/增强现实,创始人为范明明教授，研究领域为人机交互，特别是智能无障碍与“适老化”交互技术设计。,范明明 (Mingming Fan),"Toward Facilitating Search in VR With the Assistance of Vision Large Language Models, Investigating Size Congruency, Designing Unobtrusive Modulated Electrotactile Feedback",113.394231,22.637649,https://www.mingmingfan.com/,Mingming Fan,Hong Kong University of Science and Technology,,SpriteHand: Real-Time Versatile Hand-Object Interaction with Autoregressive Video Generation|https://arxiv.org/abs/2512.01960
机器人与机械智能实验室 (ROMI Lab),香港,China,香港理工大学,机器人“运动智能”、数值优化、智能机器人运动、医疗手术机器人,专注于使具有腿和手臂的机器人能够像生物一样灵活地移动和与环境互动。,David Navarro-Alarcon,"机器人感知-计算传感器模型, 自主操控-基于传感器的规划, 多机器人-集体灵巧",114.179494,22.695772,https://www.romi-lab.org/,David Navarro-Alarcon,Hong Kong Polytechnic University,,Iterative Shaping of Multi-Particle Aggregates based on Action Trees and VLM|https://arxiv.org/abs/2501.13507;Prescribed Performance Control of Deformable Object Manipulation in Spatial Latent Space|https://arxiv.org/abs/2510.14234;Non-Prehensile Tool-Object Manipulation by Integrating LLM-Based Planning and Manoeuvrability-Driven Controls|https://arxiv.org/abs/2412.06931;Encircling General 2-D Boundaries by Mobile Robots with Collision Avoidance: A Vector Field Guided Approach|https://arxiv.org/abs/2501.02242;Phy-Tac: Toward Human-Like Grasping via Physics-Conditioned Tactile Goals|https://arxiv.org/abs/2511.01520;Instruction-Augmented Long-Horizon Planning: Embedding Grounding Mechanisms in Embodied Mobile Manipulation|https://arxiv.org/abs/2503.08084;HuBE: Cross-Embodiment Human-like Behavior Execution for Humanoid Robots|https://arxiv.org/abs/2508.19002
Multimedia Lab (MMLab),香港,China,香港中文大学(CUHK),计算机视觉、生成式模型、多模态AI、具身智能、AI for Science,研究方向广泛，在具身智能领域，致力于提升智能体在3D环境中的感知、推理和定位能力。,刘希慧,"DiM: Diffusion Mamba, 4Diffusion, Divide and Conquer: Language Models can Plan and Self-Correct",114.203789,22.421639,http://mmlab.ie.cuhk.edu.hk/,Xihui Liu,Chinese University of Hong Kong,,"Self-NPO: Data-Free Diffusion Model Enhancement via Truncated Diffusion Fine-Tuning|https://arxiv.org/abs/2505.11777;Loong: Generating Minute-level Long Videos with Autoregressive Language Models|https://arxiv.org/abs/2410.02757;GRPO-CARE: Consistency-Aware Reinforcement Learning for Multimodal Reasoning|https://arxiv.org/abs/2506.16141;GoT: Unleashing Reasoning Capability of Multimodal Large Language Model for Visual Generation and Editing|https://arxiv.org/abs/2503.10639;Routing Matters in MoE: Scaling Diffusion Transformers with Explicit Routing Guidance|https://arxiv.org/abs/2510.24711;The 1st International Workshop on Disentangled Representation Learning for Controllable Generation (DRL4Real): Methods and Results|https://arxiv.org/abs/2509.10463;A Survey of Interactive Generative Video|https://arxiv.org/abs/2504.21853;DreamCube: 3D Panorama Generation via Multi-plane Synchronization|https://arxiv.org/abs/2506.17206;StreamVLN: Streaming Vision-and-Language Navigation via SlowFast Context Modeling|https://arxiv.org/abs/2507.05240;DSPv2: Improved Dense Policy for Effective and Generalizable Whole-body Mobile Manipulation|https://arxiv.org/abs/2509.16063;Accelerating Auto-regressive Text-to-Image Generation with Training-free Speculative Jacobi Decoding|https://arxiv.org/abs/2410.01699;EgoPlan-Bench2: A Benchmark for Multimodal Large Language Model Planning in Real-World Scenarios|https://arxiv.org/abs/2412.04447;Speculative Jacobi-Denoising Decoding for Accelerating Autoregressive Text-to-image Generation|https://arxiv.org/abs/2510.08994;OST-Bench: Evaluating the Capabilities of MLLMs in Online Spatio-temporal Scene Understanding|https://arxiv.org/abs/2507.07984;Parallelized Autoregressive Visual Generation|https://arxiv.org/abs/2412.15119;AccVideo: Accelerating Video Diffusion Model with Synthetic Dataset|https://arxiv.org/abs/2503.19462;V2PE: Improving Multimodal Long-Context Capability of Vision-Language Models with Variable Visual Position Encoding|https://arxiv.org/abs/2412.09616;UniMC: Taming Diffusion Transformer for Unified Keypoint-Guided Multi-Class Image Generation|https://arxiv.org/abs/2507.02713;Bridging Continuous and Discrete Tokens for Autoregressive Visual Generation|https://arxiv.org/abs/2503.16430;FilMaster: Bridging Cinematic Principles and Generative AI for Automated Film Generation|https://arxiv.org/abs/2506.18899;Context as Memory: Scene-Consistent Interactive Long Video Generation with Memory Retrieval|https://arxiv.org/abs/2506.03141;FLUX-Reason-6M & PRISM-Bench: A Million-Scale Text-to-Image Reasoning Dataset and Comprehensive Benchmark|https://arxiv.org/abs/2509.09680;GameFactory: Creating New Games with Generative Interactive Videos|https://arxiv.org/abs/2501.08325;OmniX: From Unified Panoramic Generation and Perception to Graphics-Ready 3D Scenes|https://arxiv.org/abs/2510.26800;CodePlot-CoT: Mathematical Visual Reasoning by Thinking with Code-Driven Images|https://arxiv.org/abs/2510.11718;HMAR: Efficient Hierarchical Masked Auto-Regressive Image Generation|https://arxiv.org/abs/2506.04421;LiT: Delving into a Simple Linear Diffusion Transformer for Image Generation|https://arxiv.org/abs/2501.12976;Moto: Latent Motion Token as the Bridging Language for Learning Robot Manipulation from Videos|https://arxiv.org/abs/2412.04445;GoT-R1: Unleashing Reasoning Capability of MLLM for Visual Generation with Reinforcement Learning|https://arxiv.org/abs/2505.17022;MBQ: Modality-Balanced Quantization for Large Vision-Language Models|https://arxiv.org/abs/2412.19509;Wan-Move: Motion-controllable Video Generation via Latent Trajectory Guidance|https://arxiv.org/abs/2512.08765;RoboFactory: Exploring Embodied Agent Collaboration with Compositional Constraints|https://arxiv.org/abs/2503.16408;GeoSAM2: Unleashing the Power of SAM2 for 3D Part Segmentation|https://arxiv.org/abs/2508.14036;HoloPart: Generative 3D Part Amodal Segmentation|https://arxiv.org/abs/2504.07943;AnimeShooter: A Multi-Shot Animation Dataset for Reference-Guided Video Generation|https://arxiv.org/abs/2506.03126;OmniPart: Part-Aware 3D Generation with Semantic Decoupling and Structural Cohesion|https://arxiv.org/abs/2507.06165;T2I-CompBench++: An Enhanced and Comprehensive Benchmark for Compositional Text-to-image Generation|https://arxiv.org/abs/2307.06350;T2I-ReasonBench: Benchmarking Reasoning-Informed Text-to-Image Generation|https://arxiv.org/abs/2508.17472;GigaTok: Scaling Visual Tokenizers to 3 Billion Parameters for Autoregressive Image Generation|https://arxiv.org/abs/2504.08736;TTS-VAR: A Test-Time Scaling Framework for Visual Auto-Regressive Generation|https://arxiv.org/abs/2507.18537;Exploring the Effect of Reinforcement Learning on Video Understanding: Insights from SEED-Bench-R1|https://arxiv.org/abs/2503.24376;LLaVA-3D: A Simple yet Effective Pathway to Empowering LMMs with 3D-awareness|https://arxiv.org/abs/2409.18125;DreamComposer++: Empowering Diffusion Models with Multi-View Conditions for 3D Content Generation|https://arxiv.org/abs/2507.02299;SRUM: Fine-Grained Self-Rewarding for Unified Multimodal Models|https://arxiv.org/abs/2510.12784;T2V-CompBench: A Comprehensive Benchmark for Compositional Text-to-video Generation|https://arxiv.org/abs/2407.14505;Understand Before You Generate: Self-Guided Training for Autoregressive Image Generation|https://arxiv.org/abs/2509.15185;SJD++: Improved Speculative Jacobi Decoding for Training-free Acceleration of Discrete Auto-regressive Text-to-Image Generation|https://arxiv.org/abs/2512.07503;Ground Slow, Move Fast: A Dual-System Foundation Model for Generalizable Vision-and-Language Navigation|https://arxiv.org/abs/2512.08186;Personalized Text-to-Image Generation with Auto-Regressive Models|https://arxiv.org/abs/2504.13162;Position: Interactive Generative Video as Next-Generation Game Engine|https://arxiv.org/abs/2503.17359;MIDI: Multi-Instance Diffusion for Single Image to 3D Scene Generation|https://arxiv.org/abs/2412.03558"
机器人与自动化研究中心,香港,China,香港中文大学(CUHK),机器人设计制造、智能系统、医疗机器人、软体机器人等,研究方向广泛，涵盖从工业自动化到医疗机器人，从微纳机器人到服务机器人的多个领域。,窦琪（手术机器人自动化）、孙正隆（连续体机器人规划）、朱建（软体机器人）,CLF-CBF-QP在线运动重规划（IEEE T-RO 2025）、Configuration-Adaptive Visual Relative Localization (ICRA 2025),114.203789,22.421639,https://www4.mae.cuhk.edu.hk/research/robotics-and-automation/,Qi Dou; Zhenglong Sun; Jian Zhu,Chinese University of Hong Kong,,"Toward Reliable AR-Guided Surgical Navigation: Interactive Deformation Modeling with Data-Driven Biomechanics and Prompts|https://arxiv.org/abs/2506.08048;NPHardEval4V: Dynamic Evaluation of Large Vision-Language Models with Effects of Vision|https://arxiv.org/abs/2403.01777;Grounded Knowledge-Enhanced Medical Vision-Language Pre-training for Chest X-Ray|https://arxiv.org/abs/2404.14750;Context Consistency Learning via Sentence Removal for Semi-Supervised Video Paragraph Grounding|https://arxiv.org/abs/2506.18476;EndoPerfect: High-Accuracy Monocular Depth Estimation and 3D Reconstruction for Endoscopic Surgery via NeRF-Stereo Fusion|https://arxiv.org/abs/2410.04041;Kernel-based Unsupervised Embedding Alignment for Enhanced Visual Representation in Vision-language Models|https://arxiv.org/abs/2506.02557;SAP-Bench: Benchmarking Multimodal Large Language Models in Surgical Action Planning|https://arxiv.org/abs/2506.07196;On Robustness of Vision-Language-Action Model against Multi-Modal Perturbations|https://arxiv.org/abs/2510.00037;THCRL: Trusted Hierarchical Contrastive Representation Learning for Multi-View Clustering|https://arxiv.org/abs/2512.00368;Boomda: Balanced Multi-objective Optimization for Multimodal Domain Adaptation|https://arxiv.org/abs/2511.08152;GenTe: Generative Real-world Terrains for General Legged Robot Locomotion Control|https://arxiv.org/abs/2504.09997;Can Generalist Vision Language Models (VLMs) Rival Specialist Medical VLMs? Benchmarking and Strategic Insights|https://arxiv.org/abs/2506.17337;SurgVLM: A Large Vision-Language Model and Systematic Evaluation Benchmark for Surgical Intelligence|https://arxiv.org/abs/2506.02555;FLUX-Makeup: High-Fidelity, Identity-Consistent, and Robust Makeup Transfer via Diffusion Transformer|https://arxiv.org/abs/2508.05069;Other Vehicle Trajectories Are Also Needed: A Driving World Model Unifies Ego-Other Vehicle Trajectories in Video Latent Space|https://arxiv.org/abs/2503.09215;Endo3R: Unified Online Reconstruction from Dynamic Monocular Endoscopic Video|https://arxiv.org/abs/2504.03198;DynaMIC: Dynamic Multimodal In-Context Learning Enabled Embodied Robot Counterfactual Resistance Ability|https://arxiv.org/abs/2509.24413;UCAgents: Unidirectional Convergence for Visual Evidence Anchored Multi-Agent Medical Decision-Making|https://arxiv.org/abs/2512.02485;Systematic Evaluation and Guidelines for Segment Anything Model in Surgical Video Analysis|https://arxiv.org/abs/2501.00525;CustomVideo: Customizing Text-to-Video Generation with Multiple Subjects|https://arxiv.org/abs/2401.09962;NanoControl: A Lightweight Framework for Precise and Efficient Control in Diffusion Transformer|https://arxiv.org/abs/2508.10424;Generative Diffusion Contrastive Network for Multi-View Clustering|https://arxiv.org/abs/2509.09527;ClipGS: Clippable Gaussian Splatting for Interactive Cinematic Visualization of Volumetric Medical Data|https://arxiv.org/abs/2507.06647;CTA-Flux: Integrating Chinese Cultural Semantics into High-Quality English Text-to-Image Communities|https://arxiv.org/abs/2508.14405;Self-Learning Hyperspectral and Multispectral Image Fusion via Adaptive Residual Guided Subspace Diffusion Model|https://arxiv.org/abs/2505.11800;Extreme Cardiac MRI Analysis under Respiratory Motion: Results of the CMRxMotion Challenge|https://arxiv.org/abs/2507.19165;Trusted Mamba Contrastive Network for Multi-View Clustering|https://arxiv.org/abs/2412.16487;A Super-pixel-based Approach to the Stable Interpretation of Neural Networks|https://arxiv.org/abs/2412.14509;Diffusion Stabilizer Policy for Automated Surgical Robot Manipulations|https://arxiv.org/abs/2503.01252;ToonComposer: Streamlining Cartoon Production with Generative Post-Keyframing|https://arxiv.org/abs/2508.10881;A Learning-based Control Methodology for Transitioning VTOL UAVs|https://arxiv.org/abs/2512.03548;Learning dissection trajectories from expert surgical videos via imitation learning with equivariant diffusion|https://arxiv.org/abs/2506.04716;AnyCharV: Bootstrap Controllable Character Video Generation with Fine-to-Coarse Guidance|https://arxiv.org/abs/2502.08189;Towards High-Consistency Embodied World Model with Multi-View Trajectory Videos|https://arxiv.org/abs/2511.12882;SegSTRONG-C: Segmenting Surgical Tools Robustly On Non-adversarial Generated Corruptions -- An EndoVis'24 Challenge|https://arxiv.org/abs/2407.11906;Align-Then-stEer: Adapting the Vision-Language Action Models through Unified Latent Guidance|https://arxiv.org/abs/2509.02055;WonderVerse: Extendable 3D Scene Generation with Video Generative Models|https://arxiv.org/abs/2503.09160;Medical Large Vision Language Models with Multi-Image Visual Ability|https://arxiv.org/abs/2505.19031;Toward Robust Medical Fairness: Debiased Dual-Modal Alignment via Text-Guided Attribute-Disentangled Prompt Learning for Vision-Language Models|https://arxiv.org/abs/2508.18886;Contact Map Transfer with Conditional Diffusion Model for Generalizable Dexterous Grasp Generation|https://arxiv.org/abs/2511.01276;MoEGCL: Mixture of Ego-Graphs Contrastive Representation Learning for Multi-View Clustering|https://arxiv.org/abs/2511.05876;MagicRoad: Semantic-Aware 3D Road Surface Reconstruction via Obstacle Inpainting|https://arxiv.org/abs/2507.23340"
机器人与人工智能实验室,香港,China,香港中文大学(CUHK),航天/工业/服务/特种/医疗/智能汽车机器人,由徐扬生院士带领，成功研制了30多个机器人和智能系统，研究成果世界领先。,徐扬生,"AB-Mapper, Learning to Coordinate for a Worker-Station Heterogeneous Multi-robot System, Fast and Comfortable Interactive Robot-to-Human Object Handover",114.203789,22.421639,https://rail.cuhk.edu.cn/zh-hans/about,Yangsheng Xu,Chinese University of Hong Kong,,
深圳市人工智能与机器人研究院 (AIRS),中国,China,香港中文大学（深圳）,群体智能、特种机器人、医疗机器人、具身智能、软体机器人等,深圳市政府依托港中大（深圳）建立的基础研究机构，研究多种应用场景的机器人。,"徐扬生, 丁宁, 黄建伟, 韩龙","Snail-inspired robotic swarms, PepperPose, A magnetic multi-layer soft robot for on-demand targeted adhesion",114.179494,22.695772,https://airs.cuhk.edu.cn/,Yangsheng Xu; Ning Ding; Jianwei Huang; Long Han,Chinese University of Hong Kong,,"Disentangling Static and Dynamic Information for Reducing Static Bias in Action Recognition|https://arxiv.org/abs/2509.23009;Shot2Tactic-Caption: Multi-Scale Captioning of Badminton Videos for Tactical Understanding|https://arxiv.org/abs/2510.14617;SimpleVLA-RL: Scaling VLA Training via Reinforcement Learning|https://arxiv.org/abs/2509.09674;MS-Occ: Multi-Stage LiDAR-Camera Fusion for 3D Semantic Occupancy Prediction|https://arxiv.org/abs/2504.15888;The 1st International Workshop on Disentangled Representation Learning for Controllable Generation (DRL4Real): Methods and Results|https://arxiv.org/abs/2509.10463;Intuitive Surgical SurgToolLoc Challenge Results: 2022-2023|https://arxiv.org/abs/2305.07152;MoExDA: Domain Adaptation for Edge-based Action Recognition|https://arxiv.org/abs/2508.02981;Instruct-IPT: All-in-One Image Processing Transformer via Weight Modulation|https://arxiv.org/abs/2407.00676;FunHOI: Annotation-Free 3D Hand-Object Interaction Generation via Functional Text Guidanc|https://arxiv.org/abs/2502.20805;Vehicle-to-Everything Cooperative Perception for Autonomous Driving|https://arxiv.org/abs/2310.03525;Optimal Smooth Coverage Trajectory Planning for Quadrotors in Cluttered Environment|https://arxiv.org/abs/2510.03169;MiniCPM-V 4.5: Cooking Efficient MLLMs via Architecture, Data, and Training Recipe|https://arxiv.org/abs/2509.18154;AerialMind: Towards Referring Multi-Object Tracking in UAV Scenarios|https://arxiv.org/abs/2511.21053;LEGO: Learning and Graph-Optimized Modular Tracker for Online Multi-Object Tracking with Point Clouds|https://arxiv.org/abs/2308.09908;SymBridge: A Human-in-the-Loop Cyber-Physical Interactive System for Adaptive Human-Robot Symbiosis|https://arxiv.org/abs/2502.07358;MedXpertQA: Benchmarking Expert-Level Medical Reasoning and Understanding|https://arxiv.org/abs/2501.18362;Underwater Motions Analysis and Control of a Coupling-Tiltable Unmanned Aerial-Aquatic Vehicle|https://arxiv.org/abs/2312.07290;GPT4Image: Large Pre-trained Models Help Vision Models Learn Better on Perception Task|https://arxiv.org/abs/2306.00693;Post-Training Quantization for Diffusion Transformer via Hierarchical Timestep Grouping|https://arxiv.org/abs/2503.06930;Self-Reflective Reinforcement Learning for Diffusion-based Image Reasoning Generation|https://arxiv.org/abs/2505.22407;Separating Shared and Domain-Specific LoRAs for Multi-Domain Learning|https://arxiv.org/abs/2508.02978;TrackID3x3: A Dataset and Algorithm for Multi-Player Tracking with Identification and Pose Estimation in 3x3 Basketball Full-court Videos|https://arxiv.org/abs/2503.18282;Intern-S1: A Scientific Multimodal Foundation Model|https://arxiv.org/abs/2508.15763"
机器人与人工智能实验室 (RAIL),中国,China,香港中文大学（深圳）,机器人与智能制造,由徐扬生院士带领，是深圳和国家发改委重点建设单位。,徐扬生,"书法机器人, 爬树机器人, 单轮机器人",114.179494,22.695772,https://rail.cuhk.edu.cn/zh-hans/about,Yangsheng Xu,Chinese University of Hong Kong,,DCPI-Depth: Explicitly Infusing Dense Correspondence Prior to Unsupervised Monocular Depth Estimation|https://arxiv.org/abs/2405.16960;NavComposer: Composing Language Instructions for Navigation Trajectories through Action-Scene-Object Modularization|https://arxiv.org/abs/2507.10894;Rethinking the Embodied Gap in Vision-and-Language Navigation: A Holistic Study of Physical and Visual Disparities|https://arxiv.org/abs/2507.13019;Realizing Text-Driven Motion Generation on NAO Robot: A Reinforcement Learning-Optimized Control Pipeline|https://arxiv.org/abs/2506.05117;Fully Exploiting Vision Foundation Model's Profound Prior Knowledge for Generalizable RGB-Depth Driving Scene Parsing|https://arxiv.org/abs/2502.06219;SparseWorld-TC: Trajectory-Conditioned Sparse Occupancy World Model|https://arxiv.org/abs/2511.22039;Bootstrapping Vision-language Models for Self-supervised Remote Physiological Measurement|https://arxiv.org/abs/2407.08507;CleanPose: Category-Level Object Pose Estimation via Causal Learning and Knowledge Distillation|https://arxiv.org/abs/2502.01312;CLASH: Collaborative Large-Small Hierarchical Framework for Continuous Vision-and-Language Navigation|https://arxiv.org/abs/2512.10360;LIX: Implicitly Infusing Spatial Geometric Prior Knowledge into Visual Semantic Segmentation for Autonomous Driving|https://arxiv.org/abs/2403.08215;A Birotation Solution for Relative Pose Problems|https://arxiv.org/abs/2505.02025;SAM-LAD: Segment Anything Model Meets Zero-Shot Logic Anomaly Detection|https://arxiv.org/abs/2406.00625;MLANet: Multi-Level Attention Network with Sub-instruction for Continuous Vision-and-Language Navigation|https://arxiv.org/abs/2303.01396;Kinematics-Aware Multi-Policy Reinforcement Learning for Force-Capable Humanoid Loco-Manipulation|https://arxiv.org/abs/2511.21169
NUS AI LAB,新加坡,China,新加坡国立大学 (NUS),具身AI、交互式AI、可信AI,隶属于新加坡国立大学，研究方向几乎涵盖AI的所有领域。,"Arnab BHATTACHARYYA, Xavier BRESSON","Coarse-to-fine Animal Pose and Shape Estimation, Self-supervised 3D hand pose estimation, Towards Effective Tactile Identification of Textures",103.780227,1.2966,https://nusail.comp.nus.edu.sg,Arnab BHATTACHARYYA; Xavier BRESSON,National University of Singapore,,Functional correspondence by matrix completion|https://arxiv.org/abs/1412.8070
Advanced Robotics Centre,新加坡,China,新加坡国立大学 (NUS),智能抓取技术、软机器人、机器人发展及应用,研究智能抓取技术，并探讨机器人从工业制造到服务机器人的发展历程及应用场景。,Professor Cecilia Laschi,Model-based reinforcement learning for closed-loop dynamic control of soft robotic manipulators,103.780227,1.2966,https://arc.nus.edu.sg/,Cecilia Laschi,National University of Singapore,,Origami-Inspired Soft Gripper with Tunable Constant Force Output|https://arxiv.org/abs/2503.01481;A Quantitative Comparison of Centralised and Distributed Reinforcement Learning-Based Control for Soft Robotic Arms|https://arxiv.org/abs/2511.02192;Source-Free Bistable Fluidic Gripper for Size-Selective and Stiffness-Adaptive Grasping|https://arxiv.org/abs/2511.03691;The Role of Touch: Towards Optimal Tactile Sensing Distribution in Anthropomorphic Hands for Dexterous In-Hand Manipulation|https://arxiv.org/abs/2509.14984;Adaptive and Multi-object Grasping via Deformable Origami Modules|https://arxiv.org/abs/2511.00516;Octopus-Swimming-Like Robot with Soft Asymmetric Arms|https://arxiv.org/abs/2410.11764;Grasping by Spiraling: Reproducing Elephant Movements with Rigid-Soft Robot Synergy|https://arxiv.org/abs/2504.01507;Octopus-like Reaching Motion: A Perspective Inspired by Whipping|https://arxiv.org/abs/2510.25520;Soft yet Effective Robots via Holistic Co-Design|https://arxiv.org/abs/2505.03761
Synteraction Lab,新加坡,China,新加坡国立大学 (NUS),人机交互、抬头计算、可穿戴平台、多模式交互,亚洲及世界最活跃的人机交互研究中心之一，愿景是通过可穿戴平台和多模式交互改变人与技术的交互方式。,Shengdong Zhao,"What's this? Understanding User Interaction Behaviour, Navigating Real-World Challenges, Heads-Up Multitasker",103.780227,1.2966,https://synteraction.org/,Shengdong Zhao,National University of Singapore,,
Microsystem Engineering and Robotics,新加坡,China,新加坡国立大学 (NUS),微系统工程、机器人技术,从事微系统和机器人技术的研究与开发，重点是对从微观到宏观尺度的物理和生物系统进行机械操作。,Peter C. Y. Chen,Three-dimensional characterization of mechanical interactions between endothelial cells and extracellular matrix,103.780227,1.2966,https://cde.nus.edu.sg/me/graduate/msc-robotics/,Peter C. Y. Chen,National University of Singapore,,
人机物智能融合实验室 (HCP),中国,China,中山大学,"多模态认知计算, 机器人与嵌入式系统, 元宇宙与数字人, 可控内容生成",围绕“人工智能前沿技术与产业化”布局研究方向，输出原创技术及孵化创业团队。,林倞、李冠彬、梁小丹、王可泽、王广润、杨思蓓等,"CorNav, NeRF-HuGS, OVER-NAV",113.307449,23.097549,https://www.sysu-hcp.net/home/,Jing Lin; Guanbin Li; Xiaodan Liang; Keze Wang; Guangrun Wang; Sibei Yang,Sun Yat-sen University,,"Towards Long-Horizon Vision-Language Navigation: Platform, Benchmark and Method|https://arxiv.org/abs/2412.09082;MVTokenFlow: High-quality 4D Content Generation using Multiview Token Flow|https://arxiv.org/abs/2502.11697;VTON 360: High-Fidelity Virtual Try-On from Any Viewing Direction|https://arxiv.org/abs/2503.12165;BridgeIV: Bridging Customized Image and Video Generation through Test-Time Autoregressive Identity Propagation|https://arxiv.org/abs/2505.06985;Eyes Wide Open: Ego Proactive Video-LLM for Streaming Video|https://arxiv.org/abs/2510.14560;SQLNet: Scale-Modulated Query and Localization Network for Few-Shot Class-Agnostic Counting|https://arxiv.org/abs/2311.10011;WISA: World Simulator Assistant for Physics-Aware Text-to-Video Generation|https://arxiv.org/abs/2503.08153;Can Atomic Step Decomposition Enhance the Self-structured Reasoning of Multimodal Large Models?|https://arxiv.org/abs/2503.06252;Studying Various Activation Functions and Non-IID Data for Machine Learning Model Robustness|https://arxiv.org/abs/2512.04264;Single Image Reflection Removal via inter-layer Complementarity|https://arxiv.org/abs/2505.12641;A Stepwise Distillation Learning Strategy for Non-differentiable Visual Programming Frameworks on Visual Reasoning Tasks|https://arxiv.org/abs/2309.09809;Structured Preference Optimization for Vision-Language Long-Horizon Task Planning|https://arxiv.org/abs/2502.20742;S2-Track: A Simple yet Strong Approach for End-to-End 3D Multi-Object Tracking|https://arxiv.org/abs/2406.02147;The Quest for Generalizable Motion Generation: Data, Model, and Evaluation|https://arxiv.org/abs/2510.26794;Video Spatial Reasoning with Object-Centric 3D Rollout|https://arxiv.org/abs/2511.13190;AtomThink: Multimodal Slow Thinking with Atomic Step Reasoning|https://arxiv.org/abs/2411.11930;LaneDiffusion: Improving Centerline Graph Learning via Prior Injected BEV Feature Generation|https://arxiv.org/abs/2511.06272;Towards Top-Down Reasoning: An Explainable Multi-Agent Approach for Visual Question Answering|https://arxiv.org/abs/2311.17331;MUSE: Mamba is Efficient Multi-scale Learner for Text-video Retrieval|https://arxiv.org/abs/2408.10575;Cross-Modal Causal Intervention for Medical Report Generation|https://arxiv.org/abs/2303.09117;RoboPearls: Editable Video Simulation for Robot Manipulation|https://arxiv.org/abs/2506.22756;EvolveNav: Empowering LLM-Based Vision-Language Navigation via Self-Improving Embodied Reasoning|https://arxiv.org/abs/2506.01551;Augmenting Moment Retrieval: Zero-Dependency Two-Stage Learning|https://arxiv.org/abs/2510.19622;AdaDrive: Self-Adaptive Slow-Fast System for Language-Grounded Autonomous Driving|https://arxiv.org/abs/2511.06253;Aligning Cyber Space with Physical World: A Comprehensive Survey on Embodied AI|https://arxiv.org/abs/2407.06886;BinaryHPE: 3D Human Pose and Shape Estimation via Binarization|https://arxiv.org/abs/2311.14323;Aerial Vision-and-Language Navigation with Grid-based View Selection and Map Construction|https://arxiv.org/abs/2503.11091;3DAffordSplat: Efficient Affordance Reasoning with 3D Gaussians|https://arxiv.org/abs/2504.11218;ContextDrag: Precise Drag-Based Image Editing via Context-Preserving Token Injection and Position-Consistent Attention|https://arxiv.org/abs/2512.08477;SkillMimic: Learning Basketball Interaction Skills from Demonstrations|https://arxiv.org/abs/2408.15270;Unseen from Seen: Rewriting Observation-Instruction Using Foundation Models for Augmenting Vision-Language Navigation|https://arxiv.org/abs/2503.18065;Mod-Adapter: Tuning-Free and Versatile Multi-concept Personalization via Modulation Adapter|https://arxiv.org/abs/2505.18612;SpatialDreamer: Incentivizing Spatial Reasoning via Active Mental Imagery|https://arxiv.org/abs/2512.07733;RoBridge: A Hierarchical Architecture Bridging Cognition and Execution for General Robotic Manipulation|https://arxiv.org/abs/2505.01709;$\mathcal{E}_0$: Enhancing Generalization and Fine-Grained Control in VLA Models via Continuized Discrete Diffusion|https://arxiv.org/abs/2511.21542;Seed3D 1.0: From Images to High-Fidelity Simulation-Ready 3D Assets|https://arxiv.org/abs/2510.19944;SeqAfford: Sequential 3D Affordance Reasoning via Multimodal Large Language Model|https://arxiv.org/abs/2412.01550;MAT-Agent: Adaptive Multi-Agent Training Optimization|https://arxiv.org/abs/2510.17845;VLDrive: Vision-Augmented Lightweight MLLMs for Efficient Language-grounded Autonomous Driving|https://arxiv.org/abs/2511.06256;High-Quality Mask Tuning Matters for Open-Vocabulary Segmentation|https://arxiv.org/abs/2412.11464;FastFit: Accelerating Multi-Reference Virtual Try-On via Cacheable Diffusion Models|https://arxiv.org/abs/2508.20586;DreamLayer: Simultaneous Multi-Layer Generation via Diffusion Mode|https://arxiv.org/abs/2503.12838;ChatHuman: Chatting about 3D Humans with Tools|https://arxiv.org/abs/2405.04533;AutoStudio: Crafting Consistent Subjects in Multi-turn Interactive Image Generation|https://arxiv.org/abs/2406.01388;C2-Evo: Co-Evolving Multimodal Data and Model for Self-Improving Reasoning|https://arxiv.org/abs/2507.16518;RoomTour3D: Geometry-Aware Video-Instruction Tuning for Embodied Navigation|https://arxiv.org/abs/2412.08591;Does Your 3D Encoder Really Work? When Pretrain-SFT from 2D VLMs Meets 3D VLMs|https://arxiv.org/abs/2506.05318;DPoser-X: Diffusion Model as Robust 3D Whole-body Human Pose Prior|https://arxiv.org/abs/2508.00599;In-Situ Tweedie Discrete Diffusion Models|https://arxiv.org/abs/2510.01047;Why LVLMs Are More Prone to Hallucinations in Longer Responses: The Role of Context|https://arxiv.org/abs/2510.20229;Top-Down Semantic Refinement for Image Captioning|https://arxiv.org/abs/2510.22391;Beyond the Destination: A Novel Benchmark for Exploration-Aware Embodied Question Answering|https://arxiv.org/abs/2503.11117;X-SAM: From Segment Anything to Any Segmentation|https://arxiv.org/abs/2508.04655;HumanMM: Global Human Motion Recovery from Multi-shot Videos|https://arxiv.org/abs/2503.07597;DreamFuse: Adaptive Image Fusion with Diffusion Transformer|https://arxiv.org/abs/2504.08291;HumanGenesis: Agent-Based Geometric and Generative Modeling for Synthetic Human Dynamics|https://arxiv.org/abs/2508.09858;TalkVid: A Large-Scale Diversified Dataset for Audio-Driven Talking Head Synthesis|https://arxiv.org/abs/2508.13618;ReferSplat: Referring Segmentation in 3D Gaussian Splatting|https://arxiv.org/abs/2508.08252;HybridToken-VLM: Hybrid Token Compression for Vision-Language Models|https://arxiv.org/abs/2512.08240;VLA Models Are More Generalizable Than You Think: Revisiting Physical and Spatial Modeling|https://arxiv.org/abs/2512.02902;Closed-Loop Transfer for Weakly-supervised Affordance Grounding|https://arxiv.org/abs/2510.17384;VideoVerse: How Far is Your T2V Generator from a World Model?|https://arxiv.org/abs/2510.08398;Seeing through Imagination: Learning Scene Geometry via Implicit Spatial World Modeling|https://arxiv.org/abs/2512.01821;SeqAffordSplat: Scene-level Sequential Affordance Reasoning on 3D Gaussian Splatting|https://arxiv.org/abs/2507.23772;SIRI-Bench: Challenging VLMs' Spatial Intelligence through Complex Reasoning Tasks|https://arxiv.org/abs/2506.14512;DeepShield: Fortifying Deepfake Video Detection with Local and Global Forgery Analysis|https://arxiv.org/abs/2510.25237;ODMixer: Fine-grained Spatial-temporal MLP for Metro Origin-Destination Prediction|https://arxiv.org/abs/2404.15734;AutoLayout: Closed-Loop Layout Synthesis via Slow-Fast Collaborative Reasoning|https://arxiv.org/abs/2507.04293;One Model for All: Unified Try-On and Try-Off in Any Pose via LLM-Inspired Bidirectional Tweedie Diffusion|https://arxiv.org/abs/2508.04559;DirectSwap: Mask-Free Cross-Identity Training and Benchmarking for Expression-Consistent Video Head Swapping|https://arxiv.org/abs/2512.09417;TransXNet: Learning Both Global and Local Dynamics with a Dual Dynamic Token Mixer for Visual Recognition|https://arxiv.org/abs/2310.19380;3D-MoRe: Unified Modal-Contextual Reasoning for Embodied Question Answering|https://arxiv.org/abs/2507.12026;RoboTron-Drive: All-in-One Large Multimodal Model for Autonomous Driving|https://arxiv.org/abs/2412.07689;DreamFit: Garment-Centric Human Generation via a Lightweight Anything-Dressing Encoder|https://arxiv.org/abs/2412.17644;ActionSink: Toward Precise Robot Manipulation with Dynamic Integration of Action Flow|https://arxiv.org/abs/2508.03218;ComposeAnyone: Controllable Layout-to-Human Generation with Decoupled Multimodal Conditions|https://arxiv.org/abs/2501.12173;Video SimpleQA: Towards Factuality Evaluation in Large Video Language Models|https://arxiv.org/abs/2503.18923;Sitcom-Crafter: A Plot-Driven Human Motion Generation System in 3D Scenes|https://arxiv.org/abs/2410.10790;Embodied Arena: A Comprehensive, Unified, and Evolving Evaluation Platform for Embodied AI|https://arxiv.org/abs/2509.15273;From Motion to Behavior: Hierarchical Modeling of Humanoid Generative Behavior Control|https://arxiv.org/abs/2506.00043;Vision Function Layer in Multimodal LLMs|https://arxiv.org/abs/2509.24791;CatVTON: Concatenation Is All You Need for Virtual Try-On with Diffusion Models|https://arxiv.org/abs/2407.15886;3D-CovDiffusion: 3D-Aware Diffusion Policy for Coverage Path Planning|https://arxiv.org/abs/2510.03011;EMOVA: Empowering Language Models to See, Hear and Speak with Vivid Emotions|https://arxiv.org/abs/2409.18042;Complementary Information Guided Occupancy Prediction via Multi-Level Representation Fusion|https://arxiv.org/abs/2510.13198;GS: Generative Segmentation via Label Diffusion|https://arxiv.org/abs/2508.20020;CatV2TON: Taming Diffusion Transformers for Vision-Based Virtual Try-On with Temporal Concatenation|https://arxiv.org/abs/2501.11325;SAM2-UNeXT: An Improved High-Resolution Baseline for Adapting Foundation Models to Downstream Segmentation Tasks|https://arxiv.org/abs/2508.03566;DAGSM: Disentangled Avatar Generation with GS-enhanced Mesh|https://arxiv.org/abs/2411.15205;UniGS: Unified Language-Image-3D Pretraining with Gaussian Splatting|https://arxiv.org/abs/2502.17860;Rethinking Query-based Transformer for Continual Image Segmentation|https://arxiv.org/abs/2507.07831;GeoSplatting: Towards Geometry Guided Gaussian Splatting for Physically-based Inverse Rendering|https://arxiv.org/abs/2410.24204;RainFusion: Adaptive Video Generation Acceleration via Multi-Dimensional Visual Redundancy|https://arxiv.org/abs/2505.21036;Ground-R1: Incentivizing Grounded Visual Reasoning via Reinforcement Learning|https://arxiv.org/abs/2505.20272;TheaterGen: Character Management with LLM for Consistent Multi-turn Image Generation|https://arxiv.org/abs/2404.18919;Towards Fine-Grained Human Motion Video Captioning|https://arxiv.org/abs/2510.24767;Dual-domain Adaptation Networks for Realistic Image Super-resolution|https://arxiv.org/abs/2511.17217;SDQM: Synthetic Data Quality Metric for Object Detection Dataset Evaluation|https://arxiv.org/abs/2510.06596;3DAlign-DAER: Dynamic Attention Policy and Efficient Retrieval Strategy for Fine-grained 3D-Text Alignment at Scale|https://arxiv.org/abs/2511.13211;MAC: A Benchmark for Multiple Attributes Compositional Zero-Shot Learning|https://arxiv.org/abs/2406.12757;TIMERIPPLE: Accelerating vDiTs by Understanding the Spatio-Temporal Correlations in Latent Space|https://arxiv.org/abs/2511.12035;MM-CoT:A Benchmark for Probing Visual Chain-of-Thought Reasoning in Multimodal Models|https://arxiv.org/abs/2512.08228;TimeCausality: Evaluating the Causal Ability in Time Dimension for Vision Language Models|https://arxiv.org/abs/2505.15435;Empowering Large Language Models with 3D Situation Awareness|https://arxiv.org/abs/2503.23024;Bridging Knowledge Gap Between Image Inpainting and Large-Area Visible Watermark Removal|https://arxiv.org/abs/2504.04687;A Survey on Agentic Multimodal Large Language Models|https://arxiv.org/abs/2510.10991;Dissecting and Mitigating Diffusion Bias via Mechanistic Interpretability|https://arxiv.org/abs/2503.20483;Mitigating Hallucination for Large Vision Language Model by Inter-Modality Correlation Calibration Decoding|https://arxiv.org/abs/2501.01926;A0: An Affordance-Aware Hierarchical Model for General Robotic Manipulation|https://arxiv.org/abs/2504.12636;Motion-X++: A Large-Scale Multimodal 3D Whole-body Human Motion Dataset|https://arxiv.org/abs/2501.05098;GRPO-Guard: Mitigating Implicit Over-Optimization in Flow Matching via Regulated Clipping|https://arxiv.org/abs/2510.22319;Intervene-All-Paths: Unified Mitigation of LVLM Hallucinations across Alignment Formats|https://arxiv.org/abs/2511.17254;Getting More Juice Out of Your Data: Hard Pair Refinement Enhances Visual-Language Models Without Extra Data|https://arxiv.org/abs/2305.05208;PhyBlock: A Progressive Benchmark for Physical Understanding and Planning via 3D Block Assembly|https://arxiv.org/abs/2506.08708;ConsistentID: Portrait Generation with Multimodal Fine-Grained Identity Preserving|https://arxiv.org/abs/2404.16771;NavCoT: Boosting LLM-Based Vision-and-Language Navigation via Learning Disentangled Reasoning|https://arxiv.org/abs/2403.07376;Free-MoRef: Instantly Multiplexing Context Perception Capabilities of Video-MLLMs within Single Inference|https://arxiv.org/abs/2508.02134;DialogGen: Multi-modal Interactive Dialogue System for Multi-turn Text-to-Image Generation|https://arxiv.org/abs/2403.08857;EACO: Enhancing Alignment in Multimodal LLMs via Critical Observation|https://arxiv.org/abs/2412.04903;EchoVLA: Robotic Vision-Language-Action Model with Synergistic Declarative Memory for Mobile Manipulation|https://arxiv.org/abs/2511.18112;GLaD: Geometric Latent Distillation for Vision-Language-Action Models|https://arxiv.org/abs/2512.09619;LaVieID: Local Autoregressive Diffusion Transformers for Identity-Preserving Video Creation|https://arxiv.org/abs/2508.07603;Adaptive Part Learning for Fine-Grained Generalized Category Discovery: A Plug-and-Play Enhancement|https://arxiv.org/abs/2507.06928;Dynamic Try-On: Taming Video Virtual Try-on with Dynamic Attention Mechanism|https://arxiv.org/abs/2412.09822;Sim-DETR: Unlock DETR for Temporal Sentence Grounding|https://arxiv.org/abs/2509.23867;Human-Centric Open-Future Task Discovery: Formulation, Benchmark, and Scalable Tree-Based Search|https://arxiv.org/abs/2511.18929;Style-Preserving Lip Sync via Audio-Aware Style Reference|https://arxiv.org/abs/2408.05412;Discovering Influential Neuron Path in Vision Transformers|https://arxiv.org/abs/2503.09046;EgoSplat: Open-Vocabulary Egocentric Scene Understanding with Language Embedded 3D Gaussian Splatting|https://arxiv.org/abs/2503.11345;ProPhy: Progressive Physical Alignment for Dynamic World Simulation|https://arxiv.org/abs/2512.05564;SemHiTok: A Unified Image Tokenizer via Semantic-Guided Hierarchical Codebook for Multimodal Understanding and Generation|https://arxiv.org/abs/2503.06764;Semi- and Weakly-Supervised Learning for Mammogram Mass Segmentation with Limited Annotations|https://arxiv.org/abs/2403.09315;PDC-Net: Pattern Divide-and-Conquer Network for Pelvic Radiation Injury Segmentation|https://arxiv.org/abs/2506.17712;Penalizing Boundary Activation for Object Completeness in Diffusion Models|https://arxiv.org/abs/2509.16968;DART: Dual Adaptive Refinement Transfer for Open-Vocabulary Multi-Label Recognition|https://arxiv.org/abs/2508.05585;Exploiting Temporal Audio-Visual Correlation Embedding for Audio-Driven One-Shot Talking Head Animation|https://arxiv.org/abs/2504.05746;UML-CoT: Structured Reasoning and Planning with Unified Modeling Language for Robotic Room Cleaning|https://arxiv.org/abs/2509.22628;TransMamba: Fast Universal Architecture Adaption from Transformers to Mamba|https://arxiv.org/abs/2502.15130;Free on the Fly: Enhancing Flexibility in Test-Time Adaptation with Online EM|https://arxiv.org/abs/2507.06973;No More Sibling Rivalry: Debiasing Human-Object Interaction Detection|https://arxiv.org/abs/2509.00760;PhyDetEx: Detecting and Explaining the Physical Plausibility of T2V Models|https://arxiv.org/abs/2512.01843;Learning to See and Act: Task-Aware Virtual View Exploration for Robotic Manipulation|https://arxiv.org/abs/2508.05186;Thinking with Drafts: Speculative Temporal Reasoning for Efficient Long Video Understanding|https://arxiv.org/abs/2512.00805;Astraea: A Token-wise Acceleration Framework for Video Diffusion Transformers|https://arxiv.org/abs/2506.05096;FireEdit: Fine-grained Instruction-based Image Editing via Region-aware Vision Language Model|https://arxiv.org/abs/2503.19839;Physical Autoregressive Model for Robotic Manipulation without Action Pretraining|https://arxiv.org/abs/2508.09822;DSPNet: Dual-vision Scene Perception for Robust 3D Question Answering|https://arxiv.org/abs/2503.03190"
具身智能实验室,中国,China,西北工业大学,"具身智能，机器人操作，计算机视觉, 遥感图像处理",西北工业大学光电智能研究院具身智能团队关注人工智能软硬件一体化研究，聚焦非结构化场景下的具身智能机器人技术创新和应用落地。经过长期研究实践，团队积累了大模型算法创新、软硬件开发、高价值场景应用等多方面技术经验，大模型异构智能体、自主无人机等相关成果应用于应急救援、安防巡检等场景,赵斌,人工智能软硬件一体化研究，具身智能落地，大模型异构智能体、自主无人机,108.9126,34.2427,https://vone-zhao.github.io/,Bin Zhao,Northwestern Polytechnical University,,"Diffusion Model is an Effective Planner and Data Synthesizer for Multi-Task Reinforcement Learning|https://arxiv.org/abs/2305.18459;Reconstructive Sequence-Graph Network for Video Summarization|https://arxiv.org/abs/2105.04066;Weather GAN: Multi-Domain Weather Translation Using Generative Adversarial Networks|https://arxiv.org/abs/2103.05422;Semantics-Consistent Representation Learning for Remote Sensing Image-Voice Retrieval|https://arxiv.org/abs/2103.05302;Low-Light Hyperspectral Image Enhancement|https://arxiv.org/abs/2104.10729;Weakly Supervised Object Localization with Multi-fold Multiple Instance Learning|https://arxiv.org/abs/1911.05383;Rethinking IoU-based Localization for Weakly Supervised Object Localization|https://arxiv.org/abs/2008.06283;Learning 2D Invariant Affordance Knowledge for 3D Affordance Grounding|https://arxiv.org/abs/2408.13024;AerialVG: A Challenging Benchmark for Aerial Visual Grounding by Exploring Positional Relations|https://arxiv.org/abs/2504.07836;OpenFly: A Versatile Toolchain and Large-scale Benchmark for Aerial Vision-Language Navigation|https://arxiv.org/abs/2502.18041;MoMa-Kitchen: A 100K+ Benchmark for Affordance-Grounded Last-Mile Navigation in Mobile Manipulation|https://arxiv.org/abs/2503.11081;Think Small, Act Big: Primitive Prompt Learning for Lifelong Robot Manipulation|https://arxiv.org/abs/2504.00420;SpatialVLA: Exploring Spatial Representations for Visual-Language-Action Model|https://arxiv.org/abs/2501.15830;Cross from Left to Right Brain: Adaptive Text Dreamer for Vision-and-Language Navigation|https://arxiv.org/abs/2505.20897"
机器人与智能系统研究所,中国,China,西安交通大学,具身智能、生机电融合、刚柔软复合机器人、智能系统装备与激光加工,面向国家智能制造与陕西机器人产业需求，开展智能感知-控制-系统集成全链条研究，孵化出优艾智合等头部企业，,梅雪松（长江学者、所长）、陈贵敏（柔顺机构）、张小栋（智能检测）,Industrial robot precision control for laser processing,108.98,34.24,http://mec.xjtu.edu.cn/info/1064/9589.htm,Xuesong Mei,Xian Jiaotong University,,"Multi-agent Embodied AI: Advances and Future Directions|https://arxiv.org/abs/2505.05108;State Revisit and Re-explore: Bridging Sim-to-Real Gaps in Offline-and-Online Reinforcement Learning with An Imperfect Simulator|https://arxiv.org/abs/2411.11327;EfficientFlow: Efficient Equivariant Flow Policy Learning for Embodied AI|https://arxiv.org/abs/2503.14734;OpenHelix: A Short Survey, Empirical Analysis, and Open-Source Dual-System VLA Model for Robotic Manipulation|https://arxiv.org/abs/2410.05273;Driving with prior maps: Unified vector prior encoding for autonomous vehicle mapping|https://arxiv.org/abs/2505.17685;Long-horizon Task Planning Based on Multi-modal Diffusion Policy|https://arxiv.org/abs/2405.01534;JanuSVLN: Decoupling Semantics and Spatiality with Dual Implicit Memory for Vision-Language Navigation|https://arxiv.org/abs/2409.05352"
,中国,China,中国科学院大学,多模态智能、智能决策与优化、柔性触力觉感知,云计算与智能信息处理实验室聚焦多模态智能、智能决策与优化、具身智能；研制柔性触力觉传感器，实现“机器手抓取生鸡蛋”级精细操作,,,116.23,39.99,,,Chinese Academy of Sciences,,
浙江大学,中国,China,浙江大学,人形机器人、四足机器人、多模态大模型与机器人融合,聚焦“大模型+机器人”端到端系统，研发人形/四足机器人高速运动与复杂环境导航，建设“具身智能与机器人（余姚）”产教项目,,,120.13,30.28,,,Zhejiang University,,
南京大学,中国,China,南京大学,复杂场景巡检机器人、强化学习+具身智能、视觉-语言导航,开发“云硕智能”极端场景巡检机器人，强化学习赋能具身决策，研究视觉-语言导航与开放世界操作,,,118.78,32.05,,,Nanjing University,,
电子科技大学,中国,China,电子科技大学,人形机器人、多模态传感、空间智能导航,发布“天行者1号”人形机器人，多目VSLAM定位误差<2cm，离子凝胶触觉传感；与京东共建智能机器人联合实验室,,,103.94,30.67,,,University of Electronic Science and Technology of China,,
中国人民大学,中国,China,中国人民大学,铰链物体操纵、大模型决策、人形机器人数据训练平台,提出可泛化铰链物体3D操纵框架，建设3000m²人形机器人数据训练中心，聚焦“人机对齐”示教与数据采集,,,116.31,39.95,,,Renmin University of China,,
中国科学技术大学,中国,China,中国科学技术大学,具身智能与机器人技术、非结构化环境自适应,依托机器人平台采集真实世界数据，研究复杂操控、导航与非结构化环境自适应，提升制造流水线人机协作能力,,,117.26,31.84,,,University of Science and Technology of China,,
北京航空航天大学,中国,China,北京航空航天大学,空中-地面具身智能、人形机器人整机、无人机视觉-语言导航,打造“感知-理解-决策-执行”完整闭环，研发人形机器人整机、无人机视觉语言导航与卫星具身智能，构建“1+M+N”学科交叉体系,,,116.4,39.92,,,Beihang University,,
武汉大学,中国,China,武汉大学,“天问”人形机器人、刚柔耦合灵巧手、视触觉融合抓取,6个月完成“天问”机器人立项到行走，0.1mm精度视触觉灵巧手，80%核心零部件湖北本地化，建设武汉人形机器人完整生态,,,114.37,30.54,,,Wuhan University,,
厦门大学,中国,China,厦门大学,多模态大模型驱动的群体具身智能模拟器,研发“Crowdverse”群体具身智能模拟器，构建“1+N”数字孪生训练平台，服务制造业数字化转型与设备协同,,,118.08,24.44,,,Xiamen University,,
天津大学,中国,China,天津大学,大模型驱动具身智能、人形机器人关键零部件、海陆空无人系统,构建“大模型-关键部件-整机”全链条，研发人形关节、集群控制与医疗机器人，主办具身智能产业论坛,,,117.28,39.12,,,Tianjin University,,
深圳大学,中国,China,深圳大学,群体具身智能仿真、人形机器人整机,依托“广东省具身智能机器人工程技术研究中心”建设 1+N 群体具身智能仿真平台，研发人形整机与场景应用,,,114.05,22.54,,,Shenzhen University,,
苏州大学,中国,China,苏州大学,人形/医疗/农业机器人、微纳-巨系统全尺度,孙立宁团队领衔，布局人形、医疗、农业、微纳机器人，建设苏州市具身智能机器人综合创新中心与 14 亿元产业基金,,,120.62,31.3,,,Soochow University,,
南开大学,中国,China,南开大学,低空系统具身智能、脑机接口康复机器人,揭牌“低空系统与具身智能研究院”，攻关低空无人系统具身智能；牵头国家重点研发计划“脑功能重塑机器人在回路康复”,,,117.16,39.08,,,Nankai University,,
西安电子科技大学,中国,China,西安电子科技大学,人形机器人感知认知、多模态融合,作为工信部人形机器人与具身智能标准支撑单位，聚焦感知认知、多模态融合等底层关键技术,,,108.9,34.12,,,Xidian University,,
上海科技大学,中国,China,上海科技大学,real2sim2real、扩散策略抓取、3D 可供性推理,联合北京通研院打造 Vision and Humanoid Intelligence Lab，提出 AffordDP、DexGrasp Anything 等扩散策略与灵巧抓取框架,,,121.9,31.23,,,ShanghaiTech University,,
南方科技大学,中国,China,南方科技大学,人形机器人整机、具身智能芯片与加速卡,发布“南科盘古”人形机器人，负重/自重比提升 50%；余浩团队研发矢量脉动高能效加速卡，实现 LLM 边缘 7.55× 能效提升,,,113.94,22.6,,,Southern University of Science and Technology,,
四川大学,中国,China,四川大学,工业级具身打磨机器人、医疗机器人与 VR 具身系统,研发“东方智磨”窄流道攀爬打磨机器人，效率翻倍；建设医疗机器人创新中心，攻关 VR-具身智能多模态交互与康复系统,,,104.07,30.67,,,Sichuan University,,
上海人工智能实验室,中国,China,上海人工智能实验室,通用具身大模型、虚实贯通的全栈引擎、多模态三维感知,构建“浦源·桃源”仿真平台、Intern-Robotics全栈引擎、EmbodiedScan/PointLLM多模态数据集，攻关本体-场景-任务三泛化与足式/臂式操作基模型,,,121.47,31.2,,,Shanghai AI Laboratory,,
