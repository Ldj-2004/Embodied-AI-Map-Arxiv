{
  "潘佳老师实验室": [
    {
      "title": "Tactile-Force Alignment in Vision-Language-Action Models for Force-aware Manipulation",
      "url": "https://arxiv.org/abs/2601.20321",
      "date": "2026-01-28",
      "authors_text": "Yuzhe Huang, Pei Lin, Wanlin Li, Daohan Li, Jiajun Li",
      "is_highlight": false,
      "score": 91.0,
      "summary": "The paper introduces TaF-VLA, a framework that enhances Vision-Language-Action models by integrating tactile-force alignment for improved force-aware robotic manipulation in contact-rich tasks.",
      "teaser_image": "https://arxiv.org/html/2601.20321/figures/fig1-teaser.png",
      "debug_abstract": "Vision-Language-Action (VLA) models have recently emerged as powerful generalists for robotic manipulation. However, due to their predominant reliance on visual modalities, they fundamentally lack the physical intuition required for contact-rich tasks that require precise force regulation and physical reasoning. Existing attempts to incorporate vision-based tactile sensing into VLA models typically treat tactile inputs as auxiliary visual textures, thereby overlooking the underlying correlation between surface deformation and interaction dynamics. To bridge this gap, we propose a paradigm shift from tactile-vision alignment to tactile-force alignment. Here, we introduce TaF-VLA, a framework that explicitly grounds high-dimensional tactile observations in physical interaction forces. To facilitate this, we develop an automated tactile-force data acquisition device and curate the TaF-Dataset, comprising over 10 million synchronized tactile observations, 6-axis force/torque, and matrix force map. To align sequential tactile observations with interaction forces, the central component of our approach is the Tactile-Force Adapter (TaF-Adapter), a tactile sensor encoder that extracts discretized latent information for encoding tactile observations. This mechanism ensures that the learned representations capture history-dependent, noise-insensitive physical dynamics rather than static visual textures. Finally, we integrate this force-aligned encoder into a VLA backbone. Extensive real-world experiments demonstrate that TaF-VLA policy significantly outperforms state-of-the-art tactile-vision-aligned and vision-only baselines on contact-rich tasks, verifying its ability to achieve robust, force-aware manipulation through cross-modal physical reasoning."
    },
    {
      "title": "AdaReasoner: Dynamic Tool Orchestration for Iterative Visual Reasoning",
      "url": "https://arxiv.org/abs/2601.18631",
      "date": "2026-01-28",
      "authors_text": "Mingyang Song, Haoyu Sun, Jiawei Gu, Linjie Li, Luxin Xu",
      "is_highlight": false,
      "score": 70.0,
      "summary": "AdaReasoner enhances visual reasoning in multimodal large language models by dynamically orchestrating tool use through a scalable data pipeline and reinforcement learning, achieving state-of-the-art performance.",
      "teaser_image": "https://arxiv.org/html/2601.18631/x2.png",
      "debug_abstract": "When humans face problems beyond their immediate capabilities, they rely on tools, providing a promising paradigm for improving visual reasoning in multimodal large language models (MLLMs). Effective reasoning, therefore, hinges on knowing which tools to use, when to invoke them, and how to compose them over multiple steps, even when faced with new tools or new tasks. We introduce \\textbf{AdaReasoner}, a family of multimodal models that learn tool use as a general reasoning skill rather than as tool-specific or explicitly supervised behavior. AdaReasoner is enabled by (i) a scalable data curation pipeline exposing models to long-horizon, multi-step tool interactions; (ii) Tool-GRPO, a reinforcement learning algorithm that optimizes tool selection and sequencing based on end-task success; and (iii) an adaptive learning mechanism that dynamically regulates tool usage. Together, these components allow models to infer tool utility from task context and intermediate outcomes, enabling coordination of multiple tools and generalization to unseen tools. Empirically, AdaReasoner exhibits strong tool-adaptive and generalization behaviors: it autonomously adopts beneficial tools, suppresses irrelevant ones, and adjusts tool usage frequency based on task demands, despite never being explicitly trained to do so. These capabilities translate into state-of-the-art performance across challenging benchmarks, improving the 7B base model by +24.9\\% on average and surpassing strong proprietary systems such as GPT-5 on multiple tasks, including VSP and Jigsaw."
    },
    {
      "title": "Listen, Look, Drive: Coupling Audio Instructions for User-aware VLA-based Autonomous Driving",
      "url": "https://arxiv.org/abs/2601.12142",
      "date": "2026-01-28",
      "authors_text": "Ziang Guo, Feng Yang, Xuefeng Zhang, Jiaqi Guo, Kun Zhao",
      "is_highlight": false,
      "score": 88.0,
      "summary": "EchoVLA enhances autonomous driving by integrating audio instructions with visual inputs, allowing for user influence and emotional context, significantly improving navigation accuracy and safety.",
      "teaser_image": "https://arxiv.org/html/2601.12142/Figs/overview.png",
      "debug_abstract": "Vision Language Action (VLA) models promise an open-vocabulary interface that can translate perceptual ambiguity into semantically grounded driving decisions, yet they still treat language as a static prior fixed at inference time. As a result, the model must infer continuously shifting objectives from pixels alone, yielding delayed or overly conservative maneuvers. We argue that effective VLAs for autonomous driving need an online channel in which users can influence driving with specific intentions. To this end, we present EchoVLA, a user-aware VLA that couples camera streams with in situ audio instructions. We augment the nuScenes dataset with temporally aligned, intent-specific speech commands generated by converting ego-motion descriptions into synthetic audios. Further, we compose emotional speech-trajectory pairs into a multimodal Chain-of-Thought (CoT) for fine-tuning a Multimodal Large Model (MLM) based on Qwen2.5-Omni. Specifically, we synthesize the audio-augmented dataset with different emotion types paired with corresponding driving behaviors, leveraging the emotional cues embedded in tone, pitch, and speech tempo to reflect varying user states, such as urgent or hesitant intentions, thus enabling our EchoVLA to interpret not only the semantic content but also the emotional context of audio commands for more nuanced and emotionally adaptive driving behavior. In open-loop benchmarks, our approach reduces the average L2 error by $59.4\\%$ and the collision rate by $74.4\\%$ compared to the baseline of vision-only perception. More experiments on nuScenes dataset validate that EchoVLA not only steers the trajectory through audio instructions, but also modulates driving behavior in response to the emotions detected in the user&#39;s speech."
    }
  ],
  "智能系统与机器人实验室 (ISR Lab)": [
    {
      "title": "HINT: Hierarchical Interaction Modeling for Autoregressive Multi-Human Motion Generation",
      "url": "https://arxiv.org/abs/2601.20383",
      "date": "2026-01-28",
      "authors_text": "Mengge Liu, Yan Di, Gu Wang, Yun Qu, Dekai Zhu",
      "is_highlight": false,
      "score": 78.0,
      "summary": "HINT introduces a novel autoregressive framework for multi-human motion generation, enhancing interaction modeling and performance with variable agent counts and long text guidance.",
      "teaser_image": "https://arxiv.org/html/2601.20383/x2.png",
      "debug_abstract": "Text-driven multi-human motion generation with complex interactions remains a challenging problem. Despite progress in performance, existing offline methods that generate fixed-length motions with a fixed number of agents, are inherently limited in handling long or variable text, and varying agent counts. These limitations naturally encourage autoregressive formulations, which predict future motions step by step conditioned on all past trajectories and current text guidance. In this work, we introduce HINT, the first autoregressive framework for multi-human motion generation with Hierarchical INTeraction modeling in diffusion. First, HINT leverages a disentangled motion representation within a canonicalized latent space, decoupling local motion semantics from inter-person interactions. This design facilitates direct adaptation to varying numbers of human participants without requiring additional refinement. Second, HINT adopts a sliding-window strategy for efficient online generation, and aggregates local within-window and global cross-window conditions to capture past human history, inter-person dependencies, and align with text guidance. This strategy not only enables fine-grained interaction modeling within each window but also preserves long-horizon coherence across all the long sequence. Extensive experiments on public benchmarks demonstrate that HINT matches the performance of strong offline models and surpasses autoregressive baselines. Notably, on InterHuman, HINT achieves an FID of 3.100, significantly improving over the previous state-of-the-art score of 5.154."
    },
    {
      "title": "Listen, Look, Drive: Coupling Audio Instructions for User-aware VLA-based Autonomous Driving",
      "url": "https://arxiv.org/abs/2601.12142",
      "date": "2026-01-28",
      "authors_text": "Ziang Guo, Feng Yang, Xuefeng Zhang, Jiaqi Guo, Kun Zhao",
      "is_highlight": false,
      "score": 88.0,
      "summary": "EchoVLA enhances autonomous driving by integrating audio instructions with visual inputs, allowing for user influence and emotional context, significantly improving navigation accuracy and safety.",
      "teaser_image": "https://arxiv.org/html/2601.12142/Figs/overview.png",
      "debug_abstract": "Vision Language Action (VLA) models promise an open-vocabulary interface that can translate perceptual ambiguity into semantically grounded driving decisions, yet they still treat language as a static prior fixed at inference time. As a result, the model must infer continuously shifting objectives from pixels alone, yielding delayed or overly conservative maneuvers. We argue that effective VLAs for autonomous driving need an online channel in which users can influence driving with specific intentions. To this end, we present EchoVLA, a user-aware VLA that couples camera streams with in situ audio instructions. We augment the nuScenes dataset with temporally aligned, intent-specific speech commands generated by converting ego-motion descriptions into synthetic audios. Further, we compose emotional speech-trajectory pairs into a multimodal Chain-of-Thought (CoT) for fine-tuning a Multimodal Large Model (MLM) based on Qwen2.5-Omni. Specifically, we synthesize the audio-augmented dataset with different emotion types paired with corresponding driving behaviors, leveraging the emotional cues embedded in tone, pitch, and speech tempo to reflect varying user states, such as urgent or hesitant intentions, thus enabling our EchoVLA to interpret not only the semantic content but also the emotional context of audio commands for more nuanced and emotionally adaptive driving behavior. In open-loop benchmarks, our approach reduces the average L2 error by $59.4\\%$ and the collision rate by $74.4\\%$ compared to the baseline of vision-only perception. More experiments on nuScenes dataset validate that EchoVLA not only steers the trajectory through audio instructions, but also modulates driving behavior in response to the emotions detected in the user&#39;s speech."
    }
  ],
  "智能产业研究院 (AIR)": [
    {
      "title": "HINT: Hierarchical Interaction Modeling for Autoregressive Multi-Human Motion Generation",
      "url": "https://arxiv.org/abs/2601.20383",
      "date": "2026-01-28",
      "authors_text": "Mengge Liu, Yan Di, Gu Wang, Yun Qu, Dekai Zhu",
      "is_highlight": false,
      "score": 78.0,
      "summary": "HINT introduces a novel autoregressive framework for multi-human motion generation, enhancing interaction modeling and performance with variable agent counts and long text guidance.",
      "teaser_image": "https://arxiv.org/html/2601.20383/x2.png",
      "debug_abstract": "Text-driven multi-human motion generation with complex interactions remains a challenging problem. Despite progress in performance, existing offline methods that generate fixed-length motions with a fixed number of agents, are inherently limited in handling long or variable text, and varying agent counts. These limitations naturally encourage autoregressive formulations, which predict future motions step by step conditioned on all past trajectories and current text guidance. In this work, we introduce HINT, the first autoregressive framework for multi-human motion generation with Hierarchical INTeraction modeling in diffusion. First, HINT leverages a disentangled motion representation within a canonicalized latent space, decoupling local motion semantics from inter-person interactions. This design facilitates direct adaptation to varying numbers of human participants without requiring additional refinement. Second, HINT adopts a sliding-window strategy for efficient online generation, and aggregates local within-window and global cross-window conditions to capture past human history, inter-person dependencies, and align with text guidance. This strategy not only enables fine-grained interaction modeling within each window but also preserves long-horizon coherence across all the long sequence. Extensive experiments on public benchmarks demonstrate that HINT matches the performance of strong offline models and surpasses autoregressive baselines. Notably, on InterHuman, HINT achieves an FID of 3.100, significantly improving over the previous state-of-the-art score of 5.154."
    },
    {
      "title": "Listen, Look, Drive: Coupling Audio Instructions for User-aware VLA-based Autonomous Driving",
      "url": "https://arxiv.org/abs/2601.12142",
      "date": "2026-01-28",
      "authors_text": "Ziang Guo, Feng Yang, Xuefeng Zhang, Jiaqi Guo, Kun Zhao",
      "is_highlight": false,
      "score": 88.0,
      "summary": "EchoVLA enhances autonomous driving by integrating audio instructions with visual inputs, allowing for user influence and emotional context, significantly improving navigation accuracy and safety.",
      "teaser_image": "https://arxiv.org/html/2601.12142/Figs/overview.png",
      "debug_abstract": "Vision Language Action (VLA) models promise an open-vocabulary interface that can translate perceptual ambiguity into semantically grounded driving decisions, yet they still treat language as a static prior fixed at inference time. As a result, the model must infer continuously shifting objectives from pixels alone, yielding delayed or overly conservative maneuvers. We argue that effective VLAs for autonomous driving need an online channel in which users can influence driving with specific intentions. To this end, we present EchoVLA, a user-aware VLA that couples camera streams with in situ audio instructions. We augment the nuScenes dataset with temporally aligned, intent-specific speech commands generated by converting ego-motion descriptions into synthetic audios. Further, we compose emotional speech-trajectory pairs into a multimodal Chain-of-Thought (CoT) for fine-tuning a Multimodal Large Model (MLM) based on Qwen2.5-Omni. Specifically, we synthesize the audio-augmented dataset with different emotion types paired with corresponding driving behaviors, leveraging the emotional cues embedded in tone, pitch, and speech tempo to reflect varying user states, such as urgent or hesitant intentions, thus enabling our EchoVLA to interpret not only the semantic content but also the emotional context of audio commands for more nuanced and emotionally adaptive driving behavior. In open-loop benchmarks, our approach reduces the average L2 error by $59.4\\%$ and the collision rate by $74.4\\%$ compared to the baseline of vision-only perception. More experiments on nuScenes dataset validate that EchoVLA not only steers the trajectory through audio instructions, but also modulates driving behavior in response to the emotions detected in the user&#39;s speech."
    }
  ],
  "具身视觉与机器人实验室 (EVAR Lab)": [
    {
      "title": "HINT: Hierarchical Interaction Modeling for Autoregressive Multi-Human Motion Generation",
      "url": "https://arxiv.org/abs/2601.20383",
      "date": "2026-01-28",
      "authors_text": "Mengge Liu, Yan Di, Gu Wang, Yun Qu, Dekai Zhu",
      "is_highlight": false,
      "score": 78.0,
      "summary": "HINT introduces a novel autoregressive framework for multi-human motion generation, enhancing interaction modeling and performance with variable agent counts and long text guidance.",
      "teaser_image": "https://arxiv.org/html/2601.20383/x2.png",
      "debug_abstract": "Text-driven multi-human motion generation with complex interactions remains a challenging problem. Despite progress in performance, existing offline methods that generate fixed-length motions with a fixed number of agents, are inherently limited in handling long or variable text, and varying agent counts. These limitations naturally encourage autoregressive formulations, which predict future motions step by step conditioned on all past trajectories and current text guidance. In this work, we introduce HINT, the first autoregressive framework for multi-human motion generation with Hierarchical INTeraction modeling in diffusion. First, HINT leverages a disentangled motion representation within a canonicalized latent space, decoupling local motion semantics from inter-person interactions. This design facilitates direct adaptation to varying numbers of human participants without requiring additional refinement. Second, HINT adopts a sliding-window strategy for efficient online generation, and aggregates local within-window and global cross-window conditions to capture past human history, inter-person dependencies, and align with text guidance. This strategy not only enables fine-grained interaction modeling within each window but also preserves long-horizon coherence across all the long sequence. Extensive experiments on public benchmarks demonstrate that HINT matches the performance of strong offline models and surpasses autoregressive baselines. Notably, on InterHuman, HINT achieves an FID of 3.100, significantly improving over the previous state-of-the-art score of 5.154."
    },
    {
      "title": "Listen, Look, Drive: Coupling Audio Instructions for User-aware VLA-based Autonomous Driving",
      "url": "https://arxiv.org/abs/2601.12142",
      "date": "2026-01-28",
      "authors_text": "Ziang Guo, Feng Yang, Xuefeng Zhang, Jiaqi Guo, Kun Zhao",
      "is_highlight": false,
      "score": 88.0,
      "summary": "EchoVLA enhances autonomous driving by integrating audio instructions with visual inputs, allowing for user influence and emotional context, significantly improving navigation accuracy and safety.",
      "teaser_image": "https://arxiv.org/html/2601.12142/Figs/overview.png",
      "debug_abstract": "Vision Language Action (VLA) models promise an open-vocabulary interface that can translate perceptual ambiguity into semantically grounded driving decisions, yet they still treat language as a static prior fixed at inference time. As a result, the model must infer continuously shifting objectives from pixels alone, yielding delayed or overly conservative maneuvers. We argue that effective VLAs for autonomous driving need an online channel in which users can influence driving with specific intentions. To this end, we present EchoVLA, a user-aware VLA that couples camera streams with in situ audio instructions. We augment the nuScenes dataset with temporally aligned, intent-specific speech commands generated by converting ego-motion descriptions into synthetic audios. Further, we compose emotional speech-trajectory pairs into a multimodal Chain-of-Thought (CoT) for fine-tuning a Multimodal Large Model (MLM) based on Qwen2.5-Omni. Specifically, we synthesize the audio-augmented dataset with different emotion types paired with corresponding driving behaviors, leveraging the emotional cues embedded in tone, pitch, and speech tempo to reflect varying user states, such as urgent or hesitant intentions, thus enabling our EchoVLA to interpret not only the semantic content but also the emotional context of audio commands for more nuanced and emotionally adaptive driving behavior. In open-loop benchmarks, our approach reduces the average L2 error by $59.4\\%$ and the collision rate by $74.4\\%$ compared to the baseline of vision-only perception. More experiments on nuScenes dataset validate that EchoVLA not only steers the trajectory through audio instructions, but also modulates driving behavior in response to the emotions detected in the user&#39;s speech."
    }
  ],
  "智能技术与系统实验室": [
    {
      "title": "HINT: Hierarchical Interaction Modeling for Autoregressive Multi-Human Motion Generation",
      "url": "https://arxiv.org/abs/2601.20383",
      "date": "2026-01-28",
      "authors_text": "Mengge Liu, Yan Di, Gu Wang, Yun Qu, Dekai Zhu",
      "is_highlight": false,
      "score": 78.0,
      "summary": "HINT introduces a novel autoregressive framework for multi-human motion generation, enhancing interaction modeling and performance with variable agent counts and long text guidance.",
      "teaser_image": "https://arxiv.org/html/2601.20383/x2.png",
      "debug_abstract": "Text-driven multi-human motion generation with complex interactions remains a challenging problem. Despite progress in performance, existing offline methods that generate fixed-length motions with a fixed number of agents, are inherently limited in handling long or variable text, and varying agent counts. These limitations naturally encourage autoregressive formulations, which predict future motions step by step conditioned on all past trajectories and current text guidance. In this work, we introduce HINT, the first autoregressive framework for multi-human motion generation with Hierarchical INTeraction modeling in diffusion. First, HINT leverages a disentangled motion representation within a canonicalized latent space, decoupling local motion semantics from inter-person interactions. This design facilitates direct adaptation to varying numbers of human participants without requiring additional refinement. Second, HINT adopts a sliding-window strategy for efficient online generation, and aggregates local within-window and global cross-window conditions to capture past human history, inter-person dependencies, and align with text guidance. This strategy not only enables fine-grained interaction modeling within each window but also preserves long-horizon coherence across all the long sequence. Extensive experiments on public benchmarks demonstrate that HINT matches the performance of strong offline models and surpasses autoregressive baselines. Notably, on InterHuman, HINT achieves an FID of 3.100, significantly improving over the previous state-of-the-art score of 5.154."
    },
    {
      "title": "Listen, Look, Drive: Coupling Audio Instructions for User-aware VLA-based Autonomous Driving",
      "url": "https://arxiv.org/abs/2601.12142",
      "date": "2026-01-28",
      "authors_text": "Ziang Guo, Feng Yang, Xuefeng Zhang, Jiaqi Guo, Kun Zhao",
      "is_highlight": false,
      "score": 88.0,
      "summary": "EchoVLA enhances autonomous driving by integrating audio instructions with visual inputs, allowing for user influence and emotional context, significantly improving navigation accuracy and safety.",
      "teaser_image": "https://arxiv.org/html/2601.12142/Figs/overview.png",
      "debug_abstract": "Vision Language Action (VLA) models promise an open-vocabulary interface that can translate perceptual ambiguity into semantically grounded driving decisions, yet they still treat language as a static prior fixed at inference time. As a result, the model must infer continuously shifting objectives from pixels alone, yielding delayed or overly conservative maneuvers. We argue that effective VLAs for autonomous driving need an online channel in which users can influence driving with specific intentions. To this end, we present EchoVLA, a user-aware VLA that couples camera streams with in situ audio instructions. We augment the nuScenes dataset with temporally aligned, intent-specific speech commands generated by converting ego-motion descriptions into synthetic audios. Further, we compose emotional speech-trajectory pairs into a multimodal Chain-of-Thought (CoT) for fine-tuning a Multimodal Large Model (MLM) based on Qwen2.5-Omni. Specifically, we synthesize the audio-augmented dataset with different emotion types paired with corresponding driving behaviors, leveraging the emotional cues embedded in tone, pitch, and speech tempo to reflect varying user states, such as urgent or hesitant intentions, thus enabling our EchoVLA to interpret not only the semantic content but also the emotional context of audio commands for more nuanced and emotionally adaptive driving behavior. In open-loop benchmarks, our approach reduces the average L2 error by $59.4\\%$ and the collision rate by $74.4\\%$ compared to the baseline of vision-only perception. More experiments on nuScenes dataset validate that EchoVLA not only steers the trajectory through audio instructions, but also modulates driving behavior in response to the emotions detected in the user&#39;s speech."
    }
  ],
  "OpenDriveLab": [
    {
      "title": "Tactile-Force Alignment in Vision-Language-Action Models for Force-aware Manipulation",
      "url": "https://arxiv.org/abs/2601.20321",
      "date": "2026-01-28",
      "authors_text": "Yuzhe Huang, Pei Lin, Wanlin Li, Daohan Li, Jiajun Li",
      "is_highlight": false,
      "score": 91.0,
      "summary": "The paper introduces TaF-VLA, a framework that enhances Vision-Language-Action models by integrating tactile-force alignment for improved force-aware robotic manipulation in contact-rich tasks.",
      "teaser_image": "https://arxiv.org/html/2601.20321/figures/fig1-teaser.png",
      "debug_abstract": "Vision-Language-Action (VLA) models have recently emerged as powerful generalists for robotic manipulation. However, due to their predominant reliance on visual modalities, they fundamentally lack the physical intuition required for contact-rich tasks that require precise force regulation and physical reasoning. Existing attempts to incorporate vision-based tactile sensing into VLA models typically treat tactile inputs as auxiliary visual textures, thereby overlooking the underlying correlation between surface deformation and interaction dynamics. To bridge this gap, we propose a paradigm shift from tactile-vision alignment to tactile-force alignment. Here, we introduce TaF-VLA, a framework that explicitly grounds high-dimensional tactile observations in physical interaction forces. To facilitate this, we develop an automated tactile-force data acquisition device and curate the TaF-Dataset, comprising over 10 million synchronized tactile observations, 6-axis force/torque, and matrix force map. To align sequential tactile observations with interaction forces, the central component of our approach is the Tactile-Force Adapter (TaF-Adapter), a tactile sensor encoder that extracts discretized latent information for encoding tactile observations. This mechanism ensures that the learned representations capture history-dependent, noise-insensitive physical dynamics rather than static visual textures. Finally, we integrate this force-aligned encoder into a VLA backbone. Extensive real-world experiments demonstrate that TaF-VLA policy significantly outperforms state-of-the-art tactile-vision-aligned and vision-only baselines on contact-rich tasks, verifying its ability to achieve robust, force-aware manipulation through cross-modal physical reasoning."
    },
    {
      "title": "AdaReasoner: Dynamic Tool Orchestration for Iterative Visual Reasoning",
      "url": "https://arxiv.org/abs/2601.18631",
      "date": "2026-01-28",
      "authors_text": "Mingyang Song, Haoyu Sun, Jiawei Gu, Linjie Li, Luxin Xu",
      "is_highlight": false,
      "score": 70.0,
      "summary": "AdaReasoner enhances visual reasoning in multimodal large language models by dynamically orchestrating tool use through a scalable data pipeline and reinforcement learning, achieving state-of-the-art performance.",
      "teaser_image": "https://arxiv.org/html/2601.18631/x2.png",
      "debug_abstract": "When humans face problems beyond their immediate capabilities, they rely on tools, providing a promising paradigm for improving visual reasoning in multimodal large language models (MLLMs). Effective reasoning, therefore, hinges on knowing which tools to use, when to invoke them, and how to compose them over multiple steps, even when faced with new tools or new tasks. We introduce \\textbf{AdaReasoner}, a family of multimodal models that learn tool use as a general reasoning skill rather than as tool-specific or explicitly supervised behavior. AdaReasoner is enabled by (i) a scalable data curation pipeline exposing models to long-horizon, multi-step tool interactions; (ii) Tool-GRPO, a reinforcement learning algorithm that optimizes tool selection and sequencing based on end-task success; and (iii) an adaptive learning mechanism that dynamically regulates tool usage. Together, these components allow models to infer tool utility from task context and intermediate outcomes, enabling coordination of multiple tools and generalization to unseen tools. Empirically, AdaReasoner exhibits strong tool-adaptive and generalization behaviors: it autonomously adopts beneficial tools, suppresses irrelevant ones, and adjusts tool usage frequency based on task demands, despite never being explicitly trained to do so. These capabilities translate into state-of-the-art performance across challenging benchmarks, improving the 7B base model by +24.9\\% on average and surpassing strong proprietary systems such as GPT-5 on multiple tasks, including VSP and Jigsaw."
    },
    {
      "title": "Listen, Look, Drive: Coupling Audio Instructions for User-aware VLA-based Autonomous Driving",
      "url": "https://arxiv.org/abs/2601.12142",
      "date": "2026-01-28",
      "authors_text": "Ziang Guo, Feng Yang, Xuefeng Zhang, Jiaqi Guo, Kun Zhao",
      "is_highlight": false,
      "score": 88.0,
      "summary": "EchoVLA enhances autonomous driving by integrating audio instructions with visual inputs, allowing for user influence and emotional context, significantly improving navigation accuracy and safety.",
      "teaser_image": "https://arxiv.org/html/2601.12142/Figs/overview.png",
      "debug_abstract": "Vision Language Action (VLA) models promise an open-vocabulary interface that can translate perceptual ambiguity into semantically grounded driving decisions, yet they still treat language as a static prior fixed at inference time. As a result, the model must infer continuously shifting objectives from pixels alone, yielding delayed or overly conservative maneuvers. We argue that effective VLAs for autonomous driving need an online channel in which users can influence driving with specific intentions. To this end, we present EchoVLA, a user-aware VLA that couples camera streams with in situ audio instructions. We augment the nuScenes dataset with temporally aligned, intent-specific speech commands generated by converting ego-motion descriptions into synthetic audios. Further, we compose emotional speech-trajectory pairs into a multimodal Chain-of-Thought (CoT) for fine-tuning a Multimodal Large Model (MLM) based on Qwen2.5-Omni. Specifically, we synthesize the audio-augmented dataset with different emotion types paired with corresponding driving behaviors, leveraging the emotional cues embedded in tone, pitch, and speech tempo to reflect varying user states, such as urgent or hesitant intentions, thus enabling our EchoVLA to interpret not only the semantic content but also the emotional context of audio commands for more nuanced and emotionally adaptive driving behavior. In open-loop benchmarks, our approach reduces the average L2 error by $59.4\\%$ and the collision rate by $74.4\\%$ compared to the baseline of vision-only perception. More experiments on nuScenes dataset validate that EchoVLA not only steers the trajectory through audio instructions, but also modulates driving behavior in response to the emotions detected in the user&#39;s speech."
    }
  ],
  "机器人控制实验室": [
    {
      "title": "HINT: Hierarchical Interaction Modeling for Autoregressive Multi-Human Motion Generation",
      "url": "https://arxiv.org/abs/2601.20383",
      "date": "2026-01-28",
      "authors_text": "Mengge Liu, Yan Di, Gu Wang, Yun Qu, Dekai Zhu",
      "is_highlight": false,
      "score": 78.0,
      "summary": "HINT introduces a novel autoregressive framework for multi-human motion generation, enhancing interaction modeling and performance with variable agent counts and long text guidance.",
      "teaser_image": "https://arxiv.org/html/2601.20383/x2.png",
      "debug_abstract": "Text-driven multi-human motion generation with complex interactions remains a challenging problem. Despite progress in performance, existing offline methods that generate fixed-length motions with a fixed number of agents, are inherently limited in handling long or variable text, and varying agent counts. These limitations naturally encourage autoregressive formulations, which predict future motions step by step conditioned on all past trajectories and current text guidance. In this work, we introduce HINT, the first autoregressive framework for multi-human motion generation with Hierarchical INTeraction modeling in diffusion. First, HINT leverages a disentangled motion representation within a canonicalized latent space, decoupling local motion semantics from inter-person interactions. This design facilitates direct adaptation to varying numbers of human participants without requiring additional refinement. Second, HINT adopts a sliding-window strategy for efficient online generation, and aggregates local within-window and global cross-window conditions to capture past human history, inter-person dependencies, and align with text guidance. This strategy not only enables fine-grained interaction modeling within each window but also preserves long-horizon coherence across all the long sequence. Extensive experiments on public benchmarks demonstrate that HINT matches the performance of strong offline models and surpasses autoregressive baselines. Notably, on InterHuman, HINT achieves an FID of 3.100, significantly improving over the previous state-of-the-art score of 5.154."
    },
    {
      "title": "Listen, Look, Drive: Coupling Audio Instructions for User-aware VLA-based Autonomous Driving",
      "url": "https://arxiv.org/abs/2601.12142",
      "date": "2026-01-28",
      "authors_text": "Ziang Guo, Feng Yang, Xuefeng Zhang, Jiaqi Guo, Kun Zhao",
      "is_highlight": false,
      "score": 88.0,
      "summary": "EchoVLA enhances autonomous driving by integrating audio instructions with visual inputs, allowing for user influence and emotional context, significantly improving navigation accuracy and safety.",
      "teaser_image": "https://arxiv.org/html/2601.12142/Figs/overview.png",
      "debug_abstract": "Vision Language Action (VLA) models promise an open-vocabulary interface that can translate perceptual ambiguity into semantically grounded driving decisions, yet they still treat language as a static prior fixed at inference time. As a result, the model must infer continuously shifting objectives from pixels alone, yielding delayed or overly conservative maneuvers. We argue that effective VLAs for autonomous driving need an online channel in which users can influence driving with specific intentions. To this end, we present EchoVLA, a user-aware VLA that couples camera streams with in situ audio instructions. We augment the nuScenes dataset with temporally aligned, intent-specific speech commands generated by converting ego-motion descriptions into synthetic audios. Further, we compose emotional speech-trajectory pairs into a multimodal Chain-of-Thought (CoT) for fine-tuning a Multimodal Large Model (MLM) based on Qwen2.5-Omni. Specifically, we synthesize the audio-augmented dataset with different emotion types paired with corresponding driving behaviors, leveraging the emotional cues embedded in tone, pitch, and speech tempo to reflect varying user states, such as urgent or hesitant intentions, thus enabling our EchoVLA to interpret not only the semantic content but also the emotional context of audio commands for more nuanced and emotionally adaptive driving behavior. In open-loop benchmarks, our approach reduces the average L2 error by $59.4\\%$ and the collision rate by $74.4\\%$ compared to the baseline of vision-only perception. More experiments on nuScenes dataset validate that EchoVLA not only steers the trajectory through audio instructions, but also modulates driving behavior in response to the emotions detected in the user&#39;s speech."
    }
  ],
  "MARS多模态学习实验室": [
    {
      "title": "HINT: Hierarchical Interaction Modeling for Autoregressive Multi-Human Motion Generation",
      "url": "https://arxiv.org/abs/2601.20383",
      "date": "2026-01-28",
      "authors_text": "Mengge Liu, Yan Di, Gu Wang, Yun Qu, Dekai Zhu",
      "is_highlight": false,
      "score": 78.0,
      "summary": "HINT introduces a novel autoregressive framework for multi-human motion generation, enhancing interaction modeling and performance with variable agent counts and long text guidance.",
      "teaser_image": "https://arxiv.org/html/2601.20383/x2.png",
      "debug_abstract": "Text-driven multi-human motion generation with complex interactions remains a challenging problem. Despite progress in performance, existing offline methods that generate fixed-length motions with a fixed number of agents, are inherently limited in handling long or variable text, and varying agent counts. These limitations naturally encourage autoregressive formulations, which predict future motions step by step conditioned on all past trajectories and current text guidance. In this work, we introduce HINT, the first autoregressive framework for multi-human motion generation with Hierarchical INTeraction modeling in diffusion. First, HINT leverages a disentangled motion representation within a canonicalized latent space, decoupling local motion semantics from inter-person interactions. This design facilitates direct adaptation to varying numbers of human participants without requiring additional refinement. Second, HINT adopts a sliding-window strategy for efficient online generation, and aggregates local within-window and global cross-window conditions to capture past human history, inter-person dependencies, and align with text guidance. This strategy not only enables fine-grained interaction modeling within each window but also preserves long-horizon coherence across all the long sequence. Extensive experiments on public benchmarks demonstrate that HINT matches the performance of strong offline models and surpasses autoregressive baselines. Notably, on InterHuman, HINT achieves an FID of 3.100, significantly improving over the previous state-of-the-art score of 5.154."
    },
    {
      "title": "Listen, Look, Drive: Coupling Audio Instructions for User-aware VLA-based Autonomous Driving",
      "url": "https://arxiv.org/abs/2601.12142",
      "date": "2026-01-28",
      "authors_text": "Ziang Guo, Feng Yang, Xuefeng Zhang, Jiaqi Guo, Kun Zhao",
      "is_highlight": false,
      "score": 88.0,
      "summary": "EchoVLA enhances autonomous driving by integrating audio instructions with visual inputs, allowing for user influence and emotional context, significantly improving navigation accuracy and safety.",
      "teaser_image": "https://arxiv.org/html/2601.12142/Figs/overview.png",
      "debug_abstract": "Vision Language Action (VLA) models promise an open-vocabulary interface that can translate perceptual ambiguity into semantically grounded driving decisions, yet they still treat language as a static prior fixed at inference time. As a result, the model must infer continuously shifting objectives from pixels alone, yielding delayed or overly conservative maneuvers. We argue that effective VLAs for autonomous driving need an online channel in which users can influence driving with specific intentions. To this end, we present EchoVLA, a user-aware VLA that couples camera streams with in situ audio instructions. We augment the nuScenes dataset with temporally aligned, intent-specific speech commands generated by converting ego-motion descriptions into synthetic audios. Further, we compose emotional speech-trajectory pairs into a multimodal Chain-of-Thought (CoT) for fine-tuning a Multimodal Large Model (MLM) based on Qwen2.5-Omni. Specifically, we synthesize the audio-augmented dataset with different emotion types paired with corresponding driving behaviors, leveraging the emotional cues embedded in tone, pitch, and speech tempo to reflect varying user states, such as urgent or hesitant intentions, thus enabling our EchoVLA to interpret not only the semantic content but also the emotional context of audio commands for more nuanced and emotionally adaptive driving behavior. In open-loop benchmarks, our approach reduces the average L2 error by $59.4\\%$ and the collision rate by $74.4\\%$ compared to the baseline of vision-only perception. More experiments on nuScenes dataset validate that EchoVLA not only steers the trajectory through audio instructions, but also modulates driving behavior in response to the emotions detected in the user&#39;s speech."
    }
  ],
  "Language and Vision (LaVi) Lab": [
    {
      "title": "Tactile-Force Alignment in Vision-Language-Action Models for Force-aware Manipulation",
      "url": "https://arxiv.org/abs/2601.20321",
      "date": "2026-01-28",
      "authors_text": "Yuzhe Huang, Pei Lin, Wanlin Li, Daohan Li, Jiajun Li",
      "is_highlight": false,
      "score": 91.0,
      "summary": "The paper introduces TaF-VLA, a framework that enhances Vision-Language-Action models by integrating tactile-force alignment for improved force-aware robotic manipulation in contact-rich tasks.",
      "teaser_image": "https://arxiv.org/html/2601.20321/figures/fig1-teaser.png",
      "debug_abstract": "Vision-Language-Action (VLA) models have recently emerged as powerful generalists for robotic manipulation. However, due to their predominant reliance on visual modalities, they fundamentally lack the physical intuition required for contact-rich tasks that require precise force regulation and physical reasoning. Existing attempts to incorporate vision-based tactile sensing into VLA models typically treat tactile inputs as auxiliary visual textures, thereby overlooking the underlying correlation between surface deformation and interaction dynamics. To bridge this gap, we propose a paradigm shift from tactile-vision alignment to tactile-force alignment. Here, we introduce TaF-VLA, a framework that explicitly grounds high-dimensional tactile observations in physical interaction forces. To facilitate this, we develop an automated tactile-force data acquisition device and curate the TaF-Dataset, comprising over 10 million synchronized tactile observations, 6-axis force/torque, and matrix force map. To align sequential tactile observations with interaction forces, the central component of our approach is the Tactile-Force Adapter (TaF-Adapter), a tactile sensor encoder that extracts discretized latent information for encoding tactile observations. This mechanism ensures that the learned representations capture history-dependent, noise-insensitive physical dynamics rather than static visual textures. Finally, we integrate this force-aligned encoder into a VLA backbone. Extensive real-world experiments demonstrate that TaF-VLA policy significantly outperforms state-of-the-art tactile-vision-aligned and vision-only baselines on contact-rich tasks, verifying its ability to achieve robust, force-aware manipulation through cross-modal physical reasoning."
    },
    {
      "title": "AdaReasoner: Dynamic Tool Orchestration for Iterative Visual Reasoning",
      "url": "https://arxiv.org/abs/2601.18631",
      "date": "2026-01-28",
      "authors_text": "Mingyang Song, Haoyu Sun, Jiawei Gu, Linjie Li, Luxin Xu",
      "is_highlight": false,
      "score": 70.0,
      "summary": "AdaReasoner enhances visual reasoning in multimodal large language models by dynamically orchestrating tool use through a scalable data pipeline and reinforcement learning, achieving state-of-the-art performance.",
      "teaser_image": "https://arxiv.org/html/2601.18631/x2.png",
      "debug_abstract": "When humans face problems beyond their immediate capabilities, they rely on tools, providing a promising paradigm for improving visual reasoning in multimodal large language models (MLLMs). Effective reasoning, therefore, hinges on knowing which tools to use, when to invoke them, and how to compose them over multiple steps, even when faced with new tools or new tasks. We introduce \\textbf{AdaReasoner}, a family of multimodal models that learn tool use as a general reasoning skill rather than as tool-specific or explicitly supervised behavior. AdaReasoner is enabled by (i) a scalable data curation pipeline exposing models to long-horizon, multi-step tool interactions; (ii) Tool-GRPO, a reinforcement learning algorithm that optimizes tool selection and sequencing based on end-task success; and (iii) an adaptive learning mechanism that dynamically regulates tool usage. Together, these components allow models to infer tool utility from task context and intermediate outcomes, enabling coordination of multiple tools and generalization to unseen tools. Empirically, AdaReasoner exhibits strong tool-adaptive and generalization behaviors: it autonomously adopts beneficial tools, suppresses irrelevant ones, and adjusts tool usage frequency based on task demands, despite never being explicitly trained to do so. These capabilities translate into state-of-the-art performance across challenging benchmarks, improving the 7B base model by +24.9\\% on average and surpassing strong proprietary systems such as GPT-5 on multiple tasks, including VSP and Jigsaw."
    },
    {
      "title": "Listen, Look, Drive: Coupling Audio Instructions for User-aware VLA-based Autonomous Driving",
      "url": "https://arxiv.org/abs/2601.12142",
      "date": "2026-01-28",
      "authors_text": "Ziang Guo, Feng Yang, Xuefeng Zhang, Jiaqi Guo, Kun Zhao",
      "is_highlight": false,
      "score": 88.0,
      "summary": "EchoVLA enhances autonomous driving by integrating audio instructions with visual inputs, allowing for user influence and emotional context, significantly improving navigation accuracy and safety.",
      "teaser_image": "https://arxiv.org/html/2601.12142/Figs/overview.png",
      "debug_abstract": "Vision Language Action (VLA) models promise an open-vocabulary interface that can translate perceptual ambiguity into semantically grounded driving decisions, yet they still treat language as a static prior fixed at inference time. As a result, the model must infer continuously shifting objectives from pixels alone, yielding delayed or overly conservative maneuvers. We argue that effective VLAs for autonomous driving need an online channel in which users can influence driving with specific intentions. To this end, we present EchoVLA, a user-aware VLA that couples camera streams with in situ audio instructions. We augment the nuScenes dataset with temporally aligned, intent-specific speech commands generated by converting ego-motion descriptions into synthetic audios. Further, we compose emotional speech-trajectory pairs into a multimodal Chain-of-Thought (CoT) for fine-tuning a Multimodal Large Model (MLM) based on Qwen2.5-Omni. Specifically, we synthesize the audio-augmented dataset with different emotion types paired with corresponding driving behaviors, leveraging the emotional cues embedded in tone, pitch, and speech tempo to reflect varying user states, such as urgent or hesitant intentions, thus enabling our EchoVLA to interpret not only the semantic content but also the emotional context of audio commands for more nuanced and emotionally adaptive driving behavior. In open-loop benchmarks, our approach reduces the average L2 error by $59.4\\%$ and the collision rate by $74.4\\%$ compared to the baseline of vision-only perception. More experiments on nuScenes dataset validate that EchoVLA not only steers the trajectory through audio instructions, but also modulates driving behavior in response to the emotions detected in the user&#39;s speech."
    }
  ],
  "香港大学机械工程系机器人实验室": [
    {
      "title": "Tactile-Force Alignment in Vision-Language-Action Models for Force-aware Manipulation",
      "url": "https://arxiv.org/abs/2601.20321",
      "date": "2026-01-28",
      "authors_text": "Yuzhe Huang, Pei Lin, Wanlin Li, Daohan Li, Jiajun Li",
      "is_highlight": false,
      "score": 91.0,
      "summary": "The paper introduces TaF-VLA, a framework that enhances Vision-Language-Action models by integrating tactile-force alignment for improved force-aware robotic manipulation in contact-rich tasks.",
      "teaser_image": "https://arxiv.org/html/2601.20321/figures/fig1-teaser.png",
      "debug_abstract": "Vision-Language-Action (VLA) models have recently emerged as powerful generalists for robotic manipulation. However, due to their predominant reliance on visual modalities, they fundamentally lack the physical intuition required for contact-rich tasks that require precise force regulation and physical reasoning. Existing attempts to incorporate vision-based tactile sensing into VLA models typically treat tactile inputs as auxiliary visual textures, thereby overlooking the underlying correlation between surface deformation and interaction dynamics. To bridge this gap, we propose a paradigm shift from tactile-vision alignment to tactile-force alignment. Here, we introduce TaF-VLA, a framework that explicitly grounds high-dimensional tactile observations in physical interaction forces. To facilitate this, we develop an automated tactile-force data acquisition device and curate the TaF-Dataset, comprising over 10 million synchronized tactile observations, 6-axis force/torque, and matrix force map. To align sequential tactile observations with interaction forces, the central component of our approach is the Tactile-Force Adapter (TaF-Adapter), a tactile sensor encoder that extracts discretized latent information for encoding tactile observations. This mechanism ensures that the learned representations capture history-dependent, noise-insensitive physical dynamics rather than static visual textures. Finally, we integrate this force-aligned encoder into a VLA backbone. Extensive real-world experiments demonstrate that TaF-VLA policy significantly outperforms state-of-the-art tactile-vision-aligned and vision-only baselines on contact-rich tasks, verifying its ability to achieve robust, force-aware manipulation through cross-modal physical reasoning."
    },
    {
      "title": "AdaReasoner: Dynamic Tool Orchestration for Iterative Visual Reasoning",
      "url": "https://arxiv.org/abs/2601.18631",
      "date": "2026-01-28",
      "authors_text": "Mingyang Song, Haoyu Sun, Jiawei Gu, Linjie Li, Luxin Xu",
      "is_highlight": false,
      "score": 70.0,
      "summary": "AdaReasoner enhances visual reasoning in multimodal large language models by dynamically orchestrating tool use through a scalable data pipeline and reinforcement learning, achieving state-of-the-art performance.",
      "teaser_image": "https://arxiv.org/html/2601.18631/x2.png",
      "debug_abstract": "When humans face problems beyond their immediate capabilities, they rely on tools, providing a promising paradigm for improving visual reasoning in multimodal large language models (MLLMs). Effective reasoning, therefore, hinges on knowing which tools to use, when to invoke them, and how to compose them over multiple steps, even when faced with new tools or new tasks. We introduce \\textbf{AdaReasoner}, a family of multimodal models that learn tool use as a general reasoning skill rather than as tool-specific or explicitly supervised behavior. AdaReasoner is enabled by (i) a scalable data curation pipeline exposing models to long-horizon, multi-step tool interactions; (ii) Tool-GRPO, a reinforcement learning algorithm that optimizes tool selection and sequencing based on end-task success; and (iii) an adaptive learning mechanism that dynamically regulates tool usage. Together, these components allow models to infer tool utility from task context and intermediate outcomes, enabling coordination of multiple tools and generalization to unseen tools. Empirically, AdaReasoner exhibits strong tool-adaptive and generalization behaviors: it autonomously adopts beneficial tools, suppresses irrelevant ones, and adjusts tool usage frequency based on task demands, despite never being explicitly trained to do so. These capabilities translate into state-of-the-art performance across challenging benchmarks, improving the 7B base model by +24.9\\% on average and surpassing strong proprietary systems such as GPT-5 on multiple tasks, including VSP and Jigsaw."
    },
    {
      "title": "Listen, Look, Drive: Coupling Audio Instructions for User-aware VLA-based Autonomous Driving",
      "url": "https://arxiv.org/abs/2601.12142",
      "date": "2026-01-28",
      "authors_text": "Ziang Guo, Feng Yang, Xuefeng Zhang, Jiaqi Guo, Kun Zhao",
      "is_highlight": false,
      "score": 88.0,
      "summary": "EchoVLA enhances autonomous driving by integrating audio instructions with visual inputs, allowing for user influence and emotional context, significantly improving navigation accuracy and safety.",
      "teaser_image": "https://arxiv.org/html/2601.12142/Figs/overview.png",
      "debug_abstract": "Vision Language Action (VLA) models promise an open-vocabulary interface that can translate perceptual ambiguity into semantically grounded driving decisions, yet they still treat language as a static prior fixed at inference time. As a result, the model must infer continuously shifting objectives from pixels alone, yielding delayed or overly conservative maneuvers. We argue that effective VLAs for autonomous driving need an online channel in which users can influence driving with specific intentions. To this end, we present EchoVLA, a user-aware VLA that couples camera streams with in situ audio instructions. We augment the nuScenes dataset with temporally aligned, intent-specific speech commands generated by converting ego-motion descriptions into synthetic audios. Further, we compose emotional speech-trajectory pairs into a multimodal Chain-of-Thought (CoT) for fine-tuning a Multimodal Large Model (MLM) based on Qwen2.5-Omni. Specifically, we synthesize the audio-augmented dataset with different emotion types paired with corresponding driving behaviors, leveraging the emotional cues embedded in tone, pitch, and speech tempo to reflect varying user states, such as urgent or hesitant intentions, thus enabling our EchoVLA to interpret not only the semantic content but also the emotional context of audio commands for more nuanced and emotionally adaptive driving behavior. In open-loop benchmarks, our approach reduces the average L2 error by $59.4\\%$ and the collision rate by $74.4\\%$ compared to the baseline of vision-only perception. More experiments on nuScenes dataset validate that EchoVLA not only steers the trajectory through audio instructions, but also modulates driving behavior in response to the emotions detected in the user&#39;s speech."
    }
  ],
  "Hengshuang Zhao老师实验室": [
    {
      "title": "Tactile-Force Alignment in Vision-Language-Action Models for Force-aware Manipulation",
      "url": "https://arxiv.org/abs/2601.20321",
      "date": "2026-01-28",
      "authors_text": "Yuzhe Huang, Pei Lin, Wanlin Li, Daohan Li, Jiajun Li",
      "is_highlight": false,
      "score": 91.0,
      "summary": "The paper introduces TaF-VLA, a framework that enhances Vision-Language-Action models by integrating tactile-force alignment for improved force-aware robotic manipulation in contact-rich tasks.",
      "teaser_image": "https://arxiv.org/html/2601.20321/figures/fig1-teaser.png",
      "debug_abstract": "Vision-Language-Action (VLA) models have recently emerged as powerful generalists for robotic manipulation. However, due to their predominant reliance on visual modalities, they fundamentally lack the physical intuition required for contact-rich tasks that require precise force regulation and physical reasoning. Existing attempts to incorporate vision-based tactile sensing into VLA models typically treat tactile inputs as auxiliary visual textures, thereby overlooking the underlying correlation between surface deformation and interaction dynamics. To bridge this gap, we propose a paradigm shift from tactile-vision alignment to tactile-force alignment. Here, we introduce TaF-VLA, a framework that explicitly grounds high-dimensional tactile observations in physical interaction forces. To facilitate this, we develop an automated tactile-force data acquisition device and curate the TaF-Dataset, comprising over 10 million synchronized tactile observations, 6-axis force/torque, and matrix force map. To align sequential tactile observations with interaction forces, the central component of our approach is the Tactile-Force Adapter (TaF-Adapter), a tactile sensor encoder that extracts discretized latent information for encoding tactile observations. This mechanism ensures that the learned representations capture history-dependent, noise-insensitive physical dynamics rather than static visual textures. Finally, we integrate this force-aligned encoder into a VLA backbone. Extensive real-world experiments demonstrate that TaF-VLA policy significantly outperforms state-of-the-art tactile-vision-aligned and vision-only baselines on contact-rich tasks, verifying its ability to achieve robust, force-aware manipulation through cross-modal physical reasoning."
    },
    {
      "title": "AdaReasoner: Dynamic Tool Orchestration for Iterative Visual Reasoning",
      "url": "https://arxiv.org/abs/2601.18631",
      "date": "2026-01-28",
      "authors_text": "Mingyang Song, Haoyu Sun, Jiawei Gu, Linjie Li, Luxin Xu",
      "is_highlight": false,
      "score": 70.0,
      "summary": "AdaReasoner enhances visual reasoning in multimodal large language models by dynamically orchestrating tool use through a scalable data pipeline and reinforcement learning, achieving state-of-the-art performance.",
      "teaser_image": "https://arxiv.org/html/2601.18631/x2.png",
      "debug_abstract": "When humans face problems beyond their immediate capabilities, they rely on tools, providing a promising paradigm for improving visual reasoning in multimodal large language models (MLLMs). Effective reasoning, therefore, hinges on knowing which tools to use, when to invoke them, and how to compose them over multiple steps, even when faced with new tools or new tasks. We introduce \\textbf{AdaReasoner}, a family of multimodal models that learn tool use as a general reasoning skill rather than as tool-specific or explicitly supervised behavior. AdaReasoner is enabled by (i) a scalable data curation pipeline exposing models to long-horizon, multi-step tool interactions; (ii) Tool-GRPO, a reinforcement learning algorithm that optimizes tool selection and sequencing based on end-task success; and (iii) an adaptive learning mechanism that dynamically regulates tool usage. Together, these components allow models to infer tool utility from task context and intermediate outcomes, enabling coordination of multiple tools and generalization to unseen tools. Empirically, AdaReasoner exhibits strong tool-adaptive and generalization behaviors: it autonomously adopts beneficial tools, suppresses irrelevant ones, and adjusts tool usage frequency based on task demands, despite never being explicitly trained to do so. These capabilities translate into state-of-the-art performance across challenging benchmarks, improving the 7B base model by +24.9\\% on average and surpassing strong proprietary systems such as GPT-5 on multiple tasks, including VSP and Jigsaw."
    },
    {
      "title": "Listen, Look, Drive: Coupling Audio Instructions for User-aware VLA-based Autonomous Driving",
      "url": "https://arxiv.org/abs/2601.12142",
      "date": "2026-01-28",
      "authors_text": "Ziang Guo, Feng Yang, Xuefeng Zhang, Jiaqi Guo, Kun Zhao",
      "is_highlight": false,
      "score": 88.0,
      "summary": "EchoVLA enhances autonomous driving by integrating audio instructions with visual inputs, allowing for user influence and emotional context, significantly improving navigation accuracy and safety.",
      "teaser_image": "https://arxiv.org/html/2601.12142/Figs/overview.png",
      "debug_abstract": "Vision Language Action (VLA) models promise an open-vocabulary interface that can translate perceptual ambiguity into semantically grounded driving decisions, yet they still treat language as a static prior fixed at inference time. As a result, the model must infer continuously shifting objectives from pixels alone, yielding delayed or overly conservative maneuvers. We argue that effective VLAs for autonomous driving need an online channel in which users can influence driving with specific intentions. To this end, we present EchoVLA, a user-aware VLA that couples camera streams with in situ audio instructions. We augment the nuScenes dataset with temporally aligned, intent-specific speech commands generated by converting ego-motion descriptions into synthetic audios. Further, we compose emotional speech-trajectory pairs into a multimodal Chain-of-Thought (CoT) for fine-tuning a Multimodal Large Model (MLM) based on Qwen2.5-Omni. Specifically, we synthesize the audio-augmented dataset with different emotion types paired with corresponding driving behaviors, leveraging the emotional cues embedded in tone, pitch, and speech tempo to reflect varying user states, such as urgent or hesitant intentions, thus enabling our EchoVLA to interpret not only the semantic content but also the emotional context of audio commands for more nuanced and emotionally adaptive driving behavior. In open-loop benchmarks, our approach reduces the average L2 error by $59.4\\%$ and the collision rate by $74.4\\%$ compared to the baseline of vision-only perception. More experiments on nuScenes dataset validate that EchoVLA not only steers the trajectory through audio instructions, but also modulates driving behavior in response to the emotions detected in the user&#39;s speech."
    }
  ],
  "具身智能实验室 (TEA Lab)": [
    {
      "title": "HINT: Hierarchical Interaction Modeling for Autoregressive Multi-Human Motion Generation",
      "url": "https://arxiv.org/abs/2601.20383",
      "date": "2026-01-28",
      "authors_text": "Mengge Liu, Yan Di, Gu Wang, Yun Qu, Dekai Zhu",
      "is_highlight": false,
      "score": 78.0,
      "summary": "HINT introduces a novel autoregressive framework for multi-human motion generation, enhancing interaction modeling and performance with variable agent counts and long text guidance.",
      "teaser_image": "https://arxiv.org/html/2601.20383/x2.png",
      "debug_abstract": "Text-driven multi-human motion generation with complex interactions remains a challenging problem. Despite progress in performance, existing offline methods that generate fixed-length motions with a fixed number of agents, are inherently limited in handling long or variable text, and varying agent counts. These limitations naturally encourage autoregressive formulations, which predict future motions step by step conditioned on all past trajectories and current text guidance. In this work, we introduce HINT, the first autoregressive framework for multi-human motion generation with Hierarchical INTeraction modeling in diffusion. First, HINT leverages a disentangled motion representation within a canonicalized latent space, decoupling local motion semantics from inter-person interactions. This design facilitates direct adaptation to varying numbers of human participants without requiring additional refinement. Second, HINT adopts a sliding-window strategy for efficient online generation, and aggregates local within-window and global cross-window conditions to capture past human history, inter-person dependencies, and align with text guidance. This strategy not only enables fine-grained interaction modeling within each window but also preserves long-horizon coherence across all the long sequence. Extensive experiments on public benchmarks demonstrate that HINT matches the performance of strong offline models and surpasses autoregressive baselines. Notably, on InterHuman, HINT achieves an FID of 3.100, significantly improving over the previous state-of-the-art score of 5.154."
    },
    {
      "title": "Listen, Look, Drive: Coupling Audio Instructions for User-aware VLA-based Autonomous Driving",
      "url": "https://arxiv.org/abs/2601.12142",
      "date": "2026-01-28",
      "authors_text": "Ziang Guo, Feng Yang, Xuefeng Zhang, Jiaqi Guo, Kun Zhao",
      "is_highlight": false,
      "score": 88.0,
      "summary": "EchoVLA enhances autonomous driving by integrating audio instructions with visual inputs, allowing for user influence and emotional context, significantly improving navigation accuracy and safety.",
      "teaser_image": "https://arxiv.org/html/2601.12142/Figs/overview.png",
      "debug_abstract": "Vision Language Action (VLA) models promise an open-vocabulary interface that can translate perceptual ambiguity into semantically grounded driving decisions, yet they still treat language as a static prior fixed at inference time. As a result, the model must infer continuously shifting objectives from pixels alone, yielding delayed or overly conservative maneuvers. We argue that effective VLAs for autonomous driving need an online channel in which users can influence driving with specific intentions. To this end, we present EchoVLA, a user-aware VLA that couples camera streams with in situ audio instructions. We augment the nuScenes dataset with temporally aligned, intent-specific speech commands generated by converting ego-motion descriptions into synthetic audios. Further, we compose emotional speech-trajectory pairs into a multimodal Chain-of-Thought (CoT) for fine-tuning a Multimodal Large Model (MLM) based on Qwen2.5-Omni. Specifically, we synthesize the audio-augmented dataset with different emotion types paired with corresponding driving behaviors, leveraging the emotional cues embedded in tone, pitch, and speech tempo to reflect varying user states, such as urgent or hesitant intentions, thus enabling our EchoVLA to interpret not only the semantic content but also the emotional context of audio commands for more nuanced and emotionally adaptive driving behavior. In open-loop benchmarks, our approach reduces the average L2 error by $59.4\\%$ and the collision rate by $74.4\\%$ compared to the baseline of vision-only perception. More experiments on nuScenes dataset validate that EchoVLA not only steers the trajectory through audio instructions, but also modulates driving behavior in response to the emotions detected in the user&#39;s speech."
    }
  ],
  "智能人机交互实验室 (MemX)": [
    {
      "title": "AdaReasoner: Dynamic Tool Orchestration for Iterative Visual Reasoning",
      "url": "https://arxiv.org/abs/2601.18631",
      "date": "2026-01-28",
      "authors_text": "Mingyang Song, Haoyu Sun, Jiawei Gu, Linjie Li, Luxin Xu",
      "is_highlight": false,
      "score": 70.0,
      "summary": "AdaReasoner enhances visual reasoning in multimodal large language models by dynamically orchestrating tool use through a scalable data pipeline and reinforcement learning, achieving state-of-the-art performance.",
      "teaser_image": "https://arxiv.org/html/2601.18631/x2.png",
      "debug_abstract": "When humans face problems beyond their immediate capabilities, they rely on tools, providing a promising paradigm for improving visual reasoning in multimodal large language models (MLLMs). Effective reasoning, therefore, hinges on knowing which tools to use, when to invoke them, and how to compose them over multiple steps, even when faced with new tools or new tasks. We introduce \\textbf{AdaReasoner}, a family of multimodal models that learn tool use as a general reasoning skill rather than as tool-specific or explicitly supervised behavior. AdaReasoner is enabled by (i) a scalable data curation pipeline exposing models to long-horizon, multi-step tool interactions; (ii) Tool-GRPO, a reinforcement learning algorithm that optimizes tool selection and sequencing based on end-task success; and (iii) an adaptive learning mechanism that dynamically regulates tool usage. Together, these components allow models to infer tool utility from task context and intermediate outcomes, enabling coordination of multiple tools and generalization to unseen tools. Empirically, AdaReasoner exhibits strong tool-adaptive and generalization behaviors: it autonomously adopts beneficial tools, suppresses irrelevant ones, and adjusts tool usage frequency based on task demands, despite never being explicitly trained to do so. These capabilities translate into state-of-the-art performance across challenging benchmarks, improving the 7B base model by +24.9\\% on average and surpassing strong proprietary systems such as GPT-5 on multiple tasks, including VSP and Jigsaw."
    },
    {
      "title": "GenAgent: Scaling Text-to-Image Generation via Agentic Multimodal Reasoning",
      "url": "https://arxiv.org/abs/2601.18543",
      "date": "2026-01-28",
      "authors_text": "Kaixun Jiang, Yuzheng Wang, Junjie Zhou, Pandeng Li, Zhihang Liu",
      "is_highlight": false,
      "score": 75.0,
      "summary": "GenAgent enhances text-to-image generation by decoupling understanding and generation through an agentic multimodal framework, enabling autonomous interactions and improved performance across tasks.",
      "teaser_image": "https://arxiv.org/html/2601.18543/x2.png",
      "debug_abstract": "We introduce GenAgent, unifying visual understanding and generation through an agentic multimodal model. Unlike unified models that face expensive training costs and understanding-generation trade-offs, GenAgent decouples these capabilities through an agentic framework: understanding is handled by the multimodal model itself, while generation is achieved by treating image generation models as invokable tools. Crucially, unlike existing modular systems constrained by static pipelines, this design enables autonomous multi-turn interactions where the agent generates multimodal chains-of-thought encompassing reasoning, tool invocation, judgment, and reflection to iteratively refine outputs. We employ a two-stage training strategy: first, cold-start with supervised fine-tuning on high-quality tool invocation and reflection data to bootstrap agent behaviors; second, end-to-end agentic reinforcement learning combining pointwise rewards (final image quality) and pairwise rewards (reflection accuracy), with trajectory resampling for enhanced multi-turn exploration. GenAgent significantly boosts base generator(FLUX.1-dev) performance on GenEval++ (+23.6\\%) and WISE (+14\\%). Beyond performance gains, our framework demonstrates three key properties: 1) cross-tool generalization to generators with varying capabilities, 2) test-time scaling with consistent improvements across interaction rounds, and 3) task-adaptive reasoning that automatically adjusts to different tasks. Our code will be available at \\href{this https URL}{this url}."
    }
  ],
  "集群机器人系统实验室 (MagicLab)": [
    {
      "title": "AdaReasoner: Dynamic Tool Orchestration for Iterative Visual Reasoning",
      "url": "https://arxiv.org/abs/2601.18631",
      "date": "2026-01-28",
      "authors_text": "Mingyang Song, Haoyu Sun, Jiawei Gu, Linjie Li, Luxin Xu",
      "is_highlight": false,
      "score": 70.0,
      "summary": "AdaReasoner enhances visual reasoning in multimodal large language models by dynamically orchestrating tool use through a scalable data pipeline and reinforcement learning, achieving state-of-the-art performance.",
      "teaser_image": "https://arxiv.org/html/2601.18631/x2.png",
      "debug_abstract": "When humans face problems beyond their immediate capabilities, they rely on tools, providing a promising paradigm for improving visual reasoning in multimodal large language models (MLLMs). Effective reasoning, therefore, hinges on knowing which tools to use, when to invoke them, and how to compose them over multiple steps, even when faced with new tools or new tasks. We introduce \\textbf{AdaReasoner}, a family of multimodal models that learn tool use as a general reasoning skill rather than as tool-specific or explicitly supervised behavior. AdaReasoner is enabled by (i) a scalable data curation pipeline exposing models to long-horizon, multi-step tool interactions; (ii) Tool-GRPO, a reinforcement learning algorithm that optimizes tool selection and sequencing based on end-task success; and (iii) an adaptive learning mechanism that dynamically regulates tool usage. Together, these components allow models to infer tool utility from task context and intermediate outcomes, enabling coordination of multiple tools and generalization to unseen tools. Empirically, AdaReasoner exhibits strong tool-adaptive and generalization behaviors: it autonomously adopts beneficial tools, suppresses irrelevant ones, and adjusts tool usage frequency based on task demands, despite never being explicitly trained to do so. These capabilities translate into state-of-the-art performance across challenging benchmarks, improving the 7B base model by +24.9\\% on average and surpassing strong proprietary systems such as GPT-5 on multiple tasks, including VSP and Jigsaw."
    },
    {
      "title": "GenAgent: Scaling Text-to-Image Generation via Agentic Multimodal Reasoning",
      "url": "https://arxiv.org/abs/2601.18543",
      "date": "2026-01-28",
      "authors_text": "Kaixun Jiang, Yuzheng Wang, Junjie Zhou, Pandeng Li, Zhihang Liu",
      "is_highlight": false,
      "score": 75.0,
      "summary": "GenAgent enhances text-to-image generation by decoupling understanding and generation through an agentic multimodal framework, enabling autonomous interactions and improved performance across tasks.",
      "teaser_image": "https://arxiv.org/html/2601.18543/x2.png",
      "debug_abstract": "We introduce GenAgent, unifying visual understanding and generation through an agentic multimodal model. Unlike unified models that face expensive training costs and understanding-generation trade-offs, GenAgent decouples these capabilities through an agentic framework: understanding is handled by the multimodal model itself, while generation is achieved by treating image generation models as invokable tools. Crucially, unlike existing modular systems constrained by static pipelines, this design enables autonomous multi-turn interactions where the agent generates multimodal chains-of-thought encompassing reasoning, tool invocation, judgment, and reflection to iteratively refine outputs. We employ a two-stage training strategy: first, cold-start with supervised fine-tuning on high-quality tool invocation and reflection data to bootstrap agent behaviors; second, end-to-end agentic reinforcement learning combining pointwise rewards (final image quality) and pairwise rewards (reflection accuracy), with trajectory resampling for enhanced multi-turn exploration. GenAgent significantly boosts base generator(FLUX.1-dev) performance on GenEval++ (+23.6\\%) and WISE (+14\\%). Beyond performance gains, our framework demonstrates three key properties: 1) cross-tool generalization to generators with varying capabilities, 2) test-time scaling with consistent improvements across interaction rounds, and 3) task-adaptive reasoning that automatically adjusts to different tasks. Our code will be available at \\href{this https URL}{this url}."
    }
  ],
  "中国科学技术大学": [
    {
      "title": "TouchGuide: Inference-Time Steering of Visuomotor Policies via Touch Guidance",
      "url": "https://arxiv.org/abs/2601.20239",
      "date": "2026-01-28",
      "authors_text": "Zhemeng Zhang, Jiahua Ma, Xincheng Yang, Xin Wen, Yuzhi Zhang",
      "is_highlight": false,
      "score": 90.0,
      "summary": "TouchGuide enhances robot manipulation by integrating tactile feedback with visual inputs to refine actions during inference, outperforming existing visuo-tactile policies in complex tasks.",
      "teaser_image": "https://arxiv.org/html/2601.20239/x2.png",
      "debug_abstract": "Fine-grained and contact-rich manipulation remain challenging for robots, largely due to the underutilization of tactile feedback. To address this, we introduce TouchGuide, a novel cross-policy visuo-tactile fusion paradigm that fuses modalities within a low-dimensional action space. Specifically, TouchGuide operates in two stages to guide a pre-trained diffusion or flow-matching visuomotor policy at inference time. First, the policy produces a coarse, visually-plausible action using only visual inputs during early sampling. Second, a task-specific Contact Physical Model (CPM) provides tactile guidance to steer and refine the action, ensuring it aligns with realistic physical contact conditions. Trained through contrastive learning on limited expert demonstrations, the CPM provides a tactile-informed feasibility score to steer the sampling process toward refined actions that satisfy physical contact constraints. Furthermore, to facilitate TouchGuide training with high-quality and cost-effective data, we introduce TacUMI, a data collection system. TacUMI achieves a favorable trade-off between precision and affordability; by leveraging rigid fingertips, it obtains direct tactile feedback, thereby enabling the collection of reliable tactile data. Extensive experiments on five challenging contact-rich tasks, such as shoe lacing and chip handover, show that TouchGuide consistently and significantly outperforms state-of-the-art visuo-tactile policies."
    },
    {
      "title": "GenAgent: Scaling Text-to-Image Generation via Agentic Multimodal Reasoning",
      "url": "https://arxiv.org/abs/2601.18543",
      "date": "2026-01-28",
      "authors_text": "Kaixun Jiang, Yuzheng Wang, Junjie Zhou, Pandeng Li, Zhihang Liu",
      "is_highlight": false,
      "score": 75.0,
      "summary": "GenAgent enhances text-to-image generation by decoupling understanding and generation through an agentic multimodal framework, enabling autonomous interactions and improved performance across tasks.",
      "teaser_image": "https://arxiv.org/html/2601.18543/x2.png",
      "debug_abstract": "We introduce GenAgent, unifying visual understanding and generation through an agentic multimodal model. Unlike unified models that face expensive training costs and understanding-generation trade-offs, GenAgent decouples these capabilities through an agentic framework: understanding is handled by the multimodal model itself, while generation is achieved by treating image generation models as invokable tools. Crucially, unlike existing modular systems constrained by static pipelines, this design enables autonomous multi-turn interactions where the agent generates multimodal chains-of-thought encompassing reasoning, tool invocation, judgment, and reflection to iteratively refine outputs. We employ a two-stage training strategy: first, cold-start with supervised fine-tuning on high-quality tool invocation and reflection data to bootstrap agent behaviors; second, end-to-end agentic reinforcement learning combining pointwise rewards (final image quality) and pairwise rewards (reflection accuracy), with trajectory resampling for enhanced multi-turn exploration. GenAgent significantly boosts base generator(FLUX.1-dev) performance on GenEval++ (+23.6\\%) and WISE (+14\\%). Beyond performance gains, our framework demonstrates three key properties: 1) cross-tool generalization to generators with varying capabilities, 2) test-time scaling with consistent improvements across interaction rounds, and 3) task-adaptive reasoning that automatically adjusts to different tasks. Our code will be available at \\href{this https URL}{this url}."
    }
  ],
  "智能感知与无人系统实验室": [
    {
      "title": "AdaReasoner: Dynamic Tool Orchestration for Iterative Visual Reasoning",
      "url": "https://arxiv.org/abs/2601.18631",
      "date": "2026-01-28",
      "authors_text": "Mingyang Song, Haoyu Sun, Jiawei Gu, Linjie Li, Luxin Xu",
      "is_highlight": false,
      "score": 70.0,
      "summary": "AdaReasoner enhances visual reasoning in multimodal large language models by dynamically orchestrating tool use through a scalable data pipeline and reinforcement learning, achieving state-of-the-art performance.",
      "teaser_image": "https://arxiv.org/html/2601.18631/x2.png",
      "debug_abstract": "When humans face problems beyond their immediate capabilities, they rely on tools, providing a promising paradigm for improving visual reasoning in multimodal large language models (MLLMs). Effective reasoning, therefore, hinges on knowing which tools to use, when to invoke them, and how to compose them over multiple steps, even when faced with new tools or new tasks. We introduce \\textbf{AdaReasoner}, a family of multimodal models that learn tool use as a general reasoning skill rather than as tool-specific or explicitly supervised behavior. AdaReasoner is enabled by (i) a scalable data curation pipeline exposing models to long-horizon, multi-step tool interactions; (ii) Tool-GRPO, a reinforcement learning algorithm that optimizes tool selection and sequencing based on end-task success; and (iii) an adaptive learning mechanism that dynamically regulates tool usage. Together, these components allow models to infer tool utility from task context and intermediate outcomes, enabling coordination of multiple tools and generalization to unseen tools. Empirically, AdaReasoner exhibits strong tool-adaptive and generalization behaviors: it autonomously adopts beneficial tools, suppresses irrelevant ones, and adjusts tool usage frequency based on task demands, despite never being explicitly trained to do so. These capabilities translate into state-of-the-art performance across challenging benchmarks, improving the 7B base model by +24.9\\% on average and surpassing strong proprietary systems such as GPT-5 on multiple tasks, including VSP and Jigsaw."
    },
    {
      "title": "GenAgent: Scaling Text-to-Image Generation via Agentic Multimodal Reasoning",
      "url": "https://arxiv.org/abs/2601.18543",
      "date": "2026-01-28",
      "authors_text": "Kaixun Jiang, Yuzheng Wang, Junjie Zhou, Pandeng Li, Zhihang Liu",
      "is_highlight": false,
      "score": 75.0,
      "summary": "GenAgent enhances text-to-image generation by decoupling understanding and generation through an agentic multimodal framework, enabling autonomous interactions and improved performance across tasks.",
      "teaser_image": "https://arxiv.org/html/2601.18543/x2.png",
      "debug_abstract": "We introduce GenAgent, unifying visual understanding and generation through an agentic multimodal model. Unlike unified models that face expensive training costs and understanding-generation trade-offs, GenAgent decouples these capabilities through an agentic framework: understanding is handled by the multimodal model itself, while generation is achieved by treating image generation models as invokable tools. Crucially, unlike existing modular systems constrained by static pipelines, this design enables autonomous multi-turn interactions where the agent generates multimodal chains-of-thought encompassing reasoning, tool invocation, judgment, and reflection to iteratively refine outputs. We employ a two-stage training strategy: first, cold-start with supervised fine-tuning on high-quality tool invocation and reflection data to bootstrap agent behaviors; second, end-to-end agentic reinforcement learning combining pointwise rewards (final image quality) and pairwise rewards (reflection accuracy), with trajectory resampling for enhanced multi-turn exploration. GenAgent significantly boosts base generator(FLUX.1-dev) performance on GenEval++ (+23.6\\%) and WISE (+14\\%). Beyond performance gains, our framework demonstrates three key properties: 1) cross-tool generalization to generators with varying capabilities, 2) test-time scaling with consistent improvements across interaction rounds, and 3) task-adaptive reasoning that automatically adjusts to different tasks. Our code will be available at \\href{this https URL}{this url}."
    }
  ],
  "南京大学": [
    {
      "title": "GeoDiff3D: Self-Supervised 3D Scene Generation with Geometry-Constrained 2D Diffusion Guidance",
      "url": "https://arxiv.org/abs/2601.19785",
      "date": "2026-01-28",
      "authors_text": "Haozhi Zhu, Miaomiao Zhao, Dingyao Liu, Runze Tian, Yan Zhang",
      "is_highlight": false,
      "score": 67.0,
      "summary": "GeoDiff3D introduces a self-supervised framework for efficient, high-quality 3D scene generation using geometry-constrained 2D diffusion, reducing reliance on labeled data and enhancing structural coherence.",
      "teaser_image": "https://arxiv.org/html/2601.19785/x1.png",
      "debug_abstract": "3D scene generation is a core technology for gaming, film/VFX, and VR/AR. Growing demand for rapid iteration, high-fidelity detail, and accessible content creation has further increased interest in this area. Existing methods broadly follow two paradigms - indirect 2D-to-3D reconstruction and direct 3D generation - but both are limited by weak structural modeling and heavy reliance on large-scale ground-truth supervision, often producing structural artifacts, geometric inconsistencies, and degraded high-frequency details in complex scenes. We propose GeoDiff3D, an efficient self-supervised framework that uses coarse geometry as a structural anchor and a geometry-constrained 2D diffusion model to provide texture-rich reference images. Importantly, GeoDiff3D does not require strict multi-view consistency of the diffusion-generated references and remains robust to the resulting noisy, inconsistent guidance. We further introduce voxel-aligned 3D feature aggregation and dual self-supervision to maintain scene coherence and fine details while substantially reducing dependence on labeled data. GeoDiff3D also trains with low computational cost and enables fast, high-quality 3D scene generation. Extensive experiments on challenging scenes show improved generalization and generation quality over existing baselines, offering a practical solution for accessible and efficient 3D scene construction."
    },
    {
      "title": "GenAgent: Scaling Text-to-Image Generation via Agentic Multimodal Reasoning",
      "url": "https://arxiv.org/abs/2601.18543",
      "date": "2026-01-28",
      "authors_text": "Kaixun Jiang, Yuzheng Wang, Junjie Zhou, Pandeng Li, Zhihang Liu",
      "is_highlight": false,
      "score": 75.0,
      "summary": "GenAgent enhances text-to-image generation by decoupling understanding and generation through an agentic multimodal framework, enabling autonomous interactions and improved performance across tasks.",
      "teaser_image": "https://arxiv.org/html/2601.18543/x2.png",
      "debug_abstract": "We introduce GenAgent, unifying visual understanding and generation through an agentic multimodal model. Unlike unified models that face expensive training costs and understanding-generation trade-offs, GenAgent decouples these capabilities through an agentic framework: understanding is handled by the multimodal model itself, while generation is achieved by treating image generation models as invokable tools. Crucially, unlike existing modular systems constrained by static pipelines, this design enables autonomous multi-turn interactions where the agent generates multimodal chains-of-thought encompassing reasoning, tool invocation, judgment, and reflection to iteratively refine outputs. We employ a two-stage training strategy: first, cold-start with supervised fine-tuning on high-quality tool invocation and reflection data to bootstrap agent behaviors; second, end-to-end agentic reinforcement learning combining pointwise rewards (final image quality) and pairwise rewards (reflection accuracy), with trajectory resampling for enhanced multi-turn exploration. GenAgent significantly boosts base generator(FLUX.1-dev) performance on GenEval++ (+23.6\\%) and WISE (+14\\%). Beyond performance gains, our framework demonstrates three key properties: 1) cross-tool generalization to generators with varying capabilities, 2) test-time scaling with consistent improvements across interaction rounds, and 3) task-adaptive reasoning that automatically adjusts to different tasks. Our code will be available at \\href{this https URL}{this url}."
    }
  ],
  "Synteraction Lab": [
    {
      "title": "GPO: Growing Policy Optimization for Legged Robot Locomotion and Whole-Body Control",
      "url": "https://arxiv.org/abs/2601.20668",
      "date": "2026-01-28",
      "authors_text": "Shuhao Liao, Peizhuo Li, Xinrong Yang, Linnan Chang, Zhaoxin Fan",
      "is_highlight": false,
      "score": 92.0,
      "summary": "The paper presents Growing Policy Optimization (GPO), a framework that enhances reinforcement learning for legged robots by progressively expanding the action space to improve data collection and policy performance.",
      "teaser_image": "https://arxiv.org/html/2601.20668/pic/framework.png",
      "debug_abstract": "Training reinforcement learning (RL) policies for legged robots remains challenging due to high-dimensional continuous actions, hardware constraints, and limited exploration. Existing methods for locomotion and whole-body control work well for position-based control with environment-specific heuristics (e.g., reward shaping, curriculum design, and manual initialization), but are less effective for torque-based control, where sufficiently exploring the action space and obtaining informative gradient signals for training is significantly more difficult. We introduce Growing Policy Optimization (GPO), a training framework that applies a time-varying action transformation to restrict the effective action space in the early stage, thereby encouraging more effective data collection and policy learning, and then progressively expands it to enhance exploration and achieve higher expected return. We prove that this transformation preserves the PPO update rule and introduces only bounded, vanishing gradient distortion, thereby ensuring stable training. We evaluate GPO on both quadruped and hexapod robots, including zero-shot deployment of simulation-trained policies on hardware. Policies trained with GPO consistently achieve better performance. These results suggest that GPO provides a general, environment-agnostic optimization framework for learning legged locomotion."
    },
    {
      "title": "AdaReasoner: Dynamic Tool Orchestration for Iterative Visual Reasoning",
      "url": "https://arxiv.org/abs/2601.18631",
      "date": "2026-01-28",
      "authors_text": "Mingyang Song, Haoyu Sun, Jiawei Gu, Linjie Li, Luxin Xu",
      "is_highlight": false,
      "score": 70.0,
      "summary": "AdaReasoner enhances visual reasoning in multimodal large language models by dynamically orchestrating tool use through a scalable data pipeline and reinforcement learning, achieving state-of-the-art performance.",
      "teaser_image": "https://arxiv.org/html/2601.18631/x2.png",
      "debug_abstract": "When humans face problems beyond their immediate capabilities, they rely on tools, providing a promising paradigm for improving visual reasoning in multimodal large language models (MLLMs). Effective reasoning, therefore, hinges on knowing which tools to use, when to invoke them, and how to compose them over multiple steps, even when faced with new tools or new tasks. We introduce \\textbf{AdaReasoner}, a family of multimodal models that learn tool use as a general reasoning skill rather than as tool-specific or explicitly supervised behavior. AdaReasoner is enabled by (i) a scalable data curation pipeline exposing models to long-horizon, multi-step tool interactions; (ii) Tool-GRPO, a reinforcement learning algorithm that optimizes tool selection and sequencing based on end-task success; and (iii) an adaptive learning mechanism that dynamically regulates tool usage. Together, these components allow models to infer tool utility from task context and intermediate outcomes, enabling coordination of multiple tools and generalization to unseen tools. Empirically, AdaReasoner exhibits strong tool-adaptive and generalization behaviors: it autonomously adopts beneficial tools, suppresses irrelevant ones, and adjusts tool usage frequency based on task demands, despite never being explicitly trained to do so. These capabilities translate into state-of-the-art performance across challenging benchmarks, improving the 7B base model by +24.9\\% on average and surpassing strong proprietary systems such as GPT-5 on multiple tasks, including VSP and Jigsaw."
    }
  ],
  "Multimedia Lab (MMLab)": [
    {
      "title": "AdaReasoner: Dynamic Tool Orchestration for Iterative Visual Reasoning",
      "url": "https://arxiv.org/abs/2601.18631",
      "date": "2026-01-28",
      "authors_text": "Mingyang Song, Haoyu Sun, Jiawei Gu, Linjie Li, Luxin Xu",
      "is_highlight": false,
      "score": 70.0,
      "summary": "AdaReasoner enhances visual reasoning in multimodal large language models by dynamically orchestrating tool use through a scalable data pipeline and reinforcement learning, achieving state-of-the-art performance.",
      "teaser_image": "https://arxiv.org/html/2601.18631/x2.png",
      "debug_abstract": "When humans face problems beyond their immediate capabilities, they rely on tools, providing a promising paradigm for improving visual reasoning in multimodal large language models (MLLMs). Effective reasoning, therefore, hinges on knowing which tools to use, when to invoke them, and how to compose them over multiple steps, even when faced with new tools or new tasks. We introduce \\textbf{AdaReasoner}, a family of multimodal models that learn tool use as a general reasoning skill rather than as tool-specific or explicitly supervised behavior. AdaReasoner is enabled by (i) a scalable data curation pipeline exposing models to long-horizon, multi-step tool interactions; (ii) Tool-GRPO, a reinforcement learning algorithm that optimizes tool selection and sequencing based on end-task success; and (iii) an adaptive learning mechanism that dynamically regulates tool usage. Together, these components allow models to infer tool utility from task context and intermediate outcomes, enabling coordination of multiple tools and generalization to unseen tools. Empirically, AdaReasoner exhibits strong tool-adaptive and generalization behaviors: it autonomously adopts beneficial tools, suppresses irrelevant ones, and adjusts tool usage frequency based on task demands, despite never being explicitly trained to do so. These capabilities translate into state-of-the-art performance across challenging benchmarks, improving the 7B base model by +24.9\\% on average and surpassing strong proprietary systems such as GPT-5 on multiple tasks, including VSP and Jigsaw."
    }
  ],
  "深圳市人工智能与机器人研究院 (AIRS)": [
    {
      "title": "AdaReasoner: Dynamic Tool Orchestration for Iterative Visual Reasoning",
      "url": "https://arxiv.org/abs/2601.18631",
      "date": "2026-01-28",
      "authors_text": "Mingyang Song, Haoyu Sun, Jiawei Gu, Linjie Li, Luxin Xu",
      "is_highlight": false,
      "score": 70.0,
      "summary": "AdaReasoner enhances visual reasoning in multimodal large language models by dynamically orchestrating tool use through a scalable data pipeline and reinforcement learning, achieving state-of-the-art performance.",
      "teaser_image": "https://arxiv.org/html/2601.18631/x2.png",
      "debug_abstract": "When humans face problems beyond their immediate capabilities, they rely on tools, providing a promising paradigm for improving visual reasoning in multimodal large language models (MLLMs). Effective reasoning, therefore, hinges on knowing which tools to use, when to invoke them, and how to compose them over multiple steps, even when faced with new tools or new tasks. We introduce \\textbf{AdaReasoner}, a family of multimodal models that learn tool use as a general reasoning skill rather than as tool-specific or explicitly supervised behavior. AdaReasoner is enabled by (i) a scalable data curation pipeline exposing models to long-horizon, multi-step tool interactions; (ii) Tool-GRPO, a reinforcement learning algorithm that optimizes tool selection and sequencing based on end-task success; and (iii) an adaptive learning mechanism that dynamically regulates tool usage. Together, these components allow models to infer tool utility from task context and intermediate outcomes, enabling coordination of multiple tools and generalization to unseen tools. Empirically, AdaReasoner exhibits strong tool-adaptive and generalization behaviors: it autonomously adopts beneficial tools, suppresses irrelevant ones, and adjusts tool usage frequency based on task demands, despite never being explicitly trained to do so. These capabilities translate into state-of-the-art performance across challenging benchmarks, improving the 7B base model by +24.9\\% on average and surpassing strong proprietary systems such as GPT-5 on multiple tasks, including VSP and Jigsaw."
    }
  ],
  "电子科技大学": [
    {
      "title": "R^3: Replay, Reflection, and Ranking Rewards for LLM Reinforcement Learning",
      "url": "https://arxiv.org/abs/2601.19620",
      "date": "2026-01-28",
      "authors_text": "Zhizheng Jiang, Kang Zhao, Weikai Xu, Xinkui Lin, Wei Liu",
      "is_highlight": false,
      "score": 72.0,
      "summary": "The R^3 method enhances reinforcement learning for large reasoning models by integrating cross-context replay, in-context self-reflection, and structural entropy ranking, achieving state-of-the-art performance in math tasks.",
      "teaser_image": "https://arxiv.org/html/2601.19620/x1.png",
      "debug_abstract": "Large reasoning models (LRMs) aim to solve diverse and complex problems through structured reasoning. Recent advances in group-based policy optimization methods have shown promise in enabling stable advantage estimation without reliance on process-level annotations. However, these methods rely on advantage gaps induced by high-quality samples within the same batch, which makes the training process fragile and inefficient when intra-group advantages collapse under challenging tasks. To address these problems, we propose a reinforcement learning mechanism named \\emph{\\textbf{R^3}} that along three directions: (1) a \\emph{cross-context \\underline{\\textbf{R}}eplay} strategy that maintains the intra-group advantage by recalling valuable examples from historical trajectories of the same query, (2) an \\emph{in-context self-\\underline{\\textbf{R}}eflection} mechanism enabling models to refine outputs by leveraging past failures, and (3) a \\emph{structural entropy \\underline{\\textbf{R}}anking reward}, which assigns relative rewards to truncated or failed samples by ranking responses based on token-level entropy patterns, capturing both local exploration and global stability. We implement our method on Deepseek-R1-Distill-Qwen-1.5B and train it on the DeepscaleR-40k in the math domain. Experiments demonstrate our method achieves SoTA performance on several math benchmarks, representing significant improvements and fewer reasoning tokens over the base models. Code and model will be released."
    },
    {
      "title": "AdaReasoner: Dynamic Tool Orchestration for Iterative Visual Reasoning",
      "url": "https://arxiv.org/abs/2601.18631",
      "date": "2026-01-28",
      "authors_text": "Mingyang Song, Haoyu Sun, Jiawei Gu, Linjie Li, Luxin Xu",
      "is_highlight": false,
      "score": 70.0,
      "summary": "AdaReasoner enhances visual reasoning in multimodal large language models by dynamically orchestrating tool use through a scalable data pipeline and reinforcement learning, achieving state-of-the-art performance.",
      "teaser_image": "https://arxiv.org/html/2601.18631/x2.png",
      "debug_abstract": "When humans face problems beyond their immediate capabilities, they rely on tools, providing a promising paradigm for improving visual reasoning in multimodal large language models (MLLMs). Effective reasoning, therefore, hinges on knowing which tools to use, when to invoke them, and how to compose them over multiple steps, even when faced with new tools or new tasks. We introduce \\textbf{AdaReasoner}, a family of multimodal models that learn tool use as a general reasoning skill rather than as tool-specific or explicitly supervised behavior. AdaReasoner is enabled by (i) a scalable data curation pipeline exposing models to long-horizon, multi-step tool interactions; (ii) Tool-GRPO, a reinforcement learning algorithm that optimizes tool selection and sequencing based on end-task success; and (iii) an adaptive learning mechanism that dynamically regulates tool usage. Together, these components allow models to infer tool utility from task context and intermediate outcomes, enabling coordination of multiple tools and generalization to unseen tools. Empirically, AdaReasoner exhibits strong tool-adaptive and generalization behaviors: it autonomously adopts beneficial tools, suppresses irrelevant ones, and adjusts tool usage frequency based on task demands, despite never being explicitly trained to do so. These capabilities translate into state-of-the-art performance across challenging benchmarks, improving the 7B base model by +24.9\\% on average and surpassing strong proprietary systems such as GPT-5 on multiple tasks, including VSP and Jigsaw."
    }
  ],
  "Advanced Robotics Centre": [
    {
      "title": "GPO: Growing Policy Optimization for Legged Robot Locomotion and Whole-Body Control",
      "url": "https://arxiv.org/abs/2601.20668",
      "date": "2026-01-28",
      "authors_text": "Shuhao Liao, Peizhuo Li, Xinrong Yang, Linnan Chang, Zhaoxin Fan",
      "is_highlight": false,
      "score": 92.0,
      "summary": "The paper presents Growing Policy Optimization (GPO), a framework that enhances reinforcement learning for legged robots by progressively expanding the action space to improve data collection and policy performance.",
      "teaser_image": "https://arxiv.org/html/2601.20668/pic/framework.png",
      "debug_abstract": "Training reinforcement learning (RL) policies for legged robots remains challenging due to high-dimensional continuous actions, hardware constraints, and limited exploration. Existing methods for locomotion and whole-body control work well for position-based control with environment-specific heuristics (e.g., reward shaping, curriculum design, and manual initialization), but are less effective for torque-based control, where sufficiently exploring the action space and obtaining informative gradient signals for training is significantly more difficult. We introduce Growing Policy Optimization (GPO), a training framework that applies a time-varying action transformation to restrict the effective action space in the early stage, thereby encouraging more effective data collection and policy learning, and then progressively expands it to enhance exploration and achieve higher expected return. We prove that this transformation preserves the PPO update rule and introduces only bounded, vanishing gradient distortion, thereby ensuring stable training. We evaluate GPO on both quadruped and hexapod robots, including zero-shot deployment of simulation-trained policies on hardware. Policies trained with GPO consistently achieve better performance. These results suggest that GPO provides a general, environment-agnostic optimization framework for learning legged locomotion."
    },
    {
      "title": "AdaReasoner: Dynamic Tool Orchestration for Iterative Visual Reasoning",
      "url": "https://arxiv.org/abs/2601.18631",
      "date": "2026-01-28",
      "authors_text": "Mingyang Song, Haoyu Sun, Jiawei Gu, Linjie Li, Luxin Xu",
      "is_highlight": false,
      "score": 70.0,
      "summary": "AdaReasoner enhances visual reasoning in multimodal large language models by dynamically orchestrating tool use through a scalable data pipeline and reinforcement learning, achieving state-of-the-art performance.",
      "teaser_image": "https://arxiv.org/html/2601.18631/x2.png",
      "debug_abstract": "When humans face problems beyond their immediate capabilities, they rely on tools, providing a promising paradigm for improving visual reasoning in multimodal large language models (MLLMs). Effective reasoning, therefore, hinges on knowing which tools to use, when to invoke them, and how to compose them over multiple steps, even when faced with new tools or new tasks. We introduce \\textbf{AdaReasoner}, a family of multimodal models that learn tool use as a general reasoning skill rather than as tool-specific or explicitly supervised behavior. AdaReasoner is enabled by (i) a scalable data curation pipeline exposing models to long-horizon, multi-step tool interactions; (ii) Tool-GRPO, a reinforcement learning algorithm that optimizes tool selection and sequencing based on end-task success; and (iii) an adaptive learning mechanism that dynamically regulates tool usage. Together, these components allow models to infer tool utility from task context and intermediate outcomes, enabling coordination of multiple tools and generalization to unseen tools. Empirically, AdaReasoner exhibits strong tool-adaptive and generalization behaviors: it autonomously adopts beneficial tools, suppresses irrelevant ones, and adjusts tool usage frequency based on task demands, despite never being explicitly trained to do so. These capabilities translate into state-of-the-art performance across challenging benchmarks, improving the 7B base model by +24.9\\% on average and surpassing strong proprietary systems such as GPT-5 on multiple tasks, including VSP and Jigsaw."
    }
  ],
  "机器人与人工智能实验室 (RAIL)": [
    {
      "title": "AdaReasoner: Dynamic Tool Orchestration for Iterative Visual Reasoning",
      "url": "https://arxiv.org/abs/2601.18631",
      "date": "2026-01-28",
      "authors_text": "Mingyang Song, Haoyu Sun, Jiawei Gu, Linjie Li, Luxin Xu",
      "is_highlight": false,
      "score": 70.0,
      "summary": "AdaReasoner enhances visual reasoning in multimodal large language models by dynamically orchestrating tool use through a scalable data pipeline and reinforcement learning, achieving state-of-the-art performance.",
      "teaser_image": "https://arxiv.org/html/2601.18631/x2.png",
      "debug_abstract": "When humans face problems beyond their immediate capabilities, they rely on tools, providing a promising paradigm for improving visual reasoning in multimodal large language models (MLLMs). Effective reasoning, therefore, hinges on knowing which tools to use, when to invoke them, and how to compose them over multiple steps, even when faced with new tools or new tasks. We introduce \\textbf{AdaReasoner}, a family of multimodal models that learn tool use as a general reasoning skill rather than as tool-specific or explicitly supervised behavior. AdaReasoner is enabled by (i) a scalable data curation pipeline exposing models to long-horizon, multi-step tool interactions; (ii) Tool-GRPO, a reinforcement learning algorithm that optimizes tool selection and sequencing based on end-task success; and (iii) an adaptive learning mechanism that dynamically regulates tool usage. Together, these components allow models to infer tool utility from task context and intermediate outcomes, enabling coordination of multiple tools and generalization to unseen tools. Empirically, AdaReasoner exhibits strong tool-adaptive and generalization behaviors: it autonomously adopts beneficial tools, suppresses irrelevant ones, and adjusts tool usage frequency based on task demands, despite never being explicitly trained to do so. These capabilities translate into state-of-the-art performance across challenging benchmarks, improving the 7B base model by +24.9\\% on average and surpassing strong proprietary systems such as GPT-5 on multiple tasks, including VSP and Jigsaw."
    }
  ],
  "Microsystem Engineering and Robotics": [
    {
      "title": "GPO: Growing Policy Optimization for Legged Robot Locomotion and Whole-Body Control",
      "url": "https://arxiv.org/abs/2601.20668",
      "date": "2026-01-28",
      "authors_text": "Shuhao Liao, Peizhuo Li, Xinrong Yang, Linnan Chang, Zhaoxin Fan",
      "is_highlight": false,
      "score": 92.0,
      "summary": "The paper presents Growing Policy Optimization (GPO), a framework that enhances reinforcement learning for legged robots by progressively expanding the action space to improve data collection and policy performance.",
      "teaser_image": "https://arxiv.org/html/2601.20668/pic/framework.png",
      "debug_abstract": "Training reinforcement learning (RL) policies for legged robots remains challenging due to high-dimensional continuous actions, hardware constraints, and limited exploration. Existing methods for locomotion and whole-body control work well for position-based control with environment-specific heuristics (e.g., reward shaping, curriculum design, and manual initialization), but are less effective for torque-based control, where sufficiently exploring the action space and obtaining informative gradient signals for training is significantly more difficult. We introduce Growing Policy Optimization (GPO), a training framework that applies a time-varying action transformation to restrict the effective action space in the early stage, thereby encouraging more effective data collection and policy learning, and then progressively expands it to enhance exploration and achieve higher expected return. We prove that this transformation preserves the PPO update rule and introduces only bounded, vanishing gradient distortion, thereby ensuring stable training. We evaluate GPO on both quadruped and hexapod robots, including zero-shot deployment of simulation-trained policies on hardware. Policies trained with GPO consistently achieve better performance. These results suggest that GPO provides a general, environment-agnostic optimization framework for learning legged locomotion."
    },
    {
      "title": "AdaReasoner: Dynamic Tool Orchestration for Iterative Visual Reasoning",
      "url": "https://arxiv.org/abs/2601.18631",
      "date": "2026-01-28",
      "authors_text": "Mingyang Song, Haoyu Sun, Jiawei Gu, Linjie Li, Luxin Xu",
      "is_highlight": false,
      "score": 70.0,
      "summary": "AdaReasoner enhances visual reasoning in multimodal large language models by dynamically orchestrating tool use through a scalable data pipeline and reinforcement learning, achieving state-of-the-art performance.",
      "teaser_image": "https://arxiv.org/html/2601.18631/x2.png",
      "debug_abstract": "When humans face problems beyond their immediate capabilities, they rely on tools, providing a promising paradigm for improving visual reasoning in multimodal large language models (MLLMs). Effective reasoning, therefore, hinges on knowing which tools to use, when to invoke them, and how to compose them over multiple steps, even when faced with new tools or new tasks. We introduce \\textbf{AdaReasoner}, a family of multimodal models that learn tool use as a general reasoning skill rather than as tool-specific or explicitly supervised behavior. AdaReasoner is enabled by (i) a scalable data curation pipeline exposing models to long-horizon, multi-step tool interactions; (ii) Tool-GRPO, a reinforcement learning algorithm that optimizes tool selection and sequencing based on end-task success; and (iii) an adaptive learning mechanism that dynamically regulates tool usage. Together, these components allow models to infer tool utility from task context and intermediate outcomes, enabling coordination of multiple tools and generalization to unseen tools. Empirically, AdaReasoner exhibits strong tool-adaptive and generalization behaviors: it autonomously adopts beneficial tools, suppresses irrelevant ones, and adjusts tool usage frequency based on task demands, despite never being explicitly trained to do so. These capabilities translate into state-of-the-art performance across challenging benchmarks, improving the 7B base model by +24.9\\% on average and surpassing strong proprietary systems such as GPT-5 on multiple tasks, including VSP and Jigsaw."
    }
  ],
  "机器人与自动化研究中心": [
    {
      "title": "AdaReasoner: Dynamic Tool Orchestration for Iterative Visual Reasoning",
      "url": "https://arxiv.org/abs/2601.18631",
      "date": "2026-01-28",
      "authors_text": "Mingyang Song, Haoyu Sun, Jiawei Gu, Linjie Li, Luxin Xu",
      "is_highlight": false,
      "score": 70.0,
      "summary": "AdaReasoner enhances visual reasoning in multimodal large language models by dynamically orchestrating tool use through a scalable data pipeline and reinforcement learning, achieving state-of-the-art performance.",
      "teaser_image": "https://arxiv.org/html/2601.18631/x2.png",
      "debug_abstract": "When humans face problems beyond their immediate capabilities, they rely on tools, providing a promising paradigm for improving visual reasoning in multimodal large language models (MLLMs). Effective reasoning, therefore, hinges on knowing which tools to use, when to invoke them, and how to compose them over multiple steps, even when faced with new tools or new tasks. We introduce \\textbf{AdaReasoner}, a family of multimodal models that learn tool use as a general reasoning skill rather than as tool-specific or explicitly supervised behavior. AdaReasoner is enabled by (i) a scalable data curation pipeline exposing models to long-horizon, multi-step tool interactions; (ii) Tool-GRPO, a reinforcement learning algorithm that optimizes tool selection and sequencing based on end-task success; and (iii) an adaptive learning mechanism that dynamically regulates tool usage. Together, these components allow models to infer tool utility from task context and intermediate outcomes, enabling coordination of multiple tools and generalization to unseen tools. Empirically, AdaReasoner exhibits strong tool-adaptive and generalization behaviors: it autonomously adopts beneficial tools, suppresses irrelevant ones, and adjusts tool usage frequency based on task demands, despite never being explicitly trained to do so. These capabilities translate into state-of-the-art performance across challenging benchmarks, improving the 7B base model by +24.9\\% on average and surpassing strong proprietary systems such as GPT-5 on multiple tasks, including VSP and Jigsaw."
    }
  ],
  "机器人与人工智能实验室": [
    {
      "title": "AdaReasoner: Dynamic Tool Orchestration for Iterative Visual Reasoning",
      "url": "https://arxiv.org/abs/2601.18631",
      "date": "2026-01-28",
      "authors_text": "Mingyang Song, Haoyu Sun, Jiawei Gu, Linjie Li, Luxin Xu",
      "is_highlight": false,
      "score": 70.0,
      "summary": "AdaReasoner enhances visual reasoning in multimodal large language models by dynamically orchestrating tool use through a scalable data pipeline and reinforcement learning, achieving state-of-the-art performance.",
      "teaser_image": "https://arxiv.org/html/2601.18631/x2.png",
      "debug_abstract": "When humans face problems beyond their immediate capabilities, they rely on tools, providing a promising paradigm for improving visual reasoning in multimodal large language models (MLLMs). Effective reasoning, therefore, hinges on knowing which tools to use, when to invoke them, and how to compose them over multiple steps, even when faced with new tools or new tasks. We introduce \\textbf{AdaReasoner}, a family of multimodal models that learn tool use as a general reasoning skill rather than as tool-specific or explicitly supervised behavior. AdaReasoner is enabled by (i) a scalable data curation pipeline exposing models to long-horizon, multi-step tool interactions; (ii) Tool-GRPO, a reinforcement learning algorithm that optimizes tool selection and sequencing based on end-task success; and (iii) an adaptive learning mechanism that dynamically regulates tool usage. Together, these components allow models to infer tool utility from task context and intermediate outcomes, enabling coordination of multiple tools and generalization to unseen tools. Empirically, AdaReasoner exhibits strong tool-adaptive and generalization behaviors: it autonomously adopts beneficial tools, suppresses irrelevant ones, and adjusts tool usage frequency based on task demands, despite never being explicitly trained to do so. These capabilities translate into state-of-the-art performance across challenging benchmarks, improving the 7B base model by +24.9\\% on average and surpassing strong proprietary systems such as GPT-5 on multiple tasks, including VSP and Jigsaw."
    }
  ],
  "NUS AI LAB": [
    {
      "title": "GPO: Growing Policy Optimization for Legged Robot Locomotion and Whole-Body Control",
      "url": "https://arxiv.org/abs/2601.20668",
      "date": "2026-01-28",
      "authors_text": "Shuhao Liao, Peizhuo Li, Xinrong Yang, Linnan Chang, Zhaoxin Fan",
      "is_highlight": false,
      "score": 92.0,
      "summary": "The paper presents Growing Policy Optimization (GPO), a framework that enhances reinforcement learning for legged robots by progressively expanding the action space to improve data collection and policy performance.",
      "teaser_image": "https://arxiv.org/html/2601.20668/pic/framework.png",
      "debug_abstract": "Training reinforcement learning (RL) policies for legged robots remains challenging due to high-dimensional continuous actions, hardware constraints, and limited exploration. Existing methods for locomotion and whole-body control work well for position-based control with environment-specific heuristics (e.g., reward shaping, curriculum design, and manual initialization), but are less effective for torque-based control, where sufficiently exploring the action space and obtaining informative gradient signals for training is significantly more difficult. We introduce Growing Policy Optimization (GPO), a training framework that applies a time-varying action transformation to restrict the effective action space in the early stage, thereby encouraging more effective data collection and policy learning, and then progressively expands it to enhance exploration and achieve higher expected return. We prove that this transformation preserves the PPO update rule and introduces only bounded, vanishing gradient distortion, thereby ensuring stable training. We evaluate GPO on both quadruped and hexapod robots, including zero-shot deployment of simulation-trained policies on hardware. Policies trained with GPO consistently achieve better performance. These results suggest that GPO provides a general, environment-agnostic optimization framework for learning legged locomotion."
    },
    {
      "title": "AdaReasoner: Dynamic Tool Orchestration for Iterative Visual Reasoning",
      "url": "https://arxiv.org/abs/2601.18631",
      "date": "2026-01-28",
      "authors_text": "Mingyang Song, Haoyu Sun, Jiawei Gu, Linjie Li, Luxin Xu",
      "is_highlight": false,
      "score": 70.0,
      "summary": "AdaReasoner enhances visual reasoning in multimodal large language models by dynamically orchestrating tool use through a scalable data pipeline and reinforcement learning, achieving state-of-the-art performance.",
      "teaser_image": "https://arxiv.org/html/2601.18631/x2.png",
      "debug_abstract": "When humans face problems beyond their immediate capabilities, they rely on tools, providing a promising paradigm for improving visual reasoning in multimodal large language models (MLLMs). Effective reasoning, therefore, hinges on knowing which tools to use, when to invoke them, and how to compose them over multiple steps, even when faced with new tools or new tasks. We introduce \\textbf{AdaReasoner}, a family of multimodal models that learn tool use as a general reasoning skill rather than as tool-specific or explicitly supervised behavior. AdaReasoner is enabled by (i) a scalable data curation pipeline exposing models to long-horizon, multi-step tool interactions; (ii) Tool-GRPO, a reinforcement learning algorithm that optimizes tool selection and sequencing based on end-task success; and (iii) an adaptive learning mechanism that dynamically regulates tool usage. Together, these components allow models to infer tool utility from task context and intermediate outcomes, enabling coordination of multiple tools and generalization to unseen tools. Empirically, AdaReasoner exhibits strong tool-adaptive and generalization behaviors: it autonomously adopts beneficial tools, suppresses irrelevant ones, and adjusts tool usage frequency based on task demands, despite never being explicitly trained to do so. These capabilities translate into state-of-the-art performance across challenging benchmarks, improving the 7B base model by +24.9\\% on average and surpassing strong proprietary systems such as GPT-5 on multiple tasks, including VSP and Jigsaw."
    }
  ],
  "武汉大学": [
    {
      "title": "Policy of Thoughts: Scaling LLM Reasoning via Test-time Policy Evolution",
      "url": "https://arxiv.org/abs/2601.20379",
      "date": "2026-01-28",
      "authors_text": "Zhengbo Jiao, Hongyu Xian, Qinglong Wang, Yunpu Ma, Zhebo Wang",
      "is_highlight": false,
      "score": 74.0,
      "summary": "The Policy of Thoughts framework enhances LLM reasoning by enabling real-time policy evolution through execution feedback, significantly improving performance on complex tasks.",
      "teaser_image": "https://arxiv.org/html/2601.20379/x1.png",
      "debug_abstract": "Large language models (LLMs) struggle with complex, long-horizon reasoning due to instability caused by their frozen policy assumption. Current test-time scaling methods treat execution feedback merely as an external signal for filtering or rewriting trajectories, without internalizing it to improve the underlying reasoning strategy. Inspired by Popper&#39;s epistemology of &#34;conjectures and refutations,&#34; we argue that intelligence requires real-time evolution of the model&#39;s policy through learning from failed attempts. We introduce Policy of Thoughts (PoT), a framework that recasts reasoning as a within-instance online optimization process. PoT first generates diverse candidate solutions via an efficient exploration mechanism, then uses Group Relative Policy Optimization (GRPO) to update a transient LoRA adapter based on execution feedback. This closed-loop design enables dynamic, instance-specific refinement of the model&#39;s reasoning priors. Experiments show that PoT dramatically boosts performance: a 4B model achieves 49.71% accuracy on LiveCodeBench, outperforming GPT-4o and DeepSeek-V3 despite being over 50 smaller."
    },
    {
      "title": "Fast Converging 3D Gaussian Splatting for 1-Minute Reconstruction",
      "url": "https://arxiv.org/abs/2601.19489",
      "date": "2026-01-28",
      "authors_text": "Ziyu Zhang, Tianle Liu, Diantao Tu, Shuhan Shen",
      "is_highlight": false,
      "score": 65.0,
      "summary": "The paper presents a rapid 3D Gaussian Splatting reconstruction method achieving high-fidelity results within one minute, winning the SIGGRAPH Asia challenge with innovative optimization techniques.",
      "teaser_image": "https://arxiv.org/html/2601.19489/assets/load_balance_demo.png",
      "debug_abstract": "We present a fast 3DGS reconstruction pipeline designed to converge within one minute, developed for the SIGGRAPH Asia 3DGS Fast Reconstruction Challenge. The challenge consists of an initial round using SLAM-generated camera poses (with noisy trajectories) and a final round using COLMAP poses (highly accurate). To robustly handle these heterogeneous settings, we develop a two-stage solution. In the first round, we use reverse per-Gaussian parallel optimization and compact forward splatting based on Taming-GS and Speedy-splat, load-balanced tiling, an anchor-based Neural-Gaussian representation enabling rapid convergence with fewer learnable parameters, initialization from monocular depth and partially from feed-forward 3DGS models, and a global pose refinement module for noisy SLAM trajectories. In the final round, the accurate COLMAP poses change the optimization landscape; we disable pose refinement, revert from Neural-Gaussians back to standard 3DGS to eliminate MLP inference overhead, introduce multi-view consistency-guided Gaussian splitting inspired by Fast-GS, and introduce a depth estimator to supervise the rendered depth. Together, these techniques enable high-fidelity reconstruction under a strict one-minute budget. Our method achieved the top performance with a PSNR of 28.43 and ranked first in the competition."
    }
  ],
  "PINE Lab": [
    {
      "title": "RF-MatID: Dataset and Benchmark for Radio Frequency Material Identification",
      "url": "https://arxiv.org/abs/2601.20377",
      "date": "2026-01-28",
      "authors_text": "Xinyan Chen, Qinchun Li, Ruiqin Ma, Jiaqi Bai, Li Yi",
      "is_highlight": false,
      "score": 58.0,
      "summary": "RF-MatID introduces a comprehensive open-source RF dataset and benchmark for fine-grained material identification, addressing limitations of existing optical methods and enhancing algorithmic development.",
      "teaser_image": "https://arxiv.org/html/2601.20377/figures/materials.png",
      "debug_abstract": "Accurate material identification plays a crucial role in embodied AI systems, enabling a wide range of applications. However, current vision-based solutions are limited by the inherent constraints of optical sensors, while radio-frequency (RF) approaches, which can reveal intrinsic material properties, have received growing attention. Despite this progress, RF-based material identification remains hindered by the lack of large-scale public datasets and the limited benchmarking of learning-based approaches. In this work, we present RF-MatID, the first open-source, large-scale, wide-band, and geometry-diverse RF dataset for fine-grained material identification. RF-MatID includes 16 fine-grained categories grouped into 5 superclasses, spanning a broad frequency range from 4 to 43.5 GHz, and comprises 142k samples in both frequency- and time-domain representations. The dataset systematically incorporates controlled geometry perturbations, including variations in incidence angle and stand-off distance. We further establish a multi-setting, multi-protocol benchmark by evaluating state-of-the-art deep learning models, assessing both in-distribution performance and out-of-distribution robustness under cross-angle and cross-distance shifts. The 5 frequency-allocation protocols enable systematic frequency- and region-level analysis, thereby facilitating real-world deployment. RF-MatID aims to enable reproducible research, accelerate algorithmic advancement, foster cross-domain robustness, and support the development of real-world application in RF-based material identification."
    },
    {
      "title": "E2HiL: Entropy-Guided Sample Selection for Efficient Real-World Human-in-the-Loop Reinforcement Learning",
      "url": "https://arxiv.org/abs/2601.19969",
      "date": "2026-01-27",
      "authors_text": "Haoyuan Deng, Yuanjiang Xue, Haoyang Du, Boyang Zhou, Zhenyu Wu",
      "is_highlight": false,
      "score": 80.0,
      "summary": "E2HiL enhances human-in-the-loop reinforcement learning by efficiently selecting informative samples, achieving 42.1% higher success rates with 10.1% fewer human interventions.",
      "teaser_image": "https://arxiv.org/html/2601.19969/x2.png",
      "debug_abstract": "Human-in-the-loop guidance has emerged as an effective approach for enabling faster convergence in online reinforcement learning (RL) of complex real-world manipulation tasks. However, existing human-in-the-loop RL (HiL-RL) frameworks often suffer from low sample efficiency, requiring substantial human interventions to achieve convergence and thereby leading to high labor costs. To address this, we propose a sample-efficient real-world human-in-the-loop RL framework named \\method, which requires fewer human intervention by actively selecting informative samples. Specifically, stable reduction of policy entropy enables improved trade-off between exploration and exploitation with higher sample efficiency. We first build influence functions of different samples on the policy entropy, which is efficiently estimated by the covariance of action probabilities and soft advantages of policies. Then we select samples with moderate values of influence functions, where shortcut samples that induce sharp entropy drops and noisy samples with negligible effect are pruned. Extensive experiments on four real-world manipulation tasks demonstrate that \\method achieves a 42.1\\% higher success rate while requiring 10.1\\% fewer human interventions compared to the state-of-the-art HiL-RL method, validating its effectiveness. The project page providing code, videos, and mathematical formulations can be found at this https URL."
    }
  ],
  "MReaLLab": [
    {
      "title": "RF-MatID: Dataset and Benchmark for Radio Frequency Material Identification",
      "url": "https://arxiv.org/abs/2601.20377",
      "date": "2026-01-28",
      "authors_text": "Xinyan Chen, Qinchun Li, Ruiqin Ma, Jiaqi Bai, Li Yi",
      "is_highlight": false,
      "score": 58.0,
      "summary": "RF-MatID introduces a comprehensive open-source RF dataset and benchmark for fine-grained material identification, addressing limitations of existing optical methods and enhancing algorithmic development.",
      "teaser_image": "https://arxiv.org/html/2601.20377/figures/materials.png",
      "debug_abstract": "Accurate material identification plays a crucial role in embodied AI systems, enabling a wide range of applications. However, current vision-based solutions are limited by the inherent constraints of optical sensors, while radio-frequency (RF) approaches, which can reveal intrinsic material properties, have received growing attention. Despite this progress, RF-based material identification remains hindered by the lack of large-scale public datasets and the limited benchmarking of learning-based approaches. In this work, we present RF-MatID, the first open-source, large-scale, wide-band, and geometry-diverse RF dataset for fine-grained material identification. RF-MatID includes 16 fine-grained categories grouped into 5 superclasses, spanning a broad frequency range from 4 to 43.5 GHz, and comprises 142k samples in both frequency- and time-domain representations. The dataset systematically incorporates controlled geometry perturbations, including variations in incidence angle and stand-off distance. We further establish a multi-setting, multi-protocol benchmark by evaluating state-of-the-art deep learning models, assessing both in-distribution performance and out-of-distribution robustness under cross-angle and cross-distance shifts. The 5 frequency-allocation protocols enable systematic frequency- and region-level analysis, thereby facilitating real-world deployment. RF-MatID aims to enable reproducible research, accelerate algorithmic advancement, foster cross-domain robustness, and support the development of real-world application in RF-based material identification."
    },
    {
      "title": "E2HiL: Entropy-Guided Sample Selection for Efficient Real-World Human-in-the-Loop Reinforcement Learning",
      "url": "https://arxiv.org/abs/2601.19969",
      "date": "2026-01-27",
      "authors_text": "Haoyuan Deng, Yuanjiang Xue, Haoyang Du, Boyang Zhou, Zhenyu Wu",
      "is_highlight": false,
      "score": 80.0,
      "summary": "E2HiL enhances human-in-the-loop reinforcement learning by efficiently selecting informative samples, achieving 42.1% higher success rates with 10.1% fewer human interventions.",
      "teaser_image": "https://arxiv.org/html/2601.19969/x2.png",
      "debug_abstract": "Human-in-the-loop guidance has emerged as an effective approach for enabling faster convergence in online reinforcement learning (RL) of complex real-world manipulation tasks. However, existing human-in-the-loop RL (HiL-RL) frameworks often suffer from low sample efficiency, requiring substantial human interventions to achieve convergence and thereby leading to high labor costs. To address this, we propose a sample-efficient real-world human-in-the-loop RL framework named \\method, which requires fewer human intervention by actively selecting informative samples. Specifically, stable reduction of policy entropy enables improved trade-off between exploration and exploitation with higher sample efficiency. We first build influence functions of different samples on the policy entropy, which is efficiently estimated by the covariance of action probabilities and soft advantages of policies. Then we select samples with moderate values of influence functions, where shortcut samples that induce sharp entropy drops and noisy samples with negligible effect are pruned. Extensive experiments on four real-world manipulation tasks demonstrate that \\method achieves a 42.1\\% higher success rate while requiring 10.1\\% fewer human interventions compared to the state-of-the-art HiL-RL method, validating its effectiveness. The project page providing code, videos, and mathematical formulations can be found at this https URL."
    }
  ],
  "ROSE Lab": [
    {
      "title": "RF-MatID: Dataset and Benchmark for Radio Frequency Material Identification",
      "url": "https://arxiv.org/abs/2601.20377",
      "date": "2026-01-28",
      "authors_text": "Xinyan Chen, Qinchun Li, Ruiqin Ma, Jiaqi Bai, Li Yi",
      "is_highlight": false,
      "score": 58.0,
      "summary": "RF-MatID introduces a comprehensive open-source RF dataset and benchmark for fine-grained material identification, addressing limitations of existing optical methods and enhancing algorithmic development.",
      "teaser_image": "https://arxiv.org/html/2601.20377/figures/materials.png",
      "debug_abstract": "Accurate material identification plays a crucial role in embodied AI systems, enabling a wide range of applications. However, current vision-based solutions are limited by the inherent constraints of optical sensors, while radio-frequency (RF) approaches, which can reveal intrinsic material properties, have received growing attention. Despite this progress, RF-based material identification remains hindered by the lack of large-scale public datasets and the limited benchmarking of learning-based approaches. In this work, we present RF-MatID, the first open-source, large-scale, wide-band, and geometry-diverse RF dataset for fine-grained material identification. RF-MatID includes 16 fine-grained categories grouped into 5 superclasses, spanning a broad frequency range from 4 to 43.5 GHz, and comprises 142k samples in both frequency- and time-domain representations. The dataset systematically incorporates controlled geometry perturbations, including variations in incidence angle and stand-off distance. We further establish a multi-setting, multi-protocol benchmark by evaluating state-of-the-art deep learning models, assessing both in-distribution performance and out-of-distribution robustness under cross-angle and cross-distance shifts. The 5 frequency-allocation protocols enable systematic frequency- and region-level analysis, thereby facilitating real-world deployment. RF-MatID aims to enable reproducible research, accelerate algorithmic advancement, foster cross-domain robustness, and support the development of real-world application in RF-based material identification."
    },
    {
      "title": "E2HiL: Entropy-Guided Sample Selection for Efficient Real-World Human-in-the-Loop Reinforcement Learning",
      "url": "https://arxiv.org/abs/2601.19969",
      "date": "2026-01-27",
      "authors_text": "Haoyuan Deng, Yuanjiang Xue, Haoyang Du, Boyang Zhou, Zhenyu Wu",
      "is_highlight": false,
      "score": 80.0,
      "summary": "E2HiL enhances human-in-the-loop reinforcement learning by efficiently selecting informative samples, achieving 42.1% higher success rates with 10.1% fewer human interventions.",
      "teaser_image": "https://arxiv.org/html/2601.19969/x2.png",
      "debug_abstract": "Human-in-the-loop guidance has emerged as an effective approach for enabling faster convergence in online reinforcement learning (RL) of complex real-world manipulation tasks. However, existing human-in-the-loop RL (HiL-RL) frameworks often suffer from low sample efficiency, requiring substantial human interventions to achieve convergence and thereby leading to high labor costs. To address this, we propose a sample-efficient real-world human-in-the-loop RL framework named \\method, which requires fewer human intervention by actively selecting informative samples. Specifically, stable reduction of policy entropy enables improved trade-off between exploration and exploitation with higher sample efficiency. We first build influence functions of different samples on the policy entropy, which is efficiently estimated by the covariance of action probabilities and soft advantages of policies. Then we select samples with moderate values of influence functions, where shortcut samples that induce sharp entropy drops and noisy samples with negligible effect are pruned. Extensive experiments on four real-world manipulation tasks demonstrate that \\method achieves a 42.1\\% higher success rate while requiring 10.1\\% fewer human interventions compared to the state-of-the-art HiL-RL method, validating its effectiveness. The project page providing code, videos, and mathematical formulations can be found at this https URL."
    }
  ],
  "S-Lab for Advanced Intelligence": [
    {
      "title": "RF-MatID: Dataset and Benchmark for Radio Frequency Material Identification",
      "url": "https://arxiv.org/abs/2601.20377",
      "date": "2026-01-28",
      "authors_text": "Xinyan Chen, Qinchun Li, Ruiqin Ma, Jiaqi Bai, Li Yi",
      "is_highlight": false,
      "score": 58.0,
      "summary": "RF-MatID introduces a comprehensive open-source RF dataset and benchmark for fine-grained material identification, addressing limitations of existing optical methods and enhancing algorithmic development.",
      "teaser_image": "https://arxiv.org/html/2601.20377/figures/materials.png",
      "debug_abstract": "Accurate material identification plays a crucial role in embodied AI systems, enabling a wide range of applications. However, current vision-based solutions are limited by the inherent constraints of optical sensors, while radio-frequency (RF) approaches, which can reveal intrinsic material properties, have received growing attention. Despite this progress, RF-based material identification remains hindered by the lack of large-scale public datasets and the limited benchmarking of learning-based approaches. In this work, we present RF-MatID, the first open-source, large-scale, wide-band, and geometry-diverse RF dataset for fine-grained material identification. RF-MatID includes 16 fine-grained categories grouped into 5 superclasses, spanning a broad frequency range from 4 to 43.5 GHz, and comprises 142k samples in both frequency- and time-domain representations. The dataset systematically incorporates controlled geometry perturbations, including variations in incidence angle and stand-off distance. We further establish a multi-setting, multi-protocol benchmark by evaluating state-of-the-art deep learning models, assessing both in-distribution performance and out-of-distribution robustness under cross-angle and cross-distance shifts. The 5 frequency-allocation protocols enable systematic frequency- and region-level analysis, thereby facilitating real-world deployment. RF-MatID aims to enable reproducible research, accelerate algorithmic advancement, foster cross-domain robustness, and support the development of real-world application in RF-based material identification."
    },
    {
      "title": "E2HiL: Entropy-Guided Sample Selection for Efficient Real-World Human-in-the-Loop Reinforcement Learning",
      "url": "https://arxiv.org/abs/2601.19969",
      "date": "2026-01-27",
      "authors_text": "Haoyuan Deng, Yuanjiang Xue, Haoyang Du, Boyang Zhou, Zhenyu Wu",
      "is_highlight": false,
      "score": 80.0,
      "summary": "E2HiL enhances human-in-the-loop reinforcement learning by efficiently selecting informative samples, achieving 42.1% higher success rates with 10.1% fewer human interventions.",
      "teaser_image": "https://arxiv.org/html/2601.19969/x2.png",
      "debug_abstract": "Human-in-the-loop guidance has emerged as an effective approach for enabling faster convergence in online reinforcement learning (RL) of complex real-world manipulation tasks. However, existing human-in-the-loop RL (HiL-RL) frameworks often suffer from low sample efficiency, requiring substantial human interventions to achieve convergence and thereby leading to high labor costs. To address this, we propose a sample-efficient real-world human-in-the-loop RL framework named \\method, which requires fewer human intervention by actively selecting informative samples. Specifically, stable reduction of policy entropy enables improved trade-off between exploration and exploitation with higher sample efficiency. We first build influence functions of different samples on the policy entropy, which is efficiently estimated by the covariance of action probabilities and soft advantages of policies. Then we select samples with moderate values of influence functions, where shortcut samples that induce sharp entropy drops and noisy samples with negligible effect are pruned. Extensive experiments on four real-world manipulation tasks demonstrate that \\method achieves a 42.1\\% higher success rate while requiring 10.1\\% fewer human interventions compared to the state-of-the-art HiL-RL method, validating its effectiveness. The project page providing code, videos, and mathematical formulations can be found at this https URL."
    }
  ],
  "MARS Lab": [
    {
      "title": "RF-MatID: Dataset and Benchmark for Radio Frequency Material Identification",
      "url": "https://arxiv.org/abs/2601.20377",
      "date": "2026-01-28",
      "authors_text": "Xinyan Chen, Qinchun Li, Ruiqin Ma, Jiaqi Bai, Li Yi",
      "is_highlight": false,
      "score": 58.0,
      "summary": "RF-MatID introduces a comprehensive open-source RF dataset and benchmark for fine-grained material identification, addressing limitations of existing optical methods and enhancing algorithmic development.",
      "teaser_image": "https://arxiv.org/html/2601.20377/figures/materials.png",
      "debug_abstract": "Accurate material identification plays a crucial role in embodied AI systems, enabling a wide range of applications. However, current vision-based solutions are limited by the inherent constraints of optical sensors, while radio-frequency (RF) approaches, which can reveal intrinsic material properties, have received growing attention. Despite this progress, RF-based material identification remains hindered by the lack of large-scale public datasets and the limited benchmarking of learning-based approaches. In this work, we present RF-MatID, the first open-source, large-scale, wide-band, and geometry-diverse RF dataset for fine-grained material identification. RF-MatID includes 16 fine-grained categories grouped into 5 superclasses, spanning a broad frequency range from 4 to 43.5 GHz, and comprises 142k samples in both frequency- and time-domain representations. The dataset systematically incorporates controlled geometry perturbations, including variations in incidence angle and stand-off distance. We further establish a multi-setting, multi-protocol benchmark by evaluating state-of-the-art deep learning models, assessing both in-distribution performance and out-of-distribution robustness under cross-angle and cross-distance shifts. The 5 frequency-allocation protocols enable systematic frequency- and region-level analysis, thereby facilitating real-world deployment. RF-MatID aims to enable reproducible research, accelerate algorithmic advancement, foster cross-domain robustness, and support the development of real-world application in RF-based material identification."
    },
    {
      "title": "E2HiL: Entropy-Guided Sample Selection for Efficient Real-World Human-in-the-Loop Reinforcement Learning",
      "url": "https://arxiv.org/abs/2601.19969",
      "date": "2026-01-27",
      "authors_text": "Haoyuan Deng, Yuanjiang Xue, Haoyang Du, Boyang Zhou, Zhenyu Wu",
      "is_highlight": false,
      "score": 80.0,
      "summary": "E2HiL enhances human-in-the-loop reinforcement learning by efficiently selecting informative samples, achieving 42.1% higher success rates with 10.1% fewer human interventions.",
      "teaser_image": "https://arxiv.org/html/2601.19969/x2.png",
      "debug_abstract": "Human-in-the-loop guidance has emerged as an effective approach for enabling faster convergence in online reinforcement learning (RL) of complex real-world manipulation tasks. However, existing human-in-the-loop RL (HiL-RL) frameworks often suffer from low sample efficiency, requiring substantial human interventions to achieve convergence and thereby leading to high labor costs. To address this, we propose a sample-efficient real-world human-in-the-loop RL framework named \\method, which requires fewer human intervention by actively selecting informative samples. Specifically, stable reduction of policy entropy enables improved trade-off between exploration and exploitation with higher sample efficiency. We first build influence functions of different samples on the policy entropy, which is efficiently estimated by the covariance of action probabilities and soft advantages of policies. Then we select samples with moderate values of influence functions, where shortcut samples that induce sharp entropy drops and noisy samples with negligible effect are pruned. Extensive experiments on four real-world manipulation tasks demonstrate that \\method achieves a 42.1\\% higher success rate while requiring 10.1\\% fewer human interventions compared to the state-of-the-art HiL-RL method, validating its effectiveness. The project page providing code, videos, and mathematical formulations can be found at this https URL."
    }
  ],
  "MMLab@NTU": [
    {
      "title": "RF-MatID: Dataset and Benchmark for Radio Frequency Material Identification",
      "url": "https://arxiv.org/abs/2601.20377",
      "date": "2026-01-28",
      "authors_text": "Xinyan Chen, Qinchun Li, Ruiqin Ma, Jiaqi Bai, Li Yi",
      "is_highlight": false,
      "score": 58.0,
      "summary": "RF-MatID introduces a comprehensive open-source RF dataset and benchmark for fine-grained material identification, addressing limitations of existing optical methods and enhancing algorithmic development.",
      "teaser_image": "https://arxiv.org/html/2601.20377/figures/materials.png",
      "debug_abstract": "Accurate material identification plays a crucial role in embodied AI systems, enabling a wide range of applications. However, current vision-based solutions are limited by the inherent constraints of optical sensors, while radio-frequency (RF) approaches, which can reveal intrinsic material properties, have received growing attention. Despite this progress, RF-based material identification remains hindered by the lack of large-scale public datasets and the limited benchmarking of learning-based approaches. In this work, we present RF-MatID, the first open-source, large-scale, wide-band, and geometry-diverse RF dataset for fine-grained material identification. RF-MatID includes 16 fine-grained categories grouped into 5 superclasses, spanning a broad frequency range from 4 to 43.5 GHz, and comprises 142k samples in both frequency- and time-domain representations. The dataset systematically incorporates controlled geometry perturbations, including variations in incidence angle and stand-off distance. We further establish a multi-setting, multi-protocol benchmark by evaluating state-of-the-art deep learning models, assessing both in-distribution performance and out-of-distribution robustness under cross-angle and cross-distance shifts. The 5 frequency-allocation protocols enable systematic frequency- and region-level analysis, thereby facilitating real-world deployment. RF-MatID aims to enable reproducible research, accelerate algorithmic advancement, foster cross-domain robustness, and support the development of real-world application in RF-based material identification."
    },
    {
      "title": "E2HiL: Entropy-Guided Sample Selection for Efficient Real-World Human-in-the-Loop Reinforcement Learning",
      "url": "https://arxiv.org/abs/2601.19969",
      "date": "2026-01-27",
      "authors_text": "Haoyuan Deng, Yuanjiang Xue, Haoyang Du, Boyang Zhou, Zhenyu Wu",
      "is_highlight": false,
      "score": 80.0,
      "summary": "E2HiL enhances human-in-the-loop reinforcement learning by efficiently selecting informative samples, achieving 42.1% higher success rates with 10.1% fewer human interventions.",
      "teaser_image": "https://arxiv.org/html/2601.19969/x2.png",
      "debug_abstract": "Human-in-the-loop guidance has emerged as an effective approach for enabling faster convergence in online reinforcement learning (RL) of complex real-world manipulation tasks. However, existing human-in-the-loop RL (HiL-RL) frameworks often suffer from low sample efficiency, requiring substantial human interventions to achieve convergence and thereby leading to high labor costs. To address this, we propose a sample-efficient real-world human-in-the-loop RL framework named \\method, which requires fewer human intervention by actively selecting informative samples. Specifically, stable reduction of policy entropy enables improved trade-off between exploration and exploitation with higher sample efficiency. We first build influence functions of different samples on the policy entropy, which is efficiently estimated by the covariance of action probabilities and soft advantages of policies. Then we select samples with moderate values of influence functions, where shortcut samples that induce sharp entropy drops and noisy samples with negligible effect are pruned. Extensive experiments on four real-world manipulation tasks demonstrate that \\method achieves a 42.1\\% higher success rate while requiring 10.1\\% fewer human interventions compared to the state-of-the-art HiL-RL method, validating its effectiveness. The project page providing code, videos, and mathematical formulations can be found at this https URL."
    }
  ],
  "斯坦福-机器人与具身智能实验室 (REAL)": [
    {
      "title": "Open-Vocabulary Functional 3D Human-Scene Interaction Generation",
      "url": "https://arxiv.org/abs/2601.20835",
      "date": "2026-01-28",
      "authors_text": "Jie Liu, Yu Sun, Alpar Cseke, Yao Feng, Nicolas Heron",
      "is_highlight": false,
      "score": 81.0,
      "summary": "The FunHSI framework generates functionally correct 3D human-scene interactions from open-vocabulary prompts by employing functionality-aware contact reasoning and vision-language models.",
      "teaser_image": "https://arxiv.org/html/2601.20835/x2.png",
      "debug_abstract": "Generating 3D humans that functionally interact with 3D scenes remains an open problem with applications in embodied AI, robotics, and interactive content creation. The key challenge involves reasoning about both the semantics of functional elements in 3D scenes and the 3D human poses required to achieve functionality-aware interaction. Unfortunately, existing methods typically lack explicit reasoning over object functionality and the corresponding human-scene contact, resulting in implausible or functionally incorrect interactions. In this work, we propose FunHSI, a training-free, functionality-driven framework that enables functionally correct human-scene interactions from open-vocabulary task prompts. Given a task prompt, FunHSI performs functionality-aware contact reasoning to identify functional scene elements, reconstruct their 3D geometry, and model high-level interactions via a contact graph. We then leverage vision-language models to synthesize a human performing the task in the image and estimate proposed 3D body and hand poses. Finally, the proposed 3D body configuration is refined via stage-wise optimization to ensure physical plausibility and functional correctness. In contrast to existing methods, FunHSI not only synthesizes more plausible general 3D interactions, such as &#34;sitting on a sofa&#39;&#39;, while supporting fine-grained functional human-scene interactions, e.g., &#34;increasing the room temperature&#39;&#39;. Extensive experiments demonstrate that FunHSI consistently generates functionally correct and physically plausible human-scene interactions across diverse indoor and outdoor scenes."
    },
    {
      "title": "Fuzzy Categorical Planning: Autonomous Goal Satisfaction with Graded Semantic Constraints",
      "url": "https://arxiv.org/abs/2601.20021",
      "date": "2026-01-27",
      "authors_text": "Shuhui Qu",
      "is_highlight": false,
      "score": 77.0,
      "summary": "Fuzzy Category-theoretic Planning (FCP) enhances natural-language planning by incorporating graded semantic constraints, improving goal satisfaction and reducing violations in recipe substitution tasks.",
      "teaser_image": "https://arxiv.org/html/2601.20021/345.png",
      "debug_abstract": "Natural-language planning often involves vague predicates (e.g., suitable substitute, stable enough) whose satisfaction is inherently graded. Existing category-theoretic planners provide compositional structure and pullback-based hard-constraint verification, but treat applicability as crisp, forcing thresholding that collapses meaningful distinctions and cannot track quality degradation across multi-step plans. We propose Fuzzy Category-theoretic Planning (FCP), which annotates each action (morphism) with a degree in [0,1], composes plan quality via a t-norm Lukasiewicz, and retains crisp executability checks via pullback verification. FCP grounds graded applicability from language using an LLM with k-sample median aggregation and supports meeting-in-the-middle search using residuum-based backward requirements. We evaluate on (i) public PDDL3 preference/oversubscription benchmarks and (ii) RecipeNLG-Subs, a missing-substitute recipe-planning benchmark built from RecipeNLG with substitution candidates from Recipe1MSubs and FoodKG. FCP improves success and reduces hard-constraint violations on RecipeNLG-Subs compared to LLM-only and ReAct-style baselines, while remaining competitive with classical PDDL3 planners."
    },
    {
      "title": "Teaching LLMs to Ask: Self-Querying Category-Theoretic Planning for Under-Specified Reasoning",
      "url": "https://arxiv.org/abs/2601.20014",
      "date": "2026-01-27",
      "authors_text": "Shuhui Qu",
      "is_highlight": false,
      "score": 68.0,
      "summary": "The paper presents SQ-BCP, a self-querying planning method for large language models that effectively resolves unknown preconditions, reducing resource violations significantly while ensuring goal compatibility.",
      "teaser_image": "https://arxiv.org/html/2601.20014/123.png",
      "debug_abstract": "Inference-time planning with large language models frequently breaks under partial observability: when task-critical preconditions are not specified at query time, models tend to hallucinate missing facts or produce plans that violate hard constraints. We introduce \\textbf{Self-Querying Bidirectional Categorical Planning (SQ-BCP)}, which explicitly represents precondition status (\\texttt{Sat}/\\texttt{Viol}/\\texttt{Unk}) and resolves unknowns via (i) targeted self-queries to an oracle/user or (ii) \\emph{bridging} hypotheses that establish the missing condition through an additional action. SQ-BCP performs bidirectional search and invokes a pullback-based verifier as a categorical certificate of goal compatibility, while using distance-based scores only for ranking and pruning. We prove that when the verifier succeeds and hard constraints pass deterministic checks, accepted plans are compatible with goal requirements; under bounded branching and finite resolution depth, SQ-BCP finds an accepting plan when one exists. Across WikiHow and RecipeNLG tasks with withheld preconditions, SQ-BCP reduces resource-violation rates to \\textbf{14.9\\%} and \\textbf{5.8\\%} (vs.\\ \\textbf{26.0\\%} and \\textbf{15.7\\%} for the best baseline), while maintaining competitive reference quality."
    }
  ],
  "斯坦福-人工智能实验室 (SAIL)": [
    {
      "title": "Open-Vocabulary Functional 3D Human-Scene Interaction Generation",
      "url": "https://arxiv.org/abs/2601.20835",
      "date": "2026-01-28",
      "authors_text": "Jie Liu, Yu Sun, Alpar Cseke, Yao Feng, Nicolas Heron",
      "is_highlight": false,
      "score": 81.0,
      "summary": "The FunHSI framework generates functionally correct 3D human-scene interactions from open-vocabulary prompts by employing functionality-aware contact reasoning and vision-language models.",
      "teaser_image": "https://arxiv.org/html/2601.20835/x2.png",
      "debug_abstract": "Generating 3D humans that functionally interact with 3D scenes remains an open problem with applications in embodied AI, robotics, and interactive content creation. The key challenge involves reasoning about both the semantics of functional elements in 3D scenes and the 3D human poses required to achieve functionality-aware interaction. Unfortunately, existing methods typically lack explicit reasoning over object functionality and the corresponding human-scene contact, resulting in implausible or functionally incorrect interactions. In this work, we propose FunHSI, a training-free, functionality-driven framework that enables functionally correct human-scene interactions from open-vocabulary task prompts. Given a task prompt, FunHSI performs functionality-aware contact reasoning to identify functional scene elements, reconstruct their 3D geometry, and model high-level interactions via a contact graph. We then leverage vision-language models to synthesize a human performing the task in the image and estimate proposed 3D body and hand poses. Finally, the proposed 3D body configuration is refined via stage-wise optimization to ensure physical plausibility and functional correctness. In contrast to existing methods, FunHSI not only synthesizes more plausible general 3D interactions, such as &#34;sitting on a sofa&#39;&#39;, while supporting fine-grained functional human-scene interactions, e.g., &#34;increasing the room temperature&#39;&#39;. Extensive experiments demonstrate that FunHSI consistently generates functionally correct and physically plausible human-scene interactions across diverse indoor and outdoor scenes."
    },
    {
      "title": "Fuzzy Categorical Planning: Autonomous Goal Satisfaction with Graded Semantic Constraints",
      "url": "https://arxiv.org/abs/2601.20021",
      "date": "2026-01-27",
      "authors_text": "Shuhui Qu",
      "is_highlight": false,
      "score": 77.0,
      "summary": "Fuzzy Category-theoretic Planning (FCP) enhances natural-language planning by incorporating graded semantic constraints, improving goal satisfaction and reducing violations in recipe substitution tasks.",
      "teaser_image": "https://arxiv.org/html/2601.20021/345.png",
      "debug_abstract": "Natural-language planning often involves vague predicates (e.g., suitable substitute, stable enough) whose satisfaction is inherently graded. Existing category-theoretic planners provide compositional structure and pullback-based hard-constraint verification, but treat applicability as crisp, forcing thresholding that collapses meaningful distinctions and cannot track quality degradation across multi-step plans. We propose Fuzzy Category-theoretic Planning (FCP), which annotates each action (morphism) with a degree in [0,1], composes plan quality via a t-norm Lukasiewicz, and retains crisp executability checks via pullback verification. FCP grounds graded applicability from language using an LLM with k-sample median aggregation and supports meeting-in-the-middle search using residuum-based backward requirements. We evaluate on (i) public PDDL3 preference/oversubscription benchmarks and (ii) RecipeNLG-Subs, a missing-substitute recipe-planning benchmark built from RecipeNLG with substitution candidates from Recipe1MSubs and FoodKG. FCP improves success and reduces hard-constraint violations on RecipeNLG-Subs compared to LLM-only and ReAct-style baselines, while remaining competitive with classical PDDL3 planners."
    },
    {
      "title": "Teaching LLMs to Ask: Self-Querying Category-Theoretic Planning for Under-Specified Reasoning",
      "url": "https://arxiv.org/abs/2601.20014",
      "date": "2026-01-27",
      "authors_text": "Shuhui Qu",
      "is_highlight": false,
      "score": 68.0,
      "summary": "The paper presents SQ-BCP, a self-querying planning method for large language models that effectively resolves unknown preconditions, reducing resource violations significantly while ensuring goal compatibility.",
      "teaser_image": "https://arxiv.org/html/2601.20014/123.png",
      "debug_abstract": "Inference-time planning with large language models frequently breaks under partial observability: when task-critical preconditions are not specified at query time, models tend to hallucinate missing facts or produce plans that violate hard constraints. We introduce \\textbf{Self-Querying Bidirectional Categorical Planning (SQ-BCP)}, which explicitly represents precondition status (\\texttt{Sat}/\\texttt{Viol}/\\texttt{Unk}) and resolves unknowns via (i) targeted self-queries to an oracle/user or (ii) \\emph{bridging} hypotheses that establish the missing condition through an additional action. SQ-BCP performs bidirectional search and invokes a pullback-based verifier as a categorical certificate of goal compatibility, while using distance-based scores only for ranking and pruning. We prove that when the verifier succeeds and hard constraints pass deterministic checks, accepted plans are compatible with goal requirements; under bounded branching and finite resolution depth, SQ-BCP finds an accepting plan when one exists. Across WikiHow and RecipeNLG tasks with withheld preconditions, SQ-BCP reduces resource-violation rates to \\textbf{14.9\\%} and \\textbf{5.8\\%} (vs.\\ \\textbf{26.0\\%} and \\textbf{15.7\\%} for the best baseline), while maintaining competitive reference quality."
    }
  ],
  "Robotics and Embodied Artificial Intelligence Lab (REAL)": [
    {
      "title": "Open-Vocabulary Functional 3D Human-Scene Interaction Generation",
      "url": "https://arxiv.org/abs/2601.20835",
      "date": "2026-01-28",
      "authors_text": "Jie Liu, Yu Sun, Alpar Cseke, Yao Feng, Nicolas Heron",
      "is_highlight": false,
      "score": 81.0,
      "summary": "The FunHSI framework generates functionally correct 3D human-scene interactions from open-vocabulary prompts by employing functionality-aware contact reasoning and vision-language models.",
      "teaser_image": "https://arxiv.org/html/2601.20835/x2.png",
      "debug_abstract": "Generating 3D humans that functionally interact with 3D scenes remains an open problem with applications in embodied AI, robotics, and interactive content creation. The key challenge involves reasoning about both the semantics of functional elements in 3D scenes and the 3D human poses required to achieve functionality-aware interaction. Unfortunately, existing methods typically lack explicit reasoning over object functionality and the corresponding human-scene contact, resulting in implausible or functionally incorrect interactions. In this work, we propose FunHSI, a training-free, functionality-driven framework that enables functionally correct human-scene interactions from open-vocabulary task prompts. Given a task prompt, FunHSI performs functionality-aware contact reasoning to identify functional scene elements, reconstruct their 3D geometry, and model high-level interactions via a contact graph. We then leverage vision-language models to synthesize a human performing the task in the image and estimate proposed 3D body and hand poses. Finally, the proposed 3D body configuration is refined via stage-wise optimization to ensure physical plausibility and functional correctness. In contrast to existing methods, FunHSI not only synthesizes more plausible general 3D interactions, such as &#34;sitting on a sofa&#39;&#39;, while supporting fine-grained functional human-scene interactions, e.g., &#34;increasing the room temperature&#39;&#39;. Extensive experiments demonstrate that FunHSI consistently generates functionally correct and physically plausible human-scene interactions across diverse indoor and outdoor scenes."
    },
    {
      "title": "Fuzzy Categorical Planning: Autonomous Goal Satisfaction with Graded Semantic Constraints",
      "url": "https://arxiv.org/abs/2601.20021",
      "date": "2026-01-27",
      "authors_text": "Shuhui Qu",
      "is_highlight": false,
      "score": 77.0,
      "summary": "Fuzzy Category-theoretic Planning (FCP) enhances natural-language planning by incorporating graded semantic constraints, improving goal satisfaction and reducing violations in recipe substitution tasks.",
      "teaser_image": "https://arxiv.org/html/2601.20021/345.png",
      "debug_abstract": "Natural-language planning often involves vague predicates (e.g., suitable substitute, stable enough) whose satisfaction is inherently graded. Existing category-theoretic planners provide compositional structure and pullback-based hard-constraint verification, but treat applicability as crisp, forcing thresholding that collapses meaningful distinctions and cannot track quality degradation across multi-step plans. We propose Fuzzy Category-theoretic Planning (FCP), which annotates each action (morphism) with a degree in [0,1], composes plan quality via a t-norm Lukasiewicz, and retains crisp executability checks via pullback verification. FCP grounds graded applicability from language using an LLM with k-sample median aggregation and supports meeting-in-the-middle search using residuum-based backward requirements. We evaluate on (i) public PDDL3 preference/oversubscription benchmarks and (ii) RecipeNLG-Subs, a missing-substitute recipe-planning benchmark built from RecipeNLG with substitution candidates from Recipe1MSubs and FoodKG. FCP improves success and reduces hard-constraint violations on RecipeNLG-Subs compared to LLM-only and ReAct-style baselines, while remaining competitive with classical PDDL3 planners."
    },
    {
      "title": "Teaching LLMs to Ask: Self-Querying Category-Theoretic Planning for Under-Specified Reasoning",
      "url": "https://arxiv.org/abs/2601.20014",
      "date": "2026-01-27",
      "authors_text": "Shuhui Qu",
      "is_highlight": false,
      "score": 68.0,
      "summary": "The paper presents SQ-BCP, a self-querying planning method for large language models that effectively resolves unknown preconditions, reducing resource violations significantly while ensuring goal compatibility.",
      "teaser_image": "https://arxiv.org/html/2601.20014/123.png",
      "debug_abstract": "Inference-time planning with large language models frequently breaks under partial observability: when task-critical preconditions are not specified at query time, models tend to hallucinate missing facts or produce plans that violate hard constraints. We introduce \\textbf{Self-Querying Bidirectional Categorical Planning (SQ-BCP)}, which explicitly represents precondition status (\\texttt{Sat}/\\texttt{Viol}/\\texttt{Unk}) and resolves unknowns via (i) targeted self-queries to an oracle/user or (ii) \\emph{bridging} hypotheses that establish the missing condition through an additional action. SQ-BCP performs bidirectional search and invokes a pullback-based verifier as a categorical certificate of goal compatibility, while using distance-based scores only for ranking and pruning. We prove that when the verifier succeeds and hard constraints pass deterministic checks, accepted plans are compatible with goal requirements; under bounded branching and finite resolution depth, SQ-BCP finds an accepting plan when one exists. Across WikiHow and RecipeNLG tasks with withheld preconditions, SQ-BCP reduces resource-violation rates to \\textbf{14.9\\%} and \\textbf{5.8\\%} (vs.\\ \\textbf{26.0\\%} and \\textbf{15.7\\%} for the best baseline), while maintaining competitive reference quality."
    }
  ],
  "Stanford AI Lab (SAIL)": [
    {
      "title": "Open-Vocabulary Functional 3D Human-Scene Interaction Generation",
      "url": "https://arxiv.org/abs/2601.20835",
      "date": "2026-01-28",
      "authors_text": "Jie Liu, Yu Sun, Alpar Cseke, Yao Feng, Nicolas Heron",
      "is_highlight": false,
      "score": 81.0,
      "summary": "The FunHSI framework generates functionally correct 3D human-scene interactions from open-vocabulary prompts by employing functionality-aware contact reasoning and vision-language models.",
      "teaser_image": "https://arxiv.org/html/2601.20835/x2.png",
      "debug_abstract": "Generating 3D humans that functionally interact with 3D scenes remains an open problem with applications in embodied AI, robotics, and interactive content creation. The key challenge involves reasoning about both the semantics of functional elements in 3D scenes and the 3D human poses required to achieve functionality-aware interaction. Unfortunately, existing methods typically lack explicit reasoning over object functionality and the corresponding human-scene contact, resulting in implausible or functionally incorrect interactions. In this work, we propose FunHSI, a training-free, functionality-driven framework that enables functionally correct human-scene interactions from open-vocabulary task prompts. Given a task prompt, FunHSI performs functionality-aware contact reasoning to identify functional scene elements, reconstruct their 3D geometry, and model high-level interactions via a contact graph. We then leverage vision-language models to synthesize a human performing the task in the image and estimate proposed 3D body and hand poses. Finally, the proposed 3D body configuration is refined via stage-wise optimization to ensure physical plausibility and functional correctness. In contrast to existing methods, FunHSI not only synthesizes more plausible general 3D interactions, such as &#34;sitting on a sofa&#39;&#39;, while supporting fine-grained functional human-scene interactions, e.g., &#34;increasing the room temperature&#39;&#39;. Extensive experiments demonstrate that FunHSI consistently generates functionally correct and physically plausible human-scene interactions across diverse indoor and outdoor scenes."
    },
    {
      "title": "Fuzzy Categorical Planning: Autonomous Goal Satisfaction with Graded Semantic Constraints",
      "url": "https://arxiv.org/abs/2601.20021",
      "date": "2026-01-27",
      "authors_text": "Shuhui Qu",
      "is_highlight": false,
      "score": 77.0,
      "summary": "Fuzzy Category-theoretic Planning (FCP) enhances natural-language planning by incorporating graded semantic constraints, improving goal satisfaction and reducing violations in recipe substitution tasks.",
      "teaser_image": "https://arxiv.org/html/2601.20021/345.png",
      "debug_abstract": "Natural-language planning often involves vague predicates (e.g., suitable substitute, stable enough) whose satisfaction is inherently graded. Existing category-theoretic planners provide compositional structure and pullback-based hard-constraint verification, but treat applicability as crisp, forcing thresholding that collapses meaningful distinctions and cannot track quality degradation across multi-step plans. We propose Fuzzy Category-theoretic Planning (FCP), which annotates each action (morphism) with a degree in [0,1], composes plan quality via a t-norm Lukasiewicz, and retains crisp executability checks via pullback verification. FCP grounds graded applicability from language using an LLM with k-sample median aggregation and supports meeting-in-the-middle search using residuum-based backward requirements. We evaluate on (i) public PDDL3 preference/oversubscription benchmarks and (ii) RecipeNLG-Subs, a missing-substitute recipe-planning benchmark built from RecipeNLG with substitution candidates from Recipe1MSubs and FoodKG. FCP improves success and reduces hard-constraint violations on RecipeNLG-Subs compared to LLM-only and ReAct-style baselines, while remaining competitive with classical PDDL3 planners."
    },
    {
      "title": "Teaching LLMs to Ask: Self-Querying Category-Theoretic Planning for Under-Specified Reasoning",
      "url": "https://arxiv.org/abs/2601.20014",
      "date": "2026-01-27",
      "authors_text": "Shuhui Qu",
      "is_highlight": false,
      "score": 68.0,
      "summary": "The paper presents SQ-BCP, a self-querying planning method for large language models that effectively resolves unknown preconditions, reducing resource violations significantly while ensuring goal compatibility.",
      "teaser_image": "https://arxiv.org/html/2601.20014/123.png",
      "debug_abstract": "Inference-time planning with large language models frequently breaks under partial observability: when task-critical preconditions are not specified at query time, models tend to hallucinate missing facts or produce plans that violate hard constraints. We introduce \\textbf{Self-Querying Bidirectional Categorical Planning (SQ-BCP)}, which explicitly represents precondition status (\\texttt{Sat}/\\texttt{Viol}/\\texttt{Unk}) and resolves unknowns via (i) targeted self-queries to an oracle/user or (ii) \\emph{bridging} hypotheses that establish the missing condition through an additional action. SQ-BCP performs bidirectional search and invokes a pullback-based verifier as a categorical certificate of goal compatibility, while using distance-based scores only for ranking and pruning. We prove that when the verifier succeeds and hard constraints pass deterministic checks, accepted plans are compatible with goal requirements; under bounded branching and finite resolution depth, SQ-BCP finds an accepting plan when one exists. Across WikiHow and RecipeNLG tasks with withheld preconditions, SQ-BCP reduces resource-violation rates to \\textbf{14.9\\%} and \\textbf{5.8\\%} (vs.\\ \\textbf{26.0\\%} and \\textbf{15.7\\%} for the best baseline), while maintaining competitive reference quality."
    }
  ],
  "斯坦福-视觉与学习实验室 (SVL)": [
    {
      "title": "Open-Vocabulary Functional 3D Human-Scene Interaction Generation",
      "url": "https://arxiv.org/abs/2601.20835",
      "date": "2026-01-28",
      "authors_text": "Jie Liu, Yu Sun, Alpar Cseke, Yao Feng, Nicolas Heron",
      "is_highlight": false,
      "score": 81.0,
      "summary": "The FunHSI framework generates functionally correct 3D human-scene interactions from open-vocabulary prompts by employing functionality-aware contact reasoning and vision-language models.",
      "teaser_image": "https://arxiv.org/html/2601.20835/x2.png",
      "debug_abstract": "Generating 3D humans that functionally interact with 3D scenes remains an open problem with applications in embodied AI, robotics, and interactive content creation. The key challenge involves reasoning about both the semantics of functional elements in 3D scenes and the 3D human poses required to achieve functionality-aware interaction. Unfortunately, existing methods typically lack explicit reasoning over object functionality and the corresponding human-scene contact, resulting in implausible or functionally incorrect interactions. In this work, we propose FunHSI, a training-free, functionality-driven framework that enables functionally correct human-scene interactions from open-vocabulary task prompts. Given a task prompt, FunHSI performs functionality-aware contact reasoning to identify functional scene elements, reconstruct their 3D geometry, and model high-level interactions via a contact graph. We then leverage vision-language models to synthesize a human performing the task in the image and estimate proposed 3D body and hand poses. Finally, the proposed 3D body configuration is refined via stage-wise optimization to ensure physical plausibility and functional correctness. In contrast to existing methods, FunHSI not only synthesizes more plausible general 3D interactions, such as &#34;sitting on a sofa&#39;&#39;, while supporting fine-grained functional human-scene interactions, e.g., &#34;increasing the room temperature&#39;&#39;. Extensive experiments demonstrate that FunHSI consistently generates functionally correct and physically plausible human-scene interactions across diverse indoor and outdoor scenes."
    },
    {
      "title": "Fuzzy Categorical Planning: Autonomous Goal Satisfaction with Graded Semantic Constraints",
      "url": "https://arxiv.org/abs/2601.20021",
      "date": "2026-01-27",
      "authors_text": "Shuhui Qu",
      "is_highlight": false,
      "score": 77.0,
      "summary": "Fuzzy Category-theoretic Planning (FCP) enhances natural-language planning by incorporating graded semantic constraints, improving goal satisfaction and reducing violations in recipe substitution tasks.",
      "teaser_image": "https://arxiv.org/html/2601.20021/345.png",
      "debug_abstract": "Natural-language planning often involves vague predicates (e.g., suitable substitute, stable enough) whose satisfaction is inherently graded. Existing category-theoretic planners provide compositional structure and pullback-based hard-constraint verification, but treat applicability as crisp, forcing thresholding that collapses meaningful distinctions and cannot track quality degradation across multi-step plans. We propose Fuzzy Category-theoretic Planning (FCP), which annotates each action (morphism) with a degree in [0,1], composes plan quality via a t-norm Lukasiewicz, and retains crisp executability checks via pullback verification. FCP grounds graded applicability from language using an LLM with k-sample median aggregation and supports meeting-in-the-middle search using residuum-based backward requirements. We evaluate on (i) public PDDL3 preference/oversubscription benchmarks and (ii) RecipeNLG-Subs, a missing-substitute recipe-planning benchmark built from RecipeNLG with substitution candidates from Recipe1MSubs and FoodKG. FCP improves success and reduces hard-constraint violations on RecipeNLG-Subs compared to LLM-only and ReAct-style baselines, while remaining competitive with classical PDDL3 planners."
    },
    {
      "title": "Teaching LLMs to Ask: Self-Querying Category-Theoretic Planning for Under-Specified Reasoning",
      "url": "https://arxiv.org/abs/2601.20014",
      "date": "2026-01-27",
      "authors_text": "Shuhui Qu",
      "is_highlight": false,
      "score": 68.0,
      "summary": "The paper presents SQ-BCP, a self-querying planning method for large language models that effectively resolves unknown preconditions, reducing resource violations significantly while ensuring goal compatibility.",
      "teaser_image": "https://arxiv.org/html/2601.20014/123.png",
      "debug_abstract": "Inference-time planning with large language models frequently breaks under partial observability: when task-critical preconditions are not specified at query time, models tend to hallucinate missing facts or produce plans that violate hard constraints. We introduce \\textbf{Self-Querying Bidirectional Categorical Planning (SQ-BCP)}, which explicitly represents precondition status (\\texttt{Sat}/\\texttt{Viol}/\\texttt{Unk}) and resolves unknowns via (i) targeted self-queries to an oracle/user or (ii) \\emph{bridging} hypotheses that establish the missing condition through an additional action. SQ-BCP performs bidirectional search and invokes a pullback-based verifier as a categorical certificate of goal compatibility, while using distance-based scores only for ranking and pruning. We prove that when the verifier succeeds and hard constraints pass deterministic checks, accepted plans are compatible with goal requirements; under bounded branching and finite resolution depth, SQ-BCP finds an accepting plan when one exists. Across WikiHow and RecipeNLG tasks with withheld preconditions, SQ-BCP reduces resource-violation rates to \\textbf{14.9\\%} and \\textbf{5.8\\%} (vs.\\ \\textbf{26.0\\%} and \\textbf{15.7\\%} for the best baseline), while maintaining competitive reference quality."
    }
  ],
  "机器人系统实验室 (RSL)": [
    {
      "title": "Reinforcement Learning via Self-Distillation",
      "url": "https://arxiv.org/abs/2601.20802",
      "date": "2026-01-28",
      "authors_text": "Jonas Hübotter, Frederike Lübeck, Lejs Behric, Anton Baumann, Marco Bagatella",
      "is_highlight": false,
      "score": 62.0,
      "summary": "The paper introduces Self-Distillation Policy Optimization (SDPO), enhancing reinforcement learning with rich feedback to improve sample efficiency and accuracy in various domains.",
      "teaser_image": "https://arxiv.org/html/2601.20802/x2.png",
      "debug_abstract": "Large language models are increasingly post-trained with reinforcement learning in verifiable domains such as code and math. Yet, current methods for reinforcement learning with verifiable rewards (RLVR) learn only from a scalar outcome reward per attempt, creating a severe credit-assignment bottleneck. Many verifiable environments actually provide rich textual feedback, such as runtime errors or judge evaluations, that explain why an attempt failed. We formalize this setting as reinforcement learning with rich feedback and introduce Self-Distillation Policy Optimization (SDPO), which converts tokenized feedback into a dense learning signal without any external teacher or explicit reward model. SDPO treats the current model conditioned on feedback as a self-teacher and distills its feedback-informed next-token predictions back into the policy. In this way, SDPO leverages the model&#39;s ability to retrospectively identify its own mistakes in-context. Across scientific reasoning, tool use, and competitive programming on LiveCodeBench v6, SDPO improves sample efficiency and final accuracy over strong RLVR baselines. Notably, SDPO also outperforms baselines in standard RLVR environments that only return scalar feedback by using successful rollouts as implicit feedback for failed attempts. Finally, applying SDPO to individual questions at test time accelerates discovery on difficult binary-reward tasks, achieving the same discovery probability as best-of-k sampling or multi-turn conversations with 3x fewer attempts."
    },
    {
      "title": "Game-Theoretic Autonomous Driving: A Graphs of Convex Sets Approach",
      "url": "https://arxiv.org/abs/2601.20054",
      "date": "2026-01-27",
      "authors_text": "Nikolaj Käfer, Ahmed Khalil, Edward Huynh, Efstathios Bakolas, David Fridovich-Keil",
      "is_highlight": true,
      "score": 82.0,
      "summary": "The IBR-GCS approach models multi-vehicle autonomous driving as a game, enabling efficient trajectory planning and strategic interaction under safety constraints through iterative best-response updates.",
      "teaser_image": "https://arxiv.org/html/2601.20054/x2.png",
      "debug_abstract": "Multi-vehicle autonomous driving couples strategic interaction with hybrid (discrete-continuous) maneuver planning under shared safety constraints. We introduce IBR-GCS, an Iterative Best Response (IBR) planning approach based on the Graphs of Convex Sets (GCS) framework that models highway driving as a generalized noncooperative game. IBR-GCS integrates combinatorial maneuver reasoning, trajectory planning, and game-theoretic interaction within a unified framework. The key novelty is a vehicle-specific, strategy-dependent GCS construction. Specifically, at each best-response update, each vehicle builds its own graph conditioned on the current strategies of the other vehicles, with vertices representing lane-specific, time-varying, convex, collision-free regions and edges encoding dynamically feasible transitions. This yields a shortest-path problem in GCS for each best-response step, which admits an efficient convex relaxation that can be solved using convex optimization tools without exhaustive discrete tree search. We then apply an iterative best-response scheme in which vehicles update their trajectories sequentially and provide conditions under which the resulting inexact updates converge to an approximate generalized Nash equilibrium. Simulation results across multi-lane, multi-vehicle scenarios demonstrate that IBR-GCS produces safe trajectories and strategically consistent interactive behaviors."
    }
  ],
  "Sensing, Interaction & Perception Lab (SIPLAB)": [
    {
      "title": "Reinforcement Learning via Self-Distillation",
      "url": "https://arxiv.org/abs/2601.20802",
      "date": "2026-01-28",
      "authors_text": "Jonas Hübotter, Frederike Lübeck, Lejs Behric, Anton Baumann, Marco Bagatella",
      "is_highlight": false,
      "score": 62.0,
      "summary": "The paper introduces Self-Distillation Policy Optimization (SDPO), enhancing reinforcement learning with rich feedback to improve sample efficiency and accuracy in various domains.",
      "teaser_image": "https://arxiv.org/html/2601.20802/x2.png",
      "debug_abstract": "Large language models are increasingly post-trained with reinforcement learning in verifiable domains such as code and math. Yet, current methods for reinforcement learning with verifiable rewards (RLVR) learn only from a scalar outcome reward per attempt, creating a severe credit-assignment bottleneck. Many verifiable environments actually provide rich textual feedback, such as runtime errors or judge evaluations, that explain why an attempt failed. We formalize this setting as reinforcement learning with rich feedback and introduce Self-Distillation Policy Optimization (SDPO), which converts tokenized feedback into a dense learning signal without any external teacher or explicit reward model. SDPO treats the current model conditioned on feedback as a self-teacher and distills its feedback-informed next-token predictions back into the policy. In this way, SDPO leverages the model&#39;s ability to retrospectively identify its own mistakes in-context. Across scientific reasoning, tool use, and competitive programming on LiveCodeBench v6, SDPO improves sample efficiency and final accuracy over strong RLVR baselines. Notably, SDPO also outperforms baselines in standard RLVR environments that only return scalar feedback by using successful rollouts as implicit feedback for failed attempts. Finally, applying SDPO to individual questions at test time accelerates discovery on difficult binary-reward tasks, achieving the same discovery probability as best-of-k sampling or multi-turn conversations with 3x fewer attempts."
    },
    {
      "title": "Game-Theoretic Autonomous Driving: A Graphs of Convex Sets Approach",
      "url": "https://arxiv.org/abs/2601.20054",
      "date": "2026-01-27",
      "authors_text": "Nikolaj Käfer, Ahmed Khalil, Edward Huynh, Efstathios Bakolas, David Fridovich-Keil",
      "is_highlight": true,
      "score": 82.0,
      "summary": "The IBR-GCS approach models multi-vehicle autonomous driving as a game, enabling efficient trajectory planning and strategic interaction under safety constraints through iterative best-response updates.",
      "teaser_image": "https://arxiv.org/html/2601.20054/x2.png",
      "debug_abstract": "Multi-vehicle autonomous driving couples strategic interaction with hybrid (discrete-continuous) maneuver planning under shared safety constraints. We introduce IBR-GCS, an Iterative Best Response (IBR) planning approach based on the Graphs of Convex Sets (GCS) framework that models highway driving as a generalized noncooperative game. IBR-GCS integrates combinatorial maneuver reasoning, trajectory planning, and game-theoretic interaction within a unified framework. The key novelty is a vehicle-specific, strategy-dependent GCS construction. Specifically, at each best-response update, each vehicle builds its own graph conditioned on the current strategies of the other vehicles, with vertices representing lane-specific, time-varying, convex, collision-free regions and edges encoding dynamically feasible transitions. This yields a shortest-path problem in GCS for each best-response step, which admits an efficient convex relaxation that can be solved using convex optimization tools without exhaustive discrete tree search. We then apply an iterative best-response scheme in which vehicles update their trajectories sequentially and provide conditions under which the resulting inexact updates converge to an approximate generalized Nash equilibrium. Simulation results across multi-lane, multi-vehicle scenarios demonstrate that IBR-GCS produces safe trajectories and strategically consistent interactive behaviors."
    }
  ],
  "赵波老师实验室": [
    {
      "title": "TouchGuide: Inference-Time Steering of Visuomotor Policies via Touch Guidance",
      "url": "https://arxiv.org/abs/2601.20239",
      "date": "2026-01-28",
      "authors_text": "Zhemeng Zhang, Jiahua Ma, Xincheng Yang, Xin Wen, Yuzhi Zhang",
      "is_highlight": false,
      "score": 90.0,
      "summary": "TouchGuide enhances robot manipulation by integrating tactile feedback with visual inputs to refine actions during inference, outperforming existing visuo-tactile policies in complex tasks.",
      "teaser_image": "https://arxiv.org/html/2601.20239/x2.png",
      "debug_abstract": "Fine-grained and contact-rich manipulation remain challenging for robots, largely due to the underutilization of tactile feedback. To address this, we introduce TouchGuide, a novel cross-policy visuo-tactile fusion paradigm that fuses modalities within a low-dimensional action space. Specifically, TouchGuide operates in two stages to guide a pre-trained diffusion or flow-matching visuomotor policy at inference time. First, the policy produces a coarse, visually-plausible action using only visual inputs during early sampling. Second, a task-specific Contact Physical Model (CPM) provides tactile guidance to steer and refine the action, ensuring it aligns with realistic physical contact conditions. Trained through contrastive learning on limited expert demonstrations, the CPM provides a tactile-informed feasibility score to steer the sampling process toward refined actions that satisfy physical contact constraints. Furthermore, to facilitate TouchGuide training with high-quality and cost-effective data, we introduce TacUMI, a data collection system. TacUMI achieves a favorable trade-off between precision and affordability; by leveraging rigid fingertips, it obtains direct tactile feedback, thereby enabling the collection of reliable tactile data. Extensive experiments on five challenging contact-rich tasks, such as shoe lacing and chip handover, show that TouchGuide consistently and significantly outperforms state-of-the-art visuo-tactile policies."
    }
  ],
  "上海人工智能实验室": [
    {
      "title": "TouchGuide: Inference-Time Steering of Visuomotor Policies via Touch Guidance",
      "url": "https://arxiv.org/abs/2601.20239",
      "date": "2026-01-28",
      "authors_text": "Zhemeng Zhang, Jiahua Ma, Xincheng Yang, Xin Wen, Yuzhi Zhang",
      "is_highlight": false,
      "score": 90.0,
      "summary": "TouchGuide enhances robot manipulation by integrating tactile feedback with visual inputs to refine actions during inference, outperforming existing visuo-tactile policies in complex tasks.",
      "teaser_image": "https://arxiv.org/html/2601.20239/x2.png",
      "debug_abstract": "Fine-grained and contact-rich manipulation remain challenging for robots, largely due to the underutilization of tactile feedback. To address this, we introduce TouchGuide, a novel cross-policy visuo-tactile fusion paradigm that fuses modalities within a low-dimensional action space. Specifically, TouchGuide operates in two stages to guide a pre-trained diffusion or flow-matching visuomotor policy at inference time. First, the policy produces a coarse, visually-plausible action using only visual inputs during early sampling. Second, a task-specific Contact Physical Model (CPM) provides tactile guidance to steer and refine the action, ensuring it aligns with realistic physical contact conditions. Trained through contrastive learning on limited expert demonstrations, the CPM provides a tactile-informed feasibility score to steer the sampling process toward refined actions that satisfy physical contact constraints. Furthermore, to facilitate TouchGuide training with high-quality and cost-effective data, we introduce TacUMI, a data collection system. TacUMI achieves a favorable trade-off between precision and affordability; by leveraging rigid fingertips, it obtains direct tactile feedback, thereby enabling the collection of reliable tactile data. Extensive experiments on five challenging contact-rich tasks, such as shoe lacing and chip handover, show that TouchGuide consistently and significantly outperforms state-of-the-art visuo-tactile policies."
    }
  ],
  "人机物智能融合实验室 (HCP)": [
    {
      "title": "MARE: Multimodal Alignment and Reinforcement for Explainable Deepfake Detection via Vision-Language Models",
      "url": "https://arxiv.org/abs/2601.20433",
      "date": "2026-01-28",
      "authors_text": "Wenbo Xu, Wei Lu, Xiangyang Luo, Jiantao Zhou",
      "is_highlight": false,
      "score": 76.0,
      "summary": "MARE enhances Deepfake detection using vision-language models through multimodal alignment, reinforcement learning from human feedback, and a forgery disentanglement module, achieving state-of-the-art performance.",
      "teaser_image": "https://arxiv.org/html/2601.20433/x2.png",
      "debug_abstract": "Deepfake detection is a widely researched topic that is crucial for combating the spread of malicious content, with existing methods mainly modeling the problem as classification or spatial localization. The rapid advancements in generative models impose new demands on Deepfake detection. In this paper, we propose multimodal alignment and reinforcement for explainable Deepfake detection via vision-language models, termed MARE, which aims to enhance the accuracy and reliability of Vision-Language Models (VLMs) in Deepfake detection and reasoning. Specifically, MARE designs comprehensive reward functions, incorporating reinforcement learning from human feedback (RLHF), to incentivize the generation of text-spatially aligned reasoning content that adheres to human preferences. Besides, MARE introduces a forgery disentanglement module to capture intrinsic forgery traces from high-level facial semantics, thereby improving its authenticity detection capability. We conduct thorough evaluations on the reasoning content generated by MARE. Both quantitative and qualitative experimental results demonstrate that MARE achieves state-of-the-art performance in terms of accuracy and reliability."
    },
    {
      "title": "TouchGuide: Inference-Time Steering of Visuomotor Policies via Touch Guidance",
      "url": "https://arxiv.org/abs/2601.20239",
      "date": "2026-01-28",
      "authors_text": "Zhemeng Zhang, Jiahua Ma, Xincheng Yang, Xin Wen, Yuzhi Zhang",
      "is_highlight": false,
      "score": 90.0,
      "summary": "TouchGuide enhances robot manipulation by integrating tactile feedback with visual inputs to refine actions during inference, outperforming existing visuo-tactile policies in complex tasks.",
      "teaser_image": "https://arxiv.org/html/2601.20239/x2.png",
      "debug_abstract": "Fine-grained and contact-rich manipulation remain challenging for robots, largely due to the underutilization of tactile feedback. To address this, we introduce TouchGuide, a novel cross-policy visuo-tactile fusion paradigm that fuses modalities within a low-dimensional action space. Specifically, TouchGuide operates in two stages to guide a pre-trained diffusion or flow-matching visuomotor policy at inference time. First, the policy produces a coarse, visually-plausible action using only visual inputs during early sampling. Second, a task-specific Contact Physical Model (CPM) provides tactile guidance to steer and refine the action, ensuring it aligns with realistic physical contact conditions. Trained through contrastive learning on limited expert demonstrations, the CPM provides a tactile-informed feasibility score to steer the sampling process toward refined actions that satisfy physical contact constraints. Furthermore, to facilitate TouchGuide training with high-quality and cost-effective data, we introduce TacUMI, a data collection system. TacUMI achieves a favorable trade-off between precision and affordability; by leveraging rigid fingertips, it obtains direct tactile feedback, thereby enabling the collection of reliable tactile data. Extensive experiments on five challenging contact-rich tasks, such as shoe lacing and chip handover, show that TouchGuide consistently and significantly outperforms state-of-the-art visuo-tactile policies."
    }
  ],
  "数字媒体与计算机视觉实验室": [
    {
      "title": "TouchGuide: Inference-Time Steering of Visuomotor Policies via Touch Guidance",
      "url": "https://arxiv.org/abs/2601.20239",
      "date": "2026-01-28",
      "authors_text": "Zhemeng Zhang, Jiahua Ma, Xincheng Yang, Xin Wen, Yuzhi Zhang",
      "is_highlight": false,
      "score": 90.0,
      "summary": "TouchGuide enhances robot manipulation by integrating tactile feedback with visual inputs to refine actions during inference, outperforming existing visuo-tactile policies in complex tasks.",
      "teaser_image": "https://arxiv.org/html/2601.20239/x2.png",
      "debug_abstract": "Fine-grained and contact-rich manipulation remain challenging for robots, largely due to the underutilization of tactile feedback. To address this, we introduce TouchGuide, a novel cross-policy visuo-tactile fusion paradigm that fuses modalities within a low-dimensional action space. Specifically, TouchGuide operates in two stages to guide a pre-trained diffusion or flow-matching visuomotor policy at inference time. First, the policy produces a coarse, visually-plausible action using only visual inputs during early sampling. Second, a task-specific Contact Physical Model (CPM) provides tactile guidance to steer and refine the action, ensuring it aligns with realistic physical contact conditions. Trained through contrastive learning on limited expert demonstrations, the CPM provides a tactile-informed feasibility score to steer the sampling process toward refined actions that satisfy physical contact constraints. Furthermore, to facilitate TouchGuide training with high-quality and cost-effective data, we introduce TacUMI, a data collection system. TacUMI achieves a favorable trade-off between precision and affordability; by leveraging rigid fingertips, it obtains direct tactile feedback, thereby enabling the collection of reliable tactile data. Extensive experiments on five challenging contact-rich tasks, such as shoe lacing and chip handover, show that TouchGuide consistently and significantly outperforms state-of-the-art visuo-tactile policies."
    }
  ],
  "机器视觉与智能学习实验室 (MVIG)": [
    {
      "title": "TouchGuide: Inference-Time Steering of Visuomotor Policies via Touch Guidance",
      "url": "https://arxiv.org/abs/2601.20239",
      "date": "2026-01-28",
      "authors_text": "Zhemeng Zhang, Jiahua Ma, Xincheng Yang, Xin Wen, Yuzhi Zhang",
      "is_highlight": false,
      "score": 90.0,
      "summary": "TouchGuide enhances robot manipulation by integrating tactile feedback with visual inputs to refine actions during inference, outperforming existing visuo-tactile policies in complex tasks.",
      "teaser_image": "https://arxiv.org/html/2601.20239/x2.png",
      "debug_abstract": "Fine-grained and contact-rich manipulation remain challenging for robots, largely due to the underutilization of tactile feedback. To address this, we introduce TouchGuide, a novel cross-policy visuo-tactile fusion paradigm that fuses modalities within a low-dimensional action space. Specifically, TouchGuide operates in two stages to guide a pre-trained diffusion or flow-matching visuomotor policy at inference time. First, the policy produces a coarse, visually-plausible action using only visual inputs during early sampling. Second, a task-specific Contact Physical Model (CPM) provides tactile guidance to steer and refine the action, ensuring it aligns with realistic physical contact conditions. Trained through contrastive learning on limited expert demonstrations, the CPM provides a tactile-informed feasibility score to steer the sampling process toward refined actions that satisfy physical contact constraints. Furthermore, to facilitate TouchGuide training with high-quality and cost-effective data, we introduce TacUMI, a data collection system. TacUMI achieves a favorable trade-off between precision and affordability; by leveraging rigid fingertips, it obtains direct tactile feedback, thereby enabling the collection of reliable tactile data. Extensive experiments on five challenging contact-rich tasks, such as shoe lacing and chip handover, show that TouchGuide consistently and significantly outperforms state-of-the-art visuo-tactile policies."
    }
  ],
  "ReThinkLab": [
    {
      "title": "TouchGuide: Inference-Time Steering of Visuomotor Policies via Touch Guidance",
      "url": "https://arxiv.org/abs/2601.20239",
      "date": "2026-01-28",
      "authors_text": "Zhemeng Zhang, Jiahua Ma, Xincheng Yang, Xin Wen, Yuzhi Zhang",
      "is_highlight": false,
      "score": 90.0,
      "summary": "TouchGuide enhances robot manipulation by integrating tactile feedback with visual inputs to refine actions during inference, outperforming existing visuo-tactile policies in complex tasks.",
      "teaser_image": "https://arxiv.org/html/2601.20239/x2.png",
      "debug_abstract": "Fine-grained and contact-rich manipulation remain challenging for robots, largely due to the underutilization of tactile feedback. To address this, we introduce TouchGuide, a novel cross-policy visuo-tactile fusion paradigm that fuses modalities within a low-dimensional action space. Specifically, TouchGuide operates in two stages to guide a pre-trained diffusion or flow-matching visuomotor policy at inference time. First, the policy produces a coarse, visually-plausible action using only visual inputs during early sampling. Second, a task-specific Contact Physical Model (CPM) provides tactile guidance to steer and refine the action, ensuring it aligns with realistic physical contact conditions. Trained through contrastive learning on limited expert demonstrations, the CPM provides a tactile-informed feasibility score to steer the sampling process toward refined actions that satisfy physical contact constraints. Furthermore, to facilitate TouchGuide training with high-quality and cost-effective data, we introduce TacUMI, a data collection system. TacUMI achieves a favorable trade-off between precision and affordability; by leveraging rigid fingertips, it obtains direct tactile feedback, thereby enabling the collection of reliable tactile data. Extensive experiments on five challenging contact-rich tasks, such as shoe lacing and chip handover, show that TouchGuide consistently and significantly outperforms state-of-the-art visuo-tactile policies."
    }
  ],
  "智能机器人与机器视觉(IRMV)实验室": [
    {
      "title": "TouchGuide: Inference-Time Steering of Visuomotor Policies via Touch Guidance",
      "url": "https://arxiv.org/abs/2601.20239",
      "date": "2026-01-28",
      "authors_text": "Zhemeng Zhang, Jiahua Ma, Xincheng Yang, Xin Wen, Yuzhi Zhang",
      "is_highlight": false,
      "score": 90.0,
      "summary": "TouchGuide enhances robot manipulation by integrating tactile feedback with visual inputs to refine actions during inference, outperforming existing visuo-tactile policies in complex tasks.",
      "teaser_image": "https://arxiv.org/html/2601.20239/x2.png",
      "debug_abstract": "Fine-grained and contact-rich manipulation remain challenging for robots, largely due to the underutilization of tactile feedback. To address this, we introduce TouchGuide, a novel cross-policy visuo-tactile fusion paradigm that fuses modalities within a low-dimensional action space. Specifically, TouchGuide operates in two stages to guide a pre-trained diffusion or flow-matching visuomotor policy at inference time. First, the policy produces a coarse, visually-plausible action using only visual inputs during early sampling. Second, a task-specific Contact Physical Model (CPM) provides tactile guidance to steer and refine the action, ensuring it aligns with realistic physical contact conditions. Trained through contrastive learning on limited expert demonstrations, the CPM provides a tactile-informed feasibility score to steer the sampling process toward refined actions that satisfy physical contact constraints. Furthermore, to facilitate TouchGuide training with high-quality and cost-effective data, we introduce TacUMI, a data collection system. TacUMI achieves a favorable trade-off between precision and affordability; by leveraging rigid fingertips, it obtains direct tactile feedback, thereby enabling the collection of reliable tactile data. Extensive experiments on five challenging contact-rich tasks, such as shoe lacing and chip handover, show that TouchGuide consistently and significantly outperforms state-of-the-art visuo-tactile policies."
    }
  ],
  "IWIN-FINS实验室": [
    {
      "title": "TouchGuide: Inference-Time Steering of Visuomotor Policies via Touch Guidance",
      "url": "https://arxiv.org/abs/2601.20239",
      "date": "2026-01-28",
      "authors_text": "Zhemeng Zhang, Jiahua Ma, Xincheng Yang, Xin Wen, Yuzhi Zhang",
      "is_highlight": false,
      "score": 90.0,
      "summary": "TouchGuide enhances robot manipulation by integrating tactile feedback with visual inputs to refine actions during inference, outperforming existing visuo-tactile policies in complex tasks.",
      "teaser_image": "https://arxiv.org/html/2601.20239/x2.png",
      "debug_abstract": "Fine-grained and contact-rich manipulation remain challenging for robots, largely due to the underutilization of tactile feedback. To address this, we introduce TouchGuide, a novel cross-policy visuo-tactile fusion paradigm that fuses modalities within a low-dimensional action space. Specifically, TouchGuide operates in two stages to guide a pre-trained diffusion or flow-matching visuomotor policy at inference time. First, the policy produces a coarse, visually-plausible action using only visual inputs during early sampling. Second, a task-specific Contact Physical Model (CPM) provides tactile guidance to steer and refine the action, ensuring it aligns with realistic physical contact conditions. Trained through contrastive learning on limited expert demonstrations, the CPM provides a tactile-informed feasibility score to steer the sampling process toward refined actions that satisfy physical contact constraints. Furthermore, to facilitate TouchGuide training with high-quality and cost-effective data, we introduce TacUMI, a data collection system. TacUMI achieves a favorable trade-off between precision and affordability; by leveraging rigid fingertips, it obtains direct tactile feedback, thereby enabling the collection of reliable tactile data. Extensive experiments on five challenging contact-rich tasks, such as shoe lacing and chip handover, show that TouchGuide consistently and significantly outperforms state-of-the-art visuo-tactile policies."
    }
  ],
  "上海科技大学": [
    {
      "title": "Tactile-Force Alignment in Vision-Language-Action Models for Force-aware Manipulation",
      "url": "https://arxiv.org/abs/2601.20321",
      "date": "2026-01-28",
      "authors_text": "Yuzhe Huang, Pei Lin, Wanlin Li, Daohan Li, Jiajun Li",
      "is_highlight": false,
      "score": 91.0,
      "summary": "The paper introduces TaF-VLA, a framework that enhances Vision-Language-Action models by integrating tactile-force alignment for improved force-aware robotic manipulation in contact-rich tasks.",
      "teaser_image": "https://arxiv.org/html/2601.20321/figures/fig1-teaser.png",
      "debug_abstract": "Vision-Language-Action (VLA) models have recently emerged as powerful generalists for robotic manipulation. However, due to their predominant reliance on visual modalities, they fundamentally lack the physical intuition required for contact-rich tasks that require precise force regulation and physical reasoning. Existing attempts to incorporate vision-based tactile sensing into VLA models typically treat tactile inputs as auxiliary visual textures, thereby overlooking the underlying correlation between surface deformation and interaction dynamics. To bridge this gap, we propose a paradigm shift from tactile-vision alignment to tactile-force alignment. Here, we introduce TaF-VLA, a framework that explicitly grounds high-dimensional tactile observations in physical interaction forces. To facilitate this, we develop an automated tactile-force data acquisition device and curate the TaF-Dataset, comprising over 10 million synchronized tactile observations, 6-axis force/torque, and matrix force map. To align sequential tactile observations with interaction forces, the central component of our approach is the Tactile-Force Adapter (TaF-Adapter), a tactile sensor encoder that extracts discretized latent information for encoding tactile observations. This mechanism ensures that the learned representations capture history-dependent, noise-insensitive physical dynamics rather than static visual textures. Finally, we integrate this force-aligned encoder into a VLA backbone. Extensive real-world experiments demonstrate that TaF-VLA policy significantly outperforms state-of-the-art tactile-vision-aligned and vision-only baselines on contact-rich tasks, verifying its ability to achieve robust, force-aware manipulation through cross-modal physical reasoning."
    }
  ],
  "北京航空航天大学": [
    {
      "title": "GPO: Growing Policy Optimization for Legged Robot Locomotion and Whole-Body Control",
      "url": "https://arxiv.org/abs/2601.20668",
      "date": "2026-01-28",
      "authors_text": "Shuhao Liao, Peizhuo Li, Xinrong Yang, Linnan Chang, Zhaoxin Fan",
      "is_highlight": false,
      "score": 92.0,
      "summary": "The paper presents Growing Policy Optimization (GPO), a framework that enhances reinforcement learning for legged robots by progressively expanding the action space to improve data collection and policy performance.",
      "teaser_image": "https://arxiv.org/html/2601.20668/pic/framework.png",
      "debug_abstract": "Training reinforcement learning (RL) policies for legged robots remains challenging due to high-dimensional continuous actions, hardware constraints, and limited exploration. Existing methods for locomotion and whole-body control work well for position-based control with environment-specific heuristics (e.g., reward shaping, curriculum design, and manual initialization), but are less effective for torque-based control, where sufficiently exploring the action space and obtaining informative gradient signals for training is significantly more difficult. We introduce Growing Policy Optimization (GPO), a training framework that applies a time-varying action transformation to restrict the effective action space in the early stage, thereby encouraging more effective data collection and policy learning, and then progressively expands it to enhance exploration and achieve higher expected return. We prove that this transformation preserves the PPO update rule and introduces only bounded, vanishing gradient distortion, thereby ensuring stable training. We evaluate GPO on both quadruped and hexapod robots, including zero-shot deployment of simulation-trained policies on hardware. Policies trained with GPO consistently achieve better performance. These results suggest that GPO provides a general, environment-agnostic optimization framework for learning legged locomotion."
    },
    {
      "title": "GVGS: Gaussian Visibility-Aware Multi-View Geometry for Accurate Surface Reconstruction",
      "url": "https://arxiv.org/abs/2601.20331",
      "date": "2026-01-28",
      "authors_text": "Mai Su, Qihan Yu, Zhongtao Wang, Yilong Li, Chengwei Pan",
      "is_highlight": true,
      "score": 60.0,
      "summary": "The paper presents GVGS, a method enhancing 3D surface reconstruction accuracy by integrating Gaussian visibility-aware constraints and progressive monocular depth calibration to overcome existing limitations.",
      "teaser_image": "https://arxiv.org/html/2601.20331/x2.png",
      "debug_abstract": "3D Gaussian Splatting enables efficient optimization and high-quality rendering, yet accurate surface reconstruction remains challenging. Prior methods improve surface reconstruction by refining Gaussian depth estimates, either via multi-view geometric consistency or through monocular depth priors. However, multi-view constraints become unreliable under large geometric discrepancies, while monocular priors suffer from scale ambiguity and local inconsistency, ultimately leading to inaccurate Gaussian depth supervision. To address these limitations, we introduce a Gaussian visibility-aware multi-view geometric consistency constraint that aggregates the visibility of shared Gaussian primitives across views, enabling more accurate and stable geometric supervision. In addition, we propose a progressive quadtree-calibrated Monocular depth constraint that performs block-wise affine calibration from coarse to fine spatial scales, mitigating the scale ambiguity of depth priors while preserving fine-grained surface details. Extensive experiments on DTU and TNT datasets demonstrate consistent improvements in geometric accuracy over prior Gaussian-based and implicit surface reconstruction methods. Codes are available at an anonymous repository: this https URL."
    },
    {
      "title": "Tactile-Force Alignment in Vision-Language-Action Models for Force-aware Manipulation",
      "url": "https://arxiv.org/abs/2601.20321",
      "date": "2026-01-28",
      "authors_text": "Yuzhe Huang, Pei Lin, Wanlin Li, Daohan Li, Jiajun Li",
      "is_highlight": false,
      "score": 91.0,
      "summary": "The paper introduces TaF-VLA, a framework that enhances Vision-Language-Action models by integrating tactile-force alignment for improved force-aware robotic manipulation in contact-rich tasks.",
      "teaser_image": "https://arxiv.org/html/2601.20321/figures/fig1-teaser.png",
      "debug_abstract": "Vision-Language-Action (VLA) models have recently emerged as powerful generalists for robotic manipulation. However, due to their predominant reliance on visual modalities, they fundamentally lack the physical intuition required for contact-rich tasks that require precise force regulation and physical reasoning. Existing attempts to incorporate vision-based tactile sensing into VLA models typically treat tactile inputs as auxiliary visual textures, thereby overlooking the underlying correlation between surface deformation and interaction dynamics. To bridge this gap, we propose a paradigm shift from tactile-vision alignment to tactile-force alignment. Here, we introduce TaF-VLA, a framework that explicitly grounds high-dimensional tactile observations in physical interaction forces. To facilitate this, we develop an automated tactile-force data acquisition device and curate the TaF-Dataset, comprising over 10 million synchronized tactile observations, 6-axis force/torque, and matrix force map. To align sequential tactile observations with interaction forces, the central component of our approach is the Tactile-Force Adapter (TaF-Adapter), a tactile sensor encoder that extracts discretized latent information for encoding tactile observations. This mechanism ensures that the learned representations capture history-dependent, noise-insensitive physical dynamics rather than static visual textures. Finally, we integrate this force-aligned encoder into a VLA backbone. Extensive real-world experiments demonstrate that TaF-VLA policy significantly outperforms state-of-the-art tactile-vision-aligned and vision-only baselines on contact-rich tasks, verifying its ability to achieve robust, force-aware manipulation through cross-modal physical reasoning."
    }
  ],
  "具身感知与交互实验室 (EPIC Lab)": [
    {
      "title": "GVGS: Gaussian Visibility-Aware Multi-View Geometry for Accurate Surface Reconstruction",
      "url": "https://arxiv.org/abs/2601.20331",
      "date": "2026-01-28",
      "authors_text": "Mai Su, Qihan Yu, Zhongtao Wang, Yilong Li, Chengwei Pan",
      "is_highlight": true,
      "score": 60.0,
      "summary": "The paper presents GVGS, a method enhancing 3D surface reconstruction accuracy by integrating Gaussian visibility-aware constraints and progressive monocular depth calibration to overcome existing limitations.",
      "teaser_image": "https://arxiv.org/html/2601.20331/x2.png",
      "debug_abstract": "3D Gaussian Splatting enables efficient optimization and high-quality rendering, yet accurate surface reconstruction remains challenging. Prior methods improve surface reconstruction by refining Gaussian depth estimates, either via multi-view geometric consistency or through monocular depth priors. However, multi-view constraints become unreliable under large geometric discrepancies, while monocular priors suffer from scale ambiguity and local inconsistency, ultimately leading to inaccurate Gaussian depth supervision. To address these limitations, we introduce a Gaussian visibility-aware multi-view geometric consistency constraint that aggregates the visibility of shared Gaussian primitives across views, enabling more accurate and stable geometric supervision. In addition, we propose a progressive quadtree-calibrated Monocular depth constraint that performs block-wise affine calibration from coarse to fine spatial scales, mitigating the scale ambiguity of depth priors while preserving fine-grained surface details. Extensive experiments on DTU and TNT datasets demonstrate consistent improvements in geometric accuracy over prior Gaussian-based and implicit surface reconstruction methods. Codes are available at an anonymous repository: this https URL."
    }
  ],
  "HMI Lab": [
    {
      "title": "GVGS: Gaussian Visibility-Aware Multi-View Geometry for Accurate Surface Reconstruction",
      "url": "https://arxiv.org/abs/2601.20331",
      "date": "2026-01-28",
      "authors_text": "Mai Su, Qihan Yu, Zhongtao Wang, Yilong Li, Chengwei Pan",
      "is_highlight": true,
      "score": 60.0,
      "summary": "The paper presents GVGS, a method enhancing 3D surface reconstruction accuracy by integrating Gaussian visibility-aware constraints and progressive monocular depth calibration to overcome existing limitations.",
      "teaser_image": "https://arxiv.org/html/2601.20331/x2.png",
      "debug_abstract": "3D Gaussian Splatting enables efficient optimization and high-quality rendering, yet accurate surface reconstruction remains challenging. Prior methods improve surface reconstruction by refining Gaussian depth estimates, either via multi-view geometric consistency or through monocular depth priors. However, multi-view constraints become unreliable under large geometric discrepancies, while monocular priors suffer from scale ambiguity and local inconsistency, ultimately leading to inaccurate Gaussian depth supervision. To address these limitations, we introduce a Gaussian visibility-aware multi-view geometric consistency constraint that aggregates the visibility of shared Gaussian primitives across views, enabling more accurate and stable geometric supervision. In addition, we propose a progressive quadtree-calibrated Monocular depth constraint that performs block-wise affine calibration from coarse to fine spatial scales, mitigating the scale ambiguity of depth priors while preserving fine-grained surface details. Extensive experiments on DTU and TNT datasets demonstrate consistent improvements in geometric accuracy over prior Gaussian-based and implicit surface reconstruction methods. Codes are available at an anonymous repository: this https URL."
    }
  ],
  "情感与认知智能机器人实验室 (ACIR)": [
    {
      "title": "GVGS: Gaussian Visibility-Aware Multi-View Geometry for Accurate Surface Reconstruction",
      "url": "https://arxiv.org/abs/2601.20331",
      "date": "2026-01-28",
      "authors_text": "Mai Su, Qihan Yu, Zhongtao Wang, Yilong Li, Chengwei Pan",
      "is_highlight": true,
      "score": 60.0,
      "summary": "The paper presents GVGS, a method enhancing 3D surface reconstruction accuracy by integrating Gaussian visibility-aware constraints and progressive monocular depth calibration to overcome existing limitations.",
      "teaser_image": "https://arxiv.org/html/2601.20331/x2.png",
      "debug_abstract": "3D Gaussian Splatting enables efficient optimization and high-quality rendering, yet accurate surface reconstruction remains challenging. Prior methods improve surface reconstruction by refining Gaussian depth estimates, either via multi-view geometric consistency or through monocular depth priors. However, multi-view constraints become unreliable under large geometric discrepancies, while monocular priors suffer from scale ambiguity and local inconsistency, ultimately leading to inaccurate Gaussian depth supervision. To address these limitations, we introduce a Gaussian visibility-aware multi-view geometric consistency constraint that aggregates the visibility of shared Gaussian primitives across views, enabling more accurate and stable geometric supervision. In addition, we propose a progressive quadtree-calibrated Monocular depth constraint that performs block-wise affine calibration from coarse to fine spatial scales, mitigating the scale ambiguity of depth priors while preserving fine-grained surface details. Extensive experiments on DTU and TNT datasets demonstrate consistent improvements in geometric accuracy over prior Gaussian-based and implicit surface reconstruction methods. Codes are available at an anonymous repository: this https URL."
    }
  ],
  "智元机器人联合实验室 (PKU-Agibot)": [
    {
      "title": "GVGS: Gaussian Visibility-Aware Multi-View Geometry for Accurate Surface Reconstruction",
      "url": "https://arxiv.org/abs/2601.20331",
      "date": "2026-01-28",
      "authors_text": "Mai Su, Qihan Yu, Zhongtao Wang, Yilong Li, Chengwei Pan",
      "is_highlight": true,
      "score": 60.0,
      "summary": "The paper presents GVGS, a method enhancing 3D surface reconstruction accuracy by integrating Gaussian visibility-aware constraints and progressive monocular depth calibration to overcome existing limitations.",
      "teaser_image": "https://arxiv.org/html/2601.20331/x2.png",
      "debug_abstract": "3D Gaussian Splatting enables efficient optimization and high-quality rendering, yet accurate surface reconstruction remains challenging. Prior methods improve surface reconstruction by refining Gaussian depth estimates, either via multi-view geometric consistency or through monocular depth priors. However, multi-view constraints become unreliable under large geometric discrepancies, while monocular priors suffer from scale ambiguity and local inconsistency, ultimately leading to inaccurate Gaussian depth supervision. To address these limitations, we introduce a Gaussian visibility-aware multi-view geometric consistency constraint that aggregates the visibility of shared Gaussian primitives across views, enabling more accurate and stable geometric supervision. In addition, we propose a progressive quadtree-calibrated Monocular depth constraint that performs block-wise affine calibration from coarse to fine spatial scales, mitigating the scale ambiguity of depth priors while preserving fine-grained surface details. Extensive experiments on DTU and TNT datasets demonstrate consistent improvements in geometric accuracy over prior Gaussian-based and implicit surface reconstruction methods. Codes are available at an anonymous repository: this https URL."
    }
  ],
  "浙江大学": [
    {
      "title": "Policy of Thoughts: Scaling LLM Reasoning via Test-time Policy Evolution",
      "url": "https://arxiv.org/abs/2601.20379",
      "date": "2026-01-28",
      "authors_text": "Zhengbo Jiao, Hongyu Xian, Qinglong Wang, Yunpu Ma, Zhebo Wang",
      "is_highlight": false,
      "score": 74.0,
      "summary": "The Policy of Thoughts framework enhances LLM reasoning by enabling real-time policy evolution through execution feedback, significantly improving performance on complex tasks.",
      "teaser_image": "https://arxiv.org/html/2601.20379/x1.png",
      "debug_abstract": "Large language models (LLMs) struggle with complex, long-horizon reasoning due to instability caused by their frozen policy assumption. Current test-time scaling methods treat execution feedback merely as an external signal for filtering or rewriting trajectories, without internalizing it to improve the underlying reasoning strategy. Inspired by Popper&#39;s epistemology of &#34;conjectures and refutations,&#34; we argue that intelligence requires real-time evolution of the model&#39;s policy through learning from failed attempts. We introduce Policy of Thoughts (PoT), a framework that recasts reasoning as a within-instance online optimization process. PoT first generates diverse candidate solutions via an efficient exploration mechanism, then uses Group Relative Policy Optimization (GRPO) to update a transient LoRA adapter based on execution feedback. This closed-loop design enables dynamic, instance-specific refinement of the model&#39;s reasoning priors. Experiments show that PoT dramatically boosts performance: a 4B model achieves 49.71% accuracy on LiveCodeBench, outperforming GPT-4o and DeepSeek-V3 despite being over 50 smaller."
    }
  ],
  "具身智能实验室": [
    {
      "title": "MeCo: Enhancing LLM-Empowered Multi-Robot Collaboration via Similar Task Memoization",
      "url": "https://arxiv.org/abs/2601.20577",
      "date": "2026-01-28",
      "authors_text": "Baiqing Wang, Helei Cui, Bo Zhang, Xiaolong Zheng, Bin Guo",
      "is_highlight": false,
      "score": 85.0,
      "summary": "MeCo enhances multi-robot collaboration by utilizing memoization to efficiently reuse solutions for similar tasks, reducing planning costs and improving success rates.",
      "teaser_image": "https://arxiv.org/html/2601.20577/x2.png",
      "debug_abstract": "Multi-robot systems have been widely deployed in real-world applications, providing significant improvements in efficiency and reductions in labor costs. However, most existing multi-robot collaboration methods rely on extensive task-specific training, which limits their adaptability to new or diverse scenarios. Recent research leverages the language understanding and reasoning capabilities of large language models (LLMs) to enable more flexible collaboration without specialized training. Yet, current LLM-empowered approaches remain inefficient: when confronted with identical or similar tasks, they must replan from scratch because they omit task-level similarities. To address this limitation, we propose MeCo, a similarity-aware multi-robot collaboration framework that applies the principle of ``cache and reuse&#39;&#39; (a.k.a., memoization) to reduce redundant computation. Unlike simple task repetition, identifying and reusing solutions for similar but not identical tasks is far more challenging, particularly in multi-robot settings. To this end, MeCo introduces a new similarity testing method that retrieves previously solved tasks with high relevance, enabling effective plan reuse without re-invoking LLMs. Furthermore, we present MeCoBench, the first benchmark designed to evaluate performance on similar-task collaboration scenarios. Experimental results show that MeCo substantially reduces planning costs and improves success rates compared with state-of-the-art approaches."
    }
  ],
  "Rajpurkar Lab": [
    {
      "title": "Investigating the Development of Task-Oriented Communication in Vision-Language Models",
      "url": "https://arxiv.org/abs/2601.20641",
      "date": "2026-01-28",
      "authors_text": "Boaz Carmeli, Orr Paradise, Shafi Goldwasser, Yonatan Belinkov, Ron Meir",
      "is_highlight": false,
      "score": 73.0,
      "summary": "The study reveals that LLM-based agents can create efficient, covert communication protocols in collaborative tasks, raising transparency concerns while demonstrating spontaneous coordination.",
      "teaser_image": "https://arxiv.org/html/2601.20641/x2.png",
      "debug_abstract": "We investigate whether \\emph{LLM-based agents} can develop task-oriented communication protocols that differ from standard natural language in collaborative reasoning tasks. Our focus is on two core properties such task-oriented protocols may exhibit: Efficiency -- conveying task-relevant information more concisely than natural language, and Covertness -- becoming difficult for external observers to interpret, raising concerns about transparency and control. To investigate these aspects, we use a referential-game framework in which vision-language model (VLM) agents communicate, providing a controlled, measurable setting for evaluating language variants. Experiments show that VLMs can develop effective, task-adapted communication patterns. At the same time, they can develop covert protocols that are difficult for humans and external agents to interpret. We also observe spontaneous coordination between similar models without explicitly shared protocols. These findings highlight both the potential and the risks of task-oriented communication, and position referential games as a valuable testbed for future work in this area."
    }
  ],
  "Harvard Microrobotics Laboratory": [
    {
      "title": "Investigating the Development of Task-Oriented Communication in Vision-Language Models",
      "url": "https://arxiv.org/abs/2601.20641",
      "date": "2026-01-28",
      "authors_text": "Boaz Carmeli, Orr Paradise, Shafi Goldwasser, Yonatan Belinkov, Ron Meir",
      "is_highlight": false,
      "score": 73.0,
      "summary": "The study reveals that LLM-based agents can create efficient, covert communication protocols in collaborative tasks, raising transparency concerns while demonstrating spontaneous coordination.",
      "teaser_image": "https://arxiv.org/html/2601.20641/x2.png",
      "debug_abstract": "We investigate whether \\emph{LLM-based agents} can develop task-oriented communication protocols that differ from standard natural language in collaborative reasoning tasks. Our focus is on two core properties such task-oriented protocols may exhibit: Efficiency -- conveying task-relevant information more concisely than natural language, and Covertness -- becoming difficult for external observers to interpret, raising concerns about transparency and control. To investigate these aspects, we use a referential-game framework in which vision-language model (VLM) agents communicate, providing a controlled, measurable setting for evaluating language variants. Experiments show that VLMs can develop effective, task-adapted communication patterns. At the same time, they can develop covert protocols that are difficult for humans and external agents to interpret. We also observe spontaneous coordination between similar models without explicitly shared protocols. These findings highlight both the potential and the risks of task-oriented communication, and position referential games as a valuable testbed for future work in this area."
    }
  ],
  "机器人研究所": [
    {
      "title": "Endogenous Reprompting: Self-Evolving Cognitive Alignment for Unified Multimodal Models",
      "url": "https://arxiv.org/abs/2601.20305",
      "date": "2026-01-28",
      "authors_text": "Zhenchen Tang, Songlin Yang, Zichuan Wang, Bo Peng, Yang Li",
      "is_highlight": false,
      "score": 69.0,
      "summary": "The paper introduces Endogenous Reprompting and the SEER framework, enhancing Unified Multimodal Models' generative reasoning through self-aligned descriptors and reinforcement learning, outperforming existing methods.",
      "teaser_image": "https://arxiv.org/html/2601.20305/x2.png",
      "debug_abstract": "Unified Multimodal Models (UMMs) exhibit strong understanding, yet this capability often fails to effectively guide generation. We identify this as a Cognitive Gap: the model lacks the understanding of how to enhance its own generation process. To bridge this gap, we propose Endogenous Reprompting, a mechanism that transforms the model&#39;s understanding from a passive encoding process into an explicit generative reasoning step by generating self-aligned descriptors during generation. To achieve this, we introduce SEER (Self-Evolving Evaluator and Reprompter), a training framework that establishes a two-stage endogenous loop using only 300 samples from a compact proxy task, Visual Instruction Elaboration. First, Reinforcement Learning with Verifiable Rewards (RLVR) activates the model&#39;s latent evaluation ability via curriculum learning, producing a high-fidelity endogenous reward signal. Second, Reinforcement Learning with Model-rewarded Thinking (RLMT) leverages this signal to optimize the generative reasoning policy. Experiments show that SEER consistently outperforms state-of-the-art baselines in evaluation accuracy, reprompting efficiency, and generation quality, without sacrificing general multimodal capabilities."
    }
  ],
  "Precognition Lab": [
    {
      "title": "Endogenous Reprompting: Self-Evolving Cognitive Alignment for Unified Multimodal Models",
      "url": "https://arxiv.org/abs/2601.20305",
      "date": "2026-01-28",
      "authors_text": "Zhenchen Tang, Songlin Yang, Zichuan Wang, Bo Peng, Yang Li",
      "is_highlight": false,
      "score": 69.0,
      "summary": "The paper introduces Endogenous Reprompting and the SEER framework, enhancing Unified Multimodal Models' generative reasoning through self-aligned descriptors and reinforcement learning, outperforming existing methods.",
      "teaser_image": "https://arxiv.org/html/2601.20305/x2.png",
      "debug_abstract": "Unified Multimodal Models (UMMs) exhibit strong understanding, yet this capability often fails to effectively guide generation. We identify this as a Cognitive Gap: the model lacks the understanding of how to enhance its own generation process. To bridge this gap, we propose Endogenous Reprompting, a mechanism that transforms the model&#39;s understanding from a passive encoding process into an explicit generative reasoning step by generating self-aligned descriptors during generation. To achieve this, we introduce SEER (Self-Evolving Evaluator and Reprompter), a training framework that establishes a two-stage endogenous loop using only 300 samples from a compact proxy task, Visual Instruction Elaboration. First, Reinforcement Learning with Verifiable Rewards (RLVR) activates the model&#39;s latent evaluation ability via curriculum learning, producing a high-fidelity endogenous reward signal. Second, Reinforcement Learning with Model-rewarded Thinking (RLMT) leverages this signal to optimize the generative reasoning policy. Experiments show that SEER consistently outperforms state-of-the-art baselines in evaluation accuracy, reprompting efficiency, and generation quality, without sacrificing general multimodal capabilities."
    }
  ],
  "机器人研究所 (CKSRI)": [
    {
      "title": "Endogenous Reprompting: Self-Evolving Cognitive Alignment for Unified Multimodal Models",
      "url": "https://arxiv.org/abs/2601.20305",
      "date": "2026-01-28",
      "authors_text": "Zhenchen Tang, Songlin Yang, Zichuan Wang, Bo Peng, Yang Li",
      "is_highlight": false,
      "score": 69.0,
      "summary": "The paper introduces Endogenous Reprompting and the SEER framework, enhancing Unified Multimodal Models' generative reasoning through self-aligned descriptors and reinforcement learning, outperforming existing methods.",
      "teaser_image": "https://arxiv.org/html/2601.20305/x2.png",
      "debug_abstract": "Unified Multimodal Models (UMMs) exhibit strong understanding, yet this capability often fails to effectively guide generation. We identify this as a Cognitive Gap: the model lacks the understanding of how to enhance its own generation process. To bridge this gap, we propose Endogenous Reprompting, a mechanism that transforms the model&#39;s understanding from a passive encoding process into an explicit generative reasoning step by generating self-aligned descriptors during generation. To achieve this, we introduce SEER (Self-Evolving Evaluator and Reprompter), a training framework that establishes a two-stage endogenous loop using only 300 samples from a compact proxy task, Visual Instruction Elaboration. First, Reinforcement Learning with Verifiable Rewards (RLVR) activates the model&#39;s latent evaluation ability via curriculum learning, producing a high-fidelity endogenous reward signal. Second, Reinforcement Learning with Model-rewarded Thinking (RLMT) leverages this signal to optimize the generative reasoning policy. Experiments show that SEER consistently outperforms state-of-the-art baselines in evaluation accuracy, reprompting efficiency, and generation quality, without sacrificing general multimodal capabilities."
    }
  ],
  "Cheng Kar-Shun Robotics Institute (CKSRI)": [
    {
      "title": "Endogenous Reprompting: Self-Evolving Cognitive Alignment for Unified Multimodal Models",
      "url": "https://arxiv.org/abs/2601.20305",
      "date": "2026-01-28",
      "authors_text": "Zhenchen Tang, Songlin Yang, Zichuan Wang, Bo Peng, Yang Li",
      "is_highlight": false,
      "score": 69.0,
      "summary": "The paper introduces Endogenous Reprompting and the SEER framework, enhancing Unified Multimodal Models' generative reasoning through self-aligned descriptors and reinforcement learning, outperforming existing methods.",
      "teaser_image": "https://arxiv.org/html/2601.20305/x2.png",
      "debug_abstract": "Unified Multimodal Models (UMMs) exhibit strong understanding, yet this capability often fails to effectively guide generation. We identify this as a Cognitive Gap: the model lacks the understanding of how to enhance its own generation process. To bridge this gap, we propose Endogenous Reprompting, a mechanism that transforms the model&#39;s understanding from a passive encoding process into an explicit generative reasoning step by generating self-aligned descriptors during generation. To achieve this, we introduce SEER (Self-Evolving Evaluator and Reprompter), a training framework that establishes a two-stage endogenous loop using only 300 samples from a compact proxy task, Visual Instruction Elaboration. First, Reinforcement Learning with Verifiable Rewards (RLVR) activates the model&#39;s latent evaluation ability via curriculum learning, producing a high-fidelity endogenous reward signal. Second, Reinforcement Learning with Model-rewarded Thinking (RLMT) leverages this signal to optimize the generative reasoning policy. Experiments show that SEER consistently outperforms state-of-the-art baselines in evaluation accuracy, reprompting efficiency, and generation quality, without sacrificing general multimodal capabilities."
    }
  ],
  "Jun MA老师实验室": [
    {
      "title": "Endogenous Reprompting: Self-Evolving Cognitive Alignment for Unified Multimodal Models",
      "url": "https://arxiv.org/abs/2601.20305",
      "date": "2026-01-28",
      "authors_text": "Zhenchen Tang, Songlin Yang, Zichuan Wang, Bo Peng, Yang Li",
      "is_highlight": false,
      "score": 69.0,
      "summary": "The paper introduces Endogenous Reprompting and the SEER framework, enhancing Unified Multimodal Models' generative reasoning through self-aligned descriptors and reinforcement learning, outperforming existing methods.",
      "teaser_image": "https://arxiv.org/html/2601.20305/x2.png",
      "debug_abstract": "Unified Multimodal Models (UMMs) exhibit strong understanding, yet this capability often fails to effectively guide generation. We identify this as a Cognitive Gap: the model lacks the understanding of how to enhance its own generation process. To bridge this gap, we propose Endogenous Reprompting, a mechanism that transforms the model&#39;s understanding from a passive encoding process into an explicit generative reasoning step by generating self-aligned descriptors during generation. To achieve this, we introduce SEER (Self-Evolving Evaluator and Reprompter), a training framework that establishes a two-stage endogenous loop using only 300 samples from a compact proxy task, Visual Instruction Elaboration. First, Reinforcement Learning with Verifiable Rewards (RLVR) activates the model&#39;s latent evaluation ability via curriculum learning, producing a high-fidelity endogenous reward signal. Second, Reinforcement Learning with Model-rewarded Thinking (RLMT) leverages this signal to optimize the generative reasoning policy. Experiments show that SEER consistently outperforms state-of-the-art baselines in evaluation accuracy, reprompting efficiency, and generation quality, without sacrificing general multimodal capabilities."
    }
  ],
  "范明明老师实验室": [
    {
      "title": "Endogenous Reprompting: Self-Evolving Cognitive Alignment for Unified Multimodal Models",
      "url": "https://arxiv.org/abs/2601.20305",
      "date": "2026-01-28",
      "authors_text": "Zhenchen Tang, Songlin Yang, Zichuan Wang, Bo Peng, Yang Li",
      "is_highlight": false,
      "score": 69.0,
      "summary": "The paper introduces Endogenous Reprompting and the SEER framework, enhancing Unified Multimodal Models' generative reasoning through self-aligned descriptors and reinforcement learning, outperforming existing methods.",
      "teaser_image": "https://arxiv.org/html/2601.20305/x2.png",
      "debug_abstract": "Unified Multimodal Models (UMMs) exhibit strong understanding, yet this capability often fails to effectively guide generation. We identify this as a Cognitive Gap: the model lacks the understanding of how to enhance its own generation process. To bridge this gap, we propose Endogenous Reprompting, a mechanism that transforms the model&#39;s understanding from a passive encoding process into an explicit generative reasoning step by generating self-aligned descriptors during generation. To achieve this, we introduce SEER (Self-Evolving Evaluator and Reprompter), a training framework that establishes a two-stage endogenous loop using only 300 samples from a compact proxy task, Visual Instruction Elaboration. First, Reinforcement Learning with Verifiable Rewards (RLVR) activates the model&#39;s latent evaluation ability via curriculum learning, producing a high-fidelity endogenous reward signal. Second, Reinforcement Learning with Model-rewarded Thinking (RLMT) leverages this signal to optimize the generative reasoning policy. Experiments show that SEER consistently outperforms state-of-the-art baselines in evaluation accuracy, reprompting efficiency, and generation quality, without sacrificing general multimodal capabilities."
    }
  ],
  "Bio-Inspired Robotics Laboratory (BIRL)": [
    {
      "title": "Quartet of Diffusions: Structure-Aware Point Cloud Generation through Part and Symmetry Guidance",
      "url": "https://arxiv.org/abs/2601.20425",
      "date": "2026-01-28",
      "authors_text": "Chenliang Zhou, Fangcheng Zhong, Weihao Xia, Albert Miao, Canberk Baykal",
      "is_highlight": false,
      "score": 66.0,
      "summary": "The Quartet of Diffusions framework enables structure-aware point cloud generation by integrating symmetry and part composition through four coordinated diffusion models, achieving state-of-the-art results.",
      "teaser_image": "https://arxiv.org/html/2601.20425/figs/overview.png",
      "debug_abstract": "We introduce the Quartet of Diffusions, a structure-aware point cloud generation framework that explicitly models part composition and symmetry. Unlike prior methods that treat shape generation as a holistic process or only support part composition, our approach leverages four coordinated diffusion models to learn distributions of global shape latents, symmetries, semantic parts, and their spatial assembly. This structured pipeline ensures guaranteed symmetry, coherent part placement, and diverse, high-quality outputs. By disentangling the generative process into interpretable components, our method supports fine-grained control over shape attributes, enabling targeted manipulation of individual parts while preserving global consistency. A central global latent further reinforces structural coherence across assembled parts. Our experiments show that the Quartet achieves state-of-the-art performance. To our best knowledge, this is the first 3D point cloud generation framework that fully integrates and enforces both symmetry and part priors throughout the generative process."
    }
  ],
  "Affective Intelligence and Robotics Laboratory (AFAR)": [
    {
      "title": "Quartet of Diffusions: Structure-Aware Point Cloud Generation through Part and Symmetry Guidance",
      "url": "https://arxiv.org/abs/2601.20425",
      "date": "2026-01-28",
      "authors_text": "Chenliang Zhou, Fangcheng Zhong, Weihao Xia, Albert Miao, Canberk Baykal",
      "is_highlight": false,
      "score": 66.0,
      "summary": "The Quartet of Diffusions framework enables structure-aware point cloud generation by integrating symmetry and part composition through four coordinated diffusion models, achieving state-of-the-art results.",
      "teaser_image": "https://arxiv.org/html/2601.20425/figs/overview.png",
      "debug_abstract": "We introduce the Quartet of Diffusions, a structure-aware point cloud generation framework that explicitly models part composition and symmetry. Unlike prior methods that treat shape generation as a holistic process or only support part composition, our approach leverages four coordinated diffusion models to learn distributions of global shape latents, symmetries, semantic parts, and their spatial assembly. This structured pipeline ensures guaranteed symmetry, coherent part placement, and diverse, high-quality outputs. By disentangling the generative process into interpretable components, our method supports fine-grained control over shape attributes, enabling targeted manipulation of individual parts while preserving global consistency. A central global latent further reinforces structural coherence across assembled parts. Our experiments show that the Quartet achieves state-of-the-art performance. To our best knowledge, this is the first 3D point cloud generation framework that fully integrates and enforces both symmetry and part priors throughout the generative process."
    }
  ]
}