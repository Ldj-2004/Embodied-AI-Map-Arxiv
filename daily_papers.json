{
  "卡内基-机器人研究所": [
    {
      "title": "Mirage2Matter: A Physically Grounded Gaussian World Model from Video",
      "url": "https://arxiv.org/abs/2602.00096",
      "date": "2026-01-24",
      "authors_text": "Zhengqing Gao, Ziwen Li, Xin Wang, Jiaxin Huang, Zhenyang Ren",
      "is_highlight": false,
      "score": 85.0,
      "summary": "The paper presents Mirage2Matter, a framework that generates high-fidelity embodied training data from videos, enabling scalable and practical world modeling for embodied intelligence.",
      "teaser_image": "https://arxiv.org/html/2602.00096/figures/teaser.png",
      "debug_abstract": "The scalability of embodied intelligence is fundamentally constrained by the scarcity of real-world interaction data. While simulation platforms provide a promising alternative, existing approaches often suffer from a substantial visual and physical gap to real environments and rely on expensive sensors, precise robot calibration, or depth measurements, limiting their practicality at scale. We present Simulate Anything, a graphics-driven world modeling and simulation framework that enables efficient generation of high-fidelity embodied training data using only multi-view environment videos and off-the-shelf assets. Our approach reconstructs real-world environments into a photorealistic scene representation using 3D Gaussian Splatting (3DGS), seamlessly capturing fine-grained geometry and appearance from video. We then leverage generative models to recover a physically realistic representation and integrate it into a simulation environment via a precision calibration target, enabling accurate scale alignment between the reconstructed scene and the real world. Together, these components provide a unified, editable, and physically grounded world model. Vision Language Action (VLA) models trained on our simulated data achieve strong zero-shot performance on downstream tasks, matching or even surpassing results obtained with real-world data, highlighting the potential of reconstruction-driven world modeling for scalable and practical embodied intelligence training."
    }
  ],
  "具身视觉与机器人实验室 (EVAR Lab)": [
    {
      "title": "Enhancing Indoor Occupancy Prediction via Sparse Query-Based Multi-Level Consistent Knowledge Distillation",
      "url": "https://arxiv.org/abs/2602.02318",
      "date": "2026-02-02",
      "authors_text": "Xiang Li, Yupeng Zheng, Pengfei Li, Yilun Chen, Ya-Qin Zhang",
      "is_highlight": false,
      "score": 0,
      "summary": "DiScene introduces a sparse query-based framework for efficient and robust indoor occupancy prediction through multi-level knowledge distillation, achieving state-of-the-art performance and faster inference.",
      "teaser_image": "https://arxiv.org/html/2602.02318/x2.png",
      "debug_abstract": "Occupancy prediction provides critical geometric and semantic understanding for robotics but faces efficiency-accuracy trade-offs. Current dense methods suffer computational waste on empty voxels, while sparse query-based approaches lack robustness in diverse and complex indoor scenes. In this paper, we propose DiScene, a novel sparse query-based framework that leverages multi-level distillation to achieve efficient and robust occupancy prediction. In particular, our method incorporates two key innovations: (1) a Multi-level Consistent Knowledge Distillation strategy, which transfers hierarchical representations from large teacher models to lightweight students through coordinated alignment across four levels, including encoder-level feature alignment, query-level feature matching, prior-level spatial guidance, and anchor-level high-confidence knowledge transfer and (2) a Teacher-Guided Initialization policy, employing optimized parameter warm-up to accelerate model convergence. Validated on the Occ-Scannet benchmark, DiScene achieves 23.2 FPS without depth priors while outperforming our baseline method, OPUS, by 36.1% and even better than the depth-enhanced version, OPUS†. With depth integration, DiScene† attains new SOTA performance, surpassing EmbodiedOcc by 3.7% with 1.62$\\times$ faster inference speed. Furthermore, experiments on the Occ3D-nuScenes benchmark and in-the-wild scenarios demonstrate the versatility of our approach in various environments. Code and models can be accessed at this https URL."
    },
    {
      "title": "UV-M3TL: A Unified and Versatile Multimodal Multi-Task Learning Framework for Assistive Driving Perception",
      "url": "https://arxiv.org/abs/2602.01594",
      "date": "2026-02-02",
      "authors_text": "Wenzhuo Liu, Qiannan Guo, Zhen Wang, Wenshuo Wang, Lei Yang",
      "is_highlight": false,
      "score": 8.0,
      "summary": "The UV-M3TL framework enhances ADAS by effectively recognizing driver behavior and context through dual-branch multimodal embedding and adaptive loss, achieving state-of-the-art performance across multiple tasks.",
      "teaser_image": "https://arxiv.org/html/2602.01594/x2.png",
      "debug_abstract": "Advanced Driver Assistance Systems (ADAS) need to understand human driver behavior while perceiving their navigation context, but jointly learning these heterogeneous tasks would cause inter-task negative transfer and impair system performance. Here, we propose a Unified and Versatile Multimodal Multi-Task Learning (UV-M3TL) framework to simultaneously recognize driver behavior, driver emotion, vehicle behavior, and traffic context, while mitigating inter-task negative transfer. Our framework incorporates two core components: dual-branch spatial channel multimodal embedding (DB-SCME) and adaptive feature-decoupled multi-task loss (AFD-Loss). DB-SCME enhances cross-task knowledge transfer while mitigating task conflicts by employing a dual-branch structure to explicitly model salient task-shared and task-specific features. AFD-Loss improves the stability of joint optimization while guiding the model to learn diverse multi-task representations by introducing an adaptive weighting mechanism based on learning dynamics and feature decoupling constraints. We evaluate our method on the AIDE dataset, and the experimental results demonstrate that UV-M3TL achieves state-of-the-art performance across all four tasks. To further prove the versatility, we evaluate UV-M3TL on additional public multi-task perception benchmarks (BDD100K, CityScapes, NYUD-v2, and PASCAL-Context), where it consistently delivers strong performance across diverse task combinations, attaining state-of-the-art results on most tasks."
    },
    {
      "title": "Context Learning for Multi-Agent Discussion",
      "url": "https://arxiv.org/abs/2602.02350",
      "date": "2026-02-02",
      "authors_text": "Xingyuan Hua, Sheng Yue, Xinyi Li, Yizhe Zhao, Jinrui Zhang",
      "is_highlight": false,
      "score": 17.0,
      "summary": "The paper presents M2CL, a multi-LLM context learning method that enhances coherence in Multi-Agent Discussions, improving problem-solving performance by 20%-50% over existing approaches.",
      "teaser_image": "https://arxiv.org/html/2602.02350/x2.png",
      "debug_abstract": "Multi-Agent Discussion (MAD) has garnered increasing attention very recently, where multiple LLM instances collaboratively solve problems via structured discussion. However, we find that current MAD methods easily suffer from discussion inconsistency, LLMs fail to reach a coherent solution, due to the misalignment between their individual this http URL this paper, we introduce a multi-LLM context learning method (M2CL) that learns a context generator for each agent, capable of dynamically generating context instructions per discussion round via automatic information organization and refinement. Specifically, inspired by our theoretical insights on the context instruction, M2CL train the generators to control context coherence and output discrepancies via a carefully crafted self-adaptive this http URL enables LLMs to avoid premature convergence on majority noise and progressively reach the correct consensus. We evaluate M2CL on challenging tasks, including academic reasoning, embodied tasks, and mobile control. The results show that the performance of M2CL significantly surpasses existing methods by 20%--50%, while enjoying favorable transferability and computational efficiency."
    },
    {
      "title": "TTT-Parkour: Rapid Test-Time Training for Perceptive Robot Parkour",
      "url": "https://arxiv.org/abs/2602.02331",
      "date": "2026-02-02",
      "authors_text": "Shaoting Zhu, Baijun Ye, Jiaxuan Wang, Jiakang Chen, Ziwen Zhuang",
      "is_highlight": false,
      "score": 18.0,
      "summary": "TTT-Parkour introduces a rapid test-time training framework enabling humanoid robots to effectively navigate complex terrains through efficient real-to-sim-to-real learning and geometry reconstruction.",
      "teaser_image": "https://arxiv.org/html/2602.02331/x1.png",
      "debug_abstract": "Achieving highly dynamic humanoid parkour on unseen, complex terrains remains a challenge in robotics. Although general locomotion policies demonstrate capabilities across broad terrain distributions, they often struggle with arbitrary and highly challenging environments. To overcome this limitation, we propose a real-to-sim-to-real framework that leverages rapid test-time training (TTT) on novel terrains, significantly enhancing the robot&#39;s capability to traverse extremely difficult geometries. We adopt a two-stage end-to-end learning paradigm: a policy is first pre-trained on diverse procedurally generated terrains, followed by rapid fine-tuning on high-fidelity meshes reconstructed from real-world captures. Specifically, we develop a feed-forward, efficient, and high-fidelity geometry reconstruction pipeline using RGB-D inputs, ensuring both speed and quality during test-time training. We demonstrate that TTT-Parkour empowers humanoid robots to master complex obstacles, including wedges, stakes, boxes, trapezoids, and narrow beams. The whole pipeline of capturing, reconstructing, and test-time training requires less than 10 minutes on most tested terrains. Extensive experiments show that the policy after test-time training exhibits robust zero-shot sim-to-real transfer capability."
    },
    {
      "title": "Small Generalizable Prompt Predictive Models Can Steer Efficient RL Post-Training of Large Reasoning Models",
      "url": "https://arxiv.org/abs/2602.01970",
      "date": "2026-02-02",
      "authors_text": "Yun Qu, Qi Wang, Yixiu Mao, Heming Zou, Yuhang Jiang",
      "is_highlight": false,
      "score": 26.0,
      "summary": "The paper presents Generalizable Predictive Prompt Selection (GPS), a method that enhances reinforcement learning efficiency in large language models by utilizing a lightweight generative model for prompt selection.",
      "teaser_image": "https://arxiv.org/html/2602.01970/x1.png",
      "debug_abstract": "Reinforcement learning enhances the reasoning capabilities of large language models but often involves high computational costs due to rollout-intensive optimization. Online prompt selection presents a plausible solution by prioritizing informative prompts to improve training efficiency. However, current methods either depend on costly, exact evaluations or construct prompt-specific predictive models lacking generalization across prompts. This study introduces Generalizable Predictive Prompt Selection (GPS), which performs Bayesian inference towards prompt difficulty using a lightweight generative model trained on the shared optimization history. Intermediate-difficulty prioritization and history-anchored diversity are incorporated into the batch acquisition principle to select informative prompt batches. The small predictive model also generalizes at test-time for efficient computational allocation. Experiments across varied reasoning benchmarks indicate GPS&#39;s substantial improvements in training efficiency, final performance, and test-time efficiency over superior baseline methods."
    },
    {
      "title": "ForSim: Stepwise Forward Simulation for Traffic Policy Fine-Tuning",
      "url": "https://arxiv.org/abs/2602.01916",
      "date": "2026-02-02",
      "authors_text": "Keyu Chen, Wenchao Sun, Hao Cheng, Zheng Fu, Sifa Zheng",
      "is_highlight": true,
      "score": 29.0,
      "summary": "ForSim introduces a stepwise closed-loop simulation method that enhances traffic policy fine-tuning by improving agent interactions and preserving multimodal behaviors in autonomous driving.",
      "teaser_image": "https://arxiv.org/html/2602.01916/x2.png",
      "debug_abstract": "As the foundation of closed-loop training and evaluation in autonomous driving, traffic simulation still faces two fundamental challenges: covariate shift introduced by open-loop imitation learning and limited capacity to reflect the multimodal behaviors observed in real-world traffic. Although recent frameworks such as RIFT have partially addressed these issues through group-relative optimization, their forward simulation procedures remain largely non-reactive, leading to unrealistic agent interactions within the virtual domain and ultimately limiting simulation fidelity. To address these issues, we propose ForSim, a stepwise closed-loop forward simulation paradigm. At each virtual timestep, the traffic agent propagates the virtual candidate trajectory that best spatiotemporally matches the reference trajectory through physically grounded motion dynamics, thereby preserving multimodal behavioral diversity while ensuring intra-modality consistency. Other agents are updated with stepwise predictions, yielding coherent and interaction-aware evolution. When incorporated into the RIFT traffic simulation framework, ForSim operates in conjunction with group-relative optimization to fine-tune traffic policy. Extensive experiments confirm that this integration consistently improves safety while maintaining efficiency, realism, and comfort. These results underscore the importance of modeling closed-loop multimodal interactions within forward simulation and enhance the fidelity and reliability of traffic simulation for autonomous driving. Project Page: this https URL"
    },
    {
      "title": "From Frames to Sequences: Temporally Consistent Human-Centric Dense Prediction",
      "url": "https://arxiv.org/abs/2602.01661",
      "date": "2026-02-02",
      "authors_text": "Xingyu Miao, Junting Dong, Qin Zhao, Yuhang Yang, Junhao Chen",
      "is_highlight": false,
      "score": 43.0,
      "summary": "This paper presents a novel synthetic data pipeline and a ViT-based model for achieving temporally consistent human-centric dense prediction in video sequences, outperforming existing methods.",
      "teaser_image": "https://arxiv.org/html/2602.01661/x2.png",
      "debug_abstract": "In this work, we focus on the challenge of temporally consistent human-centric dense prediction across video sequences. Existing models achieve strong per-frame accuracy but often flicker under motion, occlusion, and lighting changes, and they rarely have paired human video supervision for multiple dense tasks. We address this gap with a scalable synthetic data pipeline that generates photorealistic human frames and motion-aligned sequences with pixel-accurate depth, normals, and masks. Unlike prior static data synthetic pipelines, our pipeline provides both frame-level labels for spatial learning and sequence-level supervision for temporal learning. Building on this, we train a unified ViT-based dense predictor that (i) injects an explicit human geometric prior via CSE embeddings and (ii) improves geometry-feature reliability with a lightweight channel reweighting module after feature fusion. Our two-stage training strategy, combining static pretraining with dynamic sequence supervision, enables the model first to acquire robust spatial representations and then refine temporal consistency across motion-aligned sequences. Extensive experiments show that we achieve state-of-the-art performance on THuman2.1 and Hi4D and generalize effectively to in-the-wild videos."
    },
    {
      "title": "Reasoning with Autoregressive-Diffusion Collaborative Thoughts",
      "url": "https://arxiv.org/abs/2602.01608",
      "date": "2026-02-02",
      "authors_text": "Mu Yuan, Liekang Zeng, Guoliang Xing, Lan Zhang, Yunhao Liu",
      "is_highlight": false,
      "score": 41.0,
      "summary": "The paper presents Collaborative Thoughts, a framework integrating autoregressive and diffusion models for enhanced spatial reasoning and generation through iterative feedback and joint reasoning.",
      "teaser_image": "https://arxiv.org/html/2602.01608/x2.png",
      "debug_abstract": "Autoregressive and diffusion models represent two complementary generative paradigms. Autoregressive models excel at sequential planning and constraint composition, yet struggle with tasks that require explicit spatial or physical grounding. Diffusion models, in contrast, capture rich spatial structure through high-dimensional generation, but lack the stepwise logical control needed to satisfy complex, multi-stage constraints or to reliably identify and correct errors. We introduce Collaborative Thoughts, a unified collaborative framework that enables autoregressive and diffusion models to reason and generate jointly through a closed-loop interaction. In Collaborative Thoughts, autoregressive models perform structured planning and constraint management, diffusion models instantiate these constraints as intermediate visual thoughts, and a vision-based critic module evaluates whether the visual thoughts satisfy the intended structural and physical requirements. This feedback is then used to iteratively refine subsequent planning and generation steps, mitigating error propagation across modalities. Importantly, Collaborative Thoughts uses the same collaborative loop regardless of whether the task is autoregressive question answering or diffusion-based visual generation. Through representative examples, we illustrate how Collaborative Thoughts can improve the reliability of spatial reasoning and the controllability of generation."
    },
    {
      "title": "Making Avatars Interact: Towards Text-Driven Human-Object Interaction for Controllable Talking Avatars",
      "url": "https://arxiv.org/abs/2602.01538",
      "date": "2026-02-02",
      "authors_text": "Youliang Zhang, Zhengguang Zhou, Zhentao Yu, Ziyao Huang, Teng Hu",
      "is_highlight": false,
      "score": 46.0,
      "summary": "The paper presents InteractAvatar, a dual-stream framework for generating talking avatars that perform text-driven human-object interactions by decoupling perception and planning from video synthesis.",
      "teaser_image": "https://arxiv.org/html/2602.01538/x1.png",
      "debug_abstract": "Generating talking avatars is a fundamental task in video generation. Although existing methods can generate full-body talking avatars with simple human motion, extending this task to grounded human-object interaction (GHOI) remains an open challenge, requiring the avatar to perform text-aligned interactions with surrounding objects. This challenge stems from the need for environmental perception and the control-quality dilemma in GHOI generation. To address this, we propose a novel dual-stream framework, InteractAvatar, which decouples perception and planning from video synthesis for grounded human-object interaction. Leveraging detection to enhance environmental perception, we introduce a Perception and Interaction Module (PIM) to generate text-aligned interaction motions. Additionally, an Audio-Interaction Aware Generation Module (AIM) is proposed to synthesize vivid talking avatars performing object interactions. With a specially designed motion-to-video aligner, PIM and AIM share a similar network structure and enable parallel co-generation of motions and plausible videos, effectively mitigating the control-quality dilemma. Finally, we establish a benchmark, GroundedInter, for evaluating GHOI video generation. Extensive experiments and comparisons demonstrate the effectiveness of our method in generating grounded human-object interactions for talking avatars. Project page: this https URL"
    },
    {
      "title": "Resource-Efficient Reinforcement for Reasoning Large Language Models via Dynamic One-Shot Policy Refinement",
      "url": "https://arxiv.org/abs/2602.00815",
      "date": "2026-01-31",
      "authors_text": "Yunjian Zhang, Sudong Wang, Yang Li, Peiran Xu, Conghao Zhou",
      "is_highlight": false,
      "score": 57.0,
      "summary": "The paper introduces Dynamic One-Shot Policy Refinement (DoPR), a resource-efficient reinforcement learning strategy that enhances reasoning in large language models while significantly reducing training costs.",
      "teaser_image": "https://arxiv.org/html/2602.00815/dopr.png",
      "debug_abstract": "Large language models (LLMs) have exhibited remarkable performance on complex reasoning tasks, with reinforcement learning under verifiable rewards (RLVR) emerging as a principled framework for aligning model behavior with reasoning chains. Despite its promise, RLVR remains prohibitively resource-intensive, requiring extensive reward signals and incurring substantial rollout costs during training. In this work, we revisit the fundamental question of data and compute efficiency in RLVR. We first establish a theoretical lower bound on the sample complexity required to unlock reasoning capabilities, and empirically validate that strong performance can be achieved with a surprisingly small number of training instances. To tackle the computational burden, we propose Dynamic One-Shot Policy Refinement (DoPR), an uncertainty-aware RL strategy that dynamically selects a single informative training sample per batch for policy updates, guided by reward volatility and exploration-driven acquisition. DoPR reduces rollout overhead by nearly an order of magnitude while preserving competitive reasoning accuracy, offering a scalable and resource-efficient solution for LLM post-training. This approach offers a practical path toward more efficient and accessible RL-based training for reasoning-intensive LLM applications."
    },
    {
      "title": "V2X-DSC: Multi-Agent Collaborative Perception with Distributed Source Coding Guided Communication",
      "url": "https://arxiv.org/abs/2602.00687",
      "date": "2026-01-31",
      "authors_text": "Yuankun Zeng, Shaohui Li, Zhi Li, Shulan Ruan, Yu Liu",
      "is_highlight": false,
      "score": 76.0,
      "summary": "V2X-DSC introduces a Conditional Codec for efficient multi-agent collaborative perception by compressing and reconstructing BEV features, optimizing bandwidth usage while maintaining accuracy.",
      "teaser_image": "https://arxiv.org/html/2602.00687/figs/overview.jpg",
      "debug_abstract": "Collaborative perception improves 3D understanding by fusing multi-agent observations, yet intermediate-feature sharing faces strict bandwidth constraints as dense BEV features saturate V2X links. We observe that collaborators view the same physical world, making their features strongly correlated; thus receivers only need innovation beyond their local context. Revisiting this from a distributed source coding perspective, we propose V2X-DSC, a framework with a Conditional Codec (DCC) for bandwidth-constrained fusion. The sender compresses BEV features into compact codes, while the receiver performs conditional reconstruction using its local features as side information, allocating bits to complementary cues rather than redundant content. This conditional structure regularizes learning, encouraging incremental representation and yielding lower-noise features. Experiments on DAIR-V2X, OPV2V, and V2X-Real demonstrate state-of-the-art accuracy-bandwidth trade-offs under KB-level communication, and generalizes as a plug-and-play communication layer across multiple fusion backbones."
    },
    {
      "title": "How Far Are LLMs from Professional Poker Players? Revisiting Game-Theoretic Reasoning with Agentic Tool Use",
      "url": "https://arxiv.org/abs/2602.00528",
      "date": "2026-01-31",
      "authors_text": "Minhua Lin, Enyan Dai, Hui Liu, Xianfeng Tang, Yuliang Yan",
      "is_highlight": false,
      "score": 55.0,
      "summary": "This paper evaluates LLMs in poker, revealing their strategic reasoning flaws and proposing ToolPoker, which enhances gameplay and reasoning through integrated external solvers.",
      "teaser_image": "https://arxiv.org/html/2602.00528/x2.png",
      "debug_abstract": "As Large Language Models (LLMs) are increasingly applied in high-stakes domains, their ability to reason strategically under uncertainty becomes critical. Poker provides a rigorous testbed, requiring not only strong actions but also principled, game-theoretic reasoning. In this paper, we conduct a systematic study of LLMs in multiple realistic poker tasks, evaluating both gameplay outcomes and reasoning traces. Our analysis reveals LLMs fail to compete against traditional algorithms and identifies three recurring flaws: reliance on heuristics, factual misunderstandings, and a &#34;knowing-doing&#34; gap where actions diverge from reasoning. An initial attempt with behavior cloning and step-level reinforcement learning improves reasoning style but remains insufficient for accurate game-theoretic play. Motivated by these limitations, we propose ToolPoker, a tool-integrated reasoning framework that combines external solvers for GTO-consistent actions with more precise professional-style explanations. Experiments demonstrate that ToolPoker achieves state-of-the-art gameplay while producing reasoning traces that closely reflect game-theoretic principles."
    },
    {
      "title": "Learning Physics-Grounded 4D Dynamics with Neural Gaussian Force Fields",
      "url": "https://arxiv.org/abs/2602.00148",
      "date": "2026-01-29",
      "authors_text": "Shiqian Li, Ruihong Shen, Junfeng Ni, Chang Pan, Chi Zhang",
      "is_highlight": false,
      "score": 82.0,
      "summary": "The Neural Gaussian Force Field (NGFF) framework enables rapid generation of interactive, physically realistic 4D videos from multi-view RGB inputs, enhancing robustness and generalization in physical dynamics.",
      "teaser_image": "https://arxiv.org/html/2602.00148/x1.png",
      "debug_abstract": "Predicting physical dynamics from raw visual data remains a major challenge in AI. While recent video generation models have achieved impressive visual quality, they still cannot consistently generate physically plausible videos due to a lack of modeling of physical laws. Recent approaches combining 3D Gaussian splatting and physics engines can produce physically plausible videos, but are hindered by high computational costs in both reconstruction and simulation, and often lack robustness in complex real-world scenarios. To address these issues, we introduce Neural Gaussian Force Field (NGFF), an end-to-end neural framework that integrates 3D Gaussian perception with physics-based dynamic modeling to generate interactive, physically realistic 4D videos from multi-view RGB inputs, achieving two orders of magnitude faster than prior Gaussian simulators. To support training, we also present GSCollision, a 4D Gaussian dataset featuring diverse materials, multi-object interactions, and complex scenes, totaling over 640k rendered physical videos (~4 TB). Evaluations on synthetic and real 3D scenarios show NGFF&#39;s strong generalization and robustness in physical reasoning, advancing video prediction towards physics-grounded world models."
    }
  ],
  "智能技术与系统实验室": [
    {
      "title": "Enhancing Indoor Occupancy Prediction via Sparse Query-Based Multi-Level Consistent Knowledge Distillation",
      "url": "https://arxiv.org/abs/2602.02318",
      "date": "2026-02-02",
      "authors_text": "Xiang Li, Yupeng Zheng, Pengfei Li, Yilun Chen, Ya-Qin Zhang",
      "is_highlight": false,
      "score": 0,
      "summary": "DiScene introduces a sparse query-based framework for efficient and robust indoor occupancy prediction through multi-level knowledge distillation, achieving state-of-the-art performance and faster inference.",
      "teaser_image": "https://arxiv.org/html/2602.02318/x2.png",
      "debug_abstract": "Occupancy prediction provides critical geometric and semantic understanding for robotics but faces efficiency-accuracy trade-offs. Current dense methods suffer computational waste on empty voxels, while sparse query-based approaches lack robustness in diverse and complex indoor scenes. In this paper, we propose DiScene, a novel sparse query-based framework that leverages multi-level distillation to achieve efficient and robust occupancy prediction. In particular, our method incorporates two key innovations: (1) a Multi-level Consistent Knowledge Distillation strategy, which transfers hierarchical representations from large teacher models to lightweight students through coordinated alignment across four levels, including encoder-level feature alignment, query-level feature matching, prior-level spatial guidance, and anchor-level high-confidence knowledge transfer and (2) a Teacher-Guided Initialization policy, employing optimized parameter warm-up to accelerate model convergence. Validated on the Occ-Scannet benchmark, DiScene achieves 23.2 FPS without depth priors while outperforming our baseline method, OPUS, by 36.1% and even better than the depth-enhanced version, OPUS†. With depth integration, DiScene† attains new SOTA performance, surpassing EmbodiedOcc by 3.7% with 1.62$\\times$ faster inference speed. Furthermore, experiments on the Occ3D-nuScenes benchmark and in-the-wild scenarios demonstrate the versatility of our approach in various environments. Code and models can be accessed at this https URL."
    },
    {
      "title": "UV-M3TL: A Unified and Versatile Multimodal Multi-Task Learning Framework for Assistive Driving Perception",
      "url": "https://arxiv.org/abs/2602.01594",
      "date": "2026-02-02",
      "authors_text": "Wenzhuo Liu, Qiannan Guo, Zhen Wang, Wenshuo Wang, Lei Yang",
      "is_highlight": false,
      "score": 8.0,
      "summary": "The UV-M3TL framework enhances ADAS by effectively recognizing driver behavior and context through dual-branch multimodal embedding and adaptive loss, achieving state-of-the-art performance across multiple tasks.",
      "teaser_image": "https://arxiv.org/html/2602.01594/x2.png",
      "debug_abstract": "Advanced Driver Assistance Systems (ADAS) need to understand human driver behavior while perceiving their navigation context, but jointly learning these heterogeneous tasks would cause inter-task negative transfer and impair system performance. Here, we propose a Unified and Versatile Multimodal Multi-Task Learning (UV-M3TL) framework to simultaneously recognize driver behavior, driver emotion, vehicle behavior, and traffic context, while mitigating inter-task negative transfer. Our framework incorporates two core components: dual-branch spatial channel multimodal embedding (DB-SCME) and adaptive feature-decoupled multi-task loss (AFD-Loss). DB-SCME enhances cross-task knowledge transfer while mitigating task conflicts by employing a dual-branch structure to explicitly model salient task-shared and task-specific features. AFD-Loss improves the stability of joint optimization while guiding the model to learn diverse multi-task representations by introducing an adaptive weighting mechanism based on learning dynamics and feature decoupling constraints. We evaluate our method on the AIDE dataset, and the experimental results demonstrate that UV-M3TL achieves state-of-the-art performance across all four tasks. To further prove the versatility, we evaluate UV-M3TL on additional public multi-task perception benchmarks (BDD100K, CityScapes, NYUD-v2, and PASCAL-Context), where it consistently delivers strong performance across diverse task combinations, attaining state-of-the-art results on most tasks."
    },
    {
      "title": "Context Learning for Multi-Agent Discussion",
      "url": "https://arxiv.org/abs/2602.02350",
      "date": "2026-02-02",
      "authors_text": "Xingyuan Hua, Sheng Yue, Xinyi Li, Yizhe Zhao, Jinrui Zhang",
      "is_highlight": false,
      "score": 17.0,
      "summary": "The paper presents M2CL, a multi-LLM context learning method that enhances coherence in Multi-Agent Discussions, improving problem-solving performance by 20%-50% over existing approaches.",
      "teaser_image": "https://arxiv.org/html/2602.02350/x2.png",
      "debug_abstract": "Multi-Agent Discussion (MAD) has garnered increasing attention very recently, where multiple LLM instances collaboratively solve problems via structured discussion. However, we find that current MAD methods easily suffer from discussion inconsistency, LLMs fail to reach a coherent solution, due to the misalignment between their individual this http URL this paper, we introduce a multi-LLM context learning method (M2CL) that learns a context generator for each agent, capable of dynamically generating context instructions per discussion round via automatic information organization and refinement. Specifically, inspired by our theoretical insights on the context instruction, M2CL train the generators to control context coherence and output discrepancies via a carefully crafted self-adaptive this http URL enables LLMs to avoid premature convergence on majority noise and progressively reach the correct consensus. We evaluate M2CL on challenging tasks, including academic reasoning, embodied tasks, and mobile control. The results show that the performance of M2CL significantly surpasses existing methods by 20%--50%, while enjoying favorable transferability and computational efficiency."
    },
    {
      "title": "TTT-Parkour: Rapid Test-Time Training for Perceptive Robot Parkour",
      "url": "https://arxiv.org/abs/2602.02331",
      "date": "2026-02-02",
      "authors_text": "Shaoting Zhu, Baijun Ye, Jiaxuan Wang, Jiakang Chen, Ziwen Zhuang",
      "is_highlight": false,
      "score": 18.0,
      "summary": "TTT-Parkour introduces a rapid test-time training framework enabling humanoid robots to effectively navigate complex terrains through efficient real-to-sim-to-real learning and geometry reconstruction.",
      "teaser_image": "https://arxiv.org/html/2602.02331/x1.png",
      "debug_abstract": "Achieving highly dynamic humanoid parkour on unseen, complex terrains remains a challenge in robotics. Although general locomotion policies demonstrate capabilities across broad terrain distributions, they often struggle with arbitrary and highly challenging environments. To overcome this limitation, we propose a real-to-sim-to-real framework that leverages rapid test-time training (TTT) on novel terrains, significantly enhancing the robot&#39;s capability to traverse extremely difficult geometries. We adopt a two-stage end-to-end learning paradigm: a policy is first pre-trained on diverse procedurally generated terrains, followed by rapid fine-tuning on high-fidelity meshes reconstructed from real-world captures. Specifically, we develop a feed-forward, efficient, and high-fidelity geometry reconstruction pipeline using RGB-D inputs, ensuring both speed and quality during test-time training. We demonstrate that TTT-Parkour empowers humanoid robots to master complex obstacles, including wedges, stakes, boxes, trapezoids, and narrow beams. The whole pipeline of capturing, reconstructing, and test-time training requires less than 10 minutes on most tested terrains. Extensive experiments show that the policy after test-time training exhibits robust zero-shot sim-to-real transfer capability."
    },
    {
      "title": "Small Generalizable Prompt Predictive Models Can Steer Efficient RL Post-Training of Large Reasoning Models",
      "url": "https://arxiv.org/abs/2602.01970",
      "date": "2026-02-02",
      "authors_text": "Yun Qu, Qi Wang, Yixiu Mao, Heming Zou, Yuhang Jiang",
      "is_highlight": false,
      "score": 26.0,
      "summary": "The paper presents Generalizable Predictive Prompt Selection (GPS), a method that enhances reinforcement learning efficiency in large language models by utilizing a lightweight generative model for prompt selection.",
      "teaser_image": "https://arxiv.org/html/2602.01970/x1.png",
      "debug_abstract": "Reinforcement learning enhances the reasoning capabilities of large language models but often involves high computational costs due to rollout-intensive optimization. Online prompt selection presents a plausible solution by prioritizing informative prompts to improve training efficiency. However, current methods either depend on costly, exact evaluations or construct prompt-specific predictive models lacking generalization across prompts. This study introduces Generalizable Predictive Prompt Selection (GPS), which performs Bayesian inference towards prompt difficulty using a lightweight generative model trained on the shared optimization history. Intermediate-difficulty prioritization and history-anchored diversity are incorporated into the batch acquisition principle to select informative prompt batches. The small predictive model also generalizes at test-time for efficient computational allocation. Experiments across varied reasoning benchmarks indicate GPS&#39;s substantial improvements in training efficiency, final performance, and test-time efficiency over superior baseline methods."
    },
    {
      "title": "ForSim: Stepwise Forward Simulation for Traffic Policy Fine-Tuning",
      "url": "https://arxiv.org/abs/2602.01916",
      "date": "2026-02-02",
      "authors_text": "Keyu Chen, Wenchao Sun, Hao Cheng, Zheng Fu, Sifa Zheng",
      "is_highlight": true,
      "score": 29.0,
      "summary": "ForSim introduces a stepwise closed-loop simulation method that enhances traffic policy fine-tuning by improving agent interactions and preserving multimodal behaviors in autonomous driving.",
      "teaser_image": "https://arxiv.org/html/2602.01916/x2.png",
      "debug_abstract": "As the foundation of closed-loop training and evaluation in autonomous driving, traffic simulation still faces two fundamental challenges: covariate shift introduced by open-loop imitation learning and limited capacity to reflect the multimodal behaviors observed in real-world traffic. Although recent frameworks such as RIFT have partially addressed these issues through group-relative optimization, their forward simulation procedures remain largely non-reactive, leading to unrealistic agent interactions within the virtual domain and ultimately limiting simulation fidelity. To address these issues, we propose ForSim, a stepwise closed-loop forward simulation paradigm. At each virtual timestep, the traffic agent propagates the virtual candidate trajectory that best spatiotemporally matches the reference trajectory through physically grounded motion dynamics, thereby preserving multimodal behavioral diversity while ensuring intra-modality consistency. Other agents are updated with stepwise predictions, yielding coherent and interaction-aware evolution. When incorporated into the RIFT traffic simulation framework, ForSim operates in conjunction with group-relative optimization to fine-tune traffic policy. Extensive experiments confirm that this integration consistently improves safety while maintaining efficiency, realism, and comfort. These results underscore the importance of modeling closed-loop multimodal interactions within forward simulation and enhance the fidelity and reliability of traffic simulation for autonomous driving. Project Page: this https URL"
    },
    {
      "title": "From Frames to Sequences: Temporally Consistent Human-Centric Dense Prediction",
      "url": "https://arxiv.org/abs/2602.01661",
      "date": "2026-02-02",
      "authors_text": "Xingyu Miao, Junting Dong, Qin Zhao, Yuhang Yang, Junhao Chen",
      "is_highlight": false,
      "score": 43.0,
      "summary": "This paper presents a novel synthetic data pipeline and a ViT-based model for achieving temporally consistent human-centric dense prediction in video sequences, outperforming existing methods.",
      "teaser_image": "https://arxiv.org/html/2602.01661/x2.png",
      "debug_abstract": "In this work, we focus on the challenge of temporally consistent human-centric dense prediction across video sequences. Existing models achieve strong per-frame accuracy but often flicker under motion, occlusion, and lighting changes, and they rarely have paired human video supervision for multiple dense tasks. We address this gap with a scalable synthetic data pipeline that generates photorealistic human frames and motion-aligned sequences with pixel-accurate depth, normals, and masks. Unlike prior static data synthetic pipelines, our pipeline provides both frame-level labels for spatial learning and sequence-level supervision for temporal learning. Building on this, we train a unified ViT-based dense predictor that (i) injects an explicit human geometric prior via CSE embeddings and (ii) improves geometry-feature reliability with a lightweight channel reweighting module after feature fusion. Our two-stage training strategy, combining static pretraining with dynamic sequence supervision, enables the model first to acquire robust spatial representations and then refine temporal consistency across motion-aligned sequences. Extensive experiments show that we achieve state-of-the-art performance on THuman2.1 and Hi4D and generalize effectively to in-the-wild videos."
    },
    {
      "title": "Reasoning with Autoregressive-Diffusion Collaborative Thoughts",
      "url": "https://arxiv.org/abs/2602.01608",
      "date": "2026-02-02",
      "authors_text": "Mu Yuan, Liekang Zeng, Guoliang Xing, Lan Zhang, Yunhao Liu",
      "is_highlight": false,
      "score": 41.0,
      "summary": "The paper presents Collaborative Thoughts, a framework integrating autoregressive and diffusion models for enhanced spatial reasoning and generation through iterative feedback and joint reasoning.",
      "teaser_image": "https://arxiv.org/html/2602.01608/x2.png",
      "debug_abstract": "Autoregressive and diffusion models represent two complementary generative paradigms. Autoregressive models excel at sequential planning and constraint composition, yet struggle with tasks that require explicit spatial or physical grounding. Diffusion models, in contrast, capture rich spatial structure through high-dimensional generation, but lack the stepwise logical control needed to satisfy complex, multi-stage constraints or to reliably identify and correct errors. We introduce Collaborative Thoughts, a unified collaborative framework that enables autoregressive and diffusion models to reason and generate jointly through a closed-loop interaction. In Collaborative Thoughts, autoregressive models perform structured planning and constraint management, diffusion models instantiate these constraints as intermediate visual thoughts, and a vision-based critic module evaluates whether the visual thoughts satisfy the intended structural and physical requirements. This feedback is then used to iteratively refine subsequent planning and generation steps, mitigating error propagation across modalities. Importantly, Collaborative Thoughts uses the same collaborative loop regardless of whether the task is autoregressive question answering or diffusion-based visual generation. Through representative examples, we illustrate how Collaborative Thoughts can improve the reliability of spatial reasoning and the controllability of generation."
    },
    {
      "title": "Making Avatars Interact: Towards Text-Driven Human-Object Interaction for Controllable Talking Avatars",
      "url": "https://arxiv.org/abs/2602.01538",
      "date": "2026-02-02",
      "authors_text": "Youliang Zhang, Zhengguang Zhou, Zhentao Yu, Ziyao Huang, Teng Hu",
      "is_highlight": false,
      "score": 46.0,
      "summary": "The paper presents InteractAvatar, a dual-stream framework for generating talking avatars that perform text-driven human-object interactions by decoupling perception and planning from video synthesis.",
      "teaser_image": "https://arxiv.org/html/2602.01538/x1.png",
      "debug_abstract": "Generating talking avatars is a fundamental task in video generation. Although existing methods can generate full-body talking avatars with simple human motion, extending this task to grounded human-object interaction (GHOI) remains an open challenge, requiring the avatar to perform text-aligned interactions with surrounding objects. This challenge stems from the need for environmental perception and the control-quality dilemma in GHOI generation. To address this, we propose a novel dual-stream framework, InteractAvatar, which decouples perception and planning from video synthesis for grounded human-object interaction. Leveraging detection to enhance environmental perception, we introduce a Perception and Interaction Module (PIM) to generate text-aligned interaction motions. Additionally, an Audio-Interaction Aware Generation Module (AIM) is proposed to synthesize vivid talking avatars performing object interactions. With a specially designed motion-to-video aligner, PIM and AIM share a similar network structure and enable parallel co-generation of motions and plausible videos, effectively mitigating the control-quality dilemma. Finally, we establish a benchmark, GroundedInter, for evaluating GHOI video generation. Extensive experiments and comparisons demonstrate the effectiveness of our method in generating grounded human-object interactions for talking avatars. Project page: this https URL"
    },
    {
      "title": "Resource-Efficient Reinforcement for Reasoning Large Language Models via Dynamic One-Shot Policy Refinement",
      "url": "https://arxiv.org/abs/2602.00815",
      "date": "2026-01-31",
      "authors_text": "Yunjian Zhang, Sudong Wang, Yang Li, Peiran Xu, Conghao Zhou",
      "is_highlight": false,
      "score": 57.0,
      "summary": "The paper introduces Dynamic One-Shot Policy Refinement (DoPR), a resource-efficient reinforcement learning strategy that enhances reasoning in large language models while significantly reducing training costs.",
      "teaser_image": "https://arxiv.org/html/2602.00815/dopr.png",
      "debug_abstract": "Large language models (LLMs) have exhibited remarkable performance on complex reasoning tasks, with reinforcement learning under verifiable rewards (RLVR) emerging as a principled framework for aligning model behavior with reasoning chains. Despite its promise, RLVR remains prohibitively resource-intensive, requiring extensive reward signals and incurring substantial rollout costs during training. In this work, we revisit the fundamental question of data and compute efficiency in RLVR. We first establish a theoretical lower bound on the sample complexity required to unlock reasoning capabilities, and empirically validate that strong performance can be achieved with a surprisingly small number of training instances. To tackle the computational burden, we propose Dynamic One-Shot Policy Refinement (DoPR), an uncertainty-aware RL strategy that dynamically selects a single informative training sample per batch for policy updates, guided by reward volatility and exploration-driven acquisition. DoPR reduces rollout overhead by nearly an order of magnitude while preserving competitive reasoning accuracy, offering a scalable and resource-efficient solution for LLM post-training. This approach offers a practical path toward more efficient and accessible RL-based training for reasoning-intensive LLM applications."
    },
    {
      "title": "V2X-DSC: Multi-Agent Collaborative Perception with Distributed Source Coding Guided Communication",
      "url": "https://arxiv.org/abs/2602.00687",
      "date": "2026-01-31",
      "authors_text": "Yuankun Zeng, Shaohui Li, Zhi Li, Shulan Ruan, Yu Liu",
      "is_highlight": false,
      "score": 76.0,
      "summary": "V2X-DSC introduces a Conditional Codec for efficient multi-agent collaborative perception by compressing and reconstructing BEV features, optimizing bandwidth usage while maintaining accuracy.",
      "teaser_image": "https://arxiv.org/html/2602.00687/figs/overview.jpg",
      "debug_abstract": "Collaborative perception improves 3D understanding by fusing multi-agent observations, yet intermediate-feature sharing faces strict bandwidth constraints as dense BEV features saturate V2X links. We observe that collaborators view the same physical world, making their features strongly correlated; thus receivers only need innovation beyond their local context. Revisiting this from a distributed source coding perspective, we propose V2X-DSC, a framework with a Conditional Codec (DCC) for bandwidth-constrained fusion. The sender compresses BEV features into compact codes, while the receiver performs conditional reconstruction using its local features as side information, allocating bits to complementary cues rather than redundant content. This conditional structure regularizes learning, encouraging incremental representation and yielding lower-noise features. Experiments on DAIR-V2X, OPV2V, and V2X-Real demonstrate state-of-the-art accuracy-bandwidth trade-offs under KB-level communication, and generalizes as a plug-and-play communication layer across multiple fusion backbones."
    },
    {
      "title": "How Far Are LLMs from Professional Poker Players? Revisiting Game-Theoretic Reasoning with Agentic Tool Use",
      "url": "https://arxiv.org/abs/2602.00528",
      "date": "2026-01-31",
      "authors_text": "Minhua Lin, Enyan Dai, Hui Liu, Xianfeng Tang, Yuliang Yan",
      "is_highlight": false,
      "score": 55.0,
      "summary": "This paper evaluates LLMs in poker, revealing their strategic reasoning flaws and proposing ToolPoker, which enhances gameplay and reasoning through integrated external solvers.",
      "teaser_image": "https://arxiv.org/html/2602.00528/x2.png",
      "debug_abstract": "As Large Language Models (LLMs) are increasingly applied in high-stakes domains, their ability to reason strategically under uncertainty becomes critical. Poker provides a rigorous testbed, requiring not only strong actions but also principled, game-theoretic reasoning. In this paper, we conduct a systematic study of LLMs in multiple realistic poker tasks, evaluating both gameplay outcomes and reasoning traces. Our analysis reveals LLMs fail to compete against traditional algorithms and identifies three recurring flaws: reliance on heuristics, factual misunderstandings, and a &#34;knowing-doing&#34; gap where actions diverge from reasoning. An initial attempt with behavior cloning and step-level reinforcement learning improves reasoning style but remains insufficient for accurate game-theoretic play. Motivated by these limitations, we propose ToolPoker, a tool-integrated reasoning framework that combines external solvers for GTO-consistent actions with more precise professional-style explanations. Experiments demonstrate that ToolPoker achieves state-of-the-art gameplay while producing reasoning traces that closely reflect game-theoretic principles."
    },
    {
      "title": "Learning Physics-Grounded 4D Dynamics with Neural Gaussian Force Fields",
      "url": "https://arxiv.org/abs/2602.00148",
      "date": "2026-01-29",
      "authors_text": "Shiqian Li, Ruihong Shen, Junfeng Ni, Chang Pan, Chi Zhang",
      "is_highlight": false,
      "score": 82.0,
      "summary": "The Neural Gaussian Force Field (NGFF) framework enables rapid generation of interactive, physically realistic 4D videos from multi-view RGB inputs, enhancing robustness and generalization in physical dynamics.",
      "teaser_image": "https://arxiv.org/html/2602.00148/x1.png",
      "debug_abstract": "Predicting physical dynamics from raw visual data remains a major challenge in AI. While recent video generation models have achieved impressive visual quality, they still cannot consistently generate physically plausible videos due to a lack of modeling of physical laws. Recent approaches combining 3D Gaussian splatting and physics engines can produce physically plausible videos, but are hindered by high computational costs in both reconstruction and simulation, and often lack robustness in complex real-world scenarios. To address these issues, we introduce Neural Gaussian Force Field (NGFF), an end-to-end neural framework that integrates 3D Gaussian perception with physics-based dynamic modeling to generate interactive, physically realistic 4D videos from multi-view RGB inputs, achieving two orders of magnitude faster than prior Gaussian simulators. To support training, we also present GSCollision, a 4D Gaussian dataset featuring diverse materials, multi-object interactions, and complex scenes, totaling over 640k rendered physical videos (~4 TB). Evaluations on synthetic and real 3D scenarios show NGFF&#39;s strong generalization and robustness in physical reasoning, advancing video prediction towards physics-grounded world models."
    }
  ],
  "具身智能实验室 (TEA Lab)": [
    {
      "title": "Enhancing Indoor Occupancy Prediction via Sparse Query-Based Multi-Level Consistent Knowledge Distillation",
      "url": "https://arxiv.org/abs/2602.02318",
      "date": "2026-02-02",
      "authors_text": "Xiang Li, Yupeng Zheng, Pengfei Li, Yilun Chen, Ya-Qin Zhang",
      "is_highlight": false,
      "score": 0,
      "summary": "DiScene introduces a sparse query-based framework for efficient and robust indoor occupancy prediction through multi-level knowledge distillation, achieving state-of-the-art performance and faster inference.",
      "teaser_image": "https://arxiv.org/html/2602.02318/x2.png",
      "debug_abstract": "Occupancy prediction provides critical geometric and semantic understanding for robotics but faces efficiency-accuracy trade-offs. Current dense methods suffer computational waste on empty voxels, while sparse query-based approaches lack robustness in diverse and complex indoor scenes. In this paper, we propose DiScene, a novel sparse query-based framework that leverages multi-level distillation to achieve efficient and robust occupancy prediction. In particular, our method incorporates two key innovations: (1) a Multi-level Consistent Knowledge Distillation strategy, which transfers hierarchical representations from large teacher models to lightweight students through coordinated alignment across four levels, including encoder-level feature alignment, query-level feature matching, prior-level spatial guidance, and anchor-level high-confidence knowledge transfer and (2) a Teacher-Guided Initialization policy, employing optimized parameter warm-up to accelerate model convergence. Validated on the Occ-Scannet benchmark, DiScene achieves 23.2 FPS without depth priors while outperforming our baseline method, OPUS, by 36.1% and even better than the depth-enhanced version, OPUS†. With depth integration, DiScene† attains new SOTA performance, surpassing EmbodiedOcc by 3.7% with 1.62$\\times$ faster inference speed. Furthermore, experiments on the Occ3D-nuScenes benchmark and in-the-wild scenarios demonstrate the versatility of our approach in various environments. Code and models can be accessed at this https URL."
    },
    {
      "title": "UV-M3TL: A Unified and Versatile Multimodal Multi-Task Learning Framework for Assistive Driving Perception",
      "url": "https://arxiv.org/abs/2602.01594",
      "date": "2026-02-02",
      "authors_text": "Wenzhuo Liu, Qiannan Guo, Zhen Wang, Wenshuo Wang, Lei Yang",
      "is_highlight": false,
      "score": 8.0,
      "summary": "The UV-M3TL framework enhances ADAS by effectively recognizing driver behavior and context through dual-branch multimodal embedding and adaptive loss, achieving state-of-the-art performance across multiple tasks.",
      "teaser_image": "https://arxiv.org/html/2602.01594/x2.png",
      "debug_abstract": "Advanced Driver Assistance Systems (ADAS) need to understand human driver behavior while perceiving their navigation context, but jointly learning these heterogeneous tasks would cause inter-task negative transfer and impair system performance. Here, we propose a Unified and Versatile Multimodal Multi-Task Learning (UV-M3TL) framework to simultaneously recognize driver behavior, driver emotion, vehicle behavior, and traffic context, while mitigating inter-task negative transfer. Our framework incorporates two core components: dual-branch spatial channel multimodal embedding (DB-SCME) and adaptive feature-decoupled multi-task loss (AFD-Loss). DB-SCME enhances cross-task knowledge transfer while mitigating task conflicts by employing a dual-branch structure to explicitly model salient task-shared and task-specific features. AFD-Loss improves the stability of joint optimization while guiding the model to learn diverse multi-task representations by introducing an adaptive weighting mechanism based on learning dynamics and feature decoupling constraints. We evaluate our method on the AIDE dataset, and the experimental results demonstrate that UV-M3TL achieves state-of-the-art performance across all four tasks. To further prove the versatility, we evaluate UV-M3TL on additional public multi-task perception benchmarks (BDD100K, CityScapes, NYUD-v2, and PASCAL-Context), where it consistently delivers strong performance across diverse task combinations, attaining state-of-the-art results on most tasks."
    },
    {
      "title": "Context Learning for Multi-Agent Discussion",
      "url": "https://arxiv.org/abs/2602.02350",
      "date": "2026-02-02",
      "authors_text": "Xingyuan Hua, Sheng Yue, Xinyi Li, Yizhe Zhao, Jinrui Zhang",
      "is_highlight": false,
      "score": 17.0,
      "summary": "The paper presents M2CL, a multi-LLM context learning method that enhances coherence in Multi-Agent Discussions, improving problem-solving performance by 20%-50% over existing approaches.",
      "teaser_image": "https://arxiv.org/html/2602.02350/x2.png",
      "debug_abstract": "Multi-Agent Discussion (MAD) has garnered increasing attention very recently, where multiple LLM instances collaboratively solve problems via structured discussion. However, we find that current MAD methods easily suffer from discussion inconsistency, LLMs fail to reach a coherent solution, due to the misalignment between their individual this http URL this paper, we introduce a multi-LLM context learning method (M2CL) that learns a context generator for each agent, capable of dynamically generating context instructions per discussion round via automatic information organization and refinement. Specifically, inspired by our theoretical insights on the context instruction, M2CL train the generators to control context coherence and output discrepancies via a carefully crafted self-adaptive this http URL enables LLMs to avoid premature convergence on majority noise and progressively reach the correct consensus. We evaluate M2CL on challenging tasks, including academic reasoning, embodied tasks, and mobile control. The results show that the performance of M2CL significantly surpasses existing methods by 20%--50%, while enjoying favorable transferability and computational efficiency."
    },
    {
      "title": "TTT-Parkour: Rapid Test-Time Training for Perceptive Robot Parkour",
      "url": "https://arxiv.org/abs/2602.02331",
      "date": "2026-02-02",
      "authors_text": "Shaoting Zhu, Baijun Ye, Jiaxuan Wang, Jiakang Chen, Ziwen Zhuang",
      "is_highlight": false,
      "score": 18.0,
      "summary": "TTT-Parkour introduces a rapid test-time training framework enabling humanoid robots to effectively navigate complex terrains through efficient real-to-sim-to-real learning and geometry reconstruction.",
      "teaser_image": "https://arxiv.org/html/2602.02331/x1.png",
      "debug_abstract": "Achieving highly dynamic humanoid parkour on unseen, complex terrains remains a challenge in robotics. Although general locomotion policies demonstrate capabilities across broad terrain distributions, they often struggle with arbitrary and highly challenging environments. To overcome this limitation, we propose a real-to-sim-to-real framework that leverages rapid test-time training (TTT) on novel terrains, significantly enhancing the robot&#39;s capability to traverse extremely difficult geometries. We adopt a two-stage end-to-end learning paradigm: a policy is first pre-trained on diverse procedurally generated terrains, followed by rapid fine-tuning on high-fidelity meshes reconstructed from real-world captures. Specifically, we develop a feed-forward, efficient, and high-fidelity geometry reconstruction pipeline using RGB-D inputs, ensuring both speed and quality during test-time training. We demonstrate that TTT-Parkour empowers humanoid robots to master complex obstacles, including wedges, stakes, boxes, trapezoids, and narrow beams. The whole pipeline of capturing, reconstructing, and test-time training requires less than 10 minutes on most tested terrains. Extensive experiments show that the policy after test-time training exhibits robust zero-shot sim-to-real transfer capability."
    },
    {
      "title": "Small Generalizable Prompt Predictive Models Can Steer Efficient RL Post-Training of Large Reasoning Models",
      "url": "https://arxiv.org/abs/2602.01970",
      "date": "2026-02-02",
      "authors_text": "Yun Qu, Qi Wang, Yixiu Mao, Heming Zou, Yuhang Jiang",
      "is_highlight": false,
      "score": 26.0,
      "summary": "The paper presents Generalizable Predictive Prompt Selection (GPS), a method that enhances reinforcement learning efficiency in large language models by utilizing a lightweight generative model for prompt selection.",
      "teaser_image": "https://arxiv.org/html/2602.01970/x1.png",
      "debug_abstract": "Reinforcement learning enhances the reasoning capabilities of large language models but often involves high computational costs due to rollout-intensive optimization. Online prompt selection presents a plausible solution by prioritizing informative prompts to improve training efficiency. However, current methods either depend on costly, exact evaluations or construct prompt-specific predictive models lacking generalization across prompts. This study introduces Generalizable Predictive Prompt Selection (GPS), which performs Bayesian inference towards prompt difficulty using a lightweight generative model trained on the shared optimization history. Intermediate-difficulty prioritization and history-anchored diversity are incorporated into the batch acquisition principle to select informative prompt batches. The small predictive model also generalizes at test-time for efficient computational allocation. Experiments across varied reasoning benchmarks indicate GPS&#39;s substantial improvements in training efficiency, final performance, and test-time efficiency over superior baseline methods."
    },
    {
      "title": "ForSim: Stepwise Forward Simulation for Traffic Policy Fine-Tuning",
      "url": "https://arxiv.org/abs/2602.01916",
      "date": "2026-02-02",
      "authors_text": "Keyu Chen, Wenchao Sun, Hao Cheng, Zheng Fu, Sifa Zheng",
      "is_highlight": true,
      "score": 29.0,
      "summary": "ForSim introduces a stepwise closed-loop simulation method that enhances traffic policy fine-tuning by improving agent interactions and preserving multimodal behaviors in autonomous driving.",
      "teaser_image": "https://arxiv.org/html/2602.01916/x2.png",
      "debug_abstract": "As the foundation of closed-loop training and evaluation in autonomous driving, traffic simulation still faces two fundamental challenges: covariate shift introduced by open-loop imitation learning and limited capacity to reflect the multimodal behaviors observed in real-world traffic. Although recent frameworks such as RIFT have partially addressed these issues through group-relative optimization, their forward simulation procedures remain largely non-reactive, leading to unrealistic agent interactions within the virtual domain and ultimately limiting simulation fidelity. To address these issues, we propose ForSim, a stepwise closed-loop forward simulation paradigm. At each virtual timestep, the traffic agent propagates the virtual candidate trajectory that best spatiotemporally matches the reference trajectory through physically grounded motion dynamics, thereby preserving multimodal behavioral diversity while ensuring intra-modality consistency. Other agents are updated with stepwise predictions, yielding coherent and interaction-aware evolution. When incorporated into the RIFT traffic simulation framework, ForSim operates in conjunction with group-relative optimization to fine-tune traffic policy. Extensive experiments confirm that this integration consistently improves safety while maintaining efficiency, realism, and comfort. These results underscore the importance of modeling closed-loop multimodal interactions within forward simulation and enhance the fidelity and reliability of traffic simulation for autonomous driving. Project Page: this https URL"
    },
    {
      "title": "From Frames to Sequences: Temporally Consistent Human-Centric Dense Prediction",
      "url": "https://arxiv.org/abs/2602.01661",
      "date": "2026-02-02",
      "authors_text": "Xingyu Miao, Junting Dong, Qin Zhao, Yuhang Yang, Junhao Chen",
      "is_highlight": false,
      "score": 43.0,
      "summary": "This paper presents a novel synthetic data pipeline and a ViT-based model for achieving temporally consistent human-centric dense prediction in video sequences, outperforming existing methods.",
      "teaser_image": "https://arxiv.org/html/2602.01661/x2.png",
      "debug_abstract": "In this work, we focus on the challenge of temporally consistent human-centric dense prediction across video sequences. Existing models achieve strong per-frame accuracy but often flicker under motion, occlusion, and lighting changes, and they rarely have paired human video supervision for multiple dense tasks. We address this gap with a scalable synthetic data pipeline that generates photorealistic human frames and motion-aligned sequences with pixel-accurate depth, normals, and masks. Unlike prior static data synthetic pipelines, our pipeline provides both frame-level labels for spatial learning and sequence-level supervision for temporal learning. Building on this, we train a unified ViT-based dense predictor that (i) injects an explicit human geometric prior via CSE embeddings and (ii) improves geometry-feature reliability with a lightweight channel reweighting module after feature fusion. Our two-stage training strategy, combining static pretraining with dynamic sequence supervision, enables the model first to acquire robust spatial representations and then refine temporal consistency across motion-aligned sequences. Extensive experiments show that we achieve state-of-the-art performance on THuman2.1 and Hi4D and generalize effectively to in-the-wild videos."
    },
    {
      "title": "Reasoning with Autoregressive-Diffusion Collaborative Thoughts",
      "url": "https://arxiv.org/abs/2602.01608",
      "date": "2026-02-02",
      "authors_text": "Mu Yuan, Liekang Zeng, Guoliang Xing, Lan Zhang, Yunhao Liu",
      "is_highlight": false,
      "score": 41.0,
      "summary": "The paper presents Collaborative Thoughts, a framework integrating autoregressive and diffusion models for enhanced spatial reasoning and generation through iterative feedback and joint reasoning.",
      "teaser_image": "https://arxiv.org/html/2602.01608/x2.png",
      "debug_abstract": "Autoregressive and diffusion models represent two complementary generative paradigms. Autoregressive models excel at sequential planning and constraint composition, yet struggle with tasks that require explicit spatial or physical grounding. Diffusion models, in contrast, capture rich spatial structure through high-dimensional generation, but lack the stepwise logical control needed to satisfy complex, multi-stage constraints or to reliably identify and correct errors. We introduce Collaborative Thoughts, a unified collaborative framework that enables autoregressive and diffusion models to reason and generate jointly through a closed-loop interaction. In Collaborative Thoughts, autoregressive models perform structured planning and constraint management, diffusion models instantiate these constraints as intermediate visual thoughts, and a vision-based critic module evaluates whether the visual thoughts satisfy the intended structural and physical requirements. This feedback is then used to iteratively refine subsequent planning and generation steps, mitigating error propagation across modalities. Importantly, Collaborative Thoughts uses the same collaborative loop regardless of whether the task is autoregressive question answering or diffusion-based visual generation. Through representative examples, we illustrate how Collaborative Thoughts can improve the reliability of spatial reasoning and the controllability of generation."
    },
    {
      "title": "Making Avatars Interact: Towards Text-Driven Human-Object Interaction for Controllable Talking Avatars",
      "url": "https://arxiv.org/abs/2602.01538",
      "date": "2026-02-02",
      "authors_text": "Youliang Zhang, Zhengguang Zhou, Zhentao Yu, Ziyao Huang, Teng Hu",
      "is_highlight": false,
      "score": 46.0,
      "summary": "The paper presents InteractAvatar, a dual-stream framework for generating talking avatars that perform text-driven human-object interactions by decoupling perception and planning from video synthesis.",
      "teaser_image": "https://arxiv.org/html/2602.01538/x1.png",
      "debug_abstract": "Generating talking avatars is a fundamental task in video generation. Although existing methods can generate full-body talking avatars with simple human motion, extending this task to grounded human-object interaction (GHOI) remains an open challenge, requiring the avatar to perform text-aligned interactions with surrounding objects. This challenge stems from the need for environmental perception and the control-quality dilemma in GHOI generation. To address this, we propose a novel dual-stream framework, InteractAvatar, which decouples perception and planning from video synthesis for grounded human-object interaction. Leveraging detection to enhance environmental perception, we introduce a Perception and Interaction Module (PIM) to generate text-aligned interaction motions. Additionally, an Audio-Interaction Aware Generation Module (AIM) is proposed to synthesize vivid talking avatars performing object interactions. With a specially designed motion-to-video aligner, PIM and AIM share a similar network structure and enable parallel co-generation of motions and plausible videos, effectively mitigating the control-quality dilemma. Finally, we establish a benchmark, GroundedInter, for evaluating GHOI video generation. Extensive experiments and comparisons demonstrate the effectiveness of our method in generating grounded human-object interactions for talking avatars. Project page: this https URL"
    },
    {
      "title": "Resource-Efficient Reinforcement for Reasoning Large Language Models via Dynamic One-Shot Policy Refinement",
      "url": "https://arxiv.org/abs/2602.00815",
      "date": "2026-01-31",
      "authors_text": "Yunjian Zhang, Sudong Wang, Yang Li, Peiran Xu, Conghao Zhou",
      "is_highlight": false,
      "score": 57.0,
      "summary": "The paper introduces Dynamic One-Shot Policy Refinement (DoPR), a resource-efficient reinforcement learning strategy that enhances reasoning in large language models while significantly reducing training costs.",
      "teaser_image": "https://arxiv.org/html/2602.00815/dopr.png",
      "debug_abstract": "Large language models (LLMs) have exhibited remarkable performance on complex reasoning tasks, with reinforcement learning under verifiable rewards (RLVR) emerging as a principled framework for aligning model behavior with reasoning chains. Despite its promise, RLVR remains prohibitively resource-intensive, requiring extensive reward signals and incurring substantial rollout costs during training. In this work, we revisit the fundamental question of data and compute efficiency in RLVR. We first establish a theoretical lower bound on the sample complexity required to unlock reasoning capabilities, and empirically validate that strong performance can be achieved with a surprisingly small number of training instances. To tackle the computational burden, we propose Dynamic One-Shot Policy Refinement (DoPR), an uncertainty-aware RL strategy that dynamically selects a single informative training sample per batch for policy updates, guided by reward volatility and exploration-driven acquisition. DoPR reduces rollout overhead by nearly an order of magnitude while preserving competitive reasoning accuracy, offering a scalable and resource-efficient solution for LLM post-training. This approach offers a practical path toward more efficient and accessible RL-based training for reasoning-intensive LLM applications."
    },
    {
      "title": "V2X-DSC: Multi-Agent Collaborative Perception with Distributed Source Coding Guided Communication",
      "url": "https://arxiv.org/abs/2602.00687",
      "date": "2026-01-31",
      "authors_text": "Yuankun Zeng, Shaohui Li, Zhi Li, Shulan Ruan, Yu Liu",
      "is_highlight": false,
      "score": 76.0,
      "summary": "V2X-DSC introduces a Conditional Codec for efficient multi-agent collaborative perception by compressing and reconstructing BEV features, optimizing bandwidth usage while maintaining accuracy.",
      "teaser_image": "https://arxiv.org/html/2602.00687/figs/overview.jpg",
      "debug_abstract": "Collaborative perception improves 3D understanding by fusing multi-agent observations, yet intermediate-feature sharing faces strict bandwidth constraints as dense BEV features saturate V2X links. We observe that collaborators view the same physical world, making their features strongly correlated; thus receivers only need innovation beyond their local context. Revisiting this from a distributed source coding perspective, we propose V2X-DSC, a framework with a Conditional Codec (DCC) for bandwidth-constrained fusion. The sender compresses BEV features into compact codes, while the receiver performs conditional reconstruction using its local features as side information, allocating bits to complementary cues rather than redundant content. This conditional structure regularizes learning, encouraging incremental representation and yielding lower-noise features. Experiments on DAIR-V2X, OPV2V, and V2X-Real demonstrate state-of-the-art accuracy-bandwidth trade-offs under KB-level communication, and generalizes as a plug-and-play communication layer across multiple fusion backbones."
    },
    {
      "title": "How Far Are LLMs from Professional Poker Players? Revisiting Game-Theoretic Reasoning with Agentic Tool Use",
      "url": "https://arxiv.org/abs/2602.00528",
      "date": "2026-01-31",
      "authors_text": "Minhua Lin, Enyan Dai, Hui Liu, Xianfeng Tang, Yuliang Yan",
      "is_highlight": false,
      "score": 55.0,
      "summary": "This paper evaluates LLMs in poker, revealing their strategic reasoning flaws and proposing ToolPoker, which enhances gameplay and reasoning through integrated external solvers.",
      "teaser_image": "https://arxiv.org/html/2602.00528/x2.png",
      "debug_abstract": "As Large Language Models (LLMs) are increasingly applied in high-stakes domains, their ability to reason strategically under uncertainty becomes critical. Poker provides a rigorous testbed, requiring not only strong actions but also principled, game-theoretic reasoning. In this paper, we conduct a systematic study of LLMs in multiple realistic poker tasks, evaluating both gameplay outcomes and reasoning traces. Our analysis reveals LLMs fail to compete against traditional algorithms and identifies three recurring flaws: reliance on heuristics, factual misunderstandings, and a &#34;knowing-doing&#34; gap where actions diverge from reasoning. An initial attempt with behavior cloning and step-level reinforcement learning improves reasoning style but remains insufficient for accurate game-theoretic play. Motivated by these limitations, we propose ToolPoker, a tool-integrated reasoning framework that combines external solvers for GTO-consistent actions with more precise professional-style explanations. Experiments demonstrate that ToolPoker achieves state-of-the-art gameplay while producing reasoning traces that closely reflect game-theoretic principles."
    },
    {
      "title": "Learning Physics-Grounded 4D Dynamics with Neural Gaussian Force Fields",
      "url": "https://arxiv.org/abs/2602.00148",
      "date": "2026-01-29",
      "authors_text": "Shiqian Li, Ruihong Shen, Junfeng Ni, Chang Pan, Chi Zhang",
      "is_highlight": false,
      "score": 82.0,
      "summary": "The Neural Gaussian Force Field (NGFF) framework enables rapid generation of interactive, physically realistic 4D videos from multi-view RGB inputs, enhancing robustness and generalization in physical dynamics.",
      "teaser_image": "https://arxiv.org/html/2602.00148/x1.png",
      "debug_abstract": "Predicting physical dynamics from raw visual data remains a major challenge in AI. While recent video generation models have achieved impressive visual quality, they still cannot consistently generate physically plausible videos due to a lack of modeling of physical laws. Recent approaches combining 3D Gaussian splatting and physics engines can produce physically plausible videos, but are hindered by high computational costs in both reconstruction and simulation, and often lack robustness in complex real-world scenarios. To address these issues, we introduce Neural Gaussian Force Field (NGFF), an end-to-end neural framework that integrates 3D Gaussian perception with physics-based dynamic modeling to generate interactive, physically realistic 4D videos from multi-view RGB inputs, achieving two orders of magnitude faster than prior Gaussian simulators. To support training, we also present GSCollision, a 4D Gaussian dataset featuring diverse materials, multi-object interactions, and complex scenes, totaling over 640k rendered physical videos (~4 TB). Evaluations on synthetic and real 3D scenarios show NGFF&#39;s strong generalization and robustness in physical reasoning, advancing video prediction towards physics-grounded world models."
    }
  ],
  "具身感知与交互实验室 (EPIC Lab)": [
    {
      "title": "Superman: Unifying Skeleton and Vision for Human Motion Perception and Generation",
      "url": "https://arxiv.org/abs/2602.02401",
      "date": "2026-02-02",
      "authors_text": "Xinshun Wang, Peiming Li, Ziyi Wang, Zhongbin Fang, Zhichao Deng",
      "is_highlight": false,
      "score": 15.0,
      "summary": "Superman unifies visual perception and skeleton-based motion generation through a Vision-Guided Motion Tokenizer, enabling robust cross-modal learning and superior performance in human motion tasks.",
      "teaser_image": "https://arxiv.org/html/2602.02401/x2.png",
      "debug_abstract": "Human motion analysis tasks, such as temporal 3D pose estimation, motion prediction, and motion in-betweening, play an essential role in computer vision. However, current paradigms suffer from severe fragmentation. First, the field is split between ``perception&#39;&#39; models that understand motion from video but only output text, and ``generation&#39;&#39; models that cannot perceive from raw visual input. Second, generative MLLMs are often limited to single-frame, static poses using dense, parametric SMPL models, failing to handle temporal motion. Third, existing motion vocabularies are built from skeleton data alone, severing the link to the visual domain. To address these challenges, we introduce Superman, a unified framework that bridges visual perception with temporal, skeleton-based motion generation. Our solution is twofold. First, to overcome the modality disconnect, we propose a Vision-Guided Motion Tokenizer. Leveraging the natural geometric alignment between 3D skeletons and visual data, this module pioneers robust joint learning from both modalities, creating a unified, cross-modal motion vocabulary. Second, grounded in this motion language, a single, unified MLLM architecture is trained to handle all tasks. This module flexibly processes diverse, temporal inputs, unifying 3D skeleton pose estimation from video (perception) with skeleton-based motion prediction and in-betweening (generation). Extensive experiments on standard benchmarks, including Human3.6M, demonstrate that our unified method achieves state-of-the-art or competitive performance across all motion tasks. This showcases a more efficient and scalable path for generative motion analysis using skeletons."
    },
    {
      "title": "Not All Preferences Are Created Equal: Stability-Aware and Gradient-Efficient Alignment for Reasoning Models",
      "url": "https://arxiv.org/abs/2602.01207",
      "date": "2026-02-01",
      "authors_text": "Hui Wu, Hengyi Cai, Jinman Zhao, Xinran Chen, Ziheng Li",
      "is_highlight": false,
      "score": 48.0,
      "summary": "The paper introduces SAGE, a dynamic framework for preference-based alignment in reasoning models that enhances optimization stability and efficiency by prioritizing informative training instances.",
      "teaser_image": "https://arxiv.org/html/2602.01207/x2.png",
      "debug_abstract": "Preference-based alignment is pivotal for training large reasoning models; however, standard methods like Direct Preference Optimization (DPO) typically treat all preference pairs uniformly, overlooking the evolving utility of training instances. This static approach often leads to inefficient or unstable optimization, as it wastes computation on trivial pairs with negligible gradients and suffers from noise induced by samples near uncertain decision boundaries. Facing these challenges, we propose SAGE (Stability-Aware Gradient Efficiency), a dynamic framework designed to enhance alignment reliability by maximizing the Signal-to-Noise Ratio of policy updates. Concretely, SAGE integrates a coarse-grained curriculum mechanism that refreshes candidate pools based on model competence with a fine-grained, stability-aware scoring function that prioritizes informative, confident errors while filtering out unstable samples. Experiments on multiple mathematical reasoning benchmarks demonstrate that SAGE significantly accelerates convergence and outperforms static baselines, highlighting the critical role of policy-aware, stability-conscious data selection in reasoning alignment."
    },
    {
      "title": "Residual Decoding: Mitigating Hallucinations in Large Vision-Language Models via History-Aware Residual Guidance",
      "url": "https://arxiv.org/abs/2602.01047",
      "date": "2026-02-01",
      "authors_text": "Xinrong Chen, Xu Chu, Yingmin Qiu, Hengyuan Zhang, Jing Xiong",
      "is_highlight": false,
      "score": 53.0,
      "summary": "Residual Decoding (ResDec) mitigates hallucinations in Large Vision-Language Models by leveraging historical information to enhance decoding and improve visual grounding.",
      "teaser_image": "https://arxiv.org/html/2602.01047/x1.png",
      "debug_abstract": "Large Vision-Language Models (LVLMs) can reason effectively from image-text inputs and perform well in various multimodal tasks. Despite this success, they are affected by language priors and often produce hallucinations. Hallucinations denote generated content that is grammatically and syntactically coherent, yet bears no match or direct relevance to actual visual input. To address this problem, we propose Residual Decoding (ResDec). It is a novel training-free method that uses historical information to aid decoding. The method relies on the internal implicit reasoning mechanism and token logits evolution mechanism of LVLMs to correct biases. Extensive experiments demonstrate that ResDec effectively suppresses hallucinations induced by language priors, significantly improves visual grounding, and reduces object hallucinations. In addition to mitigating hallucinations, ResDec also performs exceptionally well on comprehensive LVLM benchmarks, highlighting its broad applicability."
    },
    {
      "title": "Resource-Efficient Reinforcement for Reasoning Large Language Models via Dynamic One-Shot Policy Refinement",
      "url": "https://arxiv.org/abs/2602.00815",
      "date": "2026-01-31",
      "authors_text": "Yunjian Zhang, Sudong Wang, Yang Li, Peiran Xu, Conghao Zhou",
      "is_highlight": false,
      "score": 57.0,
      "summary": "The paper introduces Dynamic One-Shot Policy Refinement (DoPR), a resource-efficient reinforcement learning strategy that enhances reasoning in large language models while significantly reducing training costs.",
      "teaser_image": "https://arxiv.org/html/2602.00815/dopr.png",
      "debug_abstract": "Large language models (LLMs) have exhibited remarkable performance on complex reasoning tasks, with reinforcement learning under verifiable rewards (RLVR) emerging as a principled framework for aligning model behavior with reasoning chains. Despite its promise, RLVR remains prohibitively resource-intensive, requiring extensive reward signals and incurring substantial rollout costs during training. In this work, we revisit the fundamental question of data and compute efficiency in RLVR. We first establish a theoretical lower bound on the sample complexity required to unlock reasoning capabilities, and empirically validate that strong performance can be achieved with a surprisingly small number of training instances. To tackle the computational burden, we propose Dynamic One-Shot Policy Refinement (DoPR), an uncertainty-aware RL strategy that dynamically selects a single informative training sample per batch for policy updates, guided by reward volatility and exploration-driven acquisition. DoPR reduces rollout overhead by nearly an order of magnitude while preserving competitive reasoning accuracy, offering a scalable and resource-efficient solution for LLM post-training. This approach offers a practical path toward more efficient and accessible RL-based training for reasoning-intensive LLM applications."
    },
    {
      "title": "Learning Physics-Grounded 4D Dynamics with Neural Gaussian Force Fields",
      "url": "https://arxiv.org/abs/2602.00148",
      "date": "2026-01-29",
      "authors_text": "Shiqian Li, Ruihong Shen, Junfeng Ni, Chang Pan, Chi Zhang",
      "is_highlight": false,
      "score": 82.0,
      "summary": "The Neural Gaussian Force Field (NGFF) framework enables rapid generation of interactive, physically realistic 4D videos from multi-view RGB inputs, enhancing robustness and generalization in physical dynamics.",
      "teaser_image": "https://arxiv.org/html/2602.00148/x1.png",
      "debug_abstract": "Predicting physical dynamics from raw visual data remains a major challenge in AI. While recent video generation models have achieved impressive visual quality, they still cannot consistently generate physically plausible videos due to a lack of modeling of physical laws. Recent approaches combining 3D Gaussian splatting and physics engines can produce physically plausible videos, but are hindered by high computational costs in both reconstruction and simulation, and often lack robustness in complex real-world scenarios. To address these issues, we introduce Neural Gaussian Force Field (NGFF), an end-to-end neural framework that integrates 3D Gaussian perception with physics-based dynamic modeling to generate interactive, physically realistic 4D videos from multi-view RGB inputs, achieving two orders of magnitude faster than prior Gaussian simulators. To support training, we also present GSCollision, a 4D Gaussian dataset featuring diverse materials, multi-object interactions, and complex scenes, totaling over 640k rendered physical videos (~4 TB). Evaluations on synthetic and real 3D scenarios show NGFF&#39;s strong generalization and robustness in physical reasoning, advancing video prediction towards physics-grounded world models."
    }
  ],
  "智能产业研究院 (AIR)": [
    {
      "title": "Enhancing Indoor Occupancy Prediction via Sparse Query-Based Multi-Level Consistent Knowledge Distillation",
      "url": "https://arxiv.org/abs/2602.02318",
      "date": "2026-02-02",
      "authors_text": "Xiang Li, Yupeng Zheng, Pengfei Li, Yilun Chen, Ya-Qin Zhang",
      "is_highlight": false,
      "score": 0,
      "summary": "DiScene introduces a sparse query-based framework for efficient and robust indoor occupancy prediction through multi-level knowledge distillation, achieving state-of-the-art performance and faster inference.",
      "teaser_image": "https://arxiv.org/html/2602.02318/x2.png",
      "debug_abstract": "Occupancy prediction provides critical geometric and semantic understanding for robotics but faces efficiency-accuracy trade-offs. Current dense methods suffer computational waste on empty voxels, while sparse query-based approaches lack robustness in diverse and complex indoor scenes. In this paper, we propose DiScene, a novel sparse query-based framework that leverages multi-level distillation to achieve efficient and robust occupancy prediction. In particular, our method incorporates two key innovations: (1) a Multi-level Consistent Knowledge Distillation strategy, which transfers hierarchical representations from large teacher models to lightweight students through coordinated alignment across four levels, including encoder-level feature alignment, query-level feature matching, prior-level spatial guidance, and anchor-level high-confidence knowledge transfer and (2) a Teacher-Guided Initialization policy, employing optimized parameter warm-up to accelerate model convergence. Validated on the Occ-Scannet benchmark, DiScene achieves 23.2 FPS without depth priors while outperforming our baseline method, OPUS, by 36.1% and even better than the depth-enhanced version, OPUS†. With depth integration, DiScene† attains new SOTA performance, surpassing EmbodiedOcc by 3.7% with 1.62$\\times$ faster inference speed. Furthermore, experiments on the Occ3D-nuScenes benchmark and in-the-wild scenarios demonstrate the versatility of our approach in various environments. Code and models can be accessed at this https URL."
    },
    {
      "title": "UV-M3TL: A Unified and Versatile Multimodal Multi-Task Learning Framework for Assistive Driving Perception",
      "url": "https://arxiv.org/abs/2602.01594",
      "date": "2026-02-02",
      "authors_text": "Wenzhuo Liu, Qiannan Guo, Zhen Wang, Wenshuo Wang, Lei Yang",
      "is_highlight": false,
      "score": 8.0,
      "summary": "The UV-M3TL framework enhances ADAS by effectively recognizing driver behavior and context through dual-branch multimodal embedding and adaptive loss, achieving state-of-the-art performance across multiple tasks.",
      "teaser_image": "https://arxiv.org/html/2602.01594/x2.png",
      "debug_abstract": "Advanced Driver Assistance Systems (ADAS) need to understand human driver behavior while perceiving their navigation context, but jointly learning these heterogeneous tasks would cause inter-task negative transfer and impair system performance. Here, we propose a Unified and Versatile Multimodal Multi-Task Learning (UV-M3TL) framework to simultaneously recognize driver behavior, driver emotion, vehicle behavior, and traffic context, while mitigating inter-task negative transfer. Our framework incorporates two core components: dual-branch spatial channel multimodal embedding (DB-SCME) and adaptive feature-decoupled multi-task loss (AFD-Loss). DB-SCME enhances cross-task knowledge transfer while mitigating task conflicts by employing a dual-branch structure to explicitly model salient task-shared and task-specific features. AFD-Loss improves the stability of joint optimization while guiding the model to learn diverse multi-task representations by introducing an adaptive weighting mechanism based on learning dynamics and feature decoupling constraints. We evaluate our method on the AIDE dataset, and the experimental results demonstrate that UV-M3TL achieves state-of-the-art performance across all four tasks. To further prove the versatility, we evaluate UV-M3TL on additional public multi-task perception benchmarks (BDD100K, CityScapes, NYUD-v2, and PASCAL-Context), where it consistently delivers strong performance across diverse task combinations, attaining state-of-the-art results on most tasks."
    },
    {
      "title": "Context Learning for Multi-Agent Discussion",
      "url": "https://arxiv.org/abs/2602.02350",
      "date": "2026-02-02",
      "authors_text": "Xingyuan Hua, Sheng Yue, Xinyi Li, Yizhe Zhao, Jinrui Zhang",
      "is_highlight": false,
      "score": 17.0,
      "summary": "The paper presents M2CL, a multi-LLM context learning method that enhances coherence in Multi-Agent Discussions, improving problem-solving performance by 20%-50% over existing approaches.",
      "teaser_image": "https://arxiv.org/html/2602.02350/x2.png",
      "debug_abstract": "Multi-Agent Discussion (MAD) has garnered increasing attention very recently, where multiple LLM instances collaboratively solve problems via structured discussion. However, we find that current MAD methods easily suffer from discussion inconsistency, LLMs fail to reach a coherent solution, due to the misalignment between their individual this http URL this paper, we introduce a multi-LLM context learning method (M2CL) that learns a context generator for each agent, capable of dynamically generating context instructions per discussion round via automatic information organization and refinement. Specifically, inspired by our theoretical insights on the context instruction, M2CL train the generators to control context coherence and output discrepancies via a carefully crafted self-adaptive this http URL enables LLMs to avoid premature convergence on majority noise and progressively reach the correct consensus. We evaluate M2CL on challenging tasks, including academic reasoning, embodied tasks, and mobile control. The results show that the performance of M2CL significantly surpasses existing methods by 20%--50%, while enjoying favorable transferability and computational efficiency."
    },
    {
      "title": "TTT-Parkour: Rapid Test-Time Training for Perceptive Robot Parkour",
      "url": "https://arxiv.org/abs/2602.02331",
      "date": "2026-02-02",
      "authors_text": "Shaoting Zhu, Baijun Ye, Jiaxuan Wang, Jiakang Chen, Ziwen Zhuang",
      "is_highlight": false,
      "score": 18.0,
      "summary": "TTT-Parkour introduces a rapid test-time training framework enabling humanoid robots to effectively navigate complex terrains through efficient real-to-sim-to-real learning and geometry reconstruction.",
      "teaser_image": "https://arxiv.org/html/2602.02331/x1.png",
      "debug_abstract": "Achieving highly dynamic humanoid parkour on unseen, complex terrains remains a challenge in robotics. Although general locomotion policies demonstrate capabilities across broad terrain distributions, they often struggle with arbitrary and highly challenging environments. To overcome this limitation, we propose a real-to-sim-to-real framework that leverages rapid test-time training (TTT) on novel terrains, significantly enhancing the robot&#39;s capability to traverse extremely difficult geometries. We adopt a two-stage end-to-end learning paradigm: a policy is first pre-trained on diverse procedurally generated terrains, followed by rapid fine-tuning on high-fidelity meshes reconstructed from real-world captures. Specifically, we develop a feed-forward, efficient, and high-fidelity geometry reconstruction pipeline using RGB-D inputs, ensuring both speed and quality during test-time training. We demonstrate that TTT-Parkour empowers humanoid robots to master complex obstacles, including wedges, stakes, boxes, trapezoids, and narrow beams. The whole pipeline of capturing, reconstructing, and test-time training requires less than 10 minutes on most tested terrains. Extensive experiments show that the policy after test-time training exhibits robust zero-shot sim-to-real transfer capability."
    },
    {
      "title": "Small Generalizable Prompt Predictive Models Can Steer Efficient RL Post-Training of Large Reasoning Models",
      "url": "https://arxiv.org/abs/2602.01970",
      "date": "2026-02-02",
      "authors_text": "Yun Qu, Qi Wang, Yixiu Mao, Heming Zou, Yuhang Jiang",
      "is_highlight": false,
      "score": 26.0,
      "summary": "The paper presents Generalizable Predictive Prompt Selection (GPS), a method that enhances reinforcement learning efficiency in large language models by utilizing a lightweight generative model for prompt selection.",
      "teaser_image": "https://arxiv.org/html/2602.01970/x1.png",
      "debug_abstract": "Reinforcement learning enhances the reasoning capabilities of large language models but often involves high computational costs due to rollout-intensive optimization. Online prompt selection presents a plausible solution by prioritizing informative prompts to improve training efficiency. However, current methods either depend on costly, exact evaluations or construct prompt-specific predictive models lacking generalization across prompts. This study introduces Generalizable Predictive Prompt Selection (GPS), which performs Bayesian inference towards prompt difficulty using a lightweight generative model trained on the shared optimization history. Intermediate-difficulty prioritization and history-anchored diversity are incorporated into the batch acquisition principle to select informative prompt batches. The small predictive model also generalizes at test-time for efficient computational allocation. Experiments across varied reasoning benchmarks indicate GPS&#39;s substantial improvements in training efficiency, final performance, and test-time efficiency over superior baseline methods."
    },
    {
      "title": "ForSim: Stepwise Forward Simulation for Traffic Policy Fine-Tuning",
      "url": "https://arxiv.org/abs/2602.01916",
      "date": "2026-02-02",
      "authors_text": "Keyu Chen, Wenchao Sun, Hao Cheng, Zheng Fu, Sifa Zheng",
      "is_highlight": true,
      "score": 29.0,
      "summary": "ForSim introduces a stepwise closed-loop simulation method that enhances traffic policy fine-tuning by improving agent interactions and preserving multimodal behaviors in autonomous driving.",
      "teaser_image": "https://arxiv.org/html/2602.01916/x2.png",
      "debug_abstract": "As the foundation of closed-loop training and evaluation in autonomous driving, traffic simulation still faces two fundamental challenges: covariate shift introduced by open-loop imitation learning and limited capacity to reflect the multimodal behaviors observed in real-world traffic. Although recent frameworks such as RIFT have partially addressed these issues through group-relative optimization, their forward simulation procedures remain largely non-reactive, leading to unrealistic agent interactions within the virtual domain and ultimately limiting simulation fidelity. To address these issues, we propose ForSim, a stepwise closed-loop forward simulation paradigm. At each virtual timestep, the traffic agent propagates the virtual candidate trajectory that best spatiotemporally matches the reference trajectory through physically grounded motion dynamics, thereby preserving multimodal behavioral diversity while ensuring intra-modality consistency. Other agents are updated with stepwise predictions, yielding coherent and interaction-aware evolution. When incorporated into the RIFT traffic simulation framework, ForSim operates in conjunction with group-relative optimization to fine-tune traffic policy. Extensive experiments confirm that this integration consistently improves safety while maintaining efficiency, realism, and comfort. These results underscore the importance of modeling closed-loop multimodal interactions within forward simulation and enhance the fidelity and reliability of traffic simulation for autonomous driving. Project Page: this https URL"
    },
    {
      "title": "From Frames to Sequences: Temporally Consistent Human-Centric Dense Prediction",
      "url": "https://arxiv.org/abs/2602.01661",
      "date": "2026-02-02",
      "authors_text": "Xingyu Miao, Junting Dong, Qin Zhao, Yuhang Yang, Junhao Chen",
      "is_highlight": false,
      "score": 43.0,
      "summary": "This paper presents a novel synthetic data pipeline and a ViT-based model for achieving temporally consistent human-centric dense prediction in video sequences, outperforming existing methods.",
      "teaser_image": "https://arxiv.org/html/2602.01661/x2.png",
      "debug_abstract": "In this work, we focus on the challenge of temporally consistent human-centric dense prediction across video sequences. Existing models achieve strong per-frame accuracy but often flicker under motion, occlusion, and lighting changes, and they rarely have paired human video supervision for multiple dense tasks. We address this gap with a scalable synthetic data pipeline that generates photorealistic human frames and motion-aligned sequences with pixel-accurate depth, normals, and masks. Unlike prior static data synthetic pipelines, our pipeline provides both frame-level labels for spatial learning and sequence-level supervision for temporal learning. Building on this, we train a unified ViT-based dense predictor that (i) injects an explicit human geometric prior via CSE embeddings and (ii) improves geometry-feature reliability with a lightweight channel reweighting module after feature fusion. Our two-stage training strategy, combining static pretraining with dynamic sequence supervision, enables the model first to acquire robust spatial representations and then refine temporal consistency across motion-aligned sequences. Extensive experiments show that we achieve state-of-the-art performance on THuman2.1 and Hi4D and generalize effectively to in-the-wild videos."
    },
    {
      "title": "Reasoning with Autoregressive-Diffusion Collaborative Thoughts",
      "url": "https://arxiv.org/abs/2602.01608",
      "date": "2026-02-02",
      "authors_text": "Mu Yuan, Liekang Zeng, Guoliang Xing, Lan Zhang, Yunhao Liu",
      "is_highlight": false,
      "score": 41.0,
      "summary": "The paper presents Collaborative Thoughts, a framework integrating autoregressive and diffusion models for enhanced spatial reasoning and generation through iterative feedback and joint reasoning.",
      "teaser_image": "https://arxiv.org/html/2602.01608/x2.png",
      "debug_abstract": "Autoregressive and diffusion models represent two complementary generative paradigms. Autoregressive models excel at sequential planning and constraint composition, yet struggle with tasks that require explicit spatial or physical grounding. Diffusion models, in contrast, capture rich spatial structure through high-dimensional generation, but lack the stepwise logical control needed to satisfy complex, multi-stage constraints or to reliably identify and correct errors. We introduce Collaborative Thoughts, a unified collaborative framework that enables autoregressive and diffusion models to reason and generate jointly through a closed-loop interaction. In Collaborative Thoughts, autoregressive models perform structured planning and constraint management, diffusion models instantiate these constraints as intermediate visual thoughts, and a vision-based critic module evaluates whether the visual thoughts satisfy the intended structural and physical requirements. This feedback is then used to iteratively refine subsequent planning and generation steps, mitigating error propagation across modalities. Importantly, Collaborative Thoughts uses the same collaborative loop regardless of whether the task is autoregressive question answering or diffusion-based visual generation. Through representative examples, we illustrate how Collaborative Thoughts can improve the reliability of spatial reasoning and the controllability of generation."
    },
    {
      "title": "Making Avatars Interact: Towards Text-Driven Human-Object Interaction for Controllable Talking Avatars",
      "url": "https://arxiv.org/abs/2602.01538",
      "date": "2026-02-02",
      "authors_text": "Youliang Zhang, Zhengguang Zhou, Zhentao Yu, Ziyao Huang, Teng Hu",
      "is_highlight": false,
      "score": 46.0,
      "summary": "The paper presents InteractAvatar, a dual-stream framework for generating talking avatars that perform text-driven human-object interactions by decoupling perception and planning from video synthesis.",
      "teaser_image": "https://arxiv.org/html/2602.01538/x1.png",
      "debug_abstract": "Generating talking avatars is a fundamental task in video generation. Although existing methods can generate full-body talking avatars with simple human motion, extending this task to grounded human-object interaction (GHOI) remains an open challenge, requiring the avatar to perform text-aligned interactions with surrounding objects. This challenge stems from the need for environmental perception and the control-quality dilemma in GHOI generation. To address this, we propose a novel dual-stream framework, InteractAvatar, which decouples perception and planning from video synthesis for grounded human-object interaction. Leveraging detection to enhance environmental perception, we introduce a Perception and Interaction Module (PIM) to generate text-aligned interaction motions. Additionally, an Audio-Interaction Aware Generation Module (AIM) is proposed to synthesize vivid talking avatars performing object interactions. With a specially designed motion-to-video aligner, PIM and AIM share a similar network structure and enable parallel co-generation of motions and plausible videos, effectively mitigating the control-quality dilemma. Finally, we establish a benchmark, GroundedInter, for evaluating GHOI video generation. Extensive experiments and comparisons demonstrate the effectiveness of our method in generating grounded human-object interactions for talking avatars. Project page: this https URL"
    },
    {
      "title": "Resource-Efficient Reinforcement for Reasoning Large Language Models via Dynamic One-Shot Policy Refinement",
      "url": "https://arxiv.org/abs/2602.00815",
      "date": "2026-01-31",
      "authors_text": "Yunjian Zhang, Sudong Wang, Yang Li, Peiran Xu, Conghao Zhou",
      "is_highlight": false,
      "score": 57.0,
      "summary": "The paper introduces Dynamic One-Shot Policy Refinement (DoPR), a resource-efficient reinforcement learning strategy that enhances reasoning in large language models while significantly reducing training costs.",
      "teaser_image": "https://arxiv.org/html/2602.00815/dopr.png",
      "debug_abstract": "Large language models (LLMs) have exhibited remarkable performance on complex reasoning tasks, with reinforcement learning under verifiable rewards (RLVR) emerging as a principled framework for aligning model behavior with reasoning chains. Despite its promise, RLVR remains prohibitively resource-intensive, requiring extensive reward signals and incurring substantial rollout costs during training. In this work, we revisit the fundamental question of data and compute efficiency in RLVR. We first establish a theoretical lower bound on the sample complexity required to unlock reasoning capabilities, and empirically validate that strong performance can be achieved with a surprisingly small number of training instances. To tackle the computational burden, we propose Dynamic One-Shot Policy Refinement (DoPR), an uncertainty-aware RL strategy that dynamically selects a single informative training sample per batch for policy updates, guided by reward volatility and exploration-driven acquisition. DoPR reduces rollout overhead by nearly an order of magnitude while preserving competitive reasoning accuracy, offering a scalable and resource-efficient solution for LLM post-training. This approach offers a practical path toward more efficient and accessible RL-based training for reasoning-intensive LLM applications."
    },
    {
      "title": "V2X-DSC: Multi-Agent Collaborative Perception with Distributed Source Coding Guided Communication",
      "url": "https://arxiv.org/abs/2602.00687",
      "date": "2026-01-31",
      "authors_text": "Yuankun Zeng, Shaohui Li, Zhi Li, Shulan Ruan, Yu Liu",
      "is_highlight": false,
      "score": 76.0,
      "summary": "V2X-DSC introduces a Conditional Codec for efficient multi-agent collaborative perception by compressing and reconstructing BEV features, optimizing bandwidth usage while maintaining accuracy.",
      "teaser_image": "https://arxiv.org/html/2602.00687/figs/overview.jpg",
      "debug_abstract": "Collaborative perception improves 3D understanding by fusing multi-agent observations, yet intermediate-feature sharing faces strict bandwidth constraints as dense BEV features saturate V2X links. We observe that collaborators view the same physical world, making their features strongly correlated; thus receivers only need innovation beyond their local context. Revisiting this from a distributed source coding perspective, we propose V2X-DSC, a framework with a Conditional Codec (DCC) for bandwidth-constrained fusion. The sender compresses BEV features into compact codes, while the receiver performs conditional reconstruction using its local features as side information, allocating bits to complementary cues rather than redundant content. This conditional structure regularizes learning, encouraging incremental representation and yielding lower-noise features. Experiments on DAIR-V2X, OPV2V, and V2X-Real demonstrate state-of-the-art accuracy-bandwidth trade-offs under KB-level communication, and generalizes as a plug-and-play communication layer across multiple fusion backbones."
    },
    {
      "title": "How Far Are LLMs from Professional Poker Players? Revisiting Game-Theoretic Reasoning with Agentic Tool Use",
      "url": "https://arxiv.org/abs/2602.00528",
      "date": "2026-01-31",
      "authors_text": "Minhua Lin, Enyan Dai, Hui Liu, Xianfeng Tang, Yuliang Yan",
      "is_highlight": false,
      "score": 55.0,
      "summary": "This paper evaluates LLMs in poker, revealing their strategic reasoning flaws and proposing ToolPoker, which enhances gameplay and reasoning through integrated external solvers.",
      "teaser_image": "https://arxiv.org/html/2602.00528/x2.png",
      "debug_abstract": "As Large Language Models (LLMs) are increasingly applied in high-stakes domains, their ability to reason strategically under uncertainty becomes critical. Poker provides a rigorous testbed, requiring not only strong actions but also principled, game-theoretic reasoning. In this paper, we conduct a systematic study of LLMs in multiple realistic poker tasks, evaluating both gameplay outcomes and reasoning traces. Our analysis reveals LLMs fail to compete against traditional algorithms and identifies three recurring flaws: reliance on heuristics, factual misunderstandings, and a &#34;knowing-doing&#34; gap where actions diverge from reasoning. An initial attempt with behavior cloning and step-level reinforcement learning improves reasoning style but remains insufficient for accurate game-theoretic play. Motivated by these limitations, we propose ToolPoker, a tool-integrated reasoning framework that combines external solvers for GTO-consistent actions with more precise professional-style explanations. Experiments demonstrate that ToolPoker achieves state-of-the-art gameplay while producing reasoning traces that closely reflect game-theoretic principles."
    },
    {
      "title": "Learning Physics-Grounded 4D Dynamics with Neural Gaussian Force Fields",
      "url": "https://arxiv.org/abs/2602.00148",
      "date": "2026-01-29",
      "authors_text": "Shiqian Li, Ruihong Shen, Junfeng Ni, Chang Pan, Chi Zhang",
      "is_highlight": false,
      "score": 82.0,
      "summary": "The Neural Gaussian Force Field (NGFF) framework enables rapid generation of interactive, physically realistic 4D videos from multi-view RGB inputs, enhancing robustness and generalization in physical dynamics.",
      "teaser_image": "https://arxiv.org/html/2602.00148/x1.png",
      "debug_abstract": "Predicting physical dynamics from raw visual data remains a major challenge in AI. While recent video generation models have achieved impressive visual quality, they still cannot consistently generate physically plausible videos due to a lack of modeling of physical laws. Recent approaches combining 3D Gaussian splatting and physics engines can produce physically plausible videos, but are hindered by high computational costs in both reconstruction and simulation, and often lack robustness in complex real-world scenarios. To address these issues, we introduce Neural Gaussian Force Field (NGFF), an end-to-end neural framework that integrates 3D Gaussian perception with physics-based dynamic modeling to generate interactive, physically realistic 4D videos from multi-view RGB inputs, achieving two orders of magnitude faster than prior Gaussian simulators. To support training, we also present GSCollision, a 4D Gaussian dataset featuring diverse materials, multi-object interactions, and complex scenes, totaling over 640k rendered physical videos (~4 TB). Evaluations on synthetic and real 3D scenarios show NGFF&#39;s strong generalization and robustness in physical reasoning, advancing video prediction towards physics-grounded world models."
    }
  ],
  "MARS多模态学习实验室": [
    {
      "title": "Enhancing Indoor Occupancy Prediction via Sparse Query-Based Multi-Level Consistent Knowledge Distillation",
      "url": "https://arxiv.org/abs/2602.02318",
      "date": "2026-02-02",
      "authors_text": "Xiang Li, Yupeng Zheng, Pengfei Li, Yilun Chen, Ya-Qin Zhang",
      "is_highlight": false,
      "score": 0,
      "summary": "DiScene introduces a sparse query-based framework for efficient and robust indoor occupancy prediction through multi-level knowledge distillation, achieving state-of-the-art performance and faster inference.",
      "teaser_image": "https://arxiv.org/html/2602.02318/x2.png",
      "debug_abstract": "Occupancy prediction provides critical geometric and semantic understanding for robotics but faces efficiency-accuracy trade-offs. Current dense methods suffer computational waste on empty voxels, while sparse query-based approaches lack robustness in diverse and complex indoor scenes. In this paper, we propose DiScene, a novel sparse query-based framework that leverages multi-level distillation to achieve efficient and robust occupancy prediction. In particular, our method incorporates two key innovations: (1) a Multi-level Consistent Knowledge Distillation strategy, which transfers hierarchical representations from large teacher models to lightweight students through coordinated alignment across four levels, including encoder-level feature alignment, query-level feature matching, prior-level spatial guidance, and anchor-level high-confidence knowledge transfer and (2) a Teacher-Guided Initialization policy, employing optimized parameter warm-up to accelerate model convergence. Validated on the Occ-Scannet benchmark, DiScene achieves 23.2 FPS without depth priors while outperforming our baseline method, OPUS, by 36.1% and even better than the depth-enhanced version, OPUS†. With depth integration, DiScene† attains new SOTA performance, surpassing EmbodiedOcc by 3.7% with 1.62$\\times$ faster inference speed. Furthermore, experiments on the Occ3D-nuScenes benchmark and in-the-wild scenarios demonstrate the versatility of our approach in various environments. Code and models can be accessed at this https URL."
    },
    {
      "title": "UV-M3TL: A Unified and Versatile Multimodal Multi-Task Learning Framework for Assistive Driving Perception",
      "url": "https://arxiv.org/abs/2602.01594",
      "date": "2026-02-02",
      "authors_text": "Wenzhuo Liu, Qiannan Guo, Zhen Wang, Wenshuo Wang, Lei Yang",
      "is_highlight": false,
      "score": 8.0,
      "summary": "The UV-M3TL framework enhances ADAS by effectively recognizing driver behavior and context through dual-branch multimodal embedding and adaptive loss, achieving state-of-the-art performance across multiple tasks.",
      "teaser_image": "https://arxiv.org/html/2602.01594/x2.png",
      "debug_abstract": "Advanced Driver Assistance Systems (ADAS) need to understand human driver behavior while perceiving their navigation context, but jointly learning these heterogeneous tasks would cause inter-task negative transfer and impair system performance. Here, we propose a Unified and Versatile Multimodal Multi-Task Learning (UV-M3TL) framework to simultaneously recognize driver behavior, driver emotion, vehicle behavior, and traffic context, while mitigating inter-task negative transfer. Our framework incorporates two core components: dual-branch spatial channel multimodal embedding (DB-SCME) and adaptive feature-decoupled multi-task loss (AFD-Loss). DB-SCME enhances cross-task knowledge transfer while mitigating task conflicts by employing a dual-branch structure to explicitly model salient task-shared and task-specific features. AFD-Loss improves the stability of joint optimization while guiding the model to learn diverse multi-task representations by introducing an adaptive weighting mechanism based on learning dynamics and feature decoupling constraints. We evaluate our method on the AIDE dataset, and the experimental results demonstrate that UV-M3TL achieves state-of-the-art performance across all four tasks. To further prove the versatility, we evaluate UV-M3TL on additional public multi-task perception benchmarks (BDD100K, CityScapes, NYUD-v2, and PASCAL-Context), where it consistently delivers strong performance across diverse task combinations, attaining state-of-the-art results on most tasks."
    },
    {
      "title": "Context Learning for Multi-Agent Discussion",
      "url": "https://arxiv.org/abs/2602.02350",
      "date": "2026-02-02",
      "authors_text": "Xingyuan Hua, Sheng Yue, Xinyi Li, Yizhe Zhao, Jinrui Zhang",
      "is_highlight": false,
      "score": 17.0,
      "summary": "The paper presents M2CL, a multi-LLM context learning method that enhances coherence in Multi-Agent Discussions, improving problem-solving performance by 20%-50% over existing approaches.",
      "teaser_image": "https://arxiv.org/html/2602.02350/x2.png",
      "debug_abstract": "Multi-Agent Discussion (MAD) has garnered increasing attention very recently, where multiple LLM instances collaboratively solve problems via structured discussion. However, we find that current MAD methods easily suffer from discussion inconsistency, LLMs fail to reach a coherent solution, due to the misalignment between their individual this http URL this paper, we introduce a multi-LLM context learning method (M2CL) that learns a context generator for each agent, capable of dynamically generating context instructions per discussion round via automatic information organization and refinement. Specifically, inspired by our theoretical insights on the context instruction, M2CL train the generators to control context coherence and output discrepancies via a carefully crafted self-adaptive this http URL enables LLMs to avoid premature convergence on majority noise and progressively reach the correct consensus. We evaluate M2CL on challenging tasks, including academic reasoning, embodied tasks, and mobile control. The results show that the performance of M2CL significantly surpasses existing methods by 20%--50%, while enjoying favorable transferability and computational efficiency."
    },
    {
      "title": "TTT-Parkour: Rapid Test-Time Training for Perceptive Robot Parkour",
      "url": "https://arxiv.org/abs/2602.02331",
      "date": "2026-02-02",
      "authors_text": "Shaoting Zhu, Baijun Ye, Jiaxuan Wang, Jiakang Chen, Ziwen Zhuang",
      "is_highlight": false,
      "score": 18.0,
      "summary": "TTT-Parkour introduces a rapid test-time training framework enabling humanoid robots to effectively navigate complex terrains through efficient real-to-sim-to-real learning and geometry reconstruction.",
      "teaser_image": "https://arxiv.org/html/2602.02331/x1.png",
      "debug_abstract": "Achieving highly dynamic humanoid parkour on unseen, complex terrains remains a challenge in robotics. Although general locomotion policies demonstrate capabilities across broad terrain distributions, they often struggle with arbitrary and highly challenging environments. To overcome this limitation, we propose a real-to-sim-to-real framework that leverages rapid test-time training (TTT) on novel terrains, significantly enhancing the robot&#39;s capability to traverse extremely difficult geometries. We adopt a two-stage end-to-end learning paradigm: a policy is first pre-trained on diverse procedurally generated terrains, followed by rapid fine-tuning on high-fidelity meshes reconstructed from real-world captures. Specifically, we develop a feed-forward, efficient, and high-fidelity geometry reconstruction pipeline using RGB-D inputs, ensuring both speed and quality during test-time training. We demonstrate that TTT-Parkour empowers humanoid robots to master complex obstacles, including wedges, stakes, boxes, trapezoids, and narrow beams. The whole pipeline of capturing, reconstructing, and test-time training requires less than 10 minutes on most tested terrains. Extensive experiments show that the policy after test-time training exhibits robust zero-shot sim-to-real transfer capability."
    },
    {
      "title": "Small Generalizable Prompt Predictive Models Can Steer Efficient RL Post-Training of Large Reasoning Models",
      "url": "https://arxiv.org/abs/2602.01970",
      "date": "2026-02-02",
      "authors_text": "Yun Qu, Qi Wang, Yixiu Mao, Heming Zou, Yuhang Jiang",
      "is_highlight": false,
      "score": 26.0,
      "summary": "The paper presents Generalizable Predictive Prompt Selection (GPS), a method that enhances reinforcement learning efficiency in large language models by utilizing a lightweight generative model for prompt selection.",
      "teaser_image": "https://arxiv.org/html/2602.01970/x1.png",
      "debug_abstract": "Reinforcement learning enhances the reasoning capabilities of large language models but often involves high computational costs due to rollout-intensive optimization. Online prompt selection presents a plausible solution by prioritizing informative prompts to improve training efficiency. However, current methods either depend on costly, exact evaluations or construct prompt-specific predictive models lacking generalization across prompts. This study introduces Generalizable Predictive Prompt Selection (GPS), which performs Bayesian inference towards prompt difficulty using a lightweight generative model trained on the shared optimization history. Intermediate-difficulty prioritization and history-anchored diversity are incorporated into the batch acquisition principle to select informative prompt batches. The small predictive model also generalizes at test-time for efficient computational allocation. Experiments across varied reasoning benchmarks indicate GPS&#39;s substantial improvements in training efficiency, final performance, and test-time efficiency over superior baseline methods."
    },
    {
      "title": "ForSim: Stepwise Forward Simulation for Traffic Policy Fine-Tuning",
      "url": "https://arxiv.org/abs/2602.01916",
      "date": "2026-02-02",
      "authors_text": "Keyu Chen, Wenchao Sun, Hao Cheng, Zheng Fu, Sifa Zheng",
      "is_highlight": true,
      "score": 29.0,
      "summary": "ForSim introduces a stepwise closed-loop simulation method that enhances traffic policy fine-tuning by improving agent interactions and preserving multimodal behaviors in autonomous driving.",
      "teaser_image": "https://arxiv.org/html/2602.01916/x2.png",
      "debug_abstract": "As the foundation of closed-loop training and evaluation in autonomous driving, traffic simulation still faces two fundamental challenges: covariate shift introduced by open-loop imitation learning and limited capacity to reflect the multimodal behaviors observed in real-world traffic. Although recent frameworks such as RIFT have partially addressed these issues through group-relative optimization, their forward simulation procedures remain largely non-reactive, leading to unrealistic agent interactions within the virtual domain and ultimately limiting simulation fidelity. To address these issues, we propose ForSim, a stepwise closed-loop forward simulation paradigm. At each virtual timestep, the traffic agent propagates the virtual candidate trajectory that best spatiotemporally matches the reference trajectory through physically grounded motion dynamics, thereby preserving multimodal behavioral diversity while ensuring intra-modality consistency. Other agents are updated with stepwise predictions, yielding coherent and interaction-aware evolution. When incorporated into the RIFT traffic simulation framework, ForSim operates in conjunction with group-relative optimization to fine-tune traffic policy. Extensive experiments confirm that this integration consistently improves safety while maintaining efficiency, realism, and comfort. These results underscore the importance of modeling closed-loop multimodal interactions within forward simulation and enhance the fidelity and reliability of traffic simulation for autonomous driving. Project Page: this https URL"
    },
    {
      "title": "From Frames to Sequences: Temporally Consistent Human-Centric Dense Prediction",
      "url": "https://arxiv.org/abs/2602.01661",
      "date": "2026-02-02",
      "authors_text": "Xingyu Miao, Junting Dong, Qin Zhao, Yuhang Yang, Junhao Chen",
      "is_highlight": false,
      "score": 43.0,
      "summary": "This paper presents a novel synthetic data pipeline and a ViT-based model for achieving temporally consistent human-centric dense prediction in video sequences, outperforming existing methods.",
      "teaser_image": "https://arxiv.org/html/2602.01661/x2.png",
      "debug_abstract": "In this work, we focus on the challenge of temporally consistent human-centric dense prediction across video sequences. Existing models achieve strong per-frame accuracy but often flicker under motion, occlusion, and lighting changes, and they rarely have paired human video supervision for multiple dense tasks. We address this gap with a scalable synthetic data pipeline that generates photorealistic human frames and motion-aligned sequences with pixel-accurate depth, normals, and masks. Unlike prior static data synthetic pipelines, our pipeline provides both frame-level labels for spatial learning and sequence-level supervision for temporal learning. Building on this, we train a unified ViT-based dense predictor that (i) injects an explicit human geometric prior via CSE embeddings and (ii) improves geometry-feature reliability with a lightweight channel reweighting module after feature fusion. Our two-stage training strategy, combining static pretraining with dynamic sequence supervision, enables the model first to acquire robust spatial representations and then refine temporal consistency across motion-aligned sequences. Extensive experiments show that we achieve state-of-the-art performance on THuman2.1 and Hi4D and generalize effectively to in-the-wild videos."
    },
    {
      "title": "Reasoning with Autoregressive-Diffusion Collaborative Thoughts",
      "url": "https://arxiv.org/abs/2602.01608",
      "date": "2026-02-02",
      "authors_text": "Mu Yuan, Liekang Zeng, Guoliang Xing, Lan Zhang, Yunhao Liu",
      "is_highlight": false,
      "score": 41.0,
      "summary": "The paper presents Collaborative Thoughts, a framework integrating autoregressive and diffusion models for enhanced spatial reasoning and generation through iterative feedback and joint reasoning.",
      "teaser_image": "https://arxiv.org/html/2602.01608/x2.png",
      "debug_abstract": "Autoregressive and diffusion models represent two complementary generative paradigms. Autoregressive models excel at sequential planning and constraint composition, yet struggle with tasks that require explicit spatial or physical grounding. Diffusion models, in contrast, capture rich spatial structure through high-dimensional generation, but lack the stepwise logical control needed to satisfy complex, multi-stage constraints or to reliably identify and correct errors. We introduce Collaborative Thoughts, a unified collaborative framework that enables autoregressive and diffusion models to reason and generate jointly through a closed-loop interaction. In Collaborative Thoughts, autoregressive models perform structured planning and constraint management, diffusion models instantiate these constraints as intermediate visual thoughts, and a vision-based critic module evaluates whether the visual thoughts satisfy the intended structural and physical requirements. This feedback is then used to iteratively refine subsequent planning and generation steps, mitigating error propagation across modalities. Importantly, Collaborative Thoughts uses the same collaborative loop regardless of whether the task is autoregressive question answering or diffusion-based visual generation. Through representative examples, we illustrate how Collaborative Thoughts can improve the reliability of spatial reasoning and the controllability of generation."
    },
    {
      "title": "Making Avatars Interact: Towards Text-Driven Human-Object Interaction for Controllable Talking Avatars",
      "url": "https://arxiv.org/abs/2602.01538",
      "date": "2026-02-02",
      "authors_text": "Youliang Zhang, Zhengguang Zhou, Zhentao Yu, Ziyao Huang, Teng Hu",
      "is_highlight": false,
      "score": 46.0,
      "summary": "The paper presents InteractAvatar, a dual-stream framework for generating talking avatars that perform text-driven human-object interactions by decoupling perception and planning from video synthesis.",
      "teaser_image": "https://arxiv.org/html/2602.01538/x1.png",
      "debug_abstract": "Generating talking avatars is a fundamental task in video generation. Although existing methods can generate full-body talking avatars with simple human motion, extending this task to grounded human-object interaction (GHOI) remains an open challenge, requiring the avatar to perform text-aligned interactions with surrounding objects. This challenge stems from the need for environmental perception and the control-quality dilemma in GHOI generation. To address this, we propose a novel dual-stream framework, InteractAvatar, which decouples perception and planning from video synthesis for grounded human-object interaction. Leveraging detection to enhance environmental perception, we introduce a Perception and Interaction Module (PIM) to generate text-aligned interaction motions. Additionally, an Audio-Interaction Aware Generation Module (AIM) is proposed to synthesize vivid talking avatars performing object interactions. With a specially designed motion-to-video aligner, PIM and AIM share a similar network structure and enable parallel co-generation of motions and plausible videos, effectively mitigating the control-quality dilemma. Finally, we establish a benchmark, GroundedInter, for evaluating GHOI video generation. Extensive experiments and comparisons demonstrate the effectiveness of our method in generating grounded human-object interactions for talking avatars. Project page: this https URL"
    },
    {
      "title": "Resource-Efficient Reinforcement for Reasoning Large Language Models via Dynamic One-Shot Policy Refinement",
      "url": "https://arxiv.org/abs/2602.00815",
      "date": "2026-01-31",
      "authors_text": "Yunjian Zhang, Sudong Wang, Yang Li, Peiran Xu, Conghao Zhou",
      "is_highlight": false,
      "score": 57.0,
      "summary": "The paper introduces Dynamic One-Shot Policy Refinement (DoPR), a resource-efficient reinforcement learning strategy that enhances reasoning in large language models while significantly reducing training costs.",
      "teaser_image": "https://arxiv.org/html/2602.00815/dopr.png",
      "debug_abstract": "Large language models (LLMs) have exhibited remarkable performance on complex reasoning tasks, with reinforcement learning under verifiable rewards (RLVR) emerging as a principled framework for aligning model behavior with reasoning chains. Despite its promise, RLVR remains prohibitively resource-intensive, requiring extensive reward signals and incurring substantial rollout costs during training. In this work, we revisit the fundamental question of data and compute efficiency in RLVR. We first establish a theoretical lower bound on the sample complexity required to unlock reasoning capabilities, and empirically validate that strong performance can be achieved with a surprisingly small number of training instances. To tackle the computational burden, we propose Dynamic One-Shot Policy Refinement (DoPR), an uncertainty-aware RL strategy that dynamically selects a single informative training sample per batch for policy updates, guided by reward volatility and exploration-driven acquisition. DoPR reduces rollout overhead by nearly an order of magnitude while preserving competitive reasoning accuracy, offering a scalable and resource-efficient solution for LLM post-training. This approach offers a practical path toward more efficient and accessible RL-based training for reasoning-intensive LLM applications."
    },
    {
      "title": "V2X-DSC: Multi-Agent Collaborative Perception with Distributed Source Coding Guided Communication",
      "url": "https://arxiv.org/abs/2602.00687",
      "date": "2026-01-31",
      "authors_text": "Yuankun Zeng, Shaohui Li, Zhi Li, Shulan Ruan, Yu Liu",
      "is_highlight": false,
      "score": 76.0,
      "summary": "V2X-DSC introduces a Conditional Codec for efficient multi-agent collaborative perception by compressing and reconstructing BEV features, optimizing bandwidth usage while maintaining accuracy.",
      "teaser_image": "https://arxiv.org/html/2602.00687/figs/overview.jpg",
      "debug_abstract": "Collaborative perception improves 3D understanding by fusing multi-agent observations, yet intermediate-feature sharing faces strict bandwidth constraints as dense BEV features saturate V2X links. We observe that collaborators view the same physical world, making their features strongly correlated; thus receivers only need innovation beyond their local context. Revisiting this from a distributed source coding perspective, we propose V2X-DSC, a framework with a Conditional Codec (DCC) for bandwidth-constrained fusion. The sender compresses BEV features into compact codes, while the receiver performs conditional reconstruction using its local features as side information, allocating bits to complementary cues rather than redundant content. This conditional structure regularizes learning, encouraging incremental representation and yielding lower-noise features. Experiments on DAIR-V2X, OPV2V, and V2X-Real demonstrate state-of-the-art accuracy-bandwidth trade-offs under KB-level communication, and generalizes as a plug-and-play communication layer across multiple fusion backbones."
    },
    {
      "title": "How Far Are LLMs from Professional Poker Players? Revisiting Game-Theoretic Reasoning with Agentic Tool Use",
      "url": "https://arxiv.org/abs/2602.00528",
      "date": "2026-01-31",
      "authors_text": "Minhua Lin, Enyan Dai, Hui Liu, Xianfeng Tang, Yuliang Yan",
      "is_highlight": false,
      "score": 55.0,
      "summary": "This paper evaluates LLMs in poker, revealing their strategic reasoning flaws and proposing ToolPoker, which enhances gameplay and reasoning through integrated external solvers.",
      "teaser_image": "https://arxiv.org/html/2602.00528/x2.png",
      "debug_abstract": "As Large Language Models (LLMs) are increasingly applied in high-stakes domains, their ability to reason strategically under uncertainty becomes critical. Poker provides a rigorous testbed, requiring not only strong actions but also principled, game-theoretic reasoning. In this paper, we conduct a systematic study of LLMs in multiple realistic poker tasks, evaluating both gameplay outcomes and reasoning traces. Our analysis reveals LLMs fail to compete against traditional algorithms and identifies three recurring flaws: reliance on heuristics, factual misunderstandings, and a &#34;knowing-doing&#34; gap where actions diverge from reasoning. An initial attempt with behavior cloning and step-level reinforcement learning improves reasoning style but remains insufficient for accurate game-theoretic play. Motivated by these limitations, we propose ToolPoker, a tool-integrated reasoning framework that combines external solvers for GTO-consistent actions with more precise professional-style explanations. Experiments demonstrate that ToolPoker achieves state-of-the-art gameplay while producing reasoning traces that closely reflect game-theoretic principles."
    },
    {
      "title": "Learning Physics-Grounded 4D Dynamics with Neural Gaussian Force Fields",
      "url": "https://arxiv.org/abs/2602.00148",
      "date": "2026-01-29",
      "authors_text": "Shiqian Li, Ruihong Shen, Junfeng Ni, Chang Pan, Chi Zhang",
      "is_highlight": false,
      "score": 82.0,
      "summary": "The Neural Gaussian Force Field (NGFF) framework enables rapid generation of interactive, physically realistic 4D videos from multi-view RGB inputs, enhancing robustness and generalization in physical dynamics.",
      "teaser_image": "https://arxiv.org/html/2602.00148/x1.png",
      "debug_abstract": "Predicting physical dynamics from raw visual data remains a major challenge in AI. While recent video generation models have achieved impressive visual quality, they still cannot consistently generate physically plausible videos due to a lack of modeling of physical laws. Recent approaches combining 3D Gaussian splatting and physics engines can produce physically plausible videos, but are hindered by high computational costs in both reconstruction and simulation, and often lack robustness in complex real-world scenarios. To address these issues, we introduce Neural Gaussian Force Field (NGFF), an end-to-end neural framework that integrates 3D Gaussian perception with physics-based dynamic modeling to generate interactive, physically realistic 4D videos from multi-view RGB inputs, achieving two orders of magnitude faster than prior Gaussian simulators. To support training, we also present GSCollision, a 4D Gaussian dataset featuring diverse materials, multi-object interactions, and complex scenes, totaling over 640k rendered physical videos (~4 TB). Evaluations on synthetic and real 3D scenarios show NGFF&#39;s strong generalization and robustness in physical reasoning, advancing video prediction towards physics-grounded world models."
    }
  ],
  "机器人控制实验室": [
    {
      "title": "Enhancing Indoor Occupancy Prediction via Sparse Query-Based Multi-Level Consistent Knowledge Distillation",
      "url": "https://arxiv.org/abs/2602.02318",
      "date": "2026-02-02",
      "authors_text": "Xiang Li, Yupeng Zheng, Pengfei Li, Yilun Chen, Ya-Qin Zhang",
      "is_highlight": false,
      "score": 0,
      "summary": "DiScene introduces a sparse query-based framework for efficient and robust indoor occupancy prediction through multi-level knowledge distillation, achieving state-of-the-art performance and faster inference.",
      "teaser_image": "https://arxiv.org/html/2602.02318/x2.png",
      "debug_abstract": "Occupancy prediction provides critical geometric and semantic understanding for robotics but faces efficiency-accuracy trade-offs. Current dense methods suffer computational waste on empty voxels, while sparse query-based approaches lack robustness in diverse and complex indoor scenes. In this paper, we propose DiScene, a novel sparse query-based framework that leverages multi-level distillation to achieve efficient and robust occupancy prediction. In particular, our method incorporates two key innovations: (1) a Multi-level Consistent Knowledge Distillation strategy, which transfers hierarchical representations from large teacher models to lightweight students through coordinated alignment across four levels, including encoder-level feature alignment, query-level feature matching, prior-level spatial guidance, and anchor-level high-confidence knowledge transfer and (2) a Teacher-Guided Initialization policy, employing optimized parameter warm-up to accelerate model convergence. Validated on the Occ-Scannet benchmark, DiScene achieves 23.2 FPS without depth priors while outperforming our baseline method, OPUS, by 36.1% and even better than the depth-enhanced version, OPUS†. With depth integration, DiScene† attains new SOTA performance, surpassing EmbodiedOcc by 3.7% with 1.62$\\times$ faster inference speed. Furthermore, experiments on the Occ3D-nuScenes benchmark and in-the-wild scenarios demonstrate the versatility of our approach in various environments. Code and models can be accessed at this https URL."
    },
    {
      "title": "UV-M3TL: A Unified and Versatile Multimodal Multi-Task Learning Framework for Assistive Driving Perception",
      "url": "https://arxiv.org/abs/2602.01594",
      "date": "2026-02-02",
      "authors_text": "Wenzhuo Liu, Qiannan Guo, Zhen Wang, Wenshuo Wang, Lei Yang",
      "is_highlight": false,
      "score": 8.0,
      "summary": "The UV-M3TL framework enhances ADAS by effectively recognizing driver behavior and context through dual-branch multimodal embedding and adaptive loss, achieving state-of-the-art performance across multiple tasks.",
      "teaser_image": "https://arxiv.org/html/2602.01594/x2.png",
      "debug_abstract": "Advanced Driver Assistance Systems (ADAS) need to understand human driver behavior while perceiving their navigation context, but jointly learning these heterogeneous tasks would cause inter-task negative transfer and impair system performance. Here, we propose a Unified and Versatile Multimodal Multi-Task Learning (UV-M3TL) framework to simultaneously recognize driver behavior, driver emotion, vehicle behavior, and traffic context, while mitigating inter-task negative transfer. Our framework incorporates two core components: dual-branch spatial channel multimodal embedding (DB-SCME) and adaptive feature-decoupled multi-task loss (AFD-Loss). DB-SCME enhances cross-task knowledge transfer while mitigating task conflicts by employing a dual-branch structure to explicitly model salient task-shared and task-specific features. AFD-Loss improves the stability of joint optimization while guiding the model to learn diverse multi-task representations by introducing an adaptive weighting mechanism based on learning dynamics and feature decoupling constraints. We evaluate our method on the AIDE dataset, and the experimental results demonstrate that UV-M3TL achieves state-of-the-art performance across all four tasks. To further prove the versatility, we evaluate UV-M3TL on additional public multi-task perception benchmarks (BDD100K, CityScapes, NYUD-v2, and PASCAL-Context), where it consistently delivers strong performance across diverse task combinations, attaining state-of-the-art results on most tasks."
    },
    {
      "title": "Context Learning for Multi-Agent Discussion",
      "url": "https://arxiv.org/abs/2602.02350",
      "date": "2026-02-02",
      "authors_text": "Xingyuan Hua, Sheng Yue, Xinyi Li, Yizhe Zhao, Jinrui Zhang",
      "is_highlight": false,
      "score": 17.0,
      "summary": "The paper presents M2CL, a multi-LLM context learning method that enhances coherence in Multi-Agent Discussions, improving problem-solving performance by 20%-50% over existing approaches.",
      "teaser_image": "https://arxiv.org/html/2602.02350/x2.png",
      "debug_abstract": "Multi-Agent Discussion (MAD) has garnered increasing attention very recently, where multiple LLM instances collaboratively solve problems via structured discussion. However, we find that current MAD methods easily suffer from discussion inconsistency, LLMs fail to reach a coherent solution, due to the misalignment between their individual this http URL this paper, we introduce a multi-LLM context learning method (M2CL) that learns a context generator for each agent, capable of dynamically generating context instructions per discussion round via automatic information organization and refinement. Specifically, inspired by our theoretical insights on the context instruction, M2CL train the generators to control context coherence and output discrepancies via a carefully crafted self-adaptive this http URL enables LLMs to avoid premature convergence on majority noise and progressively reach the correct consensus. We evaluate M2CL on challenging tasks, including academic reasoning, embodied tasks, and mobile control. The results show that the performance of M2CL significantly surpasses existing methods by 20%--50%, while enjoying favorable transferability and computational efficiency."
    },
    {
      "title": "TTT-Parkour: Rapid Test-Time Training for Perceptive Robot Parkour",
      "url": "https://arxiv.org/abs/2602.02331",
      "date": "2026-02-02",
      "authors_text": "Shaoting Zhu, Baijun Ye, Jiaxuan Wang, Jiakang Chen, Ziwen Zhuang",
      "is_highlight": false,
      "score": 18.0,
      "summary": "TTT-Parkour introduces a rapid test-time training framework enabling humanoid robots to effectively navigate complex terrains through efficient real-to-sim-to-real learning and geometry reconstruction.",
      "teaser_image": "https://arxiv.org/html/2602.02331/x1.png",
      "debug_abstract": "Achieving highly dynamic humanoid parkour on unseen, complex terrains remains a challenge in robotics. Although general locomotion policies demonstrate capabilities across broad terrain distributions, they often struggle with arbitrary and highly challenging environments. To overcome this limitation, we propose a real-to-sim-to-real framework that leverages rapid test-time training (TTT) on novel terrains, significantly enhancing the robot&#39;s capability to traverse extremely difficult geometries. We adopt a two-stage end-to-end learning paradigm: a policy is first pre-trained on diverse procedurally generated terrains, followed by rapid fine-tuning on high-fidelity meshes reconstructed from real-world captures. Specifically, we develop a feed-forward, efficient, and high-fidelity geometry reconstruction pipeline using RGB-D inputs, ensuring both speed and quality during test-time training. We demonstrate that TTT-Parkour empowers humanoid robots to master complex obstacles, including wedges, stakes, boxes, trapezoids, and narrow beams. The whole pipeline of capturing, reconstructing, and test-time training requires less than 10 minutes on most tested terrains. Extensive experiments show that the policy after test-time training exhibits robust zero-shot sim-to-real transfer capability."
    },
    {
      "title": "Small Generalizable Prompt Predictive Models Can Steer Efficient RL Post-Training of Large Reasoning Models",
      "url": "https://arxiv.org/abs/2602.01970",
      "date": "2026-02-02",
      "authors_text": "Yun Qu, Qi Wang, Yixiu Mao, Heming Zou, Yuhang Jiang",
      "is_highlight": false,
      "score": 26.0,
      "summary": "The paper presents Generalizable Predictive Prompt Selection (GPS), a method that enhances reinforcement learning efficiency in large language models by utilizing a lightweight generative model for prompt selection.",
      "teaser_image": "https://arxiv.org/html/2602.01970/x1.png",
      "debug_abstract": "Reinforcement learning enhances the reasoning capabilities of large language models but often involves high computational costs due to rollout-intensive optimization. Online prompt selection presents a plausible solution by prioritizing informative prompts to improve training efficiency. However, current methods either depend on costly, exact evaluations or construct prompt-specific predictive models lacking generalization across prompts. This study introduces Generalizable Predictive Prompt Selection (GPS), which performs Bayesian inference towards prompt difficulty using a lightweight generative model trained on the shared optimization history. Intermediate-difficulty prioritization and history-anchored diversity are incorporated into the batch acquisition principle to select informative prompt batches. The small predictive model also generalizes at test-time for efficient computational allocation. Experiments across varied reasoning benchmarks indicate GPS&#39;s substantial improvements in training efficiency, final performance, and test-time efficiency over superior baseline methods."
    },
    {
      "title": "ForSim: Stepwise Forward Simulation for Traffic Policy Fine-Tuning",
      "url": "https://arxiv.org/abs/2602.01916",
      "date": "2026-02-02",
      "authors_text": "Keyu Chen, Wenchao Sun, Hao Cheng, Zheng Fu, Sifa Zheng",
      "is_highlight": true,
      "score": 29.0,
      "summary": "ForSim introduces a stepwise closed-loop simulation method that enhances traffic policy fine-tuning by improving agent interactions and preserving multimodal behaviors in autonomous driving.",
      "teaser_image": "https://arxiv.org/html/2602.01916/x2.png",
      "debug_abstract": "As the foundation of closed-loop training and evaluation in autonomous driving, traffic simulation still faces two fundamental challenges: covariate shift introduced by open-loop imitation learning and limited capacity to reflect the multimodal behaviors observed in real-world traffic. Although recent frameworks such as RIFT have partially addressed these issues through group-relative optimization, their forward simulation procedures remain largely non-reactive, leading to unrealistic agent interactions within the virtual domain and ultimately limiting simulation fidelity. To address these issues, we propose ForSim, a stepwise closed-loop forward simulation paradigm. At each virtual timestep, the traffic agent propagates the virtual candidate trajectory that best spatiotemporally matches the reference trajectory through physically grounded motion dynamics, thereby preserving multimodal behavioral diversity while ensuring intra-modality consistency. Other agents are updated with stepwise predictions, yielding coherent and interaction-aware evolution. When incorporated into the RIFT traffic simulation framework, ForSim operates in conjunction with group-relative optimization to fine-tune traffic policy. Extensive experiments confirm that this integration consistently improves safety while maintaining efficiency, realism, and comfort. These results underscore the importance of modeling closed-loop multimodal interactions within forward simulation and enhance the fidelity and reliability of traffic simulation for autonomous driving. Project Page: this https URL"
    },
    {
      "title": "From Frames to Sequences: Temporally Consistent Human-Centric Dense Prediction",
      "url": "https://arxiv.org/abs/2602.01661",
      "date": "2026-02-02",
      "authors_text": "Xingyu Miao, Junting Dong, Qin Zhao, Yuhang Yang, Junhao Chen",
      "is_highlight": false,
      "score": 43.0,
      "summary": "This paper presents a novel synthetic data pipeline and a ViT-based model for achieving temporally consistent human-centric dense prediction in video sequences, outperforming existing methods.",
      "teaser_image": "https://arxiv.org/html/2602.01661/x2.png",
      "debug_abstract": "In this work, we focus on the challenge of temporally consistent human-centric dense prediction across video sequences. Existing models achieve strong per-frame accuracy but often flicker under motion, occlusion, and lighting changes, and they rarely have paired human video supervision for multiple dense tasks. We address this gap with a scalable synthetic data pipeline that generates photorealistic human frames and motion-aligned sequences with pixel-accurate depth, normals, and masks. Unlike prior static data synthetic pipelines, our pipeline provides both frame-level labels for spatial learning and sequence-level supervision for temporal learning. Building on this, we train a unified ViT-based dense predictor that (i) injects an explicit human geometric prior via CSE embeddings and (ii) improves geometry-feature reliability with a lightweight channel reweighting module after feature fusion. Our two-stage training strategy, combining static pretraining with dynamic sequence supervision, enables the model first to acquire robust spatial representations and then refine temporal consistency across motion-aligned sequences. Extensive experiments show that we achieve state-of-the-art performance on THuman2.1 and Hi4D and generalize effectively to in-the-wild videos."
    },
    {
      "title": "Reasoning with Autoregressive-Diffusion Collaborative Thoughts",
      "url": "https://arxiv.org/abs/2602.01608",
      "date": "2026-02-02",
      "authors_text": "Mu Yuan, Liekang Zeng, Guoliang Xing, Lan Zhang, Yunhao Liu",
      "is_highlight": false,
      "score": 41.0,
      "summary": "The paper presents Collaborative Thoughts, a framework integrating autoregressive and diffusion models for enhanced spatial reasoning and generation through iterative feedback and joint reasoning.",
      "teaser_image": "https://arxiv.org/html/2602.01608/x2.png",
      "debug_abstract": "Autoregressive and diffusion models represent two complementary generative paradigms. Autoregressive models excel at sequential planning and constraint composition, yet struggle with tasks that require explicit spatial or physical grounding. Diffusion models, in contrast, capture rich spatial structure through high-dimensional generation, but lack the stepwise logical control needed to satisfy complex, multi-stage constraints or to reliably identify and correct errors. We introduce Collaborative Thoughts, a unified collaborative framework that enables autoregressive and diffusion models to reason and generate jointly through a closed-loop interaction. In Collaborative Thoughts, autoregressive models perform structured planning and constraint management, diffusion models instantiate these constraints as intermediate visual thoughts, and a vision-based critic module evaluates whether the visual thoughts satisfy the intended structural and physical requirements. This feedback is then used to iteratively refine subsequent planning and generation steps, mitigating error propagation across modalities. Importantly, Collaborative Thoughts uses the same collaborative loop regardless of whether the task is autoregressive question answering or diffusion-based visual generation. Through representative examples, we illustrate how Collaborative Thoughts can improve the reliability of spatial reasoning and the controllability of generation."
    },
    {
      "title": "Making Avatars Interact: Towards Text-Driven Human-Object Interaction for Controllable Talking Avatars",
      "url": "https://arxiv.org/abs/2602.01538",
      "date": "2026-02-02",
      "authors_text": "Youliang Zhang, Zhengguang Zhou, Zhentao Yu, Ziyao Huang, Teng Hu",
      "is_highlight": false,
      "score": 46.0,
      "summary": "The paper presents InteractAvatar, a dual-stream framework for generating talking avatars that perform text-driven human-object interactions by decoupling perception and planning from video synthesis.",
      "teaser_image": "https://arxiv.org/html/2602.01538/x1.png",
      "debug_abstract": "Generating talking avatars is a fundamental task in video generation. Although existing methods can generate full-body talking avatars with simple human motion, extending this task to grounded human-object interaction (GHOI) remains an open challenge, requiring the avatar to perform text-aligned interactions with surrounding objects. This challenge stems from the need for environmental perception and the control-quality dilemma in GHOI generation. To address this, we propose a novel dual-stream framework, InteractAvatar, which decouples perception and planning from video synthesis for grounded human-object interaction. Leveraging detection to enhance environmental perception, we introduce a Perception and Interaction Module (PIM) to generate text-aligned interaction motions. Additionally, an Audio-Interaction Aware Generation Module (AIM) is proposed to synthesize vivid talking avatars performing object interactions. With a specially designed motion-to-video aligner, PIM and AIM share a similar network structure and enable parallel co-generation of motions and plausible videos, effectively mitigating the control-quality dilemma. Finally, we establish a benchmark, GroundedInter, for evaluating GHOI video generation. Extensive experiments and comparisons demonstrate the effectiveness of our method in generating grounded human-object interactions for talking avatars. Project page: this https URL"
    },
    {
      "title": "Resource-Efficient Reinforcement for Reasoning Large Language Models via Dynamic One-Shot Policy Refinement",
      "url": "https://arxiv.org/abs/2602.00815",
      "date": "2026-01-31",
      "authors_text": "Yunjian Zhang, Sudong Wang, Yang Li, Peiran Xu, Conghao Zhou",
      "is_highlight": false,
      "score": 57.0,
      "summary": "The paper introduces Dynamic One-Shot Policy Refinement (DoPR), a resource-efficient reinforcement learning strategy that enhances reasoning in large language models while significantly reducing training costs.",
      "teaser_image": "https://arxiv.org/html/2602.00815/dopr.png",
      "debug_abstract": "Large language models (LLMs) have exhibited remarkable performance on complex reasoning tasks, with reinforcement learning under verifiable rewards (RLVR) emerging as a principled framework for aligning model behavior with reasoning chains. Despite its promise, RLVR remains prohibitively resource-intensive, requiring extensive reward signals and incurring substantial rollout costs during training. In this work, we revisit the fundamental question of data and compute efficiency in RLVR. We first establish a theoretical lower bound on the sample complexity required to unlock reasoning capabilities, and empirically validate that strong performance can be achieved with a surprisingly small number of training instances. To tackle the computational burden, we propose Dynamic One-Shot Policy Refinement (DoPR), an uncertainty-aware RL strategy that dynamically selects a single informative training sample per batch for policy updates, guided by reward volatility and exploration-driven acquisition. DoPR reduces rollout overhead by nearly an order of magnitude while preserving competitive reasoning accuracy, offering a scalable and resource-efficient solution for LLM post-training. This approach offers a practical path toward more efficient and accessible RL-based training for reasoning-intensive LLM applications."
    },
    {
      "title": "V2X-DSC: Multi-Agent Collaborative Perception with Distributed Source Coding Guided Communication",
      "url": "https://arxiv.org/abs/2602.00687",
      "date": "2026-01-31",
      "authors_text": "Yuankun Zeng, Shaohui Li, Zhi Li, Shulan Ruan, Yu Liu",
      "is_highlight": false,
      "score": 76.0,
      "summary": "V2X-DSC introduces a Conditional Codec for efficient multi-agent collaborative perception by compressing and reconstructing BEV features, optimizing bandwidth usage while maintaining accuracy.",
      "teaser_image": "https://arxiv.org/html/2602.00687/figs/overview.jpg",
      "debug_abstract": "Collaborative perception improves 3D understanding by fusing multi-agent observations, yet intermediate-feature sharing faces strict bandwidth constraints as dense BEV features saturate V2X links. We observe that collaborators view the same physical world, making their features strongly correlated; thus receivers only need innovation beyond their local context. Revisiting this from a distributed source coding perspective, we propose V2X-DSC, a framework with a Conditional Codec (DCC) for bandwidth-constrained fusion. The sender compresses BEV features into compact codes, while the receiver performs conditional reconstruction using its local features as side information, allocating bits to complementary cues rather than redundant content. This conditional structure regularizes learning, encouraging incremental representation and yielding lower-noise features. Experiments on DAIR-V2X, OPV2V, and V2X-Real demonstrate state-of-the-art accuracy-bandwidth trade-offs under KB-level communication, and generalizes as a plug-and-play communication layer across multiple fusion backbones."
    },
    {
      "title": "How Far Are LLMs from Professional Poker Players? Revisiting Game-Theoretic Reasoning with Agentic Tool Use",
      "url": "https://arxiv.org/abs/2602.00528",
      "date": "2026-01-31",
      "authors_text": "Minhua Lin, Enyan Dai, Hui Liu, Xianfeng Tang, Yuliang Yan",
      "is_highlight": false,
      "score": 55.0,
      "summary": "This paper evaluates LLMs in poker, revealing their strategic reasoning flaws and proposing ToolPoker, which enhances gameplay and reasoning through integrated external solvers.",
      "teaser_image": "https://arxiv.org/html/2602.00528/x2.png",
      "debug_abstract": "As Large Language Models (LLMs) are increasingly applied in high-stakes domains, their ability to reason strategically under uncertainty becomes critical. Poker provides a rigorous testbed, requiring not only strong actions but also principled, game-theoretic reasoning. In this paper, we conduct a systematic study of LLMs in multiple realistic poker tasks, evaluating both gameplay outcomes and reasoning traces. Our analysis reveals LLMs fail to compete against traditional algorithms and identifies three recurring flaws: reliance on heuristics, factual misunderstandings, and a &#34;knowing-doing&#34; gap where actions diverge from reasoning. An initial attempt with behavior cloning and step-level reinforcement learning improves reasoning style but remains insufficient for accurate game-theoretic play. Motivated by these limitations, we propose ToolPoker, a tool-integrated reasoning framework that combines external solvers for GTO-consistent actions with more precise professional-style explanations. Experiments demonstrate that ToolPoker achieves state-of-the-art gameplay while producing reasoning traces that closely reflect game-theoretic principles."
    },
    {
      "title": "Learning Physics-Grounded 4D Dynamics with Neural Gaussian Force Fields",
      "url": "https://arxiv.org/abs/2602.00148",
      "date": "2026-01-29",
      "authors_text": "Shiqian Li, Ruihong Shen, Junfeng Ni, Chang Pan, Chi Zhang",
      "is_highlight": false,
      "score": 82.0,
      "summary": "The Neural Gaussian Force Field (NGFF) framework enables rapid generation of interactive, physically realistic 4D videos from multi-view RGB inputs, enhancing robustness and generalization in physical dynamics.",
      "teaser_image": "https://arxiv.org/html/2602.00148/x1.png",
      "debug_abstract": "Predicting physical dynamics from raw visual data remains a major challenge in AI. While recent video generation models have achieved impressive visual quality, they still cannot consistently generate physically plausible videos due to a lack of modeling of physical laws. Recent approaches combining 3D Gaussian splatting and physics engines can produce physically plausible videos, but are hindered by high computational costs in both reconstruction and simulation, and often lack robustness in complex real-world scenarios. To address these issues, we introduce Neural Gaussian Force Field (NGFF), an end-to-end neural framework that integrates 3D Gaussian perception with physics-based dynamic modeling to generate interactive, physically realistic 4D videos from multi-view RGB inputs, achieving two orders of magnitude faster than prior Gaussian simulators. To support training, we also present GSCollision, a 4D Gaussian dataset featuring diverse materials, multi-object interactions, and complex scenes, totaling over 640k rendered physical videos (~4 TB). Evaluations on synthetic and real 3D scenarios show NGFF&#39;s strong generalization and robustness in physical reasoning, advancing video prediction towards physics-grounded world models."
    }
  ],
  "智能系统与机器人实验室 (ISR Lab)": [
    {
      "title": "Enhancing Indoor Occupancy Prediction via Sparse Query-Based Multi-Level Consistent Knowledge Distillation",
      "url": "https://arxiv.org/abs/2602.02318",
      "date": "2026-02-02",
      "authors_text": "Xiang Li, Yupeng Zheng, Pengfei Li, Yilun Chen, Ya-Qin Zhang",
      "is_highlight": false,
      "score": 0,
      "summary": "DiScene introduces a sparse query-based framework for efficient and robust indoor occupancy prediction through multi-level knowledge distillation, achieving state-of-the-art performance and faster inference.",
      "teaser_image": "https://arxiv.org/html/2602.02318/x2.png",
      "debug_abstract": "Occupancy prediction provides critical geometric and semantic understanding for robotics but faces efficiency-accuracy trade-offs. Current dense methods suffer computational waste on empty voxels, while sparse query-based approaches lack robustness in diverse and complex indoor scenes. In this paper, we propose DiScene, a novel sparse query-based framework that leverages multi-level distillation to achieve efficient and robust occupancy prediction. In particular, our method incorporates two key innovations: (1) a Multi-level Consistent Knowledge Distillation strategy, which transfers hierarchical representations from large teacher models to lightweight students through coordinated alignment across four levels, including encoder-level feature alignment, query-level feature matching, prior-level spatial guidance, and anchor-level high-confidence knowledge transfer and (2) a Teacher-Guided Initialization policy, employing optimized parameter warm-up to accelerate model convergence. Validated on the Occ-Scannet benchmark, DiScene achieves 23.2 FPS without depth priors while outperforming our baseline method, OPUS, by 36.1% and even better than the depth-enhanced version, OPUS†. With depth integration, DiScene† attains new SOTA performance, surpassing EmbodiedOcc by 3.7% with 1.62$\\times$ faster inference speed. Furthermore, experiments on the Occ3D-nuScenes benchmark and in-the-wild scenarios demonstrate the versatility of our approach in various environments. Code and models can be accessed at this https URL."
    },
    {
      "title": "UV-M3TL: A Unified and Versatile Multimodal Multi-Task Learning Framework for Assistive Driving Perception",
      "url": "https://arxiv.org/abs/2602.01594",
      "date": "2026-02-02",
      "authors_text": "Wenzhuo Liu, Qiannan Guo, Zhen Wang, Wenshuo Wang, Lei Yang",
      "is_highlight": false,
      "score": 8.0,
      "summary": "The UV-M3TL framework enhances ADAS by effectively recognizing driver behavior and context through dual-branch multimodal embedding and adaptive loss, achieving state-of-the-art performance across multiple tasks.",
      "teaser_image": "https://arxiv.org/html/2602.01594/x2.png",
      "debug_abstract": "Advanced Driver Assistance Systems (ADAS) need to understand human driver behavior while perceiving their navigation context, but jointly learning these heterogeneous tasks would cause inter-task negative transfer and impair system performance. Here, we propose a Unified and Versatile Multimodal Multi-Task Learning (UV-M3TL) framework to simultaneously recognize driver behavior, driver emotion, vehicle behavior, and traffic context, while mitigating inter-task negative transfer. Our framework incorporates two core components: dual-branch spatial channel multimodal embedding (DB-SCME) and adaptive feature-decoupled multi-task loss (AFD-Loss). DB-SCME enhances cross-task knowledge transfer while mitigating task conflicts by employing a dual-branch structure to explicitly model salient task-shared and task-specific features. AFD-Loss improves the stability of joint optimization while guiding the model to learn diverse multi-task representations by introducing an adaptive weighting mechanism based on learning dynamics and feature decoupling constraints. We evaluate our method on the AIDE dataset, and the experimental results demonstrate that UV-M3TL achieves state-of-the-art performance across all four tasks. To further prove the versatility, we evaluate UV-M3TL on additional public multi-task perception benchmarks (BDD100K, CityScapes, NYUD-v2, and PASCAL-Context), where it consistently delivers strong performance across diverse task combinations, attaining state-of-the-art results on most tasks."
    },
    {
      "title": "Context Learning for Multi-Agent Discussion",
      "url": "https://arxiv.org/abs/2602.02350",
      "date": "2026-02-02",
      "authors_text": "Xingyuan Hua, Sheng Yue, Xinyi Li, Yizhe Zhao, Jinrui Zhang",
      "is_highlight": false,
      "score": 17.0,
      "summary": "The paper presents M2CL, a multi-LLM context learning method that enhances coherence in Multi-Agent Discussions, improving problem-solving performance by 20%-50% over existing approaches.",
      "teaser_image": "https://arxiv.org/html/2602.02350/x2.png",
      "debug_abstract": "Multi-Agent Discussion (MAD) has garnered increasing attention very recently, where multiple LLM instances collaboratively solve problems via structured discussion. However, we find that current MAD methods easily suffer from discussion inconsistency, LLMs fail to reach a coherent solution, due to the misalignment between their individual this http URL this paper, we introduce a multi-LLM context learning method (M2CL) that learns a context generator for each agent, capable of dynamically generating context instructions per discussion round via automatic information organization and refinement. Specifically, inspired by our theoretical insights on the context instruction, M2CL train the generators to control context coherence and output discrepancies via a carefully crafted self-adaptive this http URL enables LLMs to avoid premature convergence on majority noise and progressively reach the correct consensus. We evaluate M2CL on challenging tasks, including academic reasoning, embodied tasks, and mobile control. The results show that the performance of M2CL significantly surpasses existing methods by 20%--50%, while enjoying favorable transferability and computational efficiency."
    },
    {
      "title": "TTT-Parkour: Rapid Test-Time Training for Perceptive Robot Parkour",
      "url": "https://arxiv.org/abs/2602.02331",
      "date": "2026-02-02",
      "authors_text": "Shaoting Zhu, Baijun Ye, Jiaxuan Wang, Jiakang Chen, Ziwen Zhuang",
      "is_highlight": false,
      "score": 18.0,
      "summary": "TTT-Parkour introduces a rapid test-time training framework enabling humanoid robots to effectively navigate complex terrains through efficient real-to-sim-to-real learning and geometry reconstruction.",
      "teaser_image": "https://arxiv.org/html/2602.02331/x1.png",
      "debug_abstract": "Achieving highly dynamic humanoid parkour on unseen, complex terrains remains a challenge in robotics. Although general locomotion policies demonstrate capabilities across broad terrain distributions, they often struggle with arbitrary and highly challenging environments. To overcome this limitation, we propose a real-to-sim-to-real framework that leverages rapid test-time training (TTT) on novel terrains, significantly enhancing the robot&#39;s capability to traverse extremely difficult geometries. We adopt a two-stage end-to-end learning paradigm: a policy is first pre-trained on diverse procedurally generated terrains, followed by rapid fine-tuning on high-fidelity meshes reconstructed from real-world captures. Specifically, we develop a feed-forward, efficient, and high-fidelity geometry reconstruction pipeline using RGB-D inputs, ensuring both speed and quality during test-time training. We demonstrate that TTT-Parkour empowers humanoid robots to master complex obstacles, including wedges, stakes, boxes, trapezoids, and narrow beams. The whole pipeline of capturing, reconstructing, and test-time training requires less than 10 minutes on most tested terrains. Extensive experiments show that the policy after test-time training exhibits robust zero-shot sim-to-real transfer capability."
    },
    {
      "title": "Small Generalizable Prompt Predictive Models Can Steer Efficient RL Post-Training of Large Reasoning Models",
      "url": "https://arxiv.org/abs/2602.01970",
      "date": "2026-02-02",
      "authors_text": "Yun Qu, Qi Wang, Yixiu Mao, Heming Zou, Yuhang Jiang",
      "is_highlight": false,
      "score": 26.0,
      "summary": "The paper presents Generalizable Predictive Prompt Selection (GPS), a method that enhances reinforcement learning efficiency in large language models by utilizing a lightweight generative model for prompt selection.",
      "teaser_image": "https://arxiv.org/html/2602.01970/x1.png",
      "debug_abstract": "Reinforcement learning enhances the reasoning capabilities of large language models but often involves high computational costs due to rollout-intensive optimization. Online prompt selection presents a plausible solution by prioritizing informative prompts to improve training efficiency. However, current methods either depend on costly, exact evaluations or construct prompt-specific predictive models lacking generalization across prompts. This study introduces Generalizable Predictive Prompt Selection (GPS), which performs Bayesian inference towards prompt difficulty using a lightweight generative model trained on the shared optimization history. Intermediate-difficulty prioritization and history-anchored diversity are incorporated into the batch acquisition principle to select informative prompt batches. The small predictive model also generalizes at test-time for efficient computational allocation. Experiments across varied reasoning benchmarks indicate GPS&#39;s substantial improvements in training efficiency, final performance, and test-time efficiency over superior baseline methods."
    },
    {
      "title": "ForSim: Stepwise Forward Simulation for Traffic Policy Fine-Tuning",
      "url": "https://arxiv.org/abs/2602.01916",
      "date": "2026-02-02",
      "authors_text": "Keyu Chen, Wenchao Sun, Hao Cheng, Zheng Fu, Sifa Zheng",
      "is_highlight": true,
      "score": 29.0,
      "summary": "ForSim introduces a stepwise closed-loop simulation method that enhances traffic policy fine-tuning by improving agent interactions and preserving multimodal behaviors in autonomous driving.",
      "teaser_image": "https://arxiv.org/html/2602.01916/x2.png",
      "debug_abstract": "As the foundation of closed-loop training and evaluation in autonomous driving, traffic simulation still faces two fundamental challenges: covariate shift introduced by open-loop imitation learning and limited capacity to reflect the multimodal behaviors observed in real-world traffic. Although recent frameworks such as RIFT have partially addressed these issues through group-relative optimization, their forward simulation procedures remain largely non-reactive, leading to unrealistic agent interactions within the virtual domain and ultimately limiting simulation fidelity. To address these issues, we propose ForSim, a stepwise closed-loop forward simulation paradigm. At each virtual timestep, the traffic agent propagates the virtual candidate trajectory that best spatiotemporally matches the reference trajectory through physically grounded motion dynamics, thereby preserving multimodal behavioral diversity while ensuring intra-modality consistency. Other agents are updated with stepwise predictions, yielding coherent and interaction-aware evolution. When incorporated into the RIFT traffic simulation framework, ForSim operates in conjunction with group-relative optimization to fine-tune traffic policy. Extensive experiments confirm that this integration consistently improves safety while maintaining efficiency, realism, and comfort. These results underscore the importance of modeling closed-loop multimodal interactions within forward simulation and enhance the fidelity and reliability of traffic simulation for autonomous driving. Project Page: this https URL"
    },
    {
      "title": "From Frames to Sequences: Temporally Consistent Human-Centric Dense Prediction",
      "url": "https://arxiv.org/abs/2602.01661",
      "date": "2026-02-02",
      "authors_text": "Xingyu Miao, Junting Dong, Qin Zhao, Yuhang Yang, Junhao Chen",
      "is_highlight": false,
      "score": 43.0,
      "summary": "This paper presents a novel synthetic data pipeline and a ViT-based model for achieving temporally consistent human-centric dense prediction in video sequences, outperforming existing methods.",
      "teaser_image": "https://arxiv.org/html/2602.01661/x2.png",
      "debug_abstract": "In this work, we focus on the challenge of temporally consistent human-centric dense prediction across video sequences. Existing models achieve strong per-frame accuracy but often flicker under motion, occlusion, and lighting changes, and they rarely have paired human video supervision for multiple dense tasks. We address this gap with a scalable synthetic data pipeline that generates photorealistic human frames and motion-aligned sequences with pixel-accurate depth, normals, and masks. Unlike prior static data synthetic pipelines, our pipeline provides both frame-level labels for spatial learning and sequence-level supervision for temporal learning. Building on this, we train a unified ViT-based dense predictor that (i) injects an explicit human geometric prior via CSE embeddings and (ii) improves geometry-feature reliability with a lightweight channel reweighting module after feature fusion. Our two-stage training strategy, combining static pretraining with dynamic sequence supervision, enables the model first to acquire robust spatial representations and then refine temporal consistency across motion-aligned sequences. Extensive experiments show that we achieve state-of-the-art performance on THuman2.1 and Hi4D and generalize effectively to in-the-wild videos."
    },
    {
      "title": "Reasoning with Autoregressive-Diffusion Collaborative Thoughts",
      "url": "https://arxiv.org/abs/2602.01608",
      "date": "2026-02-02",
      "authors_text": "Mu Yuan, Liekang Zeng, Guoliang Xing, Lan Zhang, Yunhao Liu",
      "is_highlight": false,
      "score": 41.0,
      "summary": "The paper presents Collaborative Thoughts, a framework integrating autoregressive and diffusion models for enhanced spatial reasoning and generation through iterative feedback and joint reasoning.",
      "teaser_image": "https://arxiv.org/html/2602.01608/x2.png",
      "debug_abstract": "Autoregressive and diffusion models represent two complementary generative paradigms. Autoregressive models excel at sequential planning and constraint composition, yet struggle with tasks that require explicit spatial or physical grounding. Diffusion models, in contrast, capture rich spatial structure through high-dimensional generation, but lack the stepwise logical control needed to satisfy complex, multi-stage constraints or to reliably identify and correct errors. We introduce Collaborative Thoughts, a unified collaborative framework that enables autoregressive and diffusion models to reason and generate jointly through a closed-loop interaction. In Collaborative Thoughts, autoregressive models perform structured planning and constraint management, diffusion models instantiate these constraints as intermediate visual thoughts, and a vision-based critic module evaluates whether the visual thoughts satisfy the intended structural and physical requirements. This feedback is then used to iteratively refine subsequent planning and generation steps, mitigating error propagation across modalities. Importantly, Collaborative Thoughts uses the same collaborative loop regardless of whether the task is autoregressive question answering or diffusion-based visual generation. Through representative examples, we illustrate how Collaborative Thoughts can improve the reliability of spatial reasoning and the controllability of generation."
    },
    {
      "title": "Making Avatars Interact: Towards Text-Driven Human-Object Interaction for Controllable Talking Avatars",
      "url": "https://arxiv.org/abs/2602.01538",
      "date": "2026-02-02",
      "authors_text": "Youliang Zhang, Zhengguang Zhou, Zhentao Yu, Ziyao Huang, Teng Hu",
      "is_highlight": false,
      "score": 46.0,
      "summary": "The paper presents InteractAvatar, a dual-stream framework for generating talking avatars that perform text-driven human-object interactions by decoupling perception and planning from video synthesis.",
      "teaser_image": "https://arxiv.org/html/2602.01538/x1.png",
      "debug_abstract": "Generating talking avatars is a fundamental task in video generation. Although existing methods can generate full-body talking avatars with simple human motion, extending this task to grounded human-object interaction (GHOI) remains an open challenge, requiring the avatar to perform text-aligned interactions with surrounding objects. This challenge stems from the need for environmental perception and the control-quality dilemma in GHOI generation. To address this, we propose a novel dual-stream framework, InteractAvatar, which decouples perception and planning from video synthesis for grounded human-object interaction. Leveraging detection to enhance environmental perception, we introduce a Perception and Interaction Module (PIM) to generate text-aligned interaction motions. Additionally, an Audio-Interaction Aware Generation Module (AIM) is proposed to synthesize vivid talking avatars performing object interactions. With a specially designed motion-to-video aligner, PIM and AIM share a similar network structure and enable parallel co-generation of motions and plausible videos, effectively mitigating the control-quality dilemma. Finally, we establish a benchmark, GroundedInter, for evaluating GHOI video generation. Extensive experiments and comparisons demonstrate the effectiveness of our method in generating grounded human-object interactions for talking avatars. Project page: this https URL"
    },
    {
      "title": "Resource-Efficient Reinforcement for Reasoning Large Language Models via Dynamic One-Shot Policy Refinement",
      "url": "https://arxiv.org/abs/2602.00815",
      "date": "2026-01-31",
      "authors_text": "Yunjian Zhang, Sudong Wang, Yang Li, Peiran Xu, Conghao Zhou",
      "is_highlight": false,
      "score": 57.0,
      "summary": "The paper introduces Dynamic One-Shot Policy Refinement (DoPR), a resource-efficient reinforcement learning strategy that enhances reasoning in large language models while significantly reducing training costs.",
      "teaser_image": "https://arxiv.org/html/2602.00815/dopr.png",
      "debug_abstract": "Large language models (LLMs) have exhibited remarkable performance on complex reasoning tasks, with reinforcement learning under verifiable rewards (RLVR) emerging as a principled framework for aligning model behavior with reasoning chains. Despite its promise, RLVR remains prohibitively resource-intensive, requiring extensive reward signals and incurring substantial rollout costs during training. In this work, we revisit the fundamental question of data and compute efficiency in RLVR. We first establish a theoretical lower bound on the sample complexity required to unlock reasoning capabilities, and empirically validate that strong performance can be achieved with a surprisingly small number of training instances. To tackle the computational burden, we propose Dynamic One-Shot Policy Refinement (DoPR), an uncertainty-aware RL strategy that dynamically selects a single informative training sample per batch for policy updates, guided by reward volatility and exploration-driven acquisition. DoPR reduces rollout overhead by nearly an order of magnitude while preserving competitive reasoning accuracy, offering a scalable and resource-efficient solution for LLM post-training. This approach offers a practical path toward more efficient and accessible RL-based training for reasoning-intensive LLM applications."
    },
    {
      "title": "V2X-DSC: Multi-Agent Collaborative Perception with Distributed Source Coding Guided Communication",
      "url": "https://arxiv.org/abs/2602.00687",
      "date": "2026-01-31",
      "authors_text": "Yuankun Zeng, Shaohui Li, Zhi Li, Shulan Ruan, Yu Liu",
      "is_highlight": false,
      "score": 76.0,
      "summary": "V2X-DSC introduces a Conditional Codec for efficient multi-agent collaborative perception by compressing and reconstructing BEV features, optimizing bandwidth usage while maintaining accuracy.",
      "teaser_image": "https://arxiv.org/html/2602.00687/figs/overview.jpg",
      "debug_abstract": "Collaborative perception improves 3D understanding by fusing multi-agent observations, yet intermediate-feature sharing faces strict bandwidth constraints as dense BEV features saturate V2X links. We observe that collaborators view the same physical world, making their features strongly correlated; thus receivers only need innovation beyond their local context. Revisiting this from a distributed source coding perspective, we propose V2X-DSC, a framework with a Conditional Codec (DCC) for bandwidth-constrained fusion. The sender compresses BEV features into compact codes, while the receiver performs conditional reconstruction using its local features as side information, allocating bits to complementary cues rather than redundant content. This conditional structure regularizes learning, encouraging incremental representation and yielding lower-noise features. Experiments on DAIR-V2X, OPV2V, and V2X-Real demonstrate state-of-the-art accuracy-bandwidth trade-offs under KB-level communication, and generalizes as a plug-and-play communication layer across multiple fusion backbones."
    },
    {
      "title": "How Far Are LLMs from Professional Poker Players? Revisiting Game-Theoretic Reasoning with Agentic Tool Use",
      "url": "https://arxiv.org/abs/2602.00528",
      "date": "2026-01-31",
      "authors_text": "Minhua Lin, Enyan Dai, Hui Liu, Xianfeng Tang, Yuliang Yan",
      "is_highlight": false,
      "score": 55.0,
      "summary": "This paper evaluates LLMs in poker, revealing their strategic reasoning flaws and proposing ToolPoker, which enhances gameplay and reasoning through integrated external solvers.",
      "teaser_image": "https://arxiv.org/html/2602.00528/x2.png",
      "debug_abstract": "As Large Language Models (LLMs) are increasingly applied in high-stakes domains, their ability to reason strategically under uncertainty becomes critical. Poker provides a rigorous testbed, requiring not only strong actions but also principled, game-theoretic reasoning. In this paper, we conduct a systematic study of LLMs in multiple realistic poker tasks, evaluating both gameplay outcomes and reasoning traces. Our analysis reveals LLMs fail to compete against traditional algorithms and identifies three recurring flaws: reliance on heuristics, factual misunderstandings, and a &#34;knowing-doing&#34; gap where actions diverge from reasoning. An initial attempt with behavior cloning and step-level reinforcement learning improves reasoning style but remains insufficient for accurate game-theoretic play. Motivated by these limitations, we propose ToolPoker, a tool-integrated reasoning framework that combines external solvers for GTO-consistent actions with more precise professional-style explanations. Experiments demonstrate that ToolPoker achieves state-of-the-art gameplay while producing reasoning traces that closely reflect game-theoretic principles."
    },
    {
      "title": "Learning Physics-Grounded 4D Dynamics with Neural Gaussian Force Fields",
      "url": "https://arxiv.org/abs/2602.00148",
      "date": "2026-01-29",
      "authors_text": "Shiqian Li, Ruihong Shen, Junfeng Ni, Chang Pan, Chi Zhang",
      "is_highlight": false,
      "score": 82.0,
      "summary": "The Neural Gaussian Force Field (NGFF) framework enables rapid generation of interactive, physically realistic 4D videos from multi-view RGB inputs, enhancing robustness and generalization in physical dynamics.",
      "teaser_image": "https://arxiv.org/html/2602.00148/x1.png",
      "debug_abstract": "Predicting physical dynamics from raw visual data remains a major challenge in AI. While recent video generation models have achieved impressive visual quality, they still cannot consistently generate physically plausible videos due to a lack of modeling of physical laws. Recent approaches combining 3D Gaussian splatting and physics engines can produce physically plausible videos, but are hindered by high computational costs in both reconstruction and simulation, and often lack robustness in complex real-world scenarios. To address these issues, we introduce Neural Gaussian Force Field (NGFF), an end-to-end neural framework that integrates 3D Gaussian perception with physics-based dynamic modeling to generate interactive, physically realistic 4D videos from multi-view RGB inputs, achieving two orders of magnitude faster than prior Gaussian simulators. To support training, we also present GSCollision, a 4D Gaussian dataset featuring diverse materials, multi-object interactions, and complex scenes, totaling over 640k rendered physical videos (~4 TB). Evaluations on synthetic and real 3D scenarios show NGFF&#39;s strong generalization and robustness in physical reasoning, advancing video prediction towards physics-grounded world models."
    }
  ],
  "智元机器人联合实验室 (PKU-Agibot)": [
    {
      "title": "Superman: Unifying Skeleton and Vision for Human Motion Perception and Generation",
      "url": "https://arxiv.org/abs/2602.02401",
      "date": "2026-02-02",
      "authors_text": "Xinshun Wang, Peiming Li, Ziyi Wang, Zhongbin Fang, Zhichao Deng",
      "is_highlight": false,
      "score": 15.0,
      "summary": "Superman unifies visual perception and skeleton-based motion generation through a Vision-Guided Motion Tokenizer, enabling robust cross-modal learning and superior performance in human motion tasks.",
      "teaser_image": "https://arxiv.org/html/2602.02401/x2.png",
      "debug_abstract": "Human motion analysis tasks, such as temporal 3D pose estimation, motion prediction, and motion in-betweening, play an essential role in computer vision. However, current paradigms suffer from severe fragmentation. First, the field is split between ``perception&#39;&#39; models that understand motion from video but only output text, and ``generation&#39;&#39; models that cannot perceive from raw visual input. Second, generative MLLMs are often limited to single-frame, static poses using dense, parametric SMPL models, failing to handle temporal motion. Third, existing motion vocabularies are built from skeleton data alone, severing the link to the visual domain. To address these challenges, we introduce Superman, a unified framework that bridges visual perception with temporal, skeleton-based motion generation. Our solution is twofold. First, to overcome the modality disconnect, we propose a Vision-Guided Motion Tokenizer. Leveraging the natural geometric alignment between 3D skeletons and visual data, this module pioneers robust joint learning from both modalities, creating a unified, cross-modal motion vocabulary. Second, grounded in this motion language, a single, unified MLLM architecture is trained to handle all tasks. This module flexibly processes diverse, temporal inputs, unifying 3D skeleton pose estimation from video (perception) with skeleton-based motion prediction and in-betweening (generation). Extensive experiments on standard benchmarks, including Human3.6M, demonstrate that our unified method achieves state-of-the-art or competitive performance across all motion tasks. This showcases a more efficient and scalable path for generative motion analysis using skeletons."
    },
    {
      "title": "Not All Preferences Are Created Equal: Stability-Aware and Gradient-Efficient Alignment for Reasoning Models",
      "url": "https://arxiv.org/abs/2602.01207",
      "date": "2026-02-01",
      "authors_text": "Hui Wu, Hengyi Cai, Jinman Zhao, Xinran Chen, Ziheng Li",
      "is_highlight": false,
      "score": 48.0,
      "summary": "The paper introduces SAGE, a dynamic framework for preference-based alignment in reasoning models that enhances optimization stability and efficiency by prioritizing informative training instances.",
      "teaser_image": "https://arxiv.org/html/2602.01207/x2.png",
      "debug_abstract": "Preference-based alignment is pivotal for training large reasoning models; however, standard methods like Direct Preference Optimization (DPO) typically treat all preference pairs uniformly, overlooking the evolving utility of training instances. This static approach often leads to inefficient or unstable optimization, as it wastes computation on trivial pairs with negligible gradients and suffers from noise induced by samples near uncertain decision boundaries. Facing these challenges, we propose SAGE (Stability-Aware Gradient Efficiency), a dynamic framework designed to enhance alignment reliability by maximizing the Signal-to-Noise Ratio of policy updates. Concretely, SAGE integrates a coarse-grained curriculum mechanism that refreshes candidate pools based on model competence with a fine-grained, stability-aware scoring function that prioritizes informative, confident errors while filtering out unstable samples. Experiments on multiple mathematical reasoning benchmarks demonstrate that SAGE significantly accelerates convergence and outperforms static baselines, highlighting the critical role of policy-aware, stability-conscious data selection in reasoning alignment."
    },
    {
      "title": "Residual Decoding: Mitigating Hallucinations in Large Vision-Language Models via History-Aware Residual Guidance",
      "url": "https://arxiv.org/abs/2602.01047",
      "date": "2026-02-01",
      "authors_text": "Xinrong Chen, Xu Chu, Yingmin Qiu, Hengyuan Zhang, Jing Xiong",
      "is_highlight": false,
      "score": 53.0,
      "summary": "Residual Decoding (ResDec) mitigates hallucinations in Large Vision-Language Models by leveraging historical information to enhance decoding and improve visual grounding.",
      "teaser_image": "https://arxiv.org/html/2602.01047/x1.png",
      "debug_abstract": "Large Vision-Language Models (LVLMs) can reason effectively from image-text inputs and perform well in various multimodal tasks. Despite this success, they are affected by language priors and often produce hallucinations. Hallucinations denote generated content that is grammatically and syntactically coherent, yet bears no match or direct relevance to actual visual input. To address this problem, we propose Residual Decoding (ResDec). It is a novel training-free method that uses historical information to aid decoding. The method relies on the internal implicit reasoning mechanism and token logits evolution mechanism of LVLMs to correct biases. Extensive experiments demonstrate that ResDec effectively suppresses hallucinations induced by language priors, significantly improves visual grounding, and reduces object hallucinations. In addition to mitigating hallucinations, ResDec also performs exceptionally well on comprehensive LVLM benchmarks, highlighting its broad applicability."
    },
    {
      "title": "Resource-Efficient Reinforcement for Reasoning Large Language Models via Dynamic One-Shot Policy Refinement",
      "url": "https://arxiv.org/abs/2602.00815",
      "date": "2026-01-31",
      "authors_text": "Yunjian Zhang, Sudong Wang, Yang Li, Peiran Xu, Conghao Zhou",
      "is_highlight": false,
      "score": 57.0,
      "summary": "The paper introduces Dynamic One-Shot Policy Refinement (DoPR), a resource-efficient reinforcement learning strategy that enhances reasoning in large language models while significantly reducing training costs.",
      "teaser_image": "https://arxiv.org/html/2602.00815/dopr.png",
      "debug_abstract": "Large language models (LLMs) have exhibited remarkable performance on complex reasoning tasks, with reinforcement learning under verifiable rewards (RLVR) emerging as a principled framework for aligning model behavior with reasoning chains. Despite its promise, RLVR remains prohibitively resource-intensive, requiring extensive reward signals and incurring substantial rollout costs during training. In this work, we revisit the fundamental question of data and compute efficiency in RLVR. We first establish a theoretical lower bound on the sample complexity required to unlock reasoning capabilities, and empirically validate that strong performance can be achieved with a surprisingly small number of training instances. To tackle the computational burden, we propose Dynamic One-Shot Policy Refinement (DoPR), an uncertainty-aware RL strategy that dynamically selects a single informative training sample per batch for policy updates, guided by reward volatility and exploration-driven acquisition. DoPR reduces rollout overhead by nearly an order of magnitude while preserving competitive reasoning accuracy, offering a scalable and resource-efficient solution for LLM post-training. This approach offers a practical path toward more efficient and accessible RL-based training for reasoning-intensive LLM applications."
    },
    {
      "title": "Learning Physics-Grounded 4D Dynamics with Neural Gaussian Force Fields",
      "url": "https://arxiv.org/abs/2602.00148",
      "date": "2026-01-29",
      "authors_text": "Shiqian Li, Ruihong Shen, Junfeng Ni, Chang Pan, Chi Zhang",
      "is_highlight": false,
      "score": 82.0,
      "summary": "The Neural Gaussian Force Field (NGFF) framework enables rapid generation of interactive, physically realistic 4D videos from multi-view RGB inputs, enhancing robustness and generalization in physical dynamics.",
      "teaser_image": "https://arxiv.org/html/2602.00148/x1.png",
      "debug_abstract": "Predicting physical dynamics from raw visual data remains a major challenge in AI. While recent video generation models have achieved impressive visual quality, they still cannot consistently generate physically plausible videos due to a lack of modeling of physical laws. Recent approaches combining 3D Gaussian splatting and physics engines can produce physically plausible videos, but are hindered by high computational costs in both reconstruction and simulation, and often lack robustness in complex real-world scenarios. To address these issues, we introduce Neural Gaussian Force Field (NGFF), an end-to-end neural framework that integrates 3D Gaussian perception with physics-based dynamic modeling to generate interactive, physically realistic 4D videos from multi-view RGB inputs, achieving two orders of magnitude faster than prior Gaussian simulators. To support training, we also present GSCollision, a 4D Gaussian dataset featuring diverse materials, multi-object interactions, and complex scenes, totaling over 640k rendered physical videos (~4 TB). Evaluations on synthetic and real 3D scenarios show NGFF&#39;s strong generalization and robustness in physical reasoning, advancing video prediction towards physics-grounded world models."
    }
  ],
  "HMI Lab": [
    {
      "title": "Superman: Unifying Skeleton and Vision for Human Motion Perception and Generation",
      "url": "https://arxiv.org/abs/2602.02401",
      "date": "2026-02-02",
      "authors_text": "Xinshun Wang, Peiming Li, Ziyi Wang, Zhongbin Fang, Zhichao Deng",
      "is_highlight": false,
      "score": 15.0,
      "summary": "Superman unifies visual perception and skeleton-based motion generation through a Vision-Guided Motion Tokenizer, enabling robust cross-modal learning and superior performance in human motion tasks.",
      "teaser_image": "https://arxiv.org/html/2602.02401/x2.png",
      "debug_abstract": "Human motion analysis tasks, such as temporal 3D pose estimation, motion prediction, and motion in-betweening, play an essential role in computer vision. However, current paradigms suffer from severe fragmentation. First, the field is split between ``perception&#39;&#39; models that understand motion from video but only output text, and ``generation&#39;&#39; models that cannot perceive from raw visual input. Second, generative MLLMs are often limited to single-frame, static poses using dense, parametric SMPL models, failing to handle temporal motion. Third, existing motion vocabularies are built from skeleton data alone, severing the link to the visual domain. To address these challenges, we introduce Superman, a unified framework that bridges visual perception with temporal, skeleton-based motion generation. Our solution is twofold. First, to overcome the modality disconnect, we propose a Vision-Guided Motion Tokenizer. Leveraging the natural geometric alignment between 3D skeletons and visual data, this module pioneers robust joint learning from both modalities, creating a unified, cross-modal motion vocabulary. Second, grounded in this motion language, a single, unified MLLM architecture is trained to handle all tasks. This module flexibly processes diverse, temporal inputs, unifying 3D skeleton pose estimation from video (perception) with skeleton-based motion prediction and in-betweening (generation). Extensive experiments on standard benchmarks, including Human3.6M, demonstrate that our unified method achieves state-of-the-art or competitive performance across all motion tasks. This showcases a more efficient and scalable path for generative motion analysis using skeletons."
    },
    {
      "title": "Not All Preferences Are Created Equal: Stability-Aware and Gradient-Efficient Alignment for Reasoning Models",
      "url": "https://arxiv.org/abs/2602.01207",
      "date": "2026-02-01",
      "authors_text": "Hui Wu, Hengyi Cai, Jinman Zhao, Xinran Chen, Ziheng Li",
      "is_highlight": false,
      "score": 48.0,
      "summary": "The paper introduces SAGE, a dynamic framework for preference-based alignment in reasoning models that enhances optimization stability and efficiency by prioritizing informative training instances.",
      "teaser_image": "https://arxiv.org/html/2602.01207/x2.png",
      "debug_abstract": "Preference-based alignment is pivotal for training large reasoning models; however, standard methods like Direct Preference Optimization (DPO) typically treat all preference pairs uniformly, overlooking the evolving utility of training instances. This static approach often leads to inefficient or unstable optimization, as it wastes computation on trivial pairs with negligible gradients and suffers from noise induced by samples near uncertain decision boundaries. Facing these challenges, we propose SAGE (Stability-Aware Gradient Efficiency), a dynamic framework designed to enhance alignment reliability by maximizing the Signal-to-Noise Ratio of policy updates. Concretely, SAGE integrates a coarse-grained curriculum mechanism that refreshes candidate pools based on model competence with a fine-grained, stability-aware scoring function that prioritizes informative, confident errors while filtering out unstable samples. Experiments on multiple mathematical reasoning benchmarks demonstrate that SAGE significantly accelerates convergence and outperforms static baselines, highlighting the critical role of policy-aware, stability-conscious data selection in reasoning alignment."
    },
    {
      "title": "Residual Decoding: Mitigating Hallucinations in Large Vision-Language Models via History-Aware Residual Guidance",
      "url": "https://arxiv.org/abs/2602.01047",
      "date": "2026-02-01",
      "authors_text": "Xinrong Chen, Xu Chu, Yingmin Qiu, Hengyuan Zhang, Jing Xiong",
      "is_highlight": false,
      "score": 53.0,
      "summary": "Residual Decoding (ResDec) mitigates hallucinations in Large Vision-Language Models by leveraging historical information to enhance decoding and improve visual grounding.",
      "teaser_image": "https://arxiv.org/html/2602.01047/x1.png",
      "debug_abstract": "Large Vision-Language Models (LVLMs) can reason effectively from image-text inputs and perform well in various multimodal tasks. Despite this success, they are affected by language priors and often produce hallucinations. Hallucinations denote generated content that is grammatically and syntactically coherent, yet bears no match or direct relevance to actual visual input. To address this problem, we propose Residual Decoding (ResDec). It is a novel training-free method that uses historical information to aid decoding. The method relies on the internal implicit reasoning mechanism and token logits evolution mechanism of LVLMs to correct biases. Extensive experiments demonstrate that ResDec effectively suppresses hallucinations induced by language priors, significantly improves visual grounding, and reduces object hallucinations. In addition to mitigating hallucinations, ResDec also performs exceptionally well on comprehensive LVLM benchmarks, highlighting its broad applicability."
    },
    {
      "title": "Resource-Efficient Reinforcement for Reasoning Large Language Models via Dynamic One-Shot Policy Refinement",
      "url": "https://arxiv.org/abs/2602.00815",
      "date": "2026-01-31",
      "authors_text": "Yunjian Zhang, Sudong Wang, Yang Li, Peiran Xu, Conghao Zhou",
      "is_highlight": false,
      "score": 57.0,
      "summary": "The paper introduces Dynamic One-Shot Policy Refinement (DoPR), a resource-efficient reinforcement learning strategy that enhances reasoning in large language models while significantly reducing training costs.",
      "teaser_image": "https://arxiv.org/html/2602.00815/dopr.png",
      "debug_abstract": "Large language models (LLMs) have exhibited remarkable performance on complex reasoning tasks, with reinforcement learning under verifiable rewards (RLVR) emerging as a principled framework for aligning model behavior with reasoning chains. Despite its promise, RLVR remains prohibitively resource-intensive, requiring extensive reward signals and incurring substantial rollout costs during training. In this work, we revisit the fundamental question of data and compute efficiency in RLVR. We first establish a theoretical lower bound on the sample complexity required to unlock reasoning capabilities, and empirically validate that strong performance can be achieved with a surprisingly small number of training instances. To tackle the computational burden, we propose Dynamic One-Shot Policy Refinement (DoPR), an uncertainty-aware RL strategy that dynamically selects a single informative training sample per batch for policy updates, guided by reward volatility and exploration-driven acquisition. DoPR reduces rollout overhead by nearly an order of magnitude while preserving competitive reasoning accuracy, offering a scalable and resource-efficient solution for LLM post-training. This approach offers a practical path toward more efficient and accessible RL-based training for reasoning-intensive LLM applications."
    },
    {
      "title": "Learning Physics-Grounded 4D Dynamics with Neural Gaussian Force Fields",
      "url": "https://arxiv.org/abs/2602.00148",
      "date": "2026-01-29",
      "authors_text": "Shiqian Li, Ruihong Shen, Junfeng Ni, Chang Pan, Chi Zhang",
      "is_highlight": false,
      "score": 82.0,
      "summary": "The Neural Gaussian Force Field (NGFF) framework enables rapid generation of interactive, physically realistic 4D videos from multi-view RGB inputs, enhancing robustness and generalization in physical dynamics.",
      "teaser_image": "https://arxiv.org/html/2602.00148/x1.png",
      "debug_abstract": "Predicting physical dynamics from raw visual data remains a major challenge in AI. While recent video generation models have achieved impressive visual quality, they still cannot consistently generate physically plausible videos due to a lack of modeling of physical laws. Recent approaches combining 3D Gaussian splatting and physics engines can produce physically plausible videos, but are hindered by high computational costs in both reconstruction and simulation, and often lack robustness in complex real-world scenarios. To address these issues, we introduce Neural Gaussian Force Field (NGFF), an end-to-end neural framework that integrates 3D Gaussian perception with physics-based dynamic modeling to generate interactive, physically realistic 4D videos from multi-view RGB inputs, achieving two orders of magnitude faster than prior Gaussian simulators. To support training, we also present GSCollision, a 4D Gaussian dataset featuring diverse materials, multi-object interactions, and complex scenes, totaling over 640k rendered physical videos (~4 TB). Evaluations on synthetic and real 3D scenarios show NGFF&#39;s strong generalization and robustness in physical reasoning, advancing video prediction towards physics-grounded world models."
    }
  ],
  "情感与认知智能机器人实验室 (ACIR)": [
    {
      "title": "Superman: Unifying Skeleton and Vision for Human Motion Perception and Generation",
      "url": "https://arxiv.org/abs/2602.02401",
      "date": "2026-02-02",
      "authors_text": "Xinshun Wang, Peiming Li, Ziyi Wang, Zhongbin Fang, Zhichao Deng",
      "is_highlight": false,
      "score": 15.0,
      "summary": "Superman unifies visual perception and skeleton-based motion generation through a Vision-Guided Motion Tokenizer, enabling robust cross-modal learning and superior performance in human motion tasks.",
      "teaser_image": "https://arxiv.org/html/2602.02401/x2.png",
      "debug_abstract": "Human motion analysis tasks, such as temporal 3D pose estimation, motion prediction, and motion in-betweening, play an essential role in computer vision. However, current paradigms suffer from severe fragmentation. First, the field is split between ``perception&#39;&#39; models that understand motion from video but only output text, and ``generation&#39;&#39; models that cannot perceive from raw visual input. Second, generative MLLMs are often limited to single-frame, static poses using dense, parametric SMPL models, failing to handle temporal motion. Third, existing motion vocabularies are built from skeleton data alone, severing the link to the visual domain. To address these challenges, we introduce Superman, a unified framework that bridges visual perception with temporal, skeleton-based motion generation. Our solution is twofold. First, to overcome the modality disconnect, we propose a Vision-Guided Motion Tokenizer. Leveraging the natural geometric alignment between 3D skeletons and visual data, this module pioneers robust joint learning from both modalities, creating a unified, cross-modal motion vocabulary. Second, grounded in this motion language, a single, unified MLLM architecture is trained to handle all tasks. This module flexibly processes diverse, temporal inputs, unifying 3D skeleton pose estimation from video (perception) with skeleton-based motion prediction and in-betweening (generation). Extensive experiments on standard benchmarks, including Human3.6M, demonstrate that our unified method achieves state-of-the-art or competitive performance across all motion tasks. This showcases a more efficient and scalable path for generative motion analysis using skeletons."
    },
    {
      "title": "Not All Preferences Are Created Equal: Stability-Aware and Gradient-Efficient Alignment for Reasoning Models",
      "url": "https://arxiv.org/abs/2602.01207",
      "date": "2026-02-01",
      "authors_text": "Hui Wu, Hengyi Cai, Jinman Zhao, Xinran Chen, Ziheng Li",
      "is_highlight": false,
      "score": 48.0,
      "summary": "The paper introduces SAGE, a dynamic framework for preference-based alignment in reasoning models that enhances optimization stability and efficiency by prioritizing informative training instances.",
      "teaser_image": "https://arxiv.org/html/2602.01207/x2.png",
      "debug_abstract": "Preference-based alignment is pivotal for training large reasoning models; however, standard methods like Direct Preference Optimization (DPO) typically treat all preference pairs uniformly, overlooking the evolving utility of training instances. This static approach often leads to inefficient or unstable optimization, as it wastes computation on trivial pairs with negligible gradients and suffers from noise induced by samples near uncertain decision boundaries. Facing these challenges, we propose SAGE (Stability-Aware Gradient Efficiency), a dynamic framework designed to enhance alignment reliability by maximizing the Signal-to-Noise Ratio of policy updates. Concretely, SAGE integrates a coarse-grained curriculum mechanism that refreshes candidate pools based on model competence with a fine-grained, stability-aware scoring function that prioritizes informative, confident errors while filtering out unstable samples. Experiments on multiple mathematical reasoning benchmarks demonstrate that SAGE significantly accelerates convergence and outperforms static baselines, highlighting the critical role of policy-aware, stability-conscious data selection in reasoning alignment."
    },
    {
      "title": "Residual Decoding: Mitigating Hallucinations in Large Vision-Language Models via History-Aware Residual Guidance",
      "url": "https://arxiv.org/abs/2602.01047",
      "date": "2026-02-01",
      "authors_text": "Xinrong Chen, Xu Chu, Yingmin Qiu, Hengyuan Zhang, Jing Xiong",
      "is_highlight": false,
      "score": 53.0,
      "summary": "Residual Decoding (ResDec) mitigates hallucinations in Large Vision-Language Models by leveraging historical information to enhance decoding and improve visual grounding.",
      "teaser_image": "https://arxiv.org/html/2602.01047/x1.png",
      "debug_abstract": "Large Vision-Language Models (LVLMs) can reason effectively from image-text inputs and perform well in various multimodal tasks. Despite this success, they are affected by language priors and often produce hallucinations. Hallucinations denote generated content that is grammatically and syntactically coherent, yet bears no match or direct relevance to actual visual input. To address this problem, we propose Residual Decoding (ResDec). It is a novel training-free method that uses historical information to aid decoding. The method relies on the internal implicit reasoning mechanism and token logits evolution mechanism of LVLMs to correct biases. Extensive experiments demonstrate that ResDec effectively suppresses hallucinations induced by language priors, significantly improves visual grounding, and reduces object hallucinations. In addition to mitigating hallucinations, ResDec also performs exceptionally well on comprehensive LVLM benchmarks, highlighting its broad applicability."
    },
    {
      "title": "Resource-Efficient Reinforcement for Reasoning Large Language Models via Dynamic One-Shot Policy Refinement",
      "url": "https://arxiv.org/abs/2602.00815",
      "date": "2026-01-31",
      "authors_text": "Yunjian Zhang, Sudong Wang, Yang Li, Peiran Xu, Conghao Zhou",
      "is_highlight": false,
      "score": 57.0,
      "summary": "The paper introduces Dynamic One-Shot Policy Refinement (DoPR), a resource-efficient reinforcement learning strategy that enhances reasoning in large language models while significantly reducing training costs.",
      "teaser_image": "https://arxiv.org/html/2602.00815/dopr.png",
      "debug_abstract": "Large language models (LLMs) have exhibited remarkable performance on complex reasoning tasks, with reinforcement learning under verifiable rewards (RLVR) emerging as a principled framework for aligning model behavior with reasoning chains. Despite its promise, RLVR remains prohibitively resource-intensive, requiring extensive reward signals and incurring substantial rollout costs during training. In this work, we revisit the fundamental question of data and compute efficiency in RLVR. We first establish a theoretical lower bound on the sample complexity required to unlock reasoning capabilities, and empirically validate that strong performance can be achieved with a surprisingly small number of training instances. To tackle the computational burden, we propose Dynamic One-Shot Policy Refinement (DoPR), an uncertainty-aware RL strategy that dynamically selects a single informative training sample per batch for policy updates, guided by reward volatility and exploration-driven acquisition. DoPR reduces rollout overhead by nearly an order of magnitude while preserving competitive reasoning accuracy, offering a scalable and resource-efficient solution for LLM post-training. This approach offers a practical path toward more efficient and accessible RL-based training for reasoning-intensive LLM applications."
    },
    {
      "title": "Learning Physics-Grounded 4D Dynamics with Neural Gaussian Force Fields",
      "url": "https://arxiv.org/abs/2602.00148",
      "date": "2026-01-29",
      "authors_text": "Shiqian Li, Ruihong Shen, Junfeng Ni, Chang Pan, Chi Zhang",
      "is_highlight": false,
      "score": 82.0,
      "summary": "The Neural Gaussian Force Field (NGFF) framework enables rapid generation of interactive, physically realistic 4D videos from multi-view RGB inputs, enhancing robustness and generalization in physical dynamics.",
      "teaser_image": "https://arxiv.org/html/2602.00148/x1.png",
      "debug_abstract": "Predicting physical dynamics from raw visual data remains a major challenge in AI. While recent video generation models have achieved impressive visual quality, they still cannot consistently generate physically plausible videos due to a lack of modeling of physical laws. Recent approaches combining 3D Gaussian splatting and physics engines can produce physically plausible videos, but are hindered by high computational costs in both reconstruction and simulation, and often lack robustness in complex real-world scenarios. To address these issues, we introduce Neural Gaussian Force Field (NGFF), an end-to-end neural framework that integrates 3D Gaussian perception with physics-based dynamic modeling to generate interactive, physically realistic 4D videos from multi-view RGB inputs, achieving two orders of magnitude faster than prior Gaussian simulators. To support training, we also present GSCollision, a 4D Gaussian dataset featuring diverse materials, multi-object interactions, and complex scenes, totaling over 640k rendered physical videos (~4 TB). Evaluations on synthetic and real 3D scenarios show NGFF&#39;s strong generalization and robustness in physical reasoning, advancing video prediction towards physics-grounded world models."
    }
  ],
  "中国人民大学": [
    {
      "title": "CloDS: Visual-Only Unsupervised Cloth Dynamics Learning in Unknown Conditions",
      "url": "https://arxiv.org/abs/2602.01844",
      "date": "2026-02-02",
      "authors_text": "Yuliang Zhan, Jian Li, Wenbing Huang, Wenbing Huang, Yang Liu",
      "is_highlight": false,
      "score": 32.0,
      "summary": "CloDS introduces an unsupervised framework for learning cloth dynamics from multi-view visual data, overcoming challenges of non-linear deformations and occlusions without requiring known physical properties.",
      "teaser_image": "https://arxiv.org/html/2602.01844/x2.png",
      "debug_abstract": "Deep learning has demonstrated remarkable capabilities in simulating complex dynamic systems. However, existing methods require known physical properties as supervision or inputs, limiting their applicability under unknown conditions. To explore this challenge, we introduce Cloth Dynamics Grounding (CDG), a novel scenario for unsupervised learning of cloth dynamics from multi-view visual observations. We further propose Cloth Dynamics Splatting (CloDS), an unsupervised dynamic learning framework designed for CDG. CloDS adopts a three-stage pipeline that first performs video-to-geometry grounding and then trains a dynamics model on the grounded meshes. To cope with large non-linear deformations and severe self-occlusions during grounding, we introduce a dual-position opacity modulation that supports bidirectional mapping between 2D observations and 3D geometry via mesh-based Gaussian splatting in video-to-geometry grounding stage. It jointly considers the absolute and relative position of Gaussian components. Comprehensive experimental evaluations demonstrate that CloDS effectively learns cloth dynamics from visual data while maintaining strong generalization capabilities for unseen configurations. Our code is available at this https URL. Visualization results are available at this https URL}.%\\footnote{As in this example."
    },
    {
      "title": "Adaptive Ability Decomposing for Unlocking Large Reasoning Model Effective Reinforcement Learning",
      "url": "https://arxiv.org/abs/2602.00759",
      "date": "2026-01-31",
      "authors_text": "Zhipeng Chen, Xiaobo Qin, Wayne Xin Zhao, Youbin Wu, Ji-Rong Wen",
      "is_highlight": false,
      "score": 58.0,
      "summary": "The paper introduces A$^2$D, an Adaptive Ability Decomposing method that enhances reinforcement learning with verifiable rewards by breaking complex questions into simpler sub-questions for improved reasoning in large language models.",
      "teaser_image": "https://arxiv.org/html/2602.00759/x1.png",
      "debug_abstract": "Reinforcement learning with verifiable rewards (RLVR) has shown great potential to enhance the reasoning ability of large language models (LLMs). However, due to the limited amount of information provided during the RLVR process, the model can only engage in largely blind exploration, which often results in failure on challenging problems. To provide additional information for the RLVR process without relying on a teacher model, we propose A$^2$D, an Adaptive Ability Decomposing method for enhancing the effectiveness of RLVR. Specifically, we first train a decomposer via RLVR without distillation, enabling it to decompose complex questions into a set of simpler sub-questions. Next, we use this decomposer to annotate sub-questions for each question in the training dataset, and then train the reasoner under RLVR with sub-question guidance. To better understand A$^2$D, we first compare its performance with competitive baselines, showing its effectiveness. Next, we observe that our method functions as a plug-and-play module that can be applied to different RLVR algorithms. Furthermore, we conduct an analysis of the decomposer, revealing how the RLVR process affects its performance and behavior, and which type of guidance is better suited for enhancing the reasoner&#39;s exploration and exploitation abilities."
    },
    {
      "title": "MapDream: Task-Driven Map Learning for Vision-Language Navigation",
      "url": "https://arxiv.org/abs/2602.00222",
      "date": "2026-01-30",
      "authors_text": "Guoxin Lian, Shuo Wang, Yucheng Wang, Yongcai Wang, Maiyue Chen",
      "is_highlight": false,
      "score": 90.0,
      "summary": "MapDream introduces a task-driven framework for Vision-Language Navigation that learns compact, navigation-focused maps through autoregressive synthesis, achieving state-of-the-art performance in relevant benchmarks.",
      "teaser_image": "https://arxiv.org/html/2602.00222/x1.png",
      "debug_abstract": "Vision-Language Navigation (VLN) requires agents to follow natural language instructions in partially observed 3D environments, motivating map representations that aggregate spatial context beyond local perception. However, most existing approaches rely on hand-crafted maps constructed independently of the navigation policy. We argue that maps should instead be learned representations shaped directly by navigation objectives rather than exhaustive reconstructions. Based on this insight, we propose MapDream, a map-in-the-loop framework that formulates map construction as autoregressive bird&#39;s-eye-view (BEV) image synthesis. The framework jointly learns map generation and action prediction, distilling environmental context into a compact three-channel BEV map that preserves only navigation-critical affordances. Supervised pre-training bootstraps a reliable mapping-to-control interface, while the autoregressive design enables end-to-end joint optimization through reinforcement fine-tuning. Experiments on R2R-CE and RxR-CE achieve state-of-the-art monocular performance, validating task-driven generative map learning."
    }
  ],
  "机器人与人工智能实验室": [
    {
      "title": "FlowSteer: Interactive Agentic Workflow Orchestration via End-to-End Reinforcement Learning",
      "url": "https://arxiv.org/abs/2602.01664",
      "date": "2026-02-02",
      "authors_text": "Mingda Zhang, Haoran Luo, Tiesunlong Shen, Qika Lin, Xiaoying Tang",
      "is_highlight": false,
      "score": 7.0,
      "summary": "FlowSteer is an end-to-end reinforcement learning framework that automates workflow orchestration through interactive learning, addressing manual costs and improving performance across diverse tasks.",
      "teaser_image": "https://arxiv.org/html/2602.01664/x1.png",
      "debug_abstract": "In recent years, a variety of powerful agentic workflows have been applied to solve a wide range of human problems. However, existing workflow orchestration still faces key challenges, including high manual cost, reliance on specific operators/large language models (LLMs), and sparse reward signals. To address these challenges, we propose FlowSteer, an end-to-end reinforcement learning framework that takes a lightweight policy model as the agent and an executable canvas environment, automating workflow orchestration through multi-turn interaction. In this process, the policy model analyzes execution states and selects editing actions, while the canvas executes operators and returns feedback for iterative refinement. Moreover, FlowSteer provides a plug-and-play framework that supports diverse operator libraries and interchangeable LLM backends. To effectively train this interaction paradigm, we propose Canvas Workflow Relative Policy Optimization (CWRPO), which introduces diversity-constrained rewards with conditional release to stabilize learning and suppress shortcut behaviors. Experimental results on twelve datasets show that FlowSteer significantly outperforms baselines across various tasks."
    },
    {
      "title": "Towards Agentic Intelligence for Materials Science",
      "url": "https://arxiv.org/abs/2602.00169",
      "date": "2026-01-29",
      "authors_text": "Huan Zhang, Yizhan Li, Wenhao Huang, Ziyu Hou, Yu Song",
      "is_highlight": false,
      "score": 12.0,
      "summary": "This paper advocates for developing agentic AI systems in materials science that autonomously plan, act, and learn to enhance discovery processes beyond traditional models.",
      "teaser_image": "https://arxiv.org/html/2602.00169/x1.png",
      "debug_abstract": "The convergence of artificial intelligence and materials science presents a transformative opportunity, but achieving true acceleration in discovery requires moving beyond task-isolated, fine-tuned models toward agentic systems that plan, act, and learn across the full discovery loop. This survey advances a unique pipeline-centric view that spans from corpus curation and pretraining, through domain adaptation and instruction tuning, to goal-conditioned agents interfacing with simulation and experimental platforms. Unlike prior reviews, we treat the entire process as an end-to-end system to be optimized for tangible discovery outcomes rather than proxy benchmarks. This perspective allows us to trace how upstream design choices-such as data curation and training objectives-can be aligned with downstream experimental success through effective credit assignment. To bridge communities and establish a shared frame of reference, we first present an integrated lens that aligns terminology, evaluation, and workflow stages across AI and materials science. We then analyze the field through two focused lenses: From the AI perspective, the survey details LLM strengths in pattern recognition, predictive analytics, and natural language processing for literature mining, materials characterization, and property prediction; from the materials science perspective, it highlights applications in materials design, process optimization, and the acceleration of computational workflows via integration with external tools (e.g., DFT, robotic labs). Finally, we contrast passive, reactive approaches with agentic design, cataloging current contributions while motivating systems that pursue long-horizon goals with autonomy, memory, and tool use. This survey charts a practical roadmap towards autonomous, safety-aware LLM agents aimed at discovering novel and useful materials."
    },
    {
      "title": "Breaking the Static Graph: Context-Aware Traversal for Robust Retrieval-Augmented Generation",
      "url": "https://arxiv.org/abs/2602.01965",
      "date": "2026-02-02",
      "authors_text": "Kwun Hang Lau, Fangyuan Zhang, Boyu Ruan, Yingli Zhou, Qintian Guo",
      "is_highlight": false,
      "score": 27.0,
      "summary": "CatRAG enhances Retrieval-Augmented Generation by transforming static Knowledge Graphs into query-adaptive structures, improving multi-hop evidence retrieval and reasoning completeness.",
      "teaser_image": "https://arxiv.org/html/2602.01965/image/HubBiasShift.png",
      "debug_abstract": "Recent advances in Retrieval-Augmented Generation (RAG) have shifted from simple vector similarity to structure-aware approaches like HippoRAG, which leverage Knowledge Graphs (KGs) and Personalized PageRank (PPR) to capture multi-hop dependencies. However, these methods suffer from a &#34;Static Graph Fallacy&#34;: they rely on fixed transition probabilities determined during indexing. This rigidity ignores the query-dependent nature of edge relevance, causing semantic drift where random walks are diverted into high-degree &#34;hub&#34; nodes before reaching critical downstream evidence. Consequently, models often achieve high partial recall but fail to retrieve the complete evidence chain required for multi-hop queries. To address this, we propose CatRAG, Context-Aware Traversal for robust RAG, a framework that builds on the HippoRAG 2 architecture and transforms the static KG into a query-adaptive navigation structure. We introduce a multi-faceted framework to steer the random walk: (1) Symbolic Anchoring, which injects weak entity constraints to regularize the random walk; (2) Query-Aware Dynamic Edge Weighting, which dynamically modulates graph structure, to prune irrelevant paths while amplifying those aligned with the query&#39;s intent; and (3) Key-Fact Passage Weight Enhancement, a cost-efficient bias that structurally anchors the random walk to likely evidence. Experiments across four multi-hop benchmarks demonstrate that CatRAG consistently outperforms state of the art baselines. Our analysis reveals that while standard Recall metrics show modest gains, CatRAG achieves substantial improvements in reasoning completeness, the capacity to recover the entire evidence path without gaps. These results reveal that our approach effectively bridges the gap between retrieving partial context and enabling fully grounded reasoning. Resources are available at this https URL."
    },
    {
      "title": "Tilt-Ropter: A Novel Hybrid Aerial and Terrestrial Vehicle with Tilt Rotors and Passive Wheels",
      "url": "https://arxiv.org/abs/2602.01700",
      "date": "2026-02-02",
      "authors_text": "Ruoyu Wang, Xuchen Liu, Zongzhou Wu, Zixuan Guo, Wendi Ding",
      "is_highlight": false,
      "score": 38.0,
      "summary": "The Tilt-Ropter is a hybrid aerial-terrestrial vehicle that utilizes tilt rotors and passive wheels for efficient locomotion, featuring advanced control systems for enhanced mobility and energy savings.",
      "teaser_image": "https://arxiv.org/html/2602.01700/x2.png",
      "debug_abstract": "In this work, we present Tilt-Ropter, a novel hybrid aerial-terrestrial vehicle (HATV) that combines tilt rotors with passive wheels to achieve energy-efficient multi-mode locomotion. Unlike existing under-actuated HATVs, the fully actuated design of Tilt-Ropter enables decoupled force and torque control, greatly enhancing its mobility and environmental adaptability. A nonlinear model predictive controller (NMPC) is developed to track reference trajectories and handle contact constraints across locomotion modes, while a dedicated control allocation module exploits actuation redundancy to achieve energy-efficient control of actuators. Additionally, to enhance robustness during ground contact, we introduce an external wrench estimation algorithm that estimates environmental interaction forces and torques in real time. The system is validated through both simulation and real-world experiments, including seamless air-ground transitions and trajectory tracking. Results show low tracking errors in both modes and highlight a 92.8% reduction in power consumption during ground locomotion, demonstrating the system&#39;s potential for long-duration missions across large-scale and energy-constrained environments."
    },
    {
      "title": "Reasoning with Autoregressive-Diffusion Collaborative Thoughts",
      "url": "https://arxiv.org/abs/2602.01608",
      "date": "2026-02-02",
      "authors_text": "Mu Yuan, Liekang Zeng, Guoliang Xing, Lan Zhang, Yunhao Liu",
      "is_highlight": false,
      "score": 41.0,
      "summary": "The paper presents Collaborative Thoughts, a framework integrating autoregressive and diffusion models for enhanced spatial reasoning and generation through iterative feedback and joint reasoning.",
      "teaser_image": "https://arxiv.org/html/2602.01608/x2.png",
      "debug_abstract": "Autoregressive and diffusion models represent two complementary generative paradigms. Autoregressive models excel at sequential planning and constraint composition, yet struggle with tasks that require explicit spatial or physical grounding. Diffusion models, in contrast, capture rich spatial structure through high-dimensional generation, but lack the stepwise logical control needed to satisfy complex, multi-stage constraints or to reliably identify and correct errors. We introduce Collaborative Thoughts, a unified collaborative framework that enables autoregressive and diffusion models to reason and generate jointly through a closed-loop interaction. In Collaborative Thoughts, autoregressive models perform structured planning and constraint management, diffusion models instantiate these constraints as intermediate visual thoughts, and a vision-based critic module evaluates whether the visual thoughts satisfy the intended structural and physical requirements. This feedback is then used to iteratively refine subsequent planning and generation steps, mitigating error propagation across modalities. Importantly, Collaborative Thoughts uses the same collaborative loop regardless of whether the task is autoregressive question answering or diffusion-based visual generation. Through representative examples, we illustrate how Collaborative Thoughts can improve the reliability of spatial reasoning and the controllability of generation."
    },
    {
      "title": "Adaptive Rollout Allocation for Online Reinforcement Learning with Verifiable Rewards",
      "url": "https://arxiv.org/abs/2602.01601",
      "date": "2026-02-02",
      "authors_text": "Hieu Trung Nguyen, Bao Nguyen, Wenao Ma, Yuzhi Zhao, Ruifeng She",
      "is_highlight": false,
      "score": 40.0,
      "summary": "The paper presents \\Ours, a variance-informed strategy for adaptive rollout allocation in online reinforcement learning, enhancing sampling efficiency and performance over traditional methods.",
      "teaser_image": "https://arxiv.org/html/2602.01601/x2.png",
      "debug_abstract": "Sampling efficiency is a key bottleneck in reinforcement learning with verifiable rewards. Existing group-based policy optimization methods, such as GRPO, allocate a fixed number of rollouts for all training prompts. This uniform allocation implicitly treats all prompts as equally informative, and could lead to inefficient computational budget usage and impede training progress. We introduce \\Ours, a Variance-Informed Predictive allocation strategy that allocates a given rollout budget to the prompts in the incumbent batch to minimize the expected gradient variance of the policy update. At each iteration, \\Ours~uses a lightweight Gaussian process model to predict per-prompt success probabilities based on recent rollouts. These probability predictions are translated into variance estimates, which are then fed into a convex optimization problem to determine the optimal rollout allocations under a hard compute budget constraint. Empirical results show that \\Ours~consistently improves sampling efficiency and achieves higher performance than uniform or heuristic allocation strategies in multiple benchmarks. Our code will be available at this https URL."
    },
    {
      "title": "MedAD-R1: Eliciting Consistent Reasoning in Interpretible Medical Anomaly Detection via Consistency-Reinforced Policy Optimization",
      "url": "https://arxiv.org/abs/2602.01081",
      "date": "2026-02-01",
      "authors_text": "Haitao Zhang, Yingying Wang, Jiaxiang Wang, Haote Xu, Hongyang Zhang",
      "is_highlight": false,
      "score": 64.0,
      "summary": "MedAD-R1 enhances medical anomaly detection by utilizing a two-stage training framework with consistency-reinforced policy optimization, achieving state-of-the-art performance on the MedAD-38K benchmark.",
      "teaser_image": "https://arxiv.org/html/2602.01081/x3.png",
      "debug_abstract": "Medical Anomaly Detection (MedAD) presents a significant opportunity to enhance diagnostic accuracy using Large Multimodal Models (LMMs) to interpret and answer questions based on medical images. However, the reliance on Supervised Fine-Tuning (SFT) on simplistic and fragmented datasets has hindered the development of models capable of plausible reasoning and robust multimodal generalization. To overcome this, we introduce MedAD-38K, the first large-scale, multi-modal, and multi-center benchmark for MedAD featuring diagnostic Chain-of-Thought (CoT) annotations alongside structured Visual Question-Answering (VQA) pairs. On this foundation, we propose a two-stage training framework. The first stage, Cognitive Injection, uses SFT to instill foundational medical knowledge and align the model with a structured think-then-answer paradigm. Given that standard policy optimization can produce reasoning that is disconnected from the final answer, the second stage incorporates Consistency Group Relative Policy Optimization (Con-GRPO). This novel algorithm incorporates a crucial consistency reward to ensure the generated reasoning process is relevant and logically coherent with the final diagnosis. Our proposed model, MedAD-R1, achieves state-of-the-art (SOTA) performance on the MedAD-38K benchmark, outperforming strong baselines by more than 10\\%. This superior performance stems from its ability to generate transparent and logically consistent reasoning pathways, offering a promising approach to enhancing the trustworthiness and interpretability of AI for clinical decision support."
    },
    {
      "title": "Dual Quaternion SE(3) Synchronization with Recovery Guarantees",
      "url": "https://arxiv.org/abs/2602.00324",
      "date": "2026-01-30",
      "authors_text": "Jianing Zhao, Linglingzhi Zhu, Anthony Man-Cho So",
      "is_highlight": false,
      "score": 75.0,
      "summary": "This paper presents a dual quaternion-based SE(3) synchronization method with a two-stage algorithm that guarantees recovery of absolute poses from noisy transformations, outperforming traditional matrix methods.",
      "teaser_image": "https://arxiv.org/html/2602.00324/x2.png",
      "debug_abstract": "Synchronization over the special Euclidean group SE(3) aims to recover absolute poses from noisy pairwise relative transformations and is a core primitive in robotics and 3D vision. Standard approaches often require multi-step heuristic procedures to recover valid poses, which are difficult to analyze and typically lack theoretical guarantees. This paper adopts a dual quaternion representation and formulates SE(3) synchronization directly over the unit dual quaternion. A two-stage algorithm is developed: A spectral initializer computed via the power method on a Hermitian dual quaternion measurement matrix, followed by a dual quaternion generalized power method (DQGPM) that enforces feasibility through per-iteration projection. The estimation error bounds are established for spectral estimators, and DQGPM is shown to admit a finite-iteration error bound and achieves linear error contraction up to an explicit noise-dependent threshold. Experiments on synthetic benchmarks and real-world multi-scan point-set registration demonstrate that the proposed pipeline improves both accuracy and efficiency over representative matrix-based methods."
    }
  ],
  "Multimedia Lab (MMLab)": [
    {
      "title": "FlowSteer: Interactive Agentic Workflow Orchestration via End-to-End Reinforcement Learning",
      "url": "https://arxiv.org/abs/2602.01664",
      "date": "2026-02-02",
      "authors_text": "Mingda Zhang, Haoran Luo, Tiesunlong Shen, Qika Lin, Xiaoying Tang",
      "is_highlight": false,
      "score": 7.0,
      "summary": "FlowSteer is an end-to-end reinforcement learning framework that automates workflow orchestration through interactive learning, addressing manual costs and improving performance across diverse tasks.",
      "teaser_image": "https://arxiv.org/html/2602.01664/x1.png",
      "debug_abstract": "In recent years, a variety of powerful agentic workflows have been applied to solve a wide range of human problems. However, existing workflow orchestration still faces key challenges, including high manual cost, reliance on specific operators/large language models (LLMs), and sparse reward signals. To address these challenges, we propose FlowSteer, an end-to-end reinforcement learning framework that takes a lightweight policy model as the agent and an executable canvas environment, automating workflow orchestration through multi-turn interaction. In this process, the policy model analyzes execution states and selects editing actions, while the canvas executes operators and returns feedback for iterative refinement. Moreover, FlowSteer provides a plug-and-play framework that supports diverse operator libraries and interchangeable LLM backends. To effectively train this interaction paradigm, we propose Canvas Workflow Relative Policy Optimization (CWRPO), which introduces diversity-constrained rewards with conditional release to stabilize learning and suppress shortcut behaviors. Experimental results on twelve datasets show that FlowSteer significantly outperforms baselines across various tasks."
    },
    {
      "title": "Towards Agentic Intelligence for Materials Science",
      "url": "https://arxiv.org/abs/2602.00169",
      "date": "2026-01-29",
      "authors_text": "Huan Zhang, Yizhan Li, Wenhao Huang, Ziyu Hou, Yu Song",
      "is_highlight": false,
      "score": 12.0,
      "summary": "This paper advocates for developing agentic AI systems in materials science that autonomously plan, act, and learn to enhance discovery processes beyond traditional models.",
      "teaser_image": "https://arxiv.org/html/2602.00169/x1.png",
      "debug_abstract": "The convergence of artificial intelligence and materials science presents a transformative opportunity, but achieving true acceleration in discovery requires moving beyond task-isolated, fine-tuned models toward agentic systems that plan, act, and learn across the full discovery loop. This survey advances a unique pipeline-centric view that spans from corpus curation and pretraining, through domain adaptation and instruction tuning, to goal-conditioned agents interfacing with simulation and experimental platforms. Unlike prior reviews, we treat the entire process as an end-to-end system to be optimized for tangible discovery outcomes rather than proxy benchmarks. This perspective allows us to trace how upstream design choices-such as data curation and training objectives-can be aligned with downstream experimental success through effective credit assignment. To bridge communities and establish a shared frame of reference, we first present an integrated lens that aligns terminology, evaluation, and workflow stages across AI and materials science. We then analyze the field through two focused lenses: From the AI perspective, the survey details LLM strengths in pattern recognition, predictive analytics, and natural language processing for literature mining, materials characterization, and property prediction; from the materials science perspective, it highlights applications in materials design, process optimization, and the acceleration of computational workflows via integration with external tools (e.g., DFT, robotic labs). Finally, we contrast passive, reactive approaches with agentic design, cataloging current contributions while motivating systems that pursue long-horizon goals with autonomy, memory, and tool use. This survey charts a practical roadmap towards autonomous, safety-aware LLM agents aimed at discovering novel and useful materials."
    },
    {
      "title": "Breaking the Static Graph: Context-Aware Traversal for Robust Retrieval-Augmented Generation",
      "url": "https://arxiv.org/abs/2602.01965",
      "date": "2026-02-02",
      "authors_text": "Kwun Hang Lau, Fangyuan Zhang, Boyu Ruan, Yingli Zhou, Qintian Guo",
      "is_highlight": false,
      "score": 27.0,
      "summary": "CatRAG enhances Retrieval-Augmented Generation by transforming static Knowledge Graphs into query-adaptive structures, improving multi-hop evidence retrieval and reasoning completeness.",
      "teaser_image": "https://arxiv.org/html/2602.01965/image/HubBiasShift.png",
      "debug_abstract": "Recent advances in Retrieval-Augmented Generation (RAG) have shifted from simple vector similarity to structure-aware approaches like HippoRAG, which leverage Knowledge Graphs (KGs) and Personalized PageRank (PPR) to capture multi-hop dependencies. However, these methods suffer from a &#34;Static Graph Fallacy&#34;: they rely on fixed transition probabilities determined during indexing. This rigidity ignores the query-dependent nature of edge relevance, causing semantic drift where random walks are diverted into high-degree &#34;hub&#34; nodes before reaching critical downstream evidence. Consequently, models often achieve high partial recall but fail to retrieve the complete evidence chain required for multi-hop queries. To address this, we propose CatRAG, Context-Aware Traversal for robust RAG, a framework that builds on the HippoRAG 2 architecture and transforms the static KG into a query-adaptive navigation structure. We introduce a multi-faceted framework to steer the random walk: (1) Symbolic Anchoring, which injects weak entity constraints to regularize the random walk; (2) Query-Aware Dynamic Edge Weighting, which dynamically modulates graph structure, to prune irrelevant paths while amplifying those aligned with the query&#39;s intent; and (3) Key-Fact Passage Weight Enhancement, a cost-efficient bias that structurally anchors the random walk to likely evidence. Experiments across four multi-hop benchmarks demonstrate that CatRAG consistently outperforms state of the art baselines. Our analysis reveals that while standard Recall metrics show modest gains, CatRAG achieves substantial improvements in reasoning completeness, the capacity to recover the entire evidence path without gaps. These results reveal that our approach effectively bridges the gap between retrieving partial context and enabling fully grounded reasoning. Resources are available at this https URL."
    },
    {
      "title": "Tilt-Ropter: A Novel Hybrid Aerial and Terrestrial Vehicle with Tilt Rotors and Passive Wheels",
      "url": "https://arxiv.org/abs/2602.01700",
      "date": "2026-02-02",
      "authors_text": "Ruoyu Wang, Xuchen Liu, Zongzhou Wu, Zixuan Guo, Wendi Ding",
      "is_highlight": false,
      "score": 38.0,
      "summary": "The Tilt-Ropter is a hybrid aerial-terrestrial vehicle that utilizes tilt rotors and passive wheels for efficient locomotion, featuring advanced control systems for enhanced mobility and energy savings.",
      "teaser_image": "https://arxiv.org/html/2602.01700/x2.png",
      "debug_abstract": "In this work, we present Tilt-Ropter, a novel hybrid aerial-terrestrial vehicle (HATV) that combines tilt rotors with passive wheels to achieve energy-efficient multi-mode locomotion. Unlike existing under-actuated HATVs, the fully actuated design of Tilt-Ropter enables decoupled force and torque control, greatly enhancing its mobility and environmental adaptability. A nonlinear model predictive controller (NMPC) is developed to track reference trajectories and handle contact constraints across locomotion modes, while a dedicated control allocation module exploits actuation redundancy to achieve energy-efficient control of actuators. Additionally, to enhance robustness during ground contact, we introduce an external wrench estimation algorithm that estimates environmental interaction forces and torques in real time. The system is validated through both simulation and real-world experiments, including seamless air-ground transitions and trajectory tracking. Results show low tracking errors in both modes and highlight a 92.8% reduction in power consumption during ground locomotion, demonstrating the system&#39;s potential for long-duration missions across large-scale and energy-constrained environments."
    },
    {
      "title": "Reasoning with Autoregressive-Diffusion Collaborative Thoughts",
      "url": "https://arxiv.org/abs/2602.01608",
      "date": "2026-02-02",
      "authors_text": "Mu Yuan, Liekang Zeng, Guoliang Xing, Lan Zhang, Yunhao Liu",
      "is_highlight": false,
      "score": 41.0,
      "summary": "The paper presents Collaborative Thoughts, a framework integrating autoregressive and diffusion models for enhanced spatial reasoning and generation through iterative feedback and joint reasoning.",
      "teaser_image": "https://arxiv.org/html/2602.01608/x2.png",
      "debug_abstract": "Autoregressive and diffusion models represent two complementary generative paradigms. Autoregressive models excel at sequential planning and constraint composition, yet struggle with tasks that require explicit spatial or physical grounding. Diffusion models, in contrast, capture rich spatial structure through high-dimensional generation, but lack the stepwise logical control needed to satisfy complex, multi-stage constraints or to reliably identify and correct errors. We introduce Collaborative Thoughts, a unified collaborative framework that enables autoregressive and diffusion models to reason and generate jointly through a closed-loop interaction. In Collaborative Thoughts, autoregressive models perform structured planning and constraint management, diffusion models instantiate these constraints as intermediate visual thoughts, and a vision-based critic module evaluates whether the visual thoughts satisfy the intended structural and physical requirements. This feedback is then used to iteratively refine subsequent planning and generation steps, mitigating error propagation across modalities. Importantly, Collaborative Thoughts uses the same collaborative loop regardless of whether the task is autoregressive question answering or diffusion-based visual generation. Through representative examples, we illustrate how Collaborative Thoughts can improve the reliability of spatial reasoning and the controllability of generation."
    },
    {
      "title": "Adaptive Rollout Allocation for Online Reinforcement Learning with Verifiable Rewards",
      "url": "https://arxiv.org/abs/2602.01601",
      "date": "2026-02-02",
      "authors_text": "Hieu Trung Nguyen, Bao Nguyen, Wenao Ma, Yuzhi Zhao, Ruifeng She",
      "is_highlight": false,
      "score": 40.0,
      "summary": "The paper presents \\Ours, a variance-informed strategy for adaptive rollout allocation in online reinforcement learning, enhancing sampling efficiency and performance over traditional methods.",
      "teaser_image": "https://arxiv.org/html/2602.01601/x2.png",
      "debug_abstract": "Sampling efficiency is a key bottleneck in reinforcement learning with verifiable rewards. Existing group-based policy optimization methods, such as GRPO, allocate a fixed number of rollouts for all training prompts. This uniform allocation implicitly treats all prompts as equally informative, and could lead to inefficient computational budget usage and impede training progress. We introduce \\Ours, a Variance-Informed Predictive allocation strategy that allocates a given rollout budget to the prompts in the incumbent batch to minimize the expected gradient variance of the policy update. At each iteration, \\Ours~uses a lightweight Gaussian process model to predict per-prompt success probabilities based on recent rollouts. These probability predictions are translated into variance estimates, which are then fed into a convex optimization problem to determine the optimal rollout allocations under a hard compute budget constraint. Empirical results show that \\Ours~consistently improves sampling efficiency and achieves higher performance than uniform or heuristic allocation strategies in multiple benchmarks. Our code will be available at this https URL."
    },
    {
      "title": "MedAD-R1: Eliciting Consistent Reasoning in Interpretible Medical Anomaly Detection via Consistency-Reinforced Policy Optimization",
      "url": "https://arxiv.org/abs/2602.01081",
      "date": "2026-02-01",
      "authors_text": "Haitao Zhang, Yingying Wang, Jiaxiang Wang, Haote Xu, Hongyang Zhang",
      "is_highlight": false,
      "score": 64.0,
      "summary": "MedAD-R1 enhances medical anomaly detection by utilizing a two-stage training framework with consistency-reinforced policy optimization, achieving state-of-the-art performance on the MedAD-38K benchmark.",
      "teaser_image": "https://arxiv.org/html/2602.01081/x3.png",
      "debug_abstract": "Medical Anomaly Detection (MedAD) presents a significant opportunity to enhance diagnostic accuracy using Large Multimodal Models (LMMs) to interpret and answer questions based on medical images. However, the reliance on Supervised Fine-Tuning (SFT) on simplistic and fragmented datasets has hindered the development of models capable of plausible reasoning and robust multimodal generalization. To overcome this, we introduce MedAD-38K, the first large-scale, multi-modal, and multi-center benchmark for MedAD featuring diagnostic Chain-of-Thought (CoT) annotations alongside structured Visual Question-Answering (VQA) pairs. On this foundation, we propose a two-stage training framework. The first stage, Cognitive Injection, uses SFT to instill foundational medical knowledge and align the model with a structured think-then-answer paradigm. Given that standard policy optimization can produce reasoning that is disconnected from the final answer, the second stage incorporates Consistency Group Relative Policy Optimization (Con-GRPO). This novel algorithm incorporates a crucial consistency reward to ensure the generated reasoning process is relevant and logically coherent with the final diagnosis. Our proposed model, MedAD-R1, achieves state-of-the-art (SOTA) performance on the MedAD-38K benchmark, outperforming strong baselines by more than 10\\%. This superior performance stems from its ability to generate transparent and logically consistent reasoning pathways, offering a promising approach to enhancing the trustworthiness and interpretability of AI for clinical decision support."
    },
    {
      "title": "Dual Quaternion SE(3) Synchronization with Recovery Guarantees",
      "url": "https://arxiv.org/abs/2602.00324",
      "date": "2026-01-30",
      "authors_text": "Jianing Zhao, Linglingzhi Zhu, Anthony Man-Cho So",
      "is_highlight": false,
      "score": 75.0,
      "summary": "This paper presents a dual quaternion-based SE(3) synchronization method with a two-stage algorithm that guarantees recovery of absolute poses from noisy transformations, outperforming traditional matrix methods.",
      "teaser_image": "https://arxiv.org/html/2602.00324/x2.png",
      "debug_abstract": "Synchronization over the special Euclidean group SE(3) aims to recover absolute poses from noisy pairwise relative transformations and is a core primitive in robotics and 3D vision. Standard approaches often require multi-step heuristic procedures to recover valid poses, which are difficult to analyze and typically lack theoretical guarantees. This paper adopts a dual quaternion representation and formulates SE(3) synchronization directly over the unit dual quaternion. A two-stage algorithm is developed: A spectral initializer computed via the power method on a Hermitian dual quaternion measurement matrix, followed by a dual quaternion generalized power method (DQGPM) that enforces feasibility through per-iteration projection. The estimation error bounds are established for spectral estimators, and DQGPM is shown to admit a finite-iteration error bound and achieves linear error contraction up to an explicit noise-dependent threshold. Experiments on synthetic benchmarks and real-world multi-scan point-set registration demonstrate that the proposed pipeline improves both accuracy and efficiency over representative matrix-based methods."
    }
  ],
  "机器人与自动化研究中心": [
    {
      "title": "FlowSteer: Interactive Agentic Workflow Orchestration via End-to-End Reinforcement Learning",
      "url": "https://arxiv.org/abs/2602.01664",
      "date": "2026-02-02",
      "authors_text": "Mingda Zhang, Haoran Luo, Tiesunlong Shen, Qika Lin, Xiaoying Tang",
      "is_highlight": false,
      "score": 7.0,
      "summary": "FlowSteer is an end-to-end reinforcement learning framework that automates workflow orchestration through interactive learning, addressing manual costs and improving performance across diverse tasks.",
      "teaser_image": "https://arxiv.org/html/2602.01664/x1.png",
      "debug_abstract": "In recent years, a variety of powerful agentic workflows have been applied to solve a wide range of human problems. However, existing workflow orchestration still faces key challenges, including high manual cost, reliance on specific operators/large language models (LLMs), and sparse reward signals. To address these challenges, we propose FlowSteer, an end-to-end reinforcement learning framework that takes a lightweight policy model as the agent and an executable canvas environment, automating workflow orchestration through multi-turn interaction. In this process, the policy model analyzes execution states and selects editing actions, while the canvas executes operators and returns feedback for iterative refinement. Moreover, FlowSteer provides a plug-and-play framework that supports diverse operator libraries and interchangeable LLM backends. To effectively train this interaction paradigm, we propose Canvas Workflow Relative Policy Optimization (CWRPO), which introduces diversity-constrained rewards with conditional release to stabilize learning and suppress shortcut behaviors. Experimental results on twelve datasets show that FlowSteer significantly outperforms baselines across various tasks."
    },
    {
      "title": "Towards Agentic Intelligence for Materials Science",
      "url": "https://arxiv.org/abs/2602.00169",
      "date": "2026-01-29",
      "authors_text": "Huan Zhang, Yizhan Li, Wenhao Huang, Ziyu Hou, Yu Song",
      "is_highlight": false,
      "score": 12.0,
      "summary": "This paper advocates for developing agentic AI systems in materials science that autonomously plan, act, and learn to enhance discovery processes beyond traditional models.",
      "teaser_image": "https://arxiv.org/html/2602.00169/x1.png",
      "debug_abstract": "The convergence of artificial intelligence and materials science presents a transformative opportunity, but achieving true acceleration in discovery requires moving beyond task-isolated, fine-tuned models toward agentic systems that plan, act, and learn across the full discovery loop. This survey advances a unique pipeline-centric view that spans from corpus curation and pretraining, through domain adaptation and instruction tuning, to goal-conditioned agents interfacing with simulation and experimental platforms. Unlike prior reviews, we treat the entire process as an end-to-end system to be optimized for tangible discovery outcomes rather than proxy benchmarks. This perspective allows us to trace how upstream design choices-such as data curation and training objectives-can be aligned with downstream experimental success through effective credit assignment. To bridge communities and establish a shared frame of reference, we first present an integrated lens that aligns terminology, evaluation, and workflow stages across AI and materials science. We then analyze the field through two focused lenses: From the AI perspective, the survey details LLM strengths in pattern recognition, predictive analytics, and natural language processing for literature mining, materials characterization, and property prediction; from the materials science perspective, it highlights applications in materials design, process optimization, and the acceleration of computational workflows via integration with external tools (e.g., DFT, robotic labs). Finally, we contrast passive, reactive approaches with agentic design, cataloging current contributions while motivating systems that pursue long-horizon goals with autonomy, memory, and tool use. This survey charts a practical roadmap towards autonomous, safety-aware LLM agents aimed at discovering novel and useful materials."
    },
    {
      "title": "Breaking the Static Graph: Context-Aware Traversal for Robust Retrieval-Augmented Generation",
      "url": "https://arxiv.org/abs/2602.01965",
      "date": "2026-02-02",
      "authors_text": "Kwun Hang Lau, Fangyuan Zhang, Boyu Ruan, Yingli Zhou, Qintian Guo",
      "is_highlight": false,
      "score": 27.0,
      "summary": "CatRAG enhances Retrieval-Augmented Generation by transforming static Knowledge Graphs into query-adaptive structures, improving multi-hop evidence retrieval and reasoning completeness.",
      "teaser_image": "https://arxiv.org/html/2602.01965/image/HubBiasShift.png",
      "debug_abstract": "Recent advances in Retrieval-Augmented Generation (RAG) have shifted from simple vector similarity to structure-aware approaches like HippoRAG, which leverage Knowledge Graphs (KGs) and Personalized PageRank (PPR) to capture multi-hop dependencies. However, these methods suffer from a &#34;Static Graph Fallacy&#34;: they rely on fixed transition probabilities determined during indexing. This rigidity ignores the query-dependent nature of edge relevance, causing semantic drift where random walks are diverted into high-degree &#34;hub&#34; nodes before reaching critical downstream evidence. Consequently, models often achieve high partial recall but fail to retrieve the complete evidence chain required for multi-hop queries. To address this, we propose CatRAG, Context-Aware Traversal for robust RAG, a framework that builds on the HippoRAG 2 architecture and transforms the static KG into a query-adaptive navigation structure. We introduce a multi-faceted framework to steer the random walk: (1) Symbolic Anchoring, which injects weak entity constraints to regularize the random walk; (2) Query-Aware Dynamic Edge Weighting, which dynamically modulates graph structure, to prune irrelevant paths while amplifying those aligned with the query&#39;s intent; and (3) Key-Fact Passage Weight Enhancement, a cost-efficient bias that structurally anchors the random walk to likely evidence. Experiments across four multi-hop benchmarks demonstrate that CatRAG consistently outperforms state of the art baselines. Our analysis reveals that while standard Recall metrics show modest gains, CatRAG achieves substantial improvements in reasoning completeness, the capacity to recover the entire evidence path without gaps. These results reveal that our approach effectively bridges the gap between retrieving partial context and enabling fully grounded reasoning. Resources are available at this https URL."
    },
    {
      "title": "Tilt-Ropter: A Novel Hybrid Aerial and Terrestrial Vehicle with Tilt Rotors and Passive Wheels",
      "url": "https://arxiv.org/abs/2602.01700",
      "date": "2026-02-02",
      "authors_text": "Ruoyu Wang, Xuchen Liu, Zongzhou Wu, Zixuan Guo, Wendi Ding",
      "is_highlight": false,
      "score": 38.0,
      "summary": "The Tilt-Ropter is a hybrid aerial-terrestrial vehicle that utilizes tilt rotors and passive wheels for efficient locomotion, featuring advanced control systems for enhanced mobility and energy savings.",
      "teaser_image": "https://arxiv.org/html/2602.01700/x2.png",
      "debug_abstract": "In this work, we present Tilt-Ropter, a novel hybrid aerial-terrestrial vehicle (HATV) that combines tilt rotors with passive wheels to achieve energy-efficient multi-mode locomotion. Unlike existing under-actuated HATVs, the fully actuated design of Tilt-Ropter enables decoupled force and torque control, greatly enhancing its mobility and environmental adaptability. A nonlinear model predictive controller (NMPC) is developed to track reference trajectories and handle contact constraints across locomotion modes, while a dedicated control allocation module exploits actuation redundancy to achieve energy-efficient control of actuators. Additionally, to enhance robustness during ground contact, we introduce an external wrench estimation algorithm that estimates environmental interaction forces and torques in real time. The system is validated through both simulation and real-world experiments, including seamless air-ground transitions and trajectory tracking. Results show low tracking errors in both modes and highlight a 92.8% reduction in power consumption during ground locomotion, demonstrating the system&#39;s potential for long-duration missions across large-scale and energy-constrained environments."
    },
    {
      "title": "Reasoning with Autoregressive-Diffusion Collaborative Thoughts",
      "url": "https://arxiv.org/abs/2602.01608",
      "date": "2026-02-02",
      "authors_text": "Mu Yuan, Liekang Zeng, Guoliang Xing, Lan Zhang, Yunhao Liu",
      "is_highlight": false,
      "score": 41.0,
      "summary": "The paper presents Collaborative Thoughts, a framework integrating autoregressive and diffusion models for enhanced spatial reasoning and generation through iterative feedback and joint reasoning.",
      "teaser_image": "https://arxiv.org/html/2602.01608/x2.png",
      "debug_abstract": "Autoregressive and diffusion models represent two complementary generative paradigms. Autoregressive models excel at sequential planning and constraint composition, yet struggle with tasks that require explicit spatial or physical grounding. Diffusion models, in contrast, capture rich spatial structure through high-dimensional generation, but lack the stepwise logical control needed to satisfy complex, multi-stage constraints or to reliably identify and correct errors. We introduce Collaborative Thoughts, a unified collaborative framework that enables autoregressive and diffusion models to reason and generate jointly through a closed-loop interaction. In Collaborative Thoughts, autoregressive models perform structured planning and constraint management, diffusion models instantiate these constraints as intermediate visual thoughts, and a vision-based critic module evaluates whether the visual thoughts satisfy the intended structural and physical requirements. This feedback is then used to iteratively refine subsequent planning and generation steps, mitigating error propagation across modalities. Importantly, Collaborative Thoughts uses the same collaborative loop regardless of whether the task is autoregressive question answering or diffusion-based visual generation. Through representative examples, we illustrate how Collaborative Thoughts can improve the reliability of spatial reasoning and the controllability of generation."
    },
    {
      "title": "Adaptive Rollout Allocation for Online Reinforcement Learning with Verifiable Rewards",
      "url": "https://arxiv.org/abs/2602.01601",
      "date": "2026-02-02",
      "authors_text": "Hieu Trung Nguyen, Bao Nguyen, Wenao Ma, Yuzhi Zhao, Ruifeng She",
      "is_highlight": false,
      "score": 40.0,
      "summary": "The paper presents \\Ours, a variance-informed strategy for adaptive rollout allocation in online reinforcement learning, enhancing sampling efficiency and performance over traditional methods.",
      "teaser_image": "https://arxiv.org/html/2602.01601/x2.png",
      "debug_abstract": "Sampling efficiency is a key bottleneck in reinforcement learning with verifiable rewards. Existing group-based policy optimization methods, such as GRPO, allocate a fixed number of rollouts for all training prompts. This uniform allocation implicitly treats all prompts as equally informative, and could lead to inefficient computational budget usage and impede training progress. We introduce \\Ours, a Variance-Informed Predictive allocation strategy that allocates a given rollout budget to the prompts in the incumbent batch to minimize the expected gradient variance of the policy update. At each iteration, \\Ours~uses a lightweight Gaussian process model to predict per-prompt success probabilities based on recent rollouts. These probability predictions are translated into variance estimates, which are then fed into a convex optimization problem to determine the optimal rollout allocations under a hard compute budget constraint. Empirical results show that \\Ours~consistently improves sampling efficiency and achieves higher performance than uniform or heuristic allocation strategies in multiple benchmarks. Our code will be available at this https URL."
    },
    {
      "title": "MedAD-R1: Eliciting Consistent Reasoning in Interpretible Medical Anomaly Detection via Consistency-Reinforced Policy Optimization",
      "url": "https://arxiv.org/abs/2602.01081",
      "date": "2026-02-01",
      "authors_text": "Haitao Zhang, Yingying Wang, Jiaxiang Wang, Haote Xu, Hongyang Zhang",
      "is_highlight": false,
      "score": 64.0,
      "summary": "MedAD-R1 enhances medical anomaly detection by utilizing a two-stage training framework with consistency-reinforced policy optimization, achieving state-of-the-art performance on the MedAD-38K benchmark.",
      "teaser_image": "https://arxiv.org/html/2602.01081/x3.png",
      "debug_abstract": "Medical Anomaly Detection (MedAD) presents a significant opportunity to enhance diagnostic accuracy using Large Multimodal Models (LMMs) to interpret and answer questions based on medical images. However, the reliance on Supervised Fine-Tuning (SFT) on simplistic and fragmented datasets has hindered the development of models capable of plausible reasoning and robust multimodal generalization. To overcome this, we introduce MedAD-38K, the first large-scale, multi-modal, and multi-center benchmark for MedAD featuring diagnostic Chain-of-Thought (CoT) annotations alongside structured Visual Question-Answering (VQA) pairs. On this foundation, we propose a two-stage training framework. The first stage, Cognitive Injection, uses SFT to instill foundational medical knowledge and align the model with a structured think-then-answer paradigm. Given that standard policy optimization can produce reasoning that is disconnected from the final answer, the second stage incorporates Consistency Group Relative Policy Optimization (Con-GRPO). This novel algorithm incorporates a crucial consistency reward to ensure the generated reasoning process is relevant and logically coherent with the final diagnosis. Our proposed model, MedAD-R1, achieves state-of-the-art (SOTA) performance on the MedAD-38K benchmark, outperforming strong baselines by more than 10\\%. This superior performance stems from its ability to generate transparent and logically consistent reasoning pathways, offering a promising approach to enhancing the trustworthiness and interpretability of AI for clinical decision support."
    },
    {
      "title": "Dual Quaternion SE(3) Synchronization with Recovery Guarantees",
      "url": "https://arxiv.org/abs/2602.00324",
      "date": "2026-01-30",
      "authors_text": "Jianing Zhao, Linglingzhi Zhu, Anthony Man-Cho So",
      "is_highlight": false,
      "score": 75.0,
      "summary": "This paper presents a dual quaternion-based SE(3) synchronization method with a two-stage algorithm that guarantees recovery of absolute poses from noisy transformations, outperforming traditional matrix methods.",
      "teaser_image": "https://arxiv.org/html/2602.00324/x2.png",
      "debug_abstract": "Synchronization over the special Euclidean group SE(3) aims to recover absolute poses from noisy pairwise relative transformations and is a core primitive in robotics and 3D vision. Standard approaches often require multi-step heuristic procedures to recover valid poses, which are difficult to analyze and typically lack theoretical guarantees. This paper adopts a dual quaternion representation and formulates SE(3) synchronization directly over the unit dual quaternion. A two-stage algorithm is developed: A spectral initializer computed via the power method on a Hermitian dual quaternion measurement matrix, followed by a dual quaternion generalized power method (DQGPM) that enforces feasibility through per-iteration projection. The estimation error bounds are established for spectral estimators, and DQGPM is shown to admit a finite-iteration error bound and achieves linear error contraction up to an explicit noise-dependent threshold. Experiments on synthetic benchmarks and real-world multi-scan point-set registration demonstrate that the proposed pipeline improves both accuracy and efficiency over representative matrix-based methods."
    }
  ],
  "OpenDriveLab": [
    {
      "title": "FlowSteer: Interactive Agentic Workflow Orchestration via End-to-End Reinforcement Learning",
      "url": "https://arxiv.org/abs/2602.01664",
      "date": "2026-02-02",
      "authors_text": "Mingda Zhang, Haoran Luo, Tiesunlong Shen, Qika Lin, Xiaoying Tang",
      "is_highlight": false,
      "score": 7.0,
      "summary": "FlowSteer is an end-to-end reinforcement learning framework that automates workflow orchestration through interactive learning, addressing manual costs and improving performance across diverse tasks.",
      "teaser_image": "https://arxiv.org/html/2602.01664/x1.png",
      "debug_abstract": "In recent years, a variety of powerful agentic workflows have been applied to solve a wide range of human problems. However, existing workflow orchestration still faces key challenges, including high manual cost, reliance on specific operators/large language models (LLMs), and sparse reward signals. To address these challenges, we propose FlowSteer, an end-to-end reinforcement learning framework that takes a lightweight policy model as the agent and an executable canvas environment, automating workflow orchestration through multi-turn interaction. In this process, the policy model analyzes execution states and selects editing actions, while the canvas executes operators and returns feedback for iterative refinement. Moreover, FlowSteer provides a plug-and-play framework that supports diverse operator libraries and interchangeable LLM backends. To effectively train this interaction paradigm, we propose Canvas Workflow Relative Policy Optimization (CWRPO), which introduces diversity-constrained rewards with conditional release to stabilize learning and suppress shortcut behaviors. Experimental results on twelve datasets show that FlowSteer significantly outperforms baselines across various tasks."
    },
    {
      "title": "Failure-Aware Bimanual Teleoperation via Conservative Value Guided Assistance",
      "url": "https://arxiv.org/abs/2602.01092",
      "date": "2026-02-01",
      "authors_text": "Peng Zhou, Zhongxuan Li, Jinsong Wu, Jiaming Qi, Jun Hu",
      "is_highlight": false,
      "score": 9.0,
      "summary": "This paper presents a failure-aware bimanual teleoperation framework that uses conservative value learning to enhance task success and reduce operator workload through compliant haptic assistance.",
      "teaser_image": "https://arxiv.org/html/2602.01092/x1.png",
      "debug_abstract": "Teleoperation of high-precision manipulation is con-strained by tight success tolerances and complex contact dy-namics, which make impending failures difficult for human operators to anticipate under partial observability. This paper proposes a value-guided, failure-aware framework for bimanual teleoperation that provides compliant haptic assistance while pre-serving continuous human authority. The framework is trained entirely from heterogeneous offline teleoperation data containing both successful and failed executions. Task feasibility is mod-eled as a conservative success score learned via Conservative Value Learning, yielding a risk-sensitive estimate that remains reliable under distribution shift. During online operation, the learned success score regulates the level of assistance, while a learned actor provides a corrective motion direction. Both are integrated through a joint-space impedance interface on the master side, yielding continuous guidance that steers the operator away from failure-prone actions without overriding intent. Experimental results on contact-rich manipulation tasks demonstrate improved task success rates and reduced operator workload compared to conventional teleoperation and shared-autonomy baselines, indicating that conservative value learning provides an effective mechanism for embedding failure awareness into bilateral teleoperation. Experimental videos are available at this https URL"
    },
    {
      "title": "Towards Agentic Intelligence for Materials Science",
      "url": "https://arxiv.org/abs/2602.00169",
      "date": "2026-01-29",
      "authors_text": "Huan Zhang, Yizhan Li, Wenhao Huang, Ziyu Hou, Yu Song",
      "is_highlight": false,
      "score": 12.0,
      "summary": "This paper advocates for developing agentic AI systems in materials science that autonomously plan, act, and learn to enhance discovery processes beyond traditional models.",
      "teaser_image": "https://arxiv.org/html/2602.00169/x1.png",
      "debug_abstract": "The convergence of artificial intelligence and materials science presents a transformative opportunity, but achieving true acceleration in discovery requires moving beyond task-isolated, fine-tuned models toward agentic systems that plan, act, and learn across the full discovery loop. This survey advances a unique pipeline-centric view that spans from corpus curation and pretraining, through domain adaptation and instruction tuning, to goal-conditioned agents interfacing with simulation and experimental platforms. Unlike prior reviews, we treat the entire process as an end-to-end system to be optimized for tangible discovery outcomes rather than proxy benchmarks. This perspective allows us to trace how upstream design choices-such as data curation and training objectives-can be aligned with downstream experimental success through effective credit assignment. To bridge communities and establish a shared frame of reference, we first present an integrated lens that aligns terminology, evaluation, and workflow stages across AI and materials science. We then analyze the field through two focused lenses: From the AI perspective, the survey details LLM strengths in pattern recognition, predictive analytics, and natural language processing for literature mining, materials characterization, and property prediction; from the materials science perspective, it highlights applications in materials design, process optimization, and the acceleration of computational workflows via integration with external tools (e.g., DFT, robotic labs). Finally, we contrast passive, reactive approaches with agentic design, cataloging current contributions while motivating systems that pursue long-horizon goals with autonomy, memory, and tool use. This survey charts a practical roadmap towards autonomous, safety-aware LLM agents aimed at discovering novel and useful materials."
    },
    {
      "title": "TIDE: Trajectory-based Diagnostic Evaluation of Test-Time Improvement in LLM Agents",
      "url": "https://arxiv.org/abs/2602.02196",
      "date": "2026-02-02",
      "authors_text": "Hang Yan, Xinyu Che, Fangzhi Xu, Qiushi Sun, Zichen Ding",
      "is_highlight": false,
      "score": 20.0,
      "summary": "The TIDE framework evaluates Test-Time Improvement in LLM agents by analyzing task completion dynamics, recursive behaviors, and memory constraints to enhance performance optimization.",
      "teaser_image": "https://arxiv.org/html/2602.02196/x1.png",
      "debug_abstract": "Recent advances in autonomous LLM agents demonstrate their ability to improve performance through iterative interaction with the environment. We define this paradigm as Test-Time Improvement (TTI). However, the mechanisms under how and why TTI succeed or fail remain poorly understood, and existing evaluation metrics fail to capture their task optimization efficiency, behavior adaptation after erroneous actions, and the specific utility of working memory for task completion. To address these gaps, we propose Test-time Improvement Diagnostic Evaluation (TIDE), an agent-agnostic and environment-agnostic framework that decomposes TTI into three comprehensive and interconnected dimensions. The framework measures (1) the overall temporal dynamics of task completion and (2) identifies whether performance is primarily constrained by recursive looping behaviors or (3) by burdensome accumulated memory. Through extensive experiments across diverse agents and environments, TIDE highlights that improving agent performance requires more than scaling internal reasoning, calling for explicitly optimizing the interaction dynamics between the agent and the environment."
    },
    {
      "title": "Breaking the Static Graph: Context-Aware Traversal for Robust Retrieval-Augmented Generation",
      "url": "https://arxiv.org/abs/2602.01965",
      "date": "2026-02-02",
      "authors_text": "Kwun Hang Lau, Fangyuan Zhang, Boyu Ruan, Yingli Zhou, Qintian Guo",
      "is_highlight": false,
      "score": 27.0,
      "summary": "CatRAG enhances Retrieval-Augmented Generation by transforming static Knowledge Graphs into query-adaptive structures, improving multi-hop evidence retrieval and reasoning completeness.",
      "teaser_image": "https://arxiv.org/html/2602.01965/image/HubBiasShift.png",
      "debug_abstract": "Recent advances in Retrieval-Augmented Generation (RAG) have shifted from simple vector similarity to structure-aware approaches like HippoRAG, which leverage Knowledge Graphs (KGs) and Personalized PageRank (PPR) to capture multi-hop dependencies. However, these methods suffer from a &#34;Static Graph Fallacy&#34;: they rely on fixed transition probabilities determined during indexing. This rigidity ignores the query-dependent nature of edge relevance, causing semantic drift where random walks are diverted into high-degree &#34;hub&#34; nodes before reaching critical downstream evidence. Consequently, models often achieve high partial recall but fail to retrieve the complete evidence chain required for multi-hop queries. To address this, we propose CatRAG, Context-Aware Traversal for robust RAG, a framework that builds on the HippoRAG 2 architecture and transforms the static KG into a query-adaptive navigation structure. We introduce a multi-faceted framework to steer the random walk: (1) Symbolic Anchoring, which injects weak entity constraints to regularize the random walk; (2) Query-Aware Dynamic Edge Weighting, which dynamically modulates graph structure, to prune irrelevant paths while amplifying those aligned with the query&#39;s intent; and (3) Key-Fact Passage Weight Enhancement, a cost-efficient bias that structurally anchors the random walk to likely evidence. Experiments across four multi-hop benchmarks demonstrate that CatRAG consistently outperforms state of the art baselines. Our analysis reveals that while standard Recall metrics show modest gains, CatRAG achieves substantial improvements in reasoning completeness, the capacity to recover the entire evidence path without gaps. These results reveal that our approach effectively bridges the gap between retrieving partial context and enabling fully grounded reasoning. Resources are available at this https URL."
    },
    {
      "title": "Tilt-Ropter: A Novel Hybrid Aerial and Terrestrial Vehicle with Tilt Rotors and Passive Wheels",
      "url": "https://arxiv.org/abs/2602.01700",
      "date": "2026-02-02",
      "authors_text": "Ruoyu Wang, Xuchen Liu, Zongzhou Wu, Zixuan Guo, Wendi Ding",
      "is_highlight": false,
      "score": 38.0,
      "summary": "The Tilt-Ropter is a hybrid aerial-terrestrial vehicle that utilizes tilt rotors and passive wheels for efficient locomotion, featuring advanced control systems for enhanced mobility and energy savings.",
      "teaser_image": "https://arxiv.org/html/2602.01700/x2.png",
      "debug_abstract": "In this work, we present Tilt-Ropter, a novel hybrid aerial-terrestrial vehicle (HATV) that combines tilt rotors with passive wheels to achieve energy-efficient multi-mode locomotion. Unlike existing under-actuated HATVs, the fully actuated design of Tilt-Ropter enables decoupled force and torque control, greatly enhancing its mobility and environmental adaptability. A nonlinear model predictive controller (NMPC) is developed to track reference trajectories and handle contact constraints across locomotion modes, while a dedicated control allocation module exploits actuation redundancy to achieve energy-efficient control of actuators. Additionally, to enhance robustness during ground contact, we introduce an external wrench estimation algorithm that estimates environmental interaction forces and torques in real time. The system is validated through both simulation and real-world experiments, including seamless air-ground transitions and trajectory tracking. Results show low tracking errors in both modes and highlight a 92.8% reduction in power consumption during ground locomotion, demonstrating the system&#39;s potential for long-duration missions across large-scale and energy-constrained environments."
    },
    {
      "title": "Reasoning with Autoregressive-Diffusion Collaborative Thoughts",
      "url": "https://arxiv.org/abs/2602.01608",
      "date": "2026-02-02",
      "authors_text": "Mu Yuan, Liekang Zeng, Guoliang Xing, Lan Zhang, Yunhao Liu",
      "is_highlight": false,
      "score": 41.0,
      "summary": "The paper presents Collaborative Thoughts, a framework integrating autoregressive and diffusion models for enhanced spatial reasoning and generation through iterative feedback and joint reasoning.",
      "teaser_image": "https://arxiv.org/html/2602.01608/x2.png",
      "debug_abstract": "Autoregressive and diffusion models represent two complementary generative paradigms. Autoregressive models excel at sequential planning and constraint composition, yet struggle with tasks that require explicit spatial or physical grounding. Diffusion models, in contrast, capture rich spatial structure through high-dimensional generation, but lack the stepwise logical control needed to satisfy complex, multi-stage constraints or to reliably identify and correct errors. We introduce Collaborative Thoughts, a unified collaborative framework that enables autoregressive and diffusion models to reason and generate jointly through a closed-loop interaction. In Collaborative Thoughts, autoregressive models perform structured planning and constraint management, diffusion models instantiate these constraints as intermediate visual thoughts, and a vision-based critic module evaluates whether the visual thoughts satisfy the intended structural and physical requirements. This feedback is then used to iteratively refine subsequent planning and generation steps, mitigating error propagation across modalities. Importantly, Collaborative Thoughts uses the same collaborative loop regardless of whether the task is autoregressive question answering or diffusion-based visual generation. Through representative examples, we illustrate how Collaborative Thoughts can improve the reliability of spatial reasoning and the controllability of generation."
    },
    {
      "title": "Adaptive Rollout Allocation for Online Reinforcement Learning with Verifiable Rewards",
      "url": "https://arxiv.org/abs/2602.01601",
      "date": "2026-02-02",
      "authors_text": "Hieu Trung Nguyen, Bao Nguyen, Wenao Ma, Yuzhi Zhao, Ruifeng She",
      "is_highlight": false,
      "score": 40.0,
      "summary": "The paper presents \\Ours, a variance-informed strategy for adaptive rollout allocation in online reinforcement learning, enhancing sampling efficiency and performance over traditional methods.",
      "teaser_image": "https://arxiv.org/html/2602.01601/x2.png",
      "debug_abstract": "Sampling efficiency is a key bottleneck in reinforcement learning with verifiable rewards. Existing group-based policy optimization methods, such as GRPO, allocate a fixed number of rollouts for all training prompts. This uniform allocation implicitly treats all prompts as equally informative, and could lead to inefficient computational budget usage and impede training progress. We introduce \\Ours, a Variance-Informed Predictive allocation strategy that allocates a given rollout budget to the prompts in the incumbent batch to minimize the expected gradient variance of the policy update. At each iteration, \\Ours~uses a lightweight Gaussian process model to predict per-prompt success probabilities based on recent rollouts. These probability predictions are translated into variance estimates, which are then fed into a convex optimization problem to determine the optimal rollout allocations under a hard compute budget constraint. Empirical results show that \\Ours~consistently improves sampling efficiency and achieves higher performance than uniform or heuristic allocation strategies in multiple benchmarks. Our code will be available at this https URL."
    },
    {
      "title": "MedAD-R1: Eliciting Consistent Reasoning in Interpretible Medical Anomaly Detection via Consistency-Reinforced Policy Optimization",
      "url": "https://arxiv.org/abs/2602.01081",
      "date": "2026-02-01",
      "authors_text": "Haitao Zhang, Yingying Wang, Jiaxiang Wang, Haote Xu, Hongyang Zhang",
      "is_highlight": false,
      "score": 64.0,
      "summary": "MedAD-R1 enhances medical anomaly detection by utilizing a two-stage training framework with consistency-reinforced policy optimization, achieving state-of-the-art performance on the MedAD-38K benchmark.",
      "teaser_image": "https://arxiv.org/html/2602.01081/x3.png",
      "debug_abstract": "Medical Anomaly Detection (MedAD) presents a significant opportunity to enhance diagnostic accuracy using Large Multimodal Models (LMMs) to interpret and answer questions based on medical images. However, the reliance on Supervised Fine-Tuning (SFT) on simplistic and fragmented datasets has hindered the development of models capable of plausible reasoning and robust multimodal generalization. To overcome this, we introduce MedAD-38K, the first large-scale, multi-modal, and multi-center benchmark for MedAD featuring diagnostic Chain-of-Thought (CoT) annotations alongside structured Visual Question-Answering (VQA) pairs. On this foundation, we propose a two-stage training framework. The first stage, Cognitive Injection, uses SFT to instill foundational medical knowledge and align the model with a structured think-then-answer paradigm. Given that standard policy optimization can produce reasoning that is disconnected from the final answer, the second stage incorporates Consistency Group Relative Policy Optimization (Con-GRPO). This novel algorithm incorporates a crucial consistency reward to ensure the generated reasoning process is relevant and logically coherent with the final diagnosis. Our proposed model, MedAD-R1, achieves state-of-the-art (SOTA) performance on the MedAD-38K benchmark, outperforming strong baselines by more than 10\\%. This superior performance stems from its ability to generate transparent and logically consistent reasoning pathways, offering a promising approach to enhancing the trustworthiness and interpretability of AI for clinical decision support."
    },
    {
      "title": "Residual Decoding: Mitigating Hallucinations in Large Vision-Language Models via History-Aware Residual Guidance",
      "url": "https://arxiv.org/abs/2602.01047",
      "date": "2026-02-01",
      "authors_text": "Xinrong Chen, Xu Chu, Yingmin Qiu, Hengyuan Zhang, Jing Xiong",
      "is_highlight": false,
      "score": 53.0,
      "summary": "Residual Decoding (ResDec) mitigates hallucinations in Large Vision-Language Models by leveraging historical information to enhance decoding and improve visual grounding.",
      "teaser_image": "https://arxiv.org/html/2602.01047/x1.png",
      "debug_abstract": "Large Vision-Language Models (LVLMs) can reason effectively from image-text inputs and perform well in various multimodal tasks. Despite this success, they are affected by language priors and often produce hallucinations. Hallucinations denote generated content that is grammatically and syntactically coherent, yet bears no match or direct relevance to actual visual input. To address this problem, we propose Residual Decoding (ResDec). It is a novel training-free method that uses historical information to aid decoding. The method relies on the internal implicit reasoning mechanism and token logits evolution mechanism of LVLMs to correct biases. Extensive experiments demonstrate that ResDec effectively suppresses hallucinations induced by language priors, significantly improves visual grounding, and reduces object hallucinations. In addition to mitigating hallucinations, ResDec also performs exceptionally well on comprehensive LVLM benchmarks, highlighting its broad applicability."
    },
    {
      "title": "Offline Discovery of Interpretable Skills from Multi-Task Trajectories",
      "url": "https://arxiv.org/abs/2602.01018",
      "date": "2026-02-01",
      "authors_text": "Chongyu Zhu, Mithun Vanniasinghe, Jiayu Chen, Chi-Guhn Lee",
      "is_highlight": false,
      "score": 77.0,
      "summary": "LOKI is a three-stage framework for offline skill discovery and hierarchical imitation learning, effectively extracting interpretable skills from multi-task trajectories without explicit rewards.",
      "teaser_image": "https://arxiv.org/html/2602.01018/fig/two_stage_skill_learning.png",
      "debug_abstract": "Hierarchical Imitation Learning is a powerful paradigm for acquiring complex robot behaviors from demonstrations. A central challenge, however, lies in discovering reusable skills from long-horizon, multi-task offline data, especially when the data lacks explicit rewards or subtask annotations. In this work, we introduce LOKI, a three-stage end-to-end learning framework designed for offline skill discovery and hierarchical imitation. The framework commences with a two-stage, weakly supervised skill discovery process: Stage one performs coarse, task-aware macro-segmentation by employing an alignment-enforced Vector Quantized VAE guided by weak task labels. Stage two then refines these segments at a micro-level using a self-supervised sequential model, followed by an iterative clustering process to consolidate skill boundaries. The third stage then leverages these precise boundaries to construct a hierarchical policy within an option-based framework-complete with a learned termination condition beta for explicit skill switching. LOKI achieves high success rates on the challenging D4RL Kitchen benchmark and outperforms standard HIL baselines. Furthermore, we demonstrate that the discovered skills are semantically meaningful, aligning with human intuition, and exhibit compositionality by successfully sequencing them to solve a novel, unseen task."
    },
    {
      "title": "Dual Quaternion SE(3) Synchronization with Recovery Guarantees",
      "url": "https://arxiv.org/abs/2602.00324",
      "date": "2026-01-30",
      "authors_text": "Jianing Zhao, Linglingzhi Zhu, Anthony Man-Cho So",
      "is_highlight": false,
      "score": 75.0,
      "summary": "This paper presents a dual quaternion-based SE(3) synchronization method with a two-stage algorithm that guarantees recovery of absolute poses from noisy transformations, outperforming traditional matrix methods.",
      "teaser_image": "https://arxiv.org/html/2602.00324/x2.png",
      "debug_abstract": "Synchronization over the special Euclidean group SE(3) aims to recover absolute poses from noisy pairwise relative transformations and is a core primitive in robotics and 3D vision. Standard approaches often require multi-step heuristic procedures to recover valid poses, which are difficult to analyze and typically lack theoretical guarantees. This paper adopts a dual quaternion representation and formulates SE(3) synchronization directly over the unit dual quaternion. A two-stage algorithm is developed: A spectral initializer computed via the power method on a Hermitian dual quaternion measurement matrix, followed by a dual quaternion generalized power method (DQGPM) that enforces feasibility through per-iteration projection. The estimation error bounds are established for spectral estimators, and DQGPM is shown to admit a finite-iteration error bound and achieves linear error contraction up to an explicit noise-dependent threshold. Experiments on synthetic benchmarks and real-world multi-scan point-set registration demonstrate that the proposed pipeline improves both accuracy and efficiency over representative matrix-based methods."
    }
  ],
  "香港大学机械工程系机器人实验室": [
    {
      "title": "FlowSteer: Interactive Agentic Workflow Orchestration via End-to-End Reinforcement Learning",
      "url": "https://arxiv.org/abs/2602.01664",
      "date": "2026-02-02",
      "authors_text": "Mingda Zhang, Haoran Luo, Tiesunlong Shen, Qika Lin, Xiaoying Tang",
      "is_highlight": false,
      "score": 7.0,
      "summary": "FlowSteer is an end-to-end reinforcement learning framework that automates workflow orchestration through interactive learning, addressing manual costs and improving performance across diverse tasks.",
      "teaser_image": "https://arxiv.org/html/2602.01664/x1.png",
      "debug_abstract": "In recent years, a variety of powerful agentic workflows have been applied to solve a wide range of human problems. However, existing workflow orchestration still faces key challenges, including high manual cost, reliance on specific operators/large language models (LLMs), and sparse reward signals. To address these challenges, we propose FlowSteer, an end-to-end reinforcement learning framework that takes a lightweight policy model as the agent and an executable canvas environment, automating workflow orchestration through multi-turn interaction. In this process, the policy model analyzes execution states and selects editing actions, while the canvas executes operators and returns feedback for iterative refinement. Moreover, FlowSteer provides a plug-and-play framework that supports diverse operator libraries and interchangeable LLM backends. To effectively train this interaction paradigm, we propose Canvas Workflow Relative Policy Optimization (CWRPO), which introduces diversity-constrained rewards with conditional release to stabilize learning and suppress shortcut behaviors. Experimental results on twelve datasets show that FlowSteer significantly outperforms baselines across various tasks."
    },
    {
      "title": "Failure-Aware Bimanual Teleoperation via Conservative Value Guided Assistance",
      "url": "https://arxiv.org/abs/2602.01092",
      "date": "2026-02-01",
      "authors_text": "Peng Zhou, Zhongxuan Li, Jinsong Wu, Jiaming Qi, Jun Hu",
      "is_highlight": false,
      "score": 9.0,
      "summary": "This paper presents a failure-aware bimanual teleoperation framework that uses conservative value learning to enhance task success and reduce operator workload through compliant haptic assistance.",
      "teaser_image": "https://arxiv.org/html/2602.01092/x1.png",
      "debug_abstract": "Teleoperation of high-precision manipulation is con-strained by tight success tolerances and complex contact dy-namics, which make impending failures difficult for human operators to anticipate under partial observability. This paper proposes a value-guided, failure-aware framework for bimanual teleoperation that provides compliant haptic assistance while pre-serving continuous human authority. The framework is trained entirely from heterogeneous offline teleoperation data containing both successful and failed executions. Task feasibility is mod-eled as a conservative success score learned via Conservative Value Learning, yielding a risk-sensitive estimate that remains reliable under distribution shift. During online operation, the learned success score regulates the level of assistance, while a learned actor provides a corrective motion direction. Both are integrated through a joint-space impedance interface on the master side, yielding continuous guidance that steers the operator away from failure-prone actions without overriding intent. Experimental results on contact-rich manipulation tasks demonstrate improved task success rates and reduced operator workload compared to conventional teleoperation and shared-autonomy baselines, indicating that conservative value learning provides an effective mechanism for embedding failure awareness into bilateral teleoperation. Experimental videos are available at this https URL"
    },
    {
      "title": "Towards Agentic Intelligence for Materials Science",
      "url": "https://arxiv.org/abs/2602.00169",
      "date": "2026-01-29",
      "authors_text": "Huan Zhang, Yizhan Li, Wenhao Huang, Ziyu Hou, Yu Song",
      "is_highlight": false,
      "score": 12.0,
      "summary": "This paper advocates for developing agentic AI systems in materials science that autonomously plan, act, and learn to enhance discovery processes beyond traditional models.",
      "teaser_image": "https://arxiv.org/html/2602.00169/x1.png",
      "debug_abstract": "The convergence of artificial intelligence and materials science presents a transformative opportunity, but achieving true acceleration in discovery requires moving beyond task-isolated, fine-tuned models toward agentic systems that plan, act, and learn across the full discovery loop. This survey advances a unique pipeline-centric view that spans from corpus curation and pretraining, through domain adaptation and instruction tuning, to goal-conditioned agents interfacing with simulation and experimental platforms. Unlike prior reviews, we treat the entire process as an end-to-end system to be optimized for tangible discovery outcomes rather than proxy benchmarks. This perspective allows us to trace how upstream design choices-such as data curation and training objectives-can be aligned with downstream experimental success through effective credit assignment. To bridge communities and establish a shared frame of reference, we first present an integrated lens that aligns terminology, evaluation, and workflow stages across AI and materials science. We then analyze the field through two focused lenses: From the AI perspective, the survey details LLM strengths in pattern recognition, predictive analytics, and natural language processing for literature mining, materials characterization, and property prediction; from the materials science perspective, it highlights applications in materials design, process optimization, and the acceleration of computational workflows via integration with external tools (e.g., DFT, robotic labs). Finally, we contrast passive, reactive approaches with agentic design, cataloging current contributions while motivating systems that pursue long-horizon goals with autonomy, memory, and tool use. This survey charts a practical roadmap towards autonomous, safety-aware LLM agents aimed at discovering novel and useful materials."
    },
    {
      "title": "TIDE: Trajectory-based Diagnostic Evaluation of Test-Time Improvement in LLM Agents",
      "url": "https://arxiv.org/abs/2602.02196",
      "date": "2026-02-02",
      "authors_text": "Hang Yan, Xinyu Che, Fangzhi Xu, Qiushi Sun, Zichen Ding",
      "is_highlight": false,
      "score": 20.0,
      "summary": "The TIDE framework evaluates Test-Time Improvement in LLM agents by analyzing task completion dynamics, recursive behaviors, and memory constraints to enhance performance optimization.",
      "teaser_image": "https://arxiv.org/html/2602.02196/x1.png",
      "debug_abstract": "Recent advances in autonomous LLM agents demonstrate their ability to improve performance through iterative interaction with the environment. We define this paradigm as Test-Time Improvement (TTI). However, the mechanisms under how and why TTI succeed or fail remain poorly understood, and existing evaluation metrics fail to capture their task optimization efficiency, behavior adaptation after erroneous actions, and the specific utility of working memory for task completion. To address these gaps, we propose Test-time Improvement Diagnostic Evaluation (TIDE), an agent-agnostic and environment-agnostic framework that decomposes TTI into three comprehensive and interconnected dimensions. The framework measures (1) the overall temporal dynamics of task completion and (2) identifies whether performance is primarily constrained by recursive looping behaviors or (3) by burdensome accumulated memory. Through extensive experiments across diverse agents and environments, TIDE highlights that improving agent performance requires more than scaling internal reasoning, calling for explicitly optimizing the interaction dynamics between the agent and the environment."
    },
    {
      "title": "Breaking the Static Graph: Context-Aware Traversal for Robust Retrieval-Augmented Generation",
      "url": "https://arxiv.org/abs/2602.01965",
      "date": "2026-02-02",
      "authors_text": "Kwun Hang Lau, Fangyuan Zhang, Boyu Ruan, Yingli Zhou, Qintian Guo",
      "is_highlight": false,
      "score": 27.0,
      "summary": "CatRAG enhances Retrieval-Augmented Generation by transforming static Knowledge Graphs into query-adaptive structures, improving multi-hop evidence retrieval and reasoning completeness.",
      "teaser_image": "https://arxiv.org/html/2602.01965/image/HubBiasShift.png",
      "debug_abstract": "Recent advances in Retrieval-Augmented Generation (RAG) have shifted from simple vector similarity to structure-aware approaches like HippoRAG, which leverage Knowledge Graphs (KGs) and Personalized PageRank (PPR) to capture multi-hop dependencies. However, these methods suffer from a &#34;Static Graph Fallacy&#34;: they rely on fixed transition probabilities determined during indexing. This rigidity ignores the query-dependent nature of edge relevance, causing semantic drift where random walks are diverted into high-degree &#34;hub&#34; nodes before reaching critical downstream evidence. Consequently, models often achieve high partial recall but fail to retrieve the complete evidence chain required for multi-hop queries. To address this, we propose CatRAG, Context-Aware Traversal for robust RAG, a framework that builds on the HippoRAG 2 architecture and transforms the static KG into a query-adaptive navigation structure. We introduce a multi-faceted framework to steer the random walk: (1) Symbolic Anchoring, which injects weak entity constraints to regularize the random walk; (2) Query-Aware Dynamic Edge Weighting, which dynamically modulates graph structure, to prune irrelevant paths while amplifying those aligned with the query&#39;s intent; and (3) Key-Fact Passage Weight Enhancement, a cost-efficient bias that structurally anchors the random walk to likely evidence. Experiments across four multi-hop benchmarks demonstrate that CatRAG consistently outperforms state of the art baselines. Our analysis reveals that while standard Recall metrics show modest gains, CatRAG achieves substantial improvements in reasoning completeness, the capacity to recover the entire evidence path without gaps. These results reveal that our approach effectively bridges the gap between retrieving partial context and enabling fully grounded reasoning. Resources are available at this https URL."
    },
    {
      "title": "Tilt-Ropter: A Novel Hybrid Aerial and Terrestrial Vehicle with Tilt Rotors and Passive Wheels",
      "url": "https://arxiv.org/abs/2602.01700",
      "date": "2026-02-02",
      "authors_text": "Ruoyu Wang, Xuchen Liu, Zongzhou Wu, Zixuan Guo, Wendi Ding",
      "is_highlight": false,
      "score": 38.0,
      "summary": "The Tilt-Ropter is a hybrid aerial-terrestrial vehicle that utilizes tilt rotors and passive wheels for efficient locomotion, featuring advanced control systems for enhanced mobility and energy savings.",
      "teaser_image": "https://arxiv.org/html/2602.01700/x2.png",
      "debug_abstract": "In this work, we present Tilt-Ropter, a novel hybrid aerial-terrestrial vehicle (HATV) that combines tilt rotors with passive wheels to achieve energy-efficient multi-mode locomotion. Unlike existing under-actuated HATVs, the fully actuated design of Tilt-Ropter enables decoupled force and torque control, greatly enhancing its mobility and environmental adaptability. A nonlinear model predictive controller (NMPC) is developed to track reference trajectories and handle contact constraints across locomotion modes, while a dedicated control allocation module exploits actuation redundancy to achieve energy-efficient control of actuators. Additionally, to enhance robustness during ground contact, we introduce an external wrench estimation algorithm that estimates environmental interaction forces and torques in real time. The system is validated through both simulation and real-world experiments, including seamless air-ground transitions and trajectory tracking. Results show low tracking errors in both modes and highlight a 92.8% reduction in power consumption during ground locomotion, demonstrating the system&#39;s potential for long-duration missions across large-scale and energy-constrained environments."
    },
    {
      "title": "Reasoning with Autoregressive-Diffusion Collaborative Thoughts",
      "url": "https://arxiv.org/abs/2602.01608",
      "date": "2026-02-02",
      "authors_text": "Mu Yuan, Liekang Zeng, Guoliang Xing, Lan Zhang, Yunhao Liu",
      "is_highlight": false,
      "score": 41.0,
      "summary": "The paper presents Collaborative Thoughts, a framework integrating autoregressive and diffusion models for enhanced spatial reasoning and generation through iterative feedback and joint reasoning.",
      "teaser_image": "https://arxiv.org/html/2602.01608/x2.png",
      "debug_abstract": "Autoregressive and diffusion models represent two complementary generative paradigms. Autoregressive models excel at sequential planning and constraint composition, yet struggle with tasks that require explicit spatial or physical grounding. Diffusion models, in contrast, capture rich spatial structure through high-dimensional generation, but lack the stepwise logical control needed to satisfy complex, multi-stage constraints or to reliably identify and correct errors. We introduce Collaborative Thoughts, a unified collaborative framework that enables autoregressive and diffusion models to reason and generate jointly through a closed-loop interaction. In Collaborative Thoughts, autoregressive models perform structured planning and constraint management, diffusion models instantiate these constraints as intermediate visual thoughts, and a vision-based critic module evaluates whether the visual thoughts satisfy the intended structural and physical requirements. This feedback is then used to iteratively refine subsequent planning and generation steps, mitigating error propagation across modalities. Importantly, Collaborative Thoughts uses the same collaborative loop regardless of whether the task is autoregressive question answering or diffusion-based visual generation. Through representative examples, we illustrate how Collaborative Thoughts can improve the reliability of spatial reasoning and the controllability of generation."
    },
    {
      "title": "Adaptive Rollout Allocation for Online Reinforcement Learning with Verifiable Rewards",
      "url": "https://arxiv.org/abs/2602.01601",
      "date": "2026-02-02",
      "authors_text": "Hieu Trung Nguyen, Bao Nguyen, Wenao Ma, Yuzhi Zhao, Ruifeng She",
      "is_highlight": false,
      "score": 40.0,
      "summary": "The paper presents \\Ours, a variance-informed strategy for adaptive rollout allocation in online reinforcement learning, enhancing sampling efficiency and performance over traditional methods.",
      "teaser_image": "https://arxiv.org/html/2602.01601/x2.png",
      "debug_abstract": "Sampling efficiency is a key bottleneck in reinforcement learning with verifiable rewards. Existing group-based policy optimization methods, such as GRPO, allocate a fixed number of rollouts for all training prompts. This uniform allocation implicitly treats all prompts as equally informative, and could lead to inefficient computational budget usage and impede training progress. We introduce \\Ours, a Variance-Informed Predictive allocation strategy that allocates a given rollout budget to the prompts in the incumbent batch to minimize the expected gradient variance of the policy update. At each iteration, \\Ours~uses a lightweight Gaussian process model to predict per-prompt success probabilities based on recent rollouts. These probability predictions are translated into variance estimates, which are then fed into a convex optimization problem to determine the optimal rollout allocations under a hard compute budget constraint. Empirical results show that \\Ours~consistently improves sampling efficiency and achieves higher performance than uniform or heuristic allocation strategies in multiple benchmarks. Our code will be available at this https URL."
    },
    {
      "title": "MedAD-R1: Eliciting Consistent Reasoning in Interpretible Medical Anomaly Detection via Consistency-Reinforced Policy Optimization",
      "url": "https://arxiv.org/abs/2602.01081",
      "date": "2026-02-01",
      "authors_text": "Haitao Zhang, Yingying Wang, Jiaxiang Wang, Haote Xu, Hongyang Zhang",
      "is_highlight": false,
      "score": 64.0,
      "summary": "MedAD-R1 enhances medical anomaly detection by utilizing a two-stage training framework with consistency-reinforced policy optimization, achieving state-of-the-art performance on the MedAD-38K benchmark.",
      "teaser_image": "https://arxiv.org/html/2602.01081/x3.png",
      "debug_abstract": "Medical Anomaly Detection (MedAD) presents a significant opportunity to enhance diagnostic accuracy using Large Multimodal Models (LMMs) to interpret and answer questions based on medical images. However, the reliance on Supervised Fine-Tuning (SFT) on simplistic and fragmented datasets has hindered the development of models capable of plausible reasoning and robust multimodal generalization. To overcome this, we introduce MedAD-38K, the first large-scale, multi-modal, and multi-center benchmark for MedAD featuring diagnostic Chain-of-Thought (CoT) annotations alongside structured Visual Question-Answering (VQA) pairs. On this foundation, we propose a two-stage training framework. The first stage, Cognitive Injection, uses SFT to instill foundational medical knowledge and align the model with a structured think-then-answer paradigm. Given that standard policy optimization can produce reasoning that is disconnected from the final answer, the second stage incorporates Consistency Group Relative Policy Optimization (Con-GRPO). This novel algorithm incorporates a crucial consistency reward to ensure the generated reasoning process is relevant and logically coherent with the final diagnosis. Our proposed model, MedAD-R1, achieves state-of-the-art (SOTA) performance on the MedAD-38K benchmark, outperforming strong baselines by more than 10\\%. This superior performance stems from its ability to generate transparent and logically consistent reasoning pathways, offering a promising approach to enhancing the trustworthiness and interpretability of AI for clinical decision support."
    },
    {
      "title": "Residual Decoding: Mitigating Hallucinations in Large Vision-Language Models via History-Aware Residual Guidance",
      "url": "https://arxiv.org/abs/2602.01047",
      "date": "2026-02-01",
      "authors_text": "Xinrong Chen, Xu Chu, Yingmin Qiu, Hengyuan Zhang, Jing Xiong",
      "is_highlight": false,
      "score": 53.0,
      "summary": "Residual Decoding (ResDec) mitigates hallucinations in Large Vision-Language Models by leveraging historical information to enhance decoding and improve visual grounding.",
      "teaser_image": "https://arxiv.org/html/2602.01047/x1.png",
      "debug_abstract": "Large Vision-Language Models (LVLMs) can reason effectively from image-text inputs and perform well in various multimodal tasks. Despite this success, they are affected by language priors and often produce hallucinations. Hallucinations denote generated content that is grammatically and syntactically coherent, yet bears no match or direct relevance to actual visual input. To address this problem, we propose Residual Decoding (ResDec). It is a novel training-free method that uses historical information to aid decoding. The method relies on the internal implicit reasoning mechanism and token logits evolution mechanism of LVLMs to correct biases. Extensive experiments demonstrate that ResDec effectively suppresses hallucinations induced by language priors, significantly improves visual grounding, and reduces object hallucinations. In addition to mitigating hallucinations, ResDec also performs exceptionally well on comprehensive LVLM benchmarks, highlighting its broad applicability."
    },
    {
      "title": "Offline Discovery of Interpretable Skills from Multi-Task Trajectories",
      "url": "https://arxiv.org/abs/2602.01018",
      "date": "2026-02-01",
      "authors_text": "Chongyu Zhu, Mithun Vanniasinghe, Jiayu Chen, Chi-Guhn Lee",
      "is_highlight": false,
      "score": 77.0,
      "summary": "LOKI is a three-stage framework for offline skill discovery and hierarchical imitation learning, effectively extracting interpretable skills from multi-task trajectories without explicit rewards.",
      "teaser_image": "https://arxiv.org/html/2602.01018/fig/two_stage_skill_learning.png",
      "debug_abstract": "Hierarchical Imitation Learning is a powerful paradigm for acquiring complex robot behaviors from demonstrations. A central challenge, however, lies in discovering reusable skills from long-horizon, multi-task offline data, especially when the data lacks explicit rewards or subtask annotations. In this work, we introduce LOKI, a three-stage end-to-end learning framework designed for offline skill discovery and hierarchical imitation. The framework commences with a two-stage, weakly supervised skill discovery process: Stage one performs coarse, task-aware macro-segmentation by employing an alignment-enforced Vector Quantized VAE guided by weak task labels. Stage two then refines these segments at a micro-level using a self-supervised sequential model, followed by an iterative clustering process to consolidate skill boundaries. The third stage then leverages these precise boundaries to construct a hierarchical policy within an option-based framework-complete with a learned termination condition beta for explicit skill switching. LOKI achieves high success rates on the challenging D4RL Kitchen benchmark and outperforms standard HIL baselines. Furthermore, we demonstrate that the discovered skills are semantically meaningful, aligning with human intuition, and exhibit compositionality by successfully sequencing them to solve a novel, unseen task."
    },
    {
      "title": "Dual Quaternion SE(3) Synchronization with Recovery Guarantees",
      "url": "https://arxiv.org/abs/2602.00324",
      "date": "2026-01-30",
      "authors_text": "Jianing Zhao, Linglingzhi Zhu, Anthony Man-Cho So",
      "is_highlight": false,
      "score": 75.0,
      "summary": "This paper presents a dual quaternion-based SE(3) synchronization method with a two-stage algorithm that guarantees recovery of absolute poses from noisy transformations, outperforming traditional matrix methods.",
      "teaser_image": "https://arxiv.org/html/2602.00324/x2.png",
      "debug_abstract": "Synchronization over the special Euclidean group SE(3) aims to recover absolute poses from noisy pairwise relative transformations and is a core primitive in robotics and 3D vision. Standard approaches often require multi-step heuristic procedures to recover valid poses, which are difficult to analyze and typically lack theoretical guarantees. This paper adopts a dual quaternion representation and formulates SE(3) synchronization directly over the unit dual quaternion. A two-stage algorithm is developed: A spectral initializer computed via the power method on a Hermitian dual quaternion measurement matrix, followed by a dual quaternion generalized power method (DQGPM) that enforces feasibility through per-iteration projection. The estimation error bounds are established for spectral estimators, and DQGPM is shown to admit a finite-iteration error bound and achieves linear error contraction up to an explicit noise-dependent threshold. Experiments on synthetic benchmarks and real-world multi-scan point-set registration demonstrate that the proposed pipeline improves both accuracy and efficiency over representative matrix-based methods."
    }
  ],
  "Language and Vision (LaVi) Lab": [
    {
      "title": "FlowSteer: Interactive Agentic Workflow Orchestration via End-to-End Reinforcement Learning",
      "url": "https://arxiv.org/abs/2602.01664",
      "date": "2026-02-02",
      "authors_text": "Mingda Zhang, Haoran Luo, Tiesunlong Shen, Qika Lin, Xiaoying Tang",
      "is_highlight": false,
      "score": 7.0,
      "summary": "FlowSteer is an end-to-end reinforcement learning framework that automates workflow orchestration through interactive learning, addressing manual costs and improving performance across diverse tasks.",
      "teaser_image": "https://arxiv.org/html/2602.01664/x1.png",
      "debug_abstract": "In recent years, a variety of powerful agentic workflows have been applied to solve a wide range of human problems. However, existing workflow orchestration still faces key challenges, including high manual cost, reliance on specific operators/large language models (LLMs), and sparse reward signals. To address these challenges, we propose FlowSteer, an end-to-end reinforcement learning framework that takes a lightweight policy model as the agent and an executable canvas environment, automating workflow orchestration through multi-turn interaction. In this process, the policy model analyzes execution states and selects editing actions, while the canvas executes operators and returns feedback for iterative refinement. Moreover, FlowSteer provides a plug-and-play framework that supports diverse operator libraries and interchangeable LLM backends. To effectively train this interaction paradigm, we propose Canvas Workflow Relative Policy Optimization (CWRPO), which introduces diversity-constrained rewards with conditional release to stabilize learning and suppress shortcut behaviors. Experimental results on twelve datasets show that FlowSteer significantly outperforms baselines across various tasks."
    },
    {
      "title": "Failure-Aware Bimanual Teleoperation via Conservative Value Guided Assistance",
      "url": "https://arxiv.org/abs/2602.01092",
      "date": "2026-02-01",
      "authors_text": "Peng Zhou, Zhongxuan Li, Jinsong Wu, Jiaming Qi, Jun Hu",
      "is_highlight": false,
      "score": 9.0,
      "summary": "This paper presents a failure-aware bimanual teleoperation framework that uses conservative value learning to enhance task success and reduce operator workload through compliant haptic assistance.",
      "teaser_image": "https://arxiv.org/html/2602.01092/x1.png",
      "debug_abstract": "Teleoperation of high-precision manipulation is con-strained by tight success tolerances and complex contact dy-namics, which make impending failures difficult for human operators to anticipate under partial observability. This paper proposes a value-guided, failure-aware framework for bimanual teleoperation that provides compliant haptic assistance while pre-serving continuous human authority. The framework is trained entirely from heterogeneous offline teleoperation data containing both successful and failed executions. Task feasibility is mod-eled as a conservative success score learned via Conservative Value Learning, yielding a risk-sensitive estimate that remains reliable under distribution shift. During online operation, the learned success score regulates the level of assistance, while a learned actor provides a corrective motion direction. Both are integrated through a joint-space impedance interface on the master side, yielding continuous guidance that steers the operator away from failure-prone actions without overriding intent. Experimental results on contact-rich manipulation tasks demonstrate improved task success rates and reduced operator workload compared to conventional teleoperation and shared-autonomy baselines, indicating that conservative value learning provides an effective mechanism for embedding failure awareness into bilateral teleoperation. Experimental videos are available at this https URL"
    },
    {
      "title": "Towards Agentic Intelligence for Materials Science",
      "url": "https://arxiv.org/abs/2602.00169",
      "date": "2026-01-29",
      "authors_text": "Huan Zhang, Yizhan Li, Wenhao Huang, Ziyu Hou, Yu Song",
      "is_highlight": false,
      "score": 12.0,
      "summary": "This paper advocates for developing agentic AI systems in materials science that autonomously plan, act, and learn to enhance discovery processes beyond traditional models.",
      "teaser_image": "https://arxiv.org/html/2602.00169/x1.png",
      "debug_abstract": "The convergence of artificial intelligence and materials science presents a transformative opportunity, but achieving true acceleration in discovery requires moving beyond task-isolated, fine-tuned models toward agentic systems that plan, act, and learn across the full discovery loop. This survey advances a unique pipeline-centric view that spans from corpus curation and pretraining, through domain adaptation and instruction tuning, to goal-conditioned agents interfacing with simulation and experimental platforms. Unlike prior reviews, we treat the entire process as an end-to-end system to be optimized for tangible discovery outcomes rather than proxy benchmarks. This perspective allows us to trace how upstream design choices-such as data curation and training objectives-can be aligned with downstream experimental success through effective credit assignment. To bridge communities and establish a shared frame of reference, we first present an integrated lens that aligns terminology, evaluation, and workflow stages across AI and materials science. We then analyze the field through two focused lenses: From the AI perspective, the survey details LLM strengths in pattern recognition, predictive analytics, and natural language processing for literature mining, materials characterization, and property prediction; from the materials science perspective, it highlights applications in materials design, process optimization, and the acceleration of computational workflows via integration with external tools (e.g., DFT, robotic labs). Finally, we contrast passive, reactive approaches with agentic design, cataloging current contributions while motivating systems that pursue long-horizon goals with autonomy, memory, and tool use. This survey charts a practical roadmap towards autonomous, safety-aware LLM agents aimed at discovering novel and useful materials."
    },
    {
      "title": "TIDE: Trajectory-based Diagnostic Evaluation of Test-Time Improvement in LLM Agents",
      "url": "https://arxiv.org/abs/2602.02196",
      "date": "2026-02-02",
      "authors_text": "Hang Yan, Xinyu Che, Fangzhi Xu, Qiushi Sun, Zichen Ding",
      "is_highlight": false,
      "score": 20.0,
      "summary": "The TIDE framework evaluates Test-Time Improvement in LLM agents by analyzing task completion dynamics, recursive behaviors, and memory constraints to enhance performance optimization.",
      "teaser_image": "https://arxiv.org/html/2602.02196/x1.png",
      "debug_abstract": "Recent advances in autonomous LLM agents demonstrate their ability to improve performance through iterative interaction with the environment. We define this paradigm as Test-Time Improvement (TTI). However, the mechanisms under how and why TTI succeed or fail remain poorly understood, and existing evaluation metrics fail to capture their task optimization efficiency, behavior adaptation after erroneous actions, and the specific utility of working memory for task completion. To address these gaps, we propose Test-time Improvement Diagnostic Evaluation (TIDE), an agent-agnostic and environment-agnostic framework that decomposes TTI into three comprehensive and interconnected dimensions. The framework measures (1) the overall temporal dynamics of task completion and (2) identifies whether performance is primarily constrained by recursive looping behaviors or (3) by burdensome accumulated memory. Through extensive experiments across diverse agents and environments, TIDE highlights that improving agent performance requires more than scaling internal reasoning, calling for explicitly optimizing the interaction dynamics between the agent and the environment."
    },
    {
      "title": "Breaking the Static Graph: Context-Aware Traversal for Robust Retrieval-Augmented Generation",
      "url": "https://arxiv.org/abs/2602.01965",
      "date": "2026-02-02",
      "authors_text": "Kwun Hang Lau, Fangyuan Zhang, Boyu Ruan, Yingli Zhou, Qintian Guo",
      "is_highlight": false,
      "score": 27.0,
      "summary": "CatRAG enhances Retrieval-Augmented Generation by transforming static Knowledge Graphs into query-adaptive structures, improving multi-hop evidence retrieval and reasoning completeness.",
      "teaser_image": "https://arxiv.org/html/2602.01965/image/HubBiasShift.png",
      "debug_abstract": "Recent advances in Retrieval-Augmented Generation (RAG) have shifted from simple vector similarity to structure-aware approaches like HippoRAG, which leverage Knowledge Graphs (KGs) and Personalized PageRank (PPR) to capture multi-hop dependencies. However, these methods suffer from a &#34;Static Graph Fallacy&#34;: they rely on fixed transition probabilities determined during indexing. This rigidity ignores the query-dependent nature of edge relevance, causing semantic drift where random walks are diverted into high-degree &#34;hub&#34; nodes before reaching critical downstream evidence. Consequently, models often achieve high partial recall but fail to retrieve the complete evidence chain required for multi-hop queries. To address this, we propose CatRAG, Context-Aware Traversal for robust RAG, a framework that builds on the HippoRAG 2 architecture and transforms the static KG into a query-adaptive navigation structure. We introduce a multi-faceted framework to steer the random walk: (1) Symbolic Anchoring, which injects weak entity constraints to regularize the random walk; (2) Query-Aware Dynamic Edge Weighting, which dynamically modulates graph structure, to prune irrelevant paths while amplifying those aligned with the query&#39;s intent; and (3) Key-Fact Passage Weight Enhancement, a cost-efficient bias that structurally anchors the random walk to likely evidence. Experiments across four multi-hop benchmarks demonstrate that CatRAG consistently outperforms state of the art baselines. Our analysis reveals that while standard Recall metrics show modest gains, CatRAG achieves substantial improvements in reasoning completeness, the capacity to recover the entire evidence path without gaps. These results reveal that our approach effectively bridges the gap between retrieving partial context and enabling fully grounded reasoning. Resources are available at this https URL."
    },
    {
      "title": "Tilt-Ropter: A Novel Hybrid Aerial and Terrestrial Vehicle with Tilt Rotors and Passive Wheels",
      "url": "https://arxiv.org/abs/2602.01700",
      "date": "2026-02-02",
      "authors_text": "Ruoyu Wang, Xuchen Liu, Zongzhou Wu, Zixuan Guo, Wendi Ding",
      "is_highlight": false,
      "score": 38.0,
      "summary": "The Tilt-Ropter is a hybrid aerial-terrestrial vehicle that utilizes tilt rotors and passive wheels for efficient locomotion, featuring advanced control systems for enhanced mobility and energy savings.",
      "teaser_image": "https://arxiv.org/html/2602.01700/x2.png",
      "debug_abstract": "In this work, we present Tilt-Ropter, a novel hybrid aerial-terrestrial vehicle (HATV) that combines tilt rotors with passive wheels to achieve energy-efficient multi-mode locomotion. Unlike existing under-actuated HATVs, the fully actuated design of Tilt-Ropter enables decoupled force and torque control, greatly enhancing its mobility and environmental adaptability. A nonlinear model predictive controller (NMPC) is developed to track reference trajectories and handle contact constraints across locomotion modes, while a dedicated control allocation module exploits actuation redundancy to achieve energy-efficient control of actuators. Additionally, to enhance robustness during ground contact, we introduce an external wrench estimation algorithm that estimates environmental interaction forces and torques in real time. The system is validated through both simulation and real-world experiments, including seamless air-ground transitions and trajectory tracking. Results show low tracking errors in both modes and highlight a 92.8% reduction in power consumption during ground locomotion, demonstrating the system&#39;s potential for long-duration missions across large-scale and energy-constrained environments."
    },
    {
      "title": "Reasoning with Autoregressive-Diffusion Collaborative Thoughts",
      "url": "https://arxiv.org/abs/2602.01608",
      "date": "2026-02-02",
      "authors_text": "Mu Yuan, Liekang Zeng, Guoliang Xing, Lan Zhang, Yunhao Liu",
      "is_highlight": false,
      "score": 41.0,
      "summary": "The paper presents Collaborative Thoughts, a framework integrating autoregressive and diffusion models for enhanced spatial reasoning and generation through iterative feedback and joint reasoning.",
      "teaser_image": "https://arxiv.org/html/2602.01608/x2.png",
      "debug_abstract": "Autoregressive and diffusion models represent two complementary generative paradigms. Autoregressive models excel at sequential planning and constraint composition, yet struggle with tasks that require explicit spatial or physical grounding. Diffusion models, in contrast, capture rich spatial structure through high-dimensional generation, but lack the stepwise logical control needed to satisfy complex, multi-stage constraints or to reliably identify and correct errors. We introduce Collaborative Thoughts, a unified collaborative framework that enables autoregressive and diffusion models to reason and generate jointly through a closed-loop interaction. In Collaborative Thoughts, autoregressive models perform structured planning and constraint management, diffusion models instantiate these constraints as intermediate visual thoughts, and a vision-based critic module evaluates whether the visual thoughts satisfy the intended structural and physical requirements. This feedback is then used to iteratively refine subsequent planning and generation steps, mitigating error propagation across modalities. Importantly, Collaborative Thoughts uses the same collaborative loop regardless of whether the task is autoregressive question answering or diffusion-based visual generation. Through representative examples, we illustrate how Collaborative Thoughts can improve the reliability of spatial reasoning and the controllability of generation."
    },
    {
      "title": "Adaptive Rollout Allocation for Online Reinforcement Learning with Verifiable Rewards",
      "url": "https://arxiv.org/abs/2602.01601",
      "date": "2026-02-02",
      "authors_text": "Hieu Trung Nguyen, Bao Nguyen, Wenao Ma, Yuzhi Zhao, Ruifeng She",
      "is_highlight": false,
      "score": 40.0,
      "summary": "The paper presents \\Ours, a variance-informed strategy for adaptive rollout allocation in online reinforcement learning, enhancing sampling efficiency and performance over traditional methods.",
      "teaser_image": "https://arxiv.org/html/2602.01601/x2.png",
      "debug_abstract": "Sampling efficiency is a key bottleneck in reinforcement learning with verifiable rewards. Existing group-based policy optimization methods, such as GRPO, allocate a fixed number of rollouts for all training prompts. This uniform allocation implicitly treats all prompts as equally informative, and could lead to inefficient computational budget usage and impede training progress. We introduce \\Ours, a Variance-Informed Predictive allocation strategy that allocates a given rollout budget to the prompts in the incumbent batch to minimize the expected gradient variance of the policy update. At each iteration, \\Ours~uses a lightweight Gaussian process model to predict per-prompt success probabilities based on recent rollouts. These probability predictions are translated into variance estimates, which are then fed into a convex optimization problem to determine the optimal rollout allocations under a hard compute budget constraint. Empirical results show that \\Ours~consistently improves sampling efficiency and achieves higher performance than uniform or heuristic allocation strategies in multiple benchmarks. Our code will be available at this https URL."
    },
    {
      "title": "MedAD-R1: Eliciting Consistent Reasoning in Interpretible Medical Anomaly Detection via Consistency-Reinforced Policy Optimization",
      "url": "https://arxiv.org/abs/2602.01081",
      "date": "2026-02-01",
      "authors_text": "Haitao Zhang, Yingying Wang, Jiaxiang Wang, Haote Xu, Hongyang Zhang",
      "is_highlight": false,
      "score": 64.0,
      "summary": "MedAD-R1 enhances medical anomaly detection by utilizing a two-stage training framework with consistency-reinforced policy optimization, achieving state-of-the-art performance on the MedAD-38K benchmark.",
      "teaser_image": "https://arxiv.org/html/2602.01081/x3.png",
      "debug_abstract": "Medical Anomaly Detection (MedAD) presents a significant opportunity to enhance diagnostic accuracy using Large Multimodal Models (LMMs) to interpret and answer questions based on medical images. However, the reliance on Supervised Fine-Tuning (SFT) on simplistic and fragmented datasets has hindered the development of models capable of plausible reasoning and robust multimodal generalization. To overcome this, we introduce MedAD-38K, the first large-scale, multi-modal, and multi-center benchmark for MedAD featuring diagnostic Chain-of-Thought (CoT) annotations alongside structured Visual Question-Answering (VQA) pairs. On this foundation, we propose a two-stage training framework. The first stage, Cognitive Injection, uses SFT to instill foundational medical knowledge and align the model with a structured think-then-answer paradigm. Given that standard policy optimization can produce reasoning that is disconnected from the final answer, the second stage incorporates Consistency Group Relative Policy Optimization (Con-GRPO). This novel algorithm incorporates a crucial consistency reward to ensure the generated reasoning process is relevant and logically coherent with the final diagnosis. Our proposed model, MedAD-R1, achieves state-of-the-art (SOTA) performance on the MedAD-38K benchmark, outperforming strong baselines by more than 10\\%. This superior performance stems from its ability to generate transparent and logically consistent reasoning pathways, offering a promising approach to enhancing the trustworthiness and interpretability of AI for clinical decision support."
    },
    {
      "title": "Residual Decoding: Mitigating Hallucinations in Large Vision-Language Models via History-Aware Residual Guidance",
      "url": "https://arxiv.org/abs/2602.01047",
      "date": "2026-02-01",
      "authors_text": "Xinrong Chen, Xu Chu, Yingmin Qiu, Hengyuan Zhang, Jing Xiong",
      "is_highlight": false,
      "score": 53.0,
      "summary": "Residual Decoding (ResDec) mitigates hallucinations in Large Vision-Language Models by leveraging historical information to enhance decoding and improve visual grounding.",
      "teaser_image": "https://arxiv.org/html/2602.01047/x1.png",
      "debug_abstract": "Large Vision-Language Models (LVLMs) can reason effectively from image-text inputs and perform well in various multimodal tasks. Despite this success, they are affected by language priors and often produce hallucinations. Hallucinations denote generated content that is grammatically and syntactically coherent, yet bears no match or direct relevance to actual visual input. To address this problem, we propose Residual Decoding (ResDec). It is a novel training-free method that uses historical information to aid decoding. The method relies on the internal implicit reasoning mechanism and token logits evolution mechanism of LVLMs to correct biases. Extensive experiments demonstrate that ResDec effectively suppresses hallucinations induced by language priors, significantly improves visual grounding, and reduces object hallucinations. In addition to mitigating hallucinations, ResDec also performs exceptionally well on comprehensive LVLM benchmarks, highlighting its broad applicability."
    },
    {
      "title": "Offline Discovery of Interpretable Skills from Multi-Task Trajectories",
      "url": "https://arxiv.org/abs/2602.01018",
      "date": "2026-02-01",
      "authors_text": "Chongyu Zhu, Mithun Vanniasinghe, Jiayu Chen, Chi-Guhn Lee",
      "is_highlight": false,
      "score": 77.0,
      "summary": "LOKI is a three-stage framework for offline skill discovery and hierarchical imitation learning, effectively extracting interpretable skills from multi-task trajectories without explicit rewards.",
      "teaser_image": "https://arxiv.org/html/2602.01018/fig/two_stage_skill_learning.png",
      "debug_abstract": "Hierarchical Imitation Learning is a powerful paradigm for acquiring complex robot behaviors from demonstrations. A central challenge, however, lies in discovering reusable skills from long-horizon, multi-task offline data, especially when the data lacks explicit rewards or subtask annotations. In this work, we introduce LOKI, a three-stage end-to-end learning framework designed for offline skill discovery and hierarchical imitation. The framework commences with a two-stage, weakly supervised skill discovery process: Stage one performs coarse, task-aware macro-segmentation by employing an alignment-enforced Vector Quantized VAE guided by weak task labels. Stage two then refines these segments at a micro-level using a self-supervised sequential model, followed by an iterative clustering process to consolidate skill boundaries. The third stage then leverages these precise boundaries to construct a hierarchical policy within an option-based framework-complete with a learned termination condition beta for explicit skill switching. LOKI achieves high success rates on the challenging D4RL Kitchen benchmark and outperforms standard HIL baselines. Furthermore, we demonstrate that the discovered skills are semantically meaningful, aligning with human intuition, and exhibit compositionality by successfully sequencing them to solve a novel, unseen task."
    },
    {
      "title": "Dual Quaternion SE(3) Synchronization with Recovery Guarantees",
      "url": "https://arxiv.org/abs/2602.00324",
      "date": "2026-01-30",
      "authors_text": "Jianing Zhao, Linglingzhi Zhu, Anthony Man-Cho So",
      "is_highlight": false,
      "score": 75.0,
      "summary": "This paper presents a dual quaternion-based SE(3) synchronization method with a two-stage algorithm that guarantees recovery of absolute poses from noisy transformations, outperforming traditional matrix methods.",
      "teaser_image": "https://arxiv.org/html/2602.00324/x2.png",
      "debug_abstract": "Synchronization over the special Euclidean group SE(3) aims to recover absolute poses from noisy pairwise relative transformations and is a core primitive in robotics and 3D vision. Standard approaches often require multi-step heuristic procedures to recover valid poses, which are difficult to analyze and typically lack theoretical guarantees. This paper adopts a dual quaternion representation and formulates SE(3) synchronization directly over the unit dual quaternion. A two-stage algorithm is developed: A spectral initializer computed via the power method on a Hermitian dual quaternion measurement matrix, followed by a dual quaternion generalized power method (DQGPM) that enforces feasibility through per-iteration projection. The estimation error bounds are established for spectral estimators, and DQGPM is shown to admit a finite-iteration error bound and achieves linear error contraction up to an explicit noise-dependent threshold. Experiments on synthetic benchmarks and real-world multi-scan point-set registration demonstrate that the proposed pipeline improves both accuracy and efficiency over representative matrix-based methods."
    }
  ],
  "深圳市人工智能与机器人研究院 (AIRS)": [
    {
      "title": "FlowSteer: Interactive Agentic Workflow Orchestration via End-to-End Reinforcement Learning",
      "url": "https://arxiv.org/abs/2602.01664",
      "date": "2026-02-02",
      "authors_text": "Mingda Zhang, Haoran Luo, Tiesunlong Shen, Qika Lin, Xiaoying Tang",
      "is_highlight": false,
      "score": 7.0,
      "summary": "FlowSteer is an end-to-end reinforcement learning framework that automates workflow orchestration through interactive learning, addressing manual costs and improving performance across diverse tasks.",
      "teaser_image": "https://arxiv.org/html/2602.01664/x1.png",
      "debug_abstract": "In recent years, a variety of powerful agentic workflows have been applied to solve a wide range of human problems. However, existing workflow orchestration still faces key challenges, including high manual cost, reliance on specific operators/large language models (LLMs), and sparse reward signals. To address these challenges, we propose FlowSteer, an end-to-end reinforcement learning framework that takes a lightweight policy model as the agent and an executable canvas environment, automating workflow orchestration through multi-turn interaction. In this process, the policy model analyzes execution states and selects editing actions, while the canvas executes operators and returns feedback for iterative refinement. Moreover, FlowSteer provides a plug-and-play framework that supports diverse operator libraries and interchangeable LLM backends. To effectively train this interaction paradigm, we propose Canvas Workflow Relative Policy Optimization (CWRPO), which introduces diversity-constrained rewards with conditional release to stabilize learning and suppress shortcut behaviors. Experimental results on twelve datasets show that FlowSteer significantly outperforms baselines across various tasks."
    },
    {
      "title": "Towards Agentic Intelligence for Materials Science",
      "url": "https://arxiv.org/abs/2602.00169",
      "date": "2026-01-29",
      "authors_text": "Huan Zhang, Yizhan Li, Wenhao Huang, Ziyu Hou, Yu Song",
      "is_highlight": false,
      "score": 12.0,
      "summary": "This paper advocates for developing agentic AI systems in materials science that autonomously plan, act, and learn to enhance discovery processes beyond traditional models.",
      "teaser_image": "https://arxiv.org/html/2602.00169/x1.png",
      "debug_abstract": "The convergence of artificial intelligence and materials science presents a transformative opportunity, but achieving true acceleration in discovery requires moving beyond task-isolated, fine-tuned models toward agentic systems that plan, act, and learn across the full discovery loop. This survey advances a unique pipeline-centric view that spans from corpus curation and pretraining, through domain adaptation and instruction tuning, to goal-conditioned agents interfacing with simulation and experimental platforms. Unlike prior reviews, we treat the entire process as an end-to-end system to be optimized for tangible discovery outcomes rather than proxy benchmarks. This perspective allows us to trace how upstream design choices-such as data curation and training objectives-can be aligned with downstream experimental success through effective credit assignment. To bridge communities and establish a shared frame of reference, we first present an integrated lens that aligns terminology, evaluation, and workflow stages across AI and materials science. We then analyze the field through two focused lenses: From the AI perspective, the survey details LLM strengths in pattern recognition, predictive analytics, and natural language processing for literature mining, materials characterization, and property prediction; from the materials science perspective, it highlights applications in materials design, process optimization, and the acceleration of computational workflows via integration with external tools (e.g., DFT, robotic labs). Finally, we contrast passive, reactive approaches with agentic design, cataloging current contributions while motivating systems that pursue long-horizon goals with autonomy, memory, and tool use. This survey charts a practical roadmap towards autonomous, safety-aware LLM agents aimed at discovering novel and useful materials."
    },
    {
      "title": "Breaking the Static Graph: Context-Aware Traversal for Robust Retrieval-Augmented Generation",
      "url": "https://arxiv.org/abs/2602.01965",
      "date": "2026-02-02",
      "authors_text": "Kwun Hang Lau, Fangyuan Zhang, Boyu Ruan, Yingli Zhou, Qintian Guo",
      "is_highlight": false,
      "score": 27.0,
      "summary": "CatRAG enhances Retrieval-Augmented Generation by transforming static Knowledge Graphs into query-adaptive structures, improving multi-hop evidence retrieval and reasoning completeness.",
      "teaser_image": "https://arxiv.org/html/2602.01965/image/HubBiasShift.png",
      "debug_abstract": "Recent advances in Retrieval-Augmented Generation (RAG) have shifted from simple vector similarity to structure-aware approaches like HippoRAG, which leverage Knowledge Graphs (KGs) and Personalized PageRank (PPR) to capture multi-hop dependencies. However, these methods suffer from a &#34;Static Graph Fallacy&#34;: they rely on fixed transition probabilities determined during indexing. This rigidity ignores the query-dependent nature of edge relevance, causing semantic drift where random walks are diverted into high-degree &#34;hub&#34; nodes before reaching critical downstream evidence. Consequently, models often achieve high partial recall but fail to retrieve the complete evidence chain required for multi-hop queries. To address this, we propose CatRAG, Context-Aware Traversal for robust RAG, a framework that builds on the HippoRAG 2 architecture and transforms the static KG into a query-adaptive navigation structure. We introduce a multi-faceted framework to steer the random walk: (1) Symbolic Anchoring, which injects weak entity constraints to regularize the random walk; (2) Query-Aware Dynamic Edge Weighting, which dynamically modulates graph structure, to prune irrelevant paths while amplifying those aligned with the query&#39;s intent; and (3) Key-Fact Passage Weight Enhancement, a cost-efficient bias that structurally anchors the random walk to likely evidence. Experiments across four multi-hop benchmarks demonstrate that CatRAG consistently outperforms state of the art baselines. Our analysis reveals that while standard Recall metrics show modest gains, CatRAG achieves substantial improvements in reasoning completeness, the capacity to recover the entire evidence path without gaps. These results reveal that our approach effectively bridges the gap between retrieving partial context and enabling fully grounded reasoning. Resources are available at this https URL."
    },
    {
      "title": "Tilt-Ropter: A Novel Hybrid Aerial and Terrestrial Vehicle with Tilt Rotors and Passive Wheels",
      "url": "https://arxiv.org/abs/2602.01700",
      "date": "2026-02-02",
      "authors_text": "Ruoyu Wang, Xuchen Liu, Zongzhou Wu, Zixuan Guo, Wendi Ding",
      "is_highlight": false,
      "score": 38.0,
      "summary": "The Tilt-Ropter is a hybrid aerial-terrestrial vehicle that utilizes tilt rotors and passive wheels for efficient locomotion, featuring advanced control systems for enhanced mobility and energy savings.",
      "teaser_image": "https://arxiv.org/html/2602.01700/x2.png",
      "debug_abstract": "In this work, we present Tilt-Ropter, a novel hybrid aerial-terrestrial vehicle (HATV) that combines tilt rotors with passive wheels to achieve energy-efficient multi-mode locomotion. Unlike existing under-actuated HATVs, the fully actuated design of Tilt-Ropter enables decoupled force and torque control, greatly enhancing its mobility and environmental adaptability. A nonlinear model predictive controller (NMPC) is developed to track reference trajectories and handle contact constraints across locomotion modes, while a dedicated control allocation module exploits actuation redundancy to achieve energy-efficient control of actuators. Additionally, to enhance robustness during ground contact, we introduce an external wrench estimation algorithm that estimates environmental interaction forces and torques in real time. The system is validated through both simulation and real-world experiments, including seamless air-ground transitions and trajectory tracking. Results show low tracking errors in both modes and highlight a 92.8% reduction in power consumption during ground locomotion, demonstrating the system&#39;s potential for long-duration missions across large-scale and energy-constrained environments."
    },
    {
      "title": "Reasoning with Autoregressive-Diffusion Collaborative Thoughts",
      "url": "https://arxiv.org/abs/2602.01608",
      "date": "2026-02-02",
      "authors_text": "Mu Yuan, Liekang Zeng, Guoliang Xing, Lan Zhang, Yunhao Liu",
      "is_highlight": false,
      "score": 41.0,
      "summary": "The paper presents Collaborative Thoughts, a framework integrating autoregressive and diffusion models for enhanced spatial reasoning and generation through iterative feedback and joint reasoning.",
      "teaser_image": "https://arxiv.org/html/2602.01608/x2.png",
      "debug_abstract": "Autoregressive and diffusion models represent two complementary generative paradigms. Autoregressive models excel at sequential planning and constraint composition, yet struggle with tasks that require explicit spatial or physical grounding. Diffusion models, in contrast, capture rich spatial structure through high-dimensional generation, but lack the stepwise logical control needed to satisfy complex, multi-stage constraints or to reliably identify and correct errors. We introduce Collaborative Thoughts, a unified collaborative framework that enables autoregressive and diffusion models to reason and generate jointly through a closed-loop interaction. In Collaborative Thoughts, autoregressive models perform structured planning and constraint management, diffusion models instantiate these constraints as intermediate visual thoughts, and a vision-based critic module evaluates whether the visual thoughts satisfy the intended structural and physical requirements. This feedback is then used to iteratively refine subsequent planning and generation steps, mitigating error propagation across modalities. Importantly, Collaborative Thoughts uses the same collaborative loop regardless of whether the task is autoregressive question answering or diffusion-based visual generation. Through representative examples, we illustrate how Collaborative Thoughts can improve the reliability of spatial reasoning and the controllability of generation."
    },
    {
      "title": "Adaptive Rollout Allocation for Online Reinforcement Learning with Verifiable Rewards",
      "url": "https://arxiv.org/abs/2602.01601",
      "date": "2026-02-02",
      "authors_text": "Hieu Trung Nguyen, Bao Nguyen, Wenao Ma, Yuzhi Zhao, Ruifeng She",
      "is_highlight": false,
      "score": 40.0,
      "summary": "The paper presents \\Ours, a variance-informed strategy for adaptive rollout allocation in online reinforcement learning, enhancing sampling efficiency and performance over traditional methods.",
      "teaser_image": "https://arxiv.org/html/2602.01601/x2.png",
      "debug_abstract": "Sampling efficiency is a key bottleneck in reinforcement learning with verifiable rewards. Existing group-based policy optimization methods, such as GRPO, allocate a fixed number of rollouts for all training prompts. This uniform allocation implicitly treats all prompts as equally informative, and could lead to inefficient computational budget usage and impede training progress. We introduce \\Ours, a Variance-Informed Predictive allocation strategy that allocates a given rollout budget to the prompts in the incumbent batch to minimize the expected gradient variance of the policy update. At each iteration, \\Ours~uses a lightweight Gaussian process model to predict per-prompt success probabilities based on recent rollouts. These probability predictions are translated into variance estimates, which are then fed into a convex optimization problem to determine the optimal rollout allocations under a hard compute budget constraint. Empirical results show that \\Ours~consistently improves sampling efficiency and achieves higher performance than uniform or heuristic allocation strategies in multiple benchmarks. Our code will be available at this https URL."
    },
    {
      "title": "MedAD-R1: Eliciting Consistent Reasoning in Interpretible Medical Anomaly Detection via Consistency-Reinforced Policy Optimization",
      "url": "https://arxiv.org/abs/2602.01081",
      "date": "2026-02-01",
      "authors_text": "Haitao Zhang, Yingying Wang, Jiaxiang Wang, Haote Xu, Hongyang Zhang",
      "is_highlight": false,
      "score": 64.0,
      "summary": "MedAD-R1 enhances medical anomaly detection by utilizing a two-stage training framework with consistency-reinforced policy optimization, achieving state-of-the-art performance on the MedAD-38K benchmark.",
      "teaser_image": "https://arxiv.org/html/2602.01081/x3.png",
      "debug_abstract": "Medical Anomaly Detection (MedAD) presents a significant opportunity to enhance diagnostic accuracy using Large Multimodal Models (LMMs) to interpret and answer questions based on medical images. However, the reliance on Supervised Fine-Tuning (SFT) on simplistic and fragmented datasets has hindered the development of models capable of plausible reasoning and robust multimodal generalization. To overcome this, we introduce MedAD-38K, the first large-scale, multi-modal, and multi-center benchmark for MedAD featuring diagnostic Chain-of-Thought (CoT) annotations alongside structured Visual Question-Answering (VQA) pairs. On this foundation, we propose a two-stage training framework. The first stage, Cognitive Injection, uses SFT to instill foundational medical knowledge and align the model with a structured think-then-answer paradigm. Given that standard policy optimization can produce reasoning that is disconnected from the final answer, the second stage incorporates Consistency Group Relative Policy Optimization (Con-GRPO). This novel algorithm incorporates a crucial consistency reward to ensure the generated reasoning process is relevant and logically coherent with the final diagnosis. Our proposed model, MedAD-R1, achieves state-of-the-art (SOTA) performance on the MedAD-38K benchmark, outperforming strong baselines by more than 10\\%. This superior performance stems from its ability to generate transparent and logically consistent reasoning pathways, offering a promising approach to enhancing the trustworthiness and interpretability of AI for clinical decision support."
    },
    {
      "title": "Dual Quaternion SE(3) Synchronization with Recovery Guarantees",
      "url": "https://arxiv.org/abs/2602.00324",
      "date": "2026-01-30",
      "authors_text": "Jianing Zhao, Linglingzhi Zhu, Anthony Man-Cho So",
      "is_highlight": false,
      "score": 75.0,
      "summary": "This paper presents a dual quaternion-based SE(3) synchronization method with a two-stage algorithm that guarantees recovery of absolute poses from noisy transformations, outperforming traditional matrix methods.",
      "teaser_image": "https://arxiv.org/html/2602.00324/x2.png",
      "debug_abstract": "Synchronization over the special Euclidean group SE(3) aims to recover absolute poses from noisy pairwise relative transformations and is a core primitive in robotics and 3D vision. Standard approaches often require multi-step heuristic procedures to recover valid poses, which are difficult to analyze and typically lack theoretical guarantees. This paper adopts a dual quaternion representation and formulates SE(3) synchronization directly over the unit dual quaternion. A two-stage algorithm is developed: A spectral initializer computed via the power method on a Hermitian dual quaternion measurement matrix, followed by a dual quaternion generalized power method (DQGPM) that enforces feasibility through per-iteration projection. The estimation error bounds are established for spectral estimators, and DQGPM is shown to admit a finite-iteration error bound and achieves linear error contraction up to an explicit noise-dependent threshold. Experiments on synthetic benchmarks and real-world multi-scan point-set registration demonstrate that the proposed pipeline improves both accuracy and efficiency over representative matrix-based methods."
    }
  ],
  "Hengshuang Zhao老师实验室": [
    {
      "title": "FlowSteer: Interactive Agentic Workflow Orchestration via End-to-End Reinforcement Learning",
      "url": "https://arxiv.org/abs/2602.01664",
      "date": "2026-02-02",
      "authors_text": "Mingda Zhang, Haoran Luo, Tiesunlong Shen, Qika Lin, Xiaoying Tang",
      "is_highlight": false,
      "score": 7.0,
      "summary": "FlowSteer is an end-to-end reinforcement learning framework that automates workflow orchestration through interactive learning, addressing manual costs and improving performance across diverse tasks.",
      "teaser_image": "https://arxiv.org/html/2602.01664/x1.png",
      "debug_abstract": "In recent years, a variety of powerful agentic workflows have been applied to solve a wide range of human problems. However, existing workflow orchestration still faces key challenges, including high manual cost, reliance on specific operators/large language models (LLMs), and sparse reward signals. To address these challenges, we propose FlowSteer, an end-to-end reinforcement learning framework that takes a lightweight policy model as the agent and an executable canvas environment, automating workflow orchestration through multi-turn interaction. In this process, the policy model analyzes execution states and selects editing actions, while the canvas executes operators and returns feedback for iterative refinement. Moreover, FlowSteer provides a plug-and-play framework that supports diverse operator libraries and interchangeable LLM backends. To effectively train this interaction paradigm, we propose Canvas Workflow Relative Policy Optimization (CWRPO), which introduces diversity-constrained rewards with conditional release to stabilize learning and suppress shortcut behaviors. Experimental results on twelve datasets show that FlowSteer significantly outperforms baselines across various tasks."
    },
    {
      "title": "Failure-Aware Bimanual Teleoperation via Conservative Value Guided Assistance",
      "url": "https://arxiv.org/abs/2602.01092",
      "date": "2026-02-01",
      "authors_text": "Peng Zhou, Zhongxuan Li, Jinsong Wu, Jiaming Qi, Jun Hu",
      "is_highlight": false,
      "score": 9.0,
      "summary": "This paper presents a failure-aware bimanual teleoperation framework that uses conservative value learning to enhance task success and reduce operator workload through compliant haptic assistance.",
      "teaser_image": "https://arxiv.org/html/2602.01092/x1.png",
      "debug_abstract": "Teleoperation of high-precision manipulation is con-strained by tight success tolerances and complex contact dy-namics, which make impending failures difficult for human operators to anticipate under partial observability. This paper proposes a value-guided, failure-aware framework for bimanual teleoperation that provides compliant haptic assistance while pre-serving continuous human authority. The framework is trained entirely from heterogeneous offline teleoperation data containing both successful and failed executions. Task feasibility is mod-eled as a conservative success score learned via Conservative Value Learning, yielding a risk-sensitive estimate that remains reliable under distribution shift. During online operation, the learned success score regulates the level of assistance, while a learned actor provides a corrective motion direction. Both are integrated through a joint-space impedance interface on the master side, yielding continuous guidance that steers the operator away from failure-prone actions without overriding intent. Experimental results on contact-rich manipulation tasks demonstrate improved task success rates and reduced operator workload compared to conventional teleoperation and shared-autonomy baselines, indicating that conservative value learning provides an effective mechanism for embedding failure awareness into bilateral teleoperation. Experimental videos are available at this https URL"
    },
    {
      "title": "Towards Agentic Intelligence for Materials Science",
      "url": "https://arxiv.org/abs/2602.00169",
      "date": "2026-01-29",
      "authors_text": "Huan Zhang, Yizhan Li, Wenhao Huang, Ziyu Hou, Yu Song",
      "is_highlight": false,
      "score": 12.0,
      "summary": "This paper advocates for developing agentic AI systems in materials science that autonomously plan, act, and learn to enhance discovery processes beyond traditional models.",
      "teaser_image": "https://arxiv.org/html/2602.00169/x1.png",
      "debug_abstract": "The convergence of artificial intelligence and materials science presents a transformative opportunity, but achieving true acceleration in discovery requires moving beyond task-isolated, fine-tuned models toward agentic systems that plan, act, and learn across the full discovery loop. This survey advances a unique pipeline-centric view that spans from corpus curation and pretraining, through domain adaptation and instruction tuning, to goal-conditioned agents interfacing with simulation and experimental platforms. Unlike prior reviews, we treat the entire process as an end-to-end system to be optimized for tangible discovery outcomes rather than proxy benchmarks. This perspective allows us to trace how upstream design choices-such as data curation and training objectives-can be aligned with downstream experimental success through effective credit assignment. To bridge communities and establish a shared frame of reference, we first present an integrated lens that aligns terminology, evaluation, and workflow stages across AI and materials science. We then analyze the field through two focused lenses: From the AI perspective, the survey details LLM strengths in pattern recognition, predictive analytics, and natural language processing for literature mining, materials characterization, and property prediction; from the materials science perspective, it highlights applications in materials design, process optimization, and the acceleration of computational workflows via integration with external tools (e.g., DFT, robotic labs). Finally, we contrast passive, reactive approaches with agentic design, cataloging current contributions while motivating systems that pursue long-horizon goals with autonomy, memory, and tool use. This survey charts a practical roadmap towards autonomous, safety-aware LLM agents aimed at discovering novel and useful materials."
    },
    {
      "title": "TIDE: Trajectory-based Diagnostic Evaluation of Test-Time Improvement in LLM Agents",
      "url": "https://arxiv.org/abs/2602.02196",
      "date": "2026-02-02",
      "authors_text": "Hang Yan, Xinyu Che, Fangzhi Xu, Qiushi Sun, Zichen Ding",
      "is_highlight": false,
      "score": 20.0,
      "summary": "The TIDE framework evaluates Test-Time Improvement in LLM agents by analyzing task completion dynamics, recursive behaviors, and memory constraints to enhance performance optimization.",
      "teaser_image": "https://arxiv.org/html/2602.02196/x1.png",
      "debug_abstract": "Recent advances in autonomous LLM agents demonstrate their ability to improve performance through iterative interaction with the environment. We define this paradigm as Test-Time Improvement (TTI). However, the mechanisms under how and why TTI succeed or fail remain poorly understood, and existing evaluation metrics fail to capture their task optimization efficiency, behavior adaptation after erroneous actions, and the specific utility of working memory for task completion. To address these gaps, we propose Test-time Improvement Diagnostic Evaluation (TIDE), an agent-agnostic and environment-agnostic framework that decomposes TTI into three comprehensive and interconnected dimensions. The framework measures (1) the overall temporal dynamics of task completion and (2) identifies whether performance is primarily constrained by recursive looping behaviors or (3) by burdensome accumulated memory. Through extensive experiments across diverse agents and environments, TIDE highlights that improving agent performance requires more than scaling internal reasoning, calling for explicitly optimizing the interaction dynamics between the agent and the environment."
    },
    {
      "title": "Breaking the Static Graph: Context-Aware Traversal for Robust Retrieval-Augmented Generation",
      "url": "https://arxiv.org/abs/2602.01965",
      "date": "2026-02-02",
      "authors_text": "Kwun Hang Lau, Fangyuan Zhang, Boyu Ruan, Yingli Zhou, Qintian Guo",
      "is_highlight": false,
      "score": 27.0,
      "summary": "CatRAG enhances Retrieval-Augmented Generation by transforming static Knowledge Graphs into query-adaptive structures, improving multi-hop evidence retrieval and reasoning completeness.",
      "teaser_image": "https://arxiv.org/html/2602.01965/image/HubBiasShift.png",
      "debug_abstract": "Recent advances in Retrieval-Augmented Generation (RAG) have shifted from simple vector similarity to structure-aware approaches like HippoRAG, which leverage Knowledge Graphs (KGs) and Personalized PageRank (PPR) to capture multi-hop dependencies. However, these methods suffer from a &#34;Static Graph Fallacy&#34;: they rely on fixed transition probabilities determined during indexing. This rigidity ignores the query-dependent nature of edge relevance, causing semantic drift where random walks are diverted into high-degree &#34;hub&#34; nodes before reaching critical downstream evidence. Consequently, models often achieve high partial recall but fail to retrieve the complete evidence chain required for multi-hop queries. To address this, we propose CatRAG, Context-Aware Traversal for robust RAG, a framework that builds on the HippoRAG 2 architecture and transforms the static KG into a query-adaptive navigation structure. We introduce a multi-faceted framework to steer the random walk: (1) Symbolic Anchoring, which injects weak entity constraints to regularize the random walk; (2) Query-Aware Dynamic Edge Weighting, which dynamically modulates graph structure, to prune irrelevant paths while amplifying those aligned with the query&#39;s intent; and (3) Key-Fact Passage Weight Enhancement, a cost-efficient bias that structurally anchors the random walk to likely evidence. Experiments across four multi-hop benchmarks demonstrate that CatRAG consistently outperforms state of the art baselines. Our analysis reveals that while standard Recall metrics show modest gains, CatRAG achieves substantial improvements in reasoning completeness, the capacity to recover the entire evidence path without gaps. These results reveal that our approach effectively bridges the gap between retrieving partial context and enabling fully grounded reasoning. Resources are available at this https URL."
    },
    {
      "title": "Tilt-Ropter: A Novel Hybrid Aerial and Terrestrial Vehicle with Tilt Rotors and Passive Wheels",
      "url": "https://arxiv.org/abs/2602.01700",
      "date": "2026-02-02",
      "authors_text": "Ruoyu Wang, Xuchen Liu, Zongzhou Wu, Zixuan Guo, Wendi Ding",
      "is_highlight": false,
      "score": 38.0,
      "summary": "The Tilt-Ropter is a hybrid aerial-terrestrial vehicle that utilizes tilt rotors and passive wheels for efficient locomotion, featuring advanced control systems for enhanced mobility and energy savings.",
      "teaser_image": "https://arxiv.org/html/2602.01700/x2.png",
      "debug_abstract": "In this work, we present Tilt-Ropter, a novel hybrid aerial-terrestrial vehicle (HATV) that combines tilt rotors with passive wheels to achieve energy-efficient multi-mode locomotion. Unlike existing under-actuated HATVs, the fully actuated design of Tilt-Ropter enables decoupled force and torque control, greatly enhancing its mobility and environmental adaptability. A nonlinear model predictive controller (NMPC) is developed to track reference trajectories and handle contact constraints across locomotion modes, while a dedicated control allocation module exploits actuation redundancy to achieve energy-efficient control of actuators. Additionally, to enhance robustness during ground contact, we introduce an external wrench estimation algorithm that estimates environmental interaction forces and torques in real time. The system is validated through both simulation and real-world experiments, including seamless air-ground transitions and trajectory tracking. Results show low tracking errors in both modes and highlight a 92.8% reduction in power consumption during ground locomotion, demonstrating the system&#39;s potential for long-duration missions across large-scale and energy-constrained environments."
    },
    {
      "title": "Reasoning with Autoregressive-Diffusion Collaborative Thoughts",
      "url": "https://arxiv.org/abs/2602.01608",
      "date": "2026-02-02",
      "authors_text": "Mu Yuan, Liekang Zeng, Guoliang Xing, Lan Zhang, Yunhao Liu",
      "is_highlight": false,
      "score": 41.0,
      "summary": "The paper presents Collaborative Thoughts, a framework integrating autoregressive and diffusion models for enhanced spatial reasoning and generation through iterative feedback and joint reasoning.",
      "teaser_image": "https://arxiv.org/html/2602.01608/x2.png",
      "debug_abstract": "Autoregressive and diffusion models represent two complementary generative paradigms. Autoregressive models excel at sequential planning and constraint composition, yet struggle with tasks that require explicit spatial or physical grounding. Diffusion models, in contrast, capture rich spatial structure through high-dimensional generation, but lack the stepwise logical control needed to satisfy complex, multi-stage constraints or to reliably identify and correct errors. We introduce Collaborative Thoughts, a unified collaborative framework that enables autoregressive and diffusion models to reason and generate jointly through a closed-loop interaction. In Collaborative Thoughts, autoregressive models perform structured planning and constraint management, diffusion models instantiate these constraints as intermediate visual thoughts, and a vision-based critic module evaluates whether the visual thoughts satisfy the intended structural and physical requirements. This feedback is then used to iteratively refine subsequent planning and generation steps, mitigating error propagation across modalities. Importantly, Collaborative Thoughts uses the same collaborative loop regardless of whether the task is autoregressive question answering or diffusion-based visual generation. Through representative examples, we illustrate how Collaborative Thoughts can improve the reliability of spatial reasoning and the controllability of generation."
    },
    {
      "title": "Adaptive Rollout Allocation for Online Reinforcement Learning with Verifiable Rewards",
      "url": "https://arxiv.org/abs/2602.01601",
      "date": "2026-02-02",
      "authors_text": "Hieu Trung Nguyen, Bao Nguyen, Wenao Ma, Yuzhi Zhao, Ruifeng She",
      "is_highlight": false,
      "score": 40.0,
      "summary": "The paper presents \\Ours, a variance-informed strategy for adaptive rollout allocation in online reinforcement learning, enhancing sampling efficiency and performance over traditional methods.",
      "teaser_image": "https://arxiv.org/html/2602.01601/x2.png",
      "debug_abstract": "Sampling efficiency is a key bottleneck in reinforcement learning with verifiable rewards. Existing group-based policy optimization methods, such as GRPO, allocate a fixed number of rollouts for all training prompts. This uniform allocation implicitly treats all prompts as equally informative, and could lead to inefficient computational budget usage and impede training progress. We introduce \\Ours, a Variance-Informed Predictive allocation strategy that allocates a given rollout budget to the prompts in the incumbent batch to minimize the expected gradient variance of the policy update. At each iteration, \\Ours~uses a lightweight Gaussian process model to predict per-prompt success probabilities based on recent rollouts. These probability predictions are translated into variance estimates, which are then fed into a convex optimization problem to determine the optimal rollout allocations under a hard compute budget constraint. Empirical results show that \\Ours~consistently improves sampling efficiency and achieves higher performance than uniform or heuristic allocation strategies in multiple benchmarks. Our code will be available at this https URL."
    },
    {
      "title": "MedAD-R1: Eliciting Consistent Reasoning in Interpretible Medical Anomaly Detection via Consistency-Reinforced Policy Optimization",
      "url": "https://arxiv.org/abs/2602.01081",
      "date": "2026-02-01",
      "authors_text": "Haitao Zhang, Yingying Wang, Jiaxiang Wang, Haote Xu, Hongyang Zhang",
      "is_highlight": false,
      "score": 64.0,
      "summary": "MedAD-R1 enhances medical anomaly detection by utilizing a two-stage training framework with consistency-reinforced policy optimization, achieving state-of-the-art performance on the MedAD-38K benchmark.",
      "teaser_image": "https://arxiv.org/html/2602.01081/x3.png",
      "debug_abstract": "Medical Anomaly Detection (MedAD) presents a significant opportunity to enhance diagnostic accuracy using Large Multimodal Models (LMMs) to interpret and answer questions based on medical images. However, the reliance on Supervised Fine-Tuning (SFT) on simplistic and fragmented datasets has hindered the development of models capable of plausible reasoning and robust multimodal generalization. To overcome this, we introduce MedAD-38K, the first large-scale, multi-modal, and multi-center benchmark for MedAD featuring diagnostic Chain-of-Thought (CoT) annotations alongside structured Visual Question-Answering (VQA) pairs. On this foundation, we propose a two-stage training framework. The first stage, Cognitive Injection, uses SFT to instill foundational medical knowledge and align the model with a structured think-then-answer paradigm. Given that standard policy optimization can produce reasoning that is disconnected from the final answer, the second stage incorporates Consistency Group Relative Policy Optimization (Con-GRPO). This novel algorithm incorporates a crucial consistency reward to ensure the generated reasoning process is relevant and logically coherent with the final diagnosis. Our proposed model, MedAD-R1, achieves state-of-the-art (SOTA) performance on the MedAD-38K benchmark, outperforming strong baselines by more than 10\\%. This superior performance stems from its ability to generate transparent and logically consistent reasoning pathways, offering a promising approach to enhancing the trustworthiness and interpretability of AI for clinical decision support."
    },
    {
      "title": "Residual Decoding: Mitigating Hallucinations in Large Vision-Language Models via History-Aware Residual Guidance",
      "url": "https://arxiv.org/abs/2602.01047",
      "date": "2026-02-01",
      "authors_text": "Xinrong Chen, Xu Chu, Yingmin Qiu, Hengyuan Zhang, Jing Xiong",
      "is_highlight": false,
      "score": 53.0,
      "summary": "Residual Decoding (ResDec) mitigates hallucinations in Large Vision-Language Models by leveraging historical information to enhance decoding and improve visual grounding.",
      "teaser_image": "https://arxiv.org/html/2602.01047/x1.png",
      "debug_abstract": "Large Vision-Language Models (LVLMs) can reason effectively from image-text inputs and perform well in various multimodal tasks. Despite this success, they are affected by language priors and often produce hallucinations. Hallucinations denote generated content that is grammatically and syntactically coherent, yet bears no match or direct relevance to actual visual input. To address this problem, we propose Residual Decoding (ResDec). It is a novel training-free method that uses historical information to aid decoding. The method relies on the internal implicit reasoning mechanism and token logits evolution mechanism of LVLMs to correct biases. Extensive experiments demonstrate that ResDec effectively suppresses hallucinations induced by language priors, significantly improves visual grounding, and reduces object hallucinations. In addition to mitigating hallucinations, ResDec also performs exceptionally well on comprehensive LVLM benchmarks, highlighting its broad applicability."
    },
    {
      "title": "Offline Discovery of Interpretable Skills from Multi-Task Trajectories",
      "url": "https://arxiv.org/abs/2602.01018",
      "date": "2026-02-01",
      "authors_text": "Chongyu Zhu, Mithun Vanniasinghe, Jiayu Chen, Chi-Guhn Lee",
      "is_highlight": false,
      "score": 77.0,
      "summary": "LOKI is a three-stage framework for offline skill discovery and hierarchical imitation learning, effectively extracting interpretable skills from multi-task trajectories without explicit rewards.",
      "teaser_image": "https://arxiv.org/html/2602.01018/fig/two_stage_skill_learning.png",
      "debug_abstract": "Hierarchical Imitation Learning is a powerful paradigm for acquiring complex robot behaviors from demonstrations. A central challenge, however, lies in discovering reusable skills from long-horizon, multi-task offline data, especially when the data lacks explicit rewards or subtask annotations. In this work, we introduce LOKI, a three-stage end-to-end learning framework designed for offline skill discovery and hierarchical imitation. The framework commences with a two-stage, weakly supervised skill discovery process: Stage one performs coarse, task-aware macro-segmentation by employing an alignment-enforced Vector Quantized VAE guided by weak task labels. Stage two then refines these segments at a micro-level using a self-supervised sequential model, followed by an iterative clustering process to consolidate skill boundaries. The third stage then leverages these precise boundaries to construct a hierarchical policy within an option-based framework-complete with a learned termination condition beta for explicit skill switching. LOKI achieves high success rates on the challenging D4RL Kitchen benchmark and outperforms standard HIL baselines. Furthermore, we demonstrate that the discovered skills are semantically meaningful, aligning with human intuition, and exhibit compositionality by successfully sequencing them to solve a novel, unseen task."
    },
    {
      "title": "Dual Quaternion SE(3) Synchronization with Recovery Guarantees",
      "url": "https://arxiv.org/abs/2602.00324",
      "date": "2026-01-30",
      "authors_text": "Jianing Zhao, Linglingzhi Zhu, Anthony Man-Cho So",
      "is_highlight": false,
      "score": 75.0,
      "summary": "This paper presents a dual quaternion-based SE(3) synchronization method with a two-stage algorithm that guarantees recovery of absolute poses from noisy transformations, outperforming traditional matrix methods.",
      "teaser_image": "https://arxiv.org/html/2602.00324/x2.png",
      "debug_abstract": "Synchronization over the special Euclidean group SE(3) aims to recover absolute poses from noisy pairwise relative transformations and is a core primitive in robotics and 3D vision. Standard approaches often require multi-step heuristic procedures to recover valid poses, which are difficult to analyze and typically lack theoretical guarantees. This paper adopts a dual quaternion representation and formulates SE(3) synchronization directly over the unit dual quaternion. A two-stage algorithm is developed: A spectral initializer computed via the power method on a Hermitian dual quaternion measurement matrix, followed by a dual quaternion generalized power method (DQGPM) that enforces feasibility through per-iteration projection. The estimation error bounds are established for spectral estimators, and DQGPM is shown to admit a finite-iteration error bound and achieves linear error contraction up to an explicit noise-dependent threshold. Experiments on synthetic benchmarks and real-world multi-scan point-set registration demonstrate that the proposed pipeline improves both accuracy and efficiency over representative matrix-based methods."
    }
  ],
  "机器人与人工智能实验室 (RAIL)": [
    {
      "title": "FlowSteer: Interactive Agentic Workflow Orchestration via End-to-End Reinforcement Learning",
      "url": "https://arxiv.org/abs/2602.01664",
      "date": "2026-02-02",
      "authors_text": "Mingda Zhang, Haoran Luo, Tiesunlong Shen, Qika Lin, Xiaoying Tang",
      "is_highlight": false,
      "score": 7.0,
      "summary": "FlowSteer is an end-to-end reinforcement learning framework that automates workflow orchestration through interactive learning, addressing manual costs and improving performance across diverse tasks.",
      "teaser_image": "https://arxiv.org/html/2602.01664/x1.png",
      "debug_abstract": "In recent years, a variety of powerful agentic workflows have been applied to solve a wide range of human problems. However, existing workflow orchestration still faces key challenges, including high manual cost, reliance on specific operators/large language models (LLMs), and sparse reward signals. To address these challenges, we propose FlowSteer, an end-to-end reinforcement learning framework that takes a lightweight policy model as the agent and an executable canvas environment, automating workflow orchestration through multi-turn interaction. In this process, the policy model analyzes execution states and selects editing actions, while the canvas executes operators and returns feedback for iterative refinement. Moreover, FlowSteer provides a plug-and-play framework that supports diverse operator libraries and interchangeable LLM backends. To effectively train this interaction paradigm, we propose Canvas Workflow Relative Policy Optimization (CWRPO), which introduces diversity-constrained rewards with conditional release to stabilize learning and suppress shortcut behaviors. Experimental results on twelve datasets show that FlowSteer significantly outperforms baselines across various tasks."
    },
    {
      "title": "Towards Agentic Intelligence for Materials Science",
      "url": "https://arxiv.org/abs/2602.00169",
      "date": "2026-01-29",
      "authors_text": "Huan Zhang, Yizhan Li, Wenhao Huang, Ziyu Hou, Yu Song",
      "is_highlight": false,
      "score": 12.0,
      "summary": "This paper advocates for developing agentic AI systems in materials science that autonomously plan, act, and learn to enhance discovery processes beyond traditional models.",
      "teaser_image": "https://arxiv.org/html/2602.00169/x1.png",
      "debug_abstract": "The convergence of artificial intelligence and materials science presents a transformative opportunity, but achieving true acceleration in discovery requires moving beyond task-isolated, fine-tuned models toward agentic systems that plan, act, and learn across the full discovery loop. This survey advances a unique pipeline-centric view that spans from corpus curation and pretraining, through domain adaptation and instruction tuning, to goal-conditioned agents interfacing with simulation and experimental platforms. Unlike prior reviews, we treat the entire process as an end-to-end system to be optimized for tangible discovery outcomes rather than proxy benchmarks. This perspective allows us to trace how upstream design choices-such as data curation and training objectives-can be aligned with downstream experimental success through effective credit assignment. To bridge communities and establish a shared frame of reference, we first present an integrated lens that aligns terminology, evaluation, and workflow stages across AI and materials science. We then analyze the field through two focused lenses: From the AI perspective, the survey details LLM strengths in pattern recognition, predictive analytics, and natural language processing for literature mining, materials characterization, and property prediction; from the materials science perspective, it highlights applications in materials design, process optimization, and the acceleration of computational workflows via integration with external tools (e.g., DFT, robotic labs). Finally, we contrast passive, reactive approaches with agentic design, cataloging current contributions while motivating systems that pursue long-horizon goals with autonomy, memory, and tool use. This survey charts a practical roadmap towards autonomous, safety-aware LLM agents aimed at discovering novel and useful materials."
    },
    {
      "title": "Breaking the Static Graph: Context-Aware Traversal for Robust Retrieval-Augmented Generation",
      "url": "https://arxiv.org/abs/2602.01965",
      "date": "2026-02-02",
      "authors_text": "Kwun Hang Lau, Fangyuan Zhang, Boyu Ruan, Yingli Zhou, Qintian Guo",
      "is_highlight": false,
      "score": 27.0,
      "summary": "CatRAG enhances Retrieval-Augmented Generation by transforming static Knowledge Graphs into query-adaptive structures, improving multi-hop evidence retrieval and reasoning completeness.",
      "teaser_image": "https://arxiv.org/html/2602.01965/image/HubBiasShift.png",
      "debug_abstract": "Recent advances in Retrieval-Augmented Generation (RAG) have shifted from simple vector similarity to structure-aware approaches like HippoRAG, which leverage Knowledge Graphs (KGs) and Personalized PageRank (PPR) to capture multi-hop dependencies. However, these methods suffer from a &#34;Static Graph Fallacy&#34;: they rely on fixed transition probabilities determined during indexing. This rigidity ignores the query-dependent nature of edge relevance, causing semantic drift where random walks are diverted into high-degree &#34;hub&#34; nodes before reaching critical downstream evidence. Consequently, models often achieve high partial recall but fail to retrieve the complete evidence chain required for multi-hop queries. To address this, we propose CatRAG, Context-Aware Traversal for robust RAG, a framework that builds on the HippoRAG 2 architecture and transforms the static KG into a query-adaptive navigation structure. We introduce a multi-faceted framework to steer the random walk: (1) Symbolic Anchoring, which injects weak entity constraints to regularize the random walk; (2) Query-Aware Dynamic Edge Weighting, which dynamically modulates graph structure, to prune irrelevant paths while amplifying those aligned with the query&#39;s intent; and (3) Key-Fact Passage Weight Enhancement, a cost-efficient bias that structurally anchors the random walk to likely evidence. Experiments across four multi-hop benchmarks demonstrate that CatRAG consistently outperforms state of the art baselines. Our analysis reveals that while standard Recall metrics show modest gains, CatRAG achieves substantial improvements in reasoning completeness, the capacity to recover the entire evidence path without gaps. These results reveal that our approach effectively bridges the gap between retrieving partial context and enabling fully grounded reasoning. Resources are available at this https URL."
    },
    {
      "title": "Tilt-Ropter: A Novel Hybrid Aerial and Terrestrial Vehicle with Tilt Rotors and Passive Wheels",
      "url": "https://arxiv.org/abs/2602.01700",
      "date": "2026-02-02",
      "authors_text": "Ruoyu Wang, Xuchen Liu, Zongzhou Wu, Zixuan Guo, Wendi Ding",
      "is_highlight": false,
      "score": 38.0,
      "summary": "The Tilt-Ropter is a hybrid aerial-terrestrial vehicle that utilizes tilt rotors and passive wheels for efficient locomotion, featuring advanced control systems for enhanced mobility and energy savings.",
      "teaser_image": "https://arxiv.org/html/2602.01700/x2.png",
      "debug_abstract": "In this work, we present Tilt-Ropter, a novel hybrid aerial-terrestrial vehicle (HATV) that combines tilt rotors with passive wheels to achieve energy-efficient multi-mode locomotion. Unlike existing under-actuated HATVs, the fully actuated design of Tilt-Ropter enables decoupled force and torque control, greatly enhancing its mobility and environmental adaptability. A nonlinear model predictive controller (NMPC) is developed to track reference trajectories and handle contact constraints across locomotion modes, while a dedicated control allocation module exploits actuation redundancy to achieve energy-efficient control of actuators. Additionally, to enhance robustness during ground contact, we introduce an external wrench estimation algorithm that estimates environmental interaction forces and torques in real time. The system is validated through both simulation and real-world experiments, including seamless air-ground transitions and trajectory tracking. Results show low tracking errors in both modes and highlight a 92.8% reduction in power consumption during ground locomotion, demonstrating the system&#39;s potential for long-duration missions across large-scale and energy-constrained environments."
    },
    {
      "title": "Reasoning with Autoregressive-Diffusion Collaborative Thoughts",
      "url": "https://arxiv.org/abs/2602.01608",
      "date": "2026-02-02",
      "authors_text": "Mu Yuan, Liekang Zeng, Guoliang Xing, Lan Zhang, Yunhao Liu",
      "is_highlight": false,
      "score": 41.0,
      "summary": "The paper presents Collaborative Thoughts, a framework integrating autoregressive and diffusion models for enhanced spatial reasoning and generation through iterative feedback and joint reasoning.",
      "teaser_image": "https://arxiv.org/html/2602.01608/x2.png",
      "debug_abstract": "Autoregressive and diffusion models represent two complementary generative paradigms. Autoregressive models excel at sequential planning and constraint composition, yet struggle with tasks that require explicit spatial or physical grounding. Diffusion models, in contrast, capture rich spatial structure through high-dimensional generation, but lack the stepwise logical control needed to satisfy complex, multi-stage constraints or to reliably identify and correct errors. We introduce Collaborative Thoughts, a unified collaborative framework that enables autoregressive and diffusion models to reason and generate jointly through a closed-loop interaction. In Collaborative Thoughts, autoregressive models perform structured planning and constraint management, diffusion models instantiate these constraints as intermediate visual thoughts, and a vision-based critic module evaluates whether the visual thoughts satisfy the intended structural and physical requirements. This feedback is then used to iteratively refine subsequent planning and generation steps, mitigating error propagation across modalities. Importantly, Collaborative Thoughts uses the same collaborative loop regardless of whether the task is autoregressive question answering or diffusion-based visual generation. Through representative examples, we illustrate how Collaborative Thoughts can improve the reliability of spatial reasoning and the controllability of generation."
    },
    {
      "title": "Adaptive Rollout Allocation for Online Reinforcement Learning with Verifiable Rewards",
      "url": "https://arxiv.org/abs/2602.01601",
      "date": "2026-02-02",
      "authors_text": "Hieu Trung Nguyen, Bao Nguyen, Wenao Ma, Yuzhi Zhao, Ruifeng She",
      "is_highlight": false,
      "score": 40.0,
      "summary": "The paper presents \\Ours, a variance-informed strategy for adaptive rollout allocation in online reinforcement learning, enhancing sampling efficiency and performance over traditional methods.",
      "teaser_image": "https://arxiv.org/html/2602.01601/x2.png",
      "debug_abstract": "Sampling efficiency is a key bottleneck in reinforcement learning with verifiable rewards. Existing group-based policy optimization methods, such as GRPO, allocate a fixed number of rollouts for all training prompts. This uniform allocation implicitly treats all prompts as equally informative, and could lead to inefficient computational budget usage and impede training progress. We introduce \\Ours, a Variance-Informed Predictive allocation strategy that allocates a given rollout budget to the prompts in the incumbent batch to minimize the expected gradient variance of the policy update. At each iteration, \\Ours~uses a lightweight Gaussian process model to predict per-prompt success probabilities based on recent rollouts. These probability predictions are translated into variance estimates, which are then fed into a convex optimization problem to determine the optimal rollout allocations under a hard compute budget constraint. Empirical results show that \\Ours~consistently improves sampling efficiency and achieves higher performance than uniform or heuristic allocation strategies in multiple benchmarks. Our code will be available at this https URL."
    },
    {
      "title": "MedAD-R1: Eliciting Consistent Reasoning in Interpretible Medical Anomaly Detection via Consistency-Reinforced Policy Optimization",
      "url": "https://arxiv.org/abs/2602.01081",
      "date": "2026-02-01",
      "authors_text": "Haitao Zhang, Yingying Wang, Jiaxiang Wang, Haote Xu, Hongyang Zhang",
      "is_highlight": false,
      "score": 64.0,
      "summary": "MedAD-R1 enhances medical anomaly detection by utilizing a two-stage training framework with consistency-reinforced policy optimization, achieving state-of-the-art performance on the MedAD-38K benchmark.",
      "teaser_image": "https://arxiv.org/html/2602.01081/x3.png",
      "debug_abstract": "Medical Anomaly Detection (MedAD) presents a significant opportunity to enhance diagnostic accuracy using Large Multimodal Models (LMMs) to interpret and answer questions based on medical images. However, the reliance on Supervised Fine-Tuning (SFT) on simplistic and fragmented datasets has hindered the development of models capable of plausible reasoning and robust multimodal generalization. To overcome this, we introduce MedAD-38K, the first large-scale, multi-modal, and multi-center benchmark for MedAD featuring diagnostic Chain-of-Thought (CoT) annotations alongside structured Visual Question-Answering (VQA) pairs. On this foundation, we propose a two-stage training framework. The first stage, Cognitive Injection, uses SFT to instill foundational medical knowledge and align the model with a structured think-then-answer paradigm. Given that standard policy optimization can produce reasoning that is disconnected from the final answer, the second stage incorporates Consistency Group Relative Policy Optimization (Con-GRPO). This novel algorithm incorporates a crucial consistency reward to ensure the generated reasoning process is relevant and logically coherent with the final diagnosis. Our proposed model, MedAD-R1, achieves state-of-the-art (SOTA) performance on the MedAD-38K benchmark, outperforming strong baselines by more than 10\\%. This superior performance stems from its ability to generate transparent and logically consistent reasoning pathways, offering a promising approach to enhancing the trustworthiness and interpretability of AI for clinical decision support."
    },
    {
      "title": "Dual Quaternion SE(3) Synchronization with Recovery Guarantees",
      "url": "https://arxiv.org/abs/2602.00324",
      "date": "2026-01-30",
      "authors_text": "Jianing Zhao, Linglingzhi Zhu, Anthony Man-Cho So",
      "is_highlight": false,
      "score": 75.0,
      "summary": "This paper presents a dual quaternion-based SE(3) synchronization method with a two-stage algorithm that guarantees recovery of absolute poses from noisy transformations, outperforming traditional matrix methods.",
      "teaser_image": "https://arxiv.org/html/2602.00324/x2.png",
      "debug_abstract": "Synchronization over the special Euclidean group SE(3) aims to recover absolute poses from noisy pairwise relative transformations and is a core primitive in robotics and 3D vision. Standard approaches often require multi-step heuristic procedures to recover valid poses, which are difficult to analyze and typically lack theoretical guarantees. This paper adopts a dual quaternion representation and formulates SE(3) synchronization directly over the unit dual quaternion. A two-stage algorithm is developed: A spectral initializer computed via the power method on a Hermitian dual quaternion measurement matrix, followed by a dual quaternion generalized power method (DQGPM) that enforces feasibility through per-iteration projection. The estimation error bounds are established for spectral estimators, and DQGPM is shown to admit a finite-iteration error bound and achieves linear error contraction up to an explicit noise-dependent threshold. Experiments on synthetic benchmarks and real-world multi-scan point-set registration demonstrate that the proposed pipeline improves both accuracy and efficiency over representative matrix-based methods."
    }
  ],
  "潘佳老师实验室": [
    {
      "title": "FlowSteer: Interactive Agentic Workflow Orchestration via End-to-End Reinforcement Learning",
      "url": "https://arxiv.org/abs/2602.01664",
      "date": "2026-02-02",
      "authors_text": "Mingda Zhang, Haoran Luo, Tiesunlong Shen, Qika Lin, Xiaoying Tang",
      "is_highlight": false,
      "score": 7.0,
      "summary": "FlowSteer is an end-to-end reinforcement learning framework that automates workflow orchestration through interactive learning, addressing manual costs and improving performance across diverse tasks.",
      "teaser_image": "https://arxiv.org/html/2602.01664/x1.png",
      "debug_abstract": "In recent years, a variety of powerful agentic workflows have been applied to solve a wide range of human problems. However, existing workflow orchestration still faces key challenges, including high manual cost, reliance on specific operators/large language models (LLMs), and sparse reward signals. To address these challenges, we propose FlowSteer, an end-to-end reinforcement learning framework that takes a lightweight policy model as the agent and an executable canvas environment, automating workflow orchestration through multi-turn interaction. In this process, the policy model analyzes execution states and selects editing actions, while the canvas executes operators and returns feedback for iterative refinement. Moreover, FlowSteer provides a plug-and-play framework that supports diverse operator libraries and interchangeable LLM backends. To effectively train this interaction paradigm, we propose Canvas Workflow Relative Policy Optimization (CWRPO), which introduces diversity-constrained rewards with conditional release to stabilize learning and suppress shortcut behaviors. Experimental results on twelve datasets show that FlowSteer significantly outperforms baselines across various tasks."
    },
    {
      "title": "Failure-Aware Bimanual Teleoperation via Conservative Value Guided Assistance",
      "url": "https://arxiv.org/abs/2602.01092",
      "date": "2026-02-01",
      "authors_text": "Peng Zhou, Zhongxuan Li, Jinsong Wu, Jiaming Qi, Jun Hu",
      "is_highlight": false,
      "score": 9.0,
      "summary": "This paper presents a failure-aware bimanual teleoperation framework that uses conservative value learning to enhance task success and reduce operator workload through compliant haptic assistance.",
      "teaser_image": "https://arxiv.org/html/2602.01092/x1.png",
      "debug_abstract": "Teleoperation of high-precision manipulation is con-strained by tight success tolerances and complex contact dy-namics, which make impending failures difficult for human operators to anticipate under partial observability. This paper proposes a value-guided, failure-aware framework for bimanual teleoperation that provides compliant haptic assistance while pre-serving continuous human authority. The framework is trained entirely from heterogeneous offline teleoperation data containing both successful and failed executions. Task feasibility is mod-eled as a conservative success score learned via Conservative Value Learning, yielding a risk-sensitive estimate that remains reliable under distribution shift. During online operation, the learned success score regulates the level of assistance, while a learned actor provides a corrective motion direction. Both are integrated through a joint-space impedance interface on the master side, yielding continuous guidance that steers the operator away from failure-prone actions without overriding intent. Experimental results on contact-rich manipulation tasks demonstrate improved task success rates and reduced operator workload compared to conventional teleoperation and shared-autonomy baselines, indicating that conservative value learning provides an effective mechanism for embedding failure awareness into bilateral teleoperation. Experimental videos are available at this https URL"
    },
    {
      "title": "Towards Agentic Intelligence for Materials Science",
      "url": "https://arxiv.org/abs/2602.00169",
      "date": "2026-01-29",
      "authors_text": "Huan Zhang, Yizhan Li, Wenhao Huang, Ziyu Hou, Yu Song",
      "is_highlight": false,
      "score": 12.0,
      "summary": "This paper advocates for developing agentic AI systems in materials science that autonomously plan, act, and learn to enhance discovery processes beyond traditional models.",
      "teaser_image": "https://arxiv.org/html/2602.00169/x1.png",
      "debug_abstract": "The convergence of artificial intelligence and materials science presents a transformative opportunity, but achieving true acceleration in discovery requires moving beyond task-isolated, fine-tuned models toward agentic systems that plan, act, and learn across the full discovery loop. This survey advances a unique pipeline-centric view that spans from corpus curation and pretraining, through domain adaptation and instruction tuning, to goal-conditioned agents interfacing with simulation and experimental platforms. Unlike prior reviews, we treat the entire process as an end-to-end system to be optimized for tangible discovery outcomes rather than proxy benchmarks. This perspective allows us to trace how upstream design choices-such as data curation and training objectives-can be aligned with downstream experimental success through effective credit assignment. To bridge communities and establish a shared frame of reference, we first present an integrated lens that aligns terminology, evaluation, and workflow stages across AI and materials science. We then analyze the field through two focused lenses: From the AI perspective, the survey details LLM strengths in pattern recognition, predictive analytics, and natural language processing for literature mining, materials characterization, and property prediction; from the materials science perspective, it highlights applications in materials design, process optimization, and the acceleration of computational workflows via integration with external tools (e.g., DFT, robotic labs). Finally, we contrast passive, reactive approaches with agentic design, cataloging current contributions while motivating systems that pursue long-horizon goals with autonomy, memory, and tool use. This survey charts a practical roadmap towards autonomous, safety-aware LLM agents aimed at discovering novel and useful materials."
    },
    {
      "title": "TIDE: Trajectory-based Diagnostic Evaluation of Test-Time Improvement in LLM Agents",
      "url": "https://arxiv.org/abs/2602.02196",
      "date": "2026-02-02",
      "authors_text": "Hang Yan, Xinyu Che, Fangzhi Xu, Qiushi Sun, Zichen Ding",
      "is_highlight": false,
      "score": 20.0,
      "summary": "The TIDE framework evaluates Test-Time Improvement in LLM agents by analyzing task completion dynamics, recursive behaviors, and memory constraints to enhance performance optimization.",
      "teaser_image": "https://arxiv.org/html/2602.02196/x1.png",
      "debug_abstract": "Recent advances in autonomous LLM agents demonstrate their ability to improve performance through iterative interaction with the environment. We define this paradigm as Test-Time Improvement (TTI). However, the mechanisms under how and why TTI succeed or fail remain poorly understood, and existing evaluation metrics fail to capture their task optimization efficiency, behavior adaptation after erroneous actions, and the specific utility of working memory for task completion. To address these gaps, we propose Test-time Improvement Diagnostic Evaluation (TIDE), an agent-agnostic and environment-agnostic framework that decomposes TTI into three comprehensive and interconnected dimensions. The framework measures (1) the overall temporal dynamics of task completion and (2) identifies whether performance is primarily constrained by recursive looping behaviors or (3) by burdensome accumulated memory. Through extensive experiments across diverse agents and environments, TIDE highlights that improving agent performance requires more than scaling internal reasoning, calling for explicitly optimizing the interaction dynamics between the agent and the environment."
    },
    {
      "title": "Breaking the Static Graph: Context-Aware Traversal for Robust Retrieval-Augmented Generation",
      "url": "https://arxiv.org/abs/2602.01965",
      "date": "2026-02-02",
      "authors_text": "Kwun Hang Lau, Fangyuan Zhang, Boyu Ruan, Yingli Zhou, Qintian Guo",
      "is_highlight": false,
      "score": 27.0,
      "summary": "CatRAG enhances Retrieval-Augmented Generation by transforming static Knowledge Graphs into query-adaptive structures, improving multi-hop evidence retrieval and reasoning completeness.",
      "teaser_image": "https://arxiv.org/html/2602.01965/image/HubBiasShift.png",
      "debug_abstract": "Recent advances in Retrieval-Augmented Generation (RAG) have shifted from simple vector similarity to structure-aware approaches like HippoRAG, which leverage Knowledge Graphs (KGs) and Personalized PageRank (PPR) to capture multi-hop dependencies. However, these methods suffer from a &#34;Static Graph Fallacy&#34;: they rely on fixed transition probabilities determined during indexing. This rigidity ignores the query-dependent nature of edge relevance, causing semantic drift where random walks are diverted into high-degree &#34;hub&#34; nodes before reaching critical downstream evidence. Consequently, models often achieve high partial recall but fail to retrieve the complete evidence chain required for multi-hop queries. To address this, we propose CatRAG, Context-Aware Traversal for robust RAG, a framework that builds on the HippoRAG 2 architecture and transforms the static KG into a query-adaptive navigation structure. We introduce a multi-faceted framework to steer the random walk: (1) Symbolic Anchoring, which injects weak entity constraints to regularize the random walk; (2) Query-Aware Dynamic Edge Weighting, which dynamically modulates graph structure, to prune irrelevant paths while amplifying those aligned with the query&#39;s intent; and (3) Key-Fact Passage Weight Enhancement, a cost-efficient bias that structurally anchors the random walk to likely evidence. Experiments across four multi-hop benchmarks demonstrate that CatRAG consistently outperforms state of the art baselines. Our analysis reveals that while standard Recall metrics show modest gains, CatRAG achieves substantial improvements in reasoning completeness, the capacity to recover the entire evidence path without gaps. These results reveal that our approach effectively bridges the gap between retrieving partial context and enabling fully grounded reasoning. Resources are available at this https URL."
    },
    {
      "title": "Tilt-Ropter: A Novel Hybrid Aerial and Terrestrial Vehicle with Tilt Rotors and Passive Wheels",
      "url": "https://arxiv.org/abs/2602.01700",
      "date": "2026-02-02",
      "authors_text": "Ruoyu Wang, Xuchen Liu, Zongzhou Wu, Zixuan Guo, Wendi Ding",
      "is_highlight": false,
      "score": 38.0,
      "summary": "The Tilt-Ropter is a hybrid aerial-terrestrial vehicle that utilizes tilt rotors and passive wheels for efficient locomotion, featuring advanced control systems for enhanced mobility and energy savings.",
      "teaser_image": "https://arxiv.org/html/2602.01700/x2.png",
      "debug_abstract": "In this work, we present Tilt-Ropter, a novel hybrid aerial-terrestrial vehicle (HATV) that combines tilt rotors with passive wheels to achieve energy-efficient multi-mode locomotion. Unlike existing under-actuated HATVs, the fully actuated design of Tilt-Ropter enables decoupled force and torque control, greatly enhancing its mobility and environmental adaptability. A nonlinear model predictive controller (NMPC) is developed to track reference trajectories and handle contact constraints across locomotion modes, while a dedicated control allocation module exploits actuation redundancy to achieve energy-efficient control of actuators. Additionally, to enhance robustness during ground contact, we introduce an external wrench estimation algorithm that estimates environmental interaction forces and torques in real time. The system is validated through both simulation and real-world experiments, including seamless air-ground transitions and trajectory tracking. Results show low tracking errors in both modes and highlight a 92.8% reduction in power consumption during ground locomotion, demonstrating the system&#39;s potential for long-duration missions across large-scale and energy-constrained environments."
    },
    {
      "title": "Reasoning with Autoregressive-Diffusion Collaborative Thoughts",
      "url": "https://arxiv.org/abs/2602.01608",
      "date": "2026-02-02",
      "authors_text": "Mu Yuan, Liekang Zeng, Guoliang Xing, Lan Zhang, Yunhao Liu",
      "is_highlight": false,
      "score": 41.0,
      "summary": "The paper presents Collaborative Thoughts, a framework integrating autoregressive and diffusion models for enhanced spatial reasoning and generation through iterative feedback and joint reasoning.",
      "teaser_image": "https://arxiv.org/html/2602.01608/x2.png",
      "debug_abstract": "Autoregressive and diffusion models represent two complementary generative paradigms. Autoregressive models excel at sequential planning and constraint composition, yet struggle with tasks that require explicit spatial or physical grounding. Diffusion models, in contrast, capture rich spatial structure through high-dimensional generation, but lack the stepwise logical control needed to satisfy complex, multi-stage constraints or to reliably identify and correct errors. We introduce Collaborative Thoughts, a unified collaborative framework that enables autoregressive and diffusion models to reason and generate jointly through a closed-loop interaction. In Collaborative Thoughts, autoregressive models perform structured planning and constraint management, diffusion models instantiate these constraints as intermediate visual thoughts, and a vision-based critic module evaluates whether the visual thoughts satisfy the intended structural and physical requirements. This feedback is then used to iteratively refine subsequent planning and generation steps, mitigating error propagation across modalities. Importantly, Collaborative Thoughts uses the same collaborative loop regardless of whether the task is autoregressive question answering or diffusion-based visual generation. Through representative examples, we illustrate how Collaborative Thoughts can improve the reliability of spatial reasoning and the controllability of generation."
    },
    {
      "title": "Adaptive Rollout Allocation for Online Reinforcement Learning with Verifiable Rewards",
      "url": "https://arxiv.org/abs/2602.01601",
      "date": "2026-02-02",
      "authors_text": "Hieu Trung Nguyen, Bao Nguyen, Wenao Ma, Yuzhi Zhao, Ruifeng She",
      "is_highlight": false,
      "score": 40.0,
      "summary": "The paper presents \\Ours, a variance-informed strategy for adaptive rollout allocation in online reinforcement learning, enhancing sampling efficiency and performance over traditional methods.",
      "teaser_image": "https://arxiv.org/html/2602.01601/x2.png",
      "debug_abstract": "Sampling efficiency is a key bottleneck in reinforcement learning with verifiable rewards. Existing group-based policy optimization methods, such as GRPO, allocate a fixed number of rollouts for all training prompts. This uniform allocation implicitly treats all prompts as equally informative, and could lead to inefficient computational budget usage and impede training progress. We introduce \\Ours, a Variance-Informed Predictive allocation strategy that allocates a given rollout budget to the prompts in the incumbent batch to minimize the expected gradient variance of the policy update. At each iteration, \\Ours~uses a lightweight Gaussian process model to predict per-prompt success probabilities based on recent rollouts. These probability predictions are translated into variance estimates, which are then fed into a convex optimization problem to determine the optimal rollout allocations under a hard compute budget constraint. Empirical results show that \\Ours~consistently improves sampling efficiency and achieves higher performance than uniform or heuristic allocation strategies in multiple benchmarks. Our code will be available at this https URL."
    },
    {
      "title": "MedAD-R1: Eliciting Consistent Reasoning in Interpretible Medical Anomaly Detection via Consistency-Reinforced Policy Optimization",
      "url": "https://arxiv.org/abs/2602.01081",
      "date": "2026-02-01",
      "authors_text": "Haitao Zhang, Yingying Wang, Jiaxiang Wang, Haote Xu, Hongyang Zhang",
      "is_highlight": false,
      "score": 64.0,
      "summary": "MedAD-R1 enhances medical anomaly detection by utilizing a two-stage training framework with consistency-reinforced policy optimization, achieving state-of-the-art performance on the MedAD-38K benchmark.",
      "teaser_image": "https://arxiv.org/html/2602.01081/x3.png",
      "debug_abstract": "Medical Anomaly Detection (MedAD) presents a significant opportunity to enhance diagnostic accuracy using Large Multimodal Models (LMMs) to interpret and answer questions based on medical images. However, the reliance on Supervised Fine-Tuning (SFT) on simplistic and fragmented datasets has hindered the development of models capable of plausible reasoning and robust multimodal generalization. To overcome this, we introduce MedAD-38K, the first large-scale, multi-modal, and multi-center benchmark for MedAD featuring diagnostic Chain-of-Thought (CoT) annotations alongside structured Visual Question-Answering (VQA) pairs. On this foundation, we propose a two-stage training framework. The first stage, Cognitive Injection, uses SFT to instill foundational medical knowledge and align the model with a structured think-then-answer paradigm. Given that standard policy optimization can produce reasoning that is disconnected from the final answer, the second stage incorporates Consistency Group Relative Policy Optimization (Con-GRPO). This novel algorithm incorporates a crucial consistency reward to ensure the generated reasoning process is relevant and logically coherent with the final diagnosis. Our proposed model, MedAD-R1, achieves state-of-the-art (SOTA) performance on the MedAD-38K benchmark, outperforming strong baselines by more than 10\\%. This superior performance stems from its ability to generate transparent and logically consistent reasoning pathways, offering a promising approach to enhancing the trustworthiness and interpretability of AI for clinical decision support."
    },
    {
      "title": "Residual Decoding: Mitigating Hallucinations in Large Vision-Language Models via History-Aware Residual Guidance",
      "url": "https://arxiv.org/abs/2602.01047",
      "date": "2026-02-01",
      "authors_text": "Xinrong Chen, Xu Chu, Yingmin Qiu, Hengyuan Zhang, Jing Xiong",
      "is_highlight": false,
      "score": 53.0,
      "summary": "Residual Decoding (ResDec) mitigates hallucinations in Large Vision-Language Models by leveraging historical information to enhance decoding and improve visual grounding.",
      "teaser_image": "https://arxiv.org/html/2602.01047/x1.png",
      "debug_abstract": "Large Vision-Language Models (LVLMs) can reason effectively from image-text inputs and perform well in various multimodal tasks. Despite this success, they are affected by language priors and often produce hallucinations. Hallucinations denote generated content that is grammatically and syntactically coherent, yet bears no match or direct relevance to actual visual input. To address this problem, we propose Residual Decoding (ResDec). It is a novel training-free method that uses historical information to aid decoding. The method relies on the internal implicit reasoning mechanism and token logits evolution mechanism of LVLMs to correct biases. Extensive experiments demonstrate that ResDec effectively suppresses hallucinations induced by language priors, significantly improves visual grounding, and reduces object hallucinations. In addition to mitigating hallucinations, ResDec also performs exceptionally well on comprehensive LVLM benchmarks, highlighting its broad applicability."
    },
    {
      "title": "Offline Discovery of Interpretable Skills from Multi-Task Trajectories",
      "url": "https://arxiv.org/abs/2602.01018",
      "date": "2026-02-01",
      "authors_text": "Chongyu Zhu, Mithun Vanniasinghe, Jiayu Chen, Chi-Guhn Lee",
      "is_highlight": false,
      "score": 77.0,
      "summary": "LOKI is a three-stage framework for offline skill discovery and hierarchical imitation learning, effectively extracting interpretable skills from multi-task trajectories without explicit rewards.",
      "teaser_image": "https://arxiv.org/html/2602.01018/fig/two_stage_skill_learning.png",
      "debug_abstract": "Hierarchical Imitation Learning is a powerful paradigm for acquiring complex robot behaviors from demonstrations. A central challenge, however, lies in discovering reusable skills from long-horizon, multi-task offline data, especially when the data lacks explicit rewards or subtask annotations. In this work, we introduce LOKI, a three-stage end-to-end learning framework designed for offline skill discovery and hierarchical imitation. The framework commences with a two-stage, weakly supervised skill discovery process: Stage one performs coarse, task-aware macro-segmentation by employing an alignment-enforced Vector Quantized VAE guided by weak task labels. Stage two then refines these segments at a micro-level using a self-supervised sequential model, followed by an iterative clustering process to consolidate skill boundaries. The third stage then leverages these precise boundaries to construct a hierarchical policy within an option-based framework-complete with a learned termination condition beta for explicit skill switching. LOKI achieves high success rates on the challenging D4RL Kitchen benchmark and outperforms standard HIL baselines. Furthermore, we demonstrate that the discovered skills are semantically meaningful, aligning with human intuition, and exhibit compositionality by successfully sequencing them to solve a novel, unseen task."
    },
    {
      "title": "Dual Quaternion SE(3) Synchronization with Recovery Guarantees",
      "url": "https://arxiv.org/abs/2602.00324",
      "date": "2026-01-30",
      "authors_text": "Jianing Zhao, Linglingzhi Zhu, Anthony Man-Cho So",
      "is_highlight": false,
      "score": 75.0,
      "summary": "This paper presents a dual quaternion-based SE(3) synchronization method with a two-stage algorithm that guarantees recovery of absolute poses from noisy transformations, outperforming traditional matrix methods.",
      "teaser_image": "https://arxiv.org/html/2602.00324/x2.png",
      "debug_abstract": "Synchronization over the special Euclidean group SE(3) aims to recover absolute poses from noisy pairwise relative transformations and is a core primitive in robotics and 3D vision. Standard approaches often require multi-step heuristic procedures to recover valid poses, which are difficult to analyze and typically lack theoretical guarantees. This paper adopts a dual quaternion representation and formulates SE(3) synchronization directly over the unit dual quaternion. A two-stage algorithm is developed: A spectral initializer computed via the power method on a Hermitian dual quaternion measurement matrix, followed by a dual quaternion generalized power method (DQGPM) that enforces feasibility through per-iteration projection. The estimation error bounds are established for spectral estimators, and DQGPM is shown to admit a finite-iteration error bound and achieves linear error contraction up to an explicit noise-dependent threshold. Experiments on synthetic benchmarks and real-world multi-scan point-set registration demonstrate that the proposed pipeline improves both accuracy and efficiency over representative matrix-based methods."
    }
  ],
  "NUS AI LAB": [
    {
      "title": "FD-VLA: Force-Distilled Vision-Language-Action Model for Contact-Rich Manipulation",
      "url": "https://arxiv.org/abs/2602.02142",
      "date": "2026-02-02",
      "authors_text": "Ruiteng Zhao, Wenshuo Wang, Yicheng Ma, Xiaocong Li, Francis E. H. Tay",
      "is_highlight": false,
      "score": 0,
      "summary": "The FD-VLA framework enhances contact-rich manipulation by integrating force awareness through a novel distillation method, improving performance without physical force sensors.",
      "teaser_image": "https://arxiv.org/html/2602.02142/x1.png",
      "debug_abstract": "Force sensing is a crucial modality for Vision-Language-Action (VLA) frameworks, as it enables fine-grained perception and dexterous manipulation in contact-rich tasks. We present Force-Distilled VLA (FD-VLA), a novel framework that integrates force awareness into contact-rich manipulation without relying on physical force sensors. The core of our approach is a Force Distillation Module (FDM), which distills force by mapping a learnable query token, conditioned on visual observations and robot states, into a predicted force token aligned with the latent representation of actual force signals. During inference, this distilled force token is injected into the pretrained VLM, enabling force-aware reasoning while preserving the integrity of its vision-language semantics. This design provides two key benefits: first, it allows practical deployment across a wide range of robots that lack expensive or fragile force-torque sensors, thereby reducing hardware cost and complexity; second, the FDM introduces an additional force-vision-state fusion prior to the VLM, which improves cross-modal alignment and enhances perception-action robustness in contact-rich scenarios. Surprisingly, our physical experiments show that the distilled force token outperforms direct sensor force measurements as well as other baselines, which highlights the effectiveness of this force-distilled VLA approach."
    },
    {
      "title": "A Low-Cost Vision-Based Tactile Gripper with Pretraining Learning for Contact-Rich Manipulation",
      "url": "https://arxiv.org/abs/2602.00514",
      "date": "2026-01-31",
      "authors_text": "Yaohua Liu, Binkai Ou, Zicheng Qiu, Ce Hao, Yemin Wang",
      "is_highlight": false,
      "score": 11.0,
      "summary": "The LVTG gripper enhances contact-rich robotic manipulation through low-cost, durable visuo-tactile integration and improved policy learning via contrastive pretraining, achieving higher success rates.",
      "teaser_image": "https://arxiv.org/html/2602.00514/x1.png",
      "debug_abstract": "Robotic manipulation in contact-rich environments remains challenging, particularly when relying on conventional tactile sensors that suffer from limited sensing range, reliability, and cost-effectiveness. In this work, we present LVTG, a low-cost visuo-tactile gripper designed for stable, robust, and efficient physical interaction. Unlike existing visuo-tactile sensors, LVTG enables more effective and stable grasping of larger and heavier everyday objects, thanks to its enhanced tactile sensing area and greater opening angle. Its surface skin is made of highly wear-resistant material, significantly improving durability and extending operational lifespan. The integration of vision and tactile feedback allows LVTG to provide rich, high-fidelity sensory data, facilitating reliable perception during complex manipulation tasks. Furthermore, LVTG features a modular design that supports rapid maintenance and replacement. To effectively fuse vision and touch, We adopt a CLIP-inspired contrastive learning objective to align tactile embeddings with their corresponding visual observations, enabling a shared cross-modal representation space for visuo-tactile perception. This alignment improves the performance of an Action Chunking Transformer (ACT) policy in contact-rich manipulation, leading to more efficient data collection and more effective policy learning. Compared to the original ACT method, the proposed LVTG with pretraining achieves significantly higher success rates in manipulation tasks."
    },
    {
      "title": "TIDE: Trajectory-based Diagnostic Evaluation of Test-Time Improvement in LLM Agents",
      "url": "https://arxiv.org/abs/2602.02196",
      "date": "2026-02-02",
      "authors_text": "Hang Yan, Xinyu Che, Fangzhi Xu, Qiushi Sun, Zichen Ding",
      "is_highlight": false,
      "score": 20.0,
      "summary": "The TIDE framework evaluates Test-Time Improvement in LLM agents by analyzing task completion dynamics, recursive behaviors, and memory constraints to enhance performance optimization.",
      "teaser_image": "https://arxiv.org/html/2602.02196/x1.png",
      "debug_abstract": "Recent advances in autonomous LLM agents demonstrate their ability to improve performance through iterative interaction with the environment. We define this paradigm as Test-Time Improvement (TTI). However, the mechanisms under how and why TTI succeed or fail remain poorly understood, and existing evaluation metrics fail to capture their task optimization efficiency, behavior adaptation after erroneous actions, and the specific utility of working memory for task completion. To address these gaps, we propose Test-time Improvement Diagnostic Evaluation (TIDE), an agent-agnostic and environment-agnostic framework that decomposes TTI into three comprehensive and interconnected dimensions. The framework measures (1) the overall temporal dynamics of task completion and (2) identifies whether performance is primarily constrained by recursive looping behaviors or (3) by burdensome accumulated memory. Through extensive experiments across diverse agents and environments, TIDE highlights that improving agent performance requires more than scaling internal reasoning, calling for explicitly optimizing the interaction dynamics between the agent and the environment."
    },
    {
      "title": "UrbanGS: A Scalable and Efficient Architecture for Geometrically Accurate Large-Scene Reconstruction",
      "url": "https://arxiv.org/abs/2602.02089",
      "date": "2026-02-02",
      "authors_text": "Changbai Li, Haodong Zhu, Hanlin Chen, Xiuping Liang, Tongfei Chen",
      "is_highlight": false,
      "score": 22.0,
      "summary": "UrbanGS is a scalable framework that enhances geometric accuracy and memory efficiency in large-scale urban scene reconstruction through innovative regularization and adaptive Gaussian pruning techniques.",
      "teaser_image": "https://arxiv.org/html/2602.02089/figure/main.png",
      "debug_abstract": "While 3D Gaussian Splatting (3DGS) enables high-quality, real-time rendering for bounded scenes, its extension to large-scale urban environments gives rise to critical challenges in terms of geometric consistency, memory efficiency, and computational scalability. To address these issues, we present UrbanGS, a scalable reconstruction framework that effectively tackles these challenges for city-scale applications. First, we propose a Depth-Consistent D-Normal Regularization module. Unlike existing approaches that rely solely on monocular normal estimators, which can effectively update rotation parameters yet struggle to update position parameters, our method integrates D-Normal constraints with external depth supervision. This allows for comprehensive updates of all geometric parameters. By further incorporating an adaptive confidence weighting mechanism based on gradient consistency and inverse depth deviation, our approach significantly enhances multi-view depth alignment and geometric coherence, which effectively resolves the issue of geometric accuracy in complex large-scale scenes. To improve scalability, we introduce a Spatially Adaptive Gaussian Pruning (SAGP) strategy, which dynamically adjusts Gaussian density based on local geometric complexity and visibility to reduce redundancy. Additionally, a unified partitioning and view assignment scheme is designed to eliminate boundary artifacts and optimize computational load. Extensive experiments on multiple urban datasets demonstrate that UrbanGS achieves superior performance in rendering quality, geometric accuracy, and memory efficiency, providing a systematic solution for high-fidelity large-scale scene reconstruction."
    },
    {
      "title": "Beyond Precision: Training-Inference Mismatch is an Optimization Problem and Simple LR Scheduling Fixes It",
      "url": "https://arxiv.org/abs/2602.01826",
      "date": "2026-02-02",
      "authors_text": "Yaxiang Zhang, Yingru Li, Jiacai Liu, Jiawei Xu, Ziniu Li",
      "is_highlight": false,
      "score": 35.0,
      "summary": "This paper presents a dynamic learning rate scheduling method to mitigate training-inference mismatch and stabilize reinforcement learning in large language models by addressing gradient noise.",
      "teaser_image": "https://arxiv.org/html/2602.01826/Qwen3-4B_baseline_vs_importance_sampling_patches.png",
      "debug_abstract": "Reinforcement Learning (RL) for training Large Language Models is notoriously unstable. While recent studies attribute this to &#34;training inference mismatch stemming&#34; from inconsistent hybrid engines, standard remedies, such as Importance Sampling, might fail during extended training runs. In this work, we analyze this instability through the lens of optimization, demonstrating that gradient noise and training-inference mismatch escalate in tandem as training progresses. Meanwhile, we find that the mismatch can be effectively suppressed by shrinking the update size. Taken together, we deduce that the mismatch is not merely a static numerical discrepancy, but a dynamic failure coupled with the model&#39;s optimization. Based on this insight, we propose a simple yet effective solution: a specialized Learning Rate (LR) scheduler. Instead of pre-defined decay schedule in traditional LR scheduler, our method dynamically triggers LR decay based on response length, which we identify as a reliable early-warning signal for impending instability. Empirical evidence suggests that by reducing the learning rate as gradient noise rises, we can consistently stabilize RL training and keep the training-inference mismatch at a safe level."
    },
    {
      "title": "ReCALL: Recalibrating Capability Degradation for MLLM-based Composed Image Retrieval",
      "url": "https://arxiv.org/abs/2602.01639",
      "date": "2026-02-02",
      "authors_text": "Tianyu Yang, ChenWei He, Xiangzhao Hao, Tianyue Wang, Jiarui Guo",
      "is_highlight": false,
      "score": 42.0,
      "summary": "ReCALL addresses capability degradation in MLLM-based Composed Image Retrieval by diagnosing issues, generating corrective instructions, and refining the retriever through continual training.",
      "teaser_image": "https://arxiv.org/html/2602.01639/x1.png",
      "debug_abstract": "Composed Image Retrieval (CIR) aims to retrieve target images based on a hybrid query comprising a reference image and a modification text. Early dual-tower Vision-Language Models (VLMs) struggle with cross-modality compositional reasoning required for this task. Recently, adapting generative Multimodal Large Language Models (MLLMs) for retrieval offers a promising direction. However, we identify that this adaptation strategy overlooks a fundamental issue: adapting a generative MLLM into a single-embedding discriminative retriever triggers a paradigm conflict, which leads to Capability Degradation - the deterioration of native fine-grained reasoning after retrieval adaptation. To address this challenge, we propose ReCALL (Recalibrating Capability Degradation), a model-agnostic framework that follows a diagnose-generate-refine pipeline: Firstly, we diagnose cognitive blind spots of the retriever via self-guided informative instance mining. Next, we generate corrective instructions and triplets by CoT prompting the foundation MLLM and conduct quality control with VQA-based consistency filtering. Finally, we refine the retriever through continual training on these triplets with a grouped contrastive scheme, thereby internalizing fine-grained visual-semantic distinctions and realigning the discriminative embedding space of retriever with intrinsic compositional reasoning within the MLLM. Extensive experiments on CIRR and FashionIQ show that ReCALL consistently recalibrates degraded capabilities and achieves state-of-the-art performance. Code will be released soon."
    },
    {
      "title": "PRISM: Festina Lente Proactivity -- Risk-Sensitive, Uncertainty-Aware Deliberation for Proactive Agents",
      "url": "https://arxiv.org/abs/2602.01532",
      "date": "2026-02-02",
      "authors_text": "Yuxuan Fu, Xiaoyu Tan, Teqi Hao, Chen Zhan, Xihe Qiu",
      "is_highlight": false,
      "score": 45.0,
      "summary": "PRISM introduces a decision-theoretic framework for proactive agents that optimizes intervention timing and accuracy, significantly reducing false alarms and enhancing performance on ProactiveBench.",
      "teaser_image": "https://arxiv.org/html/2602.01532/fig2.png",
      "debug_abstract": "Proactive agents must decide not only what to say but also whether and when to intervene. Many current systems rely on brittle heuristics or indiscriminate long reasoning, which offers little control over the benefit-burden tradeoff. We formulate the problem as cost-sensitive selective intervention and present PRISM, a novel framework that couples a decision-theoretic gate with a dual-process reasoning architecture. At inference time, the agent intervenes only when a calibrated probability of user acceptance exceeds a threshold derived from asymmetric costs of missed help and false alarms. Inspired by festina lente (Latin: &#34;make haste slowly&#34;), we gate by an acceptance-calibrated, cost-derived threshold and invoke a resource-intensive Slow mode with counterfactual checks only near the decision boundary, concentrating computation on ambiguous and high-stakes cases. Training uses gate-aligned, schema-locked distillation: a teacher running the full PRISM pipeline provides dense, executable supervision on unlabeled interaction traces, while the student learns a response policy that is explicitly decoupled from the intervention gate to enable tunable and auditable control. On ProactiveBench, PRISM reduces false alarms by 22.78% and improves F1 by 20.14% over strong baselines. These results show that principled decision-theoretic gating, paired with selective slow reasoning and aligned distillation, yields proactive agents that are precise, computationally efficient, and controllable. To facilitate reproducibility, we release our code, models, and resources at this https URL all experiments use the open-source ProactiveBench benchmark."
    },
    {
      "title": "Do All Individual Layers Help? An Empirical Study of Task-Interfering Layers in Vision-Language Models",
      "url": "https://arxiv.org/abs/2602.01167",
      "date": "2026-02-01",
      "authors_text": "Zhiming Liu, Yujie Wei, Lei Feng, Xiu Su, Xiaobo Xia",
      "is_highlight": false,
      "score": 52.0,
      "summary": "This study identifies Task-Interfering Layers in vision-language models, demonstrating that selectively bypassing certain layers can enhance task performance without retraining.",
      "teaser_image": "https://arxiv.org/html/2602.01167/x1.png",
      "debug_abstract": "Current VLMs have demonstrated capabilities across a wide range of multimodal tasks. Typically, in a pretrained VLM, all layers are engaged by default to make predictions on downstream tasks. We find that intervening on a single layer, such as by zeroing its parameters, can improve the performance on certain tasks, indicating that some layers hinder rather than help downstream tasks. We systematically investigate how individual layers influence different tasks via layer intervention. Specifically, we measure the change in performance relative to the base model after intervening on each layer and observe improvements when bypassing specific layers. This improvement can be generalizable across models and datasets, indicating the presence of Task-Interfering Layers that harm downstream tasks&#39; performance. We introduce Task-Layer Interaction Vector, which quantifies the effect of intervening on each layer of a VLM given a task. These task-interfering layers exhibit task-specific sensitivity patterns: tasks requiring similar capabilities show consistent response trends under layer interventions, as evidenced by the high similarity in their task-layer interaction vectors. Inspired by these findings, we propose TaLo (Task-Adaptive Layer Knockout), a training-free, test-time adaptation method that dynamically identifies and bypasses the most interfering layer for a given task. Without parameter updates, TaLo improves performance across various models and datasets, including boosting Qwen-VL&#39;s accuracy on the Maps task in ScienceQA by up to 16.6%. Our work reveals an unexpected form of modularity in pretrained VLMs and provides a plug-and-play, training-free mechanism to unlock hidden capabilities at inference time. The source code will be publicly available."
    },
    {
      "title": "Unveiling the Cognitive Compass: Theory-of-Mind-Guided Multimodal Emotion Reasoning",
      "url": "https://arxiv.org/abs/2602.00971",
      "date": "2026-02-01",
      "authors_text": "Meng Luo, Bobo Li, Shanqing Xu, Shize Zhang, Qiuchan Chen",
      "is_highlight": false,
      "score": 62.0,
      "summary": "The paper presents HitEmotion, a benchmark for assessing and enhancing multimodal large language models' emotional reasoning through Theory of Mind-guided methods and reinforcement learning.",
      "teaser_image": "https://arxiv.org/html/2602.00971/figures/teaser.png",
      "debug_abstract": "Despite rapid progress in multimodal large language models (MLLMs), their capability for deep emotional understanding remains limited. We argue that genuine affective intelligence requires explicit modeling of Theory of Mind (ToM), the cognitive substrate from which emotions arise. To this end, we introduce HitEmotion, a ToM-grounded hierarchical benchmark that diagnoses capability breakpoints across increasing levels of cognitive depth. Second, we propose a ToM-guided reasoning chain that tracks mental states and calibrates cross-modal evidence to achieve faithful emotional reasoning. We further introduce TMPO, a reinforcement learning method that uses intermediate mental states as process-level supervision to guide and strengthen model reasoning. Extensive experiments show that HitEmotion exposes deep emotional reasoning deficits in state-of-the-art models, especially on cognitively demanding tasks. In evaluation, the ToM-guided reasoning chain and TMPO improve end-task accuracy and yield more faithful, more coherent rationales. In conclusion, our work provides the research community with a practical toolkit for evaluating and enhancing the cognition-based emotional understanding capabilities of MLLMs. Our dataset and code are available at: this https URL."
    },
    {
      "title": "Self-Guard: Defending Large Reasoning Models via enhanced self-reflection",
      "url": "https://arxiv.org/abs/2602.00707",
      "date": "2026-01-31",
      "authors_text": "Jingnan Zheng, Jingjun Xu, Yanzhen Luo, Chenhang Cui, Gelei Deng",
      "is_highlight": false,
      "score": 50.0,
      "summary": "Self-Guard is a lightweight framework enhancing safety compliance in Large Reasoning Models by promoting self-reflection and mitigating risks without sacrificing model utility.",
      "teaser_image": "https://arxiv.org/html/2602.00707/materials/workflow.png",
      "debug_abstract": "The emergence of Large Reasoning Models (LRMs) introduces a new paradigm of explicit reasoning, enabling remarkable advances yet posing unique risks such as reasoning manipulation and information leakage. To mitigate these risks, current alignment strategies predominantly rely on heavy post-training paradigms or external interventions. However, these approaches are often computationally intensive and fail to address the inherent awareness-compliance gap, a critical misalignment where models recognize potential risks yet prioritize following user instructions due to their sycophantic tendencies. To address these limitations, we propose Self-Guard, a lightweight safety defense framework that reinforces safety compliance at the representational level. Self-Guard operates through two principal stages: (1) safety-oriented prompting, which activates the model&#39;s latent safety awareness to evoke spontaneous reflection, and (2) safety activation steering, which extracts the resulting directional shift in the hidden state space and amplifies it to ensure that safety compliance prevails over sycophancy during inference. Experiments demonstrate that Self-Guard effectively bridges the awareness-compliance gap, achieving robust safety performance without compromising model utility. Furthermore, Self-Guard exhibits strong generalization across diverse unseen risks and varying model scales, offering a cost-efficient solution for LRM safety alignment."
    },
    {
      "title": "APEX: A Decoupled Memory-based Explorer for Asynchronous Aerial Object Goal Navigation",
      "url": "https://arxiv.org/abs/2602.00551",
      "date": "2026-01-31",
      "authors_text": "Daoxuan Zhang, Ping Chen, Xiaobo Xia, Xiu Su, Ruichen Zhen",
      "is_highlight": false,
      "score": 88.0,
      "summary": "APEX is a novel hierarchical agent that enhances UAV navigation through efficient exploration and target acquisition using dynamic memory, reinforcement learning, and open-vocabulary detection.",
      "teaser_image": "https://arxiv.org/html/2602.00551/x1.png",
      "debug_abstract": "Aerial Object Goal Navigation, a challenging frontier in Embodied AI, requires an Unmanned Aerial Vehicle (UAV) agent to autonomously explore, reason, and identify a specific target using only visual perception and language description. However, existing methods struggle with the memorization of complex spatial representations in aerial environments, reliable and interpretable action decision-making, and inefficient exploration and information gathering. To address these challenges, we introduce \\textbf{APEX} (Aerial Parallel Explorer), a novel hierarchical agent designed for efficient exploration and target acquisition in complex aerial settings. APEX is built upon a modular, three-part architecture: 1) Dynamic Spatio-Semantic Mapping Memory, which leverages the zero-shot capability of a Vision-Language Model (VLM) to dynamically construct high-resolution 3D Attraction, Exploration, and Obstacle maps, serving as an interpretable memory mechanism. 2) Action Decision Module, trained with reinforcement learning, which translates this rich spatial understanding into a fine-grained and robust control policy. 3) Target Grounding Module, which employs an open-vocabulary detector to achieve definitive and generalizable target identification. All these components are integrated into a hierarchical, asynchronous, and parallel framework, effectively bypassing the VLM&#39;s inference latency and boosting the agent&#39;s proactivity in exploration. Extensive experiments show that APEX outperforms the previous state of the art by +4.2\\% SR and +2.8\\% SPL on challenging UAV-ON benchmarks, demonstrating its superior efficiency and the effectiveness of its hierarchical asynchronous design. Our source code is provided in \\href{this https URL}{GitHub}"
    },
    {
      "title": "Inject Once Survive Later: Backdooring Vision-Language-Action Models to Persist Through Downstream Fine-tuning",
      "url": "https://arxiv.org/abs/2602.00500",
      "date": "2026-01-31",
      "authors_text": "Jianyi Zhou, Yujie Wei, Ruichen Zhen, Bo Zhao, Xiaobo Xia",
      "is_highlight": true,
      "score": 60.0,
      "summary": "The paper introduces INFUSE, a robust backdoor attack framework for Vision-Language-Action models that survives user fine-tuning, maintaining high attack success rates while preserving performance.",
      "teaser_image": "https://arxiv.org/html/2602.00500/x2.png",
      "debug_abstract": "Vision-Language-Action (VLA) models have become foundational to modern embodied AI systems. By integrating visual perception, language understanding, and action planning, they enable general-purpose task execution across diverse environments. Despite their importance, the security of VLA models remains underexplored -- particularly in the context of backdoor attacks, which pose realistic threats in physical-world deployments. While recent methods attempt to inject backdoors into VLA models, these backdoors are easily erased during downstream adaptation, as user-side fine-tuning with clean data significantly alters model parameters, rendering them impractical for real-world applications. To address these challenges, we propose INFUSE (INjection into Fine-tUne-inSensitive modulEs), the first backdoor attack framework for VLA base models that remains effective even with arbitrary user fine-tuning. INFUSE begins by analyzing parameter sensitivity across diverse fine-tuning scenarios to identify modules that remain largely unchanged -- the fine-tune-insensitive modules. It then injects backdoors into these stable modules while freezing the rest, ensuring malicious behavior persists after extensive user fine-tuning. Comprehensive experiments across multiple VLA architectures demonstrate INFUSE&#39;s effectiveness. After user-side fine-tuning, INFUSE maintains mean attack success rates of 91.0% on simulation environments and 79.8% on real-world robot tasks, substantially surpassing BadVLA (38.8% and 36.6%, respectively), while preserving clean-task performance comparable to standard models. These results uncover a critical threat: backdoors implanted before distribution can persist through fine-tuning and remain effective at deployment."
    }
  ],
  "Advanced Robotics Centre": [
    {
      "title": "FD-VLA: Force-Distilled Vision-Language-Action Model for Contact-Rich Manipulation",
      "url": "https://arxiv.org/abs/2602.02142",
      "date": "2026-02-02",
      "authors_text": "Ruiteng Zhao, Wenshuo Wang, Yicheng Ma, Xiaocong Li, Francis E. H. Tay",
      "is_highlight": false,
      "score": 0,
      "summary": "The FD-VLA framework enhances contact-rich manipulation by integrating force awareness through a novel distillation method, improving performance without physical force sensors.",
      "teaser_image": "https://arxiv.org/html/2602.02142/x1.png",
      "debug_abstract": "Force sensing is a crucial modality for Vision-Language-Action (VLA) frameworks, as it enables fine-grained perception and dexterous manipulation in contact-rich tasks. We present Force-Distilled VLA (FD-VLA), a novel framework that integrates force awareness into contact-rich manipulation without relying on physical force sensors. The core of our approach is a Force Distillation Module (FDM), which distills force by mapping a learnable query token, conditioned on visual observations and robot states, into a predicted force token aligned with the latent representation of actual force signals. During inference, this distilled force token is injected into the pretrained VLM, enabling force-aware reasoning while preserving the integrity of its vision-language semantics. This design provides two key benefits: first, it allows practical deployment across a wide range of robots that lack expensive or fragile force-torque sensors, thereby reducing hardware cost and complexity; second, the FDM introduces an additional force-vision-state fusion prior to the VLM, which improves cross-modal alignment and enhances perception-action robustness in contact-rich scenarios. Surprisingly, our physical experiments show that the distilled force token outperforms direct sensor force measurements as well as other baselines, which highlights the effectiveness of this force-distilled VLA approach."
    },
    {
      "title": "A Low-Cost Vision-Based Tactile Gripper with Pretraining Learning for Contact-Rich Manipulation",
      "url": "https://arxiv.org/abs/2602.00514",
      "date": "2026-01-31",
      "authors_text": "Yaohua Liu, Binkai Ou, Zicheng Qiu, Ce Hao, Yemin Wang",
      "is_highlight": false,
      "score": 11.0,
      "summary": "The LVTG gripper enhances contact-rich robotic manipulation through low-cost, durable visuo-tactile integration and improved policy learning via contrastive pretraining, achieving higher success rates.",
      "teaser_image": "https://arxiv.org/html/2602.00514/x1.png",
      "debug_abstract": "Robotic manipulation in contact-rich environments remains challenging, particularly when relying on conventional tactile sensors that suffer from limited sensing range, reliability, and cost-effectiveness. In this work, we present LVTG, a low-cost visuo-tactile gripper designed for stable, robust, and efficient physical interaction. Unlike existing visuo-tactile sensors, LVTG enables more effective and stable grasping of larger and heavier everyday objects, thanks to its enhanced tactile sensing area and greater opening angle. Its surface skin is made of highly wear-resistant material, significantly improving durability and extending operational lifespan. The integration of vision and tactile feedback allows LVTG to provide rich, high-fidelity sensory data, facilitating reliable perception during complex manipulation tasks. Furthermore, LVTG features a modular design that supports rapid maintenance and replacement. To effectively fuse vision and touch, We adopt a CLIP-inspired contrastive learning objective to align tactile embeddings with their corresponding visual observations, enabling a shared cross-modal representation space for visuo-tactile perception. This alignment improves the performance of an Action Chunking Transformer (ACT) policy in contact-rich manipulation, leading to more efficient data collection and more effective policy learning. Compared to the original ACT method, the proposed LVTG with pretraining achieves significantly higher success rates in manipulation tasks."
    },
    {
      "title": "TIDE: Trajectory-based Diagnostic Evaluation of Test-Time Improvement in LLM Agents",
      "url": "https://arxiv.org/abs/2602.02196",
      "date": "2026-02-02",
      "authors_text": "Hang Yan, Xinyu Che, Fangzhi Xu, Qiushi Sun, Zichen Ding",
      "is_highlight": false,
      "score": 20.0,
      "summary": "The TIDE framework evaluates Test-Time Improvement in LLM agents by analyzing task completion dynamics, recursive behaviors, and memory constraints to enhance performance optimization.",
      "teaser_image": "https://arxiv.org/html/2602.02196/x1.png",
      "debug_abstract": "Recent advances in autonomous LLM agents demonstrate their ability to improve performance through iterative interaction with the environment. We define this paradigm as Test-Time Improvement (TTI). However, the mechanisms under how and why TTI succeed or fail remain poorly understood, and existing evaluation metrics fail to capture their task optimization efficiency, behavior adaptation after erroneous actions, and the specific utility of working memory for task completion. To address these gaps, we propose Test-time Improvement Diagnostic Evaluation (TIDE), an agent-agnostic and environment-agnostic framework that decomposes TTI into three comprehensive and interconnected dimensions. The framework measures (1) the overall temporal dynamics of task completion and (2) identifies whether performance is primarily constrained by recursive looping behaviors or (3) by burdensome accumulated memory. Through extensive experiments across diverse agents and environments, TIDE highlights that improving agent performance requires more than scaling internal reasoning, calling for explicitly optimizing the interaction dynamics between the agent and the environment."
    },
    {
      "title": "UrbanGS: A Scalable and Efficient Architecture for Geometrically Accurate Large-Scene Reconstruction",
      "url": "https://arxiv.org/abs/2602.02089",
      "date": "2026-02-02",
      "authors_text": "Changbai Li, Haodong Zhu, Hanlin Chen, Xiuping Liang, Tongfei Chen",
      "is_highlight": false,
      "score": 22.0,
      "summary": "UrbanGS is a scalable framework that enhances geometric accuracy and memory efficiency in large-scale urban scene reconstruction through innovative regularization and adaptive Gaussian pruning techniques.",
      "teaser_image": "https://arxiv.org/html/2602.02089/figure/main.png",
      "debug_abstract": "While 3D Gaussian Splatting (3DGS) enables high-quality, real-time rendering for bounded scenes, its extension to large-scale urban environments gives rise to critical challenges in terms of geometric consistency, memory efficiency, and computational scalability. To address these issues, we present UrbanGS, a scalable reconstruction framework that effectively tackles these challenges for city-scale applications. First, we propose a Depth-Consistent D-Normal Regularization module. Unlike existing approaches that rely solely on monocular normal estimators, which can effectively update rotation parameters yet struggle to update position parameters, our method integrates D-Normal constraints with external depth supervision. This allows for comprehensive updates of all geometric parameters. By further incorporating an adaptive confidence weighting mechanism based on gradient consistency and inverse depth deviation, our approach significantly enhances multi-view depth alignment and geometric coherence, which effectively resolves the issue of geometric accuracy in complex large-scale scenes. To improve scalability, we introduce a Spatially Adaptive Gaussian Pruning (SAGP) strategy, which dynamically adjusts Gaussian density based on local geometric complexity and visibility to reduce redundancy. Additionally, a unified partitioning and view assignment scheme is designed to eliminate boundary artifacts and optimize computational load. Extensive experiments on multiple urban datasets demonstrate that UrbanGS achieves superior performance in rendering quality, geometric accuracy, and memory efficiency, providing a systematic solution for high-fidelity large-scale scene reconstruction."
    },
    {
      "title": "Beyond Precision: Training-Inference Mismatch is an Optimization Problem and Simple LR Scheduling Fixes It",
      "url": "https://arxiv.org/abs/2602.01826",
      "date": "2026-02-02",
      "authors_text": "Yaxiang Zhang, Yingru Li, Jiacai Liu, Jiawei Xu, Ziniu Li",
      "is_highlight": false,
      "score": 35.0,
      "summary": "This paper presents a dynamic learning rate scheduling method to mitigate training-inference mismatch and stabilize reinforcement learning in large language models by addressing gradient noise.",
      "teaser_image": "https://arxiv.org/html/2602.01826/Qwen3-4B_baseline_vs_importance_sampling_patches.png",
      "debug_abstract": "Reinforcement Learning (RL) for training Large Language Models is notoriously unstable. While recent studies attribute this to &#34;training inference mismatch stemming&#34; from inconsistent hybrid engines, standard remedies, such as Importance Sampling, might fail during extended training runs. In this work, we analyze this instability through the lens of optimization, demonstrating that gradient noise and training-inference mismatch escalate in tandem as training progresses. Meanwhile, we find that the mismatch can be effectively suppressed by shrinking the update size. Taken together, we deduce that the mismatch is not merely a static numerical discrepancy, but a dynamic failure coupled with the model&#39;s optimization. Based on this insight, we propose a simple yet effective solution: a specialized Learning Rate (LR) scheduler. Instead of pre-defined decay schedule in traditional LR scheduler, our method dynamically triggers LR decay based on response length, which we identify as a reliable early-warning signal for impending instability. Empirical evidence suggests that by reducing the learning rate as gradient noise rises, we can consistently stabilize RL training and keep the training-inference mismatch at a safe level."
    },
    {
      "title": "ReCALL: Recalibrating Capability Degradation for MLLM-based Composed Image Retrieval",
      "url": "https://arxiv.org/abs/2602.01639",
      "date": "2026-02-02",
      "authors_text": "Tianyu Yang, ChenWei He, Xiangzhao Hao, Tianyue Wang, Jiarui Guo",
      "is_highlight": false,
      "score": 42.0,
      "summary": "ReCALL addresses capability degradation in MLLM-based Composed Image Retrieval by diagnosing issues, generating corrective instructions, and refining the retriever through continual training.",
      "teaser_image": "https://arxiv.org/html/2602.01639/x1.png",
      "debug_abstract": "Composed Image Retrieval (CIR) aims to retrieve target images based on a hybrid query comprising a reference image and a modification text. Early dual-tower Vision-Language Models (VLMs) struggle with cross-modality compositional reasoning required for this task. Recently, adapting generative Multimodal Large Language Models (MLLMs) for retrieval offers a promising direction. However, we identify that this adaptation strategy overlooks a fundamental issue: adapting a generative MLLM into a single-embedding discriminative retriever triggers a paradigm conflict, which leads to Capability Degradation - the deterioration of native fine-grained reasoning after retrieval adaptation. To address this challenge, we propose ReCALL (Recalibrating Capability Degradation), a model-agnostic framework that follows a diagnose-generate-refine pipeline: Firstly, we diagnose cognitive blind spots of the retriever via self-guided informative instance mining. Next, we generate corrective instructions and triplets by CoT prompting the foundation MLLM and conduct quality control with VQA-based consistency filtering. Finally, we refine the retriever through continual training on these triplets with a grouped contrastive scheme, thereby internalizing fine-grained visual-semantic distinctions and realigning the discriminative embedding space of retriever with intrinsic compositional reasoning within the MLLM. Extensive experiments on CIRR and FashionIQ show that ReCALL consistently recalibrates degraded capabilities and achieves state-of-the-art performance. Code will be released soon."
    },
    {
      "title": "PRISM: Festina Lente Proactivity -- Risk-Sensitive, Uncertainty-Aware Deliberation for Proactive Agents",
      "url": "https://arxiv.org/abs/2602.01532",
      "date": "2026-02-02",
      "authors_text": "Yuxuan Fu, Xiaoyu Tan, Teqi Hao, Chen Zhan, Xihe Qiu",
      "is_highlight": false,
      "score": 45.0,
      "summary": "PRISM introduces a decision-theoretic framework for proactive agents that optimizes intervention timing and accuracy, significantly reducing false alarms and enhancing performance on ProactiveBench.",
      "teaser_image": "https://arxiv.org/html/2602.01532/fig2.png",
      "debug_abstract": "Proactive agents must decide not only what to say but also whether and when to intervene. Many current systems rely on brittle heuristics or indiscriminate long reasoning, which offers little control over the benefit-burden tradeoff. We formulate the problem as cost-sensitive selective intervention and present PRISM, a novel framework that couples a decision-theoretic gate with a dual-process reasoning architecture. At inference time, the agent intervenes only when a calibrated probability of user acceptance exceeds a threshold derived from asymmetric costs of missed help and false alarms. Inspired by festina lente (Latin: &#34;make haste slowly&#34;), we gate by an acceptance-calibrated, cost-derived threshold and invoke a resource-intensive Slow mode with counterfactual checks only near the decision boundary, concentrating computation on ambiguous and high-stakes cases. Training uses gate-aligned, schema-locked distillation: a teacher running the full PRISM pipeline provides dense, executable supervision on unlabeled interaction traces, while the student learns a response policy that is explicitly decoupled from the intervention gate to enable tunable and auditable control. On ProactiveBench, PRISM reduces false alarms by 22.78% and improves F1 by 20.14% over strong baselines. These results show that principled decision-theoretic gating, paired with selective slow reasoning and aligned distillation, yields proactive agents that are precise, computationally efficient, and controllable. To facilitate reproducibility, we release our code, models, and resources at this https URL all experiments use the open-source ProactiveBench benchmark."
    },
    {
      "title": "Do All Individual Layers Help? An Empirical Study of Task-Interfering Layers in Vision-Language Models",
      "url": "https://arxiv.org/abs/2602.01167",
      "date": "2026-02-01",
      "authors_text": "Zhiming Liu, Yujie Wei, Lei Feng, Xiu Su, Xiaobo Xia",
      "is_highlight": false,
      "score": 52.0,
      "summary": "This study identifies Task-Interfering Layers in vision-language models, demonstrating that selectively bypassing certain layers can enhance task performance without retraining.",
      "teaser_image": "https://arxiv.org/html/2602.01167/x1.png",
      "debug_abstract": "Current VLMs have demonstrated capabilities across a wide range of multimodal tasks. Typically, in a pretrained VLM, all layers are engaged by default to make predictions on downstream tasks. We find that intervening on a single layer, such as by zeroing its parameters, can improve the performance on certain tasks, indicating that some layers hinder rather than help downstream tasks. We systematically investigate how individual layers influence different tasks via layer intervention. Specifically, we measure the change in performance relative to the base model after intervening on each layer and observe improvements when bypassing specific layers. This improvement can be generalizable across models and datasets, indicating the presence of Task-Interfering Layers that harm downstream tasks&#39; performance. We introduce Task-Layer Interaction Vector, which quantifies the effect of intervening on each layer of a VLM given a task. These task-interfering layers exhibit task-specific sensitivity patterns: tasks requiring similar capabilities show consistent response trends under layer interventions, as evidenced by the high similarity in their task-layer interaction vectors. Inspired by these findings, we propose TaLo (Task-Adaptive Layer Knockout), a training-free, test-time adaptation method that dynamically identifies and bypasses the most interfering layer for a given task. Without parameter updates, TaLo improves performance across various models and datasets, including boosting Qwen-VL&#39;s accuracy on the Maps task in ScienceQA by up to 16.6%. Our work reveals an unexpected form of modularity in pretrained VLMs and provides a plug-and-play, training-free mechanism to unlock hidden capabilities at inference time. The source code will be publicly available."
    },
    {
      "title": "Unveiling the Cognitive Compass: Theory-of-Mind-Guided Multimodal Emotion Reasoning",
      "url": "https://arxiv.org/abs/2602.00971",
      "date": "2026-02-01",
      "authors_text": "Meng Luo, Bobo Li, Shanqing Xu, Shize Zhang, Qiuchan Chen",
      "is_highlight": false,
      "score": 62.0,
      "summary": "The paper presents HitEmotion, a benchmark for assessing and enhancing multimodal large language models' emotional reasoning through Theory of Mind-guided methods and reinforcement learning.",
      "teaser_image": "https://arxiv.org/html/2602.00971/figures/teaser.png",
      "debug_abstract": "Despite rapid progress in multimodal large language models (MLLMs), their capability for deep emotional understanding remains limited. We argue that genuine affective intelligence requires explicit modeling of Theory of Mind (ToM), the cognitive substrate from which emotions arise. To this end, we introduce HitEmotion, a ToM-grounded hierarchical benchmark that diagnoses capability breakpoints across increasing levels of cognitive depth. Second, we propose a ToM-guided reasoning chain that tracks mental states and calibrates cross-modal evidence to achieve faithful emotional reasoning. We further introduce TMPO, a reinforcement learning method that uses intermediate mental states as process-level supervision to guide and strengthen model reasoning. Extensive experiments show that HitEmotion exposes deep emotional reasoning deficits in state-of-the-art models, especially on cognitively demanding tasks. In evaluation, the ToM-guided reasoning chain and TMPO improve end-task accuracy and yield more faithful, more coherent rationales. In conclusion, our work provides the research community with a practical toolkit for evaluating and enhancing the cognition-based emotional understanding capabilities of MLLMs. Our dataset and code are available at: this https URL."
    },
    {
      "title": "Self-Guard: Defending Large Reasoning Models via enhanced self-reflection",
      "url": "https://arxiv.org/abs/2602.00707",
      "date": "2026-01-31",
      "authors_text": "Jingnan Zheng, Jingjun Xu, Yanzhen Luo, Chenhang Cui, Gelei Deng",
      "is_highlight": false,
      "score": 50.0,
      "summary": "Self-Guard is a lightweight framework enhancing safety compliance in Large Reasoning Models by promoting self-reflection and mitigating risks without sacrificing model utility.",
      "teaser_image": "https://arxiv.org/html/2602.00707/materials/workflow.png",
      "debug_abstract": "The emergence of Large Reasoning Models (LRMs) introduces a new paradigm of explicit reasoning, enabling remarkable advances yet posing unique risks such as reasoning manipulation and information leakage. To mitigate these risks, current alignment strategies predominantly rely on heavy post-training paradigms or external interventions. However, these approaches are often computationally intensive and fail to address the inherent awareness-compliance gap, a critical misalignment where models recognize potential risks yet prioritize following user instructions due to their sycophantic tendencies. To address these limitations, we propose Self-Guard, a lightweight safety defense framework that reinforces safety compliance at the representational level. Self-Guard operates through two principal stages: (1) safety-oriented prompting, which activates the model&#39;s latent safety awareness to evoke spontaneous reflection, and (2) safety activation steering, which extracts the resulting directional shift in the hidden state space and amplifies it to ensure that safety compliance prevails over sycophancy during inference. Experiments demonstrate that Self-Guard effectively bridges the awareness-compliance gap, achieving robust safety performance without compromising model utility. Furthermore, Self-Guard exhibits strong generalization across diverse unseen risks and varying model scales, offering a cost-efficient solution for LRM safety alignment."
    },
    {
      "title": "APEX: A Decoupled Memory-based Explorer for Asynchronous Aerial Object Goal Navigation",
      "url": "https://arxiv.org/abs/2602.00551",
      "date": "2026-01-31",
      "authors_text": "Daoxuan Zhang, Ping Chen, Xiaobo Xia, Xiu Su, Ruichen Zhen",
      "is_highlight": false,
      "score": 88.0,
      "summary": "APEX is a novel hierarchical agent that enhances UAV navigation through efficient exploration and target acquisition using dynamic memory, reinforcement learning, and open-vocabulary detection.",
      "teaser_image": "https://arxiv.org/html/2602.00551/x1.png",
      "debug_abstract": "Aerial Object Goal Navigation, a challenging frontier in Embodied AI, requires an Unmanned Aerial Vehicle (UAV) agent to autonomously explore, reason, and identify a specific target using only visual perception and language description. However, existing methods struggle with the memorization of complex spatial representations in aerial environments, reliable and interpretable action decision-making, and inefficient exploration and information gathering. To address these challenges, we introduce \\textbf{APEX} (Aerial Parallel Explorer), a novel hierarchical agent designed for efficient exploration and target acquisition in complex aerial settings. APEX is built upon a modular, three-part architecture: 1) Dynamic Spatio-Semantic Mapping Memory, which leverages the zero-shot capability of a Vision-Language Model (VLM) to dynamically construct high-resolution 3D Attraction, Exploration, and Obstacle maps, serving as an interpretable memory mechanism. 2) Action Decision Module, trained with reinforcement learning, which translates this rich spatial understanding into a fine-grained and robust control policy. 3) Target Grounding Module, which employs an open-vocabulary detector to achieve definitive and generalizable target identification. All these components are integrated into a hierarchical, asynchronous, and parallel framework, effectively bypassing the VLM&#39;s inference latency and boosting the agent&#39;s proactivity in exploration. Extensive experiments show that APEX outperforms the previous state of the art by +4.2\\% SR and +2.8\\% SPL on challenging UAV-ON benchmarks, demonstrating its superior efficiency and the effectiveness of its hierarchical asynchronous design. Our source code is provided in \\href{this https URL}{GitHub}"
    },
    {
      "title": "Inject Once Survive Later: Backdooring Vision-Language-Action Models to Persist Through Downstream Fine-tuning",
      "url": "https://arxiv.org/abs/2602.00500",
      "date": "2026-01-31",
      "authors_text": "Jianyi Zhou, Yujie Wei, Ruichen Zhen, Bo Zhao, Xiaobo Xia",
      "is_highlight": true,
      "score": 60.0,
      "summary": "The paper introduces INFUSE, a robust backdoor attack framework for Vision-Language-Action models that survives user fine-tuning, maintaining high attack success rates while preserving performance.",
      "teaser_image": "https://arxiv.org/html/2602.00500/x2.png",
      "debug_abstract": "Vision-Language-Action (VLA) models have become foundational to modern embodied AI systems. By integrating visual perception, language understanding, and action planning, they enable general-purpose task execution across diverse environments. Despite their importance, the security of VLA models remains underexplored -- particularly in the context of backdoor attacks, which pose realistic threats in physical-world deployments. While recent methods attempt to inject backdoors into VLA models, these backdoors are easily erased during downstream adaptation, as user-side fine-tuning with clean data significantly alters model parameters, rendering them impractical for real-world applications. To address these challenges, we propose INFUSE (INjection into Fine-tUne-inSensitive modulEs), the first backdoor attack framework for VLA base models that remains effective even with arbitrary user fine-tuning. INFUSE begins by analyzing parameter sensitivity across diverse fine-tuning scenarios to identify modules that remain largely unchanged -- the fine-tune-insensitive modules. It then injects backdoors into these stable modules while freezing the rest, ensuring malicious behavior persists after extensive user fine-tuning. Comprehensive experiments across multiple VLA architectures demonstrate INFUSE&#39;s effectiveness. After user-side fine-tuning, INFUSE maintains mean attack success rates of 91.0% on simulation environments and 79.8% on real-world robot tasks, substantially surpassing BadVLA (38.8% and 36.6%, respectively), while preserving clean-task performance comparable to standard models. These results uncover a critical threat: backdoors implanted before distribution can persist through fine-tuning and remain effective at deployment."
    }
  ],
  "机器人技术与系统国家重点实验室": [
    {
      "title": "Cross-Modal Alignment and Fusion for RGB-D Transmission-Line Defect Detection",
      "url": "https://arxiv.org/abs/2602.01696",
      "date": "2026-02-02",
      "authors_text": "Jiaming Cui, Shuai Zhou, Wenqiang Li, Ruifeng Qin, Feng Shen",
      "is_highlight": false,
      "score": 6.0,
      "summary": "CMAFNet enhances RGB-D transmission line defect detection by integrating RGB and depth data through a novel purification and fusion approach, significantly outperforming existing methods.",
      "teaser_image": "https://arxiv.org/html/2602.01696/figures/Overall_Architecture.png",
      "debug_abstract": "Transmission line defect detection remains challenging for automated UAV inspection due to the dominance of small-scale defects, complex backgrounds, and illumination variations. Existing RGB-based detectors, despite recent progress, struggle to distinguish geometrically subtle defects from visually similar background structures under limited chromatic contrast. This paper proposes CMAFNet, a Cross-Modal Alignment and Fusion Network that integrates RGB appearance and depth geometry through a principled purify-then-fuse paradigm. CMAFNet consists of a Semantic Recomposition Module that performs dictionary-based feature purification via a learned codebook to suppress modality-specific noise while preserving defect-discriminative information, and a Contextual Semantic Integration Framework that captures global spatial dependencies using partial-channel attention to enhance structural semantic reasoning. Position-wise normalization within the purification stage enforces explicit reconstruction-driven cross-modal alignment, ensuring statistical compatibility between heterogeneous features prior to fusion. Extensive experiments on the TLRGBD benchmark, where 94.5% of instances are small objects, demonstrate that CMAFNet achieves 32.2% mAP@50 and 12.5% APs, outperforming the strongest baseline by 9.8 and 4.0 percentage points, respectively. A lightweight variant reaches 24.8% mAP50 at 228 FPS with only 4.9M parameters, surpassing all YOLO-based detectors while matching transformer-based methods at substantially lower computational cost."
    },
    {
      "title": "Do All Individual Layers Help? An Empirical Study of Task-Interfering Layers in Vision-Language Models",
      "url": "https://arxiv.org/abs/2602.01167",
      "date": "2026-02-01",
      "authors_text": "Zhiming Liu, Yujie Wei, Lei Feng, Xiu Su, Xiaobo Xia",
      "is_highlight": false,
      "score": 52.0,
      "summary": "This study identifies Task-Interfering Layers in vision-language models, demonstrating that selectively bypassing certain layers can enhance task performance without retraining.",
      "teaser_image": "https://arxiv.org/html/2602.01167/x1.png",
      "debug_abstract": "Current VLMs have demonstrated capabilities across a wide range of multimodal tasks. Typically, in a pretrained VLM, all layers are engaged by default to make predictions on downstream tasks. We find that intervening on a single layer, such as by zeroing its parameters, can improve the performance on certain tasks, indicating that some layers hinder rather than help downstream tasks. We systematically investigate how individual layers influence different tasks via layer intervention. Specifically, we measure the change in performance relative to the base model after intervening on each layer and observe improvements when bypassing specific layers. This improvement can be generalizable across models and datasets, indicating the presence of Task-Interfering Layers that harm downstream tasks&#39; performance. We introduce Task-Layer Interaction Vector, which quantifies the effect of intervening on each layer of a VLM given a task. These task-interfering layers exhibit task-specific sensitivity patterns: tasks requiring similar capabilities show consistent response trends under layer interventions, as evidenced by the high similarity in their task-layer interaction vectors. Inspired by these findings, we propose TaLo (Task-Adaptive Layer Knockout), a training-free, test-time adaptation method that dynamically identifies and bypasses the most interfering layer for a given task. Without parameter updates, TaLo improves performance across various models and datasets, including boosting Qwen-VL&#39;s accuracy on the Maps task in ScienceQA by up to 16.6%. Our work reveals an unexpected form of modularity in pretrained VLMs and provides a plug-and-play, training-free mechanism to unlock hidden capabilities at inference time. The source code will be publicly available."
    },
    {
      "title": "Learning to Accelerate Vision-Language-Action Models through Adaptive Visual Token Caching",
      "url": "https://arxiv.org/abs/2602.00686",
      "date": "2026-01-31",
      "authors_text": "Yujie Wei, Jiahan Fan, Jiyu Guo, Ruichen Zhen, Rui Shao",
      "is_highlight": false,
      "score": 84.0,
      "summary": "This paper presents a novel framework for enhancing Vision-Language-Action models' efficiency through adaptive token caching, achieving significant speedup and improved task success rates.",
      "teaser_image": "https://arxiv.org/html/2602.00686/x2.png",
      "debug_abstract": "Vision-Language-Action (VLA) models have demonstrated remarkable generalization capabilities in robotic manipulation tasks, yet their substantial computational overhead remains a critical obstacle to real-world deployment. Improving inference efficiency is therefore essential for practical robotic applications. Existing acceleration methods often rely on heuristic or static strategies--such as rule-based token caching or pruning--that are decoupled from task objectives and fail to adapt to dynamic scene changes. In this work, we reformulate inference acceleration as a learnable policy optimization problem and propose a novel framework that integrates a dynamic, task-aware decision-making process directly into the VLA model. At its core are two lightweight, cooperative modules: a Cached Token Selector, which determines which tokens should be reused, and a Cache Ratio Predictor, which controls how many tokens to reuse. Training these modules is non-trivial due to their discrete decisions. We address this by adopting a differentiable relaxation that allows gradient-based end-to-end optimization. Extensive experiments on the LIBERO and SIMPLER benchmarks, as well as real-robot evaluations, show that our method achieves a 1.76x wall-clock inference speedup while simultaneously improving the average success rate by 1.9 percentage points (from 75.0% to 76.9%) on LIBERO and by 5.0 percentage points on real-world tasks, significantly outperforming existing baselines. This work highlights the potential of learning task-aware computational allocation policies, paving the way for VLA models that are both powerful and efficient."
    },
    {
      "title": "ConLA: Contrastive Latent Action Learning from Human Videos for Robotic Manipulation",
      "url": "https://arxiv.org/abs/2602.00557",
      "date": "2026-01-31",
      "authors_text": "Weisheng Dai, Kai Lan, Jianyi Zhou, Bo Zhao, Xiu Su",
      "is_highlight": true,
      "score": 91.0,
      "summary": "ConLA introduces a contrastive disentanglement framework for unsupervised robotic policy learning from human videos, enhancing transferability and performance beyond traditional robot trajectory pretraining.",
      "teaser_image": "https://arxiv.org/html/2602.00557/x3.png",
      "debug_abstract": "Vision-Language-Action (VLA) models achieve preliminary generalization through pretraining on large scale robot teleoperation datasets. However, acquiring datasets that comprehensively cover diverse tasks and environments is extremely costly and difficult to scale. In contrast, human demonstration videos offer a rich and scalable source of diverse scenes and manipulation behaviors, yet their lack of explicit action supervision hinders direct utilization. Prior work leverages VQ-VAE based frameworks to learn latent actions from human videos in an unsupervised manner. Nevertheless, since the training objective primarily focuses on reconstructing visual appearances rather than capturing inter-frame dynamics, the learned representations tend to rely on spurious visual cues, leading to shortcut learning and entangled latent representations that hinder transferability. To address this, we propose ConLA, an unsupervised pretraining framework for learning robotic policies from human videos. ConLA introduces a contrastive disentanglement mechanism that leverages action category priors and temporal cues to isolate motion dynamics from visual content, effectively mitigating shortcut learning. Extensive experiments show that ConLA achieves strong performance across diverse benchmarks. Notably, by pretraining solely on human videos, our method for the first time surpasses the performance obtained with real robot trajectory pretraining, highlighting its ability to extract pure and semantically consistent latent action representations for scalable robot learning."
    },
    {
      "title": "APEX: A Decoupled Memory-based Explorer for Asynchronous Aerial Object Goal Navigation",
      "url": "https://arxiv.org/abs/2602.00551",
      "date": "2026-01-31",
      "authors_text": "Daoxuan Zhang, Ping Chen, Xiaobo Xia, Xiu Su, Ruichen Zhen",
      "is_highlight": false,
      "score": 88.0,
      "summary": "APEX is a novel hierarchical agent that enhances UAV navigation through efficient exploration and target acquisition using dynamic memory, reinforcement learning, and open-vocabulary detection.",
      "teaser_image": "https://arxiv.org/html/2602.00551/x1.png",
      "debug_abstract": "Aerial Object Goal Navigation, a challenging frontier in Embodied AI, requires an Unmanned Aerial Vehicle (UAV) agent to autonomously explore, reason, and identify a specific target using only visual perception and language description. However, existing methods struggle with the memorization of complex spatial representations in aerial environments, reliable and interpretable action decision-making, and inefficient exploration and information gathering. To address these challenges, we introduce \\textbf{APEX} (Aerial Parallel Explorer), a novel hierarchical agent designed for efficient exploration and target acquisition in complex aerial settings. APEX is built upon a modular, three-part architecture: 1) Dynamic Spatio-Semantic Mapping Memory, which leverages the zero-shot capability of a Vision-Language Model (VLM) to dynamically construct high-resolution 3D Attraction, Exploration, and Obstacle maps, serving as an interpretable memory mechanism. 2) Action Decision Module, trained with reinforcement learning, which translates this rich spatial understanding into a fine-grained and robust control policy. 3) Target Grounding Module, which employs an open-vocabulary detector to achieve definitive and generalizable target identification. All these components are integrated into a hierarchical, asynchronous, and parallel framework, effectively bypassing the VLM&#39;s inference latency and boosting the agent&#39;s proactivity in exploration. Extensive experiments show that APEX outperforms the previous state of the art by +4.2\\% SR and +2.8\\% SPL on challenging UAV-ON benchmarks, demonstrating its superior efficiency and the effectiveness of its hierarchical asynchronous design. Our source code is provided in \\href{this https URL}{GitHub}"
    },
    {
      "title": "Inject Once Survive Later: Backdooring Vision-Language-Action Models to Persist Through Downstream Fine-tuning",
      "url": "https://arxiv.org/abs/2602.00500",
      "date": "2026-01-31",
      "authors_text": "Jianyi Zhou, Yujie Wei, Ruichen Zhen, Bo Zhao, Xiaobo Xia",
      "is_highlight": true,
      "score": 60.0,
      "summary": "The paper introduces INFUSE, a robust backdoor attack framework for Vision-Language-Action models that survives user fine-tuning, maintaining high attack success rates while preserving performance.",
      "teaser_image": "https://arxiv.org/html/2602.00500/x2.png",
      "debug_abstract": "Vision-Language-Action (VLA) models have become foundational to modern embodied AI systems. By integrating visual perception, language understanding, and action planning, they enable general-purpose task execution across diverse environments. Despite their importance, the security of VLA models remains underexplored -- particularly in the context of backdoor attacks, which pose realistic threats in physical-world deployments. While recent methods attempt to inject backdoors into VLA models, these backdoors are easily erased during downstream adaptation, as user-side fine-tuning with clean data significantly alters model parameters, rendering them impractical for real-world applications. To address these challenges, we propose INFUSE (INjection into Fine-tUne-inSensitive modulEs), the first backdoor attack framework for VLA base models that remains effective even with arbitrary user fine-tuning. INFUSE begins by analyzing parameter sensitivity across diverse fine-tuning scenarios to identify modules that remain largely unchanged -- the fine-tune-insensitive modules. It then injects backdoors into these stable modules while freezing the rest, ensuring malicious behavior persists after extensive user fine-tuning. Comprehensive experiments across multiple VLA architectures demonstrate INFUSE&#39;s effectiveness. After user-side fine-tuning, INFUSE maintains mean attack success rates of 91.0% on simulation environments and 79.8% on real-world robot tasks, substantially surpassing BadVLA (38.8% and 36.6%, respectively), while preserving clean-task performance comparable to standard models. These results uncover a critical threat: backdoors implanted before distribution can persist through fine-tuning and remain effective at deployment."
    }
  ],
  "Microsystem Engineering and Robotics": [
    {
      "title": "FD-VLA: Force-Distilled Vision-Language-Action Model for Contact-Rich Manipulation",
      "url": "https://arxiv.org/abs/2602.02142",
      "date": "2026-02-02",
      "authors_text": "Ruiteng Zhao, Wenshuo Wang, Yicheng Ma, Xiaocong Li, Francis E. H. Tay",
      "is_highlight": false,
      "score": 0,
      "summary": "The FD-VLA framework enhances contact-rich manipulation by integrating force awareness through a novel distillation method, improving performance without physical force sensors.",
      "teaser_image": "https://arxiv.org/html/2602.02142/x1.png",
      "debug_abstract": "Force sensing is a crucial modality for Vision-Language-Action (VLA) frameworks, as it enables fine-grained perception and dexterous manipulation in contact-rich tasks. We present Force-Distilled VLA (FD-VLA), a novel framework that integrates force awareness into contact-rich manipulation without relying on physical force sensors. The core of our approach is a Force Distillation Module (FDM), which distills force by mapping a learnable query token, conditioned on visual observations and robot states, into a predicted force token aligned with the latent representation of actual force signals. During inference, this distilled force token is injected into the pretrained VLM, enabling force-aware reasoning while preserving the integrity of its vision-language semantics. This design provides two key benefits: first, it allows practical deployment across a wide range of robots that lack expensive or fragile force-torque sensors, thereby reducing hardware cost and complexity; second, the FDM introduces an additional force-vision-state fusion prior to the VLM, which improves cross-modal alignment and enhances perception-action robustness in contact-rich scenarios. Surprisingly, our physical experiments show that the distilled force token outperforms direct sensor force measurements as well as other baselines, which highlights the effectiveness of this force-distilled VLA approach."
    },
    {
      "title": "A Low-Cost Vision-Based Tactile Gripper with Pretraining Learning for Contact-Rich Manipulation",
      "url": "https://arxiv.org/abs/2602.00514",
      "date": "2026-01-31",
      "authors_text": "Yaohua Liu, Binkai Ou, Zicheng Qiu, Ce Hao, Yemin Wang",
      "is_highlight": false,
      "score": 11.0,
      "summary": "The LVTG gripper enhances contact-rich robotic manipulation through low-cost, durable visuo-tactile integration and improved policy learning via contrastive pretraining, achieving higher success rates.",
      "teaser_image": "https://arxiv.org/html/2602.00514/x1.png",
      "debug_abstract": "Robotic manipulation in contact-rich environments remains challenging, particularly when relying on conventional tactile sensors that suffer from limited sensing range, reliability, and cost-effectiveness. In this work, we present LVTG, a low-cost visuo-tactile gripper designed for stable, robust, and efficient physical interaction. Unlike existing visuo-tactile sensors, LVTG enables more effective and stable grasping of larger and heavier everyday objects, thanks to its enhanced tactile sensing area and greater opening angle. Its surface skin is made of highly wear-resistant material, significantly improving durability and extending operational lifespan. The integration of vision and tactile feedback allows LVTG to provide rich, high-fidelity sensory data, facilitating reliable perception during complex manipulation tasks. Furthermore, LVTG features a modular design that supports rapid maintenance and replacement. To effectively fuse vision and touch, We adopt a CLIP-inspired contrastive learning objective to align tactile embeddings with their corresponding visual observations, enabling a shared cross-modal representation space for visuo-tactile perception. This alignment improves the performance of an Action Chunking Transformer (ACT) policy in contact-rich manipulation, leading to more efficient data collection and more effective policy learning. Compared to the original ACT method, the proposed LVTG with pretraining achieves significantly higher success rates in manipulation tasks."
    },
    {
      "title": "TIDE: Trajectory-based Diagnostic Evaluation of Test-Time Improvement in LLM Agents",
      "url": "https://arxiv.org/abs/2602.02196",
      "date": "2026-02-02",
      "authors_text": "Hang Yan, Xinyu Che, Fangzhi Xu, Qiushi Sun, Zichen Ding",
      "is_highlight": false,
      "score": 20.0,
      "summary": "The TIDE framework evaluates Test-Time Improvement in LLM agents by analyzing task completion dynamics, recursive behaviors, and memory constraints to enhance performance optimization.",
      "teaser_image": "https://arxiv.org/html/2602.02196/x1.png",
      "debug_abstract": "Recent advances in autonomous LLM agents demonstrate their ability to improve performance through iterative interaction with the environment. We define this paradigm as Test-Time Improvement (TTI). However, the mechanisms under how and why TTI succeed or fail remain poorly understood, and existing evaluation metrics fail to capture their task optimization efficiency, behavior adaptation after erroneous actions, and the specific utility of working memory for task completion. To address these gaps, we propose Test-time Improvement Diagnostic Evaluation (TIDE), an agent-agnostic and environment-agnostic framework that decomposes TTI into three comprehensive and interconnected dimensions. The framework measures (1) the overall temporal dynamics of task completion and (2) identifies whether performance is primarily constrained by recursive looping behaviors or (3) by burdensome accumulated memory. Through extensive experiments across diverse agents and environments, TIDE highlights that improving agent performance requires more than scaling internal reasoning, calling for explicitly optimizing the interaction dynamics between the agent and the environment."
    },
    {
      "title": "UrbanGS: A Scalable and Efficient Architecture for Geometrically Accurate Large-Scene Reconstruction",
      "url": "https://arxiv.org/abs/2602.02089",
      "date": "2026-02-02",
      "authors_text": "Changbai Li, Haodong Zhu, Hanlin Chen, Xiuping Liang, Tongfei Chen",
      "is_highlight": false,
      "score": 22.0,
      "summary": "UrbanGS is a scalable framework that enhances geometric accuracy and memory efficiency in large-scale urban scene reconstruction through innovative regularization and adaptive Gaussian pruning techniques.",
      "teaser_image": "https://arxiv.org/html/2602.02089/figure/main.png",
      "debug_abstract": "While 3D Gaussian Splatting (3DGS) enables high-quality, real-time rendering for bounded scenes, its extension to large-scale urban environments gives rise to critical challenges in terms of geometric consistency, memory efficiency, and computational scalability. To address these issues, we present UrbanGS, a scalable reconstruction framework that effectively tackles these challenges for city-scale applications. First, we propose a Depth-Consistent D-Normal Regularization module. Unlike existing approaches that rely solely on monocular normal estimators, which can effectively update rotation parameters yet struggle to update position parameters, our method integrates D-Normal constraints with external depth supervision. This allows for comprehensive updates of all geometric parameters. By further incorporating an adaptive confidence weighting mechanism based on gradient consistency and inverse depth deviation, our approach significantly enhances multi-view depth alignment and geometric coherence, which effectively resolves the issue of geometric accuracy in complex large-scale scenes. To improve scalability, we introduce a Spatially Adaptive Gaussian Pruning (SAGP) strategy, which dynamically adjusts Gaussian density based on local geometric complexity and visibility to reduce redundancy. Additionally, a unified partitioning and view assignment scheme is designed to eliminate boundary artifacts and optimize computational load. Extensive experiments on multiple urban datasets demonstrate that UrbanGS achieves superior performance in rendering quality, geometric accuracy, and memory efficiency, providing a systematic solution for high-fidelity large-scale scene reconstruction."
    },
    {
      "title": "Beyond Precision: Training-Inference Mismatch is an Optimization Problem and Simple LR Scheduling Fixes It",
      "url": "https://arxiv.org/abs/2602.01826",
      "date": "2026-02-02",
      "authors_text": "Yaxiang Zhang, Yingru Li, Jiacai Liu, Jiawei Xu, Ziniu Li",
      "is_highlight": false,
      "score": 35.0,
      "summary": "This paper presents a dynamic learning rate scheduling method to mitigate training-inference mismatch and stabilize reinforcement learning in large language models by addressing gradient noise.",
      "teaser_image": "https://arxiv.org/html/2602.01826/Qwen3-4B_baseline_vs_importance_sampling_patches.png",
      "debug_abstract": "Reinforcement Learning (RL) for training Large Language Models is notoriously unstable. While recent studies attribute this to &#34;training inference mismatch stemming&#34; from inconsistent hybrid engines, standard remedies, such as Importance Sampling, might fail during extended training runs. In this work, we analyze this instability through the lens of optimization, demonstrating that gradient noise and training-inference mismatch escalate in tandem as training progresses. Meanwhile, we find that the mismatch can be effectively suppressed by shrinking the update size. Taken together, we deduce that the mismatch is not merely a static numerical discrepancy, but a dynamic failure coupled with the model&#39;s optimization. Based on this insight, we propose a simple yet effective solution: a specialized Learning Rate (LR) scheduler. Instead of pre-defined decay schedule in traditional LR scheduler, our method dynamically triggers LR decay based on response length, which we identify as a reliable early-warning signal for impending instability. Empirical evidence suggests that by reducing the learning rate as gradient noise rises, we can consistently stabilize RL training and keep the training-inference mismatch at a safe level."
    },
    {
      "title": "ReCALL: Recalibrating Capability Degradation for MLLM-based Composed Image Retrieval",
      "url": "https://arxiv.org/abs/2602.01639",
      "date": "2026-02-02",
      "authors_text": "Tianyu Yang, ChenWei He, Xiangzhao Hao, Tianyue Wang, Jiarui Guo",
      "is_highlight": false,
      "score": 42.0,
      "summary": "ReCALL addresses capability degradation in MLLM-based Composed Image Retrieval by diagnosing issues, generating corrective instructions, and refining the retriever through continual training.",
      "teaser_image": "https://arxiv.org/html/2602.01639/x1.png",
      "debug_abstract": "Composed Image Retrieval (CIR) aims to retrieve target images based on a hybrid query comprising a reference image and a modification text. Early dual-tower Vision-Language Models (VLMs) struggle with cross-modality compositional reasoning required for this task. Recently, adapting generative Multimodal Large Language Models (MLLMs) for retrieval offers a promising direction. However, we identify that this adaptation strategy overlooks a fundamental issue: adapting a generative MLLM into a single-embedding discriminative retriever triggers a paradigm conflict, which leads to Capability Degradation - the deterioration of native fine-grained reasoning after retrieval adaptation. To address this challenge, we propose ReCALL (Recalibrating Capability Degradation), a model-agnostic framework that follows a diagnose-generate-refine pipeline: Firstly, we diagnose cognitive blind spots of the retriever via self-guided informative instance mining. Next, we generate corrective instructions and triplets by CoT prompting the foundation MLLM and conduct quality control with VQA-based consistency filtering. Finally, we refine the retriever through continual training on these triplets with a grouped contrastive scheme, thereby internalizing fine-grained visual-semantic distinctions and realigning the discriminative embedding space of retriever with intrinsic compositional reasoning within the MLLM. Extensive experiments on CIRR and FashionIQ show that ReCALL consistently recalibrates degraded capabilities and achieves state-of-the-art performance. Code will be released soon."
    },
    {
      "title": "PRISM: Festina Lente Proactivity -- Risk-Sensitive, Uncertainty-Aware Deliberation for Proactive Agents",
      "url": "https://arxiv.org/abs/2602.01532",
      "date": "2026-02-02",
      "authors_text": "Yuxuan Fu, Xiaoyu Tan, Teqi Hao, Chen Zhan, Xihe Qiu",
      "is_highlight": false,
      "score": 45.0,
      "summary": "PRISM introduces a decision-theoretic framework for proactive agents that optimizes intervention timing and accuracy, significantly reducing false alarms and enhancing performance on ProactiveBench.",
      "teaser_image": "https://arxiv.org/html/2602.01532/fig2.png",
      "debug_abstract": "Proactive agents must decide not only what to say but also whether and when to intervene. Many current systems rely on brittle heuristics or indiscriminate long reasoning, which offers little control over the benefit-burden tradeoff. We formulate the problem as cost-sensitive selective intervention and present PRISM, a novel framework that couples a decision-theoretic gate with a dual-process reasoning architecture. At inference time, the agent intervenes only when a calibrated probability of user acceptance exceeds a threshold derived from asymmetric costs of missed help and false alarms. Inspired by festina lente (Latin: &#34;make haste slowly&#34;), we gate by an acceptance-calibrated, cost-derived threshold and invoke a resource-intensive Slow mode with counterfactual checks only near the decision boundary, concentrating computation on ambiguous and high-stakes cases. Training uses gate-aligned, schema-locked distillation: a teacher running the full PRISM pipeline provides dense, executable supervision on unlabeled interaction traces, while the student learns a response policy that is explicitly decoupled from the intervention gate to enable tunable and auditable control. On ProactiveBench, PRISM reduces false alarms by 22.78% and improves F1 by 20.14% over strong baselines. These results show that principled decision-theoretic gating, paired with selective slow reasoning and aligned distillation, yields proactive agents that are precise, computationally efficient, and controllable. To facilitate reproducibility, we release our code, models, and resources at this https URL all experiments use the open-source ProactiveBench benchmark."
    },
    {
      "title": "Do All Individual Layers Help? An Empirical Study of Task-Interfering Layers in Vision-Language Models",
      "url": "https://arxiv.org/abs/2602.01167",
      "date": "2026-02-01",
      "authors_text": "Zhiming Liu, Yujie Wei, Lei Feng, Xiu Su, Xiaobo Xia",
      "is_highlight": false,
      "score": 52.0,
      "summary": "This study identifies Task-Interfering Layers in vision-language models, demonstrating that selectively bypassing certain layers can enhance task performance without retraining.",
      "teaser_image": "https://arxiv.org/html/2602.01167/x1.png",
      "debug_abstract": "Current VLMs have demonstrated capabilities across a wide range of multimodal tasks. Typically, in a pretrained VLM, all layers are engaged by default to make predictions on downstream tasks. We find that intervening on a single layer, such as by zeroing its parameters, can improve the performance on certain tasks, indicating that some layers hinder rather than help downstream tasks. We systematically investigate how individual layers influence different tasks via layer intervention. Specifically, we measure the change in performance relative to the base model after intervening on each layer and observe improvements when bypassing specific layers. This improvement can be generalizable across models and datasets, indicating the presence of Task-Interfering Layers that harm downstream tasks&#39; performance. We introduce Task-Layer Interaction Vector, which quantifies the effect of intervening on each layer of a VLM given a task. These task-interfering layers exhibit task-specific sensitivity patterns: tasks requiring similar capabilities show consistent response trends under layer interventions, as evidenced by the high similarity in their task-layer interaction vectors. Inspired by these findings, we propose TaLo (Task-Adaptive Layer Knockout), a training-free, test-time adaptation method that dynamically identifies and bypasses the most interfering layer for a given task. Without parameter updates, TaLo improves performance across various models and datasets, including boosting Qwen-VL&#39;s accuracy on the Maps task in ScienceQA by up to 16.6%. Our work reveals an unexpected form of modularity in pretrained VLMs and provides a plug-and-play, training-free mechanism to unlock hidden capabilities at inference time. The source code will be publicly available."
    },
    {
      "title": "Unveiling the Cognitive Compass: Theory-of-Mind-Guided Multimodal Emotion Reasoning",
      "url": "https://arxiv.org/abs/2602.00971",
      "date": "2026-02-01",
      "authors_text": "Meng Luo, Bobo Li, Shanqing Xu, Shize Zhang, Qiuchan Chen",
      "is_highlight": false,
      "score": 62.0,
      "summary": "The paper presents HitEmotion, a benchmark for assessing and enhancing multimodal large language models' emotional reasoning through Theory of Mind-guided methods and reinforcement learning.",
      "teaser_image": "https://arxiv.org/html/2602.00971/figures/teaser.png",
      "debug_abstract": "Despite rapid progress in multimodal large language models (MLLMs), their capability for deep emotional understanding remains limited. We argue that genuine affective intelligence requires explicit modeling of Theory of Mind (ToM), the cognitive substrate from which emotions arise. To this end, we introduce HitEmotion, a ToM-grounded hierarchical benchmark that diagnoses capability breakpoints across increasing levels of cognitive depth. Second, we propose a ToM-guided reasoning chain that tracks mental states and calibrates cross-modal evidence to achieve faithful emotional reasoning. We further introduce TMPO, a reinforcement learning method that uses intermediate mental states as process-level supervision to guide and strengthen model reasoning. Extensive experiments show that HitEmotion exposes deep emotional reasoning deficits in state-of-the-art models, especially on cognitively demanding tasks. In evaluation, the ToM-guided reasoning chain and TMPO improve end-task accuracy and yield more faithful, more coherent rationales. In conclusion, our work provides the research community with a practical toolkit for evaluating and enhancing the cognition-based emotional understanding capabilities of MLLMs. Our dataset and code are available at: this https URL."
    },
    {
      "title": "Self-Guard: Defending Large Reasoning Models via enhanced self-reflection",
      "url": "https://arxiv.org/abs/2602.00707",
      "date": "2026-01-31",
      "authors_text": "Jingnan Zheng, Jingjun Xu, Yanzhen Luo, Chenhang Cui, Gelei Deng",
      "is_highlight": false,
      "score": 50.0,
      "summary": "Self-Guard is a lightweight framework enhancing safety compliance in Large Reasoning Models by promoting self-reflection and mitigating risks without sacrificing model utility.",
      "teaser_image": "https://arxiv.org/html/2602.00707/materials/workflow.png",
      "debug_abstract": "The emergence of Large Reasoning Models (LRMs) introduces a new paradigm of explicit reasoning, enabling remarkable advances yet posing unique risks such as reasoning manipulation and information leakage. To mitigate these risks, current alignment strategies predominantly rely on heavy post-training paradigms or external interventions. However, these approaches are often computationally intensive and fail to address the inherent awareness-compliance gap, a critical misalignment where models recognize potential risks yet prioritize following user instructions due to their sycophantic tendencies. To address these limitations, we propose Self-Guard, a lightweight safety defense framework that reinforces safety compliance at the representational level. Self-Guard operates through two principal stages: (1) safety-oriented prompting, which activates the model&#39;s latent safety awareness to evoke spontaneous reflection, and (2) safety activation steering, which extracts the resulting directional shift in the hidden state space and amplifies it to ensure that safety compliance prevails over sycophancy during inference. Experiments demonstrate that Self-Guard effectively bridges the awareness-compliance gap, achieving robust safety performance without compromising model utility. Furthermore, Self-Guard exhibits strong generalization across diverse unseen risks and varying model scales, offering a cost-efficient solution for LRM safety alignment."
    },
    {
      "title": "APEX: A Decoupled Memory-based Explorer for Asynchronous Aerial Object Goal Navigation",
      "url": "https://arxiv.org/abs/2602.00551",
      "date": "2026-01-31",
      "authors_text": "Daoxuan Zhang, Ping Chen, Xiaobo Xia, Xiu Su, Ruichen Zhen",
      "is_highlight": false,
      "score": 88.0,
      "summary": "APEX is a novel hierarchical agent that enhances UAV navigation through efficient exploration and target acquisition using dynamic memory, reinforcement learning, and open-vocabulary detection.",
      "teaser_image": "https://arxiv.org/html/2602.00551/x1.png",
      "debug_abstract": "Aerial Object Goal Navigation, a challenging frontier in Embodied AI, requires an Unmanned Aerial Vehicle (UAV) agent to autonomously explore, reason, and identify a specific target using only visual perception and language description. However, existing methods struggle with the memorization of complex spatial representations in aerial environments, reliable and interpretable action decision-making, and inefficient exploration and information gathering. To address these challenges, we introduce \\textbf{APEX} (Aerial Parallel Explorer), a novel hierarchical agent designed for efficient exploration and target acquisition in complex aerial settings. APEX is built upon a modular, three-part architecture: 1) Dynamic Spatio-Semantic Mapping Memory, which leverages the zero-shot capability of a Vision-Language Model (VLM) to dynamically construct high-resolution 3D Attraction, Exploration, and Obstacle maps, serving as an interpretable memory mechanism. 2) Action Decision Module, trained with reinforcement learning, which translates this rich spatial understanding into a fine-grained and robust control policy. 3) Target Grounding Module, which employs an open-vocabulary detector to achieve definitive and generalizable target identification. All these components are integrated into a hierarchical, asynchronous, and parallel framework, effectively bypassing the VLM&#39;s inference latency and boosting the agent&#39;s proactivity in exploration. Extensive experiments show that APEX outperforms the previous state of the art by +4.2\\% SR and +2.8\\% SPL on challenging UAV-ON benchmarks, demonstrating its superior efficiency and the effectiveness of its hierarchical asynchronous design. Our source code is provided in \\href{this https URL}{GitHub}"
    },
    {
      "title": "Inject Once Survive Later: Backdooring Vision-Language-Action Models to Persist Through Downstream Fine-tuning",
      "url": "https://arxiv.org/abs/2602.00500",
      "date": "2026-01-31",
      "authors_text": "Jianyi Zhou, Yujie Wei, Ruichen Zhen, Bo Zhao, Xiaobo Xia",
      "is_highlight": true,
      "score": 60.0,
      "summary": "The paper introduces INFUSE, a robust backdoor attack framework for Vision-Language-Action models that survives user fine-tuning, maintaining high attack success rates while preserving performance.",
      "teaser_image": "https://arxiv.org/html/2602.00500/x2.png",
      "debug_abstract": "Vision-Language-Action (VLA) models have become foundational to modern embodied AI systems. By integrating visual perception, language understanding, and action planning, they enable general-purpose task execution across diverse environments. Despite their importance, the security of VLA models remains underexplored -- particularly in the context of backdoor attacks, which pose realistic threats in physical-world deployments. While recent methods attempt to inject backdoors into VLA models, these backdoors are easily erased during downstream adaptation, as user-side fine-tuning with clean data significantly alters model parameters, rendering them impractical for real-world applications. To address these challenges, we propose INFUSE (INjection into Fine-tUne-inSensitive modulEs), the first backdoor attack framework for VLA base models that remains effective even with arbitrary user fine-tuning. INFUSE begins by analyzing parameter sensitivity across diverse fine-tuning scenarios to identify modules that remain largely unchanged -- the fine-tune-insensitive modules. It then injects backdoors into these stable modules while freezing the rest, ensuring malicious behavior persists after extensive user fine-tuning. Comprehensive experiments across multiple VLA architectures demonstrate INFUSE&#39;s effectiveness. After user-side fine-tuning, INFUSE maintains mean attack success rates of 91.0% on simulation environments and 79.8% on real-world robot tasks, substantially surpassing BadVLA (38.8% and 36.6%, respectively), while preserving clean-task performance comparable to standard models. These results uncover a critical threat: backdoors implanted before distribution can persist through fine-tuning and remain effective at deployment."
    }
  ],
  "Synteraction Lab": [
    {
      "title": "FD-VLA: Force-Distilled Vision-Language-Action Model for Contact-Rich Manipulation",
      "url": "https://arxiv.org/abs/2602.02142",
      "date": "2026-02-02",
      "authors_text": "Ruiteng Zhao, Wenshuo Wang, Yicheng Ma, Xiaocong Li, Francis E. H. Tay",
      "is_highlight": false,
      "score": 0,
      "summary": "The FD-VLA framework enhances contact-rich manipulation by integrating force awareness through a novel distillation method, improving performance without physical force sensors.",
      "teaser_image": "https://arxiv.org/html/2602.02142/x1.png",
      "debug_abstract": "Force sensing is a crucial modality for Vision-Language-Action (VLA) frameworks, as it enables fine-grained perception and dexterous manipulation in contact-rich tasks. We present Force-Distilled VLA (FD-VLA), a novel framework that integrates force awareness into contact-rich manipulation without relying on physical force sensors. The core of our approach is a Force Distillation Module (FDM), which distills force by mapping a learnable query token, conditioned on visual observations and robot states, into a predicted force token aligned with the latent representation of actual force signals. During inference, this distilled force token is injected into the pretrained VLM, enabling force-aware reasoning while preserving the integrity of its vision-language semantics. This design provides two key benefits: first, it allows practical deployment across a wide range of robots that lack expensive or fragile force-torque sensors, thereby reducing hardware cost and complexity; second, the FDM introduces an additional force-vision-state fusion prior to the VLM, which improves cross-modal alignment and enhances perception-action robustness in contact-rich scenarios. Surprisingly, our physical experiments show that the distilled force token outperforms direct sensor force measurements as well as other baselines, which highlights the effectiveness of this force-distilled VLA approach."
    },
    {
      "title": "A Low-Cost Vision-Based Tactile Gripper with Pretraining Learning for Contact-Rich Manipulation",
      "url": "https://arxiv.org/abs/2602.00514",
      "date": "2026-01-31",
      "authors_text": "Yaohua Liu, Binkai Ou, Zicheng Qiu, Ce Hao, Yemin Wang",
      "is_highlight": false,
      "score": 11.0,
      "summary": "The LVTG gripper enhances contact-rich robotic manipulation through low-cost, durable visuo-tactile integration and improved policy learning via contrastive pretraining, achieving higher success rates.",
      "teaser_image": "https://arxiv.org/html/2602.00514/x1.png",
      "debug_abstract": "Robotic manipulation in contact-rich environments remains challenging, particularly when relying on conventional tactile sensors that suffer from limited sensing range, reliability, and cost-effectiveness. In this work, we present LVTG, a low-cost visuo-tactile gripper designed for stable, robust, and efficient physical interaction. Unlike existing visuo-tactile sensors, LVTG enables more effective and stable grasping of larger and heavier everyday objects, thanks to its enhanced tactile sensing area and greater opening angle. Its surface skin is made of highly wear-resistant material, significantly improving durability and extending operational lifespan. The integration of vision and tactile feedback allows LVTG to provide rich, high-fidelity sensory data, facilitating reliable perception during complex manipulation tasks. Furthermore, LVTG features a modular design that supports rapid maintenance and replacement. To effectively fuse vision and touch, We adopt a CLIP-inspired contrastive learning objective to align tactile embeddings with their corresponding visual observations, enabling a shared cross-modal representation space for visuo-tactile perception. This alignment improves the performance of an Action Chunking Transformer (ACT) policy in contact-rich manipulation, leading to more efficient data collection and more effective policy learning. Compared to the original ACT method, the proposed LVTG with pretraining achieves significantly higher success rates in manipulation tasks."
    },
    {
      "title": "TIDE: Trajectory-based Diagnostic Evaluation of Test-Time Improvement in LLM Agents",
      "url": "https://arxiv.org/abs/2602.02196",
      "date": "2026-02-02",
      "authors_text": "Hang Yan, Xinyu Che, Fangzhi Xu, Qiushi Sun, Zichen Ding",
      "is_highlight": false,
      "score": 20.0,
      "summary": "The TIDE framework evaluates Test-Time Improvement in LLM agents by analyzing task completion dynamics, recursive behaviors, and memory constraints to enhance performance optimization.",
      "teaser_image": "https://arxiv.org/html/2602.02196/x1.png",
      "debug_abstract": "Recent advances in autonomous LLM agents demonstrate their ability to improve performance through iterative interaction with the environment. We define this paradigm as Test-Time Improvement (TTI). However, the mechanisms under how and why TTI succeed or fail remain poorly understood, and existing evaluation metrics fail to capture their task optimization efficiency, behavior adaptation after erroneous actions, and the specific utility of working memory for task completion. To address these gaps, we propose Test-time Improvement Diagnostic Evaluation (TIDE), an agent-agnostic and environment-agnostic framework that decomposes TTI into three comprehensive and interconnected dimensions. The framework measures (1) the overall temporal dynamics of task completion and (2) identifies whether performance is primarily constrained by recursive looping behaviors or (3) by burdensome accumulated memory. Through extensive experiments across diverse agents and environments, TIDE highlights that improving agent performance requires more than scaling internal reasoning, calling for explicitly optimizing the interaction dynamics between the agent and the environment."
    },
    {
      "title": "UrbanGS: A Scalable and Efficient Architecture for Geometrically Accurate Large-Scene Reconstruction",
      "url": "https://arxiv.org/abs/2602.02089",
      "date": "2026-02-02",
      "authors_text": "Changbai Li, Haodong Zhu, Hanlin Chen, Xiuping Liang, Tongfei Chen",
      "is_highlight": false,
      "score": 22.0,
      "summary": "UrbanGS is a scalable framework that enhances geometric accuracy and memory efficiency in large-scale urban scene reconstruction through innovative regularization and adaptive Gaussian pruning techniques.",
      "teaser_image": "https://arxiv.org/html/2602.02089/figure/main.png",
      "debug_abstract": "While 3D Gaussian Splatting (3DGS) enables high-quality, real-time rendering for bounded scenes, its extension to large-scale urban environments gives rise to critical challenges in terms of geometric consistency, memory efficiency, and computational scalability. To address these issues, we present UrbanGS, a scalable reconstruction framework that effectively tackles these challenges for city-scale applications. First, we propose a Depth-Consistent D-Normal Regularization module. Unlike existing approaches that rely solely on monocular normal estimators, which can effectively update rotation parameters yet struggle to update position parameters, our method integrates D-Normal constraints with external depth supervision. This allows for comprehensive updates of all geometric parameters. By further incorporating an adaptive confidence weighting mechanism based on gradient consistency and inverse depth deviation, our approach significantly enhances multi-view depth alignment and geometric coherence, which effectively resolves the issue of geometric accuracy in complex large-scale scenes. To improve scalability, we introduce a Spatially Adaptive Gaussian Pruning (SAGP) strategy, which dynamically adjusts Gaussian density based on local geometric complexity and visibility to reduce redundancy. Additionally, a unified partitioning and view assignment scheme is designed to eliminate boundary artifacts and optimize computational load. Extensive experiments on multiple urban datasets demonstrate that UrbanGS achieves superior performance in rendering quality, geometric accuracy, and memory efficiency, providing a systematic solution for high-fidelity large-scale scene reconstruction."
    },
    {
      "title": "Beyond Precision: Training-Inference Mismatch is an Optimization Problem and Simple LR Scheduling Fixes It",
      "url": "https://arxiv.org/abs/2602.01826",
      "date": "2026-02-02",
      "authors_text": "Yaxiang Zhang, Yingru Li, Jiacai Liu, Jiawei Xu, Ziniu Li",
      "is_highlight": false,
      "score": 35.0,
      "summary": "This paper presents a dynamic learning rate scheduling method to mitigate training-inference mismatch and stabilize reinforcement learning in large language models by addressing gradient noise.",
      "teaser_image": "https://arxiv.org/html/2602.01826/Qwen3-4B_baseline_vs_importance_sampling_patches.png",
      "debug_abstract": "Reinforcement Learning (RL) for training Large Language Models is notoriously unstable. While recent studies attribute this to &#34;training inference mismatch stemming&#34; from inconsistent hybrid engines, standard remedies, such as Importance Sampling, might fail during extended training runs. In this work, we analyze this instability through the lens of optimization, demonstrating that gradient noise and training-inference mismatch escalate in tandem as training progresses. Meanwhile, we find that the mismatch can be effectively suppressed by shrinking the update size. Taken together, we deduce that the mismatch is not merely a static numerical discrepancy, but a dynamic failure coupled with the model&#39;s optimization. Based on this insight, we propose a simple yet effective solution: a specialized Learning Rate (LR) scheduler. Instead of pre-defined decay schedule in traditional LR scheduler, our method dynamically triggers LR decay based on response length, which we identify as a reliable early-warning signal for impending instability. Empirical evidence suggests that by reducing the learning rate as gradient noise rises, we can consistently stabilize RL training and keep the training-inference mismatch at a safe level."
    },
    {
      "title": "ReCALL: Recalibrating Capability Degradation for MLLM-based Composed Image Retrieval",
      "url": "https://arxiv.org/abs/2602.01639",
      "date": "2026-02-02",
      "authors_text": "Tianyu Yang, ChenWei He, Xiangzhao Hao, Tianyue Wang, Jiarui Guo",
      "is_highlight": false,
      "score": 42.0,
      "summary": "ReCALL addresses capability degradation in MLLM-based Composed Image Retrieval by diagnosing issues, generating corrective instructions, and refining the retriever through continual training.",
      "teaser_image": "https://arxiv.org/html/2602.01639/x1.png",
      "debug_abstract": "Composed Image Retrieval (CIR) aims to retrieve target images based on a hybrid query comprising a reference image and a modification text. Early dual-tower Vision-Language Models (VLMs) struggle with cross-modality compositional reasoning required for this task. Recently, adapting generative Multimodal Large Language Models (MLLMs) for retrieval offers a promising direction. However, we identify that this adaptation strategy overlooks a fundamental issue: adapting a generative MLLM into a single-embedding discriminative retriever triggers a paradigm conflict, which leads to Capability Degradation - the deterioration of native fine-grained reasoning after retrieval adaptation. To address this challenge, we propose ReCALL (Recalibrating Capability Degradation), a model-agnostic framework that follows a diagnose-generate-refine pipeline: Firstly, we diagnose cognitive blind spots of the retriever via self-guided informative instance mining. Next, we generate corrective instructions and triplets by CoT prompting the foundation MLLM and conduct quality control with VQA-based consistency filtering. Finally, we refine the retriever through continual training on these triplets with a grouped contrastive scheme, thereby internalizing fine-grained visual-semantic distinctions and realigning the discriminative embedding space of retriever with intrinsic compositional reasoning within the MLLM. Extensive experiments on CIRR and FashionIQ show that ReCALL consistently recalibrates degraded capabilities and achieves state-of-the-art performance. Code will be released soon."
    },
    {
      "title": "PRISM: Festina Lente Proactivity -- Risk-Sensitive, Uncertainty-Aware Deliberation for Proactive Agents",
      "url": "https://arxiv.org/abs/2602.01532",
      "date": "2026-02-02",
      "authors_text": "Yuxuan Fu, Xiaoyu Tan, Teqi Hao, Chen Zhan, Xihe Qiu",
      "is_highlight": false,
      "score": 45.0,
      "summary": "PRISM introduces a decision-theoretic framework for proactive agents that optimizes intervention timing and accuracy, significantly reducing false alarms and enhancing performance on ProactiveBench.",
      "teaser_image": "https://arxiv.org/html/2602.01532/fig2.png",
      "debug_abstract": "Proactive agents must decide not only what to say but also whether and when to intervene. Many current systems rely on brittle heuristics or indiscriminate long reasoning, which offers little control over the benefit-burden tradeoff. We formulate the problem as cost-sensitive selective intervention and present PRISM, a novel framework that couples a decision-theoretic gate with a dual-process reasoning architecture. At inference time, the agent intervenes only when a calibrated probability of user acceptance exceeds a threshold derived from asymmetric costs of missed help and false alarms. Inspired by festina lente (Latin: &#34;make haste slowly&#34;), we gate by an acceptance-calibrated, cost-derived threshold and invoke a resource-intensive Slow mode with counterfactual checks only near the decision boundary, concentrating computation on ambiguous and high-stakes cases. Training uses gate-aligned, schema-locked distillation: a teacher running the full PRISM pipeline provides dense, executable supervision on unlabeled interaction traces, while the student learns a response policy that is explicitly decoupled from the intervention gate to enable tunable and auditable control. On ProactiveBench, PRISM reduces false alarms by 22.78% and improves F1 by 20.14% over strong baselines. These results show that principled decision-theoretic gating, paired with selective slow reasoning and aligned distillation, yields proactive agents that are precise, computationally efficient, and controllable. To facilitate reproducibility, we release our code, models, and resources at this https URL all experiments use the open-source ProactiveBench benchmark."
    },
    {
      "title": "Do All Individual Layers Help? An Empirical Study of Task-Interfering Layers in Vision-Language Models",
      "url": "https://arxiv.org/abs/2602.01167",
      "date": "2026-02-01",
      "authors_text": "Zhiming Liu, Yujie Wei, Lei Feng, Xiu Su, Xiaobo Xia",
      "is_highlight": false,
      "score": 52.0,
      "summary": "This study identifies Task-Interfering Layers in vision-language models, demonstrating that selectively bypassing certain layers can enhance task performance without retraining.",
      "teaser_image": "https://arxiv.org/html/2602.01167/x1.png",
      "debug_abstract": "Current VLMs have demonstrated capabilities across a wide range of multimodal tasks. Typically, in a pretrained VLM, all layers are engaged by default to make predictions on downstream tasks. We find that intervening on a single layer, such as by zeroing its parameters, can improve the performance on certain tasks, indicating that some layers hinder rather than help downstream tasks. We systematically investigate how individual layers influence different tasks via layer intervention. Specifically, we measure the change in performance relative to the base model after intervening on each layer and observe improvements when bypassing specific layers. This improvement can be generalizable across models and datasets, indicating the presence of Task-Interfering Layers that harm downstream tasks&#39; performance. We introduce Task-Layer Interaction Vector, which quantifies the effect of intervening on each layer of a VLM given a task. These task-interfering layers exhibit task-specific sensitivity patterns: tasks requiring similar capabilities show consistent response trends under layer interventions, as evidenced by the high similarity in their task-layer interaction vectors. Inspired by these findings, we propose TaLo (Task-Adaptive Layer Knockout), a training-free, test-time adaptation method that dynamically identifies and bypasses the most interfering layer for a given task. Without parameter updates, TaLo improves performance across various models and datasets, including boosting Qwen-VL&#39;s accuracy on the Maps task in ScienceQA by up to 16.6%. Our work reveals an unexpected form of modularity in pretrained VLMs and provides a plug-and-play, training-free mechanism to unlock hidden capabilities at inference time. The source code will be publicly available."
    },
    {
      "title": "Unveiling the Cognitive Compass: Theory-of-Mind-Guided Multimodal Emotion Reasoning",
      "url": "https://arxiv.org/abs/2602.00971",
      "date": "2026-02-01",
      "authors_text": "Meng Luo, Bobo Li, Shanqing Xu, Shize Zhang, Qiuchan Chen",
      "is_highlight": false,
      "score": 62.0,
      "summary": "The paper presents HitEmotion, a benchmark for assessing and enhancing multimodal large language models' emotional reasoning through Theory of Mind-guided methods and reinforcement learning.",
      "teaser_image": "https://arxiv.org/html/2602.00971/figures/teaser.png",
      "debug_abstract": "Despite rapid progress in multimodal large language models (MLLMs), their capability for deep emotional understanding remains limited. We argue that genuine affective intelligence requires explicit modeling of Theory of Mind (ToM), the cognitive substrate from which emotions arise. To this end, we introduce HitEmotion, a ToM-grounded hierarchical benchmark that diagnoses capability breakpoints across increasing levels of cognitive depth. Second, we propose a ToM-guided reasoning chain that tracks mental states and calibrates cross-modal evidence to achieve faithful emotional reasoning. We further introduce TMPO, a reinforcement learning method that uses intermediate mental states as process-level supervision to guide and strengthen model reasoning. Extensive experiments show that HitEmotion exposes deep emotional reasoning deficits in state-of-the-art models, especially on cognitively demanding tasks. In evaluation, the ToM-guided reasoning chain and TMPO improve end-task accuracy and yield more faithful, more coherent rationales. In conclusion, our work provides the research community with a practical toolkit for evaluating and enhancing the cognition-based emotional understanding capabilities of MLLMs. Our dataset and code are available at: this https URL."
    },
    {
      "title": "Self-Guard: Defending Large Reasoning Models via enhanced self-reflection",
      "url": "https://arxiv.org/abs/2602.00707",
      "date": "2026-01-31",
      "authors_text": "Jingnan Zheng, Jingjun Xu, Yanzhen Luo, Chenhang Cui, Gelei Deng",
      "is_highlight": false,
      "score": 50.0,
      "summary": "Self-Guard is a lightweight framework enhancing safety compliance in Large Reasoning Models by promoting self-reflection and mitigating risks without sacrificing model utility.",
      "teaser_image": "https://arxiv.org/html/2602.00707/materials/workflow.png",
      "debug_abstract": "The emergence of Large Reasoning Models (LRMs) introduces a new paradigm of explicit reasoning, enabling remarkable advances yet posing unique risks such as reasoning manipulation and information leakage. To mitigate these risks, current alignment strategies predominantly rely on heavy post-training paradigms or external interventions. However, these approaches are often computationally intensive and fail to address the inherent awareness-compliance gap, a critical misalignment where models recognize potential risks yet prioritize following user instructions due to their sycophantic tendencies. To address these limitations, we propose Self-Guard, a lightweight safety defense framework that reinforces safety compliance at the representational level. Self-Guard operates through two principal stages: (1) safety-oriented prompting, which activates the model&#39;s latent safety awareness to evoke spontaneous reflection, and (2) safety activation steering, which extracts the resulting directional shift in the hidden state space and amplifies it to ensure that safety compliance prevails over sycophancy during inference. Experiments demonstrate that Self-Guard effectively bridges the awareness-compliance gap, achieving robust safety performance without compromising model utility. Furthermore, Self-Guard exhibits strong generalization across diverse unseen risks and varying model scales, offering a cost-efficient solution for LRM safety alignment."
    },
    {
      "title": "APEX: A Decoupled Memory-based Explorer for Asynchronous Aerial Object Goal Navigation",
      "url": "https://arxiv.org/abs/2602.00551",
      "date": "2026-01-31",
      "authors_text": "Daoxuan Zhang, Ping Chen, Xiaobo Xia, Xiu Su, Ruichen Zhen",
      "is_highlight": false,
      "score": 88.0,
      "summary": "APEX is a novel hierarchical agent that enhances UAV navigation through efficient exploration and target acquisition using dynamic memory, reinforcement learning, and open-vocabulary detection.",
      "teaser_image": "https://arxiv.org/html/2602.00551/x1.png",
      "debug_abstract": "Aerial Object Goal Navigation, a challenging frontier in Embodied AI, requires an Unmanned Aerial Vehicle (UAV) agent to autonomously explore, reason, and identify a specific target using only visual perception and language description. However, existing methods struggle with the memorization of complex spatial representations in aerial environments, reliable and interpretable action decision-making, and inefficient exploration and information gathering. To address these challenges, we introduce \\textbf{APEX} (Aerial Parallel Explorer), a novel hierarchical agent designed for efficient exploration and target acquisition in complex aerial settings. APEX is built upon a modular, three-part architecture: 1) Dynamic Spatio-Semantic Mapping Memory, which leverages the zero-shot capability of a Vision-Language Model (VLM) to dynamically construct high-resolution 3D Attraction, Exploration, and Obstacle maps, serving as an interpretable memory mechanism. 2) Action Decision Module, trained with reinforcement learning, which translates this rich spatial understanding into a fine-grained and robust control policy. 3) Target Grounding Module, which employs an open-vocabulary detector to achieve definitive and generalizable target identification. All these components are integrated into a hierarchical, asynchronous, and parallel framework, effectively bypassing the VLM&#39;s inference latency and boosting the agent&#39;s proactivity in exploration. Extensive experiments show that APEX outperforms the previous state of the art by +4.2\\% SR and +2.8\\% SPL on challenging UAV-ON benchmarks, demonstrating its superior efficiency and the effectiveness of its hierarchical asynchronous design. Our source code is provided in \\href{this https URL}{GitHub}"
    },
    {
      "title": "Inject Once Survive Later: Backdooring Vision-Language-Action Models to Persist Through Downstream Fine-tuning",
      "url": "https://arxiv.org/abs/2602.00500",
      "date": "2026-01-31",
      "authors_text": "Jianyi Zhou, Yujie Wei, Ruichen Zhen, Bo Zhao, Xiaobo Xia",
      "is_highlight": true,
      "score": 60.0,
      "summary": "The paper introduces INFUSE, a robust backdoor attack framework for Vision-Language-Action models that survives user fine-tuning, maintaining high attack success rates while preserving performance.",
      "teaser_image": "https://arxiv.org/html/2602.00500/x2.png",
      "debug_abstract": "Vision-Language-Action (VLA) models have become foundational to modern embodied AI systems. By integrating visual perception, language understanding, and action planning, they enable general-purpose task execution across diverse environments. Despite their importance, the security of VLA models remains underexplored -- particularly in the context of backdoor attacks, which pose realistic threats in physical-world deployments. While recent methods attempt to inject backdoors into VLA models, these backdoors are easily erased during downstream adaptation, as user-side fine-tuning with clean data significantly alters model parameters, rendering them impractical for real-world applications. To address these challenges, we propose INFUSE (INjection into Fine-tUne-inSensitive modulEs), the first backdoor attack framework for VLA base models that remains effective even with arbitrary user fine-tuning. INFUSE begins by analyzing parameter sensitivity across diverse fine-tuning scenarios to identify modules that remain largely unchanged -- the fine-tune-insensitive modules. It then injects backdoors into these stable modules while freezing the rest, ensuring malicious behavior persists after extensive user fine-tuning. Comprehensive experiments across multiple VLA architectures demonstrate INFUSE&#39;s effectiveness. After user-side fine-tuning, INFUSE maintains mean attack success rates of 91.0% on simulation environments and 79.8% on real-world robot tasks, substantially surpassing BadVLA (38.8% and 36.6%, respectively), while preserving clean-task performance comparable to standard models. These results uncover a critical threat: backdoors implanted before distribution can persist through fine-tuning and remain effective at deployment."
    }
  ],
  "天津大学": [
    {
      "title": "RGBX-R1: Visual Modality Chain-of-Thought Guided Reinforcement Learning for Multimodal Grounding",
      "url": "https://arxiv.org/abs/2602.00504",
      "date": "2026-01-31",
      "authors_text": "Jiahe Wu, Bing Cao, Qilong Wang, Qinghua Hu, Dongdong Li",
      "is_highlight": false,
      "score": 78.0,
      "summary": "RGBX-R1 enhances multimodal large language models' reasoning across various visual modalities using a novel UAV prompting strategy and a two-stage training approach, achieving significant performance improvements.",
      "teaser_image": "https://arxiv.org/html/2602.00504/x2.png",
      "debug_abstract": "Multimodal Large Language Models (MLLM) are primarily pre-trained on the RGB modality, thereby limiting their performance on other modalities, such as infrared, depth, and event data, which are crucial for complex scenarios. To address this, we propose RGBX-R1, a framework to enhance MLLM&#39;s perception and reasoning capacities across various X visual modalities. Specifically, we employ an Understand-Associate-Validate (UAV) prompting strategy to construct the Visual Modality Chain-of-Thought (VM-CoT), which aims to expand the MLLMs&#39; RGB understanding capability into X modalities. To progressively enhance reasoning capabilities, we introduce a two-stage training paradigm: Cold-Start Supervised Fine-Tuning (CS-SFT) and Spatio-Temporal Reinforcement Fine-Tuning (ST-RFT). CS-SFT supervises the reasoning process with the guidance of VM-CoT, equipping the MLLM with fundamental modality cognition. Building upon GRPO, ST-RFT employs a Modality-understanding Spatio-Temporal (MuST) reward to reinforce modality reasoning. Notably, we construct the first RGBX-Grounding benchmark, and extensive experiments verify our superiority in multimodal understanding and spatial perception, outperforming baselines by 22.71% on three RGBX grounding tasks."
    }
  ],
  "IWIN-FINS实验室": [
    {
      "title": "SurfSplat: Conquering Feedforward 2D Gaussian Splatting with Surface Continuity Priors",
      "url": "https://arxiv.org/abs/2602.02000",
      "date": "2026-02-02",
      "authors_text": "Bing He, Jingnan Gao, Yunuo Chen, Ning Cao, Gang Chen",
      "is_highlight": false,
      "score": 25.0,
      "summary": "SurfSplat introduces a feedforward 2D Gaussian Splatting framework that enhances 3D scene reconstruction by ensuring surface continuity and improving geometric precision over existing methods.",
      "teaser_image": "https://arxiv.org/html/2602.02000/x2.png",
      "debug_abstract": "Reconstructing 3D scenes from sparse images remains a challenging task due to the difficulty of recovering accurate geometry and texture without optimization. Recent approaches leverage generalizable models to generate 3D scenes using 3D Gaussian Splatting (3DGS) primitive. However, they often fail to produce continuous surfaces and instead yield discrete, color-biased point clouds that appear plausible at normal resolution but reveal severe artifacts under close-up views. To address this issue, we present SurfSplat, a feedforward framework based on 2D Gaussian Splatting (2DGS) primitive, which provides stronger anisotropy and higher geometric precision. By incorporating a surface continuity prior and a forced alpha blending strategy, SurfSplat reconstructs coherent geometry together with faithful textures. Furthermore, we introduce High-Resolution Rendering Consistency (HRRC), a new evaluation metric designed to evaluate high-resolution reconstruction quality. Extensive experiments on RealEstate10K, DL3DV, and ScanNet demonstrate that SurfSplat consistently outperforms prior methods on both standard metrics and HRRC, establishing a robust solution for high-fidelity 3D reconstruction from sparse inputs. Project page: this https URL"
    },
    {
      "title": "ProxyImg: Towards Highly-Controllable Image Representation via Hierarchical Disentangled Proxy Embedding",
      "url": "https://arxiv.org/abs/2602.01881",
      "date": "2026-02-02",
      "authors_text": "Ye Chen, Yupeng Zhu, Xiongzhen Zhang, Zhewen Wan, Yingzhe Li",
      "is_highlight": false,
      "score": 30.0,
      "summary": "The paper presents ProxyImg, a hierarchical proxy-based image representation that enables fine-grained, controllable editing by disentangling semantic, geometric, and textural attributes for high-fidelity reconstruction.",
      "teaser_image": "https://arxiv.org/html/2602.01881/x2.png",
      "debug_abstract": "Prevailing image representation methods, including explicit representations such as raster images and Gaussian primitives, as well as implicit representations such as latent images, either suffer from representation redundancy that leads to heavy manual editing effort, or lack a direct mapping from latent variables to semantic instances or parts, making fine-grained manipulation difficult. These limitations hinder efficient and controllable image and video editing. To address these issues, we propose a hierarchical proxy-based parametric image representation that disentangles semantic, geometric, and textural attributes into independent and manipulable parameter spaces. Based on a semantic-aware decomposition of the input image, our representation constructs hierarchical proxy geometries through adaptive Bezier fitting and iterative internal region subdivision and meshing. Multi-scale implicit texture parameters are embedded into the resulting geometry-aware distributed proxy nodes, enabling continuous high-fidelity reconstruction in the pixel domain and instance- or part-independent semantic editing. In addition, we introduce a locality-adaptive feature indexing mechanism to ensure spatial texture coherence, which further supports high-quality background completion without relying on generative models. Extensive experiments on image reconstruction and editing benchmarks, including ImageNet, OIR-Bench, and HumanEdit, demonstrate that our method achieves state-of-the-art rendering fidelity with significantly fewer parameters, while enabling intuitive, interactive, and physically plausible manipulation. Moreover, by integrating proxy nodes with Position-Based Dynamics, our framework supports real-time physics-driven animation using lightweight implicit rendering, achieving superior temporal consistency and visual realism compared with generative approaches."
    },
    {
      "title": "ConLA: Contrastive Latent Action Learning from Human Videos for Robotic Manipulation",
      "url": "https://arxiv.org/abs/2602.00557",
      "date": "2026-01-31",
      "authors_text": "Weisheng Dai, Kai Lan, Jianyi Zhou, Bo Zhao, Xiu Su",
      "is_highlight": true,
      "score": 91.0,
      "summary": "ConLA introduces a contrastive disentanglement framework for unsupervised robotic policy learning from human videos, enhancing transferability and performance beyond traditional robot trajectory pretraining.",
      "teaser_image": "https://arxiv.org/html/2602.00557/x3.png",
      "debug_abstract": "Vision-Language-Action (VLA) models achieve preliminary generalization through pretraining on large scale robot teleoperation datasets. However, acquiring datasets that comprehensively cover diverse tasks and environments is extremely costly and difficult to scale. In contrast, human demonstration videos offer a rich and scalable source of diverse scenes and manipulation behaviors, yet their lack of explicit action supervision hinders direct utilization. Prior work leverages VQ-VAE based frameworks to learn latent actions from human videos in an unsupervised manner. Nevertheless, since the training objective primarily focuses on reconstructing visual appearances rather than capturing inter-frame dynamics, the learned representations tend to rely on spurious visual cues, leading to shortcut learning and entangled latent representations that hinder transferability. To address this, we propose ConLA, an unsupervised pretraining framework for learning robotic policies from human videos. ConLA introduces a contrastive disentanglement mechanism that leverages action category priors and temporal cues to isolate motion dynamics from visual content, effectively mitigating shortcut learning. Extensive experiments show that ConLA achieves strong performance across diverse benchmarks. Notably, by pretraining solely on human videos, our method for the first time surpasses the performance obtained with real robot trajectory pretraining, highlighting its ability to extract pure and semantically consistent latent action representations for scalable robot learning."
    }
  ],
  "机器视觉与智能学习实验室 (MVIG)": [
    {
      "title": "SurfSplat: Conquering Feedforward 2D Gaussian Splatting with Surface Continuity Priors",
      "url": "https://arxiv.org/abs/2602.02000",
      "date": "2026-02-02",
      "authors_text": "Bing He, Jingnan Gao, Yunuo Chen, Ning Cao, Gang Chen",
      "is_highlight": false,
      "score": 25.0,
      "summary": "SurfSplat introduces a feedforward 2D Gaussian Splatting framework that enhances 3D scene reconstruction by ensuring surface continuity and improving geometric precision over existing methods.",
      "teaser_image": "https://arxiv.org/html/2602.02000/x2.png",
      "debug_abstract": "Reconstructing 3D scenes from sparse images remains a challenging task due to the difficulty of recovering accurate geometry and texture without optimization. Recent approaches leverage generalizable models to generate 3D scenes using 3D Gaussian Splatting (3DGS) primitive. However, they often fail to produce continuous surfaces and instead yield discrete, color-biased point clouds that appear plausible at normal resolution but reveal severe artifacts under close-up views. To address this issue, we present SurfSplat, a feedforward framework based on 2D Gaussian Splatting (2DGS) primitive, which provides stronger anisotropy and higher geometric precision. By incorporating a surface continuity prior and a forced alpha blending strategy, SurfSplat reconstructs coherent geometry together with faithful textures. Furthermore, we introduce High-Resolution Rendering Consistency (HRRC), a new evaluation metric designed to evaluate high-resolution reconstruction quality. Extensive experiments on RealEstate10K, DL3DV, and ScanNet demonstrate that SurfSplat consistently outperforms prior methods on both standard metrics and HRRC, establishing a robust solution for high-fidelity 3D reconstruction from sparse inputs. Project page: this https URL"
    },
    {
      "title": "ProxyImg: Towards Highly-Controllable Image Representation via Hierarchical Disentangled Proxy Embedding",
      "url": "https://arxiv.org/abs/2602.01881",
      "date": "2026-02-02",
      "authors_text": "Ye Chen, Yupeng Zhu, Xiongzhen Zhang, Zhewen Wan, Yingzhe Li",
      "is_highlight": false,
      "score": 30.0,
      "summary": "The paper presents ProxyImg, a hierarchical proxy-based image representation that enables fine-grained, controllable editing by disentangling semantic, geometric, and textural attributes for high-fidelity reconstruction.",
      "teaser_image": "https://arxiv.org/html/2602.01881/x2.png",
      "debug_abstract": "Prevailing image representation methods, including explicit representations such as raster images and Gaussian primitives, as well as implicit representations such as latent images, either suffer from representation redundancy that leads to heavy manual editing effort, or lack a direct mapping from latent variables to semantic instances or parts, making fine-grained manipulation difficult. These limitations hinder efficient and controllable image and video editing. To address these issues, we propose a hierarchical proxy-based parametric image representation that disentangles semantic, geometric, and textural attributes into independent and manipulable parameter spaces. Based on a semantic-aware decomposition of the input image, our representation constructs hierarchical proxy geometries through adaptive Bezier fitting and iterative internal region subdivision and meshing. Multi-scale implicit texture parameters are embedded into the resulting geometry-aware distributed proxy nodes, enabling continuous high-fidelity reconstruction in the pixel domain and instance- or part-independent semantic editing. In addition, we introduce a locality-adaptive feature indexing mechanism to ensure spatial texture coherence, which further supports high-quality background completion without relying on generative models. Extensive experiments on image reconstruction and editing benchmarks, including ImageNet, OIR-Bench, and HumanEdit, demonstrate that our method achieves state-of-the-art rendering fidelity with significantly fewer parameters, while enabling intuitive, interactive, and physically plausible manipulation. Moreover, by integrating proxy nodes with Position-Based Dynamics, our framework supports real-time physics-driven animation using lightweight implicit rendering, achieving superior temporal consistency and visual realism compared with generative approaches."
    },
    {
      "title": "ConLA: Contrastive Latent Action Learning from Human Videos for Robotic Manipulation",
      "url": "https://arxiv.org/abs/2602.00557",
      "date": "2026-01-31",
      "authors_text": "Weisheng Dai, Kai Lan, Jianyi Zhou, Bo Zhao, Xiu Su",
      "is_highlight": true,
      "score": 91.0,
      "summary": "ConLA introduces a contrastive disentanglement framework for unsupervised robotic policy learning from human videos, enhancing transferability and performance beyond traditional robot trajectory pretraining.",
      "teaser_image": "https://arxiv.org/html/2602.00557/x3.png",
      "debug_abstract": "Vision-Language-Action (VLA) models achieve preliminary generalization through pretraining on large scale robot teleoperation datasets. However, acquiring datasets that comprehensively cover diverse tasks and environments is extremely costly and difficult to scale. In contrast, human demonstration videos offer a rich and scalable source of diverse scenes and manipulation behaviors, yet their lack of explicit action supervision hinders direct utilization. Prior work leverages VQ-VAE based frameworks to learn latent actions from human videos in an unsupervised manner. Nevertheless, since the training objective primarily focuses on reconstructing visual appearances rather than capturing inter-frame dynamics, the learned representations tend to rely on spurious visual cues, leading to shortcut learning and entangled latent representations that hinder transferability. To address this, we propose ConLA, an unsupervised pretraining framework for learning robotic policies from human videos. ConLA introduces a contrastive disentanglement mechanism that leverages action category priors and temporal cues to isolate motion dynamics from visual content, effectively mitigating shortcut learning. Extensive experiments show that ConLA achieves strong performance across diverse benchmarks. Notably, by pretraining solely on human videos, our method for the first time surpasses the performance obtained with real robot trajectory pretraining, highlighting its ability to extract pure and semantically consistent latent action representations for scalable robot learning."
    }
  ],
  "赵波老师实验室": [
    {
      "title": "SurfSplat: Conquering Feedforward 2D Gaussian Splatting with Surface Continuity Priors",
      "url": "https://arxiv.org/abs/2602.02000",
      "date": "2026-02-02",
      "authors_text": "Bing He, Jingnan Gao, Yunuo Chen, Ning Cao, Gang Chen",
      "is_highlight": false,
      "score": 25.0,
      "summary": "SurfSplat introduces a feedforward 2D Gaussian Splatting framework that enhances 3D scene reconstruction by ensuring surface continuity and improving geometric precision over existing methods.",
      "teaser_image": "https://arxiv.org/html/2602.02000/x2.png",
      "debug_abstract": "Reconstructing 3D scenes from sparse images remains a challenging task due to the difficulty of recovering accurate geometry and texture without optimization. Recent approaches leverage generalizable models to generate 3D scenes using 3D Gaussian Splatting (3DGS) primitive. However, they often fail to produce continuous surfaces and instead yield discrete, color-biased point clouds that appear plausible at normal resolution but reveal severe artifacts under close-up views. To address this issue, we present SurfSplat, a feedforward framework based on 2D Gaussian Splatting (2DGS) primitive, which provides stronger anisotropy and higher geometric precision. By incorporating a surface continuity prior and a forced alpha blending strategy, SurfSplat reconstructs coherent geometry together with faithful textures. Furthermore, we introduce High-Resolution Rendering Consistency (HRRC), a new evaluation metric designed to evaluate high-resolution reconstruction quality. Extensive experiments on RealEstate10K, DL3DV, and ScanNet demonstrate that SurfSplat consistently outperforms prior methods on both standard metrics and HRRC, establishing a robust solution for high-fidelity 3D reconstruction from sparse inputs. Project page: this https URL"
    },
    {
      "title": "ProxyImg: Towards Highly-Controllable Image Representation via Hierarchical Disentangled Proxy Embedding",
      "url": "https://arxiv.org/abs/2602.01881",
      "date": "2026-02-02",
      "authors_text": "Ye Chen, Yupeng Zhu, Xiongzhen Zhang, Zhewen Wan, Yingzhe Li",
      "is_highlight": false,
      "score": 30.0,
      "summary": "The paper presents ProxyImg, a hierarchical proxy-based image representation that enables fine-grained, controllable editing by disentangling semantic, geometric, and textural attributes for high-fidelity reconstruction.",
      "teaser_image": "https://arxiv.org/html/2602.01881/x2.png",
      "debug_abstract": "Prevailing image representation methods, including explicit representations such as raster images and Gaussian primitives, as well as implicit representations such as latent images, either suffer from representation redundancy that leads to heavy manual editing effort, or lack a direct mapping from latent variables to semantic instances or parts, making fine-grained manipulation difficult. These limitations hinder efficient and controllable image and video editing. To address these issues, we propose a hierarchical proxy-based parametric image representation that disentangles semantic, geometric, and textural attributes into independent and manipulable parameter spaces. Based on a semantic-aware decomposition of the input image, our representation constructs hierarchical proxy geometries through adaptive Bezier fitting and iterative internal region subdivision and meshing. Multi-scale implicit texture parameters are embedded into the resulting geometry-aware distributed proxy nodes, enabling continuous high-fidelity reconstruction in the pixel domain and instance- or part-independent semantic editing. In addition, we introduce a locality-adaptive feature indexing mechanism to ensure spatial texture coherence, which further supports high-quality background completion without relying on generative models. Extensive experiments on image reconstruction and editing benchmarks, including ImageNet, OIR-Bench, and HumanEdit, demonstrate that our method achieves state-of-the-art rendering fidelity with significantly fewer parameters, while enabling intuitive, interactive, and physically plausible manipulation. Moreover, by integrating proxy nodes with Position-Based Dynamics, our framework supports real-time physics-driven animation using lightweight implicit rendering, achieving superior temporal consistency and visual realism compared with generative approaches."
    },
    {
      "title": "ConLA: Contrastive Latent Action Learning from Human Videos for Robotic Manipulation",
      "url": "https://arxiv.org/abs/2602.00557",
      "date": "2026-01-31",
      "authors_text": "Weisheng Dai, Kai Lan, Jianyi Zhou, Bo Zhao, Xiu Su",
      "is_highlight": true,
      "score": 91.0,
      "summary": "ConLA introduces a contrastive disentanglement framework for unsupervised robotic policy learning from human videos, enhancing transferability and performance beyond traditional robot trajectory pretraining.",
      "teaser_image": "https://arxiv.org/html/2602.00557/x3.png",
      "debug_abstract": "Vision-Language-Action (VLA) models achieve preliminary generalization through pretraining on large scale robot teleoperation datasets. However, acquiring datasets that comprehensively cover diverse tasks and environments is extremely costly and difficult to scale. In contrast, human demonstration videos offer a rich and scalable source of diverse scenes and manipulation behaviors, yet their lack of explicit action supervision hinders direct utilization. Prior work leverages VQ-VAE based frameworks to learn latent actions from human videos in an unsupervised manner. Nevertheless, since the training objective primarily focuses on reconstructing visual appearances rather than capturing inter-frame dynamics, the learned representations tend to rely on spurious visual cues, leading to shortcut learning and entangled latent representations that hinder transferability. To address this, we propose ConLA, an unsupervised pretraining framework for learning robotic policies from human videos. ConLA introduces a contrastive disentanglement mechanism that leverages action category priors and temporal cues to isolate motion dynamics from visual content, effectively mitigating shortcut learning. Extensive experiments show that ConLA achieves strong performance across diverse benchmarks. Notably, by pretraining solely on human videos, our method for the first time surpasses the performance obtained with real robot trajectory pretraining, highlighting its ability to extract pure and semantically consistent latent action representations for scalable robot learning."
    }
  ],
  "ReThinkLab": [
    {
      "title": "SurfSplat: Conquering Feedforward 2D Gaussian Splatting with Surface Continuity Priors",
      "url": "https://arxiv.org/abs/2602.02000",
      "date": "2026-02-02",
      "authors_text": "Bing He, Jingnan Gao, Yunuo Chen, Ning Cao, Gang Chen",
      "is_highlight": false,
      "score": 25.0,
      "summary": "SurfSplat introduces a feedforward 2D Gaussian Splatting framework that enhances 3D scene reconstruction by ensuring surface continuity and improving geometric precision over existing methods.",
      "teaser_image": "https://arxiv.org/html/2602.02000/x2.png",
      "debug_abstract": "Reconstructing 3D scenes from sparse images remains a challenging task due to the difficulty of recovering accurate geometry and texture without optimization. Recent approaches leverage generalizable models to generate 3D scenes using 3D Gaussian Splatting (3DGS) primitive. However, they often fail to produce continuous surfaces and instead yield discrete, color-biased point clouds that appear plausible at normal resolution but reveal severe artifacts under close-up views. To address this issue, we present SurfSplat, a feedforward framework based on 2D Gaussian Splatting (2DGS) primitive, which provides stronger anisotropy and higher geometric precision. By incorporating a surface continuity prior and a forced alpha blending strategy, SurfSplat reconstructs coherent geometry together with faithful textures. Furthermore, we introduce High-Resolution Rendering Consistency (HRRC), a new evaluation metric designed to evaluate high-resolution reconstruction quality. Extensive experiments on RealEstate10K, DL3DV, and ScanNet demonstrate that SurfSplat consistently outperforms prior methods on both standard metrics and HRRC, establishing a robust solution for high-fidelity 3D reconstruction from sparse inputs. Project page: this https URL"
    },
    {
      "title": "ProxyImg: Towards Highly-Controllable Image Representation via Hierarchical Disentangled Proxy Embedding",
      "url": "https://arxiv.org/abs/2602.01881",
      "date": "2026-02-02",
      "authors_text": "Ye Chen, Yupeng Zhu, Xiongzhen Zhang, Zhewen Wan, Yingzhe Li",
      "is_highlight": false,
      "score": 30.0,
      "summary": "The paper presents ProxyImg, a hierarchical proxy-based image representation that enables fine-grained, controllable editing by disentangling semantic, geometric, and textural attributes for high-fidelity reconstruction.",
      "teaser_image": "https://arxiv.org/html/2602.01881/x2.png",
      "debug_abstract": "Prevailing image representation methods, including explicit representations such as raster images and Gaussian primitives, as well as implicit representations such as latent images, either suffer from representation redundancy that leads to heavy manual editing effort, or lack a direct mapping from latent variables to semantic instances or parts, making fine-grained manipulation difficult. These limitations hinder efficient and controllable image and video editing. To address these issues, we propose a hierarchical proxy-based parametric image representation that disentangles semantic, geometric, and textural attributes into independent and manipulable parameter spaces. Based on a semantic-aware decomposition of the input image, our representation constructs hierarchical proxy geometries through adaptive Bezier fitting and iterative internal region subdivision and meshing. Multi-scale implicit texture parameters are embedded into the resulting geometry-aware distributed proxy nodes, enabling continuous high-fidelity reconstruction in the pixel domain and instance- or part-independent semantic editing. In addition, we introduce a locality-adaptive feature indexing mechanism to ensure spatial texture coherence, which further supports high-quality background completion without relying on generative models. Extensive experiments on image reconstruction and editing benchmarks, including ImageNet, OIR-Bench, and HumanEdit, demonstrate that our method achieves state-of-the-art rendering fidelity with significantly fewer parameters, while enabling intuitive, interactive, and physically plausible manipulation. Moreover, by integrating proxy nodes with Position-Based Dynamics, our framework supports real-time physics-driven animation using lightweight implicit rendering, achieving superior temporal consistency and visual realism compared with generative approaches."
    },
    {
      "title": "ConLA: Contrastive Latent Action Learning from Human Videos for Robotic Manipulation",
      "url": "https://arxiv.org/abs/2602.00557",
      "date": "2026-01-31",
      "authors_text": "Weisheng Dai, Kai Lan, Jianyi Zhou, Bo Zhao, Xiu Su",
      "is_highlight": true,
      "score": 91.0,
      "summary": "ConLA introduces a contrastive disentanglement framework for unsupervised robotic policy learning from human videos, enhancing transferability and performance beyond traditional robot trajectory pretraining.",
      "teaser_image": "https://arxiv.org/html/2602.00557/x3.png",
      "debug_abstract": "Vision-Language-Action (VLA) models achieve preliminary generalization through pretraining on large scale robot teleoperation datasets. However, acquiring datasets that comprehensively cover diverse tasks and environments is extremely costly and difficult to scale. In contrast, human demonstration videos offer a rich and scalable source of diverse scenes and manipulation behaviors, yet their lack of explicit action supervision hinders direct utilization. Prior work leverages VQ-VAE based frameworks to learn latent actions from human videos in an unsupervised manner. Nevertheless, since the training objective primarily focuses on reconstructing visual appearances rather than capturing inter-frame dynamics, the learned representations tend to rely on spurious visual cues, leading to shortcut learning and entangled latent representations that hinder transferability. To address this, we propose ConLA, an unsupervised pretraining framework for learning robotic policies from human videos. ConLA introduces a contrastive disentanglement mechanism that leverages action category priors and temporal cues to isolate motion dynamics from visual content, effectively mitigating shortcut learning. Extensive experiments show that ConLA achieves strong performance across diverse benchmarks. Notably, by pretraining solely on human videos, our method for the first time surpasses the performance obtained with real robot trajectory pretraining, highlighting its ability to extract pure and semantically consistent latent action representations for scalable robot learning."
    }
  ],
  "智能机器人与机器视觉(IRMV)实验室": [
    {
      "title": "SurfSplat: Conquering Feedforward 2D Gaussian Splatting with Surface Continuity Priors",
      "url": "https://arxiv.org/abs/2602.02000",
      "date": "2026-02-02",
      "authors_text": "Bing He, Jingnan Gao, Yunuo Chen, Ning Cao, Gang Chen",
      "is_highlight": false,
      "score": 25.0,
      "summary": "SurfSplat introduces a feedforward 2D Gaussian Splatting framework that enhances 3D scene reconstruction by ensuring surface continuity and improving geometric precision over existing methods.",
      "teaser_image": "https://arxiv.org/html/2602.02000/x2.png",
      "debug_abstract": "Reconstructing 3D scenes from sparse images remains a challenging task due to the difficulty of recovering accurate geometry and texture without optimization. Recent approaches leverage generalizable models to generate 3D scenes using 3D Gaussian Splatting (3DGS) primitive. However, they often fail to produce continuous surfaces and instead yield discrete, color-biased point clouds that appear plausible at normal resolution but reveal severe artifacts under close-up views. To address this issue, we present SurfSplat, a feedforward framework based on 2D Gaussian Splatting (2DGS) primitive, which provides stronger anisotropy and higher geometric precision. By incorporating a surface continuity prior and a forced alpha blending strategy, SurfSplat reconstructs coherent geometry together with faithful textures. Furthermore, we introduce High-Resolution Rendering Consistency (HRRC), a new evaluation metric designed to evaluate high-resolution reconstruction quality. Extensive experiments on RealEstate10K, DL3DV, and ScanNet demonstrate that SurfSplat consistently outperforms prior methods on both standard metrics and HRRC, establishing a robust solution for high-fidelity 3D reconstruction from sparse inputs. Project page: this https URL"
    },
    {
      "title": "ProxyImg: Towards Highly-Controllable Image Representation via Hierarchical Disentangled Proxy Embedding",
      "url": "https://arxiv.org/abs/2602.01881",
      "date": "2026-02-02",
      "authors_text": "Ye Chen, Yupeng Zhu, Xiongzhen Zhang, Zhewen Wan, Yingzhe Li",
      "is_highlight": false,
      "score": 30.0,
      "summary": "The paper presents ProxyImg, a hierarchical proxy-based image representation that enables fine-grained, controllable editing by disentangling semantic, geometric, and textural attributes for high-fidelity reconstruction.",
      "teaser_image": "https://arxiv.org/html/2602.01881/x2.png",
      "debug_abstract": "Prevailing image representation methods, including explicit representations such as raster images and Gaussian primitives, as well as implicit representations such as latent images, either suffer from representation redundancy that leads to heavy manual editing effort, or lack a direct mapping from latent variables to semantic instances or parts, making fine-grained manipulation difficult. These limitations hinder efficient and controllable image and video editing. To address these issues, we propose a hierarchical proxy-based parametric image representation that disentangles semantic, geometric, and textural attributes into independent and manipulable parameter spaces. Based on a semantic-aware decomposition of the input image, our representation constructs hierarchical proxy geometries through adaptive Bezier fitting and iterative internal region subdivision and meshing. Multi-scale implicit texture parameters are embedded into the resulting geometry-aware distributed proxy nodes, enabling continuous high-fidelity reconstruction in the pixel domain and instance- or part-independent semantic editing. In addition, we introduce a locality-adaptive feature indexing mechanism to ensure spatial texture coherence, which further supports high-quality background completion without relying on generative models. Extensive experiments on image reconstruction and editing benchmarks, including ImageNet, OIR-Bench, and HumanEdit, demonstrate that our method achieves state-of-the-art rendering fidelity with significantly fewer parameters, while enabling intuitive, interactive, and physically plausible manipulation. Moreover, by integrating proxy nodes with Position-Based Dynamics, our framework supports real-time physics-driven animation using lightweight implicit rendering, achieving superior temporal consistency and visual realism compared with generative approaches."
    },
    {
      "title": "ConLA: Contrastive Latent Action Learning from Human Videos for Robotic Manipulation",
      "url": "https://arxiv.org/abs/2602.00557",
      "date": "2026-01-31",
      "authors_text": "Weisheng Dai, Kai Lan, Jianyi Zhou, Bo Zhao, Xiu Su",
      "is_highlight": true,
      "score": 91.0,
      "summary": "ConLA introduces a contrastive disentanglement framework for unsupervised robotic policy learning from human videos, enhancing transferability and performance beyond traditional robot trajectory pretraining.",
      "teaser_image": "https://arxiv.org/html/2602.00557/x3.png",
      "debug_abstract": "Vision-Language-Action (VLA) models achieve preliminary generalization through pretraining on large scale robot teleoperation datasets. However, acquiring datasets that comprehensively cover diverse tasks and environments is extremely costly and difficult to scale. In contrast, human demonstration videos offer a rich and scalable source of diverse scenes and manipulation behaviors, yet their lack of explicit action supervision hinders direct utilization. Prior work leverages VQ-VAE based frameworks to learn latent actions from human videos in an unsupervised manner. Nevertheless, since the training objective primarily focuses on reconstructing visual appearances rather than capturing inter-frame dynamics, the learned representations tend to rely on spurious visual cues, leading to shortcut learning and entangled latent representations that hinder transferability. To address this, we propose ConLA, an unsupervised pretraining framework for learning robotic policies from human videos. ConLA introduces a contrastive disentanglement mechanism that leverages action category priors and temporal cues to isolate motion dynamics from visual content, effectively mitigating shortcut learning. Extensive experiments show that ConLA achieves strong performance across diverse benchmarks. Notably, by pretraining solely on human videos, our method for the first time surpasses the performance obtained with real robot trajectory pretraining, highlighting its ability to extract pure and semantically consistent latent action representations for scalable robot learning."
    }
  ],
  "数字媒体与计算机视觉实验室": [
    {
      "title": "SurfSplat: Conquering Feedforward 2D Gaussian Splatting with Surface Continuity Priors",
      "url": "https://arxiv.org/abs/2602.02000",
      "date": "2026-02-02",
      "authors_text": "Bing He, Jingnan Gao, Yunuo Chen, Ning Cao, Gang Chen",
      "is_highlight": false,
      "score": 25.0,
      "summary": "SurfSplat introduces a feedforward 2D Gaussian Splatting framework that enhances 3D scene reconstruction by ensuring surface continuity and improving geometric precision over existing methods.",
      "teaser_image": "https://arxiv.org/html/2602.02000/x2.png",
      "debug_abstract": "Reconstructing 3D scenes from sparse images remains a challenging task due to the difficulty of recovering accurate geometry and texture without optimization. Recent approaches leverage generalizable models to generate 3D scenes using 3D Gaussian Splatting (3DGS) primitive. However, they often fail to produce continuous surfaces and instead yield discrete, color-biased point clouds that appear plausible at normal resolution but reveal severe artifacts under close-up views. To address this issue, we present SurfSplat, a feedforward framework based on 2D Gaussian Splatting (2DGS) primitive, which provides stronger anisotropy and higher geometric precision. By incorporating a surface continuity prior and a forced alpha blending strategy, SurfSplat reconstructs coherent geometry together with faithful textures. Furthermore, we introduce High-Resolution Rendering Consistency (HRRC), a new evaluation metric designed to evaluate high-resolution reconstruction quality. Extensive experiments on RealEstate10K, DL3DV, and ScanNet demonstrate that SurfSplat consistently outperforms prior methods on both standard metrics and HRRC, establishing a robust solution for high-fidelity 3D reconstruction from sparse inputs. Project page: this https URL"
    },
    {
      "title": "ProxyImg: Towards Highly-Controllable Image Representation via Hierarchical Disentangled Proxy Embedding",
      "url": "https://arxiv.org/abs/2602.01881",
      "date": "2026-02-02",
      "authors_text": "Ye Chen, Yupeng Zhu, Xiongzhen Zhang, Zhewen Wan, Yingzhe Li",
      "is_highlight": false,
      "score": 30.0,
      "summary": "The paper presents ProxyImg, a hierarchical proxy-based image representation that enables fine-grained, controllable editing by disentangling semantic, geometric, and textural attributes for high-fidelity reconstruction.",
      "teaser_image": "https://arxiv.org/html/2602.01881/x2.png",
      "debug_abstract": "Prevailing image representation methods, including explicit representations such as raster images and Gaussian primitives, as well as implicit representations such as latent images, either suffer from representation redundancy that leads to heavy manual editing effort, or lack a direct mapping from latent variables to semantic instances or parts, making fine-grained manipulation difficult. These limitations hinder efficient and controllable image and video editing. To address these issues, we propose a hierarchical proxy-based parametric image representation that disentangles semantic, geometric, and textural attributes into independent and manipulable parameter spaces. Based on a semantic-aware decomposition of the input image, our representation constructs hierarchical proxy geometries through adaptive Bezier fitting and iterative internal region subdivision and meshing. Multi-scale implicit texture parameters are embedded into the resulting geometry-aware distributed proxy nodes, enabling continuous high-fidelity reconstruction in the pixel domain and instance- or part-independent semantic editing. In addition, we introduce a locality-adaptive feature indexing mechanism to ensure spatial texture coherence, which further supports high-quality background completion without relying on generative models. Extensive experiments on image reconstruction and editing benchmarks, including ImageNet, OIR-Bench, and HumanEdit, demonstrate that our method achieves state-of-the-art rendering fidelity with significantly fewer parameters, while enabling intuitive, interactive, and physically plausible manipulation. Moreover, by integrating proxy nodes with Position-Based Dynamics, our framework supports real-time physics-driven animation using lightweight implicit rendering, achieving superior temporal consistency and visual realism compared with generative approaches."
    },
    {
      "title": "ConLA: Contrastive Latent Action Learning from Human Videos for Robotic Manipulation",
      "url": "https://arxiv.org/abs/2602.00557",
      "date": "2026-01-31",
      "authors_text": "Weisheng Dai, Kai Lan, Jianyi Zhou, Bo Zhao, Xiu Su",
      "is_highlight": true,
      "score": 91.0,
      "summary": "ConLA introduces a contrastive disentanglement framework for unsupervised robotic policy learning from human videos, enhancing transferability and performance beyond traditional robot trajectory pretraining.",
      "teaser_image": "https://arxiv.org/html/2602.00557/x3.png",
      "debug_abstract": "Vision-Language-Action (VLA) models achieve preliminary generalization through pretraining on large scale robot teleoperation datasets. However, acquiring datasets that comprehensively cover diverse tasks and environments is extremely costly and difficult to scale. In contrast, human demonstration videos offer a rich and scalable source of diverse scenes and manipulation behaviors, yet their lack of explicit action supervision hinders direct utilization. Prior work leverages VQ-VAE based frameworks to learn latent actions from human videos in an unsupervised manner. Nevertheless, since the training objective primarily focuses on reconstructing visual appearances rather than capturing inter-frame dynamics, the learned representations tend to rely on spurious visual cues, leading to shortcut learning and entangled latent representations that hinder transferability. To address this, we propose ConLA, an unsupervised pretraining framework for learning robotic policies from human videos. ConLA introduces a contrastive disentanglement mechanism that leverages action category priors and temporal cues to isolate motion dynamics from visual content, effectively mitigating shortcut learning. Extensive experiments show that ConLA achieves strong performance across diverse benchmarks. Notably, by pretraining solely on human videos, our method for the first time surpasses the performance obtained with real robot trajectory pretraining, highlighting its ability to extract pure and semantically consistent latent action representations for scalable robot learning."
    }
  ],
  "集群机器人系统实验室 (MagicLab)": [
    {
      "title": "Enhancing Indoor Occupancy Prediction via Sparse Query-Based Multi-Level Consistent Knowledge Distillation",
      "url": "https://arxiv.org/abs/2602.02318",
      "date": "2026-02-02",
      "authors_text": "Xiang Li, Yupeng Zheng, Pengfei Li, Yilun Chen, Ya-Qin Zhang",
      "is_highlight": false,
      "score": 0,
      "summary": "DiScene introduces a sparse query-based framework for efficient and robust indoor occupancy prediction through multi-level knowledge distillation, achieving state-of-the-art performance and faster inference.",
      "teaser_image": "https://arxiv.org/html/2602.02318/x2.png",
      "debug_abstract": "Occupancy prediction provides critical geometric and semantic understanding for robotics but faces efficiency-accuracy trade-offs. Current dense methods suffer computational waste on empty voxels, while sparse query-based approaches lack robustness in diverse and complex indoor scenes. In this paper, we propose DiScene, a novel sparse query-based framework that leverages multi-level distillation to achieve efficient and robust occupancy prediction. In particular, our method incorporates two key innovations: (1) a Multi-level Consistent Knowledge Distillation strategy, which transfers hierarchical representations from large teacher models to lightweight students through coordinated alignment across four levels, including encoder-level feature alignment, query-level feature matching, prior-level spatial guidance, and anchor-level high-confidence knowledge transfer and (2) a Teacher-Guided Initialization policy, employing optimized parameter warm-up to accelerate model convergence. Validated on the Occ-Scannet benchmark, DiScene achieves 23.2 FPS without depth priors while outperforming our baseline method, OPUS, by 36.1% and even better than the depth-enhanced version, OPUS†. With depth integration, DiScene† attains new SOTA performance, surpassing EmbodiedOcc by 3.7% with 1.62$\\times$ faster inference speed. Furthermore, experiments on the Occ3D-nuScenes benchmark and in-the-wild scenarios demonstrate the versatility of our approach in various environments. Code and models can be accessed at this https URL."
    },
    {
      "title": "Beyond Precision: Training-Inference Mismatch is an Optimization Problem and Simple LR Scheduling Fixes It",
      "url": "https://arxiv.org/abs/2602.01826",
      "date": "2026-02-02",
      "authors_text": "Yaxiang Zhang, Yingru Li, Jiacai Liu, Jiawei Xu, Ziniu Li",
      "is_highlight": false,
      "score": 35.0,
      "summary": "This paper presents a dynamic learning rate scheduling method to mitigate training-inference mismatch and stabilize reinforcement learning in large language models by addressing gradient noise.",
      "teaser_image": "https://arxiv.org/html/2602.01826/Qwen3-4B_baseline_vs_importance_sampling_patches.png",
      "debug_abstract": "Reinforcement Learning (RL) for training Large Language Models is notoriously unstable. While recent studies attribute this to &#34;training inference mismatch stemming&#34; from inconsistent hybrid engines, standard remedies, such as Importance Sampling, might fail during extended training runs. In this work, we analyze this instability through the lens of optimization, demonstrating that gradient noise and training-inference mismatch escalate in tandem as training progresses. Meanwhile, we find that the mismatch can be effectively suppressed by shrinking the update size. Taken together, we deduce that the mismatch is not merely a static numerical discrepancy, but a dynamic failure coupled with the model&#39;s optimization. Based on this insight, we propose a simple yet effective solution: a specialized Learning Rate (LR) scheduler. Instead of pre-defined decay schedule in traditional LR scheduler, our method dynamically triggers LR decay based on response length, which we identify as a reliable early-warning signal for impending instability. Empirical evidence suggests that by reducing the learning rate as gradient noise rises, we can consistently stabilize RL training and keep the training-inference mismatch at a safe level."
    },
    {
      "title": "VVLoc: Prior-free 3-DoF Vehicle Visual Localization",
      "url": "https://arxiv.org/abs/2602.00810",
      "date": "2026-01-31",
      "authors_text": "Ze Huang, Zhongyang Xiao, Mingliang Song, Longan Yang, Hongyuan Yuan",
      "is_highlight": false,
      "score": 73.0,
      "summary": "VVLoc is a unified neural network approach for simultaneous topological and metric vehicle localization using multi-camera systems, providing confidence measures and requiring minimal training data.",
      "teaser_image": "https://arxiv.org/html/2602.00810/x1.png",
      "debug_abstract": "Localization is a critical technology in autonomous driving, encompassing both topological localization, which identifies the most similar map keyframe to the current observation, and metric localization, which provides precise spatial coordinates. Conventional methods typically address these tasks independently, rely on single-camera setups, and often require additional 3D semantic or pose priors, while lacking mechanisms to quantify the confidence of localization results, making them less feasible for real industrial applications. In this paper, we propose VVLoc, a unified pipeline that employs a single neural network to concurrently achieve topological and metric vehicle localization using multi-camera system. VVLoc first evaluates the geo-proximity between visual observations, then estimates their relative metric poses using a matching strategy, while also providing a confidence measure. Additionally, the training process for VVLoc is highly efficient, requiring only pairs of visual data and corresponding ground-truth poses, eliminating the need for complex supplementary data. We evaluate VVLoc not only on the publicly available datasets, but also on a more challenging self-collected dataset, demonstrating its ability to deliver state-of-the-art localization accuracy across a wide range of localization tasks."
    },
    {
      "title": "UniMotion: A Unified Motion Framework for Simulation, Prediction and Planning",
      "url": "https://arxiv.org/abs/2602.00566",
      "date": "2026-01-31",
      "authors_text": "Nan Song, Junzhe Jiang, Jingyu Li, Xiatian Zhu, Li Zhang",
      "is_highlight": false,
      "score": 89.0,
      "summary": "UniMotion is a unified framework leveraging a Transformer architecture to enhance simulation, prediction, and planning in autonomous driving by integrating shared capabilities across these tasks.",
      "teaser_image": "https://arxiv.org/html/2602.00566/x1.png",
      "debug_abstract": "Motion simulation, prediction and planning are foundational tasks in autonomous driving, each essential for modeling and reasoning about dynamic traffic scenarios. While often addressed in isolation due to their differing objectives, such as generating diverse motion states or estimating optimal trajectories, these tasks inherently depend on shared capabilities: understanding multi-agent interactions, modeling motion behaviors, and reasoning over temporal and spatial dynamics. Despite this underlying commonality, existing approaches typically adopt specialized model designs, which hinders cross-task generalization and system scalability. More critically, this separation overlooks the potential mutual benefits among tasks. Motivated by these observations, we propose UniMotion, a unified motion framework that captures shared structures across motion tasks while accommodating their individual requirements. Built on a decoder-only Transformer architecture, UniMotion employs dedicated interaction modes and tailored training strategies to simultaneously support these motion tasks. This unified design not only enables joint optimization and representation sharing but also allows for targeted fine-tuning to specialize in individual tasks when needed. Extensive experiments on the Waymo Open Motion Dataset demonstrate that joint training leads to robust generalization and effective task integration. With further fine-tuning, UniMotion achieves state-of-the-art performance across a range of motion tasks, establishing it as a versatile and scalable solution for autonomous driving."
    }
  ],
  "智能感知与无人系统实验室": [
    {
      "title": "Enhancing Indoor Occupancy Prediction via Sparse Query-Based Multi-Level Consistent Knowledge Distillation",
      "url": "https://arxiv.org/abs/2602.02318",
      "date": "2026-02-02",
      "authors_text": "Xiang Li, Yupeng Zheng, Pengfei Li, Yilun Chen, Ya-Qin Zhang",
      "is_highlight": false,
      "score": 0,
      "summary": "DiScene introduces a sparse query-based framework for efficient and robust indoor occupancy prediction through multi-level knowledge distillation, achieving state-of-the-art performance and faster inference.",
      "teaser_image": "https://arxiv.org/html/2602.02318/x2.png",
      "debug_abstract": "Occupancy prediction provides critical geometric and semantic understanding for robotics but faces efficiency-accuracy trade-offs. Current dense methods suffer computational waste on empty voxels, while sparse query-based approaches lack robustness in diverse and complex indoor scenes. In this paper, we propose DiScene, a novel sparse query-based framework that leverages multi-level distillation to achieve efficient and robust occupancy prediction. In particular, our method incorporates two key innovations: (1) a Multi-level Consistent Knowledge Distillation strategy, which transfers hierarchical representations from large teacher models to lightweight students through coordinated alignment across four levels, including encoder-level feature alignment, query-level feature matching, prior-level spatial guidance, and anchor-level high-confidence knowledge transfer and (2) a Teacher-Guided Initialization policy, employing optimized parameter warm-up to accelerate model convergence. Validated on the Occ-Scannet benchmark, DiScene achieves 23.2 FPS without depth priors while outperforming our baseline method, OPUS, by 36.1% and even better than the depth-enhanced version, OPUS†. With depth integration, DiScene† attains new SOTA performance, surpassing EmbodiedOcc by 3.7% with 1.62$\\times$ faster inference speed. Furthermore, experiments on the Occ3D-nuScenes benchmark and in-the-wild scenarios demonstrate the versatility of our approach in various environments. Code and models can be accessed at this https URL."
    },
    {
      "title": "Beyond Precision: Training-Inference Mismatch is an Optimization Problem and Simple LR Scheduling Fixes It",
      "url": "https://arxiv.org/abs/2602.01826",
      "date": "2026-02-02",
      "authors_text": "Yaxiang Zhang, Yingru Li, Jiacai Liu, Jiawei Xu, Ziniu Li",
      "is_highlight": false,
      "score": 35.0,
      "summary": "This paper presents a dynamic learning rate scheduling method to mitigate training-inference mismatch and stabilize reinforcement learning in large language models by addressing gradient noise.",
      "teaser_image": "https://arxiv.org/html/2602.01826/Qwen3-4B_baseline_vs_importance_sampling_patches.png",
      "debug_abstract": "Reinforcement Learning (RL) for training Large Language Models is notoriously unstable. While recent studies attribute this to &#34;training inference mismatch stemming&#34; from inconsistent hybrid engines, standard remedies, such as Importance Sampling, might fail during extended training runs. In this work, we analyze this instability through the lens of optimization, demonstrating that gradient noise and training-inference mismatch escalate in tandem as training progresses. Meanwhile, we find that the mismatch can be effectively suppressed by shrinking the update size. Taken together, we deduce that the mismatch is not merely a static numerical discrepancy, but a dynamic failure coupled with the model&#39;s optimization. Based on this insight, we propose a simple yet effective solution: a specialized Learning Rate (LR) scheduler. Instead of pre-defined decay schedule in traditional LR scheduler, our method dynamically triggers LR decay based on response length, which we identify as a reliable early-warning signal for impending instability. Empirical evidence suggests that by reducing the learning rate as gradient noise rises, we can consistently stabilize RL training and keep the training-inference mismatch at a safe level."
    },
    {
      "title": "VVLoc: Prior-free 3-DoF Vehicle Visual Localization",
      "url": "https://arxiv.org/abs/2602.00810",
      "date": "2026-01-31",
      "authors_text": "Ze Huang, Zhongyang Xiao, Mingliang Song, Longan Yang, Hongyuan Yuan",
      "is_highlight": false,
      "score": 73.0,
      "summary": "VVLoc is a unified neural network approach for simultaneous topological and metric vehicle localization using multi-camera systems, providing confidence measures and requiring minimal training data.",
      "teaser_image": "https://arxiv.org/html/2602.00810/x1.png",
      "debug_abstract": "Localization is a critical technology in autonomous driving, encompassing both topological localization, which identifies the most similar map keyframe to the current observation, and metric localization, which provides precise spatial coordinates. Conventional methods typically address these tasks independently, rely on single-camera setups, and often require additional 3D semantic or pose priors, while lacking mechanisms to quantify the confidence of localization results, making them less feasible for real industrial applications. In this paper, we propose VVLoc, a unified pipeline that employs a single neural network to concurrently achieve topological and metric vehicle localization using multi-camera system. VVLoc first evaluates the geo-proximity between visual observations, then estimates their relative metric poses using a matching strategy, while also providing a confidence measure. Additionally, the training process for VVLoc is highly efficient, requiring only pairs of visual data and corresponding ground-truth poses, eliminating the need for complex supplementary data. We evaluate VVLoc not only on the publicly available datasets, but also on a more challenging self-collected dataset, demonstrating its ability to deliver state-of-the-art localization accuracy across a wide range of localization tasks."
    },
    {
      "title": "UniMotion: A Unified Motion Framework for Simulation, Prediction and Planning",
      "url": "https://arxiv.org/abs/2602.00566",
      "date": "2026-01-31",
      "authors_text": "Nan Song, Junzhe Jiang, Jingyu Li, Xiatian Zhu, Li Zhang",
      "is_highlight": false,
      "score": 89.0,
      "summary": "UniMotion is a unified framework leveraging a Transformer architecture to enhance simulation, prediction, and planning in autonomous driving by integrating shared capabilities across these tasks.",
      "teaser_image": "https://arxiv.org/html/2602.00566/x1.png",
      "debug_abstract": "Motion simulation, prediction and planning are foundational tasks in autonomous driving, each essential for modeling and reasoning about dynamic traffic scenarios. While often addressed in isolation due to their differing objectives, such as generating diverse motion states or estimating optimal trajectories, these tasks inherently depend on shared capabilities: understanding multi-agent interactions, modeling motion behaviors, and reasoning over temporal and spatial dynamics. Despite this underlying commonality, existing approaches typically adopt specialized model designs, which hinders cross-task generalization and system scalability. More critically, this separation overlooks the potential mutual benefits among tasks. Motivated by these observations, we propose UniMotion, a unified motion framework that captures shared structures across motion tasks while accommodating their individual requirements. Built on a decoder-only Transformer architecture, UniMotion employs dedicated interaction modes and tailored training strategies to simultaneously support these motion tasks. This unified design not only enables joint optimization and representation sharing but also allows for targeted fine-tuning to specialize in individual tasks when needed. Extensive experiments on the Waymo Open Motion Dataset demonstrate that joint training leads to robust generalization and effective task integration. With further fine-tuning, UniMotion achieves state-of-the-art performance across a range of motion tasks, establishing it as a versatile and scalable solution for autonomous driving."
    }
  ],
  "智能人机交互实验室 (MemX)": [
    {
      "title": "Enhancing Indoor Occupancy Prediction via Sparse Query-Based Multi-Level Consistent Knowledge Distillation",
      "url": "https://arxiv.org/abs/2602.02318",
      "date": "2026-02-02",
      "authors_text": "Xiang Li, Yupeng Zheng, Pengfei Li, Yilun Chen, Ya-Qin Zhang",
      "is_highlight": false,
      "score": 0,
      "summary": "DiScene introduces a sparse query-based framework for efficient and robust indoor occupancy prediction through multi-level knowledge distillation, achieving state-of-the-art performance and faster inference.",
      "teaser_image": "https://arxiv.org/html/2602.02318/x2.png",
      "debug_abstract": "Occupancy prediction provides critical geometric and semantic understanding for robotics but faces efficiency-accuracy trade-offs. Current dense methods suffer computational waste on empty voxels, while sparse query-based approaches lack robustness in diverse and complex indoor scenes. In this paper, we propose DiScene, a novel sparse query-based framework that leverages multi-level distillation to achieve efficient and robust occupancy prediction. In particular, our method incorporates two key innovations: (1) a Multi-level Consistent Knowledge Distillation strategy, which transfers hierarchical representations from large teacher models to lightweight students through coordinated alignment across four levels, including encoder-level feature alignment, query-level feature matching, prior-level spatial guidance, and anchor-level high-confidence knowledge transfer and (2) a Teacher-Guided Initialization policy, employing optimized parameter warm-up to accelerate model convergence. Validated on the Occ-Scannet benchmark, DiScene achieves 23.2 FPS without depth priors while outperforming our baseline method, OPUS, by 36.1% and even better than the depth-enhanced version, OPUS†. With depth integration, DiScene† attains new SOTA performance, surpassing EmbodiedOcc by 3.7% with 1.62$\\times$ faster inference speed. Furthermore, experiments on the Occ3D-nuScenes benchmark and in-the-wild scenarios demonstrate the versatility of our approach in various environments. Code and models can be accessed at this https URL."
    },
    {
      "title": "Beyond Precision: Training-Inference Mismatch is an Optimization Problem and Simple LR Scheduling Fixes It",
      "url": "https://arxiv.org/abs/2602.01826",
      "date": "2026-02-02",
      "authors_text": "Yaxiang Zhang, Yingru Li, Jiacai Liu, Jiawei Xu, Ziniu Li",
      "is_highlight": false,
      "score": 35.0,
      "summary": "This paper presents a dynamic learning rate scheduling method to mitigate training-inference mismatch and stabilize reinforcement learning in large language models by addressing gradient noise.",
      "teaser_image": "https://arxiv.org/html/2602.01826/Qwen3-4B_baseline_vs_importance_sampling_patches.png",
      "debug_abstract": "Reinforcement Learning (RL) for training Large Language Models is notoriously unstable. While recent studies attribute this to &#34;training inference mismatch stemming&#34; from inconsistent hybrid engines, standard remedies, such as Importance Sampling, might fail during extended training runs. In this work, we analyze this instability through the lens of optimization, demonstrating that gradient noise and training-inference mismatch escalate in tandem as training progresses. Meanwhile, we find that the mismatch can be effectively suppressed by shrinking the update size. Taken together, we deduce that the mismatch is not merely a static numerical discrepancy, but a dynamic failure coupled with the model&#39;s optimization. Based on this insight, we propose a simple yet effective solution: a specialized Learning Rate (LR) scheduler. Instead of pre-defined decay schedule in traditional LR scheduler, our method dynamically triggers LR decay based on response length, which we identify as a reliable early-warning signal for impending instability. Empirical evidence suggests that by reducing the learning rate as gradient noise rises, we can consistently stabilize RL training and keep the training-inference mismatch at a safe level."
    },
    {
      "title": "VVLoc: Prior-free 3-DoF Vehicle Visual Localization",
      "url": "https://arxiv.org/abs/2602.00810",
      "date": "2026-01-31",
      "authors_text": "Ze Huang, Zhongyang Xiao, Mingliang Song, Longan Yang, Hongyuan Yuan",
      "is_highlight": false,
      "score": 73.0,
      "summary": "VVLoc is a unified neural network approach for simultaneous topological and metric vehicle localization using multi-camera systems, providing confidence measures and requiring minimal training data.",
      "teaser_image": "https://arxiv.org/html/2602.00810/x1.png",
      "debug_abstract": "Localization is a critical technology in autonomous driving, encompassing both topological localization, which identifies the most similar map keyframe to the current observation, and metric localization, which provides precise spatial coordinates. Conventional methods typically address these tasks independently, rely on single-camera setups, and often require additional 3D semantic or pose priors, while lacking mechanisms to quantify the confidence of localization results, making them less feasible for real industrial applications. In this paper, we propose VVLoc, a unified pipeline that employs a single neural network to concurrently achieve topological and metric vehicle localization using multi-camera system. VVLoc first evaluates the geo-proximity between visual observations, then estimates their relative metric poses using a matching strategy, while also providing a confidence measure. Additionally, the training process for VVLoc is highly efficient, requiring only pairs of visual data and corresponding ground-truth poses, eliminating the need for complex supplementary data. We evaluate VVLoc not only on the publicly available datasets, but also on a more challenging self-collected dataset, demonstrating its ability to deliver state-of-the-art localization accuracy across a wide range of localization tasks."
    },
    {
      "title": "UniMotion: A Unified Motion Framework for Simulation, Prediction and Planning",
      "url": "https://arxiv.org/abs/2602.00566",
      "date": "2026-01-31",
      "authors_text": "Nan Song, Junzhe Jiang, Jingyu Li, Xiatian Zhu, Li Zhang",
      "is_highlight": false,
      "score": 89.0,
      "summary": "UniMotion is a unified framework leveraging a Transformer architecture to enhance simulation, prediction, and planning in autonomous driving by integrating shared capabilities across these tasks.",
      "teaser_image": "https://arxiv.org/html/2602.00566/x1.png",
      "debug_abstract": "Motion simulation, prediction and planning are foundational tasks in autonomous driving, each essential for modeling and reasoning about dynamic traffic scenarios. While often addressed in isolation due to their differing objectives, such as generating diverse motion states or estimating optimal trajectories, these tasks inherently depend on shared capabilities: understanding multi-agent interactions, modeling motion behaviors, and reasoning over temporal and spatial dynamics. Despite this underlying commonality, existing approaches typically adopt specialized model designs, which hinders cross-task generalization and system scalability. More critically, this separation overlooks the potential mutual benefits among tasks. Motivated by these observations, we propose UniMotion, a unified motion framework that captures shared structures across motion tasks while accommodating their individual requirements. Built on a decoder-only Transformer architecture, UniMotion employs dedicated interaction modes and tailored training strategies to simultaneously support these motion tasks. This unified design not only enables joint optimization and representation sharing but also allows for targeted fine-tuning to specialize in individual tasks when needed. Extensive experiments on the Waymo Open Motion Dataset demonstrate that joint training leads to robust generalization and effective task integration. With further fine-tuning, UniMotion achieves state-of-the-art performance across a range of motion tasks, establishing it as a versatile and scalable solution for autonomous driving."
    }
  ],
  "南京大学": [
    {
      "title": "TIDE: Trajectory-based Diagnostic Evaluation of Test-Time Improvement in LLM Agents",
      "url": "https://arxiv.org/abs/2602.02196",
      "date": "2026-02-02",
      "authors_text": "Hang Yan, Xinyu Che, Fangzhi Xu, Qiushi Sun, Zichen Ding",
      "is_highlight": false,
      "score": 20.0,
      "summary": "The TIDE framework evaluates Test-Time Improvement in LLM agents by analyzing task completion dynamics, recursive behaviors, and memory constraints to enhance performance optimization.",
      "teaser_image": "https://arxiv.org/html/2602.02196/x1.png",
      "debug_abstract": "Recent advances in autonomous LLM agents demonstrate their ability to improve performance through iterative interaction with the environment. We define this paradigm as Test-Time Improvement (TTI). However, the mechanisms under how and why TTI succeed or fail remain poorly understood, and existing evaluation metrics fail to capture their task optimization efficiency, behavior adaptation after erroneous actions, and the specific utility of working memory for task completion. To address these gaps, we propose Test-time Improvement Diagnostic Evaluation (TIDE), an agent-agnostic and environment-agnostic framework that decomposes TTI into three comprehensive and interconnected dimensions. The framework measures (1) the overall temporal dynamics of task completion and (2) identifies whether performance is primarily constrained by recursive looping behaviors or (3) by burdensome accumulated memory. Through extensive experiments across diverse agents and environments, TIDE highlights that improving agent performance requires more than scaling internal reasoning, calling for explicitly optimizing the interaction dynamics between the agent and the environment."
    },
    {
      "title": "Learning Adaptive Cross-Embodiment Visuomotor Policy with Contrastive Prompt Orchestration",
      "url": "https://arxiv.org/abs/2602.01040",
      "date": "2026-02-01",
      "authors_text": "Yuhang Zhang, Chao Yan, Jiaxi Yu, Jiaping Xiao, Mir Feroskhan",
      "is_highlight": false,
      "score": 92.0,
      "summary": "The paper presents ContrAstive Prompt Orchestration (CAPO), a method for improving adaptive visuomotor policies in diverse environments through hybrid contrastive learning and dynamic prompt orchestration.",
      "teaser_image": "https://arxiv.org/html/2602.01040/x4.png",
      "debug_abstract": "Learning adaptive visuomotor policies for embodied agents remains a formidable challenge, particularly when facing cross-embodiment variations such as diverse sensor configurations and dynamic properties. Conventional learning approaches often struggle to separate task-relevant features from domain-specific variations (e.g., lighting, field-of-view, and rotation), leading to poor sample efficiency and catastrophic failure in unseen environments. To bridge this gap, we propose ContrAstive Prompt Orchestration (CAPO), a novel approach for learning visuomotor policies that integrates contrastive prompt learning and adaptive prompt orchestration. For prompt learning, we devise a hybrid contrastive learning strategy that integrates visual, temporal action, and text objectives to establish a pool of learnable prompts, where each prompt induces a visual representation encapsulating fine-grained domain factors. Based on these learned prompts, we introduce an adaptive prompt orchestration mechanism that dynamically aggregates these prompts conditioned on current observations. This enables the agent to adaptively construct optimal state representations by identifying dominant domain factors instantaneously. Consequently, the policy optimization is effectively shielded from irrelevant interference, preventing the common issue of overfitting to source domains. Extensive experiments demonstrate that CAPO significantly outperforms state-of-the-art baselines in sample efficiency and asymptotic performance. Crucially, it exhibits superior zero-shot adaptation across unseen target domains characterized by drastic environmental (e.g., illumination) and physical shifts (e.g., field-of-view and rotation), validating its effectiveness as a viable solution for cross-embodiment visuomotor policy adaptation."
    },
    {
      "title": "GLAD: Generative Language-Assisted Visual Tracking for Low-Semantic Templates",
      "url": "https://arxiv.org/abs/2602.00570",
      "date": "2026-01-31",
      "authors_text": "Xingyu Luo, Yidong Cai, Jie Liu, Jie Tang, Gangshan Wu",
      "is_highlight": false,
      "score": 70.0,
      "summary": "GLAD introduces a generative model that enhances vision-language tracking by improving compatibility between low-semantic images and text, achieving state-of-the-art performance and speed.",
      "teaser_image": "https://arxiv.org/html/2602.00570/x3.png",
      "debug_abstract": "Vision-language tracking has gained increasing attention in many scenarios. This task simultaneously deals with visual and linguistic information to localize objects in videos. Despite its growing utility, the development of vision-language tracking methods remains in its early stage. Current vision-language trackers usually employ Transformer architectures for interactive integration of template, search, and text features. However, persistent challenges about low-semantic images including prevalent image blurriness, low resolution and so on, may compromise model performance through degraded cross-modal understanding. To solve this problem, language assistance is usually used to deal with the obstacles posed by low-semantic images. However, due to the existing gap between current textual and visual features, direct concatenation and fusion of these features may have limited effectiveness. To address these challenges, we introduce a pioneering Generative Language-AssisteD tracking model, GLAD, which utilizes diffusion models for the generative multi-modal fusion of text description and template image to bolster compatibility between language and image and enhance template image semantic information. Our approach demonstrates notable improvements over the existing fusion paradigms. Blurry and semantically ambiguous template images can be restored to improve multi-modal features in the generative fusion paradigm. Experiments show that our method establishes a new state-of-the-art on multiple benchmarks and achieves an impressive inference speed. The code and models will be released at: this https URL"
    }
  ],
  "SU Lab": [
    {
      "title": "Towards Agentic Intelligence for Materials Science",
      "url": "https://arxiv.org/abs/2602.00169",
      "date": "2026-01-29",
      "authors_text": "Huan Zhang, Yizhan Li, Wenhao Huang, Ziyu Hou, Yu Song",
      "is_highlight": false,
      "score": 12.0,
      "summary": "This paper advocates for developing agentic AI systems in materials science that autonomously plan, act, and learn to enhance discovery processes beyond traditional models.",
      "teaser_image": "https://arxiv.org/html/2602.00169/x1.png",
      "debug_abstract": "The convergence of artificial intelligence and materials science presents a transformative opportunity, but achieving true acceleration in discovery requires moving beyond task-isolated, fine-tuned models toward agentic systems that plan, act, and learn across the full discovery loop. This survey advances a unique pipeline-centric view that spans from corpus curation and pretraining, through domain adaptation and instruction tuning, to goal-conditioned agents interfacing with simulation and experimental platforms. Unlike prior reviews, we treat the entire process as an end-to-end system to be optimized for tangible discovery outcomes rather than proxy benchmarks. This perspective allows us to trace how upstream design choices-such as data curation and training objectives-can be aligned with downstream experimental success through effective credit assignment. To bridge communities and establish a shared frame of reference, we first present an integrated lens that aligns terminology, evaluation, and workflow stages across AI and materials science. We then analyze the field through two focused lenses: From the AI perspective, the survey details LLM strengths in pattern recognition, predictive analytics, and natural language processing for literature mining, materials characterization, and property prediction; from the materials science perspective, it highlights applications in materials design, process optimization, and the acceleration of computational workflows via integration with external tools (e.g., DFT, robotic labs). Finally, we contrast passive, reactive approaches with agentic design, cataloging current contributions while motivating systems that pursue long-horizon goals with autonomy, memory, and tool use. This survey charts a practical roadmap towards autonomous, safety-aware LLM agents aimed at discovering novel and useful materials."
    },
    {
      "title": "Learning Modal-Mixed Chain-of-Thought Reasoning with Latent Embeddings",
      "url": "https://arxiv.org/abs/2602.00574",
      "date": "2026-01-31",
      "authors_text": "Yifei Shao, Kun Zhou, Ziming Xu, Mohammad Atif Quamar, Shibo Hao",
      "is_highlight": false,
      "score": 65.0,
      "summary": "The paper introduces modal-mixed chain-of-thought reasoning, integrating textual and visual latent embeddings to enhance multimodal reasoning in vision-intensive tasks, outperforming traditional methods.",
      "teaser_image": "https://arxiv.org/html/2602.00574/x1.png",
      "debug_abstract": "We study how to extend chain-of-thought (CoT) beyond language to better handle multimodal reasoning. While CoT helps LLMs and VLMs articulate intermediate steps, its text-only form often fails on vision-intensive problems where key intermediate states are inherently visual. We introduce modal-mixed CoT, which interleaves textual tokens with compact visual sketches represented as latent embeddings. To bridge the modality gap without eroding the original knowledge and capability of the VLM, we use the VLM itself as an encoder and train the language backbone to reconstruct its own intermediate vision embeddings, to guarantee the semantic alignment of the visual latent space. We further attach a diffusion-based latent decoder, invoked by a special control token and conditioned on hidden states from the VLM. In this way, the diffusion head carries fine-grained perceptual details while the VLM specifies high-level intent, which cleanly disentangles roles and reduces the optimization pressure of the VLM. Training proceeds in two stages: supervised fine-tuning on traces that interleave text and latents with a joint next-token and latent-reconstruction objective, followed by reinforcement learning that teaches when to switch modalities and how to compose long reasoning chains. Extensive experiments across 11 diverse multimodal reasoning tasks, demonstrate that our method yields better performance than language-only and other CoT methods. Our code will be publicly released."
    }
  ],
  "浙江大学": [
    {
      "title": "SWE-Universe: Scale Real-World Verifiable Environments to Millions",
      "url": "https://arxiv.org/abs/2602.02361",
      "date": "2026-02-02",
      "authors_text": "Mouxiang Chen, Lei Zhang, Yunlong Feng, Xuwu Wang, Wenting Zhao",
      "is_highlight": false,
      "score": 16.0,
      "summary": "SWE-Universe introduces a scalable framework for creating millions of verifiable software engineering environments from GitHub pull requests, enhancing agent training and performance.",
      "teaser_image": "https://arxiv.org/html/2602.02361/figures/env-build.png",
      "debug_abstract": "We propose SWE-Universe, a scalable and efficient framework for automatically constructing real-world software engineering (SWE) verifiable environments from GitHub pull requests (PRs). To overcome the prevalent challenges of automatic building, such as low production yield, weak verifiers, and prohibitive cost, our framework utilizes a building agent powered by an efficient custom-trained model. This agent employs iterative self-verification and in-loop hacking detection to ensure the reliable generation of high-fidelity, verifiable tasks. Using this method, we scale the number of real-world multilingual SWE environments to a million scale (807,693). We demonstrate the profound value of our environments through large-scale agentic mid-training and reinforcement learning. Finally, we applied this technique to Qwen3-Max-Thinking and achieved a score of 75.3% on SWE-Bench Verified. Our work provides both a critical resource and a robust methodology to advance the next generation of coding agents."
    },
    {
      "title": "From Frames to Sequences: Temporally Consistent Human-Centric Dense Prediction",
      "url": "https://arxiv.org/abs/2602.01661",
      "date": "2026-02-02",
      "authors_text": "Xingyu Miao, Junting Dong, Qin Zhao, Yuhang Yang, Junhao Chen",
      "is_highlight": false,
      "score": 43.0,
      "summary": "This paper presents a novel synthetic data pipeline and a ViT-based model for achieving temporally consistent human-centric dense prediction in video sequences, outperforming existing methods.",
      "teaser_image": "https://arxiv.org/html/2602.01661/x2.png",
      "debug_abstract": "In this work, we focus on the challenge of temporally consistent human-centric dense prediction across video sequences. Existing models achieve strong per-frame accuracy but often flicker under motion, occlusion, and lighting changes, and they rarely have paired human video supervision for multiple dense tasks. We address this gap with a scalable synthetic data pipeline that generates photorealistic human frames and motion-aligned sequences with pixel-accurate depth, normals, and masks. Unlike prior static data synthetic pipelines, our pipeline provides both frame-level labels for spatial learning and sequence-level supervision for temporal learning. Building on this, we train a unified ViT-based dense predictor that (i) injects an explicit human geometric prior via CSE embeddings and (ii) improves geometry-feature reliability with a lightweight channel reweighting module after feature fusion. Our two-stage training strategy, combining static pretraining with dynamic sequence supervision, enables the model first to acquire robust spatial representations and then refine temporal consistency across motion-aligned sequences. Extensive experiments show that we achieve state-of-the-art performance on THuman2.1 and Hi4D and generalize effectively to in-the-wild videos."
    },
    {
      "title": "Know Your Step: Faster and Better Alignment for Flow Matching Models via Step-aware Advantages",
      "url": "https://arxiv.org/abs/2602.01591",
      "date": "2026-02-02",
      "authors_text": "Zhixiong Yue, Zixuan Ni, Feiyang Ye, Jinshan Zhang, Sheng Shen",
      "is_highlight": false,
      "score": 39.0,
      "summary": "The paper presents TAFS GRPO, a novel framework enhancing flow matching models for text-to-image generation by improving alignment with human preferences through efficient few-step sampling and adaptive noise.",
      "teaser_image": "https://arxiv.org/html/2602.01591/x3.png",
      "debug_abstract": "Recent advances in flow matching models, particularly with reinforcement learning (RL), have significantly enhanced human preference alignment in few step text to image generators. However, existing RL based approaches for flow matching models typically rely on numerous denoising steps, while suffering from sparse and imprecise reward signals that often lead to suboptimal alignment. To address these limitations, we propose Temperature Annealed Few step Sampling with Group Relative Policy Optimization (TAFS GRPO), a novel framework for training flow matching text to image models into efficient few step generators well aligned with human preferences. Our method iteratively injects adaptive temporal noise onto the results of one step samples. By repeatedly annealing the model&#39;s sampled outputs, it introduces stochasticity into the sampling process while preserving the semantic integrity of each generated image. Moreover, its step aware advantage integration mechanism combines the GRPO to avoid the need for the differentiable of reward function and provide dense and step specific rewards for stable policy optimization. Extensive experiments demonstrate that TAFS GRPO achieves strong performance in few step text to image generation and significantly improves the alignment of generated images with human preferences. The code and models of this work will be available to facilitate further research."
    },
    {
      "title": "TriphiBot: A Triphibious Robot Combining FOC-based Propulsion with Eccentric Design",
      "url": "https://arxiv.org/abs/2602.01385",
      "date": "2026-02-01",
      "authors_text": "Xiangyu Li, Mingwei Lai, Mengke Zhang, Junxiao Lin, Tiancheng Lai",
      "is_highlight": false,
      "score": 96.0,
      "summary": "The TriphiBot is a novel triphibious robot utilizing a minimalist design and FOC-based propulsion for efficient multi-domain motion and seamless transitions across air, land, and water.",
      "teaser_image": "https://arxiv.org/html/2602.01385/x6.png",
      "debug_abstract": "Triphibious robots capable of multi-domain motion and cross-domain transitions are promising to handle complex tasks across diverse environments. However, existing designs primarily focus on dual-mode platforms, and some designs suffer from high mechanical complexity or low propulsion efficiency, which limits their application. In this paper, we propose a novel triphibious robot capable of aerial, terrestrial, and aquatic motion, by a minimalist design combining a quadcopter structure with two passive wheels, without extra actuators. To address inefficiency of ground-support motion (moving on land/seabed) for quadcopter based designs, we introduce an eccentric Center of Gravity (CoG) design that inherently aligns thrust with motion, enhancing efficiency without specialized mechanical transformation designs. Furthermore, to address the drastic differences in motion control caused by different fluids (air and water), we develop a unified propulsion system based on Field-Oriented Control (FOC). This method resolves torque matching issues and enables precise, rapid bidirectional thrust across different mediums. Grounded in the perspective of living condition and ground support, we analyse the robot&#39;s dynamics and propose a Hybrid Nonlinear Model Predictive Control (HNMPC)-PID control system to ensure stable multi-domain motion and seamless transitions. Experimental results validate the robot&#39;s multi-domain motion and cross-mode transition capability, along with the efficiency and adaptability of the proposed propulsion system."
    },
    {
      "title": "PolicyFlow: Policy Optimization with Continuous Normalizing Flow in Reinforcement Learning",
      "url": "https://arxiv.org/abs/2602.01156",
      "date": "2026-02-01",
      "authors_text": "Shunpeng Yang, Ben Liu, Hua Chen",
      "is_highlight": false,
      "score": 71.0,
      "summary": "PolicyFlow is a novel reinforcement learning algorithm that integrates continuous normalizing flows with PPO, enhancing performance and stability while reducing computational costs and encouraging diverse behaviors.",
      "teaser_image": "https://arxiv.org/html/2602.01156/x1.png",
      "debug_abstract": "Among on-policy reinforcement learning algorithms, Proximal Policy Optimization (PPO) demonstrates is widely favored for its simplicity, numerical stability, and strong empirical performance. Standard PPO relies on surrogate objectives defined via importance ratios, which require evaluating policy likelihood that is typically straightforward when the policy is modeled as a Gaussian distribution. However, extending PPO to more expressive, high-capacity policy models such as continuous normalizing flows (CNFs), also known as flow-matching models, is challenging because likelihood evaluation along the full flow trajectory is computationally expensive and often numerically unstable. To resolve this issue, we propose PolicyFlow, a novel on-policy CNF-based reinforcement learning algorithm that integrates expressive CNF policies with PPO-style objectives without requiring likelihood evaluation along the full flow path. PolicyFlow approximates importance ratios using velocity field variations along a simple interpolation path, reducing computational overhead without compromising training stability. To further prevent mode collapse and further encourage diverse behaviors, we propose the Brownian Regularizer, an implicit policy entropy regularizer inspired by Brownian motion, which is conceptually elegant and computationally lightweight. Experiments on diverse tasks across various environments including MultiGoal, PointMaze, IsaacLab and MuJoCo Playground show that PolicyFlow achieves competitive or superior performance compared to PPO using Gaussian policies and flow-based baselines including FPO and DPPO. Notably, results on MultiGoal highlight PolicyFlow&#39;s ability to capture richer multimodal action distributions."
    },
    {
      "title": "LightCity: An Urban Dataset for Outdoor Inverse Rendering and Reconstruction under Multi-illumination Conditions",
      "url": "https://arxiv.org/abs/2602.01118",
      "date": "2026-02-01",
      "authors_text": "Jingjing Wang, Qirui Hu, Chong Bao, Yuke Zhu, Hujun Bao",
      "is_highlight": false,
      "score": 69.0,
      "summary": "LightCity is a comprehensive synthetic urban dataset designed for inverse rendering and 3D reconstruction, addressing challenges of complex multi-illumination conditions with over 50K images.",
      "teaser_image": "https://arxiv.org/html/2602.01118/x2.png",
      "debug_abstract": "Inverse rendering in urban scenes is pivotal for applications like autonomous driving and digital twins. Yet, it faces significant challenges due to complex illumination conditions, including multi-illumination and indirect light and shadow effects. However, the effects of these challenges on intrinsic decomposition and 3D reconstruction have not been explored due to the lack of appropriate datasets. In this paper, we present LightCity, a novel high-quality synthetic urban dataset featuring diverse illumination conditions with realistic indirect light and shadow effects. LightCity encompasses over 300 sky maps with highly controllable illumination, varying scales with street-level and aerial perspectives over 50K images, and rich properties such as depth, normal, material components, light and indirect light, etc. Besides, we leverage LightCity to benchmark three fundamental tasks in the urban environments and conduct a comprehensive analysis of these benchmarks, laying a robust foundation for advancing related research."
    },
    {
      "title": "TransNormal: Dense Visual Semantics for Diffusion-based Transparent Object Normal Estimation",
      "url": "https://arxiv.org/abs/2602.00839",
      "date": "2026-01-31",
      "authors_text": "Mingwei Li, Hehe Fan, Yi Yang",
      "is_highlight": false,
      "score": 68.0,
      "summary": "TransNormal is a novel framework for monocular normal estimation of transparent objects, leveraging diffusion priors and dense visual semantics to enhance accuracy and detail preservation.",
      "teaser_image": "https://arxiv.org/html/2602.00839/sources/teasers/ablation_results_teaser_a/input.png",
      "debug_abstract": "Monocular normal estimation for transparent objects is critical for laboratory automation, yet it remains challenging due to complex light refraction and reflection. These optical properties often lead to catastrophic failures in conventional depth and normal sensors, hindering the deployment of embodied AI in scientific environments. We propose TransNormal, a novel framework that adapts pre-trained diffusion priors for single-step normal regression. To handle the lack of texture in transparent surfaces, TransNormal integrates dense visual semantics from DINOv3 via a cross-attention mechanism, providing strong geometric cues. Furthermore, we employ a multi-task learning objective and wavelet-based regularization to ensure the preservation of fine-grained structural details. To support this task, we introduce TransNormal-Synthetic, a physics-based dataset with high-fidelity normal maps for transparent labware. Extensive experiments demonstrate that TransNormal significantly outperforms state-of-the-art methods: on the ClearGrasp benchmark, it reduces mean error by 24.4% and improves 11.25° accuracy by 22.8%; on ClearPose, it achieves a 15.2% reduction in mean error. The code and dataset will be made publicly available at this https URL."
    },
    {
      "title": "Resource-Efficient Reinforcement for Reasoning Large Language Models via Dynamic One-Shot Policy Refinement",
      "url": "https://arxiv.org/abs/2602.00815",
      "date": "2026-01-31",
      "authors_text": "Yunjian Zhang, Sudong Wang, Yang Li, Peiran Xu, Conghao Zhou",
      "is_highlight": false,
      "score": 57.0,
      "summary": "The paper introduces Dynamic One-Shot Policy Refinement (DoPR), a resource-efficient reinforcement learning strategy that enhances reasoning in large language models while significantly reducing training costs.",
      "teaser_image": "https://arxiv.org/html/2602.00815/dopr.png",
      "debug_abstract": "Large language models (LLMs) have exhibited remarkable performance on complex reasoning tasks, with reinforcement learning under verifiable rewards (RLVR) emerging as a principled framework for aligning model behavior with reasoning chains. Despite its promise, RLVR remains prohibitively resource-intensive, requiring extensive reward signals and incurring substantial rollout costs during training. In this work, we revisit the fundamental question of data and compute efficiency in RLVR. We first establish a theoretical lower bound on the sample complexity required to unlock reasoning capabilities, and empirically validate that strong performance can be achieved with a surprisingly small number of training instances. To tackle the computational burden, we propose Dynamic One-Shot Policy Refinement (DoPR), an uncertainty-aware RL strategy that dynamically selects a single informative training sample per batch for policy updates, guided by reward volatility and exploration-driven acquisition. DoPR reduces rollout overhead by nearly an order of magnitude while preserving competitive reasoning accuracy, offering a scalable and resource-efficient solution for LLM post-training. This approach offers a practical path toward more efficient and accessible RL-based training for reasoning-intensive LLM applications."
    },
    {
      "title": "USS-Nav: Unified Spatio-Semantic Scene Graph for Lightweight UAV Zero-Shot Object Navigation",
      "url": "https://arxiv.org/abs/2602.00708",
      "date": "2026-01-31",
      "authors_text": "Weiqi Gai, Yuman Gao, Yuan Zhou, Yufan Xie, Zhiyang Liu",
      "is_highlight": false,
      "score": 80.0,
      "summary": "USS-Nav introduces a lightweight framework for UAV zero-shot object navigation, utilizing a Unified Spatio-Semantic scene graph for efficient exploration and improved computational performance.",
      "teaser_image": "https://arxiv.org/html/2602.00708/figs/skeleton_gen.png",
      "debug_abstract": "Zero-Shot Object Navigation in unknown environments poses significant challenges for Unmanned Aerial Vehicles (UAVs) due to the conflict between high-level semantic reasoning requirements and limited onboard computational resources. To address this, we present USS-Nav, a lightweight framework that incrementally constructs a Unified Spatio-Semantic scene graph and enables efficient Large Language Model (LLM)-augmented Zero-Shot Object Navigation in unknown environments. Specifically, we introduce an incremental Spatial Connectivity Graph generation method utilizing polyhedral expansion to capture global geometric topology, which is dynamically partitioned into semantic regions via graph clustering. Concurrently, open-vocabulary object semantics are instantiated and anchored to this topology to form a hierarchical environmental representation. Leveraging this hierarchical structure, we present a coarse-to-fine exploration strategy: LLM grounded in the scene graph&#39;s semantics to determine global target regions, while a local planner optimizes frontier coverage based on information gain. Experimental results demonstrate that our framework outperforms state-of-the-art methods in terms of computational efficiency and real-time update frequency (15 Hz) on a resource-constrained platform. Furthermore, ablation studies confirm the effectiveness of our framework, showing substantial improvements in Success weighted by Path Length (SPL). The source code will be made publicly available to foster further research."
    },
    {
      "title": "V2X-DSC: Multi-Agent Collaborative Perception with Distributed Source Coding Guided Communication",
      "url": "https://arxiv.org/abs/2602.00687",
      "date": "2026-01-31",
      "authors_text": "Yuankun Zeng, Shaohui Li, Zhi Li, Shulan Ruan, Yu Liu",
      "is_highlight": false,
      "score": 76.0,
      "summary": "V2X-DSC introduces a Conditional Codec for efficient multi-agent collaborative perception by compressing and reconstructing BEV features, optimizing bandwidth usage while maintaining accuracy.",
      "teaser_image": "https://arxiv.org/html/2602.00687/figs/overview.jpg",
      "debug_abstract": "Collaborative perception improves 3D understanding by fusing multi-agent observations, yet intermediate-feature sharing faces strict bandwidth constraints as dense BEV features saturate V2X links. We observe that collaborators view the same physical world, making their features strongly correlated; thus receivers only need innovation beyond their local context. Revisiting this from a distributed source coding perspective, we propose V2X-DSC, a framework with a Conditional Codec (DCC) for bandwidth-constrained fusion. The sender compresses BEV features into compact codes, while the receiver performs conditional reconstruction using its local features as side information, allocating bits to complementary cues rather than redundant content. This conditional structure regularizes learning, encouraging incremental representation and yielding lower-noise features. Experiments on DAIR-V2X, OPV2V, and V2X-Real demonstrate state-of-the-art accuracy-bandwidth trade-offs under KB-level communication, and generalizes as a plug-and-play communication layer across multiple fusion backbones."
    }
  ],
  "ROSE Lab": [
    {
      "title": "FD-VLA: Force-Distilled Vision-Language-Action Model for Contact-Rich Manipulation",
      "url": "https://arxiv.org/abs/2602.02142",
      "date": "2026-02-02",
      "authors_text": "Ruiteng Zhao, Wenshuo Wang, Yicheng Ma, Xiaocong Li, Francis E. H. Tay",
      "is_highlight": false,
      "score": 0,
      "summary": "The FD-VLA framework enhances contact-rich manipulation by integrating force awareness through a novel distillation method, improving performance without physical force sensors.",
      "teaser_image": "https://arxiv.org/html/2602.02142/x1.png",
      "debug_abstract": "Force sensing is a crucial modality for Vision-Language-Action (VLA) frameworks, as it enables fine-grained perception and dexterous manipulation in contact-rich tasks. We present Force-Distilled VLA (FD-VLA), a novel framework that integrates force awareness into contact-rich manipulation without relying on physical force sensors. The core of our approach is a Force Distillation Module (FDM), which distills force by mapping a learnable query token, conditioned on visual observations and robot states, into a predicted force token aligned with the latent representation of actual force signals. During inference, this distilled force token is injected into the pretrained VLM, enabling force-aware reasoning while preserving the integrity of its vision-language semantics. This design provides two key benefits: first, it allows practical deployment across a wide range of robots that lack expensive or fragile force-torque sensors, thereby reducing hardware cost and complexity; second, the FDM introduces an additional force-vision-state fusion prior to the VLM, which improves cross-modal alignment and enhances perception-action robustness in contact-rich scenarios. Surprisingly, our physical experiments show that the distilled force token outperforms direct sensor force measurements as well as other baselines, which highlights the effectiveness of this force-distilled VLA approach."
    },
    {
      "title": "FlowSteer: Interactive Agentic Workflow Orchestration via End-to-End Reinforcement Learning",
      "url": "https://arxiv.org/abs/2602.01664",
      "date": "2026-02-02",
      "authors_text": "Mingda Zhang, Haoran Luo, Tiesunlong Shen, Qika Lin, Xiaoying Tang",
      "is_highlight": false,
      "score": 7.0,
      "summary": "FlowSteer is an end-to-end reinforcement learning framework that automates workflow orchestration through interactive learning, addressing manual costs and improving performance across diverse tasks.",
      "teaser_image": "https://arxiv.org/html/2602.01664/x1.png",
      "debug_abstract": "In recent years, a variety of powerful agentic workflows have been applied to solve a wide range of human problems. However, existing workflow orchestration still faces key challenges, including high manual cost, reliance on specific operators/large language models (LLMs), and sparse reward signals. To address these challenges, we propose FlowSteer, an end-to-end reinforcement learning framework that takes a lightweight policy model as the agent and an executable canvas environment, automating workflow orchestration through multi-turn interaction. In this process, the policy model analyzes execution states and selects editing actions, while the canvas executes operators and returns feedback for iterative refinement. Moreover, FlowSteer provides a plug-and-play framework that supports diverse operator libraries and interchangeable LLM backends. To effectively train this interaction paradigm, we propose Canvas Workflow Relative Policy Optimization (CWRPO), which introduces diversity-constrained rewards with conditional release to stabilize learning and suppress shortcut behaviors. Experimental results on twelve datasets show that FlowSteer significantly outperforms baselines across various tasks."
    },
    {
      "title": "UV-M3TL: A Unified and Versatile Multimodal Multi-Task Learning Framework for Assistive Driving Perception",
      "url": "https://arxiv.org/abs/2602.01594",
      "date": "2026-02-02",
      "authors_text": "Wenzhuo Liu, Qiannan Guo, Zhen Wang, Wenshuo Wang, Lei Yang",
      "is_highlight": false,
      "score": 8.0,
      "summary": "The UV-M3TL framework enhances ADAS by effectively recognizing driver behavior and context through dual-branch multimodal embedding and adaptive loss, achieving state-of-the-art performance across multiple tasks.",
      "teaser_image": "https://arxiv.org/html/2602.01594/x2.png",
      "debug_abstract": "Advanced Driver Assistance Systems (ADAS) need to understand human driver behavior while perceiving their navigation context, but jointly learning these heterogeneous tasks would cause inter-task negative transfer and impair system performance. Here, we propose a Unified and Versatile Multimodal Multi-Task Learning (UV-M3TL) framework to simultaneously recognize driver behavior, driver emotion, vehicle behavior, and traffic context, while mitigating inter-task negative transfer. Our framework incorporates two core components: dual-branch spatial channel multimodal embedding (DB-SCME) and adaptive feature-decoupled multi-task loss (AFD-Loss). DB-SCME enhances cross-task knowledge transfer while mitigating task conflicts by employing a dual-branch structure to explicitly model salient task-shared and task-specific features. AFD-Loss improves the stability of joint optimization while guiding the model to learn diverse multi-task representations by introducing an adaptive weighting mechanism based on learning dynamics and feature decoupling constraints. We evaluate our method on the AIDE dataset, and the experimental results demonstrate that UV-M3TL achieves state-of-the-art performance across all four tasks. To further prove the versatility, we evaluate UV-M3TL on additional public multi-task perception benchmarks (BDD100K, CityScapes, NYUD-v2, and PASCAL-Context), where it consistently delivers strong performance across diverse task combinations, attaining state-of-the-art results on most tasks."
    },
    {
      "title": "Failure-Aware Bimanual Teleoperation via Conservative Value Guided Assistance",
      "url": "https://arxiv.org/abs/2602.01092",
      "date": "2026-02-01",
      "authors_text": "Peng Zhou, Zhongxuan Li, Jinsong Wu, Jiaming Qi, Jun Hu",
      "is_highlight": false,
      "score": 9.0,
      "summary": "This paper presents a failure-aware bimanual teleoperation framework that uses conservative value learning to enhance task success and reduce operator workload through compliant haptic assistance.",
      "teaser_image": "https://arxiv.org/html/2602.01092/x1.png",
      "debug_abstract": "Teleoperation of high-precision manipulation is con-strained by tight success tolerances and complex contact dy-namics, which make impending failures difficult for human operators to anticipate under partial observability. This paper proposes a value-guided, failure-aware framework for bimanual teleoperation that provides compliant haptic assistance while pre-serving continuous human authority. The framework is trained entirely from heterogeneous offline teleoperation data containing both successful and failed executions. Task feasibility is mod-eled as a conservative success score learned via Conservative Value Learning, yielding a risk-sensitive estimate that remains reliable under distribution shift. During online operation, the learned success score regulates the level of assistance, while a learned actor provides a corrective motion direction. Both are integrated through a joint-space impedance interface on the master side, yielding continuous guidance that steers the operator away from failure-prone actions without overriding intent. Experimental results on contact-rich manipulation tasks demonstrate improved task success rates and reduced operator workload compared to conventional teleoperation and shared-autonomy baselines, indicating that conservative value learning provides an effective mechanism for embedding failure awareness into bilateral teleoperation. Experimental videos are available at this https URL"
    },
    {
      "title": "Superman: Unifying Skeleton and Vision for Human Motion Perception and Generation",
      "url": "https://arxiv.org/abs/2602.02401",
      "date": "2026-02-02",
      "authors_text": "Xinshun Wang, Peiming Li, Ziyi Wang, Zhongbin Fang, Zhichao Deng",
      "is_highlight": false,
      "score": 15.0,
      "summary": "Superman unifies visual perception and skeleton-based motion generation through a Vision-Guided Motion Tokenizer, enabling robust cross-modal learning and superior performance in human motion tasks.",
      "teaser_image": "https://arxiv.org/html/2602.02401/x2.png",
      "debug_abstract": "Human motion analysis tasks, such as temporal 3D pose estimation, motion prediction, and motion in-betweening, play an essential role in computer vision. However, current paradigms suffer from severe fragmentation. First, the field is split between ``perception&#39;&#39; models that understand motion from video but only output text, and ``generation&#39;&#39; models that cannot perceive from raw visual input. Second, generative MLLMs are often limited to single-frame, static poses using dense, parametric SMPL models, failing to handle temporal motion. Third, existing motion vocabularies are built from skeleton data alone, severing the link to the visual domain. To address these challenges, we introduce Superman, a unified framework that bridges visual perception with temporal, skeleton-based motion generation. Our solution is twofold. First, to overcome the modality disconnect, we propose a Vision-Guided Motion Tokenizer. Leveraging the natural geometric alignment between 3D skeletons and visual data, this module pioneers robust joint learning from both modalities, creating a unified, cross-modal motion vocabulary. Second, grounded in this motion language, a single, unified MLLM architecture is trained to handle all tasks. This module flexibly processes diverse, temporal inputs, unifying 3D skeleton pose estimation from video (perception) with skeleton-based motion prediction and in-betweening (generation). Extensive experiments on standard benchmarks, including Human3.6M, demonstrate that our unified method achieves state-of-the-art or competitive performance across all motion tasks. This showcases a more efficient and scalable path for generative motion analysis using skeletons."
    },
    {
      "title": "ClueTracer: Question-to-Vision Clue Tracing for Training-Free Hallucination Suppression in Multimodal Reasoning",
      "url": "https://arxiv.org/abs/2602.02004",
      "date": "2026-02-02",
      "authors_text": "Gongli Xi, Kun Wang, Zeming Gao, Huahui Yi, Haolang Lu",
      "is_highlight": false,
      "score": 24.0,
      "summary": "ClueTracer is a training-free plugin that suppresses hallucinations in multimodal reasoning by tracing visual clue propagation, enhancing model performance across various architectures.",
      "teaser_image": "https://arxiv.org/html/2602.02004/x1.png",
      "debug_abstract": "Large multimodal reasoning models solve challenging visual problems via explicit long-chain inference: they gather visual clues from images and decode clues into textual tokens. Yet this capability also increases hallucinations, where the model generates content that is not supported by the input image or the question. To understand this failure mode, we identify \\emph{reasoning drift}: during clue gathering, the model over-focuses on question-irrelevant entities, diluting focus on task-relevant cues and gradually decoupling the reasoning trace from visual grounding. As a consequence, many inference-time localization or intervention methods developed for non-reasoning models fail to pinpoint the true clues in reasoning settings. Motivated by these insights, we introduce ClueRecall, a metric for assessing visual clue retrieval, and present ClueTracer, a training-free, parameter-free, and architecture-agnostic plugin for hallucination suppression. ClueTracer starts from the question and traces how key clues propagate along the model&#39;s reasoning pathway (question $\\rightarrow$ outputs $\\rightarrow$ visual tokens), thereby localizing task-relevant patches while suppressing spurious attention to irrelevant regions. Remarkably, \\textbf{without any additional training}, ClueTracer improves all \\textbf{reasoning} architectures (including \\texttt{R1-OneVision}, \\texttt{Ocean-R1}, \\texttt{MM-Eureka}, \\emph{etc}.) by $\\mathbf{1.21\\times}$ on reasoning benchmarks. When transferred to \\textbf{non-reasoning} settings, it yields a $\\mathbf{1.14\\times}$ gain."
    },
    {
      "title": "SGHA-Attack: Semantic-Guided Hierarchical Alignment for Transferable Targeted Attacks on Vision-Language Models",
      "url": "https://arxiv.org/abs/2602.01574",
      "date": "2026-02-02",
      "authors_text": "Haobo Wang, Weiqi Luo, Xiaojun Jia, Xiaochun Cao",
      "is_highlight": false,
      "score": 44.0,
      "summary": "SGHA-Attack enhances targeted adversarial attacks on vision-language models by leveraging multiple semantic references and aligning intermediate features for improved transferability and robustness.",
      "teaser_image": "https://arxiv.org/html/2602.01574/x1.png",
      "debug_abstract": "Large vision-language models (VLMs) are vulnerable to transfer-based adversarial perturbations, enabling attackers to optimize on surrogate models and manipulate black-box VLM outputs. Prior targeted transfer attacks often overfit surrogate-specific embedding space by relying on a single reference and emphasizing final-layer alignment, which underutilizes intermediate semantics and degrades transfer across heterogeneous VLMs. To address this, we propose SGHA-Attack, a Semantic-Guided Hierarchical Alignment framework that adopts multiple target references and enforces intermediate-layer consistency. Concretely, we generate a visually grounded reference pool by sampling a frozen text-to-image model conditioned on the target prompt, and then carefully select the Top-K most semantically relevant anchors under the surrogate to form a weighted mixture for stable optimization guidance. Building on these anchors, SGHA-Attack injects target semantics throughout the feature hierarchy by aligning intermediate visual representations at both global and spatial granularities across multiple depths, and by synchronizing intermediate visual and textual features in a shared latent subspace to provide early cross-modal supervision before the final projection. Extensive experiments on open-source and commercial black-box VLMs show that SGHA-Attack achieves stronger targeted transferability than prior methods and remains robust under preprocessing and purification defenses."
    },
    {
      "title": "Estimating Force Interactions of Deformable Linear Objects from their Shapes",
      "url": "https://arxiv.org/abs/2602.01085",
      "date": "2026-02-01",
      "authors_text": "Qi Jing Chen, Shilin Shan, Timothy Bretl, Quang-Cuong Pham",
      "is_highlight": false,
      "score": 83.0,
      "summary": "This paper presents a novel method for estimating external forces on deformable linear objects using shape data, enhancing robot interaction safety and efficiency without requiring force sensors.",
      "teaser_image": "https://arxiv.org/html/2602.01085/x2.png",
      "debug_abstract": "This work introduces an analytical approach for detecting and estimating external forces acting on deformable linear objects (DLOs) using only their observed shapes. In many robot-wire interaction tasks, contact occurs not at the end-effector but at other points along the robot&#39;s body. Such scenarios arise when robots manipulate wires indirectly (e.g., by nudging) or when wires act as passive obstacles in the environment. Accurately identifying these interactions is crucial for safe and efficient trajectory planning, helping to prevent wire damage, avoid restricted robot motions, and mitigate potential hazards. Existing approaches often rely on expensive external force-torque sensor or that contacts occur at the end-effector for accurate force estimation. Using wire shape information acquired from a depth camera and under the assumption that the wire is in or near its static equilibrium, our method estimates both the location and magnitude of external forces without additional prior knowledge. This is achieved by exploiting derived consistency conditions and solving a system of linear equations based on force-torque balance along the wire. The approach was validated through simulation, where it achieved high accuracy, and through real-world experiments, where accurate estimation was demonstrated in selected interaction scenarios."
    },
    {
      "title": "Learning Adaptive Cross-Embodiment Visuomotor Policy with Contrastive Prompt Orchestration",
      "url": "https://arxiv.org/abs/2602.01040",
      "date": "2026-02-01",
      "authors_text": "Yuhang Zhang, Chao Yan, Jiaxi Yu, Jiaping Xiao, Mir Feroskhan",
      "is_highlight": false,
      "score": 92.0,
      "summary": "The paper presents ContrAstive Prompt Orchestration (CAPO), a method for improving adaptive visuomotor policies in diverse environments through hybrid contrastive learning and dynamic prompt orchestration.",
      "teaser_image": "https://arxiv.org/html/2602.01040/x4.png",
      "debug_abstract": "Learning adaptive visuomotor policies for embodied agents remains a formidable challenge, particularly when facing cross-embodiment variations such as diverse sensor configurations and dynamic properties. Conventional learning approaches often struggle to separate task-relevant features from domain-specific variations (e.g., lighting, field-of-view, and rotation), leading to poor sample efficiency and catastrophic failure in unseen environments. To bridge this gap, we propose ContrAstive Prompt Orchestration (CAPO), a novel approach for learning visuomotor policies that integrates contrastive prompt learning and adaptive prompt orchestration. For prompt learning, we devise a hybrid contrastive learning strategy that integrates visual, temporal action, and text objectives to establish a pool of learnable prompts, where each prompt induces a visual representation encapsulating fine-grained domain factors. Based on these learned prompts, we introduce an adaptive prompt orchestration mechanism that dynamically aggregates these prompts conditioned on current observations. This enables the agent to adaptively construct optimal state representations by identifying dominant domain factors instantaneously. Consequently, the policy optimization is effectively shielded from irrelevant interference, preventing the common issue of overfitting to source domains. Extensive experiments demonstrate that CAPO significantly outperforms state-of-the-art baselines in sample efficiency and asymptotic performance. Crucially, it exhibits superior zero-shot adaptation across unseen target domains characterized by drastic environmental (e.g., illumination) and physical shifts (e.g., field-of-view and rotation), validating its effectiveness as a viable solution for cross-embodiment visuomotor policy adaptation."
    },
    {
      "title": "Self-Guard: Defending Large Reasoning Models via enhanced self-reflection",
      "url": "https://arxiv.org/abs/2602.00707",
      "date": "2026-01-31",
      "authors_text": "Jingnan Zheng, Jingjun Xu, Yanzhen Luo, Chenhang Cui, Gelei Deng",
      "is_highlight": false,
      "score": 50.0,
      "summary": "Self-Guard is a lightweight framework enhancing safety compliance in Large Reasoning Models by promoting self-reflection and mitigating risks without sacrificing model utility.",
      "teaser_image": "https://arxiv.org/html/2602.00707/materials/workflow.png",
      "debug_abstract": "The emergence of Large Reasoning Models (LRMs) introduces a new paradigm of explicit reasoning, enabling remarkable advances yet posing unique risks such as reasoning manipulation and information leakage. To mitigate these risks, current alignment strategies predominantly rely on heavy post-training paradigms or external interventions. However, these approaches are often computationally intensive and fail to address the inherent awareness-compliance gap, a critical misalignment where models recognize potential risks yet prioritize following user instructions due to their sycophantic tendencies. To address these limitations, we propose Self-Guard, a lightweight safety defense framework that reinforces safety compliance at the representational level. Self-Guard operates through two principal stages: (1) safety-oriented prompting, which activates the model&#39;s latent safety awareness to evoke spontaneous reflection, and (2) safety activation steering, which extracts the resulting directional shift in the hidden state space and amplifies it to ensure that safety compliance prevails over sycophancy during inference. Experiments demonstrate that Self-Guard effectively bridges the awareness-compliance gap, achieving robust safety performance without compromising model utility. Furthermore, Self-Guard exhibits strong generalization across diverse unseen risks and varying model scales, offering a cost-efficient solution for LRM safety alignment."
    }
  ],
  "MMLab@NTU": [
    {
      "title": "FD-VLA: Force-Distilled Vision-Language-Action Model for Contact-Rich Manipulation",
      "url": "https://arxiv.org/abs/2602.02142",
      "date": "2026-02-02",
      "authors_text": "Ruiteng Zhao, Wenshuo Wang, Yicheng Ma, Xiaocong Li, Francis E. H. Tay",
      "is_highlight": false,
      "score": 0,
      "summary": "The FD-VLA framework enhances contact-rich manipulation by integrating force awareness through a novel distillation method, improving performance without physical force sensors.",
      "teaser_image": "https://arxiv.org/html/2602.02142/x1.png",
      "debug_abstract": "Force sensing is a crucial modality for Vision-Language-Action (VLA) frameworks, as it enables fine-grained perception and dexterous manipulation in contact-rich tasks. We present Force-Distilled VLA (FD-VLA), a novel framework that integrates force awareness into contact-rich manipulation without relying on physical force sensors. The core of our approach is a Force Distillation Module (FDM), which distills force by mapping a learnable query token, conditioned on visual observations and robot states, into a predicted force token aligned with the latent representation of actual force signals. During inference, this distilled force token is injected into the pretrained VLM, enabling force-aware reasoning while preserving the integrity of its vision-language semantics. This design provides two key benefits: first, it allows practical deployment across a wide range of robots that lack expensive or fragile force-torque sensors, thereby reducing hardware cost and complexity; second, the FDM introduces an additional force-vision-state fusion prior to the VLM, which improves cross-modal alignment and enhances perception-action robustness in contact-rich scenarios. Surprisingly, our physical experiments show that the distilled force token outperforms direct sensor force measurements as well as other baselines, which highlights the effectiveness of this force-distilled VLA approach."
    },
    {
      "title": "FlowSteer: Interactive Agentic Workflow Orchestration via End-to-End Reinforcement Learning",
      "url": "https://arxiv.org/abs/2602.01664",
      "date": "2026-02-02",
      "authors_text": "Mingda Zhang, Haoran Luo, Tiesunlong Shen, Qika Lin, Xiaoying Tang",
      "is_highlight": false,
      "score": 7.0,
      "summary": "FlowSteer is an end-to-end reinforcement learning framework that automates workflow orchestration through interactive learning, addressing manual costs and improving performance across diverse tasks.",
      "teaser_image": "https://arxiv.org/html/2602.01664/x1.png",
      "debug_abstract": "In recent years, a variety of powerful agentic workflows have been applied to solve a wide range of human problems. However, existing workflow orchestration still faces key challenges, including high manual cost, reliance on specific operators/large language models (LLMs), and sparse reward signals. To address these challenges, we propose FlowSteer, an end-to-end reinforcement learning framework that takes a lightweight policy model as the agent and an executable canvas environment, automating workflow orchestration through multi-turn interaction. In this process, the policy model analyzes execution states and selects editing actions, while the canvas executes operators and returns feedback for iterative refinement. Moreover, FlowSteer provides a plug-and-play framework that supports diverse operator libraries and interchangeable LLM backends. To effectively train this interaction paradigm, we propose Canvas Workflow Relative Policy Optimization (CWRPO), which introduces diversity-constrained rewards with conditional release to stabilize learning and suppress shortcut behaviors. Experimental results on twelve datasets show that FlowSteer significantly outperforms baselines across various tasks."
    },
    {
      "title": "UV-M3TL: A Unified and Versatile Multimodal Multi-Task Learning Framework for Assistive Driving Perception",
      "url": "https://arxiv.org/abs/2602.01594",
      "date": "2026-02-02",
      "authors_text": "Wenzhuo Liu, Qiannan Guo, Zhen Wang, Wenshuo Wang, Lei Yang",
      "is_highlight": false,
      "score": 8.0,
      "summary": "The UV-M3TL framework enhances ADAS by effectively recognizing driver behavior and context through dual-branch multimodal embedding and adaptive loss, achieving state-of-the-art performance across multiple tasks.",
      "teaser_image": "https://arxiv.org/html/2602.01594/x2.png",
      "debug_abstract": "Advanced Driver Assistance Systems (ADAS) need to understand human driver behavior while perceiving their navigation context, but jointly learning these heterogeneous tasks would cause inter-task negative transfer and impair system performance. Here, we propose a Unified and Versatile Multimodal Multi-Task Learning (UV-M3TL) framework to simultaneously recognize driver behavior, driver emotion, vehicle behavior, and traffic context, while mitigating inter-task negative transfer. Our framework incorporates two core components: dual-branch spatial channel multimodal embedding (DB-SCME) and adaptive feature-decoupled multi-task loss (AFD-Loss). DB-SCME enhances cross-task knowledge transfer while mitigating task conflicts by employing a dual-branch structure to explicitly model salient task-shared and task-specific features. AFD-Loss improves the stability of joint optimization while guiding the model to learn diverse multi-task representations by introducing an adaptive weighting mechanism based on learning dynamics and feature decoupling constraints. We evaluate our method on the AIDE dataset, and the experimental results demonstrate that UV-M3TL achieves state-of-the-art performance across all four tasks. To further prove the versatility, we evaluate UV-M3TL on additional public multi-task perception benchmarks (BDD100K, CityScapes, NYUD-v2, and PASCAL-Context), where it consistently delivers strong performance across diverse task combinations, attaining state-of-the-art results on most tasks."
    },
    {
      "title": "Failure-Aware Bimanual Teleoperation via Conservative Value Guided Assistance",
      "url": "https://arxiv.org/abs/2602.01092",
      "date": "2026-02-01",
      "authors_text": "Peng Zhou, Zhongxuan Li, Jinsong Wu, Jiaming Qi, Jun Hu",
      "is_highlight": false,
      "score": 9.0,
      "summary": "This paper presents a failure-aware bimanual teleoperation framework that uses conservative value learning to enhance task success and reduce operator workload through compliant haptic assistance.",
      "teaser_image": "https://arxiv.org/html/2602.01092/x1.png",
      "debug_abstract": "Teleoperation of high-precision manipulation is con-strained by tight success tolerances and complex contact dy-namics, which make impending failures difficult for human operators to anticipate under partial observability. This paper proposes a value-guided, failure-aware framework for bimanual teleoperation that provides compliant haptic assistance while pre-serving continuous human authority. The framework is trained entirely from heterogeneous offline teleoperation data containing both successful and failed executions. Task feasibility is mod-eled as a conservative success score learned via Conservative Value Learning, yielding a risk-sensitive estimate that remains reliable under distribution shift. During online operation, the learned success score regulates the level of assistance, while a learned actor provides a corrective motion direction. Both are integrated through a joint-space impedance interface on the master side, yielding continuous guidance that steers the operator away from failure-prone actions without overriding intent. Experimental results on contact-rich manipulation tasks demonstrate improved task success rates and reduced operator workload compared to conventional teleoperation and shared-autonomy baselines, indicating that conservative value learning provides an effective mechanism for embedding failure awareness into bilateral teleoperation. Experimental videos are available at this https URL"
    },
    {
      "title": "Superman: Unifying Skeleton and Vision for Human Motion Perception and Generation",
      "url": "https://arxiv.org/abs/2602.02401",
      "date": "2026-02-02",
      "authors_text": "Xinshun Wang, Peiming Li, Ziyi Wang, Zhongbin Fang, Zhichao Deng",
      "is_highlight": false,
      "score": 15.0,
      "summary": "Superman unifies visual perception and skeleton-based motion generation through a Vision-Guided Motion Tokenizer, enabling robust cross-modal learning and superior performance in human motion tasks.",
      "teaser_image": "https://arxiv.org/html/2602.02401/x2.png",
      "debug_abstract": "Human motion analysis tasks, such as temporal 3D pose estimation, motion prediction, and motion in-betweening, play an essential role in computer vision. However, current paradigms suffer from severe fragmentation. First, the field is split between ``perception&#39;&#39; models that understand motion from video but only output text, and ``generation&#39;&#39; models that cannot perceive from raw visual input. Second, generative MLLMs are often limited to single-frame, static poses using dense, parametric SMPL models, failing to handle temporal motion. Third, existing motion vocabularies are built from skeleton data alone, severing the link to the visual domain. To address these challenges, we introduce Superman, a unified framework that bridges visual perception with temporal, skeleton-based motion generation. Our solution is twofold. First, to overcome the modality disconnect, we propose a Vision-Guided Motion Tokenizer. Leveraging the natural geometric alignment between 3D skeletons and visual data, this module pioneers robust joint learning from both modalities, creating a unified, cross-modal motion vocabulary. Second, grounded in this motion language, a single, unified MLLM architecture is trained to handle all tasks. This module flexibly processes diverse, temporal inputs, unifying 3D skeleton pose estimation from video (perception) with skeleton-based motion prediction and in-betweening (generation). Extensive experiments on standard benchmarks, including Human3.6M, demonstrate that our unified method achieves state-of-the-art or competitive performance across all motion tasks. This showcases a more efficient and scalable path for generative motion analysis using skeletons."
    },
    {
      "title": "ClueTracer: Question-to-Vision Clue Tracing for Training-Free Hallucination Suppression in Multimodal Reasoning",
      "url": "https://arxiv.org/abs/2602.02004",
      "date": "2026-02-02",
      "authors_text": "Gongli Xi, Kun Wang, Zeming Gao, Huahui Yi, Haolang Lu",
      "is_highlight": false,
      "score": 24.0,
      "summary": "ClueTracer is a training-free plugin that suppresses hallucinations in multimodal reasoning by tracing visual clue propagation, enhancing model performance across various architectures.",
      "teaser_image": "https://arxiv.org/html/2602.02004/x1.png",
      "debug_abstract": "Large multimodal reasoning models solve challenging visual problems via explicit long-chain inference: they gather visual clues from images and decode clues into textual tokens. Yet this capability also increases hallucinations, where the model generates content that is not supported by the input image or the question. To understand this failure mode, we identify \\emph{reasoning drift}: during clue gathering, the model over-focuses on question-irrelevant entities, diluting focus on task-relevant cues and gradually decoupling the reasoning trace from visual grounding. As a consequence, many inference-time localization or intervention methods developed for non-reasoning models fail to pinpoint the true clues in reasoning settings. Motivated by these insights, we introduce ClueRecall, a metric for assessing visual clue retrieval, and present ClueTracer, a training-free, parameter-free, and architecture-agnostic plugin for hallucination suppression. ClueTracer starts from the question and traces how key clues propagate along the model&#39;s reasoning pathway (question $\\rightarrow$ outputs $\\rightarrow$ visual tokens), thereby localizing task-relevant patches while suppressing spurious attention to irrelevant regions. Remarkably, \\textbf{without any additional training}, ClueTracer improves all \\textbf{reasoning} architectures (including \\texttt{R1-OneVision}, \\texttt{Ocean-R1}, \\texttt{MM-Eureka}, \\emph{etc}.) by $\\mathbf{1.21\\times}$ on reasoning benchmarks. When transferred to \\textbf{non-reasoning} settings, it yields a $\\mathbf{1.14\\times}$ gain."
    },
    {
      "title": "SGHA-Attack: Semantic-Guided Hierarchical Alignment for Transferable Targeted Attacks on Vision-Language Models",
      "url": "https://arxiv.org/abs/2602.01574",
      "date": "2026-02-02",
      "authors_text": "Haobo Wang, Weiqi Luo, Xiaojun Jia, Xiaochun Cao",
      "is_highlight": false,
      "score": 44.0,
      "summary": "SGHA-Attack enhances targeted adversarial attacks on vision-language models by leveraging multiple semantic references and aligning intermediate features for improved transferability and robustness.",
      "teaser_image": "https://arxiv.org/html/2602.01574/x1.png",
      "debug_abstract": "Large vision-language models (VLMs) are vulnerable to transfer-based adversarial perturbations, enabling attackers to optimize on surrogate models and manipulate black-box VLM outputs. Prior targeted transfer attacks often overfit surrogate-specific embedding space by relying on a single reference and emphasizing final-layer alignment, which underutilizes intermediate semantics and degrades transfer across heterogeneous VLMs. To address this, we propose SGHA-Attack, a Semantic-Guided Hierarchical Alignment framework that adopts multiple target references and enforces intermediate-layer consistency. Concretely, we generate a visually grounded reference pool by sampling a frozen text-to-image model conditioned on the target prompt, and then carefully select the Top-K most semantically relevant anchors under the surrogate to form a weighted mixture for stable optimization guidance. Building on these anchors, SGHA-Attack injects target semantics throughout the feature hierarchy by aligning intermediate visual representations at both global and spatial granularities across multiple depths, and by synchronizing intermediate visual and textual features in a shared latent subspace to provide early cross-modal supervision before the final projection. Extensive experiments on open-source and commercial black-box VLMs show that SGHA-Attack achieves stronger targeted transferability than prior methods and remains robust under preprocessing and purification defenses."
    },
    {
      "title": "Estimating Force Interactions of Deformable Linear Objects from their Shapes",
      "url": "https://arxiv.org/abs/2602.01085",
      "date": "2026-02-01",
      "authors_text": "Qi Jing Chen, Shilin Shan, Timothy Bretl, Quang-Cuong Pham",
      "is_highlight": false,
      "score": 83.0,
      "summary": "This paper presents a novel method for estimating external forces on deformable linear objects using shape data, enhancing robot interaction safety and efficiency without requiring force sensors.",
      "teaser_image": "https://arxiv.org/html/2602.01085/x2.png",
      "debug_abstract": "This work introduces an analytical approach for detecting and estimating external forces acting on deformable linear objects (DLOs) using only their observed shapes. In many robot-wire interaction tasks, contact occurs not at the end-effector but at other points along the robot&#39;s body. Such scenarios arise when robots manipulate wires indirectly (e.g., by nudging) or when wires act as passive obstacles in the environment. Accurately identifying these interactions is crucial for safe and efficient trajectory planning, helping to prevent wire damage, avoid restricted robot motions, and mitigate potential hazards. Existing approaches often rely on expensive external force-torque sensor or that contacts occur at the end-effector for accurate force estimation. Using wire shape information acquired from a depth camera and under the assumption that the wire is in or near its static equilibrium, our method estimates both the location and magnitude of external forces without additional prior knowledge. This is achieved by exploiting derived consistency conditions and solving a system of linear equations based on force-torque balance along the wire. The approach was validated through simulation, where it achieved high accuracy, and through real-world experiments, where accurate estimation was demonstrated in selected interaction scenarios."
    },
    {
      "title": "Learning Adaptive Cross-Embodiment Visuomotor Policy with Contrastive Prompt Orchestration",
      "url": "https://arxiv.org/abs/2602.01040",
      "date": "2026-02-01",
      "authors_text": "Yuhang Zhang, Chao Yan, Jiaxi Yu, Jiaping Xiao, Mir Feroskhan",
      "is_highlight": false,
      "score": 92.0,
      "summary": "The paper presents ContrAstive Prompt Orchestration (CAPO), a method for improving adaptive visuomotor policies in diverse environments through hybrid contrastive learning and dynamic prompt orchestration.",
      "teaser_image": "https://arxiv.org/html/2602.01040/x4.png",
      "debug_abstract": "Learning adaptive visuomotor policies for embodied agents remains a formidable challenge, particularly when facing cross-embodiment variations such as diverse sensor configurations and dynamic properties. Conventional learning approaches often struggle to separate task-relevant features from domain-specific variations (e.g., lighting, field-of-view, and rotation), leading to poor sample efficiency and catastrophic failure in unseen environments. To bridge this gap, we propose ContrAstive Prompt Orchestration (CAPO), a novel approach for learning visuomotor policies that integrates contrastive prompt learning and adaptive prompt orchestration. For prompt learning, we devise a hybrid contrastive learning strategy that integrates visual, temporal action, and text objectives to establish a pool of learnable prompts, where each prompt induces a visual representation encapsulating fine-grained domain factors. Based on these learned prompts, we introduce an adaptive prompt orchestration mechanism that dynamically aggregates these prompts conditioned on current observations. This enables the agent to adaptively construct optimal state representations by identifying dominant domain factors instantaneously. Consequently, the policy optimization is effectively shielded from irrelevant interference, preventing the common issue of overfitting to source domains. Extensive experiments demonstrate that CAPO significantly outperforms state-of-the-art baselines in sample efficiency and asymptotic performance. Crucially, it exhibits superior zero-shot adaptation across unseen target domains characterized by drastic environmental (e.g., illumination) and physical shifts (e.g., field-of-view and rotation), validating its effectiveness as a viable solution for cross-embodiment visuomotor policy adaptation."
    },
    {
      "title": "Self-Guard: Defending Large Reasoning Models via enhanced self-reflection",
      "url": "https://arxiv.org/abs/2602.00707",
      "date": "2026-01-31",
      "authors_text": "Jingnan Zheng, Jingjun Xu, Yanzhen Luo, Chenhang Cui, Gelei Deng",
      "is_highlight": false,
      "score": 50.0,
      "summary": "Self-Guard is a lightweight framework enhancing safety compliance in Large Reasoning Models by promoting self-reflection and mitigating risks without sacrificing model utility.",
      "teaser_image": "https://arxiv.org/html/2602.00707/materials/workflow.png",
      "debug_abstract": "The emergence of Large Reasoning Models (LRMs) introduces a new paradigm of explicit reasoning, enabling remarkable advances yet posing unique risks such as reasoning manipulation and information leakage. To mitigate these risks, current alignment strategies predominantly rely on heavy post-training paradigms or external interventions. However, these approaches are often computationally intensive and fail to address the inherent awareness-compliance gap, a critical misalignment where models recognize potential risks yet prioritize following user instructions due to their sycophantic tendencies. To address these limitations, we propose Self-Guard, a lightweight safety defense framework that reinforces safety compliance at the representational level. Self-Guard operates through two principal stages: (1) safety-oriented prompting, which activates the model&#39;s latent safety awareness to evoke spontaneous reflection, and (2) safety activation steering, which extracts the resulting directional shift in the hidden state space and amplifies it to ensure that safety compliance prevails over sycophancy during inference. Experiments demonstrate that Self-Guard effectively bridges the awareness-compliance gap, achieving robust safety performance without compromising model utility. Furthermore, Self-Guard exhibits strong generalization across diverse unseen risks and varying model scales, offering a cost-efficient solution for LRM safety alignment."
    }
  ],
  "S-Lab for Advanced Intelligence": [
    {
      "title": "FD-VLA: Force-Distilled Vision-Language-Action Model for Contact-Rich Manipulation",
      "url": "https://arxiv.org/abs/2602.02142",
      "date": "2026-02-02",
      "authors_text": "Ruiteng Zhao, Wenshuo Wang, Yicheng Ma, Xiaocong Li, Francis E. H. Tay",
      "is_highlight": false,
      "score": 0,
      "summary": "The FD-VLA framework enhances contact-rich manipulation by integrating force awareness through a novel distillation method, improving performance without physical force sensors.",
      "teaser_image": "https://arxiv.org/html/2602.02142/x1.png",
      "debug_abstract": "Force sensing is a crucial modality for Vision-Language-Action (VLA) frameworks, as it enables fine-grained perception and dexterous manipulation in contact-rich tasks. We present Force-Distilled VLA (FD-VLA), a novel framework that integrates force awareness into contact-rich manipulation without relying on physical force sensors. The core of our approach is a Force Distillation Module (FDM), which distills force by mapping a learnable query token, conditioned on visual observations and robot states, into a predicted force token aligned with the latent representation of actual force signals. During inference, this distilled force token is injected into the pretrained VLM, enabling force-aware reasoning while preserving the integrity of its vision-language semantics. This design provides two key benefits: first, it allows practical deployment across a wide range of robots that lack expensive or fragile force-torque sensors, thereby reducing hardware cost and complexity; second, the FDM introduces an additional force-vision-state fusion prior to the VLM, which improves cross-modal alignment and enhances perception-action robustness in contact-rich scenarios. Surprisingly, our physical experiments show that the distilled force token outperforms direct sensor force measurements as well as other baselines, which highlights the effectiveness of this force-distilled VLA approach."
    },
    {
      "title": "FlowSteer: Interactive Agentic Workflow Orchestration via End-to-End Reinforcement Learning",
      "url": "https://arxiv.org/abs/2602.01664",
      "date": "2026-02-02",
      "authors_text": "Mingda Zhang, Haoran Luo, Tiesunlong Shen, Qika Lin, Xiaoying Tang",
      "is_highlight": false,
      "score": 7.0,
      "summary": "FlowSteer is an end-to-end reinforcement learning framework that automates workflow orchestration through interactive learning, addressing manual costs and improving performance across diverse tasks.",
      "teaser_image": "https://arxiv.org/html/2602.01664/x1.png",
      "debug_abstract": "In recent years, a variety of powerful agentic workflows have been applied to solve a wide range of human problems. However, existing workflow orchestration still faces key challenges, including high manual cost, reliance on specific operators/large language models (LLMs), and sparse reward signals. To address these challenges, we propose FlowSteer, an end-to-end reinforcement learning framework that takes a lightweight policy model as the agent and an executable canvas environment, automating workflow orchestration through multi-turn interaction. In this process, the policy model analyzes execution states and selects editing actions, while the canvas executes operators and returns feedback for iterative refinement. Moreover, FlowSteer provides a plug-and-play framework that supports diverse operator libraries and interchangeable LLM backends. To effectively train this interaction paradigm, we propose Canvas Workflow Relative Policy Optimization (CWRPO), which introduces diversity-constrained rewards with conditional release to stabilize learning and suppress shortcut behaviors. Experimental results on twelve datasets show that FlowSteer significantly outperforms baselines across various tasks."
    },
    {
      "title": "UV-M3TL: A Unified and Versatile Multimodal Multi-Task Learning Framework for Assistive Driving Perception",
      "url": "https://arxiv.org/abs/2602.01594",
      "date": "2026-02-02",
      "authors_text": "Wenzhuo Liu, Qiannan Guo, Zhen Wang, Wenshuo Wang, Lei Yang",
      "is_highlight": false,
      "score": 8.0,
      "summary": "The UV-M3TL framework enhances ADAS by effectively recognizing driver behavior and context through dual-branch multimodal embedding and adaptive loss, achieving state-of-the-art performance across multiple tasks.",
      "teaser_image": "https://arxiv.org/html/2602.01594/x2.png",
      "debug_abstract": "Advanced Driver Assistance Systems (ADAS) need to understand human driver behavior while perceiving their navigation context, but jointly learning these heterogeneous tasks would cause inter-task negative transfer and impair system performance. Here, we propose a Unified and Versatile Multimodal Multi-Task Learning (UV-M3TL) framework to simultaneously recognize driver behavior, driver emotion, vehicle behavior, and traffic context, while mitigating inter-task negative transfer. Our framework incorporates two core components: dual-branch spatial channel multimodal embedding (DB-SCME) and adaptive feature-decoupled multi-task loss (AFD-Loss). DB-SCME enhances cross-task knowledge transfer while mitigating task conflicts by employing a dual-branch structure to explicitly model salient task-shared and task-specific features. AFD-Loss improves the stability of joint optimization while guiding the model to learn diverse multi-task representations by introducing an adaptive weighting mechanism based on learning dynamics and feature decoupling constraints. We evaluate our method on the AIDE dataset, and the experimental results demonstrate that UV-M3TL achieves state-of-the-art performance across all four tasks. To further prove the versatility, we evaluate UV-M3TL on additional public multi-task perception benchmarks (BDD100K, CityScapes, NYUD-v2, and PASCAL-Context), where it consistently delivers strong performance across diverse task combinations, attaining state-of-the-art results on most tasks."
    },
    {
      "title": "Failure-Aware Bimanual Teleoperation via Conservative Value Guided Assistance",
      "url": "https://arxiv.org/abs/2602.01092",
      "date": "2026-02-01",
      "authors_text": "Peng Zhou, Zhongxuan Li, Jinsong Wu, Jiaming Qi, Jun Hu",
      "is_highlight": false,
      "score": 9.0,
      "summary": "This paper presents a failure-aware bimanual teleoperation framework that uses conservative value learning to enhance task success and reduce operator workload through compliant haptic assistance.",
      "teaser_image": "https://arxiv.org/html/2602.01092/x1.png",
      "debug_abstract": "Teleoperation of high-precision manipulation is con-strained by tight success tolerances and complex contact dy-namics, which make impending failures difficult for human operators to anticipate under partial observability. This paper proposes a value-guided, failure-aware framework for bimanual teleoperation that provides compliant haptic assistance while pre-serving continuous human authority. The framework is trained entirely from heterogeneous offline teleoperation data containing both successful and failed executions. Task feasibility is mod-eled as a conservative success score learned via Conservative Value Learning, yielding a risk-sensitive estimate that remains reliable under distribution shift. During online operation, the learned success score regulates the level of assistance, while a learned actor provides a corrective motion direction. Both are integrated through a joint-space impedance interface on the master side, yielding continuous guidance that steers the operator away from failure-prone actions without overriding intent. Experimental results on contact-rich manipulation tasks demonstrate improved task success rates and reduced operator workload compared to conventional teleoperation and shared-autonomy baselines, indicating that conservative value learning provides an effective mechanism for embedding failure awareness into bilateral teleoperation. Experimental videos are available at this https URL"
    },
    {
      "title": "Superman: Unifying Skeleton and Vision for Human Motion Perception and Generation",
      "url": "https://arxiv.org/abs/2602.02401",
      "date": "2026-02-02",
      "authors_text": "Xinshun Wang, Peiming Li, Ziyi Wang, Zhongbin Fang, Zhichao Deng",
      "is_highlight": false,
      "score": 15.0,
      "summary": "Superman unifies visual perception and skeleton-based motion generation through a Vision-Guided Motion Tokenizer, enabling robust cross-modal learning and superior performance in human motion tasks.",
      "teaser_image": "https://arxiv.org/html/2602.02401/x2.png",
      "debug_abstract": "Human motion analysis tasks, such as temporal 3D pose estimation, motion prediction, and motion in-betweening, play an essential role in computer vision. However, current paradigms suffer from severe fragmentation. First, the field is split between ``perception&#39;&#39; models that understand motion from video but only output text, and ``generation&#39;&#39; models that cannot perceive from raw visual input. Second, generative MLLMs are often limited to single-frame, static poses using dense, parametric SMPL models, failing to handle temporal motion. Third, existing motion vocabularies are built from skeleton data alone, severing the link to the visual domain. To address these challenges, we introduce Superman, a unified framework that bridges visual perception with temporal, skeleton-based motion generation. Our solution is twofold. First, to overcome the modality disconnect, we propose a Vision-Guided Motion Tokenizer. Leveraging the natural geometric alignment between 3D skeletons and visual data, this module pioneers robust joint learning from both modalities, creating a unified, cross-modal motion vocabulary. Second, grounded in this motion language, a single, unified MLLM architecture is trained to handle all tasks. This module flexibly processes diverse, temporal inputs, unifying 3D skeleton pose estimation from video (perception) with skeleton-based motion prediction and in-betweening (generation). Extensive experiments on standard benchmarks, including Human3.6M, demonstrate that our unified method achieves state-of-the-art or competitive performance across all motion tasks. This showcases a more efficient and scalable path for generative motion analysis using skeletons."
    },
    {
      "title": "ClueTracer: Question-to-Vision Clue Tracing for Training-Free Hallucination Suppression in Multimodal Reasoning",
      "url": "https://arxiv.org/abs/2602.02004",
      "date": "2026-02-02",
      "authors_text": "Gongli Xi, Kun Wang, Zeming Gao, Huahui Yi, Haolang Lu",
      "is_highlight": false,
      "score": 24.0,
      "summary": "ClueTracer is a training-free plugin that suppresses hallucinations in multimodal reasoning by tracing visual clue propagation, enhancing model performance across various architectures.",
      "teaser_image": "https://arxiv.org/html/2602.02004/x1.png",
      "debug_abstract": "Large multimodal reasoning models solve challenging visual problems via explicit long-chain inference: they gather visual clues from images and decode clues into textual tokens. Yet this capability also increases hallucinations, where the model generates content that is not supported by the input image or the question. To understand this failure mode, we identify \\emph{reasoning drift}: during clue gathering, the model over-focuses on question-irrelevant entities, diluting focus on task-relevant cues and gradually decoupling the reasoning trace from visual grounding. As a consequence, many inference-time localization or intervention methods developed for non-reasoning models fail to pinpoint the true clues in reasoning settings. Motivated by these insights, we introduce ClueRecall, a metric for assessing visual clue retrieval, and present ClueTracer, a training-free, parameter-free, and architecture-agnostic plugin for hallucination suppression. ClueTracer starts from the question and traces how key clues propagate along the model&#39;s reasoning pathway (question $\\rightarrow$ outputs $\\rightarrow$ visual tokens), thereby localizing task-relevant patches while suppressing spurious attention to irrelevant regions. Remarkably, \\textbf{without any additional training}, ClueTracer improves all \\textbf{reasoning} architectures (including \\texttt{R1-OneVision}, \\texttt{Ocean-R1}, \\texttt{MM-Eureka}, \\emph{etc}.) by $\\mathbf{1.21\\times}$ on reasoning benchmarks. When transferred to \\textbf{non-reasoning} settings, it yields a $\\mathbf{1.14\\times}$ gain."
    },
    {
      "title": "SGHA-Attack: Semantic-Guided Hierarchical Alignment for Transferable Targeted Attacks on Vision-Language Models",
      "url": "https://arxiv.org/abs/2602.01574",
      "date": "2026-02-02",
      "authors_text": "Haobo Wang, Weiqi Luo, Xiaojun Jia, Xiaochun Cao",
      "is_highlight": false,
      "score": 44.0,
      "summary": "SGHA-Attack enhances targeted adversarial attacks on vision-language models by leveraging multiple semantic references and aligning intermediate features for improved transferability and robustness.",
      "teaser_image": "https://arxiv.org/html/2602.01574/x1.png",
      "debug_abstract": "Large vision-language models (VLMs) are vulnerable to transfer-based adversarial perturbations, enabling attackers to optimize on surrogate models and manipulate black-box VLM outputs. Prior targeted transfer attacks often overfit surrogate-specific embedding space by relying on a single reference and emphasizing final-layer alignment, which underutilizes intermediate semantics and degrades transfer across heterogeneous VLMs. To address this, we propose SGHA-Attack, a Semantic-Guided Hierarchical Alignment framework that adopts multiple target references and enforces intermediate-layer consistency. Concretely, we generate a visually grounded reference pool by sampling a frozen text-to-image model conditioned on the target prompt, and then carefully select the Top-K most semantically relevant anchors under the surrogate to form a weighted mixture for stable optimization guidance. Building on these anchors, SGHA-Attack injects target semantics throughout the feature hierarchy by aligning intermediate visual representations at both global and spatial granularities across multiple depths, and by synchronizing intermediate visual and textual features in a shared latent subspace to provide early cross-modal supervision before the final projection. Extensive experiments on open-source and commercial black-box VLMs show that SGHA-Attack achieves stronger targeted transferability than prior methods and remains robust under preprocessing and purification defenses."
    },
    {
      "title": "Estimating Force Interactions of Deformable Linear Objects from their Shapes",
      "url": "https://arxiv.org/abs/2602.01085",
      "date": "2026-02-01",
      "authors_text": "Qi Jing Chen, Shilin Shan, Timothy Bretl, Quang-Cuong Pham",
      "is_highlight": false,
      "score": 83.0,
      "summary": "This paper presents a novel method for estimating external forces on deformable linear objects using shape data, enhancing robot interaction safety and efficiency without requiring force sensors.",
      "teaser_image": "https://arxiv.org/html/2602.01085/x2.png",
      "debug_abstract": "This work introduces an analytical approach for detecting and estimating external forces acting on deformable linear objects (DLOs) using only their observed shapes. In many robot-wire interaction tasks, contact occurs not at the end-effector but at other points along the robot&#39;s body. Such scenarios arise when robots manipulate wires indirectly (e.g., by nudging) or when wires act as passive obstacles in the environment. Accurately identifying these interactions is crucial for safe and efficient trajectory planning, helping to prevent wire damage, avoid restricted robot motions, and mitigate potential hazards. Existing approaches often rely on expensive external force-torque sensor or that contacts occur at the end-effector for accurate force estimation. Using wire shape information acquired from a depth camera and under the assumption that the wire is in or near its static equilibrium, our method estimates both the location and magnitude of external forces without additional prior knowledge. This is achieved by exploiting derived consistency conditions and solving a system of linear equations based on force-torque balance along the wire. The approach was validated through simulation, where it achieved high accuracy, and through real-world experiments, where accurate estimation was demonstrated in selected interaction scenarios."
    },
    {
      "title": "Learning Adaptive Cross-Embodiment Visuomotor Policy with Contrastive Prompt Orchestration",
      "url": "https://arxiv.org/abs/2602.01040",
      "date": "2026-02-01",
      "authors_text": "Yuhang Zhang, Chao Yan, Jiaxi Yu, Jiaping Xiao, Mir Feroskhan",
      "is_highlight": false,
      "score": 92.0,
      "summary": "The paper presents ContrAstive Prompt Orchestration (CAPO), a method for improving adaptive visuomotor policies in diverse environments through hybrid contrastive learning and dynamic prompt orchestration.",
      "teaser_image": "https://arxiv.org/html/2602.01040/x4.png",
      "debug_abstract": "Learning adaptive visuomotor policies for embodied agents remains a formidable challenge, particularly when facing cross-embodiment variations such as diverse sensor configurations and dynamic properties. Conventional learning approaches often struggle to separate task-relevant features from domain-specific variations (e.g., lighting, field-of-view, and rotation), leading to poor sample efficiency and catastrophic failure in unseen environments. To bridge this gap, we propose ContrAstive Prompt Orchestration (CAPO), a novel approach for learning visuomotor policies that integrates contrastive prompt learning and adaptive prompt orchestration. For prompt learning, we devise a hybrid contrastive learning strategy that integrates visual, temporal action, and text objectives to establish a pool of learnable prompts, where each prompt induces a visual representation encapsulating fine-grained domain factors. Based on these learned prompts, we introduce an adaptive prompt orchestration mechanism that dynamically aggregates these prompts conditioned on current observations. This enables the agent to adaptively construct optimal state representations by identifying dominant domain factors instantaneously. Consequently, the policy optimization is effectively shielded from irrelevant interference, preventing the common issue of overfitting to source domains. Extensive experiments demonstrate that CAPO significantly outperforms state-of-the-art baselines in sample efficiency and asymptotic performance. Crucially, it exhibits superior zero-shot adaptation across unseen target domains characterized by drastic environmental (e.g., illumination) and physical shifts (e.g., field-of-view and rotation), validating its effectiveness as a viable solution for cross-embodiment visuomotor policy adaptation."
    },
    {
      "title": "Self-Guard: Defending Large Reasoning Models via enhanced self-reflection",
      "url": "https://arxiv.org/abs/2602.00707",
      "date": "2026-01-31",
      "authors_text": "Jingnan Zheng, Jingjun Xu, Yanzhen Luo, Chenhang Cui, Gelei Deng",
      "is_highlight": false,
      "score": 50.0,
      "summary": "Self-Guard is a lightweight framework enhancing safety compliance in Large Reasoning Models by promoting self-reflection and mitigating risks without sacrificing model utility.",
      "teaser_image": "https://arxiv.org/html/2602.00707/materials/workflow.png",
      "debug_abstract": "The emergence of Large Reasoning Models (LRMs) introduces a new paradigm of explicit reasoning, enabling remarkable advances yet posing unique risks such as reasoning manipulation and information leakage. To mitigate these risks, current alignment strategies predominantly rely on heavy post-training paradigms or external interventions. However, these approaches are often computationally intensive and fail to address the inherent awareness-compliance gap, a critical misalignment where models recognize potential risks yet prioritize following user instructions due to their sycophantic tendencies. To address these limitations, we propose Self-Guard, a lightweight safety defense framework that reinforces safety compliance at the representational level. Self-Guard operates through two principal stages: (1) safety-oriented prompting, which activates the model&#39;s latent safety awareness to evoke spontaneous reflection, and (2) safety activation steering, which extracts the resulting directional shift in the hidden state space and amplifies it to ensure that safety compliance prevails over sycophancy during inference. Experiments demonstrate that Self-Guard effectively bridges the awareness-compliance gap, achieving robust safety performance without compromising model utility. Furthermore, Self-Guard exhibits strong generalization across diverse unseen risks and varying model scales, offering a cost-efficient solution for LRM safety alignment."
    }
  ],
  "南方科技大学": [
    {
      "title": "Know Your Step: Faster and Better Alignment for Flow Matching Models via Step-aware Advantages",
      "url": "https://arxiv.org/abs/2602.01591",
      "date": "2026-02-02",
      "authors_text": "Zhixiong Yue, Zixuan Ni, Feiyang Ye, Jinshan Zhang, Sheng Shen",
      "is_highlight": false,
      "score": 39.0,
      "summary": "The paper presents TAFS GRPO, a novel framework enhancing flow matching models for text-to-image generation by improving alignment with human preferences through efficient few-step sampling and adaptive noise.",
      "teaser_image": "https://arxiv.org/html/2602.01591/x3.png",
      "debug_abstract": "Recent advances in flow matching models, particularly with reinforcement learning (RL), have significantly enhanced human preference alignment in few step text to image generators. However, existing RL based approaches for flow matching models typically rely on numerous denoising steps, while suffering from sparse and imprecise reward signals that often lead to suboptimal alignment. To address these limitations, we propose Temperature Annealed Few step Sampling with Group Relative Policy Optimization (TAFS GRPO), a novel framework for training flow matching text to image models into efficient few step generators well aligned with human preferences. Our method iteratively injects adaptive temporal noise onto the results of one step samples. By repeatedly annealing the model&#39;s sampled outputs, it introduces stochasticity into the sampling process while preserving the semantic integrity of each generated image. Moreover, its step aware advantage integration mechanism combines the GRPO to avoid the need for the differentiable of reward function and provide dense and step specific rewards for stable policy optimization. Extensive experiments demonstrate that TAFS GRPO achieves strong performance in few step text to image generation and significantly improves the alignment of generated images with human preferences. The code and models of this work will be available to facilitate further research."
    },
    {
      "title": "PolicyFlow: Policy Optimization with Continuous Normalizing Flow in Reinforcement Learning",
      "url": "https://arxiv.org/abs/2602.01156",
      "date": "2026-02-01",
      "authors_text": "Shunpeng Yang, Ben Liu, Hua Chen",
      "is_highlight": false,
      "score": 71.0,
      "summary": "PolicyFlow is a novel reinforcement learning algorithm that integrates continuous normalizing flows with PPO, enhancing performance and stability while reducing computational costs and encouraging diverse behaviors.",
      "teaser_image": "https://arxiv.org/html/2602.01156/x1.png",
      "debug_abstract": "Among on-policy reinforcement learning algorithms, Proximal Policy Optimization (PPO) demonstrates is widely favored for its simplicity, numerical stability, and strong empirical performance. Standard PPO relies on surrogate objectives defined via importance ratios, which require evaluating policy likelihood that is typically straightforward when the policy is modeled as a Gaussian distribution. However, extending PPO to more expressive, high-capacity policy models such as continuous normalizing flows (CNFs), also known as flow-matching models, is challenging because likelihood evaluation along the full flow trajectory is computationally expensive and often numerically unstable. To resolve this issue, we propose PolicyFlow, a novel on-policy CNF-based reinforcement learning algorithm that integrates expressive CNF policies with PPO-style objectives without requiring likelihood evaluation along the full flow path. PolicyFlow approximates importance ratios using velocity field variations along a simple interpolation path, reducing computational overhead without compromising training stability. To further prevent mode collapse and further encourage diverse behaviors, we propose the Brownian Regularizer, an implicit policy entropy regularizer inspired by Brownian motion, which is conceptually elegant and computationally lightweight. Experiments on diverse tasks across various environments including MultiGoal, PointMaze, IsaacLab and MuJoCo Playground show that PolicyFlow achieves competitive or superior performance compared to PPO using Gaussian policies and flow-based baselines including FPO and DPPO. Notably, results on MultiGoal highlight PolicyFlow&#39;s ability to capture richer multimodal action distributions."
    },
    {
      "title": "Self-Guard: Defending Large Reasoning Models via enhanced self-reflection",
      "url": "https://arxiv.org/abs/2602.00707",
      "date": "2026-01-31",
      "authors_text": "Jingnan Zheng, Jingjun Xu, Yanzhen Luo, Chenhang Cui, Gelei Deng",
      "is_highlight": false,
      "score": 50.0,
      "summary": "Self-Guard is a lightweight framework enhancing safety compliance in Large Reasoning Models by promoting self-reflection and mitigating risks without sacrificing model utility.",
      "teaser_image": "https://arxiv.org/html/2602.00707/materials/workflow.png",
      "debug_abstract": "The emergence of Large Reasoning Models (LRMs) introduces a new paradigm of explicit reasoning, enabling remarkable advances yet posing unique risks such as reasoning manipulation and information leakage. To mitigate these risks, current alignment strategies predominantly rely on heavy post-training paradigms or external interventions. However, these approaches are often computationally intensive and fail to address the inherent awareness-compliance gap, a critical misalignment where models recognize potential risks yet prioritize following user instructions due to their sycophantic tendencies. To address these limitations, we propose Self-Guard, a lightweight safety defense framework that reinforces safety compliance at the representational level. Self-Guard operates through two principal stages: (1) safety-oriented prompting, which activates the model&#39;s latent safety awareness to evoke spontaneous reflection, and (2) safety activation steering, which extracts the resulting directional shift in the hidden state space and amplifies it to ensure that safety compliance prevails over sycophancy during inference. Experiments demonstrate that Self-Guard effectively bridges the awareness-compliance gap, achieving robust safety performance without compromising model utility. Furthermore, Self-Guard exhibits strong generalization across diverse unseen risks and varying model scales, offering a cost-efficient solution for LRM safety alignment."
    }
  ],
  "中国科学技术大学": [
    {
      "title": "Beyond Precision: Training-Inference Mismatch is an Optimization Problem and Simple LR Scheduling Fixes It",
      "url": "https://arxiv.org/abs/2602.01826",
      "date": "2026-02-02",
      "authors_text": "Yaxiang Zhang, Yingru Li, Jiacai Liu, Jiawei Xu, Ziniu Li",
      "is_highlight": false,
      "score": 35.0,
      "summary": "This paper presents a dynamic learning rate scheduling method to mitigate training-inference mismatch and stabilize reinforcement learning in large language models by addressing gradient noise.",
      "teaser_image": "https://arxiv.org/html/2602.01826/Qwen3-4B_baseline_vs_importance_sampling_patches.png",
      "debug_abstract": "Reinforcement Learning (RL) for training Large Language Models is notoriously unstable. While recent studies attribute this to &#34;training inference mismatch stemming&#34; from inconsistent hybrid engines, standard remedies, such as Importance Sampling, might fail during extended training runs. In this work, we analyze this instability through the lens of optimization, demonstrating that gradient noise and training-inference mismatch escalate in tandem as training progresses. Meanwhile, we find that the mismatch can be effectively suppressed by shrinking the update size. Taken together, we deduce that the mismatch is not merely a static numerical discrepancy, but a dynamic failure coupled with the model&#39;s optimization. Based on this insight, we propose a simple yet effective solution: a specialized Learning Rate (LR) scheduler. Instead of pre-defined decay schedule in traditional LR scheduler, our method dynamically triggers LR decay based on response length, which we identify as a reliable early-warning signal for impending instability. Empirical evidence suggests that by reducing the learning rate as gradient noise rises, we can consistently stabilize RL training and keep the training-inference mismatch at a safe level."
    },
    {
      "title": "Reasoning with Autoregressive-Diffusion Collaborative Thoughts",
      "url": "https://arxiv.org/abs/2602.01608",
      "date": "2026-02-02",
      "authors_text": "Mu Yuan, Liekang Zeng, Guoliang Xing, Lan Zhang, Yunhao Liu",
      "is_highlight": false,
      "score": 41.0,
      "summary": "The paper presents Collaborative Thoughts, a framework integrating autoregressive and diffusion models for enhanced spatial reasoning and generation through iterative feedback and joint reasoning.",
      "teaser_image": "https://arxiv.org/html/2602.01608/x2.png",
      "debug_abstract": "Autoregressive and diffusion models represent two complementary generative paradigms. Autoregressive models excel at sequential planning and constraint composition, yet struggle with tasks that require explicit spatial or physical grounding. Diffusion models, in contrast, capture rich spatial structure through high-dimensional generation, but lack the stepwise logical control needed to satisfy complex, multi-stage constraints or to reliably identify and correct errors. We introduce Collaborative Thoughts, a unified collaborative framework that enables autoregressive and diffusion models to reason and generate jointly through a closed-loop interaction. In Collaborative Thoughts, autoregressive models perform structured planning and constraint management, diffusion models instantiate these constraints as intermediate visual thoughts, and a vision-based critic module evaluates whether the visual thoughts satisfy the intended structural and physical requirements. This feedback is then used to iteratively refine subsequent planning and generation steps, mitigating error propagation across modalities. Importantly, Collaborative Thoughts uses the same collaborative loop regardless of whether the task is autoregressive question answering or diffusion-based visual generation. Through representative examples, we illustrate how Collaborative Thoughts can improve the reliability of spatial reasoning and the controllability of generation."
    },
    {
      "title": "Med3D-R1: Incentivizing Clinical Reasoning in 3D Medical Vision-Language Models for Abnormality Diagnosis",
      "url": "https://arxiv.org/abs/2602.01200",
      "date": "2026-02-01",
      "authors_text": "Haoran Lai, Zihang Jiang, Kun Zhang, Qingsong Yao, Rongsheng Wang",
      "is_highlight": false,
      "score": 54.0,
      "summary": "Med3D-R1 introduces a two-stage reinforcement learning framework that enhances 3D medical vision-language models for improved abnormality diagnosis and clinical reasoning.",
      "teaser_image": "https://arxiv.org/html/2602.01200/x2.png",
      "debug_abstract": "Developing 3D vision-language models with robust clinical reasoning remains a challenge due to the inherent complexity of volumetric medical imaging, the tendency of models to overfit superficial report patterns, and the lack of interpretability-aware reward designs. In this paper, we propose Med3D-R1, a reinforcement learning framework with a two-stage training process: Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL). During SFT stage, we introduce a residual alignment mechanism to bridge the gap between high-dimensional 3D features and textual embeddings, and an abnormality re-weighting strategy to emphasize clinically informative tokens and reduce structural bias in reports. In RL stage, we redesign the consistency reward to explicitly promote coherent, step-by-step diagnostic reasoning. We evaluate our method on medical multiple-choice visual question answering using two 3D diagnostic benchmarks, CT-RATE and RAD-ChestCT, where our model attains state-of-the-art accuracies of 41.92\\% on CT-RATE and 44.99\\% on RAD-ChestCT. These results indicate improved abnormality diagnosis and clinical reasoning and outperform prior methods on both benchmarks. Overall, our approach holds promise for enhancing real-world diagnostic workflows by enabling more reliable and transparent 3D medical vision-language systems."
    },
    {
      "title": "Self-Guard: Defending Large Reasoning Models via enhanced self-reflection",
      "url": "https://arxiv.org/abs/2602.00707",
      "date": "2026-01-31",
      "authors_text": "Jingnan Zheng, Jingjun Xu, Yanzhen Luo, Chenhang Cui, Gelei Deng",
      "is_highlight": false,
      "score": 50.0,
      "summary": "Self-Guard is a lightweight framework enhancing safety compliance in Large Reasoning Models by promoting self-reflection and mitigating risks without sacrificing model utility.",
      "teaser_image": "https://arxiv.org/html/2602.00707/materials/workflow.png",
      "debug_abstract": "The emergence of Large Reasoning Models (LRMs) introduces a new paradigm of explicit reasoning, enabling remarkable advances yet posing unique risks such as reasoning manipulation and information leakage. To mitigate these risks, current alignment strategies predominantly rely on heavy post-training paradigms or external interventions. However, these approaches are often computationally intensive and fail to address the inherent awareness-compliance gap, a critical misalignment where models recognize potential risks yet prioritize following user instructions due to their sycophantic tendencies. To address these limitations, we propose Self-Guard, a lightweight safety defense framework that reinforces safety compliance at the representational level. Self-Guard operates through two principal stages: (1) safety-oriented prompting, which activates the model&#39;s latent safety awareness to evoke spontaneous reflection, and (2) safety activation steering, which extracts the resulting directional shift in the hidden state space and amplifies it to ensure that safety compliance prevails over sycophancy during inference. Experiments demonstrate that Self-Guard effectively bridges the awareness-compliance gap, achieving robust safety performance without compromising model utility. Furthermore, Self-Guard exhibits strong generalization across diverse unseen risks and varying model scales, offering a cost-efficient solution for LRM safety alignment."
    }
  ],
  "PINE Lab": [
    {
      "title": "FD-VLA: Force-Distilled Vision-Language-Action Model for Contact-Rich Manipulation",
      "url": "https://arxiv.org/abs/2602.02142",
      "date": "2026-02-02",
      "authors_text": "Ruiteng Zhao, Wenshuo Wang, Yicheng Ma, Xiaocong Li, Francis E. H. Tay",
      "is_highlight": false,
      "score": 0,
      "summary": "The FD-VLA framework enhances contact-rich manipulation by integrating force awareness through a novel distillation method, improving performance without physical force sensors.",
      "teaser_image": "https://arxiv.org/html/2602.02142/x1.png",
      "debug_abstract": "Force sensing is a crucial modality for Vision-Language-Action (VLA) frameworks, as it enables fine-grained perception and dexterous manipulation in contact-rich tasks. We present Force-Distilled VLA (FD-VLA), a novel framework that integrates force awareness into contact-rich manipulation without relying on physical force sensors. The core of our approach is a Force Distillation Module (FDM), which distills force by mapping a learnable query token, conditioned on visual observations and robot states, into a predicted force token aligned with the latent representation of actual force signals. During inference, this distilled force token is injected into the pretrained VLM, enabling force-aware reasoning while preserving the integrity of its vision-language semantics. This design provides two key benefits: first, it allows practical deployment across a wide range of robots that lack expensive or fragile force-torque sensors, thereby reducing hardware cost and complexity; second, the FDM introduces an additional force-vision-state fusion prior to the VLM, which improves cross-modal alignment and enhances perception-action robustness in contact-rich scenarios. Surprisingly, our physical experiments show that the distilled force token outperforms direct sensor force measurements as well as other baselines, which highlights the effectiveness of this force-distilled VLA approach."
    },
    {
      "title": "FlowSteer: Interactive Agentic Workflow Orchestration via End-to-End Reinforcement Learning",
      "url": "https://arxiv.org/abs/2602.01664",
      "date": "2026-02-02",
      "authors_text": "Mingda Zhang, Haoran Luo, Tiesunlong Shen, Qika Lin, Xiaoying Tang",
      "is_highlight": false,
      "score": 7.0,
      "summary": "FlowSteer is an end-to-end reinforcement learning framework that automates workflow orchestration through interactive learning, addressing manual costs and improving performance across diverse tasks.",
      "teaser_image": "https://arxiv.org/html/2602.01664/x1.png",
      "debug_abstract": "In recent years, a variety of powerful agentic workflows have been applied to solve a wide range of human problems. However, existing workflow orchestration still faces key challenges, including high manual cost, reliance on specific operators/large language models (LLMs), and sparse reward signals. To address these challenges, we propose FlowSteer, an end-to-end reinforcement learning framework that takes a lightweight policy model as the agent and an executable canvas environment, automating workflow orchestration through multi-turn interaction. In this process, the policy model analyzes execution states and selects editing actions, while the canvas executes operators and returns feedback for iterative refinement. Moreover, FlowSteer provides a plug-and-play framework that supports diverse operator libraries and interchangeable LLM backends. To effectively train this interaction paradigm, we propose Canvas Workflow Relative Policy Optimization (CWRPO), which introduces diversity-constrained rewards with conditional release to stabilize learning and suppress shortcut behaviors. Experimental results on twelve datasets show that FlowSteer significantly outperforms baselines across various tasks."
    },
    {
      "title": "UV-M3TL: A Unified and Versatile Multimodal Multi-Task Learning Framework for Assistive Driving Perception",
      "url": "https://arxiv.org/abs/2602.01594",
      "date": "2026-02-02",
      "authors_text": "Wenzhuo Liu, Qiannan Guo, Zhen Wang, Wenshuo Wang, Lei Yang",
      "is_highlight": false,
      "score": 8.0,
      "summary": "The UV-M3TL framework enhances ADAS by effectively recognizing driver behavior and context through dual-branch multimodal embedding and adaptive loss, achieving state-of-the-art performance across multiple tasks.",
      "teaser_image": "https://arxiv.org/html/2602.01594/x2.png",
      "debug_abstract": "Advanced Driver Assistance Systems (ADAS) need to understand human driver behavior while perceiving their navigation context, but jointly learning these heterogeneous tasks would cause inter-task negative transfer and impair system performance. Here, we propose a Unified and Versatile Multimodal Multi-Task Learning (UV-M3TL) framework to simultaneously recognize driver behavior, driver emotion, vehicle behavior, and traffic context, while mitigating inter-task negative transfer. Our framework incorporates two core components: dual-branch spatial channel multimodal embedding (DB-SCME) and adaptive feature-decoupled multi-task loss (AFD-Loss). DB-SCME enhances cross-task knowledge transfer while mitigating task conflicts by employing a dual-branch structure to explicitly model salient task-shared and task-specific features. AFD-Loss improves the stability of joint optimization while guiding the model to learn diverse multi-task representations by introducing an adaptive weighting mechanism based on learning dynamics and feature decoupling constraints. We evaluate our method on the AIDE dataset, and the experimental results demonstrate that UV-M3TL achieves state-of-the-art performance across all four tasks. To further prove the versatility, we evaluate UV-M3TL on additional public multi-task perception benchmarks (BDD100K, CityScapes, NYUD-v2, and PASCAL-Context), where it consistently delivers strong performance across diverse task combinations, attaining state-of-the-art results on most tasks."
    },
    {
      "title": "Failure-Aware Bimanual Teleoperation via Conservative Value Guided Assistance",
      "url": "https://arxiv.org/abs/2602.01092",
      "date": "2026-02-01",
      "authors_text": "Peng Zhou, Zhongxuan Li, Jinsong Wu, Jiaming Qi, Jun Hu",
      "is_highlight": false,
      "score": 9.0,
      "summary": "This paper presents a failure-aware bimanual teleoperation framework that uses conservative value learning to enhance task success and reduce operator workload through compliant haptic assistance.",
      "teaser_image": "https://arxiv.org/html/2602.01092/x1.png",
      "debug_abstract": "Teleoperation of high-precision manipulation is con-strained by tight success tolerances and complex contact dy-namics, which make impending failures difficult for human operators to anticipate under partial observability. This paper proposes a value-guided, failure-aware framework for bimanual teleoperation that provides compliant haptic assistance while pre-serving continuous human authority. The framework is trained entirely from heterogeneous offline teleoperation data containing both successful and failed executions. Task feasibility is mod-eled as a conservative success score learned via Conservative Value Learning, yielding a risk-sensitive estimate that remains reliable under distribution shift. During online operation, the learned success score regulates the level of assistance, while a learned actor provides a corrective motion direction. Both are integrated through a joint-space impedance interface on the master side, yielding continuous guidance that steers the operator away from failure-prone actions without overriding intent. Experimental results on contact-rich manipulation tasks demonstrate improved task success rates and reduced operator workload compared to conventional teleoperation and shared-autonomy baselines, indicating that conservative value learning provides an effective mechanism for embedding failure awareness into bilateral teleoperation. Experimental videos are available at this https URL"
    },
    {
      "title": "Superman: Unifying Skeleton and Vision for Human Motion Perception and Generation",
      "url": "https://arxiv.org/abs/2602.02401",
      "date": "2026-02-02",
      "authors_text": "Xinshun Wang, Peiming Li, Ziyi Wang, Zhongbin Fang, Zhichao Deng",
      "is_highlight": false,
      "score": 15.0,
      "summary": "Superman unifies visual perception and skeleton-based motion generation through a Vision-Guided Motion Tokenizer, enabling robust cross-modal learning and superior performance in human motion tasks.",
      "teaser_image": "https://arxiv.org/html/2602.02401/x2.png",
      "debug_abstract": "Human motion analysis tasks, such as temporal 3D pose estimation, motion prediction, and motion in-betweening, play an essential role in computer vision. However, current paradigms suffer from severe fragmentation. First, the field is split between ``perception&#39;&#39; models that understand motion from video but only output text, and ``generation&#39;&#39; models that cannot perceive from raw visual input. Second, generative MLLMs are often limited to single-frame, static poses using dense, parametric SMPL models, failing to handle temporal motion. Third, existing motion vocabularies are built from skeleton data alone, severing the link to the visual domain. To address these challenges, we introduce Superman, a unified framework that bridges visual perception with temporal, skeleton-based motion generation. Our solution is twofold. First, to overcome the modality disconnect, we propose a Vision-Guided Motion Tokenizer. Leveraging the natural geometric alignment between 3D skeletons and visual data, this module pioneers robust joint learning from both modalities, creating a unified, cross-modal motion vocabulary. Second, grounded in this motion language, a single, unified MLLM architecture is trained to handle all tasks. This module flexibly processes diverse, temporal inputs, unifying 3D skeleton pose estimation from video (perception) with skeleton-based motion prediction and in-betweening (generation). Extensive experiments on standard benchmarks, including Human3.6M, demonstrate that our unified method achieves state-of-the-art or competitive performance across all motion tasks. This showcases a more efficient and scalable path for generative motion analysis using skeletons."
    },
    {
      "title": "ClueTracer: Question-to-Vision Clue Tracing for Training-Free Hallucination Suppression in Multimodal Reasoning",
      "url": "https://arxiv.org/abs/2602.02004",
      "date": "2026-02-02",
      "authors_text": "Gongli Xi, Kun Wang, Zeming Gao, Huahui Yi, Haolang Lu",
      "is_highlight": false,
      "score": 24.0,
      "summary": "ClueTracer is a training-free plugin that suppresses hallucinations in multimodal reasoning by tracing visual clue propagation, enhancing model performance across various architectures.",
      "teaser_image": "https://arxiv.org/html/2602.02004/x1.png",
      "debug_abstract": "Large multimodal reasoning models solve challenging visual problems via explicit long-chain inference: they gather visual clues from images and decode clues into textual tokens. Yet this capability also increases hallucinations, where the model generates content that is not supported by the input image or the question. To understand this failure mode, we identify \\emph{reasoning drift}: during clue gathering, the model over-focuses on question-irrelevant entities, diluting focus on task-relevant cues and gradually decoupling the reasoning trace from visual grounding. As a consequence, many inference-time localization or intervention methods developed for non-reasoning models fail to pinpoint the true clues in reasoning settings. Motivated by these insights, we introduce ClueRecall, a metric for assessing visual clue retrieval, and present ClueTracer, a training-free, parameter-free, and architecture-agnostic plugin for hallucination suppression. ClueTracer starts from the question and traces how key clues propagate along the model&#39;s reasoning pathway (question $\\rightarrow$ outputs $\\rightarrow$ visual tokens), thereby localizing task-relevant patches while suppressing spurious attention to irrelevant regions. Remarkably, \\textbf{without any additional training}, ClueTracer improves all \\textbf{reasoning} architectures (including \\texttt{R1-OneVision}, \\texttt{Ocean-R1}, \\texttt{MM-Eureka}, \\emph{etc}.) by $\\mathbf{1.21\\times}$ on reasoning benchmarks. When transferred to \\textbf{non-reasoning} settings, it yields a $\\mathbf{1.14\\times}$ gain."
    },
    {
      "title": "SGHA-Attack: Semantic-Guided Hierarchical Alignment for Transferable Targeted Attacks on Vision-Language Models",
      "url": "https://arxiv.org/abs/2602.01574",
      "date": "2026-02-02",
      "authors_text": "Haobo Wang, Weiqi Luo, Xiaojun Jia, Xiaochun Cao",
      "is_highlight": false,
      "score": 44.0,
      "summary": "SGHA-Attack enhances targeted adversarial attacks on vision-language models by leveraging multiple semantic references and aligning intermediate features for improved transferability and robustness.",
      "teaser_image": "https://arxiv.org/html/2602.01574/x1.png",
      "debug_abstract": "Large vision-language models (VLMs) are vulnerable to transfer-based adversarial perturbations, enabling attackers to optimize on surrogate models and manipulate black-box VLM outputs. Prior targeted transfer attacks often overfit surrogate-specific embedding space by relying on a single reference and emphasizing final-layer alignment, which underutilizes intermediate semantics and degrades transfer across heterogeneous VLMs. To address this, we propose SGHA-Attack, a Semantic-Guided Hierarchical Alignment framework that adopts multiple target references and enforces intermediate-layer consistency. Concretely, we generate a visually grounded reference pool by sampling a frozen text-to-image model conditioned on the target prompt, and then carefully select the Top-K most semantically relevant anchors under the surrogate to form a weighted mixture for stable optimization guidance. Building on these anchors, SGHA-Attack injects target semantics throughout the feature hierarchy by aligning intermediate visual representations at both global and spatial granularities across multiple depths, and by synchronizing intermediate visual and textual features in a shared latent subspace to provide early cross-modal supervision before the final projection. Extensive experiments on open-source and commercial black-box VLMs show that SGHA-Attack achieves stronger targeted transferability than prior methods and remains robust under preprocessing and purification defenses."
    },
    {
      "title": "Estimating Force Interactions of Deformable Linear Objects from their Shapes",
      "url": "https://arxiv.org/abs/2602.01085",
      "date": "2026-02-01",
      "authors_text": "Qi Jing Chen, Shilin Shan, Timothy Bretl, Quang-Cuong Pham",
      "is_highlight": false,
      "score": 83.0,
      "summary": "This paper presents a novel method for estimating external forces on deformable linear objects using shape data, enhancing robot interaction safety and efficiency without requiring force sensors.",
      "teaser_image": "https://arxiv.org/html/2602.01085/x2.png",
      "debug_abstract": "This work introduces an analytical approach for detecting and estimating external forces acting on deformable linear objects (DLOs) using only their observed shapes. In many robot-wire interaction tasks, contact occurs not at the end-effector but at other points along the robot&#39;s body. Such scenarios arise when robots manipulate wires indirectly (e.g., by nudging) or when wires act as passive obstacles in the environment. Accurately identifying these interactions is crucial for safe and efficient trajectory planning, helping to prevent wire damage, avoid restricted robot motions, and mitigate potential hazards. Existing approaches often rely on expensive external force-torque sensor or that contacts occur at the end-effector for accurate force estimation. Using wire shape information acquired from a depth camera and under the assumption that the wire is in or near its static equilibrium, our method estimates both the location and magnitude of external forces without additional prior knowledge. This is achieved by exploiting derived consistency conditions and solving a system of linear equations based on force-torque balance along the wire. The approach was validated through simulation, where it achieved high accuracy, and through real-world experiments, where accurate estimation was demonstrated in selected interaction scenarios."
    },
    {
      "title": "Learning Adaptive Cross-Embodiment Visuomotor Policy with Contrastive Prompt Orchestration",
      "url": "https://arxiv.org/abs/2602.01040",
      "date": "2026-02-01",
      "authors_text": "Yuhang Zhang, Chao Yan, Jiaxi Yu, Jiaping Xiao, Mir Feroskhan",
      "is_highlight": false,
      "score": 92.0,
      "summary": "The paper presents ContrAstive Prompt Orchestration (CAPO), a method for improving adaptive visuomotor policies in diverse environments through hybrid contrastive learning and dynamic prompt orchestration.",
      "teaser_image": "https://arxiv.org/html/2602.01040/x4.png",
      "debug_abstract": "Learning adaptive visuomotor policies for embodied agents remains a formidable challenge, particularly when facing cross-embodiment variations such as diverse sensor configurations and dynamic properties. Conventional learning approaches often struggle to separate task-relevant features from domain-specific variations (e.g., lighting, field-of-view, and rotation), leading to poor sample efficiency and catastrophic failure in unseen environments. To bridge this gap, we propose ContrAstive Prompt Orchestration (CAPO), a novel approach for learning visuomotor policies that integrates contrastive prompt learning and adaptive prompt orchestration. For prompt learning, we devise a hybrid contrastive learning strategy that integrates visual, temporal action, and text objectives to establish a pool of learnable prompts, where each prompt induces a visual representation encapsulating fine-grained domain factors. Based on these learned prompts, we introduce an adaptive prompt orchestration mechanism that dynamically aggregates these prompts conditioned on current observations. This enables the agent to adaptively construct optimal state representations by identifying dominant domain factors instantaneously. Consequently, the policy optimization is effectively shielded from irrelevant interference, preventing the common issue of overfitting to source domains. Extensive experiments demonstrate that CAPO significantly outperforms state-of-the-art baselines in sample efficiency and asymptotic performance. Crucially, it exhibits superior zero-shot adaptation across unseen target domains characterized by drastic environmental (e.g., illumination) and physical shifts (e.g., field-of-view and rotation), validating its effectiveness as a viable solution for cross-embodiment visuomotor policy adaptation."
    },
    {
      "title": "Self-Guard: Defending Large Reasoning Models via enhanced self-reflection",
      "url": "https://arxiv.org/abs/2602.00707",
      "date": "2026-01-31",
      "authors_text": "Jingnan Zheng, Jingjun Xu, Yanzhen Luo, Chenhang Cui, Gelei Deng",
      "is_highlight": false,
      "score": 50.0,
      "summary": "Self-Guard is a lightweight framework enhancing safety compliance in Large Reasoning Models by promoting self-reflection and mitigating risks without sacrificing model utility.",
      "teaser_image": "https://arxiv.org/html/2602.00707/materials/workflow.png",
      "debug_abstract": "The emergence of Large Reasoning Models (LRMs) introduces a new paradigm of explicit reasoning, enabling remarkable advances yet posing unique risks such as reasoning manipulation and information leakage. To mitigate these risks, current alignment strategies predominantly rely on heavy post-training paradigms or external interventions. However, these approaches are often computationally intensive and fail to address the inherent awareness-compliance gap, a critical misalignment where models recognize potential risks yet prioritize following user instructions due to their sycophantic tendencies. To address these limitations, we propose Self-Guard, a lightweight safety defense framework that reinforces safety compliance at the representational level. Self-Guard operates through two principal stages: (1) safety-oriented prompting, which activates the model&#39;s latent safety awareness to evoke spontaneous reflection, and (2) safety activation steering, which extracts the resulting directional shift in the hidden state space and amplifies it to ensure that safety compliance prevails over sycophancy during inference. Experiments demonstrate that Self-Guard effectively bridges the awareness-compliance gap, achieving robust safety performance without compromising model utility. Furthermore, Self-Guard exhibits strong generalization across diverse unseen risks and varying model scales, offering a cost-efficient solution for LRM safety alignment."
    }
  ],
  "MReaLLab": [
    {
      "title": "FD-VLA: Force-Distilled Vision-Language-Action Model for Contact-Rich Manipulation",
      "url": "https://arxiv.org/abs/2602.02142",
      "date": "2026-02-02",
      "authors_text": "Ruiteng Zhao, Wenshuo Wang, Yicheng Ma, Xiaocong Li, Francis E. H. Tay",
      "is_highlight": false,
      "score": 0,
      "summary": "The FD-VLA framework enhances contact-rich manipulation by integrating force awareness through a novel distillation method, improving performance without physical force sensors.",
      "teaser_image": "https://arxiv.org/html/2602.02142/x1.png",
      "debug_abstract": "Force sensing is a crucial modality for Vision-Language-Action (VLA) frameworks, as it enables fine-grained perception and dexterous manipulation in contact-rich tasks. We present Force-Distilled VLA (FD-VLA), a novel framework that integrates force awareness into contact-rich manipulation without relying on physical force sensors. The core of our approach is a Force Distillation Module (FDM), which distills force by mapping a learnable query token, conditioned on visual observations and robot states, into a predicted force token aligned with the latent representation of actual force signals. During inference, this distilled force token is injected into the pretrained VLM, enabling force-aware reasoning while preserving the integrity of its vision-language semantics. This design provides two key benefits: first, it allows practical deployment across a wide range of robots that lack expensive or fragile force-torque sensors, thereby reducing hardware cost and complexity; second, the FDM introduces an additional force-vision-state fusion prior to the VLM, which improves cross-modal alignment and enhances perception-action robustness in contact-rich scenarios. Surprisingly, our physical experiments show that the distilled force token outperforms direct sensor force measurements as well as other baselines, which highlights the effectiveness of this force-distilled VLA approach."
    },
    {
      "title": "FlowSteer: Interactive Agentic Workflow Orchestration via End-to-End Reinforcement Learning",
      "url": "https://arxiv.org/abs/2602.01664",
      "date": "2026-02-02",
      "authors_text": "Mingda Zhang, Haoran Luo, Tiesunlong Shen, Qika Lin, Xiaoying Tang",
      "is_highlight": false,
      "score": 7.0,
      "summary": "FlowSteer is an end-to-end reinforcement learning framework that automates workflow orchestration through interactive learning, addressing manual costs and improving performance across diverse tasks.",
      "teaser_image": "https://arxiv.org/html/2602.01664/x1.png",
      "debug_abstract": "In recent years, a variety of powerful agentic workflows have been applied to solve a wide range of human problems. However, existing workflow orchestration still faces key challenges, including high manual cost, reliance on specific operators/large language models (LLMs), and sparse reward signals. To address these challenges, we propose FlowSteer, an end-to-end reinforcement learning framework that takes a lightweight policy model as the agent and an executable canvas environment, automating workflow orchestration through multi-turn interaction. In this process, the policy model analyzes execution states and selects editing actions, while the canvas executes operators and returns feedback for iterative refinement. Moreover, FlowSteer provides a plug-and-play framework that supports diverse operator libraries and interchangeable LLM backends. To effectively train this interaction paradigm, we propose Canvas Workflow Relative Policy Optimization (CWRPO), which introduces diversity-constrained rewards with conditional release to stabilize learning and suppress shortcut behaviors. Experimental results on twelve datasets show that FlowSteer significantly outperforms baselines across various tasks."
    },
    {
      "title": "UV-M3TL: A Unified and Versatile Multimodal Multi-Task Learning Framework for Assistive Driving Perception",
      "url": "https://arxiv.org/abs/2602.01594",
      "date": "2026-02-02",
      "authors_text": "Wenzhuo Liu, Qiannan Guo, Zhen Wang, Wenshuo Wang, Lei Yang",
      "is_highlight": false,
      "score": 8.0,
      "summary": "The UV-M3TL framework enhances ADAS by effectively recognizing driver behavior and context through dual-branch multimodal embedding and adaptive loss, achieving state-of-the-art performance across multiple tasks.",
      "teaser_image": "https://arxiv.org/html/2602.01594/x2.png",
      "debug_abstract": "Advanced Driver Assistance Systems (ADAS) need to understand human driver behavior while perceiving their navigation context, but jointly learning these heterogeneous tasks would cause inter-task negative transfer and impair system performance. Here, we propose a Unified and Versatile Multimodal Multi-Task Learning (UV-M3TL) framework to simultaneously recognize driver behavior, driver emotion, vehicle behavior, and traffic context, while mitigating inter-task negative transfer. Our framework incorporates two core components: dual-branch spatial channel multimodal embedding (DB-SCME) and adaptive feature-decoupled multi-task loss (AFD-Loss). DB-SCME enhances cross-task knowledge transfer while mitigating task conflicts by employing a dual-branch structure to explicitly model salient task-shared and task-specific features. AFD-Loss improves the stability of joint optimization while guiding the model to learn diverse multi-task representations by introducing an adaptive weighting mechanism based on learning dynamics and feature decoupling constraints. We evaluate our method on the AIDE dataset, and the experimental results demonstrate that UV-M3TL achieves state-of-the-art performance across all four tasks. To further prove the versatility, we evaluate UV-M3TL on additional public multi-task perception benchmarks (BDD100K, CityScapes, NYUD-v2, and PASCAL-Context), where it consistently delivers strong performance across diverse task combinations, attaining state-of-the-art results on most tasks."
    },
    {
      "title": "Failure-Aware Bimanual Teleoperation via Conservative Value Guided Assistance",
      "url": "https://arxiv.org/abs/2602.01092",
      "date": "2026-02-01",
      "authors_text": "Peng Zhou, Zhongxuan Li, Jinsong Wu, Jiaming Qi, Jun Hu",
      "is_highlight": false,
      "score": 9.0,
      "summary": "This paper presents a failure-aware bimanual teleoperation framework that uses conservative value learning to enhance task success and reduce operator workload through compliant haptic assistance.",
      "teaser_image": "https://arxiv.org/html/2602.01092/x1.png",
      "debug_abstract": "Teleoperation of high-precision manipulation is con-strained by tight success tolerances and complex contact dy-namics, which make impending failures difficult for human operators to anticipate under partial observability. This paper proposes a value-guided, failure-aware framework for bimanual teleoperation that provides compliant haptic assistance while pre-serving continuous human authority. The framework is trained entirely from heterogeneous offline teleoperation data containing both successful and failed executions. Task feasibility is mod-eled as a conservative success score learned via Conservative Value Learning, yielding a risk-sensitive estimate that remains reliable under distribution shift. During online operation, the learned success score regulates the level of assistance, while a learned actor provides a corrective motion direction. Both are integrated through a joint-space impedance interface on the master side, yielding continuous guidance that steers the operator away from failure-prone actions without overriding intent. Experimental results on contact-rich manipulation tasks demonstrate improved task success rates and reduced operator workload compared to conventional teleoperation and shared-autonomy baselines, indicating that conservative value learning provides an effective mechanism for embedding failure awareness into bilateral teleoperation. Experimental videos are available at this https URL"
    },
    {
      "title": "Superman: Unifying Skeleton and Vision for Human Motion Perception and Generation",
      "url": "https://arxiv.org/abs/2602.02401",
      "date": "2026-02-02",
      "authors_text": "Xinshun Wang, Peiming Li, Ziyi Wang, Zhongbin Fang, Zhichao Deng",
      "is_highlight": false,
      "score": 15.0,
      "summary": "Superman unifies visual perception and skeleton-based motion generation through a Vision-Guided Motion Tokenizer, enabling robust cross-modal learning and superior performance in human motion tasks.",
      "teaser_image": "https://arxiv.org/html/2602.02401/x2.png",
      "debug_abstract": "Human motion analysis tasks, such as temporal 3D pose estimation, motion prediction, and motion in-betweening, play an essential role in computer vision. However, current paradigms suffer from severe fragmentation. First, the field is split between ``perception&#39;&#39; models that understand motion from video but only output text, and ``generation&#39;&#39; models that cannot perceive from raw visual input. Second, generative MLLMs are often limited to single-frame, static poses using dense, parametric SMPL models, failing to handle temporal motion. Third, existing motion vocabularies are built from skeleton data alone, severing the link to the visual domain. To address these challenges, we introduce Superman, a unified framework that bridges visual perception with temporal, skeleton-based motion generation. Our solution is twofold. First, to overcome the modality disconnect, we propose a Vision-Guided Motion Tokenizer. Leveraging the natural geometric alignment between 3D skeletons and visual data, this module pioneers robust joint learning from both modalities, creating a unified, cross-modal motion vocabulary. Second, grounded in this motion language, a single, unified MLLM architecture is trained to handle all tasks. This module flexibly processes diverse, temporal inputs, unifying 3D skeleton pose estimation from video (perception) with skeleton-based motion prediction and in-betweening (generation). Extensive experiments on standard benchmarks, including Human3.6M, demonstrate that our unified method achieves state-of-the-art or competitive performance across all motion tasks. This showcases a more efficient and scalable path for generative motion analysis using skeletons."
    },
    {
      "title": "ClueTracer: Question-to-Vision Clue Tracing for Training-Free Hallucination Suppression in Multimodal Reasoning",
      "url": "https://arxiv.org/abs/2602.02004",
      "date": "2026-02-02",
      "authors_text": "Gongli Xi, Kun Wang, Zeming Gao, Huahui Yi, Haolang Lu",
      "is_highlight": false,
      "score": 24.0,
      "summary": "ClueTracer is a training-free plugin that suppresses hallucinations in multimodal reasoning by tracing visual clue propagation, enhancing model performance across various architectures.",
      "teaser_image": "https://arxiv.org/html/2602.02004/x1.png",
      "debug_abstract": "Large multimodal reasoning models solve challenging visual problems via explicit long-chain inference: they gather visual clues from images and decode clues into textual tokens. Yet this capability also increases hallucinations, where the model generates content that is not supported by the input image or the question. To understand this failure mode, we identify \\emph{reasoning drift}: during clue gathering, the model over-focuses on question-irrelevant entities, diluting focus on task-relevant cues and gradually decoupling the reasoning trace from visual grounding. As a consequence, many inference-time localization or intervention methods developed for non-reasoning models fail to pinpoint the true clues in reasoning settings. Motivated by these insights, we introduce ClueRecall, a metric for assessing visual clue retrieval, and present ClueTracer, a training-free, parameter-free, and architecture-agnostic plugin for hallucination suppression. ClueTracer starts from the question and traces how key clues propagate along the model&#39;s reasoning pathway (question $\\rightarrow$ outputs $\\rightarrow$ visual tokens), thereby localizing task-relevant patches while suppressing spurious attention to irrelevant regions. Remarkably, \\textbf{without any additional training}, ClueTracer improves all \\textbf{reasoning} architectures (including \\texttt{R1-OneVision}, \\texttt{Ocean-R1}, \\texttt{MM-Eureka}, \\emph{etc}.) by $\\mathbf{1.21\\times}$ on reasoning benchmarks. When transferred to \\textbf{non-reasoning} settings, it yields a $\\mathbf{1.14\\times}$ gain."
    },
    {
      "title": "SGHA-Attack: Semantic-Guided Hierarchical Alignment for Transferable Targeted Attacks on Vision-Language Models",
      "url": "https://arxiv.org/abs/2602.01574",
      "date": "2026-02-02",
      "authors_text": "Haobo Wang, Weiqi Luo, Xiaojun Jia, Xiaochun Cao",
      "is_highlight": false,
      "score": 44.0,
      "summary": "SGHA-Attack enhances targeted adversarial attacks on vision-language models by leveraging multiple semantic references and aligning intermediate features for improved transferability and robustness.",
      "teaser_image": "https://arxiv.org/html/2602.01574/x1.png",
      "debug_abstract": "Large vision-language models (VLMs) are vulnerable to transfer-based adversarial perturbations, enabling attackers to optimize on surrogate models and manipulate black-box VLM outputs. Prior targeted transfer attacks often overfit surrogate-specific embedding space by relying on a single reference and emphasizing final-layer alignment, which underutilizes intermediate semantics and degrades transfer across heterogeneous VLMs. To address this, we propose SGHA-Attack, a Semantic-Guided Hierarchical Alignment framework that adopts multiple target references and enforces intermediate-layer consistency. Concretely, we generate a visually grounded reference pool by sampling a frozen text-to-image model conditioned on the target prompt, and then carefully select the Top-K most semantically relevant anchors under the surrogate to form a weighted mixture for stable optimization guidance. Building on these anchors, SGHA-Attack injects target semantics throughout the feature hierarchy by aligning intermediate visual representations at both global and spatial granularities across multiple depths, and by synchronizing intermediate visual and textual features in a shared latent subspace to provide early cross-modal supervision before the final projection. Extensive experiments on open-source and commercial black-box VLMs show that SGHA-Attack achieves stronger targeted transferability than prior methods and remains robust under preprocessing and purification defenses."
    },
    {
      "title": "Estimating Force Interactions of Deformable Linear Objects from their Shapes",
      "url": "https://arxiv.org/abs/2602.01085",
      "date": "2026-02-01",
      "authors_text": "Qi Jing Chen, Shilin Shan, Timothy Bretl, Quang-Cuong Pham",
      "is_highlight": false,
      "score": 83.0,
      "summary": "This paper presents a novel method for estimating external forces on deformable linear objects using shape data, enhancing robot interaction safety and efficiency without requiring force sensors.",
      "teaser_image": "https://arxiv.org/html/2602.01085/x2.png",
      "debug_abstract": "This work introduces an analytical approach for detecting and estimating external forces acting on deformable linear objects (DLOs) using only their observed shapes. In many robot-wire interaction tasks, contact occurs not at the end-effector but at other points along the robot&#39;s body. Such scenarios arise when robots manipulate wires indirectly (e.g., by nudging) or when wires act as passive obstacles in the environment. Accurately identifying these interactions is crucial for safe and efficient trajectory planning, helping to prevent wire damage, avoid restricted robot motions, and mitigate potential hazards. Existing approaches often rely on expensive external force-torque sensor or that contacts occur at the end-effector for accurate force estimation. Using wire shape information acquired from a depth camera and under the assumption that the wire is in or near its static equilibrium, our method estimates both the location and magnitude of external forces without additional prior knowledge. This is achieved by exploiting derived consistency conditions and solving a system of linear equations based on force-torque balance along the wire. The approach was validated through simulation, where it achieved high accuracy, and through real-world experiments, where accurate estimation was demonstrated in selected interaction scenarios."
    },
    {
      "title": "Learning Adaptive Cross-Embodiment Visuomotor Policy with Contrastive Prompt Orchestration",
      "url": "https://arxiv.org/abs/2602.01040",
      "date": "2026-02-01",
      "authors_text": "Yuhang Zhang, Chao Yan, Jiaxi Yu, Jiaping Xiao, Mir Feroskhan",
      "is_highlight": false,
      "score": 92.0,
      "summary": "The paper presents ContrAstive Prompt Orchestration (CAPO), a method for improving adaptive visuomotor policies in diverse environments through hybrid contrastive learning and dynamic prompt orchestration.",
      "teaser_image": "https://arxiv.org/html/2602.01040/x4.png",
      "debug_abstract": "Learning adaptive visuomotor policies for embodied agents remains a formidable challenge, particularly when facing cross-embodiment variations such as diverse sensor configurations and dynamic properties. Conventional learning approaches often struggle to separate task-relevant features from domain-specific variations (e.g., lighting, field-of-view, and rotation), leading to poor sample efficiency and catastrophic failure in unseen environments. To bridge this gap, we propose ContrAstive Prompt Orchestration (CAPO), a novel approach for learning visuomotor policies that integrates contrastive prompt learning and adaptive prompt orchestration. For prompt learning, we devise a hybrid contrastive learning strategy that integrates visual, temporal action, and text objectives to establish a pool of learnable prompts, where each prompt induces a visual representation encapsulating fine-grained domain factors. Based on these learned prompts, we introduce an adaptive prompt orchestration mechanism that dynamically aggregates these prompts conditioned on current observations. This enables the agent to adaptively construct optimal state representations by identifying dominant domain factors instantaneously. Consequently, the policy optimization is effectively shielded from irrelevant interference, preventing the common issue of overfitting to source domains. Extensive experiments demonstrate that CAPO significantly outperforms state-of-the-art baselines in sample efficiency and asymptotic performance. Crucially, it exhibits superior zero-shot adaptation across unseen target domains characterized by drastic environmental (e.g., illumination) and physical shifts (e.g., field-of-view and rotation), validating its effectiveness as a viable solution for cross-embodiment visuomotor policy adaptation."
    },
    {
      "title": "Self-Guard: Defending Large Reasoning Models via enhanced self-reflection",
      "url": "https://arxiv.org/abs/2602.00707",
      "date": "2026-01-31",
      "authors_text": "Jingnan Zheng, Jingjun Xu, Yanzhen Luo, Chenhang Cui, Gelei Deng",
      "is_highlight": false,
      "score": 50.0,
      "summary": "Self-Guard is a lightweight framework enhancing safety compliance in Large Reasoning Models by promoting self-reflection and mitigating risks without sacrificing model utility.",
      "teaser_image": "https://arxiv.org/html/2602.00707/materials/workflow.png",
      "debug_abstract": "The emergence of Large Reasoning Models (LRMs) introduces a new paradigm of explicit reasoning, enabling remarkable advances yet posing unique risks such as reasoning manipulation and information leakage. To mitigate these risks, current alignment strategies predominantly rely on heavy post-training paradigms or external interventions. However, these approaches are often computationally intensive and fail to address the inherent awareness-compliance gap, a critical misalignment where models recognize potential risks yet prioritize following user instructions due to their sycophantic tendencies. To address these limitations, we propose Self-Guard, a lightweight safety defense framework that reinforces safety compliance at the representational level. Self-Guard operates through two principal stages: (1) safety-oriented prompting, which activates the model&#39;s latent safety awareness to evoke spontaneous reflection, and (2) safety activation steering, which extracts the resulting directional shift in the hidden state space and amplifies it to ensure that safety compliance prevails over sycophancy during inference. Experiments demonstrate that Self-Guard effectively bridges the awareness-compliance gap, achieving robust safety performance without compromising model utility. Furthermore, Self-Guard exhibits strong generalization across diverse unseen risks and varying model scales, offering a cost-efficient solution for LRM safety alignment."
    }
  ],
  "MARS Lab": [
    {
      "title": "FD-VLA: Force-Distilled Vision-Language-Action Model for Contact-Rich Manipulation",
      "url": "https://arxiv.org/abs/2602.02142",
      "date": "2026-02-02",
      "authors_text": "Ruiteng Zhao, Wenshuo Wang, Yicheng Ma, Xiaocong Li, Francis E. H. Tay",
      "is_highlight": false,
      "score": 0,
      "summary": "The FD-VLA framework enhances contact-rich manipulation by integrating force awareness through a novel distillation method, improving performance without physical force sensors.",
      "teaser_image": "https://arxiv.org/html/2602.02142/x1.png",
      "debug_abstract": "Force sensing is a crucial modality for Vision-Language-Action (VLA) frameworks, as it enables fine-grained perception and dexterous manipulation in contact-rich tasks. We present Force-Distilled VLA (FD-VLA), a novel framework that integrates force awareness into contact-rich manipulation without relying on physical force sensors. The core of our approach is a Force Distillation Module (FDM), which distills force by mapping a learnable query token, conditioned on visual observations and robot states, into a predicted force token aligned with the latent representation of actual force signals. During inference, this distilled force token is injected into the pretrained VLM, enabling force-aware reasoning while preserving the integrity of its vision-language semantics. This design provides two key benefits: first, it allows practical deployment across a wide range of robots that lack expensive or fragile force-torque sensors, thereby reducing hardware cost and complexity; second, the FDM introduces an additional force-vision-state fusion prior to the VLM, which improves cross-modal alignment and enhances perception-action robustness in contact-rich scenarios. Surprisingly, our physical experiments show that the distilled force token outperforms direct sensor force measurements as well as other baselines, which highlights the effectiveness of this force-distilled VLA approach."
    },
    {
      "title": "FlowSteer: Interactive Agentic Workflow Orchestration via End-to-End Reinforcement Learning",
      "url": "https://arxiv.org/abs/2602.01664",
      "date": "2026-02-02",
      "authors_text": "Mingda Zhang, Haoran Luo, Tiesunlong Shen, Qika Lin, Xiaoying Tang",
      "is_highlight": false,
      "score": 7.0,
      "summary": "FlowSteer is an end-to-end reinforcement learning framework that automates workflow orchestration through interactive learning, addressing manual costs and improving performance across diverse tasks.",
      "teaser_image": "https://arxiv.org/html/2602.01664/x1.png",
      "debug_abstract": "In recent years, a variety of powerful agentic workflows have been applied to solve a wide range of human problems. However, existing workflow orchestration still faces key challenges, including high manual cost, reliance on specific operators/large language models (LLMs), and sparse reward signals. To address these challenges, we propose FlowSteer, an end-to-end reinforcement learning framework that takes a lightweight policy model as the agent and an executable canvas environment, automating workflow orchestration through multi-turn interaction. In this process, the policy model analyzes execution states and selects editing actions, while the canvas executes operators and returns feedback for iterative refinement. Moreover, FlowSteer provides a plug-and-play framework that supports diverse operator libraries and interchangeable LLM backends. To effectively train this interaction paradigm, we propose Canvas Workflow Relative Policy Optimization (CWRPO), which introduces diversity-constrained rewards with conditional release to stabilize learning and suppress shortcut behaviors. Experimental results on twelve datasets show that FlowSteer significantly outperforms baselines across various tasks."
    },
    {
      "title": "UV-M3TL: A Unified and Versatile Multimodal Multi-Task Learning Framework for Assistive Driving Perception",
      "url": "https://arxiv.org/abs/2602.01594",
      "date": "2026-02-02",
      "authors_text": "Wenzhuo Liu, Qiannan Guo, Zhen Wang, Wenshuo Wang, Lei Yang",
      "is_highlight": false,
      "score": 8.0,
      "summary": "The UV-M3TL framework enhances ADAS by effectively recognizing driver behavior and context through dual-branch multimodal embedding and adaptive loss, achieving state-of-the-art performance across multiple tasks.",
      "teaser_image": "https://arxiv.org/html/2602.01594/x2.png",
      "debug_abstract": "Advanced Driver Assistance Systems (ADAS) need to understand human driver behavior while perceiving their navigation context, but jointly learning these heterogeneous tasks would cause inter-task negative transfer and impair system performance. Here, we propose a Unified and Versatile Multimodal Multi-Task Learning (UV-M3TL) framework to simultaneously recognize driver behavior, driver emotion, vehicle behavior, and traffic context, while mitigating inter-task negative transfer. Our framework incorporates two core components: dual-branch spatial channel multimodal embedding (DB-SCME) and adaptive feature-decoupled multi-task loss (AFD-Loss). DB-SCME enhances cross-task knowledge transfer while mitigating task conflicts by employing a dual-branch structure to explicitly model salient task-shared and task-specific features. AFD-Loss improves the stability of joint optimization while guiding the model to learn diverse multi-task representations by introducing an adaptive weighting mechanism based on learning dynamics and feature decoupling constraints. We evaluate our method on the AIDE dataset, and the experimental results demonstrate that UV-M3TL achieves state-of-the-art performance across all four tasks. To further prove the versatility, we evaluate UV-M3TL on additional public multi-task perception benchmarks (BDD100K, CityScapes, NYUD-v2, and PASCAL-Context), where it consistently delivers strong performance across diverse task combinations, attaining state-of-the-art results on most tasks."
    },
    {
      "title": "Failure-Aware Bimanual Teleoperation via Conservative Value Guided Assistance",
      "url": "https://arxiv.org/abs/2602.01092",
      "date": "2026-02-01",
      "authors_text": "Peng Zhou, Zhongxuan Li, Jinsong Wu, Jiaming Qi, Jun Hu",
      "is_highlight": false,
      "score": 9.0,
      "summary": "This paper presents a failure-aware bimanual teleoperation framework that uses conservative value learning to enhance task success and reduce operator workload through compliant haptic assistance.",
      "teaser_image": "https://arxiv.org/html/2602.01092/x1.png",
      "debug_abstract": "Teleoperation of high-precision manipulation is con-strained by tight success tolerances and complex contact dy-namics, which make impending failures difficult for human operators to anticipate under partial observability. This paper proposes a value-guided, failure-aware framework for bimanual teleoperation that provides compliant haptic assistance while pre-serving continuous human authority. The framework is trained entirely from heterogeneous offline teleoperation data containing both successful and failed executions. Task feasibility is mod-eled as a conservative success score learned via Conservative Value Learning, yielding a risk-sensitive estimate that remains reliable under distribution shift. During online operation, the learned success score regulates the level of assistance, while a learned actor provides a corrective motion direction. Both are integrated through a joint-space impedance interface on the master side, yielding continuous guidance that steers the operator away from failure-prone actions without overriding intent. Experimental results on contact-rich manipulation tasks demonstrate improved task success rates and reduced operator workload compared to conventional teleoperation and shared-autonomy baselines, indicating that conservative value learning provides an effective mechanism for embedding failure awareness into bilateral teleoperation. Experimental videos are available at this https URL"
    },
    {
      "title": "Superman: Unifying Skeleton and Vision for Human Motion Perception and Generation",
      "url": "https://arxiv.org/abs/2602.02401",
      "date": "2026-02-02",
      "authors_text": "Xinshun Wang, Peiming Li, Ziyi Wang, Zhongbin Fang, Zhichao Deng",
      "is_highlight": false,
      "score": 15.0,
      "summary": "Superman unifies visual perception and skeleton-based motion generation through a Vision-Guided Motion Tokenizer, enabling robust cross-modal learning and superior performance in human motion tasks.",
      "teaser_image": "https://arxiv.org/html/2602.02401/x2.png",
      "debug_abstract": "Human motion analysis tasks, such as temporal 3D pose estimation, motion prediction, and motion in-betweening, play an essential role in computer vision. However, current paradigms suffer from severe fragmentation. First, the field is split between ``perception&#39;&#39; models that understand motion from video but only output text, and ``generation&#39;&#39; models that cannot perceive from raw visual input. Second, generative MLLMs are often limited to single-frame, static poses using dense, parametric SMPL models, failing to handle temporal motion. Third, existing motion vocabularies are built from skeleton data alone, severing the link to the visual domain. To address these challenges, we introduce Superman, a unified framework that bridges visual perception with temporal, skeleton-based motion generation. Our solution is twofold. First, to overcome the modality disconnect, we propose a Vision-Guided Motion Tokenizer. Leveraging the natural geometric alignment between 3D skeletons and visual data, this module pioneers robust joint learning from both modalities, creating a unified, cross-modal motion vocabulary. Second, grounded in this motion language, a single, unified MLLM architecture is trained to handle all tasks. This module flexibly processes diverse, temporal inputs, unifying 3D skeleton pose estimation from video (perception) with skeleton-based motion prediction and in-betweening (generation). Extensive experiments on standard benchmarks, including Human3.6M, demonstrate that our unified method achieves state-of-the-art or competitive performance across all motion tasks. This showcases a more efficient and scalable path for generative motion analysis using skeletons."
    },
    {
      "title": "ClueTracer: Question-to-Vision Clue Tracing for Training-Free Hallucination Suppression in Multimodal Reasoning",
      "url": "https://arxiv.org/abs/2602.02004",
      "date": "2026-02-02",
      "authors_text": "Gongli Xi, Kun Wang, Zeming Gao, Huahui Yi, Haolang Lu",
      "is_highlight": false,
      "score": 24.0,
      "summary": "ClueTracer is a training-free plugin that suppresses hallucinations in multimodal reasoning by tracing visual clue propagation, enhancing model performance across various architectures.",
      "teaser_image": "https://arxiv.org/html/2602.02004/x1.png",
      "debug_abstract": "Large multimodal reasoning models solve challenging visual problems via explicit long-chain inference: they gather visual clues from images and decode clues into textual tokens. Yet this capability also increases hallucinations, where the model generates content that is not supported by the input image or the question. To understand this failure mode, we identify \\emph{reasoning drift}: during clue gathering, the model over-focuses on question-irrelevant entities, diluting focus on task-relevant cues and gradually decoupling the reasoning trace from visual grounding. As a consequence, many inference-time localization or intervention methods developed for non-reasoning models fail to pinpoint the true clues in reasoning settings. Motivated by these insights, we introduce ClueRecall, a metric for assessing visual clue retrieval, and present ClueTracer, a training-free, parameter-free, and architecture-agnostic plugin for hallucination suppression. ClueTracer starts from the question and traces how key clues propagate along the model&#39;s reasoning pathway (question $\\rightarrow$ outputs $\\rightarrow$ visual tokens), thereby localizing task-relevant patches while suppressing spurious attention to irrelevant regions. Remarkably, \\textbf{without any additional training}, ClueTracer improves all \\textbf{reasoning} architectures (including \\texttt{R1-OneVision}, \\texttt{Ocean-R1}, \\texttt{MM-Eureka}, \\emph{etc}.) by $\\mathbf{1.21\\times}$ on reasoning benchmarks. When transferred to \\textbf{non-reasoning} settings, it yields a $\\mathbf{1.14\\times}$ gain."
    },
    {
      "title": "SGHA-Attack: Semantic-Guided Hierarchical Alignment for Transferable Targeted Attacks on Vision-Language Models",
      "url": "https://arxiv.org/abs/2602.01574",
      "date": "2026-02-02",
      "authors_text": "Haobo Wang, Weiqi Luo, Xiaojun Jia, Xiaochun Cao",
      "is_highlight": false,
      "score": 44.0,
      "summary": "SGHA-Attack enhances targeted adversarial attacks on vision-language models by leveraging multiple semantic references and aligning intermediate features for improved transferability and robustness.",
      "teaser_image": "https://arxiv.org/html/2602.01574/x1.png",
      "debug_abstract": "Large vision-language models (VLMs) are vulnerable to transfer-based adversarial perturbations, enabling attackers to optimize on surrogate models and manipulate black-box VLM outputs. Prior targeted transfer attacks often overfit surrogate-specific embedding space by relying on a single reference and emphasizing final-layer alignment, which underutilizes intermediate semantics and degrades transfer across heterogeneous VLMs. To address this, we propose SGHA-Attack, a Semantic-Guided Hierarchical Alignment framework that adopts multiple target references and enforces intermediate-layer consistency. Concretely, we generate a visually grounded reference pool by sampling a frozen text-to-image model conditioned on the target prompt, and then carefully select the Top-K most semantically relevant anchors under the surrogate to form a weighted mixture for stable optimization guidance. Building on these anchors, SGHA-Attack injects target semantics throughout the feature hierarchy by aligning intermediate visual representations at both global and spatial granularities across multiple depths, and by synchronizing intermediate visual and textual features in a shared latent subspace to provide early cross-modal supervision before the final projection. Extensive experiments on open-source and commercial black-box VLMs show that SGHA-Attack achieves stronger targeted transferability than prior methods and remains robust under preprocessing and purification defenses."
    },
    {
      "title": "Estimating Force Interactions of Deformable Linear Objects from their Shapes",
      "url": "https://arxiv.org/abs/2602.01085",
      "date": "2026-02-01",
      "authors_text": "Qi Jing Chen, Shilin Shan, Timothy Bretl, Quang-Cuong Pham",
      "is_highlight": false,
      "score": 83.0,
      "summary": "This paper presents a novel method for estimating external forces on deformable linear objects using shape data, enhancing robot interaction safety and efficiency without requiring force sensors.",
      "teaser_image": "https://arxiv.org/html/2602.01085/x2.png",
      "debug_abstract": "This work introduces an analytical approach for detecting and estimating external forces acting on deformable linear objects (DLOs) using only their observed shapes. In many robot-wire interaction tasks, contact occurs not at the end-effector but at other points along the robot&#39;s body. Such scenarios arise when robots manipulate wires indirectly (e.g., by nudging) or when wires act as passive obstacles in the environment. Accurately identifying these interactions is crucial for safe and efficient trajectory planning, helping to prevent wire damage, avoid restricted robot motions, and mitigate potential hazards. Existing approaches often rely on expensive external force-torque sensor or that contacts occur at the end-effector for accurate force estimation. Using wire shape information acquired from a depth camera and under the assumption that the wire is in or near its static equilibrium, our method estimates both the location and magnitude of external forces without additional prior knowledge. This is achieved by exploiting derived consistency conditions and solving a system of linear equations based on force-torque balance along the wire. The approach was validated through simulation, where it achieved high accuracy, and through real-world experiments, where accurate estimation was demonstrated in selected interaction scenarios."
    },
    {
      "title": "Learning Adaptive Cross-Embodiment Visuomotor Policy with Contrastive Prompt Orchestration",
      "url": "https://arxiv.org/abs/2602.01040",
      "date": "2026-02-01",
      "authors_text": "Yuhang Zhang, Chao Yan, Jiaxi Yu, Jiaping Xiao, Mir Feroskhan",
      "is_highlight": false,
      "score": 92.0,
      "summary": "The paper presents ContrAstive Prompt Orchestration (CAPO), a method for improving adaptive visuomotor policies in diverse environments through hybrid contrastive learning and dynamic prompt orchestration.",
      "teaser_image": "https://arxiv.org/html/2602.01040/x4.png",
      "debug_abstract": "Learning adaptive visuomotor policies for embodied agents remains a formidable challenge, particularly when facing cross-embodiment variations such as diverse sensor configurations and dynamic properties. Conventional learning approaches often struggle to separate task-relevant features from domain-specific variations (e.g., lighting, field-of-view, and rotation), leading to poor sample efficiency and catastrophic failure in unseen environments. To bridge this gap, we propose ContrAstive Prompt Orchestration (CAPO), a novel approach for learning visuomotor policies that integrates contrastive prompt learning and adaptive prompt orchestration. For prompt learning, we devise a hybrid contrastive learning strategy that integrates visual, temporal action, and text objectives to establish a pool of learnable prompts, where each prompt induces a visual representation encapsulating fine-grained domain factors. Based on these learned prompts, we introduce an adaptive prompt orchestration mechanism that dynamically aggregates these prompts conditioned on current observations. This enables the agent to adaptively construct optimal state representations by identifying dominant domain factors instantaneously. Consequently, the policy optimization is effectively shielded from irrelevant interference, preventing the common issue of overfitting to source domains. Extensive experiments demonstrate that CAPO significantly outperforms state-of-the-art baselines in sample efficiency and asymptotic performance. Crucially, it exhibits superior zero-shot adaptation across unseen target domains characterized by drastic environmental (e.g., illumination) and physical shifts (e.g., field-of-view and rotation), validating its effectiveness as a viable solution for cross-embodiment visuomotor policy adaptation."
    },
    {
      "title": "Self-Guard: Defending Large Reasoning Models via enhanced self-reflection",
      "url": "https://arxiv.org/abs/2602.00707",
      "date": "2026-01-31",
      "authors_text": "Jingnan Zheng, Jingjun Xu, Yanzhen Luo, Chenhang Cui, Gelei Deng",
      "is_highlight": false,
      "score": 50.0,
      "summary": "Self-Guard is a lightweight framework enhancing safety compliance in Large Reasoning Models by promoting self-reflection and mitigating risks without sacrificing model utility.",
      "teaser_image": "https://arxiv.org/html/2602.00707/materials/workflow.png",
      "debug_abstract": "The emergence of Large Reasoning Models (LRMs) introduces a new paradigm of explicit reasoning, enabling remarkable advances yet posing unique risks such as reasoning manipulation and information leakage. To mitigate these risks, current alignment strategies predominantly rely on heavy post-training paradigms or external interventions. However, these approaches are often computationally intensive and fail to address the inherent awareness-compliance gap, a critical misalignment where models recognize potential risks yet prioritize following user instructions due to their sycophantic tendencies. To address these limitations, we propose Self-Guard, a lightweight safety defense framework that reinforces safety compliance at the representational level. Self-Guard operates through two principal stages: (1) safety-oriented prompting, which activates the model&#39;s latent safety awareness to evoke spontaneous reflection, and (2) safety activation steering, which extracts the resulting directional shift in the hidden state space and amplifies it to ensure that safety compliance prevails over sycophancy during inference. Experiments demonstrate that Self-Guard effectively bridges the awareness-compliance gap, achieving robust safety performance without compromising model utility. Furthermore, Self-Guard exhibits strong generalization across diverse unseen risks and varying model scales, offering a cost-efficient solution for LRM safety alignment."
    }
  ],
  "北京航空航天大学": [
    {
      "title": "UV-M3TL: A Unified and Versatile Multimodal Multi-Task Learning Framework for Assistive Driving Perception",
      "url": "https://arxiv.org/abs/2602.01594",
      "date": "2026-02-02",
      "authors_text": "Wenzhuo Liu, Qiannan Guo, Zhen Wang, Wenshuo Wang, Lei Yang",
      "is_highlight": false,
      "score": 8.0,
      "summary": "The UV-M3TL framework enhances ADAS by effectively recognizing driver behavior and context through dual-branch multimodal embedding and adaptive loss, achieving state-of-the-art performance across multiple tasks.",
      "teaser_image": "https://arxiv.org/html/2602.01594/x2.png",
      "debug_abstract": "Advanced Driver Assistance Systems (ADAS) need to understand human driver behavior while perceiving their navigation context, but jointly learning these heterogeneous tasks would cause inter-task negative transfer and impair system performance. Here, we propose a Unified and Versatile Multimodal Multi-Task Learning (UV-M3TL) framework to simultaneously recognize driver behavior, driver emotion, vehicle behavior, and traffic context, while mitigating inter-task negative transfer. Our framework incorporates two core components: dual-branch spatial channel multimodal embedding (DB-SCME) and adaptive feature-decoupled multi-task loss (AFD-Loss). DB-SCME enhances cross-task knowledge transfer while mitigating task conflicts by employing a dual-branch structure to explicitly model salient task-shared and task-specific features. AFD-Loss improves the stability of joint optimization while guiding the model to learn diverse multi-task representations by introducing an adaptive weighting mechanism based on learning dynamics and feature decoupling constraints. We evaluate our method on the AIDE dataset, and the experimental results demonstrate that UV-M3TL achieves state-of-the-art performance across all four tasks. To further prove the versatility, we evaluate UV-M3TL on additional public multi-task perception benchmarks (BDD100K, CityScapes, NYUD-v2, and PASCAL-Context), where it consistently delivers strong performance across diverse task combinations, attaining state-of-the-art results on most tasks."
    },
    {
      "title": "UrbanGS: A Scalable and Efficient Architecture for Geometrically Accurate Large-Scene Reconstruction",
      "url": "https://arxiv.org/abs/2602.02089",
      "date": "2026-02-02",
      "authors_text": "Changbai Li, Haodong Zhu, Hanlin Chen, Xiuping Liang, Tongfei Chen",
      "is_highlight": false,
      "score": 22.0,
      "summary": "UrbanGS is a scalable framework that enhances geometric accuracy and memory efficiency in large-scale urban scene reconstruction through innovative regularization and adaptive Gaussian pruning techniques.",
      "teaser_image": "https://arxiv.org/html/2602.02089/figure/main.png",
      "debug_abstract": "While 3D Gaussian Splatting (3DGS) enables high-quality, real-time rendering for bounded scenes, its extension to large-scale urban environments gives rise to critical challenges in terms of geometric consistency, memory efficiency, and computational scalability. To address these issues, we present UrbanGS, a scalable reconstruction framework that effectively tackles these challenges for city-scale applications. First, we propose a Depth-Consistent D-Normal Regularization module. Unlike existing approaches that rely solely on monocular normal estimators, which can effectively update rotation parameters yet struggle to update position parameters, our method integrates D-Normal constraints with external depth supervision. This allows for comprehensive updates of all geometric parameters. By further incorporating an adaptive confidence weighting mechanism based on gradient consistency and inverse depth deviation, our approach significantly enhances multi-view depth alignment and geometric coherence, which effectively resolves the issue of geometric accuracy in complex large-scale scenes. To improve scalability, we introduce a Spatially Adaptive Gaussian Pruning (SAGP) strategy, which dynamically adjusts Gaussian density based on local geometric complexity and visibility to reduce redundancy. Additionally, a unified partitioning and view assignment scheme is designed to eliminate boundary artifacts and optimize computational load. Extensive experiments on multiple urban datasets demonstrate that UrbanGS achieves superior performance in rendering quality, geometric accuracy, and memory efficiency, providing a systematic solution for high-fidelity large-scale scene reconstruction."
    },
    {
      "title": "MedAD-R1: Eliciting Consistent Reasoning in Interpretible Medical Anomaly Detection via Consistency-Reinforced Policy Optimization",
      "url": "https://arxiv.org/abs/2602.01081",
      "date": "2026-02-01",
      "authors_text": "Haitao Zhang, Yingying Wang, Jiaxiang Wang, Haote Xu, Hongyang Zhang",
      "is_highlight": false,
      "score": 64.0,
      "summary": "MedAD-R1 enhances medical anomaly detection by utilizing a two-stage training framework with consistency-reinforced policy optimization, achieving state-of-the-art performance on the MedAD-38K benchmark.",
      "teaser_image": "https://arxiv.org/html/2602.01081/x3.png",
      "debug_abstract": "Medical Anomaly Detection (MedAD) presents a significant opportunity to enhance diagnostic accuracy using Large Multimodal Models (LMMs) to interpret and answer questions based on medical images. However, the reliance on Supervised Fine-Tuning (SFT) on simplistic and fragmented datasets has hindered the development of models capable of plausible reasoning and robust multimodal generalization. To overcome this, we introduce MedAD-38K, the first large-scale, multi-modal, and multi-center benchmark for MedAD featuring diagnostic Chain-of-Thought (CoT) annotations alongside structured Visual Question-Answering (VQA) pairs. On this foundation, we propose a two-stage training framework. The first stage, Cognitive Injection, uses SFT to instill foundational medical knowledge and align the model with a structured think-then-answer paradigm. Given that standard policy optimization can produce reasoning that is disconnected from the final answer, the second stage incorporates Consistency Group Relative Policy Optimization (Con-GRPO). This novel algorithm incorporates a crucial consistency reward to ensure the generated reasoning process is relevant and logically coherent with the final diagnosis. Our proposed model, MedAD-R1, achieves state-of-the-art (SOTA) performance on the MedAD-38K benchmark, outperforming strong baselines by more than 10\\%. This superior performance stems from its ability to generate transparent and logically consistent reasoning pathways, offering a promising approach to enhancing the trustworthiness and interpretability of AI for clinical decision support."
    },
    {
      "title": "Meanshift Shape Formation Control Using Discrete Mass Distribution",
      "url": "https://arxiv.org/abs/2602.00980",
      "date": "2026-02-01",
      "authors_text": "Yichen Cai, Yuan Gao, Pengpeng Li, Wei Wang, Guibin Sun",
      "is_highlight": false,
      "score": 81.0,
      "summary": "This paper presents a decentralized control strategy using discrete mass distribution for adaptive swarm formation of complex shapes, validated through simulations and real-world experiments.",
      "teaser_image": "https://arxiv.org/html/2602.00980/x1.png",
      "debug_abstract": "The density-distribution method has recently become a promising paradigm owing to its adaptability to variations in swarm size. However, existing studies face practical challenges in achieving complex shape representation and decentralized implementation. This motivates us to develop a fully decentralized, distribution-based control strategy with the dual capability of forming complex shapes and adapting to swarm-size variations. Specifically, we first propose a discrete mass-distribution function defined over a set of sample points to model swarm formation. In contrast to the continuous density-distribution method, our model eliminates the requirement for defining continuous density functions-a task that is difficult for complex shapes. Second, we design a decentralized meanshift control law to coordinate the swarm&#39;s global distribution to fit the sample-point distribution by feeding back mass estimates. The mass estimates for all sample points are achieved by the robots in a decentralized manner via the designed mass estimator. It is shown that the mass estimates of the sample points can asymptotically converge to the true global values. To validate the proposed strategy, we conduct comprehensive simulations and real-world experiments to evaluate the efficiency of complex shape formation and adaptability to swarm-size variations."
    },
    {
      "title": "USS-Nav: Unified Spatio-Semantic Scene Graph for Lightweight UAV Zero-Shot Object Navigation",
      "url": "https://arxiv.org/abs/2602.00708",
      "date": "2026-01-31",
      "authors_text": "Weiqi Gai, Yuman Gao, Yuan Zhou, Yufan Xie, Zhiyang Liu",
      "is_highlight": false,
      "score": 80.0,
      "summary": "USS-Nav introduces a lightweight framework for UAV zero-shot object navigation, utilizing a Unified Spatio-Semantic scene graph for efficient exploration and improved computational performance.",
      "teaser_image": "https://arxiv.org/html/2602.00708/figs/skeleton_gen.png",
      "debug_abstract": "Zero-Shot Object Navigation in unknown environments poses significant challenges for Unmanned Aerial Vehicles (UAVs) due to the conflict between high-level semantic reasoning requirements and limited onboard computational resources. To address this, we present USS-Nav, a lightweight framework that incrementally constructs a Unified Spatio-Semantic scene graph and enables efficient Large Language Model (LLM)-augmented Zero-Shot Object Navigation in unknown environments. Specifically, we introduce an incremental Spatial Connectivity Graph generation method utilizing polyhedral expansion to capture global geometric topology, which is dynamically partitioned into semantic regions via graph clustering. Concurrently, open-vocabulary object semantics are instantiated and anchored to this topology to form a hierarchical environmental representation. Leveraging this hierarchical structure, we present a coarse-to-fine exploration strategy: LLM grounded in the scene graph&#39;s semantics to determine global target regions, while a local planner optimizes frontier coverage based on information gain. Experimental results demonstrate that our framework outperforms state-of-the-art methods in terms of computational efficiency and real-time update frequency (15 Hz) on a resource-constrained platform. Furthermore, ablation studies confirm the effectiveness of our framework, showing substantial improvements in Success weighted by Path Length (SPL). The source code will be made publicly available to foster further research."
    }
  ],
  "机器人研究所": [
    {
      "title": "Towards Agentic Intelligence for Materials Science",
      "url": "https://arxiv.org/abs/2602.00169",
      "date": "2026-01-29",
      "authors_text": "Huan Zhang, Yizhan Li, Wenhao Huang, Ziyu Hou, Yu Song",
      "is_highlight": false,
      "score": 12.0,
      "summary": "This paper advocates for developing agentic AI systems in materials science that autonomously plan, act, and learn to enhance discovery processes beyond traditional models.",
      "teaser_image": "https://arxiv.org/html/2602.00169/x1.png",
      "debug_abstract": "The convergence of artificial intelligence and materials science presents a transformative opportunity, but achieving true acceleration in discovery requires moving beyond task-isolated, fine-tuned models toward agentic systems that plan, act, and learn across the full discovery loop. This survey advances a unique pipeline-centric view that spans from corpus curation and pretraining, through domain adaptation and instruction tuning, to goal-conditioned agents interfacing with simulation and experimental platforms. Unlike prior reviews, we treat the entire process as an end-to-end system to be optimized for tangible discovery outcomes rather than proxy benchmarks. This perspective allows us to trace how upstream design choices-such as data curation and training objectives-can be aligned with downstream experimental success through effective credit assignment. To bridge communities and establish a shared frame of reference, we first present an integrated lens that aligns terminology, evaluation, and workflow stages across AI and materials science. We then analyze the field through two focused lenses: From the AI perspective, the survey details LLM strengths in pattern recognition, predictive analytics, and natural language processing for literature mining, materials characterization, and property prediction; from the materials science perspective, it highlights applications in materials design, process optimization, and the acceleration of computational workflows via integration with external tools (e.g., DFT, robotic labs). Finally, we contrast passive, reactive approaches with agentic design, cataloging current contributions while motivating systems that pursue long-horizon goals with autonomy, memory, and tool use. This survey charts a practical roadmap towards autonomous, safety-aware LLM agents aimed at discovering novel and useful materials."
    },
    {
      "title": "Breaking the Static Graph: Context-Aware Traversal for Robust Retrieval-Augmented Generation",
      "url": "https://arxiv.org/abs/2602.01965",
      "date": "2026-02-02",
      "authors_text": "Kwun Hang Lau, Fangyuan Zhang, Boyu Ruan, Yingli Zhou, Qintian Guo",
      "is_highlight": false,
      "score": 27.0,
      "summary": "CatRAG enhances Retrieval-Augmented Generation by transforming static Knowledge Graphs into query-adaptive structures, improving multi-hop evidence retrieval and reasoning completeness.",
      "teaser_image": "https://arxiv.org/html/2602.01965/image/HubBiasShift.png",
      "debug_abstract": "Recent advances in Retrieval-Augmented Generation (RAG) have shifted from simple vector similarity to structure-aware approaches like HippoRAG, which leverage Knowledge Graphs (KGs) and Personalized PageRank (PPR) to capture multi-hop dependencies. However, these methods suffer from a &#34;Static Graph Fallacy&#34;: they rely on fixed transition probabilities determined during indexing. This rigidity ignores the query-dependent nature of edge relevance, causing semantic drift where random walks are diverted into high-degree &#34;hub&#34; nodes before reaching critical downstream evidence. Consequently, models often achieve high partial recall but fail to retrieve the complete evidence chain required for multi-hop queries. To address this, we propose CatRAG, Context-Aware Traversal for robust RAG, a framework that builds on the HippoRAG 2 architecture and transforms the static KG into a query-adaptive navigation structure. We introduce a multi-faceted framework to steer the random walk: (1) Symbolic Anchoring, which injects weak entity constraints to regularize the random walk; (2) Query-Aware Dynamic Edge Weighting, which dynamically modulates graph structure, to prune irrelevant paths while amplifying those aligned with the query&#39;s intent; and (3) Key-Fact Passage Weight Enhancement, a cost-efficient bias that structurally anchors the random walk to likely evidence. Experiments across four multi-hop benchmarks demonstrate that CatRAG consistently outperforms state of the art baselines. Our analysis reveals that while standard Recall metrics show modest gains, CatRAG achieves substantial improvements in reasoning completeness, the capacity to recover the entire evidence path without gaps. These results reveal that our approach effectively bridges the gap between retrieving partial context and enabling fully grounded reasoning. Resources are available at this https URL."
    },
    {
      "title": "Towards Exploratory and Focused Manipulation with Bimanual Active Perception: A New Problem, Benchmark and Strategy",
      "url": "https://arxiv.org/abs/2602.01939",
      "date": "2026-02-02",
      "authors_text": "Yuxin He, Ruihao Zhang, Tianao Shen, Cheng Liu, Qiang Nie",
      "is_highlight": false,
      "score": 28.0,
      "summary": "The paper introduces the Exploratory and Focused Manipulation (EFM) problem, establishes the EFM-10 benchmark, and proposes a Bimanual Active Perception strategy to enhance manipulation tasks.",
      "teaser_image": "https://arxiv.org/html/2602.01939/x2.png",
      "debug_abstract": "Recently, active vision has reemerged as an important concept for manipulation, since visual occlusion occurs more frequently when main cameras are mounted on the robot heads. We reflect on the visual occlusion issue and identify its essence as the absence of information useful for task completion. Inspired by this, we come up with the more fundamental problem of Exploratory and Focused Manipulation (EFM). The proposed problem is about actively collecting information to complete challenging manipulation tasks that require exploration or focus. As an initial attempt to address this problem, we establish the EFM-10 benchmark that consists of 4 categories of tasks that align with our definition (10 tasks in total). We further come up with a Bimanual Active Perception (BAP) strategy, which leverages one arm to provide active vision and another arm to provide force sensing while manipulating. Based on this idea, we collect a dataset named BAPData for the tasks in EFM-10. With the dataset, we successfully verify the effectiveness of the BAP strategy in an imitation learning manner. We hope that the EFM-10 benchmark along with the BAP strategy can become a cornerstone that facilitates future research towards this direction. Project website: this http URL."
    },
    {
      "title": "Do All Individual Layers Help? An Empirical Study of Task-Interfering Layers in Vision-Language Models",
      "url": "https://arxiv.org/abs/2602.01167",
      "date": "2026-02-01",
      "authors_text": "Zhiming Liu, Yujie Wei, Lei Feng, Xiu Su, Xiaobo Xia",
      "is_highlight": false,
      "score": 52.0,
      "summary": "This study identifies Task-Interfering Layers in vision-language models, demonstrating that selectively bypassing certain layers can enhance task performance without retraining.",
      "teaser_image": "https://arxiv.org/html/2602.01167/x1.png",
      "debug_abstract": "Current VLMs have demonstrated capabilities across a wide range of multimodal tasks. Typically, in a pretrained VLM, all layers are engaged by default to make predictions on downstream tasks. We find that intervening on a single layer, such as by zeroing its parameters, can improve the performance on certain tasks, indicating that some layers hinder rather than help downstream tasks. We systematically investigate how individual layers influence different tasks via layer intervention. Specifically, we measure the change in performance relative to the base model after intervening on each layer and observe improvements when bypassing specific layers. This improvement can be generalizable across models and datasets, indicating the presence of Task-Interfering Layers that harm downstream tasks&#39; performance. We introduce Task-Layer Interaction Vector, which quantifies the effect of intervening on each layer of a VLM given a task. These task-interfering layers exhibit task-specific sensitivity patterns: tasks requiring similar capabilities show consistent response trends under layer interventions, as evidenced by the high similarity in their task-layer interaction vectors. Inspired by these findings, we propose TaLo (Task-Adaptive Layer Knockout), a training-free, test-time adaptation method that dynamically identifies and bypasses the most interfering layer for a given task. Without parameter updates, TaLo improves performance across various models and datasets, including boosting Qwen-VL&#39;s accuracy on the Maps task in ScienceQA by up to 16.6%. Our work reveals an unexpected form of modularity in pretrained VLMs and provides a plug-and-play, training-free mechanism to unlock hidden capabilities at inference time. The source code will be publicly available."
    },
    {
      "title": "PolicyFlow: Policy Optimization with Continuous Normalizing Flow in Reinforcement Learning",
      "url": "https://arxiv.org/abs/2602.01156",
      "date": "2026-02-01",
      "authors_text": "Shunpeng Yang, Ben Liu, Hua Chen",
      "is_highlight": false,
      "score": 71.0,
      "summary": "PolicyFlow is a novel reinforcement learning algorithm that integrates continuous normalizing flows with PPO, enhancing performance and stability while reducing computational costs and encouraging diverse behaviors.",
      "teaser_image": "https://arxiv.org/html/2602.01156/x1.png",
      "debug_abstract": "Among on-policy reinforcement learning algorithms, Proximal Policy Optimization (PPO) demonstrates is widely favored for its simplicity, numerical stability, and strong empirical performance. Standard PPO relies on surrogate objectives defined via importance ratios, which require evaluating policy likelihood that is typically straightforward when the policy is modeled as a Gaussian distribution. However, extending PPO to more expressive, high-capacity policy models such as continuous normalizing flows (CNFs), also known as flow-matching models, is challenging because likelihood evaluation along the full flow trajectory is computationally expensive and often numerically unstable. To resolve this issue, we propose PolicyFlow, a novel on-policy CNF-based reinforcement learning algorithm that integrates expressive CNF policies with PPO-style objectives without requiring likelihood evaluation along the full flow path. PolicyFlow approximates importance ratios using velocity field variations along a simple interpolation path, reducing computational overhead without compromising training stability. To further prevent mode collapse and further encourage diverse behaviors, we propose the Brownian Regularizer, an implicit policy entropy regularizer inspired by Brownian motion, which is conceptually elegant and computationally lightweight. Experiments on diverse tasks across various environments including MultiGoal, PointMaze, IsaacLab and MuJoCo Playground show that PolicyFlow achieves competitive or superior performance compared to PPO using Gaussian policies and flow-based baselines including FPO and DPPO. Notably, results on MultiGoal highlight PolicyFlow&#39;s ability to capture richer multimodal action distributions."
    },
    {
      "title": "Invariance on Manifolds: Understanding Robust Visual Representations for Place Recognition",
      "url": "https://arxiv.org/abs/2602.00841",
      "date": "2026-01-31",
      "authors_text": "Jintao Cheng, Weibin Li, Zhijian He, Jin Wu, Chi Man Vong",
      "is_highlight": false,
      "score": 66.0,
      "summary": "This paper presents a training-free Second-Order Geometric Statistics framework for robust visual place recognition, utilizing covariance descriptors on the SPD manifold for effective zero-shot generalization.",
      "teaser_image": "https://arxiv.org/html/2602.00841/x2.png",
      "debug_abstract": "Visual Place Recognition (VPR) demands representations robust to drastic environmental and viewpoint shifts. Current aggregation paradigms, however, either rely on data-hungry supervision or simplistic first-order statistics, often neglecting intrinsic structural correlations. In this work, we propose a Second-Order Geometric Statistics framework that inherently captures geometric stability without training. We conceptualize scenes as covariance descriptors on the Symmetric Positive Definite (SPD) manifold, where perturbations manifest as tractable congruence transformations. By leveraging geometry-aware Riemannian mappings, we project these descriptors into a linearized Euclidean embedding, effectively decoupling signal structure from noise. Our approach introduces a training-free framework built upon fixed, pre-trained backbones, achieving strong zero-shot generalization without parameter updates. Extensive experiments confirm that our method achieves highly competitive performance against state-of-the-art baselines, particularly excelling in challenging zero-shot scenarios."
    },
    {
      "title": "Physics-informed Diffusion Mamba Transformer for Real-world Driving",
      "url": "https://arxiv.org/abs/2602.00808",
      "date": "2026-01-31",
      "authors_text": "Hang Zhou, Qiang Zhang, Peiran Liu, Yihao Qin, Zhaoxu Yan",
      "is_highlight": false,
      "score": 74.0,
      "summary": "The paper presents a Diffusion Mamba Transformer that integrates sequential context and physical constraints for improved trajectory planning in autonomous driving, outperforming existing models.",
      "teaser_image": "https://arxiv.org/html/2602.00808/difference_pipeline3.png",
      "debug_abstract": "Autonomous driving systems demand trajectory planners that not only model the inherent uncertainty of future motions but also respect complex temporal dependencies and underlying physical laws. While diffusion-based generative models excel at capturing multi-modal distributions, they often fail to incorporate long-term sequential contexts and domain-specific physical priors. In this work, we bridge these gaps with two key innovations. First, we introduce a Diffusion Mamba Transformer architecture that embeds mamba and attention into the diffusion process, enabling more effective aggregation of sequential input contexts from sensor streams and past motion histories. Second, we design a Port-Hamiltonian Neural Network module that seamlessly integrates energy-based physical constraints into the diffusion model, thereby enhancing trajectory predictions with both consistency and interpretability. Extensive evaluations on standard autonomous driving benchmarks demonstrate that our unified framework significantly outperforms state-of-the-art baselines in predictive accuracy, physical plausibility, and robustness, thereby advancing safe and reliable motion planning."
    },
    {
      "title": "Diffusion-Driven Inter-Outer Surface Separation for Point Clouds with Open Boundaries",
      "url": "https://arxiv.org/abs/2602.00739",
      "date": "2026-01-31",
      "authors_text": "Zhengyan Qin, Liyuan Qiu",
      "is_highlight": false,
      "score": 72.0,
      "summary": "The paper presents a diffusion-based algorithm for effectively separating inter and outer surfaces in double-layered point clouds with open boundaries, addressing artifacts from TSDF fusion.",
      "teaser_image": "https://arxiv.org/html/2602.00739/figures/pipeline5.png",
      "debug_abstract": "We propose a diffusion-based algorithm for separating the inter and outer layer surfaces from double-layered point clouds, particularly those exhibiting the &#34;double surface artifact&#34; caused by truncation in Truncated Signed Distance Function (TSDF) fusion during indoor or medical 3D reconstruction. This artifact arises from asymmetric truncation thresholds, leading to erroneous inter and outer shells in the fused volume, which our method addresses by extracting the true inter layer to mitigate challenges like overlapping surfaces and disordered normals. We focus on point clouds with \\emph{open boundaries} (i.e., sampled surfaces with topological openings/holes through which particles may escape), rather than point clouds with \\emph{missing surface regions} where no samples exist. Our approach enables robust processing of both watertight and open-boundary models, achieving extraction of the inter layer from 20,000 inter and 20,000 outer points in approximately 10 seconds. This solution is particularly effective for applications requiring accurate surface representations, such as indoor scene modeling and medical imaging, where double-layered point clouds are prevalent, and it accommodates both closed (watertight) and open-boundary surface geometries. Our goal is \\emph{post-hoc} inter/outer shell separation as a lightweight module after TSDF fusion; we do not aim to replace full variational or learning-based reconstruction pipelines."
    }
  ],
  "Precognition Lab": [
    {
      "title": "Towards Agentic Intelligence for Materials Science",
      "url": "https://arxiv.org/abs/2602.00169",
      "date": "2026-01-29",
      "authors_text": "Huan Zhang, Yizhan Li, Wenhao Huang, Ziyu Hou, Yu Song",
      "is_highlight": false,
      "score": 12.0,
      "summary": "This paper advocates for developing agentic AI systems in materials science that autonomously plan, act, and learn to enhance discovery processes beyond traditional models.",
      "teaser_image": "https://arxiv.org/html/2602.00169/x1.png",
      "debug_abstract": "The convergence of artificial intelligence and materials science presents a transformative opportunity, but achieving true acceleration in discovery requires moving beyond task-isolated, fine-tuned models toward agentic systems that plan, act, and learn across the full discovery loop. This survey advances a unique pipeline-centric view that spans from corpus curation and pretraining, through domain adaptation and instruction tuning, to goal-conditioned agents interfacing with simulation and experimental platforms. Unlike prior reviews, we treat the entire process as an end-to-end system to be optimized for tangible discovery outcomes rather than proxy benchmarks. This perspective allows us to trace how upstream design choices-such as data curation and training objectives-can be aligned with downstream experimental success through effective credit assignment. To bridge communities and establish a shared frame of reference, we first present an integrated lens that aligns terminology, evaluation, and workflow stages across AI and materials science. We then analyze the field through two focused lenses: From the AI perspective, the survey details LLM strengths in pattern recognition, predictive analytics, and natural language processing for literature mining, materials characterization, and property prediction; from the materials science perspective, it highlights applications in materials design, process optimization, and the acceleration of computational workflows via integration with external tools (e.g., DFT, robotic labs). Finally, we contrast passive, reactive approaches with agentic design, cataloging current contributions while motivating systems that pursue long-horizon goals with autonomy, memory, and tool use. This survey charts a practical roadmap towards autonomous, safety-aware LLM agents aimed at discovering novel and useful materials."
    },
    {
      "title": "Breaking the Static Graph: Context-Aware Traversal for Robust Retrieval-Augmented Generation",
      "url": "https://arxiv.org/abs/2602.01965",
      "date": "2026-02-02",
      "authors_text": "Kwun Hang Lau, Fangyuan Zhang, Boyu Ruan, Yingli Zhou, Qintian Guo",
      "is_highlight": false,
      "score": 27.0,
      "summary": "CatRAG enhances Retrieval-Augmented Generation by transforming static Knowledge Graphs into query-adaptive structures, improving multi-hop evidence retrieval and reasoning completeness.",
      "teaser_image": "https://arxiv.org/html/2602.01965/image/HubBiasShift.png",
      "debug_abstract": "Recent advances in Retrieval-Augmented Generation (RAG) have shifted from simple vector similarity to structure-aware approaches like HippoRAG, which leverage Knowledge Graphs (KGs) and Personalized PageRank (PPR) to capture multi-hop dependencies. However, these methods suffer from a &#34;Static Graph Fallacy&#34;: they rely on fixed transition probabilities determined during indexing. This rigidity ignores the query-dependent nature of edge relevance, causing semantic drift where random walks are diverted into high-degree &#34;hub&#34; nodes before reaching critical downstream evidence. Consequently, models often achieve high partial recall but fail to retrieve the complete evidence chain required for multi-hop queries. To address this, we propose CatRAG, Context-Aware Traversal for robust RAG, a framework that builds on the HippoRAG 2 architecture and transforms the static KG into a query-adaptive navigation structure. We introduce a multi-faceted framework to steer the random walk: (1) Symbolic Anchoring, which injects weak entity constraints to regularize the random walk; (2) Query-Aware Dynamic Edge Weighting, which dynamically modulates graph structure, to prune irrelevant paths while amplifying those aligned with the query&#39;s intent; and (3) Key-Fact Passage Weight Enhancement, a cost-efficient bias that structurally anchors the random walk to likely evidence. Experiments across four multi-hop benchmarks demonstrate that CatRAG consistently outperforms state of the art baselines. Our analysis reveals that while standard Recall metrics show modest gains, CatRAG achieves substantial improvements in reasoning completeness, the capacity to recover the entire evidence path without gaps. These results reveal that our approach effectively bridges the gap between retrieving partial context and enabling fully grounded reasoning. Resources are available at this https URL."
    },
    {
      "title": "Towards Exploratory and Focused Manipulation with Bimanual Active Perception: A New Problem, Benchmark and Strategy",
      "url": "https://arxiv.org/abs/2602.01939",
      "date": "2026-02-02",
      "authors_text": "Yuxin He, Ruihao Zhang, Tianao Shen, Cheng Liu, Qiang Nie",
      "is_highlight": false,
      "score": 28.0,
      "summary": "The paper introduces the Exploratory and Focused Manipulation (EFM) problem, establishes the EFM-10 benchmark, and proposes a Bimanual Active Perception strategy to enhance manipulation tasks.",
      "teaser_image": "https://arxiv.org/html/2602.01939/x2.png",
      "debug_abstract": "Recently, active vision has reemerged as an important concept for manipulation, since visual occlusion occurs more frequently when main cameras are mounted on the robot heads. We reflect on the visual occlusion issue and identify its essence as the absence of information useful for task completion. Inspired by this, we come up with the more fundamental problem of Exploratory and Focused Manipulation (EFM). The proposed problem is about actively collecting information to complete challenging manipulation tasks that require exploration or focus. As an initial attempt to address this problem, we establish the EFM-10 benchmark that consists of 4 categories of tasks that align with our definition (10 tasks in total). We further come up with a Bimanual Active Perception (BAP) strategy, which leverages one arm to provide active vision and another arm to provide force sensing while manipulating. Based on this idea, we collect a dataset named BAPData for the tasks in EFM-10. With the dataset, we successfully verify the effectiveness of the BAP strategy in an imitation learning manner. We hope that the EFM-10 benchmark along with the BAP strategy can become a cornerstone that facilitates future research towards this direction. Project website: this http URL."
    },
    {
      "title": "Do All Individual Layers Help? An Empirical Study of Task-Interfering Layers in Vision-Language Models",
      "url": "https://arxiv.org/abs/2602.01167",
      "date": "2026-02-01",
      "authors_text": "Zhiming Liu, Yujie Wei, Lei Feng, Xiu Su, Xiaobo Xia",
      "is_highlight": false,
      "score": 52.0,
      "summary": "This study identifies Task-Interfering Layers in vision-language models, demonstrating that selectively bypassing certain layers can enhance task performance without retraining.",
      "teaser_image": "https://arxiv.org/html/2602.01167/x1.png",
      "debug_abstract": "Current VLMs have demonstrated capabilities across a wide range of multimodal tasks. Typically, in a pretrained VLM, all layers are engaged by default to make predictions on downstream tasks. We find that intervening on a single layer, such as by zeroing its parameters, can improve the performance on certain tasks, indicating that some layers hinder rather than help downstream tasks. We systematically investigate how individual layers influence different tasks via layer intervention. Specifically, we measure the change in performance relative to the base model after intervening on each layer and observe improvements when bypassing specific layers. This improvement can be generalizable across models and datasets, indicating the presence of Task-Interfering Layers that harm downstream tasks&#39; performance. We introduce Task-Layer Interaction Vector, which quantifies the effect of intervening on each layer of a VLM given a task. These task-interfering layers exhibit task-specific sensitivity patterns: tasks requiring similar capabilities show consistent response trends under layer interventions, as evidenced by the high similarity in their task-layer interaction vectors. Inspired by these findings, we propose TaLo (Task-Adaptive Layer Knockout), a training-free, test-time adaptation method that dynamically identifies and bypasses the most interfering layer for a given task. Without parameter updates, TaLo improves performance across various models and datasets, including boosting Qwen-VL&#39;s accuracy on the Maps task in ScienceQA by up to 16.6%. Our work reveals an unexpected form of modularity in pretrained VLMs and provides a plug-and-play, training-free mechanism to unlock hidden capabilities at inference time. The source code will be publicly available."
    },
    {
      "title": "PolicyFlow: Policy Optimization with Continuous Normalizing Flow in Reinforcement Learning",
      "url": "https://arxiv.org/abs/2602.01156",
      "date": "2026-02-01",
      "authors_text": "Shunpeng Yang, Ben Liu, Hua Chen",
      "is_highlight": false,
      "score": 71.0,
      "summary": "PolicyFlow is a novel reinforcement learning algorithm that integrates continuous normalizing flows with PPO, enhancing performance and stability while reducing computational costs and encouraging diverse behaviors.",
      "teaser_image": "https://arxiv.org/html/2602.01156/x1.png",
      "debug_abstract": "Among on-policy reinforcement learning algorithms, Proximal Policy Optimization (PPO) demonstrates is widely favored for its simplicity, numerical stability, and strong empirical performance. Standard PPO relies on surrogate objectives defined via importance ratios, which require evaluating policy likelihood that is typically straightforward when the policy is modeled as a Gaussian distribution. However, extending PPO to more expressive, high-capacity policy models such as continuous normalizing flows (CNFs), also known as flow-matching models, is challenging because likelihood evaluation along the full flow trajectory is computationally expensive and often numerically unstable. To resolve this issue, we propose PolicyFlow, a novel on-policy CNF-based reinforcement learning algorithm that integrates expressive CNF policies with PPO-style objectives without requiring likelihood evaluation along the full flow path. PolicyFlow approximates importance ratios using velocity field variations along a simple interpolation path, reducing computational overhead without compromising training stability. To further prevent mode collapse and further encourage diverse behaviors, we propose the Brownian Regularizer, an implicit policy entropy regularizer inspired by Brownian motion, which is conceptually elegant and computationally lightweight. Experiments on diverse tasks across various environments including MultiGoal, PointMaze, IsaacLab and MuJoCo Playground show that PolicyFlow achieves competitive or superior performance compared to PPO using Gaussian policies and flow-based baselines including FPO and DPPO. Notably, results on MultiGoal highlight PolicyFlow&#39;s ability to capture richer multimodal action distributions."
    },
    {
      "title": "Invariance on Manifolds: Understanding Robust Visual Representations for Place Recognition",
      "url": "https://arxiv.org/abs/2602.00841",
      "date": "2026-01-31",
      "authors_text": "Jintao Cheng, Weibin Li, Zhijian He, Jin Wu, Chi Man Vong",
      "is_highlight": false,
      "score": 66.0,
      "summary": "This paper presents a training-free Second-Order Geometric Statistics framework for robust visual place recognition, utilizing covariance descriptors on the SPD manifold for effective zero-shot generalization.",
      "teaser_image": "https://arxiv.org/html/2602.00841/x2.png",
      "debug_abstract": "Visual Place Recognition (VPR) demands representations robust to drastic environmental and viewpoint shifts. Current aggregation paradigms, however, either rely on data-hungry supervision or simplistic first-order statistics, often neglecting intrinsic structural correlations. In this work, we propose a Second-Order Geometric Statistics framework that inherently captures geometric stability without training. We conceptualize scenes as covariance descriptors on the Symmetric Positive Definite (SPD) manifold, where perturbations manifest as tractable congruence transformations. By leveraging geometry-aware Riemannian mappings, we project these descriptors into a linearized Euclidean embedding, effectively decoupling signal structure from noise. Our approach introduces a training-free framework built upon fixed, pre-trained backbones, achieving strong zero-shot generalization without parameter updates. Extensive experiments confirm that our method achieves highly competitive performance against state-of-the-art baselines, particularly excelling in challenging zero-shot scenarios."
    },
    {
      "title": "Physics-informed Diffusion Mamba Transformer for Real-world Driving",
      "url": "https://arxiv.org/abs/2602.00808",
      "date": "2026-01-31",
      "authors_text": "Hang Zhou, Qiang Zhang, Peiran Liu, Yihao Qin, Zhaoxu Yan",
      "is_highlight": false,
      "score": 74.0,
      "summary": "The paper presents a Diffusion Mamba Transformer that integrates sequential context and physical constraints for improved trajectory planning in autonomous driving, outperforming existing models.",
      "teaser_image": "https://arxiv.org/html/2602.00808/difference_pipeline3.png",
      "debug_abstract": "Autonomous driving systems demand trajectory planners that not only model the inherent uncertainty of future motions but also respect complex temporal dependencies and underlying physical laws. While diffusion-based generative models excel at capturing multi-modal distributions, they often fail to incorporate long-term sequential contexts and domain-specific physical priors. In this work, we bridge these gaps with two key innovations. First, we introduce a Diffusion Mamba Transformer architecture that embeds mamba and attention into the diffusion process, enabling more effective aggregation of sequential input contexts from sensor streams and past motion histories. Second, we design a Port-Hamiltonian Neural Network module that seamlessly integrates energy-based physical constraints into the diffusion model, thereby enhancing trajectory predictions with both consistency and interpretability. Extensive evaluations on standard autonomous driving benchmarks demonstrate that our unified framework significantly outperforms state-of-the-art baselines in predictive accuracy, physical plausibility, and robustness, thereby advancing safe and reliable motion planning."
    },
    {
      "title": "Diffusion-Driven Inter-Outer Surface Separation for Point Clouds with Open Boundaries",
      "url": "https://arxiv.org/abs/2602.00739",
      "date": "2026-01-31",
      "authors_text": "Zhengyan Qin, Liyuan Qiu",
      "is_highlight": false,
      "score": 72.0,
      "summary": "The paper presents a diffusion-based algorithm for effectively separating inter and outer surfaces in double-layered point clouds with open boundaries, addressing artifacts from TSDF fusion.",
      "teaser_image": "https://arxiv.org/html/2602.00739/figures/pipeline5.png",
      "debug_abstract": "We propose a diffusion-based algorithm for separating the inter and outer layer surfaces from double-layered point clouds, particularly those exhibiting the &#34;double surface artifact&#34; caused by truncation in Truncated Signed Distance Function (TSDF) fusion during indoor or medical 3D reconstruction. This artifact arises from asymmetric truncation thresholds, leading to erroneous inter and outer shells in the fused volume, which our method addresses by extracting the true inter layer to mitigate challenges like overlapping surfaces and disordered normals. We focus on point clouds with \\emph{open boundaries} (i.e., sampled surfaces with topological openings/holes through which particles may escape), rather than point clouds with \\emph{missing surface regions} where no samples exist. Our approach enables robust processing of both watertight and open-boundary models, achieving extraction of the inter layer from 20,000 inter and 20,000 outer points in approximately 10 seconds. This solution is particularly effective for applications requiring accurate surface representations, such as indoor scene modeling and medical imaging, where double-layered point clouds are prevalent, and it accommodates both closed (watertight) and open-boundary surface geometries. Our goal is \\emph{post-hoc} inter/outer shell separation as a lightweight module after TSDF fusion; we do not aim to replace full variational or learning-based reconstruction pipelines."
    }
  ],
  "Jun MA老师实验室": [
    {
      "title": "Towards Agentic Intelligence for Materials Science",
      "url": "https://arxiv.org/abs/2602.00169",
      "date": "2026-01-29",
      "authors_text": "Huan Zhang, Yizhan Li, Wenhao Huang, Ziyu Hou, Yu Song",
      "is_highlight": false,
      "score": 12.0,
      "summary": "This paper advocates for developing agentic AI systems in materials science that autonomously plan, act, and learn to enhance discovery processes beyond traditional models.",
      "teaser_image": "https://arxiv.org/html/2602.00169/x1.png",
      "debug_abstract": "The convergence of artificial intelligence and materials science presents a transformative opportunity, but achieving true acceleration in discovery requires moving beyond task-isolated, fine-tuned models toward agentic systems that plan, act, and learn across the full discovery loop. This survey advances a unique pipeline-centric view that spans from corpus curation and pretraining, through domain adaptation and instruction tuning, to goal-conditioned agents interfacing with simulation and experimental platforms. Unlike prior reviews, we treat the entire process as an end-to-end system to be optimized for tangible discovery outcomes rather than proxy benchmarks. This perspective allows us to trace how upstream design choices-such as data curation and training objectives-can be aligned with downstream experimental success through effective credit assignment. To bridge communities and establish a shared frame of reference, we first present an integrated lens that aligns terminology, evaluation, and workflow stages across AI and materials science. We then analyze the field through two focused lenses: From the AI perspective, the survey details LLM strengths in pattern recognition, predictive analytics, and natural language processing for literature mining, materials characterization, and property prediction; from the materials science perspective, it highlights applications in materials design, process optimization, and the acceleration of computational workflows via integration with external tools (e.g., DFT, robotic labs). Finally, we contrast passive, reactive approaches with agentic design, cataloging current contributions while motivating systems that pursue long-horizon goals with autonomy, memory, and tool use. This survey charts a practical roadmap towards autonomous, safety-aware LLM agents aimed at discovering novel and useful materials."
    },
    {
      "title": "Breaking the Static Graph: Context-Aware Traversal for Robust Retrieval-Augmented Generation",
      "url": "https://arxiv.org/abs/2602.01965",
      "date": "2026-02-02",
      "authors_text": "Kwun Hang Lau, Fangyuan Zhang, Boyu Ruan, Yingli Zhou, Qintian Guo",
      "is_highlight": false,
      "score": 27.0,
      "summary": "CatRAG enhances Retrieval-Augmented Generation by transforming static Knowledge Graphs into query-adaptive structures, improving multi-hop evidence retrieval and reasoning completeness.",
      "teaser_image": "https://arxiv.org/html/2602.01965/image/HubBiasShift.png",
      "debug_abstract": "Recent advances in Retrieval-Augmented Generation (RAG) have shifted from simple vector similarity to structure-aware approaches like HippoRAG, which leverage Knowledge Graphs (KGs) and Personalized PageRank (PPR) to capture multi-hop dependencies. However, these methods suffer from a &#34;Static Graph Fallacy&#34;: they rely on fixed transition probabilities determined during indexing. This rigidity ignores the query-dependent nature of edge relevance, causing semantic drift where random walks are diverted into high-degree &#34;hub&#34; nodes before reaching critical downstream evidence. Consequently, models often achieve high partial recall but fail to retrieve the complete evidence chain required for multi-hop queries. To address this, we propose CatRAG, Context-Aware Traversal for robust RAG, a framework that builds on the HippoRAG 2 architecture and transforms the static KG into a query-adaptive navigation structure. We introduce a multi-faceted framework to steer the random walk: (1) Symbolic Anchoring, which injects weak entity constraints to regularize the random walk; (2) Query-Aware Dynamic Edge Weighting, which dynamically modulates graph structure, to prune irrelevant paths while amplifying those aligned with the query&#39;s intent; and (3) Key-Fact Passage Weight Enhancement, a cost-efficient bias that structurally anchors the random walk to likely evidence. Experiments across four multi-hop benchmarks demonstrate that CatRAG consistently outperforms state of the art baselines. Our analysis reveals that while standard Recall metrics show modest gains, CatRAG achieves substantial improvements in reasoning completeness, the capacity to recover the entire evidence path without gaps. These results reveal that our approach effectively bridges the gap between retrieving partial context and enabling fully grounded reasoning. Resources are available at this https URL."
    },
    {
      "title": "Towards Exploratory and Focused Manipulation with Bimanual Active Perception: A New Problem, Benchmark and Strategy",
      "url": "https://arxiv.org/abs/2602.01939",
      "date": "2026-02-02",
      "authors_text": "Yuxin He, Ruihao Zhang, Tianao Shen, Cheng Liu, Qiang Nie",
      "is_highlight": false,
      "score": 28.0,
      "summary": "The paper introduces the Exploratory and Focused Manipulation (EFM) problem, establishes the EFM-10 benchmark, and proposes a Bimanual Active Perception strategy to enhance manipulation tasks.",
      "teaser_image": "https://arxiv.org/html/2602.01939/x2.png",
      "debug_abstract": "Recently, active vision has reemerged as an important concept for manipulation, since visual occlusion occurs more frequently when main cameras are mounted on the robot heads. We reflect on the visual occlusion issue and identify its essence as the absence of information useful for task completion. Inspired by this, we come up with the more fundamental problem of Exploratory and Focused Manipulation (EFM). The proposed problem is about actively collecting information to complete challenging manipulation tasks that require exploration or focus. As an initial attempt to address this problem, we establish the EFM-10 benchmark that consists of 4 categories of tasks that align with our definition (10 tasks in total). We further come up with a Bimanual Active Perception (BAP) strategy, which leverages one arm to provide active vision and another arm to provide force sensing while manipulating. Based on this idea, we collect a dataset named BAPData for the tasks in EFM-10. With the dataset, we successfully verify the effectiveness of the BAP strategy in an imitation learning manner. We hope that the EFM-10 benchmark along with the BAP strategy can become a cornerstone that facilitates future research towards this direction. Project website: this http URL."
    },
    {
      "title": "Do All Individual Layers Help? An Empirical Study of Task-Interfering Layers in Vision-Language Models",
      "url": "https://arxiv.org/abs/2602.01167",
      "date": "2026-02-01",
      "authors_text": "Zhiming Liu, Yujie Wei, Lei Feng, Xiu Su, Xiaobo Xia",
      "is_highlight": false,
      "score": 52.0,
      "summary": "This study identifies Task-Interfering Layers in vision-language models, demonstrating that selectively bypassing certain layers can enhance task performance without retraining.",
      "teaser_image": "https://arxiv.org/html/2602.01167/x1.png",
      "debug_abstract": "Current VLMs have demonstrated capabilities across a wide range of multimodal tasks. Typically, in a pretrained VLM, all layers are engaged by default to make predictions on downstream tasks. We find that intervening on a single layer, such as by zeroing its parameters, can improve the performance on certain tasks, indicating that some layers hinder rather than help downstream tasks. We systematically investigate how individual layers influence different tasks via layer intervention. Specifically, we measure the change in performance relative to the base model after intervening on each layer and observe improvements when bypassing specific layers. This improvement can be generalizable across models and datasets, indicating the presence of Task-Interfering Layers that harm downstream tasks&#39; performance. We introduce Task-Layer Interaction Vector, which quantifies the effect of intervening on each layer of a VLM given a task. These task-interfering layers exhibit task-specific sensitivity patterns: tasks requiring similar capabilities show consistent response trends under layer interventions, as evidenced by the high similarity in their task-layer interaction vectors. Inspired by these findings, we propose TaLo (Task-Adaptive Layer Knockout), a training-free, test-time adaptation method that dynamically identifies and bypasses the most interfering layer for a given task. Without parameter updates, TaLo improves performance across various models and datasets, including boosting Qwen-VL&#39;s accuracy on the Maps task in ScienceQA by up to 16.6%. Our work reveals an unexpected form of modularity in pretrained VLMs and provides a plug-and-play, training-free mechanism to unlock hidden capabilities at inference time. The source code will be publicly available."
    },
    {
      "title": "PolicyFlow: Policy Optimization with Continuous Normalizing Flow in Reinforcement Learning",
      "url": "https://arxiv.org/abs/2602.01156",
      "date": "2026-02-01",
      "authors_text": "Shunpeng Yang, Ben Liu, Hua Chen",
      "is_highlight": false,
      "score": 71.0,
      "summary": "PolicyFlow is a novel reinforcement learning algorithm that integrates continuous normalizing flows with PPO, enhancing performance and stability while reducing computational costs and encouraging diverse behaviors.",
      "teaser_image": "https://arxiv.org/html/2602.01156/x1.png",
      "debug_abstract": "Among on-policy reinforcement learning algorithms, Proximal Policy Optimization (PPO) demonstrates is widely favored for its simplicity, numerical stability, and strong empirical performance. Standard PPO relies on surrogate objectives defined via importance ratios, which require evaluating policy likelihood that is typically straightforward when the policy is modeled as a Gaussian distribution. However, extending PPO to more expressive, high-capacity policy models such as continuous normalizing flows (CNFs), also known as flow-matching models, is challenging because likelihood evaluation along the full flow trajectory is computationally expensive and often numerically unstable. To resolve this issue, we propose PolicyFlow, a novel on-policy CNF-based reinforcement learning algorithm that integrates expressive CNF policies with PPO-style objectives without requiring likelihood evaluation along the full flow path. PolicyFlow approximates importance ratios using velocity field variations along a simple interpolation path, reducing computational overhead without compromising training stability. To further prevent mode collapse and further encourage diverse behaviors, we propose the Brownian Regularizer, an implicit policy entropy regularizer inspired by Brownian motion, which is conceptually elegant and computationally lightweight. Experiments on diverse tasks across various environments including MultiGoal, PointMaze, IsaacLab and MuJoCo Playground show that PolicyFlow achieves competitive or superior performance compared to PPO using Gaussian policies and flow-based baselines including FPO and DPPO. Notably, results on MultiGoal highlight PolicyFlow&#39;s ability to capture richer multimodal action distributions."
    },
    {
      "title": "Invariance on Manifolds: Understanding Robust Visual Representations for Place Recognition",
      "url": "https://arxiv.org/abs/2602.00841",
      "date": "2026-01-31",
      "authors_text": "Jintao Cheng, Weibin Li, Zhijian He, Jin Wu, Chi Man Vong",
      "is_highlight": false,
      "score": 66.0,
      "summary": "This paper presents a training-free Second-Order Geometric Statistics framework for robust visual place recognition, utilizing covariance descriptors on the SPD manifold for effective zero-shot generalization.",
      "teaser_image": "https://arxiv.org/html/2602.00841/x2.png",
      "debug_abstract": "Visual Place Recognition (VPR) demands representations robust to drastic environmental and viewpoint shifts. Current aggregation paradigms, however, either rely on data-hungry supervision or simplistic first-order statistics, often neglecting intrinsic structural correlations. In this work, we propose a Second-Order Geometric Statistics framework that inherently captures geometric stability without training. We conceptualize scenes as covariance descriptors on the Symmetric Positive Definite (SPD) manifold, where perturbations manifest as tractable congruence transformations. By leveraging geometry-aware Riemannian mappings, we project these descriptors into a linearized Euclidean embedding, effectively decoupling signal structure from noise. Our approach introduces a training-free framework built upon fixed, pre-trained backbones, achieving strong zero-shot generalization without parameter updates. Extensive experiments confirm that our method achieves highly competitive performance against state-of-the-art baselines, particularly excelling in challenging zero-shot scenarios."
    },
    {
      "title": "Physics-informed Diffusion Mamba Transformer for Real-world Driving",
      "url": "https://arxiv.org/abs/2602.00808",
      "date": "2026-01-31",
      "authors_text": "Hang Zhou, Qiang Zhang, Peiran Liu, Yihao Qin, Zhaoxu Yan",
      "is_highlight": false,
      "score": 74.0,
      "summary": "The paper presents a Diffusion Mamba Transformer that integrates sequential context and physical constraints for improved trajectory planning in autonomous driving, outperforming existing models.",
      "teaser_image": "https://arxiv.org/html/2602.00808/difference_pipeline3.png",
      "debug_abstract": "Autonomous driving systems demand trajectory planners that not only model the inherent uncertainty of future motions but also respect complex temporal dependencies and underlying physical laws. While diffusion-based generative models excel at capturing multi-modal distributions, they often fail to incorporate long-term sequential contexts and domain-specific physical priors. In this work, we bridge these gaps with two key innovations. First, we introduce a Diffusion Mamba Transformer architecture that embeds mamba and attention into the diffusion process, enabling more effective aggregation of sequential input contexts from sensor streams and past motion histories. Second, we design a Port-Hamiltonian Neural Network module that seamlessly integrates energy-based physical constraints into the diffusion model, thereby enhancing trajectory predictions with both consistency and interpretability. Extensive evaluations on standard autonomous driving benchmarks demonstrate that our unified framework significantly outperforms state-of-the-art baselines in predictive accuracy, physical plausibility, and robustness, thereby advancing safe and reliable motion planning."
    },
    {
      "title": "Diffusion-Driven Inter-Outer Surface Separation for Point Clouds with Open Boundaries",
      "url": "https://arxiv.org/abs/2602.00739",
      "date": "2026-01-31",
      "authors_text": "Zhengyan Qin, Liyuan Qiu",
      "is_highlight": false,
      "score": 72.0,
      "summary": "The paper presents a diffusion-based algorithm for effectively separating inter and outer surfaces in double-layered point clouds with open boundaries, addressing artifacts from TSDF fusion.",
      "teaser_image": "https://arxiv.org/html/2602.00739/figures/pipeline5.png",
      "debug_abstract": "We propose a diffusion-based algorithm for separating the inter and outer layer surfaces from double-layered point clouds, particularly those exhibiting the &#34;double surface artifact&#34; caused by truncation in Truncated Signed Distance Function (TSDF) fusion during indoor or medical 3D reconstruction. This artifact arises from asymmetric truncation thresholds, leading to erroneous inter and outer shells in the fused volume, which our method addresses by extracting the true inter layer to mitigate challenges like overlapping surfaces and disordered normals. We focus on point clouds with \\emph{open boundaries} (i.e., sampled surfaces with topological openings/holes through which particles may escape), rather than point clouds with \\emph{missing surface regions} where no samples exist. Our approach enables robust processing of both watertight and open-boundary models, achieving extraction of the inter layer from 20,000 inter and 20,000 outer points in approximately 10 seconds. This solution is particularly effective for applications requiring accurate surface representations, such as indoor scene modeling and medical imaging, where double-layered point clouds are prevalent, and it accommodates both closed (watertight) and open-boundary surface geometries. Our goal is \\emph{post-hoc} inter/outer shell separation as a lightweight module after TSDF fusion; we do not aim to replace full variational or learning-based reconstruction pipelines."
    }
  ],
  "Cheng Kar-Shun Robotics Institute (CKSRI)": [
    {
      "title": "Towards Agentic Intelligence for Materials Science",
      "url": "https://arxiv.org/abs/2602.00169",
      "date": "2026-01-29",
      "authors_text": "Huan Zhang, Yizhan Li, Wenhao Huang, Ziyu Hou, Yu Song",
      "is_highlight": false,
      "score": 12.0,
      "summary": "This paper advocates for developing agentic AI systems in materials science that autonomously plan, act, and learn to enhance discovery processes beyond traditional models.",
      "teaser_image": "https://arxiv.org/html/2602.00169/x1.png",
      "debug_abstract": "The convergence of artificial intelligence and materials science presents a transformative opportunity, but achieving true acceleration in discovery requires moving beyond task-isolated, fine-tuned models toward agentic systems that plan, act, and learn across the full discovery loop. This survey advances a unique pipeline-centric view that spans from corpus curation and pretraining, through domain adaptation and instruction tuning, to goal-conditioned agents interfacing with simulation and experimental platforms. Unlike prior reviews, we treat the entire process as an end-to-end system to be optimized for tangible discovery outcomes rather than proxy benchmarks. This perspective allows us to trace how upstream design choices-such as data curation and training objectives-can be aligned with downstream experimental success through effective credit assignment. To bridge communities and establish a shared frame of reference, we first present an integrated lens that aligns terminology, evaluation, and workflow stages across AI and materials science. We then analyze the field through two focused lenses: From the AI perspective, the survey details LLM strengths in pattern recognition, predictive analytics, and natural language processing for literature mining, materials characterization, and property prediction; from the materials science perspective, it highlights applications in materials design, process optimization, and the acceleration of computational workflows via integration with external tools (e.g., DFT, robotic labs). Finally, we contrast passive, reactive approaches with agentic design, cataloging current contributions while motivating systems that pursue long-horizon goals with autonomy, memory, and tool use. This survey charts a practical roadmap towards autonomous, safety-aware LLM agents aimed at discovering novel and useful materials."
    },
    {
      "title": "Breaking the Static Graph: Context-Aware Traversal for Robust Retrieval-Augmented Generation",
      "url": "https://arxiv.org/abs/2602.01965",
      "date": "2026-02-02",
      "authors_text": "Kwun Hang Lau, Fangyuan Zhang, Boyu Ruan, Yingli Zhou, Qintian Guo",
      "is_highlight": false,
      "score": 27.0,
      "summary": "CatRAG enhances Retrieval-Augmented Generation by transforming static Knowledge Graphs into query-adaptive structures, improving multi-hop evidence retrieval and reasoning completeness.",
      "teaser_image": "https://arxiv.org/html/2602.01965/image/HubBiasShift.png",
      "debug_abstract": "Recent advances in Retrieval-Augmented Generation (RAG) have shifted from simple vector similarity to structure-aware approaches like HippoRAG, which leverage Knowledge Graphs (KGs) and Personalized PageRank (PPR) to capture multi-hop dependencies. However, these methods suffer from a &#34;Static Graph Fallacy&#34;: they rely on fixed transition probabilities determined during indexing. This rigidity ignores the query-dependent nature of edge relevance, causing semantic drift where random walks are diverted into high-degree &#34;hub&#34; nodes before reaching critical downstream evidence. Consequently, models often achieve high partial recall but fail to retrieve the complete evidence chain required for multi-hop queries. To address this, we propose CatRAG, Context-Aware Traversal for robust RAG, a framework that builds on the HippoRAG 2 architecture and transforms the static KG into a query-adaptive navigation structure. We introduce a multi-faceted framework to steer the random walk: (1) Symbolic Anchoring, which injects weak entity constraints to regularize the random walk; (2) Query-Aware Dynamic Edge Weighting, which dynamically modulates graph structure, to prune irrelevant paths while amplifying those aligned with the query&#39;s intent; and (3) Key-Fact Passage Weight Enhancement, a cost-efficient bias that structurally anchors the random walk to likely evidence. Experiments across four multi-hop benchmarks demonstrate that CatRAG consistently outperforms state of the art baselines. Our analysis reveals that while standard Recall metrics show modest gains, CatRAG achieves substantial improvements in reasoning completeness, the capacity to recover the entire evidence path without gaps. These results reveal that our approach effectively bridges the gap between retrieving partial context and enabling fully grounded reasoning. Resources are available at this https URL."
    },
    {
      "title": "Towards Exploratory and Focused Manipulation with Bimanual Active Perception: A New Problem, Benchmark and Strategy",
      "url": "https://arxiv.org/abs/2602.01939",
      "date": "2026-02-02",
      "authors_text": "Yuxin He, Ruihao Zhang, Tianao Shen, Cheng Liu, Qiang Nie",
      "is_highlight": false,
      "score": 28.0,
      "summary": "The paper introduces the Exploratory and Focused Manipulation (EFM) problem, establishes the EFM-10 benchmark, and proposes a Bimanual Active Perception strategy to enhance manipulation tasks.",
      "teaser_image": "https://arxiv.org/html/2602.01939/x2.png",
      "debug_abstract": "Recently, active vision has reemerged as an important concept for manipulation, since visual occlusion occurs more frequently when main cameras are mounted on the robot heads. We reflect on the visual occlusion issue and identify its essence as the absence of information useful for task completion. Inspired by this, we come up with the more fundamental problem of Exploratory and Focused Manipulation (EFM). The proposed problem is about actively collecting information to complete challenging manipulation tasks that require exploration or focus. As an initial attempt to address this problem, we establish the EFM-10 benchmark that consists of 4 categories of tasks that align with our definition (10 tasks in total). We further come up with a Bimanual Active Perception (BAP) strategy, which leverages one arm to provide active vision and another arm to provide force sensing while manipulating. Based on this idea, we collect a dataset named BAPData for the tasks in EFM-10. With the dataset, we successfully verify the effectiveness of the BAP strategy in an imitation learning manner. We hope that the EFM-10 benchmark along with the BAP strategy can become a cornerstone that facilitates future research towards this direction. Project website: this http URL."
    },
    {
      "title": "Do All Individual Layers Help? An Empirical Study of Task-Interfering Layers in Vision-Language Models",
      "url": "https://arxiv.org/abs/2602.01167",
      "date": "2026-02-01",
      "authors_text": "Zhiming Liu, Yujie Wei, Lei Feng, Xiu Su, Xiaobo Xia",
      "is_highlight": false,
      "score": 52.0,
      "summary": "This study identifies Task-Interfering Layers in vision-language models, demonstrating that selectively bypassing certain layers can enhance task performance without retraining.",
      "teaser_image": "https://arxiv.org/html/2602.01167/x1.png",
      "debug_abstract": "Current VLMs have demonstrated capabilities across a wide range of multimodal tasks. Typically, in a pretrained VLM, all layers are engaged by default to make predictions on downstream tasks. We find that intervening on a single layer, such as by zeroing its parameters, can improve the performance on certain tasks, indicating that some layers hinder rather than help downstream tasks. We systematically investigate how individual layers influence different tasks via layer intervention. Specifically, we measure the change in performance relative to the base model after intervening on each layer and observe improvements when bypassing specific layers. This improvement can be generalizable across models and datasets, indicating the presence of Task-Interfering Layers that harm downstream tasks&#39; performance. We introduce Task-Layer Interaction Vector, which quantifies the effect of intervening on each layer of a VLM given a task. These task-interfering layers exhibit task-specific sensitivity patterns: tasks requiring similar capabilities show consistent response trends under layer interventions, as evidenced by the high similarity in their task-layer interaction vectors. Inspired by these findings, we propose TaLo (Task-Adaptive Layer Knockout), a training-free, test-time adaptation method that dynamically identifies and bypasses the most interfering layer for a given task. Without parameter updates, TaLo improves performance across various models and datasets, including boosting Qwen-VL&#39;s accuracy on the Maps task in ScienceQA by up to 16.6%. Our work reveals an unexpected form of modularity in pretrained VLMs and provides a plug-and-play, training-free mechanism to unlock hidden capabilities at inference time. The source code will be publicly available."
    },
    {
      "title": "PolicyFlow: Policy Optimization with Continuous Normalizing Flow in Reinforcement Learning",
      "url": "https://arxiv.org/abs/2602.01156",
      "date": "2026-02-01",
      "authors_text": "Shunpeng Yang, Ben Liu, Hua Chen",
      "is_highlight": false,
      "score": 71.0,
      "summary": "PolicyFlow is a novel reinforcement learning algorithm that integrates continuous normalizing flows with PPO, enhancing performance and stability while reducing computational costs and encouraging diverse behaviors.",
      "teaser_image": "https://arxiv.org/html/2602.01156/x1.png",
      "debug_abstract": "Among on-policy reinforcement learning algorithms, Proximal Policy Optimization (PPO) demonstrates is widely favored for its simplicity, numerical stability, and strong empirical performance. Standard PPO relies on surrogate objectives defined via importance ratios, which require evaluating policy likelihood that is typically straightforward when the policy is modeled as a Gaussian distribution. However, extending PPO to more expressive, high-capacity policy models such as continuous normalizing flows (CNFs), also known as flow-matching models, is challenging because likelihood evaluation along the full flow trajectory is computationally expensive and often numerically unstable. To resolve this issue, we propose PolicyFlow, a novel on-policy CNF-based reinforcement learning algorithm that integrates expressive CNF policies with PPO-style objectives without requiring likelihood evaluation along the full flow path. PolicyFlow approximates importance ratios using velocity field variations along a simple interpolation path, reducing computational overhead without compromising training stability. To further prevent mode collapse and further encourage diverse behaviors, we propose the Brownian Regularizer, an implicit policy entropy regularizer inspired by Brownian motion, which is conceptually elegant and computationally lightweight. Experiments on diverse tasks across various environments including MultiGoal, PointMaze, IsaacLab and MuJoCo Playground show that PolicyFlow achieves competitive or superior performance compared to PPO using Gaussian policies and flow-based baselines including FPO and DPPO. Notably, results on MultiGoal highlight PolicyFlow&#39;s ability to capture richer multimodal action distributions."
    },
    {
      "title": "Invariance on Manifolds: Understanding Robust Visual Representations for Place Recognition",
      "url": "https://arxiv.org/abs/2602.00841",
      "date": "2026-01-31",
      "authors_text": "Jintao Cheng, Weibin Li, Zhijian He, Jin Wu, Chi Man Vong",
      "is_highlight": false,
      "score": 66.0,
      "summary": "This paper presents a training-free Second-Order Geometric Statistics framework for robust visual place recognition, utilizing covariance descriptors on the SPD manifold for effective zero-shot generalization.",
      "teaser_image": "https://arxiv.org/html/2602.00841/x2.png",
      "debug_abstract": "Visual Place Recognition (VPR) demands representations robust to drastic environmental and viewpoint shifts. Current aggregation paradigms, however, either rely on data-hungry supervision or simplistic first-order statistics, often neglecting intrinsic structural correlations. In this work, we propose a Second-Order Geometric Statistics framework that inherently captures geometric stability without training. We conceptualize scenes as covariance descriptors on the Symmetric Positive Definite (SPD) manifold, where perturbations manifest as tractable congruence transformations. By leveraging geometry-aware Riemannian mappings, we project these descriptors into a linearized Euclidean embedding, effectively decoupling signal structure from noise. Our approach introduces a training-free framework built upon fixed, pre-trained backbones, achieving strong zero-shot generalization without parameter updates. Extensive experiments confirm that our method achieves highly competitive performance against state-of-the-art baselines, particularly excelling in challenging zero-shot scenarios."
    },
    {
      "title": "Physics-informed Diffusion Mamba Transformer for Real-world Driving",
      "url": "https://arxiv.org/abs/2602.00808",
      "date": "2026-01-31",
      "authors_text": "Hang Zhou, Qiang Zhang, Peiran Liu, Yihao Qin, Zhaoxu Yan",
      "is_highlight": false,
      "score": 74.0,
      "summary": "The paper presents a Diffusion Mamba Transformer that integrates sequential context and physical constraints for improved trajectory planning in autonomous driving, outperforming existing models.",
      "teaser_image": "https://arxiv.org/html/2602.00808/difference_pipeline3.png",
      "debug_abstract": "Autonomous driving systems demand trajectory planners that not only model the inherent uncertainty of future motions but also respect complex temporal dependencies and underlying physical laws. While diffusion-based generative models excel at capturing multi-modal distributions, they often fail to incorporate long-term sequential contexts and domain-specific physical priors. In this work, we bridge these gaps with two key innovations. First, we introduce a Diffusion Mamba Transformer architecture that embeds mamba and attention into the diffusion process, enabling more effective aggregation of sequential input contexts from sensor streams and past motion histories. Second, we design a Port-Hamiltonian Neural Network module that seamlessly integrates energy-based physical constraints into the diffusion model, thereby enhancing trajectory predictions with both consistency and interpretability. Extensive evaluations on standard autonomous driving benchmarks demonstrate that our unified framework significantly outperforms state-of-the-art baselines in predictive accuracy, physical plausibility, and robustness, thereby advancing safe and reliable motion planning."
    },
    {
      "title": "Diffusion-Driven Inter-Outer Surface Separation for Point Clouds with Open Boundaries",
      "url": "https://arxiv.org/abs/2602.00739",
      "date": "2026-01-31",
      "authors_text": "Zhengyan Qin, Liyuan Qiu",
      "is_highlight": false,
      "score": 72.0,
      "summary": "The paper presents a diffusion-based algorithm for effectively separating inter and outer surfaces in double-layered point clouds with open boundaries, addressing artifacts from TSDF fusion.",
      "teaser_image": "https://arxiv.org/html/2602.00739/figures/pipeline5.png",
      "debug_abstract": "We propose a diffusion-based algorithm for separating the inter and outer layer surfaces from double-layered point clouds, particularly those exhibiting the &#34;double surface artifact&#34; caused by truncation in Truncated Signed Distance Function (TSDF) fusion during indoor or medical 3D reconstruction. This artifact arises from asymmetric truncation thresholds, leading to erroneous inter and outer shells in the fused volume, which our method addresses by extracting the true inter layer to mitigate challenges like overlapping surfaces and disordered normals. We focus on point clouds with \\emph{open boundaries} (i.e., sampled surfaces with topological openings/holes through which particles may escape), rather than point clouds with \\emph{missing surface regions} where no samples exist. Our approach enables robust processing of both watertight and open-boundary models, achieving extraction of the inter layer from 20,000 inter and 20,000 outer points in approximately 10 seconds. This solution is particularly effective for applications requiring accurate surface representations, such as indoor scene modeling and medical imaging, where double-layered point clouds are prevalent, and it accommodates both closed (watertight) and open-boundary surface geometries. Our goal is \\emph{post-hoc} inter/outer shell separation as a lightweight module after TSDF fusion; we do not aim to replace full variational or learning-based reconstruction pipelines."
    }
  ],
  "机器人研究所 (CKSRI)": [
    {
      "title": "Towards Agentic Intelligence for Materials Science",
      "url": "https://arxiv.org/abs/2602.00169",
      "date": "2026-01-29",
      "authors_text": "Huan Zhang, Yizhan Li, Wenhao Huang, Ziyu Hou, Yu Song",
      "is_highlight": false,
      "score": 12.0,
      "summary": "This paper advocates for developing agentic AI systems in materials science that autonomously plan, act, and learn to enhance discovery processes beyond traditional models.",
      "teaser_image": "https://arxiv.org/html/2602.00169/x1.png",
      "debug_abstract": "The convergence of artificial intelligence and materials science presents a transformative opportunity, but achieving true acceleration in discovery requires moving beyond task-isolated, fine-tuned models toward agentic systems that plan, act, and learn across the full discovery loop. This survey advances a unique pipeline-centric view that spans from corpus curation and pretraining, through domain adaptation and instruction tuning, to goal-conditioned agents interfacing with simulation and experimental platforms. Unlike prior reviews, we treat the entire process as an end-to-end system to be optimized for tangible discovery outcomes rather than proxy benchmarks. This perspective allows us to trace how upstream design choices-such as data curation and training objectives-can be aligned with downstream experimental success through effective credit assignment. To bridge communities and establish a shared frame of reference, we first present an integrated lens that aligns terminology, evaluation, and workflow stages across AI and materials science. We then analyze the field through two focused lenses: From the AI perspective, the survey details LLM strengths in pattern recognition, predictive analytics, and natural language processing for literature mining, materials characterization, and property prediction; from the materials science perspective, it highlights applications in materials design, process optimization, and the acceleration of computational workflows via integration with external tools (e.g., DFT, robotic labs). Finally, we contrast passive, reactive approaches with agentic design, cataloging current contributions while motivating systems that pursue long-horizon goals with autonomy, memory, and tool use. This survey charts a practical roadmap towards autonomous, safety-aware LLM agents aimed at discovering novel and useful materials."
    },
    {
      "title": "Breaking the Static Graph: Context-Aware Traversal for Robust Retrieval-Augmented Generation",
      "url": "https://arxiv.org/abs/2602.01965",
      "date": "2026-02-02",
      "authors_text": "Kwun Hang Lau, Fangyuan Zhang, Boyu Ruan, Yingli Zhou, Qintian Guo",
      "is_highlight": false,
      "score": 27.0,
      "summary": "CatRAG enhances Retrieval-Augmented Generation by transforming static Knowledge Graphs into query-adaptive structures, improving multi-hop evidence retrieval and reasoning completeness.",
      "teaser_image": "https://arxiv.org/html/2602.01965/image/HubBiasShift.png",
      "debug_abstract": "Recent advances in Retrieval-Augmented Generation (RAG) have shifted from simple vector similarity to structure-aware approaches like HippoRAG, which leverage Knowledge Graphs (KGs) and Personalized PageRank (PPR) to capture multi-hop dependencies. However, these methods suffer from a &#34;Static Graph Fallacy&#34;: they rely on fixed transition probabilities determined during indexing. This rigidity ignores the query-dependent nature of edge relevance, causing semantic drift where random walks are diverted into high-degree &#34;hub&#34; nodes before reaching critical downstream evidence. Consequently, models often achieve high partial recall but fail to retrieve the complete evidence chain required for multi-hop queries. To address this, we propose CatRAG, Context-Aware Traversal for robust RAG, a framework that builds on the HippoRAG 2 architecture and transforms the static KG into a query-adaptive navigation structure. We introduce a multi-faceted framework to steer the random walk: (1) Symbolic Anchoring, which injects weak entity constraints to regularize the random walk; (2) Query-Aware Dynamic Edge Weighting, which dynamically modulates graph structure, to prune irrelevant paths while amplifying those aligned with the query&#39;s intent; and (3) Key-Fact Passage Weight Enhancement, a cost-efficient bias that structurally anchors the random walk to likely evidence. Experiments across four multi-hop benchmarks demonstrate that CatRAG consistently outperforms state of the art baselines. Our analysis reveals that while standard Recall metrics show modest gains, CatRAG achieves substantial improvements in reasoning completeness, the capacity to recover the entire evidence path without gaps. These results reveal that our approach effectively bridges the gap between retrieving partial context and enabling fully grounded reasoning. Resources are available at this https URL."
    },
    {
      "title": "Towards Exploratory and Focused Manipulation with Bimanual Active Perception: A New Problem, Benchmark and Strategy",
      "url": "https://arxiv.org/abs/2602.01939",
      "date": "2026-02-02",
      "authors_text": "Yuxin He, Ruihao Zhang, Tianao Shen, Cheng Liu, Qiang Nie",
      "is_highlight": false,
      "score": 28.0,
      "summary": "The paper introduces the Exploratory and Focused Manipulation (EFM) problem, establishes the EFM-10 benchmark, and proposes a Bimanual Active Perception strategy to enhance manipulation tasks.",
      "teaser_image": "https://arxiv.org/html/2602.01939/x2.png",
      "debug_abstract": "Recently, active vision has reemerged as an important concept for manipulation, since visual occlusion occurs more frequently when main cameras are mounted on the robot heads. We reflect on the visual occlusion issue and identify its essence as the absence of information useful for task completion. Inspired by this, we come up with the more fundamental problem of Exploratory and Focused Manipulation (EFM). The proposed problem is about actively collecting information to complete challenging manipulation tasks that require exploration or focus. As an initial attempt to address this problem, we establish the EFM-10 benchmark that consists of 4 categories of tasks that align with our definition (10 tasks in total). We further come up with a Bimanual Active Perception (BAP) strategy, which leverages one arm to provide active vision and another arm to provide force sensing while manipulating. Based on this idea, we collect a dataset named BAPData for the tasks in EFM-10. With the dataset, we successfully verify the effectiveness of the BAP strategy in an imitation learning manner. We hope that the EFM-10 benchmark along with the BAP strategy can become a cornerstone that facilitates future research towards this direction. Project website: this http URL."
    },
    {
      "title": "Do All Individual Layers Help? An Empirical Study of Task-Interfering Layers in Vision-Language Models",
      "url": "https://arxiv.org/abs/2602.01167",
      "date": "2026-02-01",
      "authors_text": "Zhiming Liu, Yujie Wei, Lei Feng, Xiu Su, Xiaobo Xia",
      "is_highlight": false,
      "score": 52.0,
      "summary": "This study identifies Task-Interfering Layers in vision-language models, demonstrating that selectively bypassing certain layers can enhance task performance without retraining.",
      "teaser_image": "https://arxiv.org/html/2602.01167/x1.png",
      "debug_abstract": "Current VLMs have demonstrated capabilities across a wide range of multimodal tasks. Typically, in a pretrained VLM, all layers are engaged by default to make predictions on downstream tasks. We find that intervening on a single layer, such as by zeroing its parameters, can improve the performance on certain tasks, indicating that some layers hinder rather than help downstream tasks. We systematically investigate how individual layers influence different tasks via layer intervention. Specifically, we measure the change in performance relative to the base model after intervening on each layer and observe improvements when bypassing specific layers. This improvement can be generalizable across models and datasets, indicating the presence of Task-Interfering Layers that harm downstream tasks&#39; performance. We introduce Task-Layer Interaction Vector, which quantifies the effect of intervening on each layer of a VLM given a task. These task-interfering layers exhibit task-specific sensitivity patterns: tasks requiring similar capabilities show consistent response trends under layer interventions, as evidenced by the high similarity in their task-layer interaction vectors. Inspired by these findings, we propose TaLo (Task-Adaptive Layer Knockout), a training-free, test-time adaptation method that dynamically identifies and bypasses the most interfering layer for a given task. Without parameter updates, TaLo improves performance across various models and datasets, including boosting Qwen-VL&#39;s accuracy on the Maps task in ScienceQA by up to 16.6%. Our work reveals an unexpected form of modularity in pretrained VLMs and provides a plug-and-play, training-free mechanism to unlock hidden capabilities at inference time. The source code will be publicly available."
    },
    {
      "title": "PolicyFlow: Policy Optimization with Continuous Normalizing Flow in Reinforcement Learning",
      "url": "https://arxiv.org/abs/2602.01156",
      "date": "2026-02-01",
      "authors_text": "Shunpeng Yang, Ben Liu, Hua Chen",
      "is_highlight": false,
      "score": 71.0,
      "summary": "PolicyFlow is a novel reinforcement learning algorithm that integrates continuous normalizing flows with PPO, enhancing performance and stability while reducing computational costs and encouraging diverse behaviors.",
      "teaser_image": "https://arxiv.org/html/2602.01156/x1.png",
      "debug_abstract": "Among on-policy reinforcement learning algorithms, Proximal Policy Optimization (PPO) demonstrates is widely favored for its simplicity, numerical stability, and strong empirical performance. Standard PPO relies on surrogate objectives defined via importance ratios, which require evaluating policy likelihood that is typically straightforward when the policy is modeled as a Gaussian distribution. However, extending PPO to more expressive, high-capacity policy models such as continuous normalizing flows (CNFs), also known as flow-matching models, is challenging because likelihood evaluation along the full flow trajectory is computationally expensive and often numerically unstable. To resolve this issue, we propose PolicyFlow, a novel on-policy CNF-based reinforcement learning algorithm that integrates expressive CNF policies with PPO-style objectives without requiring likelihood evaluation along the full flow path. PolicyFlow approximates importance ratios using velocity field variations along a simple interpolation path, reducing computational overhead without compromising training stability. To further prevent mode collapse and further encourage diverse behaviors, we propose the Brownian Regularizer, an implicit policy entropy regularizer inspired by Brownian motion, which is conceptually elegant and computationally lightweight. Experiments on diverse tasks across various environments including MultiGoal, PointMaze, IsaacLab and MuJoCo Playground show that PolicyFlow achieves competitive or superior performance compared to PPO using Gaussian policies and flow-based baselines including FPO and DPPO. Notably, results on MultiGoal highlight PolicyFlow&#39;s ability to capture richer multimodal action distributions."
    },
    {
      "title": "Invariance on Manifolds: Understanding Robust Visual Representations for Place Recognition",
      "url": "https://arxiv.org/abs/2602.00841",
      "date": "2026-01-31",
      "authors_text": "Jintao Cheng, Weibin Li, Zhijian He, Jin Wu, Chi Man Vong",
      "is_highlight": false,
      "score": 66.0,
      "summary": "This paper presents a training-free Second-Order Geometric Statistics framework for robust visual place recognition, utilizing covariance descriptors on the SPD manifold for effective zero-shot generalization.",
      "teaser_image": "https://arxiv.org/html/2602.00841/x2.png",
      "debug_abstract": "Visual Place Recognition (VPR) demands representations robust to drastic environmental and viewpoint shifts. Current aggregation paradigms, however, either rely on data-hungry supervision or simplistic first-order statistics, often neglecting intrinsic structural correlations. In this work, we propose a Second-Order Geometric Statistics framework that inherently captures geometric stability without training. We conceptualize scenes as covariance descriptors on the Symmetric Positive Definite (SPD) manifold, where perturbations manifest as tractable congruence transformations. By leveraging geometry-aware Riemannian mappings, we project these descriptors into a linearized Euclidean embedding, effectively decoupling signal structure from noise. Our approach introduces a training-free framework built upon fixed, pre-trained backbones, achieving strong zero-shot generalization without parameter updates. Extensive experiments confirm that our method achieves highly competitive performance against state-of-the-art baselines, particularly excelling in challenging zero-shot scenarios."
    },
    {
      "title": "Physics-informed Diffusion Mamba Transformer for Real-world Driving",
      "url": "https://arxiv.org/abs/2602.00808",
      "date": "2026-01-31",
      "authors_text": "Hang Zhou, Qiang Zhang, Peiran Liu, Yihao Qin, Zhaoxu Yan",
      "is_highlight": false,
      "score": 74.0,
      "summary": "The paper presents a Diffusion Mamba Transformer that integrates sequential context and physical constraints for improved trajectory planning in autonomous driving, outperforming existing models.",
      "teaser_image": "https://arxiv.org/html/2602.00808/difference_pipeline3.png",
      "debug_abstract": "Autonomous driving systems demand trajectory planners that not only model the inherent uncertainty of future motions but also respect complex temporal dependencies and underlying physical laws. While diffusion-based generative models excel at capturing multi-modal distributions, they often fail to incorporate long-term sequential contexts and domain-specific physical priors. In this work, we bridge these gaps with two key innovations. First, we introduce a Diffusion Mamba Transformer architecture that embeds mamba and attention into the diffusion process, enabling more effective aggregation of sequential input contexts from sensor streams and past motion histories. Second, we design a Port-Hamiltonian Neural Network module that seamlessly integrates energy-based physical constraints into the diffusion model, thereby enhancing trajectory predictions with both consistency and interpretability. Extensive evaluations on standard autonomous driving benchmarks demonstrate that our unified framework significantly outperforms state-of-the-art baselines in predictive accuracy, physical plausibility, and robustness, thereby advancing safe and reliable motion planning."
    },
    {
      "title": "Diffusion-Driven Inter-Outer Surface Separation for Point Clouds with Open Boundaries",
      "url": "https://arxiv.org/abs/2602.00739",
      "date": "2026-01-31",
      "authors_text": "Zhengyan Qin, Liyuan Qiu",
      "is_highlight": false,
      "score": 72.0,
      "summary": "The paper presents a diffusion-based algorithm for effectively separating inter and outer surfaces in double-layered point clouds with open boundaries, addressing artifacts from TSDF fusion.",
      "teaser_image": "https://arxiv.org/html/2602.00739/figures/pipeline5.png",
      "debug_abstract": "We propose a diffusion-based algorithm for separating the inter and outer layer surfaces from double-layered point clouds, particularly those exhibiting the &#34;double surface artifact&#34; caused by truncation in Truncated Signed Distance Function (TSDF) fusion during indoor or medical 3D reconstruction. This artifact arises from asymmetric truncation thresholds, leading to erroneous inter and outer shells in the fused volume, which our method addresses by extracting the true inter layer to mitigate challenges like overlapping surfaces and disordered normals. We focus on point clouds with \\emph{open boundaries} (i.e., sampled surfaces with topological openings/holes through which particles may escape), rather than point clouds with \\emph{missing surface regions} where no samples exist. Our approach enables robust processing of both watertight and open-boundary models, achieving extraction of the inter layer from 20,000 inter and 20,000 outer points in approximately 10 seconds. This solution is particularly effective for applications requiring accurate surface representations, such as indoor scene modeling and medical imaging, where double-layered point clouds are prevalent, and it accommodates both closed (watertight) and open-boundary surface geometries. Our goal is \\emph{post-hoc} inter/outer shell separation as a lightweight module after TSDF fusion; we do not aim to replace full variational or learning-based reconstruction pipelines."
    }
  ],
  "范明明老师实验室": [
    {
      "title": "Towards Agentic Intelligence for Materials Science",
      "url": "https://arxiv.org/abs/2602.00169",
      "date": "2026-01-29",
      "authors_text": "Huan Zhang, Yizhan Li, Wenhao Huang, Ziyu Hou, Yu Song",
      "is_highlight": false,
      "score": 12.0,
      "summary": "This paper advocates for developing agentic AI systems in materials science that autonomously plan, act, and learn to enhance discovery processes beyond traditional models.",
      "teaser_image": "https://arxiv.org/html/2602.00169/x1.png",
      "debug_abstract": "The convergence of artificial intelligence and materials science presents a transformative opportunity, but achieving true acceleration in discovery requires moving beyond task-isolated, fine-tuned models toward agentic systems that plan, act, and learn across the full discovery loop. This survey advances a unique pipeline-centric view that spans from corpus curation and pretraining, through domain adaptation and instruction tuning, to goal-conditioned agents interfacing with simulation and experimental platforms. Unlike prior reviews, we treat the entire process as an end-to-end system to be optimized for tangible discovery outcomes rather than proxy benchmarks. This perspective allows us to trace how upstream design choices-such as data curation and training objectives-can be aligned with downstream experimental success through effective credit assignment. To bridge communities and establish a shared frame of reference, we first present an integrated lens that aligns terminology, evaluation, and workflow stages across AI and materials science. We then analyze the field through two focused lenses: From the AI perspective, the survey details LLM strengths in pattern recognition, predictive analytics, and natural language processing for literature mining, materials characterization, and property prediction; from the materials science perspective, it highlights applications in materials design, process optimization, and the acceleration of computational workflows via integration with external tools (e.g., DFT, robotic labs). Finally, we contrast passive, reactive approaches with agentic design, cataloging current contributions while motivating systems that pursue long-horizon goals with autonomy, memory, and tool use. This survey charts a practical roadmap towards autonomous, safety-aware LLM agents aimed at discovering novel and useful materials."
    },
    {
      "title": "Breaking the Static Graph: Context-Aware Traversal for Robust Retrieval-Augmented Generation",
      "url": "https://arxiv.org/abs/2602.01965",
      "date": "2026-02-02",
      "authors_text": "Kwun Hang Lau, Fangyuan Zhang, Boyu Ruan, Yingli Zhou, Qintian Guo",
      "is_highlight": false,
      "score": 27.0,
      "summary": "CatRAG enhances Retrieval-Augmented Generation by transforming static Knowledge Graphs into query-adaptive structures, improving multi-hop evidence retrieval and reasoning completeness.",
      "teaser_image": "https://arxiv.org/html/2602.01965/image/HubBiasShift.png",
      "debug_abstract": "Recent advances in Retrieval-Augmented Generation (RAG) have shifted from simple vector similarity to structure-aware approaches like HippoRAG, which leverage Knowledge Graphs (KGs) and Personalized PageRank (PPR) to capture multi-hop dependencies. However, these methods suffer from a &#34;Static Graph Fallacy&#34;: they rely on fixed transition probabilities determined during indexing. This rigidity ignores the query-dependent nature of edge relevance, causing semantic drift where random walks are diverted into high-degree &#34;hub&#34; nodes before reaching critical downstream evidence. Consequently, models often achieve high partial recall but fail to retrieve the complete evidence chain required for multi-hop queries. To address this, we propose CatRAG, Context-Aware Traversal for robust RAG, a framework that builds on the HippoRAG 2 architecture and transforms the static KG into a query-adaptive navigation structure. We introduce a multi-faceted framework to steer the random walk: (1) Symbolic Anchoring, which injects weak entity constraints to regularize the random walk; (2) Query-Aware Dynamic Edge Weighting, which dynamically modulates graph structure, to prune irrelevant paths while amplifying those aligned with the query&#39;s intent; and (3) Key-Fact Passage Weight Enhancement, a cost-efficient bias that structurally anchors the random walk to likely evidence. Experiments across four multi-hop benchmarks demonstrate that CatRAG consistently outperforms state of the art baselines. Our analysis reveals that while standard Recall metrics show modest gains, CatRAG achieves substantial improvements in reasoning completeness, the capacity to recover the entire evidence path without gaps. These results reveal that our approach effectively bridges the gap between retrieving partial context and enabling fully grounded reasoning. Resources are available at this https URL."
    },
    {
      "title": "Towards Exploratory and Focused Manipulation with Bimanual Active Perception: A New Problem, Benchmark and Strategy",
      "url": "https://arxiv.org/abs/2602.01939",
      "date": "2026-02-02",
      "authors_text": "Yuxin He, Ruihao Zhang, Tianao Shen, Cheng Liu, Qiang Nie",
      "is_highlight": false,
      "score": 28.0,
      "summary": "The paper introduces the Exploratory and Focused Manipulation (EFM) problem, establishes the EFM-10 benchmark, and proposes a Bimanual Active Perception strategy to enhance manipulation tasks.",
      "teaser_image": "https://arxiv.org/html/2602.01939/x2.png",
      "debug_abstract": "Recently, active vision has reemerged as an important concept for manipulation, since visual occlusion occurs more frequently when main cameras are mounted on the robot heads. We reflect on the visual occlusion issue and identify its essence as the absence of information useful for task completion. Inspired by this, we come up with the more fundamental problem of Exploratory and Focused Manipulation (EFM). The proposed problem is about actively collecting information to complete challenging manipulation tasks that require exploration or focus. As an initial attempt to address this problem, we establish the EFM-10 benchmark that consists of 4 categories of tasks that align with our definition (10 tasks in total). We further come up with a Bimanual Active Perception (BAP) strategy, which leverages one arm to provide active vision and another arm to provide force sensing while manipulating. Based on this idea, we collect a dataset named BAPData for the tasks in EFM-10. With the dataset, we successfully verify the effectiveness of the BAP strategy in an imitation learning manner. We hope that the EFM-10 benchmark along with the BAP strategy can become a cornerstone that facilitates future research towards this direction. Project website: this http URL."
    },
    {
      "title": "Do All Individual Layers Help? An Empirical Study of Task-Interfering Layers in Vision-Language Models",
      "url": "https://arxiv.org/abs/2602.01167",
      "date": "2026-02-01",
      "authors_text": "Zhiming Liu, Yujie Wei, Lei Feng, Xiu Su, Xiaobo Xia",
      "is_highlight": false,
      "score": 52.0,
      "summary": "This study identifies Task-Interfering Layers in vision-language models, demonstrating that selectively bypassing certain layers can enhance task performance without retraining.",
      "teaser_image": "https://arxiv.org/html/2602.01167/x1.png",
      "debug_abstract": "Current VLMs have demonstrated capabilities across a wide range of multimodal tasks. Typically, in a pretrained VLM, all layers are engaged by default to make predictions on downstream tasks. We find that intervening on a single layer, such as by zeroing its parameters, can improve the performance on certain tasks, indicating that some layers hinder rather than help downstream tasks. We systematically investigate how individual layers influence different tasks via layer intervention. Specifically, we measure the change in performance relative to the base model after intervening on each layer and observe improvements when bypassing specific layers. This improvement can be generalizable across models and datasets, indicating the presence of Task-Interfering Layers that harm downstream tasks&#39; performance. We introduce Task-Layer Interaction Vector, which quantifies the effect of intervening on each layer of a VLM given a task. These task-interfering layers exhibit task-specific sensitivity patterns: tasks requiring similar capabilities show consistent response trends under layer interventions, as evidenced by the high similarity in their task-layer interaction vectors. Inspired by these findings, we propose TaLo (Task-Adaptive Layer Knockout), a training-free, test-time adaptation method that dynamically identifies and bypasses the most interfering layer for a given task. Without parameter updates, TaLo improves performance across various models and datasets, including boosting Qwen-VL&#39;s accuracy on the Maps task in ScienceQA by up to 16.6%. Our work reveals an unexpected form of modularity in pretrained VLMs and provides a plug-and-play, training-free mechanism to unlock hidden capabilities at inference time. The source code will be publicly available."
    },
    {
      "title": "PolicyFlow: Policy Optimization with Continuous Normalizing Flow in Reinforcement Learning",
      "url": "https://arxiv.org/abs/2602.01156",
      "date": "2026-02-01",
      "authors_text": "Shunpeng Yang, Ben Liu, Hua Chen",
      "is_highlight": false,
      "score": 71.0,
      "summary": "PolicyFlow is a novel reinforcement learning algorithm that integrates continuous normalizing flows with PPO, enhancing performance and stability while reducing computational costs and encouraging diverse behaviors.",
      "teaser_image": "https://arxiv.org/html/2602.01156/x1.png",
      "debug_abstract": "Among on-policy reinforcement learning algorithms, Proximal Policy Optimization (PPO) demonstrates is widely favored for its simplicity, numerical stability, and strong empirical performance. Standard PPO relies on surrogate objectives defined via importance ratios, which require evaluating policy likelihood that is typically straightforward when the policy is modeled as a Gaussian distribution. However, extending PPO to more expressive, high-capacity policy models such as continuous normalizing flows (CNFs), also known as flow-matching models, is challenging because likelihood evaluation along the full flow trajectory is computationally expensive and often numerically unstable. To resolve this issue, we propose PolicyFlow, a novel on-policy CNF-based reinforcement learning algorithm that integrates expressive CNF policies with PPO-style objectives without requiring likelihood evaluation along the full flow path. PolicyFlow approximates importance ratios using velocity field variations along a simple interpolation path, reducing computational overhead without compromising training stability. To further prevent mode collapse and further encourage diverse behaviors, we propose the Brownian Regularizer, an implicit policy entropy regularizer inspired by Brownian motion, which is conceptually elegant and computationally lightweight. Experiments on diverse tasks across various environments including MultiGoal, PointMaze, IsaacLab and MuJoCo Playground show that PolicyFlow achieves competitive or superior performance compared to PPO using Gaussian policies and flow-based baselines including FPO and DPPO. Notably, results on MultiGoal highlight PolicyFlow&#39;s ability to capture richer multimodal action distributions."
    },
    {
      "title": "Invariance on Manifolds: Understanding Robust Visual Representations for Place Recognition",
      "url": "https://arxiv.org/abs/2602.00841",
      "date": "2026-01-31",
      "authors_text": "Jintao Cheng, Weibin Li, Zhijian He, Jin Wu, Chi Man Vong",
      "is_highlight": false,
      "score": 66.0,
      "summary": "This paper presents a training-free Second-Order Geometric Statistics framework for robust visual place recognition, utilizing covariance descriptors on the SPD manifold for effective zero-shot generalization.",
      "teaser_image": "https://arxiv.org/html/2602.00841/x2.png",
      "debug_abstract": "Visual Place Recognition (VPR) demands representations robust to drastic environmental and viewpoint shifts. Current aggregation paradigms, however, either rely on data-hungry supervision or simplistic first-order statistics, often neglecting intrinsic structural correlations. In this work, we propose a Second-Order Geometric Statistics framework that inherently captures geometric stability without training. We conceptualize scenes as covariance descriptors on the Symmetric Positive Definite (SPD) manifold, where perturbations manifest as tractable congruence transformations. By leveraging geometry-aware Riemannian mappings, we project these descriptors into a linearized Euclidean embedding, effectively decoupling signal structure from noise. Our approach introduces a training-free framework built upon fixed, pre-trained backbones, achieving strong zero-shot generalization without parameter updates. Extensive experiments confirm that our method achieves highly competitive performance against state-of-the-art baselines, particularly excelling in challenging zero-shot scenarios."
    },
    {
      "title": "Physics-informed Diffusion Mamba Transformer for Real-world Driving",
      "url": "https://arxiv.org/abs/2602.00808",
      "date": "2026-01-31",
      "authors_text": "Hang Zhou, Qiang Zhang, Peiran Liu, Yihao Qin, Zhaoxu Yan",
      "is_highlight": false,
      "score": 74.0,
      "summary": "The paper presents a Diffusion Mamba Transformer that integrates sequential context and physical constraints for improved trajectory planning in autonomous driving, outperforming existing models.",
      "teaser_image": "https://arxiv.org/html/2602.00808/difference_pipeline3.png",
      "debug_abstract": "Autonomous driving systems demand trajectory planners that not only model the inherent uncertainty of future motions but also respect complex temporal dependencies and underlying physical laws. While diffusion-based generative models excel at capturing multi-modal distributions, they often fail to incorporate long-term sequential contexts and domain-specific physical priors. In this work, we bridge these gaps with two key innovations. First, we introduce a Diffusion Mamba Transformer architecture that embeds mamba and attention into the diffusion process, enabling more effective aggregation of sequential input contexts from sensor streams and past motion histories. Second, we design a Port-Hamiltonian Neural Network module that seamlessly integrates energy-based physical constraints into the diffusion model, thereby enhancing trajectory predictions with both consistency and interpretability. Extensive evaluations on standard autonomous driving benchmarks demonstrate that our unified framework significantly outperforms state-of-the-art baselines in predictive accuracy, physical plausibility, and robustness, thereby advancing safe and reliable motion planning."
    },
    {
      "title": "Diffusion-Driven Inter-Outer Surface Separation for Point Clouds with Open Boundaries",
      "url": "https://arxiv.org/abs/2602.00739",
      "date": "2026-01-31",
      "authors_text": "Zhengyan Qin, Liyuan Qiu",
      "is_highlight": false,
      "score": 72.0,
      "summary": "The paper presents a diffusion-based algorithm for effectively separating inter and outer surfaces in double-layered point clouds with open boundaries, addressing artifacts from TSDF fusion.",
      "teaser_image": "https://arxiv.org/html/2602.00739/figures/pipeline5.png",
      "debug_abstract": "We propose a diffusion-based algorithm for separating the inter and outer layer surfaces from double-layered point clouds, particularly those exhibiting the &#34;double surface artifact&#34; caused by truncation in Truncated Signed Distance Function (TSDF) fusion during indoor or medical 3D reconstruction. This artifact arises from asymmetric truncation thresholds, leading to erroneous inter and outer shells in the fused volume, which our method addresses by extracting the true inter layer to mitigate challenges like overlapping surfaces and disordered normals. We focus on point clouds with \\emph{open boundaries} (i.e., sampled surfaces with topological openings/holes through which particles may escape), rather than point clouds with \\emph{missing surface regions} where no samples exist. Our approach enables robust processing of both watertight and open-boundary models, achieving extraction of the inter layer from 20,000 inter and 20,000 outer points in approximately 10 seconds. This solution is particularly effective for applications requiring accurate surface representations, such as indoor scene modeling and medical imaging, where double-layered point clouds are prevalent, and it accommodates both closed (watertight) and open-boundary surface geometries. Our goal is \\emph{post-hoc} inter/outer shell separation as a lightweight module after TSDF fusion; we do not aim to replace full variational or learning-based reconstruction pipelines."
    }
  ],
  "人机物智能融合实验室 (HCP)": [
    {
      "title": "Superman: Unifying Skeleton and Vision for Human Motion Perception and Generation",
      "url": "https://arxiv.org/abs/2602.02401",
      "date": "2026-02-02",
      "authors_text": "Xinshun Wang, Peiming Li, Ziyi Wang, Zhongbin Fang, Zhichao Deng",
      "is_highlight": false,
      "score": 15.0,
      "summary": "Superman unifies visual perception and skeleton-based motion generation through a Vision-Guided Motion Tokenizer, enabling robust cross-modal learning and superior performance in human motion tasks.",
      "teaser_image": "https://arxiv.org/html/2602.02401/x2.png",
      "debug_abstract": "Human motion analysis tasks, such as temporal 3D pose estimation, motion prediction, and motion in-betweening, play an essential role in computer vision. However, current paradigms suffer from severe fragmentation. First, the field is split between ``perception&#39;&#39; models that understand motion from video but only output text, and ``generation&#39;&#39; models that cannot perceive from raw visual input. Second, generative MLLMs are often limited to single-frame, static poses using dense, parametric SMPL models, failing to handle temporal motion. Third, existing motion vocabularies are built from skeleton data alone, severing the link to the visual domain. To address these challenges, we introduce Superman, a unified framework that bridges visual perception with temporal, skeleton-based motion generation. Our solution is twofold. First, to overcome the modality disconnect, we propose a Vision-Guided Motion Tokenizer. Leveraging the natural geometric alignment between 3D skeletons and visual data, this module pioneers robust joint learning from both modalities, creating a unified, cross-modal motion vocabulary. Second, grounded in this motion language, a single, unified MLLM architecture is trained to handle all tasks. This module flexibly processes diverse, temporal inputs, unifying 3D skeleton pose estimation from video (perception) with skeleton-based motion prediction and in-betweening (generation). Extensive experiments on standard benchmarks, including Human3.6M, demonstrate that our unified method achieves state-of-the-art or competitive performance across all motion tasks. This showcases a more efficient and scalable path for generative motion analysis using skeletons."
    },
    {
      "title": "Context Learning for Multi-Agent Discussion",
      "url": "https://arxiv.org/abs/2602.02350",
      "date": "2026-02-02",
      "authors_text": "Xingyuan Hua, Sheng Yue, Xinyi Li, Yizhe Zhao, Jinrui Zhang",
      "is_highlight": false,
      "score": 17.0,
      "summary": "The paper presents M2CL, a multi-LLM context learning method that enhances coherence in Multi-Agent Discussions, improving problem-solving performance by 20%-50% over existing approaches.",
      "teaser_image": "https://arxiv.org/html/2602.02350/x2.png",
      "debug_abstract": "Multi-Agent Discussion (MAD) has garnered increasing attention very recently, where multiple LLM instances collaboratively solve problems via structured discussion. However, we find that current MAD methods easily suffer from discussion inconsistency, LLMs fail to reach a coherent solution, due to the misalignment between their individual this http URL this paper, we introduce a multi-LLM context learning method (M2CL) that learns a context generator for each agent, capable of dynamically generating context instructions per discussion round via automatic information organization and refinement. Specifically, inspired by our theoretical insights on the context instruction, M2CL train the generators to control context coherence and output discrepancies via a carefully crafted self-adaptive this http URL enables LLMs to avoid premature convergence on majority noise and progressively reach the correct consensus. We evaluate M2CL on challenging tasks, including academic reasoning, embodied tasks, and mobile control. The results show that the performance of M2CL significantly surpasses existing methods by 20%--50%, while enjoying favorable transferability and computational efficiency."
    },
    {
      "title": "CIEC: Coupling Implicit and Explicit Cues for Multimodal Weakly Supervised Manipulation Localization",
      "url": "https://arxiv.org/abs/2602.02175",
      "date": "2026-02-02",
      "authors_text": "Xinquan Yu, Wei Lu, Xiangyang Luo",
      "is_highlight": false,
      "score": 21.0,
      "summary": "The CIEC framework enables multimodal weakly-supervised manipulation localization using only coarse annotations, integrating image and text cues for improved accuracy and reliability.",
      "teaser_image": "https://arxiv.org/html/2602.02175/x2.png",
      "debug_abstract": "To mitigate the threat of misinformation, multimodal manipulation localization has garnered growing attention. Consider that current methods rely on costly and time-consuming fine-grained annotations, such as patch/token-level annotations. This paper proposes a novel framework named Coupling Implicit and Explicit Cues (CIEC), which aims to achieve multimodal weakly-supervised manipulation localization for image-text pairs utilizing only coarse-grained image/sentence-level annotations. It comprises two branches, image-based and text-based weakly-supervised localization. For the former, we devise the Textual-guidance Refine Patch Selection (TRPS) module. It integrates forgery cues from both visual and textual perspectives to lock onto suspicious regions aided by spatial priors. Followed by the background silencing and spatial contrast constraints to suppress interference from irrelevant areas. For the latter, we devise the Visual-deviation Calibrated Token Grounding (VCTG) module. It focuses on meaningful content words and leverages relative visual bias to assist token localization. Followed by the asymmetric sparse and semantic consistency constraints to mitigate label noise and ensure reliability. Extensive experiments demonstrate the effectiveness of our CIEC, yielding results comparable to fully supervised methods on several evaluation metrics."
    },
    {
      "title": "SGHA-Attack: Semantic-Guided Hierarchical Alignment for Transferable Targeted Attacks on Vision-Language Models",
      "url": "https://arxiv.org/abs/2602.01574",
      "date": "2026-02-02",
      "authors_text": "Haobo Wang, Weiqi Luo, Xiaojun Jia, Xiaochun Cao",
      "is_highlight": false,
      "score": 44.0,
      "summary": "SGHA-Attack enhances targeted adversarial attacks on vision-language models by leveraging multiple semantic references and aligning intermediate features for improved transferability and robustness.",
      "teaser_image": "https://arxiv.org/html/2602.01574/x1.png",
      "debug_abstract": "Large vision-language models (VLMs) are vulnerable to transfer-based adversarial perturbations, enabling attackers to optimize on surrogate models and manipulate black-box VLM outputs. Prior targeted transfer attacks often overfit surrogate-specific embedding space by relying on a single reference and emphasizing final-layer alignment, which underutilizes intermediate semantics and degrades transfer across heterogeneous VLMs. To address this, we propose SGHA-Attack, a Semantic-Guided Hierarchical Alignment framework that adopts multiple target references and enforces intermediate-layer consistency. Concretely, we generate a visually grounded reference pool by sampling a frozen text-to-image model conditioned on the target prompt, and then carefully select the Top-K most semantically relevant anchors under the surrogate to form a weighted mixture for stable optimization guidance. Building on these anchors, SGHA-Attack injects target semantics throughout the feature hierarchy by aligning intermediate visual representations at both global and spatial granularities across multiple depths, and by synchronizing intermediate visual and textual features in a shared latent subspace to provide early cross-modal supervision before the final projection. Extensive experiments on open-source and commercial black-box VLMs show that SGHA-Attack achieves stronger targeted transferability than prior methods and remains robust under preprocessing and purification defenses."
    },
    {
      "title": "Resource-Efficient Reinforcement for Reasoning Large Language Models via Dynamic One-Shot Policy Refinement",
      "url": "https://arxiv.org/abs/2602.00815",
      "date": "2026-01-31",
      "authors_text": "Yunjian Zhang, Sudong Wang, Yang Li, Peiran Xu, Conghao Zhou",
      "is_highlight": false,
      "score": 57.0,
      "summary": "The paper introduces Dynamic One-Shot Policy Refinement (DoPR), a resource-efficient reinforcement learning strategy that enhances reasoning in large language models while significantly reducing training costs.",
      "teaser_image": "https://arxiv.org/html/2602.00815/dopr.png",
      "debug_abstract": "Large language models (LLMs) have exhibited remarkable performance on complex reasoning tasks, with reinforcement learning under verifiable rewards (RLVR) emerging as a principled framework for aligning model behavior with reasoning chains. Despite its promise, RLVR remains prohibitively resource-intensive, requiring extensive reward signals and incurring substantial rollout costs during training. In this work, we revisit the fundamental question of data and compute efficiency in RLVR. We first establish a theoretical lower bound on the sample complexity required to unlock reasoning capabilities, and empirically validate that strong performance can be achieved with a surprisingly small number of training instances. To tackle the computational burden, we propose Dynamic One-Shot Policy Refinement (DoPR), an uncertainty-aware RL strategy that dynamically selects a single informative training sample per batch for policy updates, guided by reward volatility and exploration-driven acquisition. DoPR reduces rollout overhead by nearly an order of magnitude while preserving competitive reasoning accuracy, offering a scalable and resource-efficient solution for LLM post-training. This approach offers a practical path toward more efficient and accessible RL-based training for reasoning-intensive LLM applications."
    }
  ],
  "西安电子科技大学": [
    {
      "title": "SPIRIT: Adapting Vision Foundation Models for Unified Single- and Multi-Frame Infrared Small Target Detection",
      "url": "https://arxiv.org/abs/2602.01843",
      "date": "2026-02-02",
      "authors_text": "Qian Xu, Xi Li, Fei Gao, Jie Guo, Haojuan Yuan",
      "is_highlight": false,
      "score": 33.0,
      "summary": "SPIRIT is a unified framework that enhances infrared small target detection by adapting vision foundation models with physics-informed plug-ins for improved single- and multi-frame analysis.",
      "teaser_image": "https://arxiv.org/html/2602.01843/x2.png",
      "debug_abstract": "Infrared small target detection (IRSTD) is crucial for surveillance and early-warning, with deployments spanning both single-frame analysis and video-mode tracking. A practical solution should leverage vision foundation models (VFMs) to mitigate infrared data scarcity, while adopting a memory-attention-based temporal propagation framework that unifies single- and multi-frame inference. However, infrared small targets exhibit weak radiometric signals and limited semantic cues, which differ markedly from visible-spectrum imagery. This modality gap makes direct use of semantics-oriented VFMs and appearance-driven cross-frame association unreliable for IRSTD: hierarchical feature aggregation can submerge localized target peaks, and appearance-only memory attention becomes ambiguous, leading to spurious clutter associations. To address these challenges, we propose SPIRIT, a unified and VFM-compatible framework that adapts VFMs to IRSTD via lightweight physics-informed plug-ins. Spatially, PIFR refines features by approximating rank-sparsity decomposition to suppress structured background components and enhance sparse target-like signals. Temporally, PGMA injects history-derived soft spatial priors into memory cross-attention to constrain cross-frame association, enabling robust video detection while naturally reverting to single-frame inference when temporal context is absent. Experiments on multiple IRSTD benchmarks show consistent gains over VFM-based baselines and SOTA performance."
    },
    {
      "title": "Resource-Efficient Reinforcement for Reasoning Large Language Models via Dynamic One-Shot Policy Refinement",
      "url": "https://arxiv.org/abs/2602.00815",
      "date": "2026-01-31",
      "authors_text": "Yunjian Zhang, Sudong Wang, Yang Li, Peiran Xu, Conghao Zhou",
      "is_highlight": false,
      "score": 57.0,
      "summary": "The paper introduces Dynamic One-Shot Policy Refinement (DoPR), a resource-efficient reinforcement learning strategy that enhances reasoning in large language models while significantly reducing training costs.",
      "teaser_image": "https://arxiv.org/html/2602.00815/dopr.png",
      "debug_abstract": "Large language models (LLMs) have exhibited remarkable performance on complex reasoning tasks, with reinforcement learning under verifiable rewards (RLVR) emerging as a principled framework for aligning model behavior with reasoning chains. Despite its promise, RLVR remains prohibitively resource-intensive, requiring extensive reward signals and incurring substantial rollout costs during training. In this work, we revisit the fundamental question of data and compute efficiency in RLVR. We first establish a theoretical lower bound on the sample complexity required to unlock reasoning capabilities, and empirically validate that strong performance can be achieved with a surprisingly small number of training instances. To tackle the computational burden, we propose Dynamic One-Shot Policy Refinement (DoPR), an uncertainty-aware RL strategy that dynamically selects a single informative training sample per batch for policy updates, guided by reward volatility and exploration-driven acquisition. DoPR reduces rollout overhead by nearly an order of magnitude while preserving competitive reasoning accuracy, offering a scalable and resource-efficient solution for LLM post-training. This approach offers a practical path toward more efficient and accessible RL-based training for reasoning-intensive LLM applications."
    }
  ],
  "机器人与机械智能实验室 (ROMI Lab)": [
    {
      "title": "Unveiling the Cognitive Compass: Theory-of-Mind-Guided Multimodal Emotion Reasoning",
      "url": "https://arxiv.org/abs/2602.00971",
      "date": "2026-02-01",
      "authors_text": "Meng Luo, Bobo Li, Shanqing Xu, Shize Zhang, Qiuchan Chen",
      "is_highlight": false,
      "score": 62.0,
      "summary": "The paper presents HitEmotion, a benchmark for assessing and enhancing multimodal large language models' emotional reasoning through Theory of Mind-guided methods and reinforcement learning.",
      "teaser_image": "https://arxiv.org/html/2602.00971/figures/teaser.png",
      "debug_abstract": "Despite rapid progress in multimodal large language models (MLLMs), their capability for deep emotional understanding remains limited. We argue that genuine affective intelligence requires explicit modeling of Theory of Mind (ToM), the cognitive substrate from which emotions arise. To this end, we introduce HitEmotion, a ToM-grounded hierarchical benchmark that diagnoses capability breakpoints across increasing levels of cognitive depth. Second, we propose a ToM-guided reasoning chain that tracks mental states and calibrates cross-modal evidence to achieve faithful emotional reasoning. We further introduce TMPO, a reinforcement learning method that uses intermediate mental states as process-level supervision to guide and strengthen model reasoning. Extensive experiments show that HitEmotion exposes deep emotional reasoning deficits in state-of-the-art models, especially on cognitively demanding tasks. In evaluation, the ToM-guided reasoning chain and TMPO improve end-task accuracy and yield more faithful, more coherent rationales. In conclusion, our work provides the research community with a practical toolkit for evaluating and enhancing the cognition-based emotional understanding capabilities of MLLMs. Our dataset and code are available at: this https URL."
    }
  ],
  "厦门大学": [
    {
      "title": "A State-Transition Framework for Efficient LLM Reasoning",
      "url": "https://arxiv.org/abs/2602.01198",
      "date": "2026-02-01",
      "authors_text": "Liang Zhang, Yu Zhao, Longyue Wang, Tianqi Shi, Weihua Luo",
      "is_highlight": false,
      "score": 49.0,
      "summary": "This paper introduces a state-transition framework for LLMs that enhances reasoning efficiency by reducing attention complexity from quadratic to linear while improving performance.",
      "teaser_image": "https://arxiv.org/html/2602.01198/figure-2.jpeg",
      "debug_abstract": "While Long Chain-of-Thought (CoT) reasoning significantly improves Large Language Models (LLMs) performance on complex reasoning tasks, the substantial computational and memory costs of generating long CoT sequences limit their efficiency and practicality. Existing studies usually enhance the reasoning efficiency of LLMs by compressing CoT sequences. However, this approach conflicts with test-time scaling, limiting the reasoning capacity of LLMs. In this paper, we propose an efficient reasoning framework that models the reasoning process of LLMs as a state-transition process. Specifically, we first apply a linear attention mechanism to estimate the LLM&#39;s reasoning state, which records the historical reasoning information from previous reasoning steps. Then, based on the query prompt and the reasoning state, the LLM can efficiently perform the current reasoning step and update the state. With the linear attention, each token in the current reasoning step can directly retrieve relevant historical reasoning information from the reasoning state, without explicitly attending to tokens in previous reasoning steps. In this way, the computational complexity of attention is reduced from quadratic to linear, significantly improving the reasoning efficiency of LLMs. In addition, we propose a state-based reasoning strategy to mitigate the over-thinking issue caused by noisy reasoning steps. Extensive experiments across multiple datasets and model sizes demonstrate that our framework not only improves the reasoning efficiency of LLMs but also enhances their reasoning performance."
    },
    {
      "title": "MedAD-R1: Eliciting Consistent Reasoning in Interpretible Medical Anomaly Detection via Consistency-Reinforced Policy Optimization",
      "url": "https://arxiv.org/abs/2602.01081",
      "date": "2026-02-01",
      "authors_text": "Haitao Zhang, Yingying Wang, Jiaxiang Wang, Haote Xu, Hongyang Zhang",
      "is_highlight": false,
      "score": 64.0,
      "summary": "MedAD-R1 enhances medical anomaly detection by utilizing a two-stage training framework with consistency-reinforced policy optimization, achieving state-of-the-art performance on the MedAD-38K benchmark.",
      "teaser_image": "https://arxiv.org/html/2602.01081/x3.png",
      "debug_abstract": "Medical Anomaly Detection (MedAD) presents a significant opportunity to enhance diagnostic accuracy using Large Multimodal Models (LMMs) to interpret and answer questions based on medical images. However, the reliance on Supervised Fine-Tuning (SFT) on simplistic and fragmented datasets has hindered the development of models capable of plausible reasoning and robust multimodal generalization. To overcome this, we introduce MedAD-38K, the first large-scale, multi-modal, and multi-center benchmark for MedAD featuring diagnostic Chain-of-Thought (CoT) annotations alongside structured Visual Question-Answering (VQA) pairs. On this foundation, we propose a two-stage training framework. The first stage, Cognitive Injection, uses SFT to instill foundational medical knowledge and align the model with a structured think-then-answer paradigm. Given that standard policy optimization can produce reasoning that is disconnected from the final answer, the second stage incorporates Consistency Group Relative Policy Optimization (Con-GRPO). This novel algorithm incorporates a crucial consistency reward to ensure the generated reasoning process is relevant and logically coherent with the final diagnosis. Our proposed model, MedAD-R1, achieves state-of-the-art (SOTA) performance on the MedAD-38K benchmark, outperforming strong baselines by more than 10\\%. This superior performance stems from its ability to generate transparent and logically consistent reasoning pathways, offering a promising approach to enhancing the trustworthiness and interpretability of AI for clinical decision support."
    }
  ],
  "Oxford Robotics Institute (ORI)": [
    {
      "title": "3D Foundation Model-Based Loop Closing for Decentralized Collaborative SLAM",
      "url": "https://arxiv.org/abs/2602.02430",
      "date": "2026-02-02",
      "authors_text": "Pierre-Yves Lajoie, Benjamin Ramtoula, Daniele De Martini, Giovanni Beltrame",
      "is_highlight": false,
      "score": 14.0,
      "summary": "This paper presents a decentralized C-SLAM method utilizing 3D foundation models for robust loop closing and efficient inter-robot measurements, enhancing mapping accuracy and resource efficiency.",
      "teaser_image": "https://arxiv.org/html/2602.02430/x1.png",
      "debug_abstract": "Decentralized Collaborative Simultaneous Localization And Mapping (C-SLAM) techniques often struggle to identify map overlaps due to significant viewpoint variations among robots. Motivated by recent advancements in 3D foundation models, which can register images despite large viewpoint differences, we propose a robust loop closing approach that leverages these models to establish inter-robot measurements. In contrast to resource-intensive methods requiring full 3D reconstruction within a centralized map, our approach integrates foundation models into existing SLAM pipelines, yielding scalable and robust multi-robot mapping. Our contributions include: (1) integrating 3D foundation models to reliably estimate relative poses from monocular image pairs within decentralized C-SLAM; (2) introducing robust outlier mitigation techniques critical to the use of these relative poses; and (3) developing specialized pose graph optimization formulations that efficiently resolve scale ambiguities. We evaluate our method against state-of-the-art approaches, demonstrating improvements in localization and mapping accuracy, alongside significant gains in computational and memory efficiency. These results highlight the potential of our approach for deployment in large-scale multi-robot scenarios."
    },
    {
      "title": "TreeLoc: 6-DoF LiDAR Global Localization in Forests via Inter-Tree Geometric Matching",
      "url": "https://arxiv.org/abs/2602.01501",
      "date": "2026-02-02",
      "authors_text": "Minwoo Jung, Nived Chebrolu, Lucas Carvalho de Lima, Haedam Oh, Maurice Fallon",
      "is_highlight": false,
      "score": 97.0,
      "summary": "TreeLoc is a LiDAR-based framework for 6-DoF global localization in forests, utilizing tree geometry for robust pose estimation and outperforming traditional methods.",
      "teaser_image": "https://arxiv.org/html/2602.01501/x2.png",
      "debug_abstract": "Reliable localization is crucial for navigation in forests, where GPS is often degraded and LiDAR measurements are repetitive, occluded, and structurally complex. These conditions weaken the assumptions of traditional urban-centric localization methods, which assume that consistent features arise from unique structural patterns, necessitating forest-centric solutions to achieve robustness in these environments. To address these challenges, we propose TreeLoc, a LiDAR-based global localization framework for forests that handles place recognition and 6-DoF pose estimation. We represent scenes using tree stems and their Diameter at Breast Height (DBH), which are aligned to a common reference frame via their axes and summarized using the tree distribution histogram (TDH) for coarse matching, followed by fine matching with a 2D triangle descriptor. Finally, pose estimation is achieved through a two-step geometric verification. On diverse forest benchmarks, TreeLoc outperforms baselines, achieving precise localization. Ablation studies validate the contribution of each component. We also propose applications for long-term forest management using descriptors from a compact global tree database. TreeLoc is open-sourced for the robotics community at this https URL."
    },
    {
      "title": "KAN We Flow? Advancing Robotic Manipulation with 3D Flow Matching via KAN & RWKV",
      "url": "https://arxiv.org/abs/2602.01115",
      "date": "2026-02-01",
      "authors_text": "Zhihao Chen, Yiyuan Ge, Ziyang Wang",
      "is_highlight": false,
      "score": 94.0,
      "summary": "KAN-We-Flow introduces a lightweight flow-matching policy utilizing RWKV and KAN for efficient 3D robotic manipulation, achieving state-of-the-art performance with significantly reduced parameters.",
      "teaser_image": "https://arxiv.org/html/2602.01115/image/fig2.png",
      "debug_abstract": "Diffusion-based visuomotor policies excel at modeling action distributions but are inference-inefficient, since recursively denoising from noise to policy requires many steps and heavy UNet backbones, which hinders deployment on resource-constrained robots. Flow matching alleviates the sampling burden by learning a one-step vector field, yet prior implementations still inherit large UNet-style architectures. In this work, we present KAN-We-Flow, a flow-matching policy that draws on recent advances in Receptance Weighted Key Value (RWKV) and Kolmogorov-Arnold Networks (KAN) from vision to build a lightweight and highly expressive backbone for 3D manipulation. Concretely, we introduce an RWKV-KAN block: an RWKV first performs efficient time/channel mixing to propagate task context, and a subsequent GroupKAN layer applies learnable spline-based, groupwise functional mappings to perform feature-wise nonlinear calibration of the action mapping on RWKV outputs. Moreover, we introduce an Action Consistency Regularization (ACR), a lightweight auxiliary loss that enforces alignment between predicted action trajectories and expert demonstrations via Euler extrapolation, providing additional supervision to stabilize training and improve policy precision. Without resorting to large UNets, our design reduces parameters by 86.8\\%, maintains fast runtime, and achieves state-of-the-art success rates on Adroit, Meta-World, and DexArt benchmarks. Our project page can be viewed in \\href{this https URL}{\\textcolor{red}{link}}"
    }
  ],
  "Robotics and Embodied Artificial Intelligence Lab (REAL)": [
    {
      "title": "Med3D-R1: Incentivizing Clinical Reasoning in 3D Medical Vision-Language Models for Abnormality Diagnosis",
      "url": "https://arxiv.org/abs/2602.01200",
      "date": "2026-02-01",
      "authors_text": "Haoran Lai, Zihang Jiang, Kun Zhang, Qingsong Yao, Rongsheng Wang",
      "is_highlight": false,
      "score": 54.0,
      "summary": "Med3D-R1 introduces a two-stage reinforcement learning framework that enhances 3D medical vision-language models for improved abnormality diagnosis and clinical reasoning.",
      "teaser_image": "https://arxiv.org/html/2602.01200/x2.png",
      "debug_abstract": "Developing 3D vision-language models with robust clinical reasoning remains a challenge due to the inherent complexity of volumetric medical imaging, the tendency of models to overfit superficial report patterns, and the lack of interpretability-aware reward designs. In this paper, we propose Med3D-R1, a reinforcement learning framework with a two-stage training process: Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL). During SFT stage, we introduce a residual alignment mechanism to bridge the gap between high-dimensional 3D features and textual embeddings, and an abnormality re-weighting strategy to emphasize clinically informative tokens and reduce structural bias in reports. In RL stage, we redesign the consistency reward to explicitly promote coherent, step-by-step diagnostic reasoning. We evaluate our method on medical multiple-choice visual question answering using two 3D diagnostic benchmarks, CT-RATE and RAD-ChestCT, where our model attains state-of-the-art accuracies of 41.92\\% on CT-RATE and 44.99\\% on RAD-ChestCT. These results indicate improved abnormality diagnosis and clinical reasoning and outperform prior methods on both benchmarks. Overall, our approach holds promise for enhancing real-world diagnostic workflows by enabling more reliable and transparent 3D medical vision-language systems."
    }
  ],
  "斯坦福-视觉与学习实验室 (SVL)": [
    {
      "title": "Med3D-R1: Incentivizing Clinical Reasoning in 3D Medical Vision-Language Models for Abnormality Diagnosis",
      "url": "https://arxiv.org/abs/2602.01200",
      "date": "2026-02-01",
      "authors_text": "Haoran Lai, Zihang Jiang, Kun Zhang, Qingsong Yao, Rongsheng Wang",
      "is_highlight": false,
      "score": 54.0,
      "summary": "Med3D-R1 introduces a two-stage reinforcement learning framework that enhances 3D medical vision-language models for improved abnormality diagnosis and clinical reasoning.",
      "teaser_image": "https://arxiv.org/html/2602.01200/x2.png",
      "debug_abstract": "Developing 3D vision-language models with robust clinical reasoning remains a challenge due to the inherent complexity of volumetric medical imaging, the tendency of models to overfit superficial report patterns, and the lack of interpretability-aware reward designs. In this paper, we propose Med3D-R1, a reinforcement learning framework with a two-stage training process: Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL). During SFT stage, we introduce a residual alignment mechanism to bridge the gap between high-dimensional 3D features and textual embeddings, and an abnormality re-weighting strategy to emphasize clinically informative tokens and reduce structural bias in reports. In RL stage, we redesign the consistency reward to explicitly promote coherent, step-by-step diagnostic reasoning. We evaluate our method on medical multiple-choice visual question answering using two 3D diagnostic benchmarks, CT-RATE and RAD-ChestCT, where our model attains state-of-the-art accuracies of 41.92\\% on CT-RATE and 44.99\\% on RAD-ChestCT. These results indicate improved abnormality diagnosis and clinical reasoning and outperform prior methods on both benchmarks. Overall, our approach holds promise for enhancing real-world diagnostic workflows by enabling more reliable and transparent 3D medical vision-language systems."
    }
  ],
  "斯坦福-人工智能实验室 (SAIL)": [
    {
      "title": "Med3D-R1: Incentivizing Clinical Reasoning in 3D Medical Vision-Language Models for Abnormality Diagnosis",
      "url": "https://arxiv.org/abs/2602.01200",
      "date": "2026-02-01",
      "authors_text": "Haoran Lai, Zihang Jiang, Kun Zhang, Qingsong Yao, Rongsheng Wang",
      "is_highlight": false,
      "score": 54.0,
      "summary": "Med3D-R1 introduces a two-stage reinforcement learning framework that enhances 3D medical vision-language models for improved abnormality diagnosis and clinical reasoning.",
      "teaser_image": "https://arxiv.org/html/2602.01200/x2.png",
      "debug_abstract": "Developing 3D vision-language models with robust clinical reasoning remains a challenge due to the inherent complexity of volumetric medical imaging, the tendency of models to overfit superficial report patterns, and the lack of interpretability-aware reward designs. In this paper, we propose Med3D-R1, a reinforcement learning framework with a two-stage training process: Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL). During SFT stage, we introduce a residual alignment mechanism to bridge the gap between high-dimensional 3D features and textual embeddings, and an abnormality re-weighting strategy to emphasize clinically informative tokens and reduce structural bias in reports. In RL stage, we redesign the consistency reward to explicitly promote coherent, step-by-step diagnostic reasoning. We evaluate our method on medical multiple-choice visual question answering using two 3D diagnostic benchmarks, CT-RATE and RAD-ChestCT, where our model attains state-of-the-art accuracies of 41.92\\% on CT-RATE and 44.99\\% on RAD-ChestCT. These results indicate improved abnormality diagnosis and clinical reasoning and outperform prior methods on both benchmarks. Overall, our approach holds promise for enhancing real-world diagnostic workflows by enabling more reliable and transparent 3D medical vision-language systems."
    }
  ],
  "斯坦福-机器人与具身智能实验室 (REAL)": [
    {
      "title": "Med3D-R1: Incentivizing Clinical Reasoning in 3D Medical Vision-Language Models for Abnormality Diagnosis",
      "url": "https://arxiv.org/abs/2602.01200",
      "date": "2026-02-01",
      "authors_text": "Haoran Lai, Zihang Jiang, Kun Zhang, Qingsong Yao, Rongsheng Wang",
      "is_highlight": false,
      "score": 54.0,
      "summary": "Med3D-R1 introduces a two-stage reinforcement learning framework that enhances 3D medical vision-language models for improved abnormality diagnosis and clinical reasoning.",
      "teaser_image": "https://arxiv.org/html/2602.01200/x2.png",
      "debug_abstract": "Developing 3D vision-language models with robust clinical reasoning remains a challenge due to the inherent complexity of volumetric medical imaging, the tendency of models to overfit superficial report patterns, and the lack of interpretability-aware reward designs. In this paper, we propose Med3D-R1, a reinforcement learning framework with a two-stage training process: Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL). During SFT stage, we introduce a residual alignment mechanism to bridge the gap between high-dimensional 3D features and textual embeddings, and an abnormality re-weighting strategy to emphasize clinically informative tokens and reduce structural bias in reports. In RL stage, we redesign the consistency reward to explicitly promote coherent, step-by-step diagnostic reasoning. We evaluate our method on medical multiple-choice visual question answering using two 3D diagnostic benchmarks, CT-RATE and RAD-ChestCT, where our model attains state-of-the-art accuracies of 41.92\\% on CT-RATE and 44.99\\% on RAD-ChestCT. These results indicate improved abnormality diagnosis and clinical reasoning and outperform prior methods on both benchmarks. Overall, our approach holds promise for enhancing real-world diagnostic workflows by enabling more reliable and transparent 3D medical vision-language systems."
    }
  ],
  "Stanford AI Lab (SAIL)": [
    {
      "title": "Med3D-R1: Incentivizing Clinical Reasoning in 3D Medical Vision-Language Models for Abnormality Diagnosis",
      "url": "https://arxiv.org/abs/2602.01200",
      "date": "2026-02-01",
      "authors_text": "Haoran Lai, Zihang Jiang, Kun Zhang, Qingsong Yao, Rongsheng Wang",
      "is_highlight": false,
      "score": 54.0,
      "summary": "Med3D-R1 introduces a two-stage reinforcement learning framework that enhances 3D medical vision-language models for improved abnormality diagnosis and clinical reasoning.",
      "teaser_image": "https://arxiv.org/html/2602.01200/x2.png",
      "debug_abstract": "Developing 3D vision-language models with robust clinical reasoning remains a challenge due to the inherent complexity of volumetric medical imaging, the tendency of models to overfit superficial report patterns, and the lack of interpretability-aware reward designs. In this paper, we propose Med3D-R1, a reinforcement learning framework with a two-stage training process: Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL). During SFT stage, we introduce a residual alignment mechanism to bridge the gap between high-dimensional 3D features and textual embeddings, and an abnormality re-weighting strategy to emphasize clinically informative tokens and reduce structural bias in reports. In RL stage, we redesign the consistency reward to explicitly promote coherent, step-by-step diagnostic reasoning. We evaluate our method on medical multiple-choice visual question answering using two 3D diagnostic benchmarks, CT-RATE and RAD-ChestCT, where our model attains state-of-the-art accuracies of 41.92\\% on CT-RATE and 44.99\\% on RAD-ChestCT. These results indicate improved abnormality diagnosis and clinical reasoning and outperform prior methods on both benchmarks. Overall, our approach holds promise for enhancing real-world diagnostic workflows by enabling more reliable and transparent 3D medical vision-language systems."
    }
  ],
  "武汉大学": [
    {
      "title": "MagicFuse: Single Image Fusion for Visual and Semantic Reinforcement",
      "url": "https://arxiv.org/abs/2602.01760",
      "date": "2026-02-02",
      "authors_text": "Hao Zhang, Yanping Zha, Zizhuo Li, Meiqi Gong, Jiayi Ma",
      "is_highlight": true,
      "score": 36.0,
      "summary": "MagicFuse is a novel framework that enables effective cross-spectral scene representation from a single low-quality visible image using knowledge-level fusion techniques.",
      "teaser_image": "https://arxiv.org/html/2602.01760/x2.png",
      "debug_abstract": "This paper focuses on a highly practical scenario: how to continue benefiting from the advantages of multi-modal image fusion under harsh conditions when only visible imaging sensors are available. To achieve this goal, we propose a novel concept of single-image fusion, which extends conventional data-level fusion to the knowledge level. Specifically, we develop MagicFuse, a novel single image fusion framework capable of deriving a comprehensive cross-spectral scene representation from a single low-quality visible image. MagicFuse first introduces an intra-spectral knowledge reinforcement branch and a cross-spectral knowledge generation branch based on the diffusion models. They mine scene information obscured in the visible spectrum and learn thermal radiation distribution patterns transferred to the infrared spectrum, respectively. Building on them, we design a multi-domain knowledge fusion branch that integrates the probabilistic noise from the diffusion streams of these two branches, from which a cross-spectral scene representation can be obtained through successive sampling. Then, we impose both visual and semantic constraints to ensure that this scene representation can satisfy human observation while supporting downstream semantic decision-making. Extensive experiments show that our MagicFuse achieves visual and semantic representation performance comparable to or even better than state-of-the-art fusion methods with multi-modal inputs, despite relying solely on a single degraded visible image."
    },
    {
      "title": "Interacted Planes Reveal 3D Line Mapping",
      "url": "https://arxiv.org/abs/2602.01296",
      "date": "2026-02-01",
      "authors_text": "Zeran Ke, Bin Tan, Gui-Song Xia, Yujun Shen, Nan Xue",
      "is_highlight": false,
      "score": 59.0,
      "summary": "LiP-Map introduces a joint optimization framework for 3D line mapping that integrates line and planar primitives, enhancing accuracy and efficiency in scene reconstruction.",
      "teaser_image": "https://arxiv.org/html/2602.01296/x2.png",
      "debug_abstract": "3D line mapping from multi-view RGB images provides a compact and structured visual representation of scenes. We study the problem from a physical and topological perspective: a 3D line most naturally emerges as the edge of a finite 3D planar patch. We present LiP-Map, a line-plane joint optimization framework that explicitly models learnable line and planar primitives. This coupling enables accurate and detailed 3D line mapping while maintaining strong efficiency (typically completing a reconstruction in 3 to 5 minutes per scene). LiP-Map pioneers the integration of planar topology into 3D line mapping, not by imposing pairwise coplanarity constraints but by explicitly constructing interactions between plane and line primitives, thus offering a principled route toward structured reconstruction in man-made environments. On more than 100 scenes from ScanNetV2, ScanNet++, Hypersim, 7Scenes, and Tanks\\&amp;Temple, LiP-Map improves both accuracy and completeness over state-of-the-art methods. Beyond line mapping quality, LiP-Map significantly advances line-assisted visual localization, establishing strong performance on 7Scenes. Our code is released at this https URL for reproducible research."
    }
  ],
  "深圳大学": [
    {
      "title": "T2M Mamba: Motion Periodicity-Saliency Coupling Approach for Stable Text-Driven Motion Generation",
      "url": "https://arxiv.org/abs/2602.01352",
      "date": "2026-02-01",
      "authors_text": "Xingzu Zhan, Chen Xie, Honghang Chen, Yixun Lin, Xiaochun Mai",
      "is_highlight": false,
      "score": 63.0,
      "summary": "T2M Mamba enhances text-to-motion generation by coupling motion periodicity and keyframe saliency, improving stability and robustness against paraphrasing in 3D human motion sequences.",
      "teaser_image": "https://arxiv.org/html/2602.01352/T2MMamba.png",
      "debug_abstract": "Text-to-motion generation, which converts motion language descriptions into coherent 3D human motion sequences, has attracted increasing attention in fields, such as avatar animation and humanoid robotic interaction. Though existing models have achieved significant fidelity, they still suffer from two core limitations: (i) They treat motion periodicity and keyframe saliency as independent factors, overlooking their coupling and causing generation drift in long sequences. (ii) They are fragile to semantically equivalent paraphrases, where minor synonym substitutions distort textual embeddings, propagating through the decoder and producing unstable or erroneous motions. In this work, we propose T2M Mamba to address these limitations by (i) proposing Periodicity-Saliency Aware Mamba, which utilizes novel algorithms for keyframe weight estimation via enhanced Density Peaks Clustering and motion periodicity estimation via FFT-accelerated autocorrelation to capture coupled dynamics with minimal computational overhead, and (ii) constructing a Periodic Differential Cross-modal Alignment Module (PDCAM) to enhance robust alignment of textual and motion embeddings. Extensive experiments on HumanML3D and KIT-ML datasets have been conducted, confirming the effectiveness of our approach, achieving an FID of 0.068 and consistent gains on all other metrics."
    }
  ],
  "Affective Intelligence and Robotics Laboratory (AFAR)": [
    {
      "title": "Governance at the Edge of Architecture: Regulating NeuroAI and Neuromorphic Systems",
      "url": "https://arxiv.org/abs/2602.01503",
      "date": "2026-02-02",
      "authors_text": "Afifah Kashif, Abdul Muhsin Hameed, Asim Iqbal",
      "is_highlight": false,
      "score": 47.0,
      "summary": "The paper critiques existing AI governance frameworks for inadequately addressing the unique challenges posed by NeuroAI and neuromorphic systems, advocating for adaptive regulatory approaches.",
      "teaser_image": "https://arxiv.org/html/2602.01503/data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==",
      "debug_abstract": "Current AI governance frameworks, including regulatory benchmarks for accuracy, latency, and energy efficiency, are built for static, centrally trained artificial neural networks on von Neumann hardware. NeuroAI systems, embodied in neuromorphic hardware and implemented via spiking neural networks, break these assumptions. This paper examines the limitations of current AI governance frameworks for NeuroAI, arguing that assurance and audit methods must co-evolve with these architectures, aligning traditional regulatory metrics with the physics, learning dynamics, and embodied efficiency of brain-inspired computation to enable technically grounded assurance."
    }
  ],
  "Bio-Inspired Robotics Laboratory (BIRL)": [
    {
      "title": "Governance at the Edge of Architecture: Regulating NeuroAI and Neuromorphic Systems",
      "url": "https://arxiv.org/abs/2602.01503",
      "date": "2026-02-02",
      "authors_text": "Afifah Kashif, Abdul Muhsin Hameed, Asim Iqbal",
      "is_highlight": false,
      "score": 47.0,
      "summary": "The paper critiques existing AI governance frameworks for inadequately addressing the unique challenges posed by NeuroAI and neuromorphic systems, advocating for adaptive regulatory approaches.",
      "teaser_image": "https://arxiv.org/html/2602.01503/data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==",
      "debug_abstract": "Current AI governance frameworks, including regulatory benchmarks for accuracy, latency, and energy efficiency, are built for static, centrally trained artificial neural networks on von Neumann hardware. NeuroAI systems, embodied in neuromorphic hardware and implemented via spiking neural networks, break these assumptions. This paper examines the limitations of current AI governance frameworks for NeuroAI, arguing that assurance and audit methods must co-evolve with these architectures, aligning traditional regulatory metrics with the physics, learning dynamics, and embodied efficiency of brain-inspired computation to enable technically grounded assurance."
    }
  ],
  "机器人系统实验室 (RSL)": [
    {
      "title": "VRGaussianAvatar: Integrating 3D Gaussian Avatars into VR",
      "url": "https://arxiv.org/abs/2602.01674",
      "date": "2026-02-02",
      "authors_text": "Hail Song, Boram Yoon, Seokhwan Yang, Seoyoung Kang, Hyunjeong Kim",
      "is_highlight": false,
      "score": 37.0,
      "summary": "VRGaussianAvatar is a real-time system that creates full-body 3D Gaussian avatars in VR using HMD tracking, enhancing rendering efficiency and user experience.",
      "teaser_image": "https://arxiv.org/html/2602.01674/x2.png",
      "debug_abstract": "We present VRGaussianAvatar, an integrated system that enables real-time full-body 3D Gaussian Splatting (3DGS) avatars in virtual reality using only head-mounted display (HMD) tracking signals. The system adopts a parallel pipeline with a VR Frontend and a GA Backend. The VR Frontend uses inverse kinematics to estimate full-body pose and streams the resulting pose along with stereo camera parameters to the backend. The GA Backend stereoscopically renders a 3DGS avatar reconstructed from a single image. To improve stereo rendering efficiency, we introduce Binocular Batching, which jointly processes left and right eye views in a single batched pass to reduce redundant computation and support high-resolution VR displays. We evaluate VRGaussianAvatar with quantitative performance tests and a within-subject user study against image- and video-based mesh avatar baselines. Results show that VRGaussianAvatar sustains interactive VR performance and yields higher perceived appearance similarity, embodiment, and plausibility. Project page and source code are available at this https URL."
    }
  ],
  "Sensing, Interaction & Perception Lab (SIPLAB)": [
    {
      "title": "VRGaussianAvatar: Integrating 3D Gaussian Avatars into VR",
      "url": "https://arxiv.org/abs/2602.01674",
      "date": "2026-02-02",
      "authors_text": "Hail Song, Boram Yoon, Seokhwan Yang, Seoyoung Kang, Hyunjeong Kim",
      "is_highlight": false,
      "score": 37.0,
      "summary": "VRGaussianAvatar is a real-time system that creates full-body 3D Gaussian avatars in VR using HMD tracking, enhancing rendering efficiency and user experience.",
      "teaser_image": "https://arxiv.org/html/2602.01674/x2.png",
      "debug_abstract": "We present VRGaussianAvatar, an integrated system that enables real-time full-body 3D Gaussian Splatting (3DGS) avatars in virtual reality using only head-mounted display (HMD) tracking signals. The system adopts a parallel pipeline with a VR Frontend and a GA Backend. The VR Frontend uses inverse kinematics to estimate full-body pose and streams the resulting pose along with stereo camera parameters to the backend. The GA Backend stereoscopically renders a 3DGS avatar reconstructed from a single image. To improve stereo rendering efficiency, we introduce Binocular Batching, which jointly processes left and right eye views in a single batched pass to reduce redundant computation and support high-resolution VR displays. We evaluate VRGaussianAvatar with quantitative performance tests and a within-subject user study against image- and video-based mesh avatar baselines. Results show that VRGaussianAvatar sustains interactive VR performance and yields higher perceived appearance similarity, embodiment, and plausibility. Project page and source code are available at this https URL."
    }
  ],
  "多智能体与具身智能研究所": [
    {
      "title": "Tilt-Ropter: A Novel Hybrid Aerial and Terrestrial Vehicle with Tilt Rotors and Passive Wheels",
      "url": "https://arxiv.org/abs/2602.01700",
      "date": "2026-02-02",
      "authors_text": "Ruoyu Wang, Xuchen Liu, Zongzhou Wu, Zixuan Guo, Wendi Ding",
      "is_highlight": false,
      "score": 38.0,
      "summary": "The Tilt-Ropter is a hybrid aerial-terrestrial vehicle that utilizes tilt rotors and passive wheels for efficient locomotion, featuring advanced control systems for enhanced mobility and energy savings.",
      "teaser_image": "https://arxiv.org/html/2602.01700/x2.png",
      "debug_abstract": "In this work, we present Tilt-Ropter, a novel hybrid aerial-terrestrial vehicle (HATV) that combines tilt rotors with passive wheels to achieve energy-efficient multi-mode locomotion. Unlike existing under-actuated HATVs, the fully actuated design of Tilt-Ropter enables decoupled force and torque control, greatly enhancing its mobility and environmental adaptability. A nonlinear model predictive controller (NMPC) is developed to track reference trajectories and handle contact constraints across locomotion modes, while a dedicated control allocation module exploits actuation redundancy to achieve energy-efficient control of actuators. Additionally, to enhance robustness during ground contact, we introduce an external wrench estimation algorithm that estimates environmental interaction forces and torques in real time. The system is validated through both simulation and real-world experiments, including seamless air-ground transitions and trajectory tracking. Results show low tracking errors in both modes and highlight a 92.8% reduction in power consumption during ground locomotion, demonstrating the system&#39;s potential for long-duration missions across large-scale and energy-constrained environments."
    }
  ],
  "高性能人形技术实验室 (H²T)": [
    {
      "title": "Efficient Cross-Country Data Acquisition Strategy for ADAS via Street-View Imagery",
      "url": "https://arxiv.org/abs/2602.01836",
      "date": "2026-02-02",
      "authors_text": "Yin Wu, Daniel Slieter, Carl Esselborn, Ahmed Abouelazm, Tsung Yuan Tseng",
      "is_highlight": false,
      "score": 34.0,
      "summary": "The paper presents a street-view-guided data acquisition strategy for ADAS, enhancing cross-country model adaptation efficiency and cost-effectiveness through innovative POI scoring methods.",
      "teaser_image": "https://arxiv.org/html/2602.01836/x1.png",
      "debug_abstract": "Deploying ADAS and ADS across countries remains challenging due to differences in legislation, traffic infrastructure, and visual conventions, which introduce domain shifts that degrade perception performance. Traditional cross-country data collection relies on extensive on-road driving, making it costly and inefficient to identify representative locations. To address this, we propose a street-view-guided data acquisition strategy that leverages publicly available imagery to identify places of interest (POI). Two POI scoring methods are introduced: a KNN-based feature distance approach using a vision foundation model, and a visual-attribution approach using a vision-language model. To enable repeatable evaluation, we adopt a collect-detect protocol and construct a co-located dataset by pairing the Zenseact Open Dataset with Mapillary street-view images. Experiments on traffic sign detection, a task particularly sensitive to cross-country variations in sign appearance, show that our approach achieves performance comparable to random sampling while using only half of the target-domain data. We further provide cost estimations for full-country analysis, demonstrating that large-scale street-view processing remains economically feasible. These results highlight the potential of street-view-guided data acquisition for efficient and cost-effective cross-country model adaptation."
    }
  ],
  "Berkeley Artificial Intelligence Research Lab (BAIR)": [
    {
      "title": "Flow Policy Gradients for Robot Control",
      "url": "https://arxiv.org/abs/2602.02481",
      "date": "2026-02-02",
      "authors_text": "Brent Yi, Hongsuk Choi, Himanshu Gaurav Singh, Xiaoyu Huang, Takara E. Truong",
      "is_highlight": false,
      "score": 13.0,
      "summary": "This paper introduces flow matching policy gradients for training expressive robot control policies, enhancing performance in locomotion, manipulation, and sim-to-real transfer tasks.",
      "teaser_image": "https://arxiv.org/html/2602.02481/x11.png",
      "debug_abstract": "Likelihood-based policy gradient methods are the dominant approach for training robot control policies from rewards. These methods rely on differentiable action likelihoods, which constrain policy outputs to simple distributions like Gaussians. In this work, we show how flow matching policy gradients -- a recent framework that bypasses likelihood computation -- can be made effective for training and fine-tuning more expressive policies in challenging robot control settings. We introduce an improved objective that enables success in legged locomotion, humanoid motion tracking, and manipulation tasks, as well as robust sim-to-real transfer on two humanoid robots. We then present ablations and analysis on training dynamics. Results show how policies can exploit the flow representation for exploration when training from scratch, as well as improved fine-tuning robustness over baselines."
    },
    {
      "title": "ROMA: Recursive Open Meta-Agent Framework for Long-Horizon Multi-Agent Systems",
      "url": "https://arxiv.org/abs/2602.01848",
      "date": "2026-02-02",
      "authors_text": "Salaheddin Alzu'bi, Baran Nama, Arda Kaz, Anushri Eswaran, Weiyuan Chen",
      "is_highlight": false,
      "score": 31.0,
      "summary": "ROMA is a recursive framework that enhances long-horizon multi-agent systems by decomposing tasks into parallel subtasks, improving performance and interpretability without fine-tuning.",
      "teaser_image": "https://arxiv.org/html/2602.01848/x2.png",
      "debug_abstract": "Current agentic frameworks underperform on long-horizon tasks. As reasoning depth increases, sequential orchestration becomes brittle, context windows impose hard limits that degrade performance, and opaque execution traces make failures difficult to localize or debug. We introduce ROMA (Recursive Open Meta-Agents), a domain-agnostic framework that addresses these limitations through recursive task decomposition and structured aggregation. ROMA decomposes goals into dependency-aware subtask trees that can be executed in parallel, while aggregation compresses and validates intermediate results to control context growth. Our framework standardizes agent construction around four modular roles --Atomizer (which decides whether a task should be decomposed), Planner, Executor, and Aggregator -- which cleanly separate orchestration from model selection and enable transparent, hierarchical execution traces. This design supports heterogeneous multi-agent systems that mix models and tools according to cost, latency, and capability. To adapt ROMA to specific tasks without fine-tuning, we further introduce GEPA$+$, an improved Genetic-Pareto prompt proposer that searches over prompts within ROMA&#39;s component hierarchy while preserving interface contracts. We show that ROMA, combined with GEPA+, delivers leading system-level performance on reasoning and long-form generation benchmarks. On SEAL-0, which evaluates reasoning over conflicting web evidence, ROMA instantiated with GLM-4.6 improves accuracy by 9.9\\% over Kimi-Researcher. On EQ-Bench, a long-form writing benchmark, ROMA enables DeepSeek-V3 to match the performance of leading closed-source models such as Claude Sonnet 4.5. Our results demonstrate that recursive, modular agent architectures can scale reasoning depth while remaining interpretable, flexible, and model-agnostic."
    }
  ],
  "Robot Perception and Learning Lab": [
    {
      "title": "SanD-Planner: Sample-Efficient Diffusion Planner in B-Spline Space for Robust Local Navigation",
      "url": "https://arxiv.org/abs/2602.00923",
      "date": "2026-01-31",
      "authors_text": "Jincheng Wang, Lingfan Bao, Tong Yang, Diego Martinez Plasencia, Jianhao Jiao",
      "is_highlight": false,
      "score": 10.0,
      "summary": "SanD-Planner introduces a sample-efficient diffusion-based local planner using B-spline space, achieving high success rates in cluttered environments with minimal training data and zero-shot transferability.",
      "teaser_image": "https://arxiv.org/html/2602.00923/Images/overview_new.png",
      "debug_abstract": "The challenge of generating reliable local plans has long hindered practical applications in highly cluttered and dynamic environments. Key fundamental bottlenecks include acquiring large-scale expert demonstrations across diverse scenes and improving learning efficiency with limited data. This paper proposes SanD-Planner, a sample-efficient diffusion-based local planner that conducts depth image-based imitation learning within the clamped B-spline space. By operating within this compact space, the proposed algorithm inherently yields smooth outputs with bounded prediction errors over local supports, naturally aligning with receding-horizon execution. Integration of an ESDF-based safety checker with explicit clearance and time-to-completion metrics further reduces the training burden associated with value-function learning for feasibility assessment. Experiments show that training with $500$ episodes (merely $0.25\\%$ of the demonstration scale used by the baseline), SanD-Planner achieves state-of-the-art performance on the evaluated open benchmark, attaining success rates of $90.1\\%$ in simulated cluttered environments and $72.0\\%$ in indoor simulations. The performance is further proven by demonstrating zero-shot transferability to realistic experimentation in both 2D and 3D scenes. The dataset and pre-trained models will also be open-sourced."
    },
    {
      "title": "See2Refine: Vision-Language Feedback Improves LLM-Based eHMI Action Designers",
      "url": "https://arxiv.org/abs/2602.02063",
      "date": "2026-02-02",
      "authors_text": "Ding Xia, Xinyue Gui, Mark Colley, Fan Gao, Zhongyi Zhou",
      "is_highlight": false,
      "score": 23.0,
      "summary": "See2Refine enhances LLM-based eHMI action designers by utilizing vision-language model feedback for automated, context-adaptive refinement, outperforming traditional methods in various evaluations.",
      "teaser_image": "https://arxiv.org/html/2602.02063/x2.png",
      "debug_abstract": "Automated vehicles lack natural communication channels with other road users, making external Human-Machine Interfaces (eHMIs) essential for conveying intent and maintaining trust in shared environments. However, most eHMI studies rely on developer-crafted message-action pairs, which are difficult to adapt to diverse and dynamic traffic contexts. A promising alternative is to use Large Language Models (LLMs) as action designers that generate context-conditioned eHMI actions, yet such designers lack perceptual verification and typically depend on fixed prompts or costly human-annotated feedback for improvement. We present See2Refine, a human-free, closed-loop framework that uses vision-language model (VLM) perceptual evaluation as automated visual feedback to improve an LLM-based eHMI action designer. Given a driving context and a candidate eHMI action, the VLM evaluates the perceived appropriateness of the action, and this feedback is used to iteratively revise the designer&#39;s outputs, enabling systematic refinement without human supervision. We evaluate our framework across three eHMI modalities (lightbar, eyes, and arm) and multiple LLM model sizes. Across settings, our framework consistently outperforms prompt-only LLM designers and manually specified baselines in both VLM-based metrics and human-subject evaluations. Results further indicate that the improvements generalize across modalities and that VLM evaluations are well aligned with human preferences, supporting the robustness and effectiveness of See2Refine for scalable action design."
    }
  ],
  "上海人工智能实验室": [
    {
      "title": "TIDE: Trajectory-based Diagnostic Evaluation of Test-Time Improvement in LLM Agents",
      "url": "https://arxiv.org/abs/2602.02196",
      "date": "2026-02-02",
      "authors_text": "Hang Yan, Xinyu Che, Fangzhi Xu, Qiushi Sun, Zichen Ding",
      "is_highlight": false,
      "score": 20.0,
      "summary": "The TIDE framework evaluates Test-Time Improvement in LLM agents by analyzing task completion dynamics, recursive behaviors, and memory constraints to enhance performance optimization.",
      "teaser_image": "https://arxiv.org/html/2602.02196/x1.png",
      "debug_abstract": "Recent advances in autonomous LLM agents demonstrate their ability to improve performance through iterative interaction with the environment. We define this paradigm as Test-Time Improvement (TTI). However, the mechanisms under how and why TTI succeed or fail remain poorly understood, and existing evaluation metrics fail to capture their task optimization efficiency, behavior adaptation after erroneous actions, and the specific utility of working memory for task completion. To address these gaps, we propose Test-time Improvement Diagnostic Evaluation (TIDE), an agent-agnostic and environment-agnostic framework that decomposes TTI into three comprehensive and interconnected dimensions. The framework measures (1) the overall temporal dynamics of task completion and (2) identifies whether performance is primarily constrained by recursive looping behaviors or (3) by burdensome accumulated memory. Through extensive experiments across diverse agents and environments, TIDE highlights that improving agent performance requires more than scaling internal reasoning, calling for explicitly optimizing the interaction dynamics between the agent and the environment."
    }
  ],
  "上海科技大学": [
    {
      "title": "Interpreting and Controlling LLM Reasoning through Integrated Policy Gradient",
      "url": "https://arxiv.org/abs/2602.02313",
      "date": "2026-02-02",
      "authors_text": "Changming Li, Kaixing Zhang, Haoyun Xu, Yingdong Shi, Zheng Zhang",
      "is_highlight": false,
      "score": 19.0,
      "summary": "The paper introduces Integrated Policy Gradient (IPG), a framework that enhances the interpretability and control of large language model reasoning by tracing sequential contributions of internal components to outcomes.",
      "teaser_image": "https://arxiv.org/html/2602.02313/x1.png",
      "debug_abstract": "Large language models (LLMs) demonstrate strong reasoning abilities in solving complex real-world problems. Yet, the internal mechanisms driving these complex reasoning behaviors remain opaque. Existing interpretability approaches targeting reasoning either identify components (e.g., neurons) correlated with special textual patterns, or rely on human-annotated contrastive pairs to derive control vectors. Consequently, current methods struggle to precisely localize complex reasoning mechanisms or capture sequential influence from model internal workings to the reasoning outputs. In this paper, built on outcome-oriented and sequential-influence-aware principles, we focus on identifying components that have sequential contribution to reasoning behavior where outcomes are cumulated by long-range effects. We propose Integrated Policy Gradient (IPG), a novel framework that attributes reasoning behaviors to model&#39;s inner components by propagating compound outcome-based signals such as post reasoning accuracy backward through model inference trajectories. Empirical evaluations demonstrate that our approach achieves more precise localization and enables reliable modulation of reasoning behaviors (e.g., reasoning capability, reasoning strength) across diverse reasoning models."
    }
  ],
  "Harvard Microrobotics Laboratory": [
    {
      "title": "FD-VLA: Force-Distilled Vision-Language-Action Model for Contact-Rich Manipulation",
      "url": "https://arxiv.org/abs/2602.02142",
      "date": "2026-02-02",
      "authors_text": "Ruiteng Zhao, Wenshuo Wang, Yicheng Ma, Xiaocong Li, Francis E. H. Tay",
      "is_highlight": false,
      "score": 0,
      "summary": "The FD-VLA framework enhances contact-rich manipulation by integrating force awareness through a novel distillation method, improving performance without physical force sensors.",
      "teaser_image": "https://arxiv.org/html/2602.02142/x1.png",
      "debug_abstract": "Force sensing is a crucial modality for Vision-Language-Action (VLA) frameworks, as it enables fine-grained perception and dexterous manipulation in contact-rich tasks. We present Force-Distilled VLA (FD-VLA), a novel framework that integrates force awareness into contact-rich manipulation without relying on physical force sensors. The core of our approach is a Force Distillation Module (FDM), which distills force by mapping a learnable query token, conditioned on visual observations and robot states, into a predicted force token aligned with the latent representation of actual force signals. During inference, this distilled force token is injected into the pretrained VLM, enabling force-aware reasoning while preserving the integrity of its vision-language semantics. This design provides two key benefits: first, it allows practical deployment across a wide range of robots that lack expensive or fragile force-torque sensors, thereby reducing hardware cost and complexity; second, the FDM introduces an additional force-vision-state fusion prior to the VLM, which improves cross-modal alignment and enhances perception-action robustness in contact-rich scenarios. Surprisingly, our physical experiments show that the distilled force token outperforms direct sensor force measurements as well as other baselines, which highlights the effectiveness of this force-distilled VLA approach."
    }
  ],
  "Rajpurkar Lab": [
    {
      "title": "FD-VLA: Force-Distilled Vision-Language-Action Model for Contact-Rich Manipulation",
      "url": "https://arxiv.org/abs/2602.02142",
      "date": "2026-02-02",
      "authors_text": "Ruiteng Zhao, Wenshuo Wang, Yicheng Ma, Xiaocong Li, Francis E. H. Tay",
      "is_highlight": false,
      "score": 0,
      "summary": "The FD-VLA framework enhances contact-rich manipulation by integrating force awareness through a novel distillation method, improving performance without physical force sensors.",
      "teaser_image": "https://arxiv.org/html/2602.02142/x1.png",
      "debug_abstract": "Force sensing is a crucial modality for Vision-Language-Action (VLA) frameworks, as it enables fine-grained perception and dexterous manipulation in contact-rich tasks. We present Force-Distilled VLA (FD-VLA), a novel framework that integrates force awareness into contact-rich manipulation without relying on physical force sensors. The core of our approach is a Force Distillation Module (FDM), which distills force by mapping a learnable query token, conditioned on visual observations and robot states, into a predicted force token aligned with the latent representation of actual force signals. During inference, this distilled force token is injected into the pretrained VLM, enabling force-aware reasoning while preserving the integrity of its vision-language semantics. This design provides two key benefits: first, it allows practical deployment across a wide range of robots that lack expensive or fragile force-torque sensors, thereby reducing hardware cost and complexity; second, the FDM introduces an additional force-vision-state fusion prior to the VLM, which improves cross-modal alignment and enhances perception-action robustness in contact-rich scenarios. Surprisingly, our physical experiments show that the distilled force token outperforms direct sensor force measurements as well as other baselines, which highlights the effectiveness of this force-distilled VLA approach."
    }
  ]
}