{
  "Jun MA老师实验室": [
    {
      "title": "SPOT-Occ: Sparse Prototype-guided Transformer for Camera-based 3D Occupancy Prediction",
      "url": "https://arxiv.org/abs/2602.04240",
      "date": "2026-02-04",
      "authors_text": "Suzeyu Chen, Leheng Li, Ying-Cong Chen",
      "is_highlight": false,
      "score": 77.0,
      "summary": "SPOT-Occ introduces a prototype-guided sparse transformer decoder for efficient and accurate camera-based 3D occupancy prediction, enhancing speed and performance in autonomous vehicles.",
      "teaser_image": "https://arxiv.org/html/2602.04240/x3.png",
      "debug_abstract": "Achieving highly accurate and real-time 3D occupancy prediction from cameras is a critical requirement for the safe and practical deployment of autonomous vehicles. While this shift to sparse 3D representations solves the encoding bottleneck, it creates a new challenge for the decoder: how to efficiently aggregate information from a sparse, non-uniformly distributed set of voxel features without resorting to computationally prohibitive dense attention. In this paper, we propose a novel Prototype-based Sparse Transformer Decoder that replaces this costly interaction with an efficient, two-stage process of guided feature selection and focused aggregation. Our core idea is to make the decoder&#39;s attention prototype-guided. We achieve this through a sparse prototype selection mechanism, where each query adaptively identifies a compact set of the most salient voxel features, termed prototypes, for focused feature aggregation. To ensure this dynamic selection is stable and effective, we introduce a complementary denoising paradigm. This approach leverages ground-truth masks to provide explicit guidance, guaranteeing a consistent query-prototype association across decoder layers. Our model, dubbed SPOT-Occ, outperforms previous methods with a significant margin in speed while also improving accuracy. Source code is released at this https URL."
    },
    {
      "title": "ALORE: Autonomous Large-Object Rearrangement with a Legged Manipulator",
      "url": "https://arxiv.org/abs/2602.04214",
      "date": "2026-02-04",
      "authors_text": "Zhihai Bi, Yushan Zhang, Kai Chen, Guoyang Zhao, Yulin Li",
      "is_highlight": false,
      "score": 92.0,
      "summary": "ALORE is an autonomous system for legged manipulators that efficiently rearranges large objects using hierarchical reinforcement learning, unified interaction representation, and optimized task planning.",
      "teaser_image": "https://arxiv.org/html/2602.04214/images/1_8.jpg",
      "debug_abstract": "Endowing robots with the ability to rearrange various large and heavy objects, such as furniture, can substantially alleviate human workload. However, this task is extremely challenging due to the need to interact with diverse objects and efficiently rearrange multiple objects in complex environments while ensuring collision-free loco-manipulation. In this work, we present ALORE, an autonomous large-object rearrangement system for a legged manipulator that can rearrange various large objects across diverse scenarios. The proposed system is characterized by three main features: (i) a hierarchical reinforcement learning training pipeline for multi-object environment learning, where a high-level object velocity controller is trained on top of a low-level whole-body controller to achieve efficient and stable joint learning across multiple objects; (ii) two key modules, a unified interaction configuration representation and an object velocity estimator, that allow a single policy to regulate planar velocity of diverse objects accurately; and (iii) a task-and-motion planning framework that jointly optimizes object visitation order and object-to-target assignment, improving task efficiency while enabling online replanning. Comparisons against strong baselines show consistent superiority in policy generalization, object-velocity tracking accuracy, and multi-object rearrangement efficiency. Key modules are systematically evaluated, and extensive simulations and real-world experiments are conducted to validate the robustness and effectiveness of the entire system, which successfully completes 8 continuous loops to rearrange 32 chairs over nearly 40 minutes without a single failure, and executes long-distance autonomous rearrangement over an approximately 40 m route. The open-source packages are available at this https URL."
    },
    {
      "title": "Invariance on Manifolds: Understanding Robust Visual Representations for Place Recognition",
      "url": "https://arxiv.org/abs/2602.00841",
      "date": "2026-02-04",
      "authors_text": "Jintao Cheng, Weibin Li, Zhijian He, Jin Wu, Chi Man Vong",
      "is_highlight": false,
      "score": 70.0,
      "summary": "This paper presents a training-free Second-Order Geometric Statistics framework for robust visual place recognition, leveraging SPD manifold representations for zero-shot generalization against environmental shifts.",
      "teaser_image": "https://arxiv.org/html/2602.00841/x2.png",
      "debug_abstract": "Visual Place Recognition (VPR) demands representations robust to drastic environmental and viewpoint shifts. Current aggregation paradigms, however, either rely on data-hungry supervision or simplistic first-order statistics, often neglecting intrinsic structural correlations. In this work, we propose a Second-Order Geometric Statistics framework that inherently captures geometric stability without training. We conceptualize scenes as covariance descriptors on the Symmetric Positive Definite (SPD) manifold, where perturbations manifest as tractable congruence transformations. By leveraging geometry-aware Riemannian mappings, we project these descriptors into a linearized Euclidean embedding, effectively decoupling signal structure from noise. Our approach introduces a training-free framework built upon fixed, pre-trained backbones, achieving strong zero-shot generalization without parameter updates. Extensive experiments confirm that our method achieves highly competitive performance against state-of-the-art baselines, particularly excelling in challenging zero-shot scenarios."
    }
  ],
  "Precognition Lab": [
    {
      "title": "SPOT-Occ: Sparse Prototype-guided Transformer for Camera-based 3D Occupancy Prediction",
      "url": "https://arxiv.org/abs/2602.04240",
      "date": "2026-02-04",
      "authors_text": "Suzeyu Chen, Leheng Li, Ying-Cong Chen",
      "is_highlight": false,
      "score": 77.0,
      "summary": "SPOT-Occ introduces a prototype-guided sparse transformer decoder for efficient and accurate camera-based 3D occupancy prediction, enhancing speed and performance in autonomous vehicles.",
      "teaser_image": "https://arxiv.org/html/2602.04240/x3.png",
      "debug_abstract": "Achieving highly accurate and real-time 3D occupancy prediction from cameras is a critical requirement for the safe and practical deployment of autonomous vehicles. While this shift to sparse 3D representations solves the encoding bottleneck, it creates a new challenge for the decoder: how to efficiently aggregate information from a sparse, non-uniformly distributed set of voxel features without resorting to computationally prohibitive dense attention. In this paper, we propose a novel Prototype-based Sparse Transformer Decoder that replaces this costly interaction with an efficient, two-stage process of guided feature selection and focused aggregation. Our core idea is to make the decoder&#39;s attention prototype-guided. We achieve this through a sparse prototype selection mechanism, where each query adaptively identifies a compact set of the most salient voxel features, termed prototypes, for focused feature aggregation. To ensure this dynamic selection is stable and effective, we introduce a complementary denoising paradigm. This approach leverages ground-truth masks to provide explicit guidance, guaranteeing a consistent query-prototype association across decoder layers. Our model, dubbed SPOT-Occ, outperforms previous methods with a significant margin in speed while also improving accuracy. Source code is released at this https URL."
    },
    {
      "title": "ALORE: Autonomous Large-Object Rearrangement with a Legged Manipulator",
      "url": "https://arxiv.org/abs/2602.04214",
      "date": "2026-02-04",
      "authors_text": "Zhihai Bi, Yushan Zhang, Kai Chen, Guoyang Zhao, Yulin Li",
      "is_highlight": false,
      "score": 92.0,
      "summary": "ALORE is an autonomous system for legged manipulators that efficiently rearranges large objects using hierarchical reinforcement learning, unified interaction representation, and optimized task planning.",
      "teaser_image": "https://arxiv.org/html/2602.04214/images/1_8.jpg",
      "debug_abstract": "Endowing robots with the ability to rearrange various large and heavy objects, such as furniture, can substantially alleviate human workload. However, this task is extremely challenging due to the need to interact with diverse objects and efficiently rearrange multiple objects in complex environments while ensuring collision-free loco-manipulation. In this work, we present ALORE, an autonomous large-object rearrangement system for a legged manipulator that can rearrange various large objects across diverse scenarios. The proposed system is characterized by three main features: (i) a hierarchical reinforcement learning training pipeline for multi-object environment learning, where a high-level object velocity controller is trained on top of a low-level whole-body controller to achieve efficient and stable joint learning across multiple objects; (ii) two key modules, a unified interaction configuration representation and an object velocity estimator, that allow a single policy to regulate planar velocity of diverse objects accurately; and (iii) a task-and-motion planning framework that jointly optimizes object visitation order and object-to-target assignment, improving task efficiency while enabling online replanning. Comparisons against strong baselines show consistent superiority in policy generalization, object-velocity tracking accuracy, and multi-object rearrangement efficiency. Key modules are systematically evaluated, and extensive simulations and real-world experiments are conducted to validate the robustness and effectiveness of the entire system, which successfully completes 8 continuous loops to rearrange 32 chairs over nearly 40 minutes without a single failure, and executes long-distance autonomous rearrangement over an approximately 40 m route. The open-source packages are available at this https URL."
    },
    {
      "title": "Invariance on Manifolds: Understanding Robust Visual Representations for Place Recognition",
      "url": "https://arxiv.org/abs/2602.00841",
      "date": "2026-02-04",
      "authors_text": "Jintao Cheng, Weibin Li, Zhijian He, Jin Wu, Chi Man Vong",
      "is_highlight": false,
      "score": 70.0,
      "summary": "This paper presents a training-free Second-Order Geometric Statistics framework for robust visual place recognition, leveraging SPD manifold representations for zero-shot generalization against environmental shifts.",
      "teaser_image": "https://arxiv.org/html/2602.00841/x2.png",
      "debug_abstract": "Visual Place Recognition (VPR) demands representations robust to drastic environmental and viewpoint shifts. Current aggregation paradigms, however, either rely on data-hungry supervision or simplistic first-order statistics, often neglecting intrinsic structural correlations. In this work, we propose a Second-Order Geometric Statistics framework that inherently captures geometric stability without training. We conceptualize scenes as covariance descriptors on the Symmetric Positive Definite (SPD) manifold, where perturbations manifest as tractable congruence transformations. By leveraging geometry-aware Riemannian mappings, we project these descriptors into a linearized Euclidean embedding, effectively decoupling signal structure from noise. Our approach introduces a training-free framework built upon fixed, pre-trained backbones, achieving strong zero-shot generalization without parameter updates. Extensive experiments confirm that our method achieves highly competitive performance against state-of-the-art baselines, particularly excelling in challenging zero-shot scenarios."
    }
  ],
  "Cheng Kar-Shun Robotics Institute (CKSRI)": [
    {
      "title": "SPOT-Occ: Sparse Prototype-guided Transformer for Camera-based 3D Occupancy Prediction",
      "url": "https://arxiv.org/abs/2602.04240",
      "date": "2026-02-04",
      "authors_text": "Suzeyu Chen, Leheng Li, Ying-Cong Chen",
      "is_highlight": false,
      "score": 77.0,
      "summary": "SPOT-Occ introduces a prototype-guided sparse transformer decoder for efficient and accurate camera-based 3D occupancy prediction, enhancing speed and performance in autonomous vehicles.",
      "teaser_image": "https://arxiv.org/html/2602.04240/x3.png",
      "debug_abstract": "Achieving highly accurate and real-time 3D occupancy prediction from cameras is a critical requirement for the safe and practical deployment of autonomous vehicles. While this shift to sparse 3D representations solves the encoding bottleneck, it creates a new challenge for the decoder: how to efficiently aggregate information from a sparse, non-uniformly distributed set of voxel features without resorting to computationally prohibitive dense attention. In this paper, we propose a novel Prototype-based Sparse Transformer Decoder that replaces this costly interaction with an efficient, two-stage process of guided feature selection and focused aggregation. Our core idea is to make the decoder&#39;s attention prototype-guided. We achieve this through a sparse prototype selection mechanism, where each query adaptively identifies a compact set of the most salient voxel features, termed prototypes, for focused feature aggregation. To ensure this dynamic selection is stable and effective, we introduce a complementary denoising paradigm. This approach leverages ground-truth masks to provide explicit guidance, guaranteeing a consistent query-prototype association across decoder layers. Our model, dubbed SPOT-Occ, outperforms previous methods with a significant margin in speed while also improving accuracy. Source code is released at this https URL."
    },
    {
      "title": "ALORE: Autonomous Large-Object Rearrangement with a Legged Manipulator",
      "url": "https://arxiv.org/abs/2602.04214",
      "date": "2026-02-04",
      "authors_text": "Zhihai Bi, Yushan Zhang, Kai Chen, Guoyang Zhao, Yulin Li",
      "is_highlight": false,
      "score": 92.0,
      "summary": "ALORE is an autonomous system for legged manipulators that efficiently rearranges large objects using hierarchical reinforcement learning, unified interaction representation, and optimized task planning.",
      "teaser_image": "https://arxiv.org/html/2602.04214/images/1_8.jpg",
      "debug_abstract": "Endowing robots with the ability to rearrange various large and heavy objects, such as furniture, can substantially alleviate human workload. However, this task is extremely challenging due to the need to interact with diverse objects and efficiently rearrange multiple objects in complex environments while ensuring collision-free loco-manipulation. In this work, we present ALORE, an autonomous large-object rearrangement system for a legged manipulator that can rearrange various large objects across diverse scenarios. The proposed system is characterized by three main features: (i) a hierarchical reinforcement learning training pipeline for multi-object environment learning, where a high-level object velocity controller is trained on top of a low-level whole-body controller to achieve efficient and stable joint learning across multiple objects; (ii) two key modules, a unified interaction configuration representation and an object velocity estimator, that allow a single policy to regulate planar velocity of diverse objects accurately; and (iii) a task-and-motion planning framework that jointly optimizes object visitation order and object-to-target assignment, improving task efficiency while enabling online replanning. Comparisons against strong baselines show consistent superiority in policy generalization, object-velocity tracking accuracy, and multi-object rearrangement efficiency. Key modules are systematically evaluated, and extensive simulations and real-world experiments are conducted to validate the robustness and effectiveness of the entire system, which successfully completes 8 continuous loops to rearrange 32 chairs over nearly 40 minutes without a single failure, and executes long-distance autonomous rearrangement over an approximately 40 m route. The open-source packages are available at this https URL."
    },
    {
      "title": "Invariance on Manifolds: Understanding Robust Visual Representations for Place Recognition",
      "url": "https://arxiv.org/abs/2602.00841",
      "date": "2026-02-04",
      "authors_text": "Jintao Cheng, Weibin Li, Zhijian He, Jin Wu, Chi Man Vong",
      "is_highlight": false,
      "score": 70.0,
      "summary": "This paper presents a training-free Second-Order Geometric Statistics framework for robust visual place recognition, leveraging SPD manifold representations for zero-shot generalization against environmental shifts.",
      "teaser_image": "https://arxiv.org/html/2602.00841/x2.png",
      "debug_abstract": "Visual Place Recognition (VPR) demands representations robust to drastic environmental and viewpoint shifts. Current aggregation paradigms, however, either rely on data-hungry supervision or simplistic first-order statistics, often neglecting intrinsic structural correlations. In this work, we propose a Second-Order Geometric Statistics framework that inherently captures geometric stability without training. We conceptualize scenes as covariance descriptors on the Symmetric Positive Definite (SPD) manifold, where perturbations manifest as tractable congruence transformations. By leveraging geometry-aware Riemannian mappings, we project these descriptors into a linearized Euclidean embedding, effectively decoupling signal structure from noise. Our approach introduces a training-free framework built upon fixed, pre-trained backbones, achieving strong zero-shot generalization without parameter updates. Extensive experiments confirm that our method achieves highly competitive performance against state-of-the-art baselines, particularly excelling in challenging zero-shot scenarios."
    }
  ],
  "机器人研究所 (CKSRI)": [
    {
      "title": "SPOT-Occ: Sparse Prototype-guided Transformer for Camera-based 3D Occupancy Prediction",
      "url": "https://arxiv.org/abs/2602.04240",
      "date": "2026-02-04",
      "authors_text": "Suzeyu Chen, Leheng Li, Ying-Cong Chen",
      "is_highlight": false,
      "score": 77.0,
      "summary": "SPOT-Occ introduces a prototype-guided sparse transformer decoder for efficient and accurate camera-based 3D occupancy prediction, enhancing speed and performance in autonomous vehicles.",
      "teaser_image": "https://arxiv.org/html/2602.04240/x3.png",
      "debug_abstract": "Achieving highly accurate and real-time 3D occupancy prediction from cameras is a critical requirement for the safe and practical deployment of autonomous vehicles. While this shift to sparse 3D representations solves the encoding bottleneck, it creates a new challenge for the decoder: how to efficiently aggregate information from a sparse, non-uniformly distributed set of voxel features without resorting to computationally prohibitive dense attention. In this paper, we propose a novel Prototype-based Sparse Transformer Decoder that replaces this costly interaction with an efficient, two-stage process of guided feature selection and focused aggregation. Our core idea is to make the decoder&#39;s attention prototype-guided. We achieve this through a sparse prototype selection mechanism, where each query adaptively identifies a compact set of the most salient voxel features, termed prototypes, for focused feature aggregation. To ensure this dynamic selection is stable and effective, we introduce a complementary denoising paradigm. This approach leverages ground-truth masks to provide explicit guidance, guaranteeing a consistent query-prototype association across decoder layers. Our model, dubbed SPOT-Occ, outperforms previous methods with a significant margin in speed while also improving accuracy. Source code is released at this https URL."
    },
    {
      "title": "ALORE: Autonomous Large-Object Rearrangement with a Legged Manipulator",
      "url": "https://arxiv.org/abs/2602.04214",
      "date": "2026-02-04",
      "authors_text": "Zhihai Bi, Yushan Zhang, Kai Chen, Guoyang Zhao, Yulin Li",
      "is_highlight": false,
      "score": 92.0,
      "summary": "ALORE is an autonomous system for legged manipulators that efficiently rearranges large objects using hierarchical reinforcement learning, unified interaction representation, and optimized task planning.",
      "teaser_image": "https://arxiv.org/html/2602.04214/images/1_8.jpg",
      "debug_abstract": "Endowing robots with the ability to rearrange various large and heavy objects, such as furniture, can substantially alleviate human workload. However, this task is extremely challenging due to the need to interact with diverse objects and efficiently rearrange multiple objects in complex environments while ensuring collision-free loco-manipulation. In this work, we present ALORE, an autonomous large-object rearrangement system for a legged manipulator that can rearrange various large objects across diverse scenarios. The proposed system is characterized by three main features: (i) a hierarchical reinforcement learning training pipeline for multi-object environment learning, where a high-level object velocity controller is trained on top of a low-level whole-body controller to achieve efficient and stable joint learning across multiple objects; (ii) two key modules, a unified interaction configuration representation and an object velocity estimator, that allow a single policy to regulate planar velocity of diverse objects accurately; and (iii) a task-and-motion planning framework that jointly optimizes object visitation order and object-to-target assignment, improving task efficiency while enabling online replanning. Comparisons against strong baselines show consistent superiority in policy generalization, object-velocity tracking accuracy, and multi-object rearrangement efficiency. Key modules are systematically evaluated, and extensive simulations and real-world experiments are conducted to validate the robustness and effectiveness of the entire system, which successfully completes 8 continuous loops to rearrange 32 chairs over nearly 40 minutes without a single failure, and executes long-distance autonomous rearrangement over an approximately 40 m route. The open-source packages are available at this https URL."
    },
    {
      "title": "Invariance on Manifolds: Understanding Robust Visual Representations for Place Recognition",
      "url": "https://arxiv.org/abs/2602.00841",
      "date": "2026-02-04",
      "authors_text": "Jintao Cheng, Weibin Li, Zhijian He, Jin Wu, Chi Man Vong",
      "is_highlight": false,
      "score": 70.0,
      "summary": "This paper presents a training-free Second-Order Geometric Statistics framework for robust visual place recognition, leveraging SPD manifold representations for zero-shot generalization against environmental shifts.",
      "teaser_image": "https://arxiv.org/html/2602.00841/x2.png",
      "debug_abstract": "Visual Place Recognition (VPR) demands representations robust to drastic environmental and viewpoint shifts. Current aggregation paradigms, however, either rely on data-hungry supervision or simplistic first-order statistics, often neglecting intrinsic structural correlations. In this work, we propose a Second-Order Geometric Statistics framework that inherently captures geometric stability without training. We conceptualize scenes as covariance descriptors on the Symmetric Positive Definite (SPD) manifold, where perturbations manifest as tractable congruence transformations. By leveraging geometry-aware Riemannian mappings, we project these descriptors into a linearized Euclidean embedding, effectively decoupling signal structure from noise. Our approach introduces a training-free framework built upon fixed, pre-trained backbones, achieving strong zero-shot generalization without parameter updates. Extensive experiments confirm that our method achieves highly competitive performance against state-of-the-art baselines, particularly excelling in challenging zero-shot scenarios."
    }
  ],
  "范明明老师实验室": [
    {
      "title": "SPOT-Occ: Sparse Prototype-guided Transformer for Camera-based 3D Occupancy Prediction",
      "url": "https://arxiv.org/abs/2602.04240",
      "date": "2026-02-04",
      "authors_text": "Suzeyu Chen, Leheng Li, Ying-Cong Chen",
      "is_highlight": false,
      "score": 77.0,
      "summary": "SPOT-Occ introduces a prototype-guided sparse transformer decoder for efficient and accurate camera-based 3D occupancy prediction, enhancing speed and performance in autonomous vehicles.",
      "teaser_image": "https://arxiv.org/html/2602.04240/x3.png",
      "debug_abstract": "Achieving highly accurate and real-time 3D occupancy prediction from cameras is a critical requirement for the safe and practical deployment of autonomous vehicles. While this shift to sparse 3D representations solves the encoding bottleneck, it creates a new challenge for the decoder: how to efficiently aggregate information from a sparse, non-uniformly distributed set of voxel features without resorting to computationally prohibitive dense attention. In this paper, we propose a novel Prototype-based Sparse Transformer Decoder that replaces this costly interaction with an efficient, two-stage process of guided feature selection and focused aggregation. Our core idea is to make the decoder&#39;s attention prototype-guided. We achieve this through a sparse prototype selection mechanism, where each query adaptively identifies a compact set of the most salient voxel features, termed prototypes, for focused feature aggregation. To ensure this dynamic selection is stable and effective, we introduce a complementary denoising paradigm. This approach leverages ground-truth masks to provide explicit guidance, guaranteeing a consistent query-prototype association across decoder layers. Our model, dubbed SPOT-Occ, outperforms previous methods with a significant margin in speed while also improving accuracy. Source code is released at this https URL."
    },
    {
      "title": "ALORE: Autonomous Large-Object Rearrangement with a Legged Manipulator",
      "url": "https://arxiv.org/abs/2602.04214",
      "date": "2026-02-04",
      "authors_text": "Zhihai Bi, Yushan Zhang, Kai Chen, Guoyang Zhao, Yulin Li",
      "is_highlight": false,
      "score": 92.0,
      "summary": "ALORE is an autonomous system for legged manipulators that efficiently rearranges large objects using hierarchical reinforcement learning, unified interaction representation, and optimized task planning.",
      "teaser_image": "https://arxiv.org/html/2602.04214/images/1_8.jpg",
      "debug_abstract": "Endowing robots with the ability to rearrange various large and heavy objects, such as furniture, can substantially alleviate human workload. However, this task is extremely challenging due to the need to interact with diverse objects and efficiently rearrange multiple objects in complex environments while ensuring collision-free loco-manipulation. In this work, we present ALORE, an autonomous large-object rearrangement system for a legged manipulator that can rearrange various large objects across diverse scenarios. The proposed system is characterized by three main features: (i) a hierarchical reinforcement learning training pipeline for multi-object environment learning, where a high-level object velocity controller is trained on top of a low-level whole-body controller to achieve efficient and stable joint learning across multiple objects; (ii) two key modules, a unified interaction configuration representation and an object velocity estimator, that allow a single policy to regulate planar velocity of diverse objects accurately; and (iii) a task-and-motion planning framework that jointly optimizes object visitation order and object-to-target assignment, improving task efficiency while enabling online replanning. Comparisons against strong baselines show consistent superiority in policy generalization, object-velocity tracking accuracy, and multi-object rearrangement efficiency. Key modules are systematically evaluated, and extensive simulations and real-world experiments are conducted to validate the robustness and effectiveness of the entire system, which successfully completes 8 continuous loops to rearrange 32 chairs over nearly 40 minutes without a single failure, and executes long-distance autonomous rearrangement over an approximately 40 m route. The open-source packages are available at this https URL."
    },
    {
      "title": "Invariance on Manifolds: Understanding Robust Visual Representations for Place Recognition",
      "url": "https://arxiv.org/abs/2602.00841",
      "date": "2026-02-04",
      "authors_text": "Jintao Cheng, Weibin Li, Zhijian He, Jin Wu, Chi Man Vong",
      "is_highlight": false,
      "score": 70.0,
      "summary": "This paper presents a training-free Second-Order Geometric Statistics framework for robust visual place recognition, leveraging SPD manifold representations for zero-shot generalization against environmental shifts.",
      "teaser_image": "https://arxiv.org/html/2602.00841/x2.png",
      "debug_abstract": "Visual Place Recognition (VPR) demands representations robust to drastic environmental and viewpoint shifts. Current aggregation paradigms, however, either rely on data-hungry supervision or simplistic first-order statistics, often neglecting intrinsic structural correlations. In this work, we propose a Second-Order Geometric Statistics framework that inherently captures geometric stability without training. We conceptualize scenes as covariance descriptors on the Symmetric Positive Definite (SPD) manifold, where perturbations manifest as tractable congruence transformations. By leveraging geometry-aware Riemannian mappings, we project these descriptors into a linearized Euclidean embedding, effectively decoupling signal structure from noise. Our approach introduces a training-free framework built upon fixed, pre-trained backbones, achieving strong zero-shot generalization without parameter updates. Extensive experiments confirm that our method achieves highly competitive performance against state-of-the-art baselines, particularly excelling in challenging zero-shot scenarios."
    }
  ],
  "机器人研究所": [
    {
      "title": "SPOT-Occ: Sparse Prototype-guided Transformer for Camera-based 3D Occupancy Prediction",
      "url": "https://arxiv.org/abs/2602.04240",
      "date": "2026-02-04",
      "authors_text": "Suzeyu Chen, Leheng Li, Ying-Cong Chen",
      "is_highlight": false,
      "score": 77.0,
      "summary": "SPOT-Occ introduces a prototype-guided sparse transformer decoder for efficient and accurate camera-based 3D occupancy prediction, enhancing speed and performance in autonomous vehicles.",
      "teaser_image": "https://arxiv.org/html/2602.04240/x3.png",
      "debug_abstract": "Achieving highly accurate and real-time 3D occupancy prediction from cameras is a critical requirement for the safe and practical deployment of autonomous vehicles. While this shift to sparse 3D representations solves the encoding bottleneck, it creates a new challenge for the decoder: how to efficiently aggregate information from a sparse, non-uniformly distributed set of voxel features without resorting to computationally prohibitive dense attention. In this paper, we propose a novel Prototype-based Sparse Transformer Decoder that replaces this costly interaction with an efficient, two-stage process of guided feature selection and focused aggregation. Our core idea is to make the decoder&#39;s attention prototype-guided. We achieve this through a sparse prototype selection mechanism, where each query adaptively identifies a compact set of the most salient voxel features, termed prototypes, for focused feature aggregation. To ensure this dynamic selection is stable and effective, we introduce a complementary denoising paradigm. This approach leverages ground-truth masks to provide explicit guidance, guaranteeing a consistent query-prototype association across decoder layers. Our model, dubbed SPOT-Occ, outperforms previous methods with a significant margin in speed while also improving accuracy. Source code is released at this https URL."
    },
    {
      "title": "ALORE: Autonomous Large-Object Rearrangement with a Legged Manipulator",
      "url": "https://arxiv.org/abs/2602.04214",
      "date": "2026-02-04",
      "authors_text": "Zhihai Bi, Yushan Zhang, Kai Chen, Guoyang Zhao, Yulin Li",
      "is_highlight": false,
      "score": 92.0,
      "summary": "ALORE is an autonomous system for legged manipulators that efficiently rearranges large objects using hierarchical reinforcement learning, unified interaction representation, and optimized task planning.",
      "teaser_image": "https://arxiv.org/html/2602.04214/images/1_8.jpg",
      "debug_abstract": "Endowing robots with the ability to rearrange various large and heavy objects, such as furniture, can substantially alleviate human workload. However, this task is extremely challenging due to the need to interact with diverse objects and efficiently rearrange multiple objects in complex environments while ensuring collision-free loco-manipulation. In this work, we present ALORE, an autonomous large-object rearrangement system for a legged manipulator that can rearrange various large objects across diverse scenarios. The proposed system is characterized by three main features: (i) a hierarchical reinforcement learning training pipeline for multi-object environment learning, where a high-level object velocity controller is trained on top of a low-level whole-body controller to achieve efficient and stable joint learning across multiple objects; (ii) two key modules, a unified interaction configuration representation and an object velocity estimator, that allow a single policy to regulate planar velocity of diverse objects accurately; and (iii) a task-and-motion planning framework that jointly optimizes object visitation order and object-to-target assignment, improving task efficiency while enabling online replanning. Comparisons against strong baselines show consistent superiority in policy generalization, object-velocity tracking accuracy, and multi-object rearrangement efficiency. Key modules are systematically evaluated, and extensive simulations and real-world experiments are conducted to validate the robustness and effectiveness of the entire system, which successfully completes 8 continuous loops to rearrange 32 chairs over nearly 40 minutes without a single failure, and executes long-distance autonomous rearrangement over an approximately 40 m route. The open-source packages are available at this https URL."
    },
    {
      "title": "Invariance on Manifolds: Understanding Robust Visual Representations for Place Recognition",
      "url": "https://arxiv.org/abs/2602.00841",
      "date": "2026-02-04",
      "authors_text": "Jintao Cheng, Weibin Li, Zhijian He, Jin Wu, Chi Man Vong",
      "is_highlight": false,
      "score": 70.0,
      "summary": "This paper presents a training-free Second-Order Geometric Statistics framework for robust visual place recognition, leveraging SPD manifold representations for zero-shot generalization against environmental shifts.",
      "teaser_image": "https://arxiv.org/html/2602.00841/x2.png",
      "debug_abstract": "Visual Place Recognition (VPR) demands representations robust to drastic environmental and viewpoint shifts. Current aggregation paradigms, however, either rely on data-hungry supervision or simplistic first-order statistics, often neglecting intrinsic structural correlations. In this work, we propose a Second-Order Geometric Statistics framework that inherently captures geometric stability without training. We conceptualize scenes as covariance descriptors on the Symmetric Positive Definite (SPD) manifold, where perturbations manifest as tractable congruence transformations. By leveraging geometry-aware Riemannian mappings, we project these descriptors into a linearized Euclidean embedding, effectively decoupling signal structure from noise. Our approach introduces a training-free framework built upon fixed, pre-trained backbones, achieving strong zero-shot generalization without parameter updates. Extensive experiments confirm that our method achieves highly competitive performance against state-of-the-art baselines, particularly excelling in challenging zero-shot scenarios."
    }
  ],
  "卡内基-机器人研究所": [
    {
      "title": "Robot-Assisted Group Tours for Blind People",
      "url": "https://arxiv.org/abs/2602.04458",
      "date": "2026-02-04",
      "authors_text": "Yaxin Hu, Masaki Kuribayashi, Allan Wang, Seita Kayukawa, Daisuke Sato",
      "is_highlight": false,
      "score": 86.0,
      "summary": "This paper explores a mobile robot's role in facilitating group interactions for blind individuals during guided tours, highlighting design implications from user feedback.",
      "teaser_image": "https://arxiv.org/html/2602.04458/Figures/teaser_ver5.png",
      "debug_abstract": "Group interactions are essential to social functioning, yet effective engagement relies on the ability to recognize and interpret visual cues, making such engagement a significant challenge for blind people. In this paper, we investigate how a mobile robot can support group interactions for blind people. We used the scenario of a guided tour with mixed-visual groups involving blind and sighted visitors. Based on insights from an interview study with blind people (n=5) and museum experts (n=5), we designed and prototyped a robotic system that supported blind visitors to join group tours. We conducted a field study in a science museum where each blind participant (n=8) joined a group tour with one guide and two sighted participants (n=8). Findings indicated users&#39; sense of safety from the robot&#39;s navigational support, concerns in the group participation, and preferences for obtaining environmental information. We present design implications for future robotic systems to support blind people&#39;s mixed-visual group participation."
    },
    {
      "title": "InterPReT: Interactive Policy Restructuring and Training Enable Effective Imitation Learning from Laypersons",
      "url": "https://arxiv.org/abs/2602.04213",
      "date": "2026-02-04",
      "authors_text": "Feiyu Gavin Zhu, Jean Oh, Reid Simmons",
      "is_highlight": false,
      "score": 81.0,
      "summary": "InterPReT enables laypersons to effectively teach AI agents through interactive policy updates and user-guided demonstrations, improving imitation learning without requiring technical expertise.",
      "teaser_image": "https://arxiv.org/html/2602.04213/x2.png",
      "debug_abstract": "Imitation learning has shown success in many tasks by learning from expert demonstrations. However, most existing work relies on large-scale demonstrations from technical professionals and close monitoring of the training process. These are challenging for a layperson when they want to teach the agent new skills. To lower the barrier of teaching AI agents, we propose Interactive Policy Restructuring and Training (InterPReT), which takes user instructions to continually update the policy structure and optimize its parameters to fit user demonstrations. This enables end-users to interactively give instructions and demonstrations, monitor the agent&#39;s performance, and review the agent&#39;s decision-making strategies. A user study (N=34) on teaching an AI agent to drive in a racing game confirms that our approach yields more robust policies without impairing system usability, compared to a generic imitation learning baseline, when a layperson is responsible for both giving demonstrations and determining when to stop. This shows that our method is more suitable for end-users without much technical background in machine learning to train a dependable policy"
    },
    {
      "title": "AgentArk: Distilling Multi-Agent Intelligence into a Single LLM Agent",
      "url": "https://arxiv.org/abs/2602.03955",
      "date": "2026-02-03",
      "authors_text": "Yinyi Luo, Yiqiao Jin, Weichen Yu, Mengqi Zhang, Srijan Kumar",
      "is_highlight": false,
      "score": 75.0,
      "summary": "AgentArk distills multi-agent intelligence into a single LLM, enhancing reasoning and efficiency through hierarchical distillation strategies while minimizing computational costs.",
      "teaser_image": "https://arxiv.org/html/2602.03955/x2.png",
      "debug_abstract": "While large language model (LLM) multi-agent systems achieve superior reasoning performance through iterative debate, practical deployment is limited by their high computational cost and error propagation. This paper proposes AgentArk, a novel framework to distill multi-agent dynamics into the weights of a single model, effectively transforming explicit test-time interactions into implicit model capabilities. This equips a single agent with the intelligence of multi-agent systems while remaining computationally efficient. Specifically, we investigate three hierarchical distillation strategies across various models, tasks, scaling, and scenarios: reasoning-enhanced fine-tuning; trajectory-based augmentation; and process-aware distillation. By shifting the burden of computation from inference to training, the distilled models preserve the efficiency of one agent while exhibiting strong reasoning and self-correction performance of multiple agents. They further demonstrate enhanced robustness and generalization across diverse reasoning tasks. We hope this work can shed light on future research on efficient and robust multi-agent development. Our code is at this https URL."
    }
  ],
  "斯坦福-人工智能实验室 (SAIL)": [
    {
      "title": "PerpetualWonder: Long-Horizon Action-Conditioned 4D Scene Generation",
      "url": "https://arxiv.org/abs/2602.04876",
      "date": "2026-02-04",
      "authors_text": "Jiahao Zhan, Zizhang Li, Hong-Xing Yu, Jiajun Wu",
      "is_highlight": true,
      "score": 79.0,
      "summary": "PerpetualWonder is a hybrid simulator that enables long-horizon, action-conditioned 4D scene generation from a single image by linking physical states with visual representations.",
      "teaser_image": "https://arxiv.org/html/2602.04876/x2.png",
      "debug_abstract": "We introduce PerpetualWonder, a hybrid generative simulator that enables long-horizon, action-conditioned 4D scene generation from a single image. Current works fail at this task because their physical state is decoupled from their visual representation, which prevents generative refinements to update the underlying physics for subsequent interactions. PerpetualWonder solves this by introducing the first true closed-loop system. It features a novel unified representation that creates a bidirectional link between the physical state and visual primitives, allowing generative refinements to correct both the dynamics and appearance. It also introduces a robust update mechanism that gathers supervision from multiple viewpoints to resolve optimization ambiguity. Experiments demonstrate that from a single image, PerpetualWonder can successfully simulate complex, multi-step interactions from long-horizon actions, maintaining physical plausibility and visual consistency."
    },
    {
      "title": "Active Epistemic Control for Query-Efficient Verified Planning",
      "url": "https://arxiv.org/abs/2602.03974",
      "date": "2026-02-03",
      "authors_text": "Shuhui Qu",
      "is_highlight": false,
      "score": 82.0,
      "summary": "The paper introduces Active Epistemic Control (AEC), a planning method that optimizes query efficiency and feasibility in partially observable environments through effective belief management and interaction strategies.",
      "teaser_image": "https://arxiv.org/html/2602.03974/ab2.png",
      "debug_abstract": "Planning in interactive environments is challenging under partial observability: task-critical preconditions (e.g., object locations or container states) may be unknown at decision time, yet grounding them through interaction is costly. Learned world models can cheaply predict missing facts, but prediction errors can silently induce infeasible commitments. We present \\textbf{Active Epistemic Control (AEC)}, an epistemic-categorical planning layer that integrates model-based belief management with categorical feasibility checks. AEC maintains a strict separation between a \\emph{grounded fact store} used for commitment and a \\emph{belief store} used only for pruning candidate plans. At each step, it either queries the environment to ground an unresolved predicate when uncertainty is high or predictions are ambiguous, or simulates the predicate to filter hypotheses when confidence is sufficient. Final commitment is gated by grounded precondition coverage and an SQ-BCP pullback-style compatibility check, so simulated beliefs affect efficiency but cannot directly certify feasibility. Experiments on ALFWorld and ScienceWorld show that AEC achieves competitive success with fewer replanning rounds than strong LLM-agent baselines."
    }
  ],
  "Robotics and Embodied Artificial Intelligence Lab (REAL)": [
    {
      "title": "PerpetualWonder: Long-Horizon Action-Conditioned 4D Scene Generation",
      "url": "https://arxiv.org/abs/2602.04876",
      "date": "2026-02-04",
      "authors_text": "Jiahao Zhan, Zizhang Li, Hong-Xing Yu, Jiajun Wu",
      "is_highlight": true,
      "score": 79.0,
      "summary": "PerpetualWonder is a hybrid simulator that enables long-horizon, action-conditioned 4D scene generation from a single image by linking physical states with visual representations.",
      "teaser_image": "https://arxiv.org/html/2602.04876/x2.png",
      "debug_abstract": "We introduce PerpetualWonder, a hybrid generative simulator that enables long-horizon, action-conditioned 4D scene generation from a single image. Current works fail at this task because their physical state is decoupled from their visual representation, which prevents generative refinements to update the underlying physics for subsequent interactions. PerpetualWonder solves this by introducing the first true closed-loop system. It features a novel unified representation that creates a bidirectional link between the physical state and visual primitives, allowing generative refinements to correct both the dynamics and appearance. It also introduces a robust update mechanism that gathers supervision from multiple viewpoints to resolve optimization ambiguity. Experiments demonstrate that from a single image, PerpetualWonder can successfully simulate complex, multi-step interactions from long-horizon actions, maintaining physical plausibility and visual consistency."
    },
    {
      "title": "Active Epistemic Control for Query-Efficient Verified Planning",
      "url": "https://arxiv.org/abs/2602.03974",
      "date": "2026-02-03",
      "authors_text": "Shuhui Qu",
      "is_highlight": false,
      "score": 82.0,
      "summary": "The paper introduces Active Epistemic Control (AEC), a planning method that optimizes query efficiency and feasibility in partially observable environments through effective belief management and interaction strategies.",
      "teaser_image": "https://arxiv.org/html/2602.03974/ab2.png",
      "debug_abstract": "Planning in interactive environments is challenging under partial observability: task-critical preconditions (e.g., object locations or container states) may be unknown at decision time, yet grounding them through interaction is costly. Learned world models can cheaply predict missing facts, but prediction errors can silently induce infeasible commitments. We present \\textbf{Active Epistemic Control (AEC)}, an epistemic-categorical planning layer that integrates model-based belief management with categorical feasibility checks. AEC maintains a strict separation between a \\emph{grounded fact store} used for commitment and a \\emph{belief store} used only for pruning candidate plans. At each step, it either queries the environment to ground an unresolved predicate when uncertainty is high or predictions are ambiguous, or simulates the predicate to filter hypotheses when confidence is sufficient. Final commitment is gated by grounded precondition coverage and an SQ-BCP pullback-style compatibility check, so simulated beliefs affect efficiency but cannot directly certify feasibility. Experiments on ALFWorld and ScienceWorld show that AEC achieves competitive success with fewer replanning rounds than strong LLM-agent baselines."
    }
  ],
  "斯坦福-视觉与学习实验室 (SVL)": [
    {
      "title": "PerpetualWonder: Long-Horizon Action-Conditioned 4D Scene Generation",
      "url": "https://arxiv.org/abs/2602.04876",
      "date": "2026-02-04",
      "authors_text": "Jiahao Zhan, Zizhang Li, Hong-Xing Yu, Jiajun Wu",
      "is_highlight": true,
      "score": 79.0,
      "summary": "PerpetualWonder is a hybrid simulator that enables long-horizon, action-conditioned 4D scene generation from a single image by linking physical states with visual representations.",
      "teaser_image": "https://arxiv.org/html/2602.04876/x2.png",
      "debug_abstract": "We introduce PerpetualWonder, a hybrid generative simulator that enables long-horizon, action-conditioned 4D scene generation from a single image. Current works fail at this task because their physical state is decoupled from their visual representation, which prevents generative refinements to update the underlying physics for subsequent interactions. PerpetualWonder solves this by introducing the first true closed-loop system. It features a novel unified representation that creates a bidirectional link between the physical state and visual primitives, allowing generative refinements to correct both the dynamics and appearance. It also introduces a robust update mechanism that gathers supervision from multiple viewpoints to resolve optimization ambiguity. Experiments demonstrate that from a single image, PerpetualWonder can successfully simulate complex, multi-step interactions from long-horizon actions, maintaining physical plausibility and visual consistency."
    },
    {
      "title": "Active Epistemic Control for Query-Efficient Verified Planning",
      "url": "https://arxiv.org/abs/2602.03974",
      "date": "2026-02-03",
      "authors_text": "Shuhui Qu",
      "is_highlight": false,
      "score": 82.0,
      "summary": "The paper introduces Active Epistemic Control (AEC), a planning method that optimizes query efficiency and feasibility in partially observable environments through effective belief management and interaction strategies.",
      "teaser_image": "https://arxiv.org/html/2602.03974/ab2.png",
      "debug_abstract": "Planning in interactive environments is challenging under partial observability: task-critical preconditions (e.g., object locations or container states) may be unknown at decision time, yet grounding them through interaction is costly. Learned world models can cheaply predict missing facts, but prediction errors can silently induce infeasible commitments. We present \\textbf{Active Epistemic Control (AEC)}, an epistemic-categorical planning layer that integrates model-based belief management with categorical feasibility checks. AEC maintains a strict separation between a \\emph{grounded fact store} used for commitment and a \\emph{belief store} used only for pruning candidate plans. At each step, it either queries the environment to ground an unresolved predicate when uncertainty is high or predictions are ambiguous, or simulates the predicate to filter hypotheses when confidence is sufficient. Final commitment is gated by grounded precondition coverage and an SQ-BCP pullback-style compatibility check, so simulated beliefs affect efficiency but cannot directly certify feasibility. Experiments on ALFWorld and ScienceWorld show that AEC achieves competitive success with fewer replanning rounds than strong LLM-agent baselines."
    }
  ],
  "Stanford AI Lab (SAIL)": [
    {
      "title": "PerpetualWonder: Long-Horizon Action-Conditioned 4D Scene Generation",
      "url": "https://arxiv.org/abs/2602.04876",
      "date": "2026-02-04",
      "authors_text": "Jiahao Zhan, Zizhang Li, Hong-Xing Yu, Jiajun Wu",
      "is_highlight": true,
      "score": 79.0,
      "summary": "PerpetualWonder is a hybrid simulator that enables long-horizon, action-conditioned 4D scene generation from a single image by linking physical states with visual representations.",
      "teaser_image": "https://arxiv.org/html/2602.04876/x2.png",
      "debug_abstract": "We introduce PerpetualWonder, a hybrid generative simulator that enables long-horizon, action-conditioned 4D scene generation from a single image. Current works fail at this task because their physical state is decoupled from their visual representation, which prevents generative refinements to update the underlying physics for subsequent interactions. PerpetualWonder solves this by introducing the first true closed-loop system. It features a novel unified representation that creates a bidirectional link between the physical state and visual primitives, allowing generative refinements to correct both the dynamics and appearance. It also introduces a robust update mechanism that gathers supervision from multiple viewpoints to resolve optimization ambiguity. Experiments demonstrate that from a single image, PerpetualWonder can successfully simulate complex, multi-step interactions from long-horizon actions, maintaining physical plausibility and visual consistency."
    },
    {
      "title": "Active Epistemic Control for Query-Efficient Verified Planning",
      "url": "https://arxiv.org/abs/2602.03974",
      "date": "2026-02-03",
      "authors_text": "Shuhui Qu",
      "is_highlight": false,
      "score": 82.0,
      "summary": "The paper introduces Active Epistemic Control (AEC), a planning method that optimizes query efficiency and feasibility in partially observable environments through effective belief management and interaction strategies.",
      "teaser_image": "https://arxiv.org/html/2602.03974/ab2.png",
      "debug_abstract": "Planning in interactive environments is challenging under partial observability: task-critical preconditions (e.g., object locations or container states) may be unknown at decision time, yet grounding them through interaction is costly. Learned world models can cheaply predict missing facts, but prediction errors can silently induce infeasible commitments. We present \\textbf{Active Epistemic Control (AEC)}, an epistemic-categorical planning layer that integrates model-based belief management with categorical feasibility checks. AEC maintains a strict separation between a \\emph{grounded fact store} used for commitment and a \\emph{belief store} used only for pruning candidate plans. At each step, it either queries the environment to ground an unresolved predicate when uncertainty is high or predictions are ambiguous, or simulates the predicate to filter hypotheses when confidence is sufficient. Final commitment is gated by grounded precondition coverage and an SQ-BCP pullback-style compatibility check, so simulated beliefs affect efficiency but cannot directly certify feasibility. Experiments on ALFWorld and ScienceWorld show that AEC achieves competitive success with fewer replanning rounds than strong LLM-agent baselines."
    }
  ],
  "斯坦福-机器人与具身智能实验室 (REAL)": [
    {
      "title": "PerpetualWonder: Long-Horizon Action-Conditioned 4D Scene Generation",
      "url": "https://arxiv.org/abs/2602.04876",
      "date": "2026-02-04",
      "authors_text": "Jiahao Zhan, Zizhang Li, Hong-Xing Yu, Jiajun Wu",
      "is_highlight": true,
      "score": 79.0,
      "summary": "PerpetualWonder is a hybrid simulator that enables long-horizon, action-conditioned 4D scene generation from a single image by linking physical states with visual representations.",
      "teaser_image": "https://arxiv.org/html/2602.04876/x2.png",
      "debug_abstract": "We introduce PerpetualWonder, a hybrid generative simulator that enables long-horizon, action-conditioned 4D scene generation from a single image. Current works fail at this task because their physical state is decoupled from their visual representation, which prevents generative refinements to update the underlying physics for subsequent interactions. PerpetualWonder solves this by introducing the first true closed-loop system. It features a novel unified representation that creates a bidirectional link between the physical state and visual primitives, allowing generative refinements to correct both the dynamics and appearance. It also introduces a robust update mechanism that gathers supervision from multiple viewpoints to resolve optimization ambiguity. Experiments demonstrate that from a single image, PerpetualWonder can successfully simulate complex, multi-step interactions from long-horizon actions, maintaining physical plausibility and visual consistency."
    },
    {
      "title": "Active Epistemic Control for Query-Efficient Verified Planning",
      "url": "https://arxiv.org/abs/2602.03974",
      "date": "2026-02-03",
      "authors_text": "Shuhui Qu",
      "is_highlight": false,
      "score": 82.0,
      "summary": "The paper introduces Active Epistemic Control (AEC), a planning method that optimizes query efficiency and feasibility in partially observable environments through effective belief management and interaction strategies.",
      "teaser_image": "https://arxiv.org/html/2602.03974/ab2.png",
      "debug_abstract": "Planning in interactive environments is challenging under partial observability: task-critical preconditions (e.g., object locations or container states) may be unknown at decision time, yet grounding them through interaction is costly. Learned world models can cheaply predict missing facts, but prediction errors can silently induce infeasible commitments. We present \\textbf{Active Epistemic Control (AEC)}, an epistemic-categorical planning layer that integrates model-based belief management with categorical feasibility checks. AEC maintains a strict separation between a \\emph{grounded fact store} used for commitment and a \\emph{belief store} used only for pruning candidate plans. At each step, it either queries the environment to ground an unresolved predicate when uncertainty is high or predictions are ambiguous, or simulates the predicate to filter hypotheses when confidence is sufficient. Final commitment is gated by grounded precondition coverage and an SQ-BCP pullback-style compatibility check, so simulated beliefs affect efficiency but cannot directly certify feasibility. Experiments on ALFWorld and ScienceWorld show that AEC achieves competitive success with fewer replanning rounds than strong LLM-agent baselines."
    }
  ],
  "通用机器人、自动化、传感和感知实验室 (GRASP)": [
    {
      "title": "Towards X-embodiment safety: A control theory perspective on transferring safety certificates across dynamical systems",
      "url": "https://arxiv.org/abs/2602.03987",
      "date": "2026-02-03",
      "authors_text": "Nikolaos Bousias, George Pappas",
      "is_highlight": false,
      "score": 85.0,
      "summary": "This paper presents a transferred control barrier function framework that enables safety guarantees to be enforced across mismatched dynamical systems, enhancing safety in complex control applications.",
      "teaser_image": "https://arxiv.org/html/2602.03987/figures/frame_1.png",
      "debug_abstract": "Control barrier functions (CBFs) provide a powerful tool for enforcing safety constraints in control systems, but their direct application to complex, high-dimensional dynamics is often challenging. In many settings, safety certificates are more naturally designed for simplified or alternative system models that do not exactly match the dynamics of interest. This paper addresses the problem of transferring safety guarantees between dynamical systems with mismatched dynamics. We propose a transferred control barrier function (tCBF) framework that enables safety constraints defined on one system to be systematically enforced on another system using a simulation function and an explicit margin term. The resulting transferred barrier accounts for model mismatch and induces a safety condition that can be enforced on the target system via a quadratic-program-based safety filter. The proposed approach is general and does not require the two systems to share the same state dimension or dynamics. We demonstrate the effectiveness of the framework on a quadrotor navigation task with the transferred barrier ensuring collision avoidance for the target system, while remaining minimally invasive to a nominal controller. These results highlight the potential of transferred control barrier functions as a general mechanism for enforcing safety across heterogeneous dynamical systems."
    }
  ],
  "SU Lab": [
    {
      "title": "Seeing Through Clutter: Structured 3D Scene Reconstruction via Iterative Object Removal",
      "url": "https://arxiv.org/abs/2602.04053",
      "date": "2026-02-03",
      "authors_text": "Rio Aguina-Kang, Kevin James Blackburn-Matzen, Thibault Groueix, Vladimir Kim, Matheus Gadelha",
      "is_highlight": false,
      "score": 78.0,
      "summary": "The SeeingThroughClutter method enhances structured 3D scene reconstruction by iteratively removing occluded objects, improving segmentation without task-specific training.",
      "teaser_image": "https://arxiv.org/html/2602.04053/x3.png",
      "debug_abstract": "We present SeeingThroughClutter, a method for reconstructing structured 3D representations from single images by segmenting and modeling objects individually. Prior approaches rely on intermediate tasks such as semantic segmentation and depth estimation, which often underperform in complex scenes, particularly in the presence of occlusion and clutter. We address this by introducing an iterative object removal and reconstruction pipeline that decomposes complex scenes into a sequence of simpler subtasks. Using VLMs as orchestrators, foreground objects are removed one at a time via detection, segmentation, object removal, and 3D fitting. We show that removing objects allows for cleaner segmentations of subsequent objects, even in highly occluded scenes. Our method requires no task-specific training and benefits directly from ongoing advances in foundation models. We demonstrate stateof-the-art robustness on 3D-Front and ADE20K datasets. Project Page: this https URL"
    }
  ],
  "电子科技大学": [
    {
      "title": "OMG-Agent: Toward Robust Missing Modality Generation with Decoupled Coarse-to-Fine Agentic Workflows",
      "url": "https://arxiv.org/abs/2602.04144",
      "date": "2026-02-04",
      "authors_text": "Ruiting Dai, Zheyu Wang, Haoyu Yang, Yihan Liu, Chengzhi Wang",
      "is_highlight": false,
      "score": 76.0,
      "summary": "The OMG-Agent framework enhances multimodal system reliability by decoupling tasks into three stages, improving robustness against data incompleteness and outperforming existing methods.",
      "teaser_image": "https://arxiv.org/html/2602.04144/figure/small-fig.png",
      "debug_abstract": "Data incompleteness severely impedes the reliability of multimodal systems. Existing reconstruction methods face distinct bottlenecks: conventional parametric/generative models are prone to hallucinations due to over-reliance on internal memory, while retrieval-augmented frameworks struggle with retrieval rigidity. Critically, these end-to-end architectures are fundamentally constrained by Semantic-Detail Entanglement -- a structural conflict between logical reasoning and signal synthesis that compromises fidelity. In this paper, we present \\textbf{\\underline{O}}mni-\\textbf{\\underline{M}}odality \\textbf{\\underline{G}}eneration Agent (\\textbf{OMG-Agent}), a novel framework that shifts the paradigm from static mapping to a dynamic coarse-to-fine Agentic Workflow. By mimicking a \\textit{deliberate-then-act} cognitive process, OMG-Agent explicitly decouples the task into three synergistic stages: (1) an MLLM-driven Semantic Planner that resolves input ambiguity via Progressive Contextual Reasoning, creating a deterministic structured semantic plan; (2) a non-parametric Evidence Retriever that grounds abstract semantics in external knowledge; and (3) a Retrieval-Injected Executor that utilizes retrieved evidence as flexible feature prompts to overcome rigidity and synthesize high-fidelity details. Extensive experiments on multiple benchmarks demonstrate that OMG-Agent consistently surpasses state-of-the-art methods, maintaining robustness under extreme missingness, e.g., a $2.6$-point gain on CMU-MOSI at $70$\\% missing rates."
    }
  ],
  "南方科技大学": [
    {
      "title": "GenMRP: A Generative Multi-Route Planning Framework for Efficient and Personalized Real-Time Industrial Navigation",
      "url": "https://arxiv.org/abs/2602.04174",
      "date": "2026-02-04",
      "authors_text": "Chengzhang Wang, Chao Chen, Jun Tao, Tengfei Liu, He Bai",
      "is_highlight": false,
      "score": 88.0,
      "summary": "GenMRP is a generative multi-route planning framework that enhances real-time industrial navigation by efficiently personalizing route diversity through a novel skeleton-to-capillary approach.",
      "teaser_image": "https://arxiv.org/html/2602.04174/x3.png",
      "debug_abstract": "Existing industrial-scale navigation applications contend with massive road networks, typically employing two main categories of approaches for route planning. The first relies on precomputed road costs for optimal routing and heuristic algorithms for generating alternatives, while the second, generative methods, has recently gained significant attention. However, the former struggles with personalization and route diversity, while the latter fails to meet the efficiency requirements of large-scale real-time scenarios. To address these limitations, we propose GenMRP, a generative framework for multi-route planning. To ensure generation efficiency, GenMRP first introduces a skeleton-to-capillary approach that dynamically constructs a relevant sub-network significantly smaller than the full road network. Within this sub-network, routes are generated iteratively. The first iteration identifies the optimal route, while the subsequent ones generate alternatives that balance quality and diversity using the newly proposed correctional boosting approach. Each iteration incorporates road features, user historical sequences, and previously generated routes into a Link Cost Model to update road costs, followed by route generation using the Dijkstra algorithm. Extensive experiments show that GenMRP achieves state-of-the-art performance with high efficiency in both offline and online environments. To facilitate further research, we have publicly released the training and evaluation dataset. GenMRP has been fully deployed in a real-world navigation app, demonstrating its effectiveness and benefits."
    }
  ],
  "具身感知与交互实验室 (EPIC Lab)": [
    {
      "title": "Integrated Exploration and Sequential Manipulation on Scene Graph with LLM-based Situated Replanning",
      "url": "https://arxiv.org/abs/2602.04419",
      "date": "2026-02-04",
      "authors_text": "Heqing Yang, Ziyuan Jiao, Shu Wang, Yida Niu, Si Liu",
      "is_highlight": false,
      "score": 91.0,
      "summary": "EPoG integrates exploration and sequential manipulation planning using scene graphs and LLMs, achieving a 91.3% success rate in complex tasks within dynamic environments.",
      "teaser_image": "https://arxiv.org/html/2602.04419/figures/fig2-structure.png",
      "debug_abstract": "In partially known environments, robots must combine exploration to gather information with task planning for efficient execution. To address this challenge, we propose EPoG, an Exploration-based sequential manipulation Planning framework on Scene Graphs. EPoG integrates a graph-based global planner with a Large Language Model (LLM)-based situated local planner, continuously updating a belief graph using observations and LLM predictions to represent known and unknown objects. Action sequences are generated by computing graph edit operations between the goal and belief graphs, ordered by temporal dependencies and movement costs. This approach seamlessly combines exploration and sequential manipulation planning. In ablation studies across 46 realistic household scenes and 5 long-horizon daily object transportation tasks, EPoG achieved a success rate of 91.3%, reducing travel distance by 36.1% on average. Furthermore, a physical mobile manipulator successfully executed complex tasks in unknown and dynamic environments, demonstrating EPoG&#39;s potential for real-world applications."
    },
    {
      "title": "Fine-tuning Pre-trained Vision-Language Models in a Human-Annotation-Free Manner",
      "url": "https://arxiv.org/abs/2602.04337",
      "date": "2026-02-04",
      "authors_text": "Qian-Wei Wang, Guanghao Meng, Ren Cai, Yaguang Song, Shu-Tao Xia",
      "is_highlight": false,
      "score": 68.0,
      "summary": "The paper presents Collaborative Fine-Tuning (CoFT), an unsupervised framework for adapting vision-language models without labeled data, enhancing performance through dual-prompt learning and collaborative filtering.",
      "teaser_image": "https://arxiv.org/html/2602.04337/pic/overall.png",
      "debug_abstract": "Large-scale vision-language models (VLMs) such as CLIP exhibit strong zero-shot generalization, but adapting them to downstream tasks typically requires costly labeled data. Existing unsupervised self-training methods rely on pseudo-labeling, yet often suffer from unreliable confidence filtering, confirmation bias, and underutilization of low-confidence samples. We propose Collaborative Fine-Tuning (CoFT), an unsupervised adaptation framework that leverages unlabeled data through a dual-model, cross-modal collaboration mechanism. CoFT introduces a dual-prompt learning strategy with positive and negative textual prompts to explicitly model pseudo-label cleanliness in a sample-dependent manner, removing the need for hand-crafted thresholds or noise assumptions. The negative prompt also regularizes lightweight visual adaptation modules, improving robustness under noisy supervision. CoFT employs a two-phase training scheme, transitioning from parameter-efficient fine-tuning on high-confidence samples to full fine-tuning guided by collaboratively filtered pseudo-labels. Building on CoFT, CoFT+ further enhances adaptation via iterative fine-tuning, momentum contrastive learning, and LLM-generated prompts. Extensive experiments demonstrate consistent gains over existing unsupervised methods and even few-shot supervised baselines."
    },
    {
      "title": "GeneralVLA: Generalizable Vision-Language-Action Models with Knowledge-Guided Trajectory Planning",
      "url": "https://arxiv.org/abs/2602.04315",
      "date": "2026-02-04",
      "authors_text": "Guoqing Ma, Siheng Wang, Zeyu Zhang, Shan Yu, Hao Tang",
      "is_highlight": false,
      "score": 89.0,
      "summary": "GeneralVLA is a hierarchical vision-language-action model that enhances zero-shot robotic manipulation by utilizing knowledge-guided trajectory planning without requiring real-world data or demonstrations.",
      "teaser_image": "https://arxiv.org/html/2602.04315/x1.png",
      "debug_abstract": "Large foundation models have shown strong open-world generalization to complex problems in vision and language, but similar levels of generalization have yet to be achieved in robotics. One fundamental challenge is that the models exhibit limited zero-shot capability, which hampers their ability to generalize effectively to unseen scenarios. In this work, we propose GeneralVLA (Generalizable Vision-Language-Action Models with Knowledge-Guided Trajectory Planning), a hierarchical vision-language-action (VLA) model that can be more effective in utilizing the generalization of foundation models, enabling zero-shot manipulation and automatically generating data for robotics. In particular, we study a class of hierarchical VLA model where the high-level ASM (Affordance Segmentation Module) is finetuned to perceive image keypoint affordances of the scene; the mid-level 3DAgent carries out task understanding, skill knowledge, and trajectory planning to produce a 3D path indicating the desired robot end-effector trajectory. The intermediate 3D path prediction is then served as guidance to the low-level, 3D-aware control policy capable of precise manipulation. Compared to alternative approaches, our method requires no real-world robotic data collection or human demonstration, making it much more scalable to diverse tasks and viewpoints. Empirically, GeneralVLA successfully generates trajectories for 14 tasks, significantly outperforming state-of-the-art methods such as VoxPoser. The generated demonstrations can train more robust behavior cloning policies than training with human demonstrations or from data generated by VoxPoser, Scaling-up, and Code-As-Policies. We believe GeneralVLA can be the scalable method for both generating data for robotics and solving novel tasks in a zero-shot setting. Code: this https URL. Website: this https URL."
    },
    {
      "title": "Reshaping Action Error Distributions for Reliable Vision-Language-Action Models",
      "url": "https://arxiv.org/abs/2602.04228",
      "date": "2026-02-04",
      "authors_text": "Shuanghao Bai, Dakai Wang, Cheng Chi, Wanqi Zhou, Jing Lyu",
      "is_highlight": false,
      "score": 90.0,
      "summary": "This paper introduces Minimum Error Entropy (MEE) to enhance continuous-action vision-language-action models, improving robustness and success rates in robotic manipulation tasks.",
      "teaser_image": "https://arxiv.org/html/2602.04228/x1.png",
      "debug_abstract": "In robotic manipulation, vision-language-action (VLA) models have emerged as a promising paradigm for learning generalizable and scalable robot policies. Most existing VLA frameworks rely on standard supervised objectives, typically cross-entropy for discrete actions and mean squared error (MSE) for continuous action regression, which impose strong pointwise constraints on individual predictions. In this work, we focus on continuous-action VLA models and move beyond conventional MSE-based regression by reshaping action error distributions during training. Drawing on information-theoretic principles, we introduce Minimum Error Entropy (MEE) into modern VLA architectures and propose a trajectory-level MEE objective, together with two weighted variants, combined with MSE for continuous-action VLA training. We evaluate our approaches across standard, few-shot, and noisy settings on multiple representative VLA architectures, using simulation benchmarks such as LIBERO and SimplerEnv as well as real-world robotic manipulation tasks. Experimental results demonstrate consistent improvements in success rates and robustness across these settings. Under imbalanced data regimes, the gains persist within a well-characterized operating range, while incurring negligible additional training cost and no impact on inference efficiency. We further provide theoretical analyses that explain why MEE-based supervision is effective and characterize its practical range. Project Page: this https URL"
    }
  ],
  "智元机器人联合实验室 (PKU-Agibot)": [
    {
      "title": "Integrated Exploration and Sequential Manipulation on Scene Graph with LLM-based Situated Replanning",
      "url": "https://arxiv.org/abs/2602.04419",
      "date": "2026-02-04",
      "authors_text": "Heqing Yang, Ziyuan Jiao, Shu Wang, Yida Niu, Si Liu",
      "is_highlight": false,
      "score": 91.0,
      "summary": "EPoG integrates exploration and sequential manipulation planning using scene graphs and LLMs, achieving a 91.3% success rate in complex tasks within dynamic environments.",
      "teaser_image": "https://arxiv.org/html/2602.04419/figures/fig2-structure.png",
      "debug_abstract": "In partially known environments, robots must combine exploration to gather information with task planning for efficient execution. To address this challenge, we propose EPoG, an Exploration-based sequential manipulation Planning framework on Scene Graphs. EPoG integrates a graph-based global planner with a Large Language Model (LLM)-based situated local planner, continuously updating a belief graph using observations and LLM predictions to represent known and unknown objects. Action sequences are generated by computing graph edit operations between the goal and belief graphs, ordered by temporal dependencies and movement costs. This approach seamlessly combines exploration and sequential manipulation planning. In ablation studies across 46 realistic household scenes and 5 long-horizon daily object transportation tasks, EPoG achieved a success rate of 91.3%, reducing travel distance by 36.1% on average. Furthermore, a physical mobile manipulator successfully executed complex tasks in unknown and dynamic environments, demonstrating EPoG&#39;s potential for real-world applications."
    },
    {
      "title": "Fine-tuning Pre-trained Vision-Language Models in a Human-Annotation-Free Manner",
      "url": "https://arxiv.org/abs/2602.04337",
      "date": "2026-02-04",
      "authors_text": "Qian-Wei Wang, Guanghao Meng, Ren Cai, Yaguang Song, Shu-Tao Xia",
      "is_highlight": false,
      "score": 68.0,
      "summary": "The paper presents Collaborative Fine-Tuning (CoFT), an unsupervised framework for adapting vision-language models without labeled data, enhancing performance through dual-prompt learning and collaborative filtering.",
      "teaser_image": "https://arxiv.org/html/2602.04337/pic/overall.png",
      "debug_abstract": "Large-scale vision-language models (VLMs) such as CLIP exhibit strong zero-shot generalization, but adapting them to downstream tasks typically requires costly labeled data. Existing unsupervised self-training methods rely on pseudo-labeling, yet often suffer from unreliable confidence filtering, confirmation bias, and underutilization of low-confidence samples. We propose Collaborative Fine-Tuning (CoFT), an unsupervised adaptation framework that leverages unlabeled data through a dual-model, cross-modal collaboration mechanism. CoFT introduces a dual-prompt learning strategy with positive and negative textual prompts to explicitly model pseudo-label cleanliness in a sample-dependent manner, removing the need for hand-crafted thresholds or noise assumptions. The negative prompt also regularizes lightweight visual adaptation modules, improving robustness under noisy supervision. CoFT employs a two-phase training scheme, transitioning from parameter-efficient fine-tuning on high-confidence samples to full fine-tuning guided by collaboratively filtered pseudo-labels. Building on CoFT, CoFT+ further enhances adaptation via iterative fine-tuning, momentum contrastive learning, and LLM-generated prompts. Extensive experiments demonstrate consistent gains over existing unsupervised methods and even few-shot supervised baselines."
    },
    {
      "title": "GeneralVLA: Generalizable Vision-Language-Action Models with Knowledge-Guided Trajectory Planning",
      "url": "https://arxiv.org/abs/2602.04315",
      "date": "2026-02-04",
      "authors_text": "Guoqing Ma, Siheng Wang, Zeyu Zhang, Shan Yu, Hao Tang",
      "is_highlight": false,
      "score": 89.0,
      "summary": "GeneralVLA is a hierarchical vision-language-action model that enhances zero-shot robotic manipulation by utilizing knowledge-guided trajectory planning without requiring real-world data or demonstrations.",
      "teaser_image": "https://arxiv.org/html/2602.04315/x1.png",
      "debug_abstract": "Large foundation models have shown strong open-world generalization to complex problems in vision and language, but similar levels of generalization have yet to be achieved in robotics. One fundamental challenge is that the models exhibit limited zero-shot capability, which hampers their ability to generalize effectively to unseen scenarios. In this work, we propose GeneralVLA (Generalizable Vision-Language-Action Models with Knowledge-Guided Trajectory Planning), a hierarchical vision-language-action (VLA) model that can be more effective in utilizing the generalization of foundation models, enabling zero-shot manipulation and automatically generating data for robotics. In particular, we study a class of hierarchical VLA model where the high-level ASM (Affordance Segmentation Module) is finetuned to perceive image keypoint affordances of the scene; the mid-level 3DAgent carries out task understanding, skill knowledge, and trajectory planning to produce a 3D path indicating the desired robot end-effector trajectory. The intermediate 3D path prediction is then served as guidance to the low-level, 3D-aware control policy capable of precise manipulation. Compared to alternative approaches, our method requires no real-world robotic data collection or human demonstration, making it much more scalable to diverse tasks and viewpoints. Empirically, GeneralVLA successfully generates trajectories for 14 tasks, significantly outperforming state-of-the-art methods such as VoxPoser. The generated demonstrations can train more robust behavior cloning policies than training with human demonstrations or from data generated by VoxPoser, Scaling-up, and Code-As-Policies. We believe GeneralVLA can be the scalable method for both generating data for robotics and solving novel tasks in a zero-shot setting. Code: this https URL. Website: this https URL."
    },
    {
      "title": "Reshaping Action Error Distributions for Reliable Vision-Language-Action Models",
      "url": "https://arxiv.org/abs/2602.04228",
      "date": "2026-02-04",
      "authors_text": "Shuanghao Bai, Dakai Wang, Cheng Chi, Wanqi Zhou, Jing Lyu",
      "is_highlight": false,
      "score": 90.0,
      "summary": "This paper introduces Minimum Error Entropy (MEE) to enhance continuous-action vision-language-action models, improving robustness and success rates in robotic manipulation tasks.",
      "teaser_image": "https://arxiv.org/html/2602.04228/x1.png",
      "debug_abstract": "In robotic manipulation, vision-language-action (VLA) models have emerged as a promising paradigm for learning generalizable and scalable robot policies. Most existing VLA frameworks rely on standard supervised objectives, typically cross-entropy for discrete actions and mean squared error (MSE) for continuous action regression, which impose strong pointwise constraints on individual predictions. In this work, we focus on continuous-action VLA models and move beyond conventional MSE-based regression by reshaping action error distributions during training. Drawing on information-theoretic principles, we introduce Minimum Error Entropy (MEE) into modern VLA architectures and propose a trajectory-level MEE objective, together with two weighted variants, combined with MSE for continuous-action VLA training. We evaluate our approaches across standard, few-shot, and noisy settings on multiple representative VLA architectures, using simulation benchmarks such as LIBERO and SimplerEnv as well as real-world robotic manipulation tasks. Experimental results demonstrate consistent improvements in success rates and robustness across these settings. Under imbalanced data regimes, the gains persist within a well-characterized operating range, while incurring negligible additional training cost and no impact on inference efficiency. We further provide theoretical analyses that explain why MEE-based supervision is effective and characterize its practical range. Project Page: this https URL"
    }
  ],
  "HMI Lab": [
    {
      "title": "Integrated Exploration and Sequential Manipulation on Scene Graph with LLM-based Situated Replanning",
      "url": "https://arxiv.org/abs/2602.04419",
      "date": "2026-02-04",
      "authors_text": "Heqing Yang, Ziyuan Jiao, Shu Wang, Yida Niu, Si Liu",
      "is_highlight": false,
      "score": 91.0,
      "summary": "EPoG integrates exploration and sequential manipulation planning using scene graphs and LLMs, achieving a 91.3% success rate in complex tasks within dynamic environments.",
      "teaser_image": "https://arxiv.org/html/2602.04419/figures/fig2-structure.png",
      "debug_abstract": "In partially known environments, robots must combine exploration to gather information with task planning for efficient execution. To address this challenge, we propose EPoG, an Exploration-based sequential manipulation Planning framework on Scene Graphs. EPoG integrates a graph-based global planner with a Large Language Model (LLM)-based situated local planner, continuously updating a belief graph using observations and LLM predictions to represent known and unknown objects. Action sequences are generated by computing graph edit operations between the goal and belief graphs, ordered by temporal dependencies and movement costs. This approach seamlessly combines exploration and sequential manipulation planning. In ablation studies across 46 realistic household scenes and 5 long-horizon daily object transportation tasks, EPoG achieved a success rate of 91.3%, reducing travel distance by 36.1% on average. Furthermore, a physical mobile manipulator successfully executed complex tasks in unknown and dynamic environments, demonstrating EPoG&#39;s potential for real-world applications."
    },
    {
      "title": "Fine-tuning Pre-trained Vision-Language Models in a Human-Annotation-Free Manner",
      "url": "https://arxiv.org/abs/2602.04337",
      "date": "2026-02-04",
      "authors_text": "Qian-Wei Wang, Guanghao Meng, Ren Cai, Yaguang Song, Shu-Tao Xia",
      "is_highlight": false,
      "score": 68.0,
      "summary": "The paper presents Collaborative Fine-Tuning (CoFT), an unsupervised framework for adapting vision-language models without labeled data, enhancing performance through dual-prompt learning and collaborative filtering.",
      "teaser_image": "https://arxiv.org/html/2602.04337/pic/overall.png",
      "debug_abstract": "Large-scale vision-language models (VLMs) such as CLIP exhibit strong zero-shot generalization, but adapting them to downstream tasks typically requires costly labeled data. Existing unsupervised self-training methods rely on pseudo-labeling, yet often suffer from unreliable confidence filtering, confirmation bias, and underutilization of low-confidence samples. We propose Collaborative Fine-Tuning (CoFT), an unsupervised adaptation framework that leverages unlabeled data through a dual-model, cross-modal collaboration mechanism. CoFT introduces a dual-prompt learning strategy with positive and negative textual prompts to explicitly model pseudo-label cleanliness in a sample-dependent manner, removing the need for hand-crafted thresholds or noise assumptions. The negative prompt also regularizes lightweight visual adaptation modules, improving robustness under noisy supervision. CoFT employs a two-phase training scheme, transitioning from parameter-efficient fine-tuning on high-confidence samples to full fine-tuning guided by collaboratively filtered pseudo-labels. Building on CoFT, CoFT+ further enhances adaptation via iterative fine-tuning, momentum contrastive learning, and LLM-generated prompts. Extensive experiments demonstrate consistent gains over existing unsupervised methods and even few-shot supervised baselines."
    },
    {
      "title": "GeneralVLA: Generalizable Vision-Language-Action Models with Knowledge-Guided Trajectory Planning",
      "url": "https://arxiv.org/abs/2602.04315",
      "date": "2026-02-04",
      "authors_text": "Guoqing Ma, Siheng Wang, Zeyu Zhang, Shan Yu, Hao Tang",
      "is_highlight": false,
      "score": 89.0,
      "summary": "GeneralVLA is a hierarchical vision-language-action model that enhances zero-shot robotic manipulation by utilizing knowledge-guided trajectory planning without requiring real-world data or demonstrations.",
      "teaser_image": "https://arxiv.org/html/2602.04315/x1.png",
      "debug_abstract": "Large foundation models have shown strong open-world generalization to complex problems in vision and language, but similar levels of generalization have yet to be achieved in robotics. One fundamental challenge is that the models exhibit limited zero-shot capability, which hampers their ability to generalize effectively to unseen scenarios. In this work, we propose GeneralVLA (Generalizable Vision-Language-Action Models with Knowledge-Guided Trajectory Planning), a hierarchical vision-language-action (VLA) model that can be more effective in utilizing the generalization of foundation models, enabling zero-shot manipulation and automatically generating data for robotics. In particular, we study a class of hierarchical VLA model where the high-level ASM (Affordance Segmentation Module) is finetuned to perceive image keypoint affordances of the scene; the mid-level 3DAgent carries out task understanding, skill knowledge, and trajectory planning to produce a 3D path indicating the desired robot end-effector trajectory. The intermediate 3D path prediction is then served as guidance to the low-level, 3D-aware control policy capable of precise manipulation. Compared to alternative approaches, our method requires no real-world robotic data collection or human demonstration, making it much more scalable to diverse tasks and viewpoints. Empirically, GeneralVLA successfully generates trajectories for 14 tasks, significantly outperforming state-of-the-art methods such as VoxPoser. The generated demonstrations can train more robust behavior cloning policies than training with human demonstrations or from data generated by VoxPoser, Scaling-up, and Code-As-Policies. We believe GeneralVLA can be the scalable method for both generating data for robotics and solving novel tasks in a zero-shot setting. Code: this https URL. Website: this https URL."
    },
    {
      "title": "Reshaping Action Error Distributions for Reliable Vision-Language-Action Models",
      "url": "https://arxiv.org/abs/2602.04228",
      "date": "2026-02-04",
      "authors_text": "Shuanghao Bai, Dakai Wang, Cheng Chi, Wanqi Zhou, Jing Lyu",
      "is_highlight": false,
      "score": 90.0,
      "summary": "This paper introduces Minimum Error Entropy (MEE) to enhance continuous-action vision-language-action models, improving robustness and success rates in robotic manipulation tasks.",
      "teaser_image": "https://arxiv.org/html/2602.04228/x1.png",
      "debug_abstract": "In robotic manipulation, vision-language-action (VLA) models have emerged as a promising paradigm for learning generalizable and scalable robot policies. Most existing VLA frameworks rely on standard supervised objectives, typically cross-entropy for discrete actions and mean squared error (MSE) for continuous action regression, which impose strong pointwise constraints on individual predictions. In this work, we focus on continuous-action VLA models and move beyond conventional MSE-based regression by reshaping action error distributions during training. Drawing on information-theoretic principles, we introduce Minimum Error Entropy (MEE) into modern VLA architectures and propose a trajectory-level MEE objective, together with two weighted variants, combined with MSE for continuous-action VLA training. We evaluate our approaches across standard, few-shot, and noisy settings on multiple representative VLA architectures, using simulation benchmarks such as LIBERO and SimplerEnv as well as real-world robotic manipulation tasks. Experimental results demonstrate consistent improvements in success rates and robustness across these settings. Under imbalanced data regimes, the gains persist within a well-characterized operating range, while incurring negligible additional training cost and no impact on inference efficiency. We further provide theoretical analyses that explain why MEE-based supervision is effective and characterize its practical range. Project Page: this https URL"
    }
  ],
  "情感与认知智能机器人实验室 (ACIR)": [
    {
      "title": "Integrated Exploration and Sequential Manipulation on Scene Graph with LLM-based Situated Replanning",
      "url": "https://arxiv.org/abs/2602.04419",
      "date": "2026-02-04",
      "authors_text": "Heqing Yang, Ziyuan Jiao, Shu Wang, Yida Niu, Si Liu",
      "is_highlight": false,
      "score": 91.0,
      "summary": "EPoG integrates exploration and sequential manipulation planning using scene graphs and LLMs, achieving a 91.3% success rate in complex tasks within dynamic environments.",
      "teaser_image": "https://arxiv.org/html/2602.04419/figures/fig2-structure.png",
      "debug_abstract": "In partially known environments, robots must combine exploration to gather information with task planning for efficient execution. To address this challenge, we propose EPoG, an Exploration-based sequential manipulation Planning framework on Scene Graphs. EPoG integrates a graph-based global planner with a Large Language Model (LLM)-based situated local planner, continuously updating a belief graph using observations and LLM predictions to represent known and unknown objects. Action sequences are generated by computing graph edit operations between the goal and belief graphs, ordered by temporal dependencies and movement costs. This approach seamlessly combines exploration and sequential manipulation planning. In ablation studies across 46 realistic household scenes and 5 long-horizon daily object transportation tasks, EPoG achieved a success rate of 91.3%, reducing travel distance by 36.1% on average. Furthermore, a physical mobile manipulator successfully executed complex tasks in unknown and dynamic environments, demonstrating EPoG&#39;s potential for real-world applications."
    },
    {
      "title": "Fine-tuning Pre-trained Vision-Language Models in a Human-Annotation-Free Manner",
      "url": "https://arxiv.org/abs/2602.04337",
      "date": "2026-02-04",
      "authors_text": "Qian-Wei Wang, Guanghao Meng, Ren Cai, Yaguang Song, Shu-Tao Xia",
      "is_highlight": false,
      "score": 68.0,
      "summary": "The paper presents Collaborative Fine-Tuning (CoFT), an unsupervised framework for adapting vision-language models without labeled data, enhancing performance through dual-prompt learning and collaborative filtering.",
      "teaser_image": "https://arxiv.org/html/2602.04337/pic/overall.png",
      "debug_abstract": "Large-scale vision-language models (VLMs) such as CLIP exhibit strong zero-shot generalization, but adapting them to downstream tasks typically requires costly labeled data. Existing unsupervised self-training methods rely on pseudo-labeling, yet often suffer from unreliable confidence filtering, confirmation bias, and underutilization of low-confidence samples. We propose Collaborative Fine-Tuning (CoFT), an unsupervised adaptation framework that leverages unlabeled data through a dual-model, cross-modal collaboration mechanism. CoFT introduces a dual-prompt learning strategy with positive and negative textual prompts to explicitly model pseudo-label cleanliness in a sample-dependent manner, removing the need for hand-crafted thresholds or noise assumptions. The negative prompt also regularizes lightweight visual adaptation modules, improving robustness under noisy supervision. CoFT employs a two-phase training scheme, transitioning from parameter-efficient fine-tuning on high-confidence samples to full fine-tuning guided by collaboratively filtered pseudo-labels. Building on CoFT, CoFT+ further enhances adaptation via iterative fine-tuning, momentum contrastive learning, and LLM-generated prompts. Extensive experiments demonstrate consistent gains over existing unsupervised methods and even few-shot supervised baselines."
    },
    {
      "title": "GeneralVLA: Generalizable Vision-Language-Action Models with Knowledge-Guided Trajectory Planning",
      "url": "https://arxiv.org/abs/2602.04315",
      "date": "2026-02-04",
      "authors_text": "Guoqing Ma, Siheng Wang, Zeyu Zhang, Shan Yu, Hao Tang",
      "is_highlight": false,
      "score": 89.0,
      "summary": "GeneralVLA is a hierarchical vision-language-action model that enhances zero-shot robotic manipulation by utilizing knowledge-guided trajectory planning without requiring real-world data or demonstrations.",
      "teaser_image": "https://arxiv.org/html/2602.04315/x1.png",
      "debug_abstract": "Large foundation models have shown strong open-world generalization to complex problems in vision and language, but similar levels of generalization have yet to be achieved in robotics. One fundamental challenge is that the models exhibit limited zero-shot capability, which hampers their ability to generalize effectively to unseen scenarios. In this work, we propose GeneralVLA (Generalizable Vision-Language-Action Models with Knowledge-Guided Trajectory Planning), a hierarchical vision-language-action (VLA) model that can be more effective in utilizing the generalization of foundation models, enabling zero-shot manipulation and automatically generating data for robotics. In particular, we study a class of hierarchical VLA model where the high-level ASM (Affordance Segmentation Module) is finetuned to perceive image keypoint affordances of the scene; the mid-level 3DAgent carries out task understanding, skill knowledge, and trajectory planning to produce a 3D path indicating the desired robot end-effector trajectory. The intermediate 3D path prediction is then served as guidance to the low-level, 3D-aware control policy capable of precise manipulation. Compared to alternative approaches, our method requires no real-world robotic data collection or human demonstration, making it much more scalable to diverse tasks and viewpoints. Empirically, GeneralVLA successfully generates trajectories for 14 tasks, significantly outperforming state-of-the-art methods such as VoxPoser. The generated demonstrations can train more robust behavior cloning policies than training with human demonstrations or from data generated by VoxPoser, Scaling-up, and Code-As-Policies. We believe GeneralVLA can be the scalable method for both generating data for robotics and solving novel tasks in a zero-shot setting. Code: this https URL. Website: this https URL."
    },
    {
      "title": "Reshaping Action Error Distributions for Reliable Vision-Language-Action Models",
      "url": "https://arxiv.org/abs/2602.04228",
      "date": "2026-02-04",
      "authors_text": "Shuanghao Bai, Dakai Wang, Cheng Chi, Wanqi Zhou, Jing Lyu",
      "is_highlight": false,
      "score": 90.0,
      "summary": "This paper introduces Minimum Error Entropy (MEE) to enhance continuous-action vision-language-action models, improving robustness and success rates in robotic manipulation tasks.",
      "teaser_image": "https://arxiv.org/html/2602.04228/x1.png",
      "debug_abstract": "In robotic manipulation, vision-language-action (VLA) models have emerged as a promising paradigm for learning generalizable and scalable robot policies. Most existing VLA frameworks rely on standard supervised objectives, typically cross-entropy for discrete actions and mean squared error (MSE) for continuous action regression, which impose strong pointwise constraints on individual predictions. In this work, we focus on continuous-action VLA models and move beyond conventional MSE-based regression by reshaping action error distributions during training. Drawing on information-theoretic principles, we introduce Minimum Error Entropy (MEE) into modern VLA architectures and propose a trajectory-level MEE objective, together with two weighted variants, combined with MSE for continuous-action VLA training. We evaluate our approaches across standard, few-shot, and noisy settings on multiple representative VLA architectures, using simulation benchmarks such as LIBERO and SimplerEnv as well as real-world robotic manipulation tasks. Experimental results demonstrate consistent improvements in success rates and robustness across these settings. Under imbalanced data regimes, the gains persist within a well-characterized operating range, while incurring negligible additional training cost and no impact on inference efficiency. We further provide theoretical analyses that explain why MEE-based supervision is effective and characterize its practical range. Project Page: this https URL"
    }
  ],
  "具身智能多模态大模型中心": [
    {
      "title": "EgoActor: Grounding Task Planning into Spatial-aware Egocentric Actions for Humanoid Robots via Visual-Language Models",
      "url": "https://arxiv.org/abs/2602.04515",
      "date": "2026-02-04",
      "authors_text": "Yu Bai, MingMing Yu, Chaojie Li, Ziyi Bai, Xinlong Wang",
      "is_highlight": false,
      "score": 93.0,
      "summary": "EgoActor is a vision-language model that enables humanoid robots to execute spatially aware actions based on high-level instructions, enhancing real-time task planning and execution.",
      "teaser_image": "https://arxiv.org/html/2602.04515/BAAI_brand.png",
      "debug_abstract": "Deploying humanoid robots in real-world settings is fundamentally challenging, as it demands tight integration of perception, locomotion, and manipulation under partial-information observations and dynamically changing environments. As well as transitioning robustly between sub-tasks of different types. Towards addressing these challenges, we propose a novel task - EgoActing, which requires directly grounding high-level instructions into various, precise, spatially aware humanoid actions. We further instantiate this task by introducing EgoActor, a unified and scalable vision-language model (VLM) that can predict locomotion primitives (e.g., walk, turn, move sideways, change height), head movements, manipulation commands, and human-robot interactions to coordinate perception and execution in real-time. We leverage broad supervision over egocentric RGB-only data from real-world demonstrations, spatial reasoning question-answering, and simulated environment demonstrations, enabling EgoActor to make robust, context-aware decisions and perform fluent action inference (under 1s) with both 8B and 4B parameter models. Extensive evaluations in both simulated and real-world environments demonstrate that EgoActor effectively bridges abstract task planning and concrete motor execution, while generalizing across diverse tasks and unseen environments."
    },
    {
      "title": "Reshaping Action Error Distributions for Reliable Vision-Language-Action Models",
      "url": "https://arxiv.org/abs/2602.04228",
      "date": "2026-02-04",
      "authors_text": "Shuanghao Bai, Dakai Wang, Cheng Chi, Wanqi Zhou, Jing Lyu",
      "is_highlight": false,
      "score": 90.0,
      "summary": "This paper introduces Minimum Error Entropy (MEE) to enhance continuous-action vision-language-action models, improving robustness and success rates in robotic manipulation tasks.",
      "teaser_image": "https://arxiv.org/html/2602.04228/x1.png",
      "debug_abstract": "In robotic manipulation, vision-language-action (VLA) models have emerged as a promising paradigm for learning generalizable and scalable robot policies. Most existing VLA frameworks rely on standard supervised objectives, typically cross-entropy for discrete actions and mean squared error (MSE) for continuous action regression, which impose strong pointwise constraints on individual predictions. In this work, we focus on continuous-action VLA models and move beyond conventional MSE-based regression by reshaping action error distributions during training. Drawing on information-theoretic principles, we introduce Minimum Error Entropy (MEE) into modern VLA architectures and propose a trajectory-level MEE objective, together with two weighted variants, combined with MSE for continuous-action VLA training. We evaluate our approaches across standard, few-shot, and noisy settings on multiple representative VLA architectures, using simulation benchmarks such as LIBERO and SimplerEnv as well as real-world robotic manipulation tasks. Experimental results demonstrate consistent improvements in success rates and robustness across these settings. Under imbalanced data regimes, the gains persist within a well-characterized operating range, while incurring negligible additional training cost and no impact on inference efficiency. We further provide theoretical analyses that explain why MEE-based supervision is effective and characterize its practical range. Project Page: this https URL"
    }
  ],
  "具身智能研究中心": [
    {
      "title": "EgoActor: Grounding Task Planning into Spatial-aware Egocentric Actions for Humanoid Robots via Visual-Language Models",
      "url": "https://arxiv.org/abs/2602.04515",
      "date": "2026-02-04",
      "authors_text": "Yu Bai, MingMing Yu, Chaojie Li, Ziyi Bai, Xinlong Wang",
      "is_highlight": false,
      "score": 93.0,
      "summary": "EgoActor is a vision-language model that enables humanoid robots to execute spatially aware actions based on high-level instructions, enhancing real-time task planning and execution.",
      "teaser_image": "https://arxiv.org/html/2602.04515/BAAI_brand.png",
      "debug_abstract": "Deploying humanoid robots in real-world settings is fundamentally challenging, as it demands tight integration of perception, locomotion, and manipulation under partial-information observations and dynamically changing environments. As well as transitioning robustly between sub-tasks of different types. Towards addressing these challenges, we propose a novel task - EgoActing, which requires directly grounding high-level instructions into various, precise, spatially aware humanoid actions. We further instantiate this task by introducing EgoActor, a unified and scalable vision-language model (VLM) that can predict locomotion primitives (e.g., walk, turn, move sideways, change height), head movements, manipulation commands, and human-robot interactions to coordinate perception and execution in real-time. We leverage broad supervision over egocentric RGB-only data from real-world demonstrations, spatial reasoning question-answering, and simulated environment demonstrations, enabling EgoActor to make robust, context-aware decisions and perform fluent action inference (under 1s) with both 8B and 4B parameter models. Extensive evaluations in both simulated and real-world environments demonstrate that EgoActor effectively bridges abstract task planning and concrete motor execution, while generalizing across diverse tasks and unseen environments."
    },
    {
      "title": "Reshaping Action Error Distributions for Reliable Vision-Language-Action Models",
      "url": "https://arxiv.org/abs/2602.04228",
      "date": "2026-02-04",
      "authors_text": "Shuanghao Bai, Dakai Wang, Cheng Chi, Wanqi Zhou, Jing Lyu",
      "is_highlight": false,
      "score": 90.0,
      "summary": "This paper introduces Minimum Error Entropy (MEE) to enhance continuous-action vision-language-action models, improving robustness and success rates in robotic manipulation tasks.",
      "teaser_image": "https://arxiv.org/html/2602.04228/x1.png",
      "debug_abstract": "In robotic manipulation, vision-language-action (VLA) models have emerged as a promising paradigm for learning generalizable and scalable robot policies. Most existing VLA frameworks rely on standard supervised objectives, typically cross-entropy for discrete actions and mean squared error (MSE) for continuous action regression, which impose strong pointwise constraints on individual predictions. In this work, we focus on continuous-action VLA models and move beyond conventional MSE-based regression by reshaping action error distributions during training. Drawing on information-theoretic principles, we introduce Minimum Error Entropy (MEE) into modern VLA architectures and propose a trajectory-level MEE objective, together with two weighted variants, combined with MSE for continuous-action VLA training. We evaluate our approaches across standard, few-shot, and noisy settings on multiple representative VLA architectures, using simulation benchmarks such as LIBERO and SimplerEnv as well as real-world robotic manipulation tasks. Experimental results demonstrate consistent improvements in success rates and robustness across these settings. Under imbalanced data regimes, the gains persist within a well-characterized operating range, while incurring negligible additional training cost and no impact on inference efficiency. We further provide theoretical analyses that explain why MEE-based supervision is effective and characterize its practical range. Project Page: this https URL"
    }
  ],
  "机器人技术与系统国家重点实验室": [
    {
      "title": "Beyond Static Cropping: Layer-Adaptive Visual Localization and Decoding Enhancement",
      "url": "https://arxiv.org/abs/2602.04304",
      "date": "2026-02-04",
      "authors_text": "Zipeng Zhu, Zhanghao Hu, Qinglin Zhu, Yuxi Hong, Yijun Liu",
      "is_highlight": false,
      "score": 72.0,
      "summary": "The paper introduces LASER, a dynamic method for visual localization and reasoning that adaptively selects attention layers based on task complexity, enhancing performance in visual question answering.",
      "teaser_image": "https://arxiv.org/html/2602.04304/x3.png",
      "debug_abstract": "Large Vision-Language Models (LVLMs) have advanced rapidly by aligning visual patches with the text embedding space, but a fixed visual-token budget forces images to be resized to a uniform pretraining resolution, often erasing fine-grained details and causing hallucinations via over-reliance on language priors. Recent attention-guided enhancement (e.g., cropping or region-focused attention allocation) alleviates this, yet it commonly hinges on a static &#34;magic layer&#34; empirically chosen on simple recognition benchmarks and thus may not transfer to complex reasoning tasks. In contrast to this static assumption, we propose a dynamic perspective on visual grounding. Through a layer-wise sensitivity analysis, we demonstrate that visual grounding is a dynamic process: while simple object recognition tasks rely on middle layers, complex visual search and reasoning tasks require visual information to be reactivated at deeper layers. Based on this observation, we introduce Visual Activation by Query (VAQ), a metric that identifies the layer whose attention map is most relevant to query-specific visual grounding by measuring attention sensitivity to the input query. Building on VAQ, we further propose LASER (Layer-adaptive Attention-guided Selective visual and decoding Enhancement for Reasoning), a training-free inference procedure that adaptively selects task-appropriate layers for visual localization and question answering. Experiments across diverse VQA benchmarks show that LASER significantly improves VQA accuracy across tasks with varying levels of complexity."
    },
    {
      "title": "AppleVLM: End-to-end Autonomous Driving with Advanced Perception and Planning-Enhanced Vision-Language Models",
      "url": "https://arxiv.org/abs/2602.04256",
      "date": "2026-02-04",
      "authors_text": "Yuxuan Han, Kunyuan Wu, Qianyi Shao, Renxiang Xiao, Zilu Wang",
      "is_highlight": false,
      "score": 94.0,
      "summary": "AppleVLM enhances end-to-end autonomous driving by integrating advanced perception and planning through a novel vision encoder and dedicated planning modality, achieving state-of-the-art performance.",
      "teaser_image": "https://arxiv.org/html/2602.04256/x1.png",
      "debug_abstract": "End-to-end autonomous driving has emerged as a promising paradigm integrating perception, decision-making, and control within a unified learning framework. Recently, Vision-Language Models (VLMs) have gained significant attention for their potential to enhance the robustness and generalization of end-to-end driving models in diverse and unseen scenarios. However, existing VLM-based approaches still face challenges, including suboptimal lane perception, language understanding biases, and difficulties in handling corner cases. To address these issues, we propose AppleVLM, an advanced perception and planning-enhanced VLM model for robust end-to-end driving. AppleVLM introduces a novel vision encoder and a planning strategy encoder to improve perception and decision-making. Firstly, the vision encoder fuses spatial-temporal information from multi-view images across multiple timesteps using a deformable transformer mechanism, enhancing robustness to camera variations and facilitating scalable deployment across different vehicle platforms. Secondly, unlike traditional VLM-based approaches, AppleVLM introduces a dedicated planning modality that encodes explicit Bird&#39;s-Eye-View spatial information, mitigating language biases in navigation instructions. Finally, a VLM decoder fine-tuned by a hierarchical Chain-of-Thought integrates vision, language, and planning features to output robust driving waypoints. We evaluate AppleVLM in closed-loop experiments on two CARLA benchmarks, achieving state-of-the-art driving performance. Furthermore, we deploy AppleVLM on an AGV platform and successfully showcase real-world end-to-end autonomous driving in complex outdoor environments."
    }
  ],
  "中国科学技术大学": [
    {
      "title": "SkeletonGaussian: Editable 4D Generation through Gaussian Skeletonization",
      "url": "https://arxiv.org/abs/2602.04271",
      "date": "2026-02-04",
      "authors_text": "Lifan Wu, Ruijie Zhu, Yubo Ai, Tianzhu Zhang",
      "is_highlight": false,
      "score": 65.0,
      "summary": "SkeletonGaussian introduces a novel framework for editable 4D generation by using a hierarchical skeleton-driven approach to enhance control over dynamic 3D object synthesis.",
      "teaser_image": "https://arxiv.org/html/2602.04271/x2.png",
      "debug_abstract": "4D generation has made remarkable progress in synthesizing dynamic 3D objects from input text, images, or videos. However, existing methods often represent motion as an implicit deformation field, which limits direct control and editability. To address this issue, we propose SkeletonGaussian, a novel framework for generating editable dynamic 3D Gaussians from monocular video input. Our approach introduces a hierarchical articulated representation that decomposes motion into sparse rigid motion explicitly driven by a skeleton and fine-grained non-rigid motion. Concretely, we extract a robust skeleton and drive rigid motion via linear blend skinning, followed by a hexplane-based refinement for non-rigid deformations, enhancing interpretability and editability. Experimental results demonstrate that SkeletonGaussian surpasses existing methods in generation quality while enabling intuitive motion editing, establishing a new paradigm for editable 4D generation. Project page: this https URL"
    }
  ],
  "四川大学": [
    {
      "title": "Multiview Self-Representation Learning across Heterogeneous Views",
      "url": "https://arxiv.org/abs/2602.04328",
      "date": "2026-02-04",
      "authors_text": "Jie Chen, Zhu Wang, Chuanbin Liu, Xi Peng",
      "is_highlight": false,
      "score": 74.0,
      "summary": "The paper presents a multiview self-representation learning method that effectively learns invariant representations from heterogeneous visual data using an information-passing mechanism and consistency scheme.",
      "teaser_image": "https://arxiv.org/html/2602.04328/x1.png",
      "debug_abstract": "Features of the same sample generated by different pretrained models often exhibit inherently distinct feature distributions because of discrepancies in the model pretraining objectives or architectures. Learning invariant representations from large-scale unlabeled visual data with various pretrained models in a fully unsupervised transfer manner remains a significant challenge. In this paper, we propose a multiview self-representation learning (MSRL) method in which invariant representations are learned by exploiting the self-representation property of features across heterogeneous views. The features are derived from large-scale unlabeled visual data through transfer learning with various pretrained models and are referred to as heterogeneous multiview data. An individual linear model is stacked on top of its corresponding frozen pretrained backbone. We introduce an information-passing mechanism that relies on self-representation learning to support feature aggregation over the outputs of the linear model. Moreover, an assignment probability distribution consistency scheme is presented to guide multiview self-representation learning by exploiting complementary information across different views. Consequently, representation invariance across different linear models is enforced through this scheme. In addition, we provide a theoretical analysis of the information-passing mechanism, the assignment probability distribution consistency and the incremental views. Extensive experiments with multiple benchmark visual datasets demonstrate that the proposed MSRL method consistently outperforms several state-of-the-art approaches."
    }
  ],
  "Hengshuang Zhao老师实验室": [
    {
      "title": "SynthVerse: A Large-Scale Diverse Synthetic Dataset for Point Tracking",
      "url": "https://arxiv.org/abs/2602.04441",
      "date": "2026-02-04",
      "authors_text": "Weiguang Zhao, Haoran Xu, Xingyu Miao, Qin Zhao, Rui Zhang",
      "is_highlight": false,
      "score": 69.0,
      "summary": "SynthVerse is a comprehensive synthetic dataset designed to enhance point tracking by offering diverse domains, high-quality annotations, and a robust benchmark for evaluating tracking methods.",
      "teaser_image": "https://arxiv.org/html/2602.04441/x2.png",
      "debug_abstract": "Point tracking aims to follow visual points through complex motion, occlusion, and viewpoint changes, and has advanced rapidly with modern foundation models. Yet progress toward general point tracking remains constrained by limited high-quality data, as existing datasets often provide insufficient diversity and imperfect trajectory annotations. To this end, we introduce SynthVerse, a large-scale, diverse synthetic dataset specifically designed for point tracking. SynthVerse includes several new domains and object types missing from existing synthetic datasets, such as animated-film-style content, embodied manipulation, scene navigation, and articulated objects. SynthVerse substantially expands dataset diversity by covering a broader range of object categories and providing high-quality dynamic motions and interactions, enabling more robust training and evaluation for general point tracking. In addition, we establish a highly diverse point tracking benchmark to systematically evaluate state-of-the-art methods under broader domain shifts. Extensive experiments and analyses demonstrate that training with SynthVerse yields consistent improvements in generalization and reveal limitations of existing trackers under diverse settings."
    },
    {
      "title": "Safe and Stylized Trajectory Planning for Autonomous Driving via Diffusion Model",
      "url": "https://arxiv.org/abs/2602.04329",
      "date": "2026-02-04",
      "authors_text": "Shuo Pei, Yong Wang, Yuanchen Zhu, Chen Sun, Qin Li",
      "is_highlight": false,
      "score": 83.0,
      "summary": "The SDD Planner utilizes a diffusion-based approach for real-time trajectory planning in autonomous driving, balancing safety and driving styles, outperforming existing benchmarks.",
      "teaser_image": "https://arxiv.org/html/2602.04329/figures/kuangjia.png",
      "debug_abstract": "Achieving safe and stylized trajectory planning in complex real-world scenarios remains a critical challenge for autonomous driving systems. This paper proposes the SDD Planner, a diffusion-based framework designed to effectively reconcile safety constraints with driving styles in real time. The framework integrates two core modules: a Multi-Source Style-Aware Encoder, which employs distance-sensitive attention to fuse dynamic agent data and environmental contexts for heterogeneous safety-style perception; and a Style-Guided Dynamic Trajectory Generator, which adaptively modulates priority weights within the diffusion denoising process to generate user-preferred yet safe trajectories. Extensive experiments demonstrate that SDD Planner achieves state-of-the-art performance. On the StyleDrive benchmark, it improves the SM-PDMS metric by 3.9% over WoTE, the strongest baseline. Furthermore, on the NuPlan Test14 and Test14-hard benchmarks, SDD Planner ranks first with overall scores of 91.76 and 80.32, respectively, outperforming leading methods such as PLUTO. Real-vehicle closed-loop tests further confirm that SDD Planner maintains high safety standards while aligning with preset driving styles, validating its practical applicability for real-world deployment."
    }
  ],
  "Language and Vision (LaVi) Lab": [
    {
      "title": "SynthVerse: A Large-Scale Diverse Synthetic Dataset for Point Tracking",
      "url": "https://arxiv.org/abs/2602.04441",
      "date": "2026-02-04",
      "authors_text": "Weiguang Zhao, Haoran Xu, Xingyu Miao, Qin Zhao, Rui Zhang",
      "is_highlight": false,
      "score": 69.0,
      "summary": "SynthVerse is a comprehensive synthetic dataset designed to enhance point tracking by offering diverse domains, high-quality annotations, and a robust benchmark for evaluating tracking methods.",
      "teaser_image": "https://arxiv.org/html/2602.04441/x2.png",
      "debug_abstract": "Point tracking aims to follow visual points through complex motion, occlusion, and viewpoint changes, and has advanced rapidly with modern foundation models. Yet progress toward general point tracking remains constrained by limited high-quality data, as existing datasets often provide insufficient diversity and imperfect trajectory annotations. To this end, we introduce SynthVerse, a large-scale, diverse synthetic dataset specifically designed for point tracking. SynthVerse includes several new domains and object types missing from existing synthetic datasets, such as animated-film-style content, embodied manipulation, scene navigation, and articulated objects. SynthVerse substantially expands dataset diversity by covering a broader range of object categories and providing high-quality dynamic motions and interactions, enabling more robust training and evaluation for general point tracking. In addition, we establish a highly diverse point tracking benchmark to systematically evaluate state-of-the-art methods under broader domain shifts. Extensive experiments and analyses demonstrate that training with SynthVerse yields consistent improvements in generalization and reveal limitations of existing trackers under diverse settings."
    },
    {
      "title": "Safe and Stylized Trajectory Planning for Autonomous Driving via Diffusion Model",
      "url": "https://arxiv.org/abs/2602.04329",
      "date": "2026-02-04",
      "authors_text": "Shuo Pei, Yong Wang, Yuanchen Zhu, Chen Sun, Qin Li",
      "is_highlight": false,
      "score": 83.0,
      "summary": "The SDD Planner utilizes a diffusion-based approach for real-time trajectory planning in autonomous driving, balancing safety and driving styles, outperforming existing benchmarks.",
      "teaser_image": "https://arxiv.org/html/2602.04329/figures/kuangjia.png",
      "debug_abstract": "Achieving safe and stylized trajectory planning in complex real-world scenarios remains a critical challenge for autonomous driving systems. This paper proposes the SDD Planner, a diffusion-based framework designed to effectively reconcile safety constraints with driving styles in real time. The framework integrates two core modules: a Multi-Source Style-Aware Encoder, which employs distance-sensitive attention to fuse dynamic agent data and environmental contexts for heterogeneous safety-style perception; and a Style-Guided Dynamic Trajectory Generator, which adaptively modulates priority weights within the diffusion denoising process to generate user-preferred yet safe trajectories. Extensive experiments demonstrate that SDD Planner achieves state-of-the-art performance. On the StyleDrive benchmark, it improves the SM-PDMS metric by 3.9% over WoTE, the strongest baseline. Furthermore, on the NuPlan Test14 and Test14-hard benchmarks, SDD Planner ranks first with overall scores of 91.76 and 80.32, respectively, outperforming leading methods such as PLUTO. Real-vehicle closed-loop tests further confirm that SDD Planner maintains high safety standards while aligning with preset driving styles, validating its practical applicability for real-world deployment."
    }
  ],
  "香港大学机械工程系机器人实验室": [
    {
      "title": "SynthVerse: A Large-Scale Diverse Synthetic Dataset for Point Tracking",
      "url": "https://arxiv.org/abs/2602.04441",
      "date": "2026-02-04",
      "authors_text": "Weiguang Zhao, Haoran Xu, Xingyu Miao, Qin Zhao, Rui Zhang",
      "is_highlight": false,
      "score": 69.0,
      "summary": "SynthVerse is a comprehensive synthetic dataset designed to enhance point tracking by offering diverse domains, high-quality annotations, and a robust benchmark for evaluating tracking methods.",
      "teaser_image": "https://arxiv.org/html/2602.04441/x2.png",
      "debug_abstract": "Point tracking aims to follow visual points through complex motion, occlusion, and viewpoint changes, and has advanced rapidly with modern foundation models. Yet progress toward general point tracking remains constrained by limited high-quality data, as existing datasets often provide insufficient diversity and imperfect trajectory annotations. To this end, we introduce SynthVerse, a large-scale, diverse synthetic dataset specifically designed for point tracking. SynthVerse includes several new domains and object types missing from existing synthetic datasets, such as animated-film-style content, embodied manipulation, scene navigation, and articulated objects. SynthVerse substantially expands dataset diversity by covering a broader range of object categories and providing high-quality dynamic motions and interactions, enabling more robust training and evaluation for general point tracking. In addition, we establish a highly diverse point tracking benchmark to systematically evaluate state-of-the-art methods under broader domain shifts. Extensive experiments and analyses demonstrate that training with SynthVerse yields consistent improvements in generalization and reveal limitations of existing trackers under diverse settings."
    },
    {
      "title": "Safe and Stylized Trajectory Planning for Autonomous Driving via Diffusion Model",
      "url": "https://arxiv.org/abs/2602.04329",
      "date": "2026-02-04",
      "authors_text": "Shuo Pei, Yong Wang, Yuanchen Zhu, Chen Sun, Qin Li",
      "is_highlight": false,
      "score": 83.0,
      "summary": "The SDD Planner utilizes a diffusion-based approach for real-time trajectory planning in autonomous driving, balancing safety and driving styles, outperforming existing benchmarks.",
      "teaser_image": "https://arxiv.org/html/2602.04329/figures/kuangjia.png",
      "debug_abstract": "Achieving safe and stylized trajectory planning in complex real-world scenarios remains a critical challenge for autonomous driving systems. This paper proposes the SDD Planner, a diffusion-based framework designed to effectively reconcile safety constraints with driving styles in real time. The framework integrates two core modules: a Multi-Source Style-Aware Encoder, which employs distance-sensitive attention to fuse dynamic agent data and environmental contexts for heterogeneous safety-style perception; and a Style-Guided Dynamic Trajectory Generator, which adaptively modulates priority weights within the diffusion denoising process to generate user-preferred yet safe trajectories. Extensive experiments demonstrate that SDD Planner achieves state-of-the-art performance. On the StyleDrive benchmark, it improves the SM-PDMS metric by 3.9% over WoTE, the strongest baseline. Furthermore, on the NuPlan Test14 and Test14-hard benchmarks, SDD Planner ranks first with overall scores of 91.76 and 80.32, respectively, outperforming leading methods such as PLUTO. Real-vehicle closed-loop tests further confirm that SDD Planner maintains high safety standards while aligning with preset driving styles, validating its practical applicability for real-world deployment."
    }
  ],
  "潘佳老师实验室": [
    {
      "title": "SynthVerse: A Large-Scale Diverse Synthetic Dataset for Point Tracking",
      "url": "https://arxiv.org/abs/2602.04441",
      "date": "2026-02-04",
      "authors_text": "Weiguang Zhao, Haoran Xu, Xingyu Miao, Qin Zhao, Rui Zhang",
      "is_highlight": false,
      "score": 69.0,
      "summary": "SynthVerse is a comprehensive synthetic dataset designed to enhance point tracking by offering diverse domains, high-quality annotations, and a robust benchmark for evaluating tracking methods.",
      "teaser_image": "https://arxiv.org/html/2602.04441/x2.png",
      "debug_abstract": "Point tracking aims to follow visual points through complex motion, occlusion, and viewpoint changes, and has advanced rapidly with modern foundation models. Yet progress toward general point tracking remains constrained by limited high-quality data, as existing datasets often provide insufficient diversity and imperfect trajectory annotations. To this end, we introduce SynthVerse, a large-scale, diverse synthetic dataset specifically designed for point tracking. SynthVerse includes several new domains and object types missing from existing synthetic datasets, such as animated-film-style content, embodied manipulation, scene navigation, and articulated objects. SynthVerse substantially expands dataset diversity by covering a broader range of object categories and providing high-quality dynamic motions and interactions, enabling more robust training and evaluation for general point tracking. In addition, we establish a highly diverse point tracking benchmark to systematically evaluate state-of-the-art methods under broader domain shifts. Extensive experiments and analyses demonstrate that training with SynthVerse yields consistent improvements in generalization and reveal limitations of existing trackers under diverse settings."
    },
    {
      "title": "Safe and Stylized Trajectory Planning for Autonomous Driving via Diffusion Model",
      "url": "https://arxiv.org/abs/2602.04329",
      "date": "2026-02-04",
      "authors_text": "Shuo Pei, Yong Wang, Yuanchen Zhu, Chen Sun, Qin Li",
      "is_highlight": false,
      "score": 83.0,
      "summary": "The SDD Planner utilizes a diffusion-based approach for real-time trajectory planning in autonomous driving, balancing safety and driving styles, outperforming existing benchmarks.",
      "teaser_image": "https://arxiv.org/html/2602.04329/figures/kuangjia.png",
      "debug_abstract": "Achieving safe and stylized trajectory planning in complex real-world scenarios remains a critical challenge for autonomous driving systems. This paper proposes the SDD Planner, a diffusion-based framework designed to effectively reconcile safety constraints with driving styles in real time. The framework integrates two core modules: a Multi-Source Style-Aware Encoder, which employs distance-sensitive attention to fuse dynamic agent data and environmental contexts for heterogeneous safety-style perception; and a Style-Guided Dynamic Trajectory Generator, which adaptively modulates priority weights within the diffusion denoising process to generate user-preferred yet safe trajectories. Extensive experiments demonstrate that SDD Planner achieves state-of-the-art performance. On the StyleDrive benchmark, it improves the SM-PDMS metric by 3.9% over WoTE, the strongest baseline. Furthermore, on the NuPlan Test14 and Test14-hard benchmarks, SDD Planner ranks first with overall scores of 91.76 and 80.32, respectively, outperforming leading methods such as PLUTO. Real-vehicle closed-loop tests further confirm that SDD Planner maintains high safety standards while aligning with preset driving styles, validating its practical applicability for real-world deployment."
    }
  ],
  "OpenDriveLab": [
    {
      "title": "SynthVerse: A Large-Scale Diverse Synthetic Dataset for Point Tracking",
      "url": "https://arxiv.org/abs/2602.04441",
      "date": "2026-02-04",
      "authors_text": "Weiguang Zhao, Haoran Xu, Xingyu Miao, Qin Zhao, Rui Zhang",
      "is_highlight": false,
      "score": 69.0,
      "summary": "SynthVerse is a comprehensive synthetic dataset designed to enhance point tracking by offering diverse domains, high-quality annotations, and a robust benchmark for evaluating tracking methods.",
      "teaser_image": "https://arxiv.org/html/2602.04441/x2.png",
      "debug_abstract": "Point tracking aims to follow visual points through complex motion, occlusion, and viewpoint changes, and has advanced rapidly with modern foundation models. Yet progress toward general point tracking remains constrained by limited high-quality data, as existing datasets often provide insufficient diversity and imperfect trajectory annotations. To this end, we introduce SynthVerse, a large-scale, diverse synthetic dataset specifically designed for point tracking. SynthVerse includes several new domains and object types missing from existing synthetic datasets, such as animated-film-style content, embodied manipulation, scene navigation, and articulated objects. SynthVerse substantially expands dataset diversity by covering a broader range of object categories and providing high-quality dynamic motions and interactions, enabling more robust training and evaluation for general point tracking. In addition, we establish a highly diverse point tracking benchmark to systematically evaluate state-of-the-art methods under broader domain shifts. Extensive experiments and analyses demonstrate that training with SynthVerse yields consistent improvements in generalization and reveal limitations of existing trackers under diverse settings."
    },
    {
      "title": "Safe and Stylized Trajectory Planning for Autonomous Driving via Diffusion Model",
      "url": "https://arxiv.org/abs/2602.04329",
      "date": "2026-02-04",
      "authors_text": "Shuo Pei, Yong Wang, Yuanchen Zhu, Chen Sun, Qin Li",
      "is_highlight": false,
      "score": 83.0,
      "summary": "The SDD Planner utilizes a diffusion-based approach for real-time trajectory planning in autonomous driving, balancing safety and driving styles, outperforming existing benchmarks.",
      "teaser_image": "https://arxiv.org/html/2602.04329/figures/kuangjia.png",
      "debug_abstract": "Achieving safe and stylized trajectory planning in complex real-world scenarios remains a critical challenge for autonomous driving systems. This paper proposes the SDD Planner, a diffusion-based framework designed to effectively reconcile safety constraints with driving styles in real time. The framework integrates two core modules: a Multi-Source Style-Aware Encoder, which employs distance-sensitive attention to fuse dynamic agent data and environmental contexts for heterogeneous safety-style perception; and a Style-Guided Dynamic Trajectory Generator, which adaptively modulates priority weights within the diffusion denoising process to generate user-preferred yet safe trajectories. Extensive experiments demonstrate that SDD Planner achieves state-of-the-art performance. On the StyleDrive benchmark, it improves the SM-PDMS metric by 3.9% over WoTE, the strongest baseline. Furthermore, on the NuPlan Test14 and Test14-hard benchmarks, SDD Planner ranks first with overall scores of 91.76 and 80.32, respectively, outperforming leading methods such as PLUTO. Real-vehicle closed-loop tests further confirm that SDD Planner maintains high safety standards while aligning with preset driving styles, validating its practical applicability for real-world deployment."
    }
  ],
  "智能技术与系统实验室": [
    {
      "title": "Skin Tokens: A Learned Compact Representation for Unified Autoregressive Rigging",
      "url": "https://arxiv.org/abs/2602.04805",
      "date": "2026-02-04",
      "authors_text": "Jia-peng Zhang, Cheng-Feng Pu, Meng-Hao Guo, Yan-Pei Cao, Shi-Min Hu",
      "is_highlight": false,
      "score": 63.0,
      "summary": "The paper introduces SkinTokens, a learned representation for skinning weights that improves rigging accuracy and efficiency in 3D animation through a unified autoregressive framework.",
      "teaser_image": "https://arxiv.org/html/2602.04805/src/teaser-v2.png",
      "debug_abstract": "The rapid proliferation of generative 3D models has created a critical bottleneck in animation pipelines: rigging. Existing automated methods are fundamentally limited by their approach to skinning, treating it as an ill-posed, high-dimensional regression task that is inefficient to optimize and is typically decoupled from skeleton generation. We posit this is a representation problem and introduce SkinTokens: a learned, compact, and discrete representation for skinning weights. By leveraging an FSQ-CVAE to capture the intrinsic sparsity of skinning, we reframe the task from continuous regression to a more tractable token sequence prediction problem. This representation enables TokenRig, a unified autoregressive framework that models the entire rig as a single sequence of skeletal parameters and SkinTokens, learning the complicated dependencies between skeletons and skin deformations. The unified model is then amenable to a reinforcement learning stage, where tailored geometric and semantic rewards improve generalization to complex, out-of-distribution assets. Quantitatively, the SkinTokens representation leads to a 98%-133% percents improvement in skinning accuracy over state-of-the-art methods, while the full TokenRig framework, refined with RL, enhances bone prediction by 17%-22%. Our work presents a unified, generative approach to rigging that yields higher fidelity and robustness, offering a scalable solution to a long-standing challenge in 3D content creation."
    },
    {
      "title": "SynthVerse: A Large-Scale Diverse Synthetic Dataset for Point Tracking",
      "url": "https://arxiv.org/abs/2602.04441",
      "date": "2026-02-04",
      "authors_text": "Weiguang Zhao, Haoran Xu, Xingyu Miao, Qin Zhao, Rui Zhang",
      "is_highlight": false,
      "score": 69.0,
      "summary": "SynthVerse is a comprehensive synthetic dataset designed to enhance point tracking by offering diverse domains, high-quality annotations, and a robust benchmark for evaluating tracking methods.",
      "teaser_image": "https://arxiv.org/html/2602.04441/x2.png",
      "debug_abstract": "Point tracking aims to follow visual points through complex motion, occlusion, and viewpoint changes, and has advanced rapidly with modern foundation models. Yet progress toward general point tracking remains constrained by limited high-quality data, as existing datasets often provide insufficient diversity and imperfect trajectory annotations. To this end, we introduce SynthVerse, a large-scale, diverse synthetic dataset specifically designed for point tracking. SynthVerse includes several new domains and object types missing from existing synthetic datasets, such as animated-film-style content, embodied manipulation, scene navigation, and articulated objects. SynthVerse substantially expands dataset diversity by covering a broader range of object categories and providing high-quality dynamic motions and interactions, enabling more robust training and evaluation for general point tracking. In addition, we establish a highly diverse point tracking benchmark to systematically evaluate state-of-the-art methods under broader domain shifts. Extensive experiments and analyses demonstrate that training with SynthVerse yields consistent improvements in generalization and reveal limitations of existing trackers under diverse settings."
    },
    {
      "title": "Self-evolving Embodied AI",
      "url": "https://arxiv.org/abs/2602.04411",
      "date": "2026-02-04",
      "authors_text": "Tongtong Feng, Xin Wang, Wenwu Zhu",
      "is_highlight": false,
      "score": 80.0,
      "summary": "This paper proposes self-evolving embodied AI, enabling agents to autonomously adapt and learn in dynamic environments through self-updating memory and evolving models.",
      "teaser_image": "https://arxiv.org/html/2602.04411/x2.png",
      "debug_abstract": "Embodied Artificial Intelligence (AI) is an intelligent system formed by agents and their environment through active perception, embodied cognition, and action interaction. Existing embodied AI remains confined to human-crafted setting, in which agents are trained on given memory and construct models for given tasks, enabling fixed embodiments to interact with relatively static environments. Such methods fail in in-the-wild setting characterized by variable embodiments and dynamic open environments. This paper introduces self-evolving embodied AI, a new paradigm in which agents operate based on their changing state and environment with memory self-updating, task self-switching, environment self-prediction, embodiment self-adaptation, and model self-evolution, aiming to achieve continually adaptive intelligence with autonomous evolution. Specifically, we present the definition, framework, components, and mechanisms of self-evolving embodied AI, systematically review state-of-the-art works for realized components, discuss practical applications, and point out future research directions. We believe that self-evolving embodied AI enables agents to autonomously learn and interact with environments in a human-like manner and provide a new perspective toward general artificial intelligence."
    },
    {
      "title": "Enabling Real-Time Colonoscopic Polyp Segmentation on Commodity CPUs via Ultra-Lightweight Architecture",
      "url": "https://arxiv.org/abs/2602.04381",
      "date": "2026-02-04",
      "authors_text": "Weihao Gao, Zhuo Deng, Zheng Gong, Lan Ma",
      "is_highlight": false,
      "score": 66.0,
      "summary": "The UltraSeg models enable real-time polyp segmentation on standard CPUs with extreme parameter compression, achieving high accuracy and speed for practical clinical deployment.",
      "teaser_image": "https://arxiv.org/html/2602.04381/x1.png",
      "debug_abstract": "Early detection of colorectal cancer hinges on real-time, accurate polyp identification and resection. Yet current high-precision segmentation models rely on GPUs, making them impractical to deploy in primary hospitals, mobile endoscopy units, or capsule robots. To bridge this gap, we present the UltraSeg family, operating in an extreme-compression regime (&lt;0.3 M parameters). UltraSeg-108K (0.108 M parameters) is optimized for single-center data, while UltraSeg-130K (0.13 M parameters) generalizes to multi-center, multi-modal images. By jointly optimizing encoder-decoder widths, incorporating constrained dilated convolutions to enlarge receptive fields, and integrating a cross-layer lightweight fusion module, the models achieve 90 FPS on a single CPU core without sacrificing accuracy. Evaluated on seven public datasets, UltraSeg retains &gt;94% of the Dice score of a 31 M-parameter U-Net while utilizing only 0.4% of its parameters, establishing a strong, clinically viable baseline for the extreme-compression domain and offering an immediately deployable solution for resource-constrained settings. This work provides not only a CPU-native solution for colonoscopy but also a reproducible blueprint for broader minimally invasive surgical vision applications. Source code is publicly available to ensure reproducibility and facilitate future benchmarking."
    },
    {
      "title": "Explicit Uncertainty Modeling for Active CLIP Adaptation with Dual Prompt Tuning",
      "url": "https://arxiv.org/abs/2602.04340",
      "date": "2026-02-04",
      "authors_text": "Qian-Wei Wang, Yaguang Song, Shu-Tao Xia",
      "is_highlight": false,
      "score": 71.0,
      "summary": "This paper presents a dual-prompt tuning framework for active CLIP adaptation, enhancing uncertainty modeling and improving sample selection in image classification tasks under limited annotations.",
      "teaser_image": "https://arxiv.org/html/2602.04340/data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==",
      "debug_abstract": "Pre-trained vision-language models such as CLIP exhibit strong transferability, yet adapting them to downstream image classification tasks under limited annotation budgets remains challenging. In active learning settings, the model must select the most informative samples for annotation from a large pool of unlabeled data. Existing approaches typically estimate uncertainty via entropy-based criteria or representation clustering, without explicitly modeling uncertainty from the model perspective. In this work, we propose a robust uncertainty modeling framework for active CLIP adaptation based on dual-prompt tuning. We introduce two learnable prompts in the textual branch of CLIP. The positive prompt enhances the discriminability of task-specific textual embeddings corresponding to light-weight tuned visual embeddings, improving classification reliability. Meanwhile, the negative prompt is trained in an reversed manner to explicitly model the probability that the predicted label is correct, providing a principled uncertainty signal for guiding active sample selection. Extensive experiments across different fine-tuning paradigms demonstrate that our method consistently outperforms existing active learning methods under the same annotation budget."
    },
    {
      "title": "Fine-tuning Pre-trained Vision-Language Models in a Human-Annotation-Free Manner",
      "url": "https://arxiv.org/abs/2602.04337",
      "date": "2026-02-04",
      "authors_text": "Qian-Wei Wang, Guanghao Meng, Ren Cai, Yaguang Song, Shu-Tao Xia",
      "is_highlight": false,
      "score": 68.0,
      "summary": "The paper presents Collaborative Fine-Tuning (CoFT), an unsupervised framework for adapting vision-language models without labeled data, enhancing performance through dual-prompt learning and collaborative filtering.",
      "teaser_image": "https://arxiv.org/html/2602.04337/pic/overall.png",
      "debug_abstract": "Large-scale vision-language models (VLMs) such as CLIP exhibit strong zero-shot generalization, but adapting them to downstream tasks typically requires costly labeled data. Existing unsupervised self-training methods rely on pseudo-labeling, yet often suffer from unreliable confidence filtering, confirmation bias, and underutilization of low-confidence samples. We propose Collaborative Fine-Tuning (CoFT), an unsupervised adaptation framework that leverages unlabeled data through a dual-model, cross-modal collaboration mechanism. CoFT introduces a dual-prompt learning strategy with positive and negative textual prompts to explicitly model pseudo-label cleanliness in a sample-dependent manner, removing the need for hand-crafted thresholds or noise assumptions. The negative prompt also regularizes lightweight visual adaptation modules, improving robustness under noisy supervision. CoFT employs a two-phase training scheme, transitioning from parameter-efficient fine-tuning on high-confidence samples to full fine-tuning guided by collaboratively filtered pseudo-labels. Building on CoFT, CoFT+ further enhances adaptation via iterative fine-tuning, momentum contrastive learning, and LLM-generated prompts. Extensive experiments demonstrate consistent gains over existing unsupervised methods and even few-shot supervised baselines."
    }
  ],
  "智能产业研究院 (AIR)": [
    {
      "title": "Skin Tokens: A Learned Compact Representation for Unified Autoregressive Rigging",
      "url": "https://arxiv.org/abs/2602.04805",
      "date": "2026-02-04",
      "authors_text": "Jia-peng Zhang, Cheng-Feng Pu, Meng-Hao Guo, Yan-Pei Cao, Shi-Min Hu",
      "is_highlight": false,
      "score": 63.0,
      "summary": "The paper introduces SkinTokens, a learned representation for skinning weights that improves rigging accuracy and efficiency in 3D animation through a unified autoregressive framework.",
      "teaser_image": "https://arxiv.org/html/2602.04805/src/teaser-v2.png",
      "debug_abstract": "The rapid proliferation of generative 3D models has created a critical bottleneck in animation pipelines: rigging. Existing automated methods are fundamentally limited by their approach to skinning, treating it as an ill-posed, high-dimensional regression task that is inefficient to optimize and is typically decoupled from skeleton generation. We posit this is a representation problem and introduce SkinTokens: a learned, compact, and discrete representation for skinning weights. By leveraging an FSQ-CVAE to capture the intrinsic sparsity of skinning, we reframe the task from continuous regression to a more tractable token sequence prediction problem. This representation enables TokenRig, a unified autoregressive framework that models the entire rig as a single sequence of skeletal parameters and SkinTokens, learning the complicated dependencies between skeletons and skin deformations. The unified model is then amenable to a reinforcement learning stage, where tailored geometric and semantic rewards improve generalization to complex, out-of-distribution assets. Quantitatively, the SkinTokens representation leads to a 98%-133% percents improvement in skinning accuracy over state-of-the-art methods, while the full TokenRig framework, refined with RL, enhances bone prediction by 17%-22%. Our work presents a unified, generative approach to rigging that yields higher fidelity and robustness, offering a scalable solution to a long-standing challenge in 3D content creation."
    },
    {
      "title": "SynthVerse: A Large-Scale Diverse Synthetic Dataset for Point Tracking",
      "url": "https://arxiv.org/abs/2602.04441",
      "date": "2026-02-04",
      "authors_text": "Weiguang Zhao, Haoran Xu, Xingyu Miao, Qin Zhao, Rui Zhang",
      "is_highlight": false,
      "score": 69.0,
      "summary": "SynthVerse is a comprehensive synthetic dataset designed to enhance point tracking by offering diverse domains, high-quality annotations, and a robust benchmark for evaluating tracking methods.",
      "teaser_image": "https://arxiv.org/html/2602.04441/x2.png",
      "debug_abstract": "Point tracking aims to follow visual points through complex motion, occlusion, and viewpoint changes, and has advanced rapidly with modern foundation models. Yet progress toward general point tracking remains constrained by limited high-quality data, as existing datasets often provide insufficient diversity and imperfect trajectory annotations. To this end, we introduce SynthVerse, a large-scale, diverse synthetic dataset specifically designed for point tracking. SynthVerse includes several new domains and object types missing from existing synthetic datasets, such as animated-film-style content, embodied manipulation, scene navigation, and articulated objects. SynthVerse substantially expands dataset diversity by covering a broader range of object categories and providing high-quality dynamic motions and interactions, enabling more robust training and evaluation for general point tracking. In addition, we establish a highly diverse point tracking benchmark to systematically evaluate state-of-the-art methods under broader domain shifts. Extensive experiments and analyses demonstrate that training with SynthVerse yields consistent improvements in generalization and reveal limitations of existing trackers under diverse settings."
    },
    {
      "title": "Self-evolving Embodied AI",
      "url": "https://arxiv.org/abs/2602.04411",
      "date": "2026-02-04",
      "authors_text": "Tongtong Feng, Xin Wang, Wenwu Zhu",
      "is_highlight": false,
      "score": 80.0,
      "summary": "This paper proposes self-evolving embodied AI, enabling agents to autonomously adapt and learn in dynamic environments through self-updating memory and evolving models.",
      "teaser_image": "https://arxiv.org/html/2602.04411/x2.png",
      "debug_abstract": "Embodied Artificial Intelligence (AI) is an intelligent system formed by agents and their environment through active perception, embodied cognition, and action interaction. Existing embodied AI remains confined to human-crafted setting, in which agents are trained on given memory and construct models for given tasks, enabling fixed embodiments to interact with relatively static environments. Such methods fail in in-the-wild setting characterized by variable embodiments and dynamic open environments. This paper introduces self-evolving embodied AI, a new paradigm in which agents operate based on their changing state and environment with memory self-updating, task self-switching, environment self-prediction, embodiment self-adaptation, and model self-evolution, aiming to achieve continually adaptive intelligence with autonomous evolution. Specifically, we present the definition, framework, components, and mechanisms of self-evolving embodied AI, systematically review state-of-the-art works for realized components, discuss practical applications, and point out future research directions. We believe that self-evolving embodied AI enables agents to autonomously learn and interact with environments in a human-like manner and provide a new perspective toward general artificial intelligence."
    },
    {
      "title": "Enabling Real-Time Colonoscopic Polyp Segmentation on Commodity CPUs via Ultra-Lightweight Architecture",
      "url": "https://arxiv.org/abs/2602.04381",
      "date": "2026-02-04",
      "authors_text": "Weihao Gao, Zhuo Deng, Zheng Gong, Lan Ma",
      "is_highlight": false,
      "score": 66.0,
      "summary": "The UltraSeg models enable real-time polyp segmentation on standard CPUs with extreme parameter compression, achieving high accuracy and speed for practical clinical deployment.",
      "teaser_image": "https://arxiv.org/html/2602.04381/x1.png",
      "debug_abstract": "Early detection of colorectal cancer hinges on real-time, accurate polyp identification and resection. Yet current high-precision segmentation models rely on GPUs, making them impractical to deploy in primary hospitals, mobile endoscopy units, or capsule robots. To bridge this gap, we present the UltraSeg family, operating in an extreme-compression regime (&lt;0.3 M parameters). UltraSeg-108K (0.108 M parameters) is optimized for single-center data, while UltraSeg-130K (0.13 M parameters) generalizes to multi-center, multi-modal images. By jointly optimizing encoder-decoder widths, incorporating constrained dilated convolutions to enlarge receptive fields, and integrating a cross-layer lightweight fusion module, the models achieve 90 FPS on a single CPU core without sacrificing accuracy. Evaluated on seven public datasets, UltraSeg retains &gt;94% of the Dice score of a 31 M-parameter U-Net while utilizing only 0.4% of its parameters, establishing a strong, clinically viable baseline for the extreme-compression domain and offering an immediately deployable solution for resource-constrained settings. This work provides not only a CPU-native solution for colonoscopy but also a reproducible blueprint for broader minimally invasive surgical vision applications. Source code is publicly available to ensure reproducibility and facilitate future benchmarking."
    },
    {
      "title": "Explicit Uncertainty Modeling for Active CLIP Adaptation with Dual Prompt Tuning",
      "url": "https://arxiv.org/abs/2602.04340",
      "date": "2026-02-04",
      "authors_text": "Qian-Wei Wang, Yaguang Song, Shu-Tao Xia",
      "is_highlight": false,
      "score": 71.0,
      "summary": "This paper presents a dual-prompt tuning framework for active CLIP adaptation, enhancing uncertainty modeling and improving sample selection in image classification tasks under limited annotations.",
      "teaser_image": "https://arxiv.org/html/2602.04340/data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==",
      "debug_abstract": "Pre-trained vision-language models such as CLIP exhibit strong transferability, yet adapting them to downstream image classification tasks under limited annotation budgets remains challenging. In active learning settings, the model must select the most informative samples for annotation from a large pool of unlabeled data. Existing approaches typically estimate uncertainty via entropy-based criteria or representation clustering, without explicitly modeling uncertainty from the model perspective. In this work, we propose a robust uncertainty modeling framework for active CLIP adaptation based on dual-prompt tuning. We introduce two learnable prompts in the textual branch of CLIP. The positive prompt enhances the discriminability of task-specific textual embeddings corresponding to light-weight tuned visual embeddings, improving classification reliability. Meanwhile, the negative prompt is trained in an reversed manner to explicitly model the probability that the predicted label is correct, providing a principled uncertainty signal for guiding active sample selection. Extensive experiments across different fine-tuning paradigms demonstrate that our method consistently outperforms existing active learning methods under the same annotation budget."
    },
    {
      "title": "Fine-tuning Pre-trained Vision-Language Models in a Human-Annotation-Free Manner",
      "url": "https://arxiv.org/abs/2602.04337",
      "date": "2026-02-04",
      "authors_text": "Qian-Wei Wang, Guanghao Meng, Ren Cai, Yaguang Song, Shu-Tao Xia",
      "is_highlight": false,
      "score": 68.0,
      "summary": "The paper presents Collaborative Fine-Tuning (CoFT), an unsupervised framework for adapting vision-language models without labeled data, enhancing performance through dual-prompt learning and collaborative filtering.",
      "teaser_image": "https://arxiv.org/html/2602.04337/pic/overall.png",
      "debug_abstract": "Large-scale vision-language models (VLMs) such as CLIP exhibit strong zero-shot generalization, but adapting them to downstream tasks typically requires costly labeled data. Existing unsupervised self-training methods rely on pseudo-labeling, yet often suffer from unreliable confidence filtering, confirmation bias, and underutilization of low-confidence samples. We propose Collaborative Fine-Tuning (CoFT), an unsupervised adaptation framework that leverages unlabeled data through a dual-model, cross-modal collaboration mechanism. CoFT introduces a dual-prompt learning strategy with positive and negative textual prompts to explicitly model pseudo-label cleanliness in a sample-dependent manner, removing the need for hand-crafted thresholds or noise assumptions. The negative prompt also regularizes lightweight visual adaptation modules, improving robustness under noisy supervision. CoFT employs a two-phase training scheme, transitioning from parameter-efficient fine-tuning on high-confidence samples to full fine-tuning guided by collaboratively filtered pseudo-labels. Building on CoFT, CoFT+ further enhances adaptation via iterative fine-tuning, momentum contrastive learning, and LLM-generated prompts. Extensive experiments demonstrate consistent gains over existing unsupervised methods and even few-shot supervised baselines."
    }
  ],
  "多智能体与具身智能研究所": [
    {
      "title": "Explicit Uncertainty Modeling for Active CLIP Adaptation with Dual Prompt Tuning",
      "url": "https://arxiv.org/abs/2602.04340",
      "date": "2026-02-04",
      "authors_text": "Qian-Wei Wang, Yaguang Song, Shu-Tao Xia",
      "is_highlight": false,
      "score": 71.0,
      "summary": "This paper presents a dual-prompt tuning framework for active CLIP adaptation, enhancing uncertainty modeling and improving sample selection in image classification tasks under limited annotations.",
      "teaser_image": "https://arxiv.org/html/2602.04340/data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==",
      "debug_abstract": "Pre-trained vision-language models such as CLIP exhibit strong transferability, yet adapting them to downstream image classification tasks under limited annotation budgets remains challenging. In active learning settings, the model must select the most informative samples for annotation from a large pool of unlabeled data. Existing approaches typically estimate uncertainty via entropy-based criteria or representation clustering, without explicitly modeling uncertainty from the model perspective. In this work, we propose a robust uncertainty modeling framework for active CLIP adaptation based on dual-prompt tuning. We introduce two learnable prompts in the textual branch of CLIP. The positive prompt enhances the discriminability of task-specific textual embeddings corresponding to light-weight tuned visual embeddings, improving classification reliability. Meanwhile, the negative prompt is trained in an reversed manner to explicitly model the probability that the predicted label is correct, providing a principled uncertainty signal for guiding active sample selection. Extensive experiments across different fine-tuning paradigms demonstrate that our method consistently outperforms existing active learning methods under the same annotation budget."
    },
    {
      "title": "Fine-tuning Pre-trained Vision-Language Models in a Human-Annotation-Free Manner",
      "url": "https://arxiv.org/abs/2602.04337",
      "date": "2026-02-04",
      "authors_text": "Qian-Wei Wang, Guanghao Meng, Ren Cai, Yaguang Song, Shu-Tao Xia",
      "is_highlight": false,
      "score": 68.0,
      "summary": "The paper presents Collaborative Fine-Tuning (CoFT), an unsupervised framework for adapting vision-language models without labeled data, enhancing performance through dual-prompt learning and collaborative filtering.",
      "teaser_image": "https://arxiv.org/html/2602.04337/pic/overall.png",
      "debug_abstract": "Large-scale vision-language models (VLMs) such as CLIP exhibit strong zero-shot generalization, but adapting them to downstream tasks typically requires costly labeled data. Existing unsupervised self-training methods rely on pseudo-labeling, yet often suffer from unreliable confidence filtering, confirmation bias, and underutilization of low-confidence samples. We propose Collaborative Fine-Tuning (CoFT), an unsupervised adaptation framework that leverages unlabeled data through a dual-model, cross-modal collaboration mechanism. CoFT introduces a dual-prompt learning strategy with positive and negative textual prompts to explicitly model pseudo-label cleanliness in a sample-dependent manner, removing the need for hand-crafted thresholds or noise assumptions. The negative prompt also regularizes lightweight visual adaptation modules, improving robustness under noisy supervision. CoFT employs a two-phase training scheme, transitioning from parameter-efficient fine-tuning on high-confidence samples to full fine-tuning guided by collaboratively filtered pseudo-labels. Building on CoFT, CoFT+ further enhances adaptation via iterative fine-tuning, momentum contrastive learning, and LLM-generated prompts. Extensive experiments demonstrate consistent gains over existing unsupervised methods and even few-shot supervised baselines."
    }
  ],
  "智能系统与机器人实验室 (ISR Lab)": [
    {
      "title": "Skin Tokens: A Learned Compact Representation for Unified Autoregressive Rigging",
      "url": "https://arxiv.org/abs/2602.04805",
      "date": "2026-02-04",
      "authors_text": "Jia-peng Zhang, Cheng-Feng Pu, Meng-Hao Guo, Yan-Pei Cao, Shi-Min Hu",
      "is_highlight": false,
      "score": 63.0,
      "summary": "The paper introduces SkinTokens, a learned representation for skinning weights that improves rigging accuracy and efficiency in 3D animation through a unified autoregressive framework.",
      "teaser_image": "https://arxiv.org/html/2602.04805/src/teaser-v2.png",
      "debug_abstract": "The rapid proliferation of generative 3D models has created a critical bottleneck in animation pipelines: rigging. Existing automated methods are fundamentally limited by their approach to skinning, treating it as an ill-posed, high-dimensional regression task that is inefficient to optimize and is typically decoupled from skeleton generation. We posit this is a representation problem and introduce SkinTokens: a learned, compact, and discrete representation for skinning weights. By leveraging an FSQ-CVAE to capture the intrinsic sparsity of skinning, we reframe the task from continuous regression to a more tractable token sequence prediction problem. This representation enables TokenRig, a unified autoregressive framework that models the entire rig as a single sequence of skeletal parameters and SkinTokens, learning the complicated dependencies between skeletons and skin deformations. The unified model is then amenable to a reinforcement learning stage, where tailored geometric and semantic rewards improve generalization to complex, out-of-distribution assets. Quantitatively, the SkinTokens representation leads to a 98%-133% percents improvement in skinning accuracy over state-of-the-art methods, while the full TokenRig framework, refined with RL, enhances bone prediction by 17%-22%. Our work presents a unified, generative approach to rigging that yields higher fidelity and robustness, offering a scalable solution to a long-standing challenge in 3D content creation."
    },
    {
      "title": "SynthVerse: A Large-Scale Diverse Synthetic Dataset for Point Tracking",
      "url": "https://arxiv.org/abs/2602.04441",
      "date": "2026-02-04",
      "authors_text": "Weiguang Zhao, Haoran Xu, Xingyu Miao, Qin Zhao, Rui Zhang",
      "is_highlight": false,
      "score": 69.0,
      "summary": "SynthVerse is a comprehensive synthetic dataset designed to enhance point tracking by offering diverse domains, high-quality annotations, and a robust benchmark for evaluating tracking methods.",
      "teaser_image": "https://arxiv.org/html/2602.04441/x2.png",
      "debug_abstract": "Point tracking aims to follow visual points through complex motion, occlusion, and viewpoint changes, and has advanced rapidly with modern foundation models. Yet progress toward general point tracking remains constrained by limited high-quality data, as existing datasets often provide insufficient diversity and imperfect trajectory annotations. To this end, we introduce SynthVerse, a large-scale, diverse synthetic dataset specifically designed for point tracking. SynthVerse includes several new domains and object types missing from existing synthetic datasets, such as animated-film-style content, embodied manipulation, scene navigation, and articulated objects. SynthVerse substantially expands dataset diversity by covering a broader range of object categories and providing high-quality dynamic motions and interactions, enabling more robust training and evaluation for general point tracking. In addition, we establish a highly diverse point tracking benchmark to systematically evaluate state-of-the-art methods under broader domain shifts. Extensive experiments and analyses demonstrate that training with SynthVerse yields consistent improvements in generalization and reveal limitations of existing trackers under diverse settings."
    },
    {
      "title": "Self-evolving Embodied AI",
      "url": "https://arxiv.org/abs/2602.04411",
      "date": "2026-02-04",
      "authors_text": "Tongtong Feng, Xin Wang, Wenwu Zhu",
      "is_highlight": false,
      "score": 80.0,
      "summary": "This paper proposes self-evolving embodied AI, enabling agents to autonomously adapt and learn in dynamic environments through self-updating memory and evolving models.",
      "teaser_image": "https://arxiv.org/html/2602.04411/x2.png",
      "debug_abstract": "Embodied Artificial Intelligence (AI) is an intelligent system formed by agents and their environment through active perception, embodied cognition, and action interaction. Existing embodied AI remains confined to human-crafted setting, in which agents are trained on given memory and construct models for given tasks, enabling fixed embodiments to interact with relatively static environments. Such methods fail in in-the-wild setting characterized by variable embodiments and dynamic open environments. This paper introduces self-evolving embodied AI, a new paradigm in which agents operate based on their changing state and environment with memory self-updating, task self-switching, environment self-prediction, embodiment self-adaptation, and model self-evolution, aiming to achieve continually adaptive intelligence with autonomous evolution. Specifically, we present the definition, framework, components, and mechanisms of self-evolving embodied AI, systematically review state-of-the-art works for realized components, discuss practical applications, and point out future research directions. We believe that self-evolving embodied AI enables agents to autonomously learn and interact with environments in a human-like manner and provide a new perspective toward general artificial intelligence."
    },
    {
      "title": "Enabling Real-Time Colonoscopic Polyp Segmentation on Commodity CPUs via Ultra-Lightweight Architecture",
      "url": "https://arxiv.org/abs/2602.04381",
      "date": "2026-02-04",
      "authors_text": "Weihao Gao, Zhuo Deng, Zheng Gong, Lan Ma",
      "is_highlight": false,
      "score": 66.0,
      "summary": "The UltraSeg models enable real-time polyp segmentation on standard CPUs with extreme parameter compression, achieving high accuracy and speed for practical clinical deployment.",
      "teaser_image": "https://arxiv.org/html/2602.04381/x1.png",
      "debug_abstract": "Early detection of colorectal cancer hinges on real-time, accurate polyp identification and resection. Yet current high-precision segmentation models rely on GPUs, making them impractical to deploy in primary hospitals, mobile endoscopy units, or capsule robots. To bridge this gap, we present the UltraSeg family, operating in an extreme-compression regime (&lt;0.3 M parameters). UltraSeg-108K (0.108 M parameters) is optimized for single-center data, while UltraSeg-130K (0.13 M parameters) generalizes to multi-center, multi-modal images. By jointly optimizing encoder-decoder widths, incorporating constrained dilated convolutions to enlarge receptive fields, and integrating a cross-layer lightweight fusion module, the models achieve 90 FPS on a single CPU core without sacrificing accuracy. Evaluated on seven public datasets, UltraSeg retains &gt;94% of the Dice score of a 31 M-parameter U-Net while utilizing only 0.4% of its parameters, establishing a strong, clinically viable baseline for the extreme-compression domain and offering an immediately deployable solution for resource-constrained settings. This work provides not only a CPU-native solution for colonoscopy but also a reproducible blueprint for broader minimally invasive surgical vision applications. Source code is publicly available to ensure reproducibility and facilitate future benchmarking."
    },
    {
      "title": "Explicit Uncertainty Modeling for Active CLIP Adaptation with Dual Prompt Tuning",
      "url": "https://arxiv.org/abs/2602.04340",
      "date": "2026-02-04",
      "authors_text": "Qian-Wei Wang, Yaguang Song, Shu-Tao Xia",
      "is_highlight": false,
      "score": 71.0,
      "summary": "This paper presents a dual-prompt tuning framework for active CLIP adaptation, enhancing uncertainty modeling and improving sample selection in image classification tasks under limited annotations.",
      "teaser_image": "https://arxiv.org/html/2602.04340/data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==",
      "debug_abstract": "Pre-trained vision-language models such as CLIP exhibit strong transferability, yet adapting them to downstream image classification tasks under limited annotation budgets remains challenging. In active learning settings, the model must select the most informative samples for annotation from a large pool of unlabeled data. Existing approaches typically estimate uncertainty via entropy-based criteria or representation clustering, without explicitly modeling uncertainty from the model perspective. In this work, we propose a robust uncertainty modeling framework for active CLIP adaptation based on dual-prompt tuning. We introduce two learnable prompts in the textual branch of CLIP. The positive prompt enhances the discriminability of task-specific textual embeddings corresponding to light-weight tuned visual embeddings, improving classification reliability. Meanwhile, the negative prompt is trained in an reversed manner to explicitly model the probability that the predicted label is correct, providing a principled uncertainty signal for guiding active sample selection. Extensive experiments across different fine-tuning paradigms demonstrate that our method consistently outperforms existing active learning methods under the same annotation budget."
    },
    {
      "title": "Fine-tuning Pre-trained Vision-Language Models in a Human-Annotation-Free Manner",
      "url": "https://arxiv.org/abs/2602.04337",
      "date": "2026-02-04",
      "authors_text": "Qian-Wei Wang, Guanghao Meng, Ren Cai, Yaguang Song, Shu-Tao Xia",
      "is_highlight": false,
      "score": 68.0,
      "summary": "The paper presents Collaborative Fine-Tuning (CoFT), an unsupervised framework for adapting vision-language models without labeled data, enhancing performance through dual-prompt learning and collaborative filtering.",
      "teaser_image": "https://arxiv.org/html/2602.04337/pic/overall.png",
      "debug_abstract": "Large-scale vision-language models (VLMs) such as CLIP exhibit strong zero-shot generalization, but adapting them to downstream tasks typically requires costly labeled data. Existing unsupervised self-training methods rely on pseudo-labeling, yet often suffer from unreliable confidence filtering, confirmation bias, and underutilization of low-confidence samples. We propose Collaborative Fine-Tuning (CoFT), an unsupervised adaptation framework that leverages unlabeled data through a dual-model, cross-modal collaboration mechanism. CoFT introduces a dual-prompt learning strategy with positive and negative textual prompts to explicitly model pseudo-label cleanliness in a sample-dependent manner, removing the need for hand-crafted thresholds or noise assumptions. The negative prompt also regularizes lightweight visual adaptation modules, improving robustness under noisy supervision. CoFT employs a two-phase training scheme, transitioning from parameter-efficient fine-tuning on high-confidence samples to full fine-tuning guided by collaboratively filtered pseudo-labels. Building on CoFT, CoFT+ further enhances adaptation via iterative fine-tuning, momentum contrastive learning, and LLM-generated prompts. Extensive experiments demonstrate consistent gains over existing unsupervised methods and even few-shot supervised baselines."
    }
  ],
  "机器人控制实验室": [
    {
      "title": "Skin Tokens: A Learned Compact Representation for Unified Autoregressive Rigging",
      "url": "https://arxiv.org/abs/2602.04805",
      "date": "2026-02-04",
      "authors_text": "Jia-peng Zhang, Cheng-Feng Pu, Meng-Hao Guo, Yan-Pei Cao, Shi-Min Hu",
      "is_highlight": false,
      "score": 63.0,
      "summary": "The paper introduces SkinTokens, a learned representation for skinning weights that improves rigging accuracy and efficiency in 3D animation through a unified autoregressive framework.",
      "teaser_image": "https://arxiv.org/html/2602.04805/src/teaser-v2.png",
      "debug_abstract": "The rapid proliferation of generative 3D models has created a critical bottleneck in animation pipelines: rigging. Existing automated methods are fundamentally limited by their approach to skinning, treating it as an ill-posed, high-dimensional regression task that is inefficient to optimize and is typically decoupled from skeleton generation. We posit this is a representation problem and introduce SkinTokens: a learned, compact, and discrete representation for skinning weights. By leveraging an FSQ-CVAE to capture the intrinsic sparsity of skinning, we reframe the task from continuous regression to a more tractable token sequence prediction problem. This representation enables TokenRig, a unified autoregressive framework that models the entire rig as a single sequence of skeletal parameters and SkinTokens, learning the complicated dependencies between skeletons and skin deformations. The unified model is then amenable to a reinforcement learning stage, where tailored geometric and semantic rewards improve generalization to complex, out-of-distribution assets. Quantitatively, the SkinTokens representation leads to a 98%-133% percents improvement in skinning accuracy over state-of-the-art methods, while the full TokenRig framework, refined with RL, enhances bone prediction by 17%-22%. Our work presents a unified, generative approach to rigging that yields higher fidelity and robustness, offering a scalable solution to a long-standing challenge in 3D content creation."
    },
    {
      "title": "SynthVerse: A Large-Scale Diverse Synthetic Dataset for Point Tracking",
      "url": "https://arxiv.org/abs/2602.04441",
      "date": "2026-02-04",
      "authors_text": "Weiguang Zhao, Haoran Xu, Xingyu Miao, Qin Zhao, Rui Zhang",
      "is_highlight": false,
      "score": 69.0,
      "summary": "SynthVerse is a comprehensive synthetic dataset designed to enhance point tracking by offering diverse domains, high-quality annotations, and a robust benchmark for evaluating tracking methods.",
      "teaser_image": "https://arxiv.org/html/2602.04441/x2.png",
      "debug_abstract": "Point tracking aims to follow visual points through complex motion, occlusion, and viewpoint changes, and has advanced rapidly with modern foundation models. Yet progress toward general point tracking remains constrained by limited high-quality data, as existing datasets often provide insufficient diversity and imperfect trajectory annotations. To this end, we introduce SynthVerse, a large-scale, diverse synthetic dataset specifically designed for point tracking. SynthVerse includes several new domains and object types missing from existing synthetic datasets, such as animated-film-style content, embodied manipulation, scene navigation, and articulated objects. SynthVerse substantially expands dataset diversity by covering a broader range of object categories and providing high-quality dynamic motions and interactions, enabling more robust training and evaluation for general point tracking. In addition, we establish a highly diverse point tracking benchmark to systematically evaluate state-of-the-art methods under broader domain shifts. Extensive experiments and analyses demonstrate that training with SynthVerse yields consistent improvements in generalization and reveal limitations of existing trackers under diverse settings."
    },
    {
      "title": "Self-evolving Embodied AI",
      "url": "https://arxiv.org/abs/2602.04411",
      "date": "2026-02-04",
      "authors_text": "Tongtong Feng, Xin Wang, Wenwu Zhu",
      "is_highlight": false,
      "score": 80.0,
      "summary": "This paper proposes self-evolving embodied AI, enabling agents to autonomously adapt and learn in dynamic environments through self-updating memory and evolving models.",
      "teaser_image": "https://arxiv.org/html/2602.04411/x2.png",
      "debug_abstract": "Embodied Artificial Intelligence (AI) is an intelligent system formed by agents and their environment through active perception, embodied cognition, and action interaction. Existing embodied AI remains confined to human-crafted setting, in which agents are trained on given memory and construct models for given tasks, enabling fixed embodiments to interact with relatively static environments. Such methods fail in in-the-wild setting characterized by variable embodiments and dynamic open environments. This paper introduces self-evolving embodied AI, a new paradigm in which agents operate based on their changing state and environment with memory self-updating, task self-switching, environment self-prediction, embodiment self-adaptation, and model self-evolution, aiming to achieve continually adaptive intelligence with autonomous evolution. Specifically, we present the definition, framework, components, and mechanisms of self-evolving embodied AI, systematically review state-of-the-art works for realized components, discuss practical applications, and point out future research directions. We believe that self-evolving embodied AI enables agents to autonomously learn and interact with environments in a human-like manner and provide a new perspective toward general artificial intelligence."
    },
    {
      "title": "Enabling Real-Time Colonoscopic Polyp Segmentation on Commodity CPUs via Ultra-Lightweight Architecture",
      "url": "https://arxiv.org/abs/2602.04381",
      "date": "2026-02-04",
      "authors_text": "Weihao Gao, Zhuo Deng, Zheng Gong, Lan Ma",
      "is_highlight": false,
      "score": 66.0,
      "summary": "The UltraSeg models enable real-time polyp segmentation on standard CPUs with extreme parameter compression, achieving high accuracy and speed for practical clinical deployment.",
      "teaser_image": "https://arxiv.org/html/2602.04381/x1.png",
      "debug_abstract": "Early detection of colorectal cancer hinges on real-time, accurate polyp identification and resection. Yet current high-precision segmentation models rely on GPUs, making them impractical to deploy in primary hospitals, mobile endoscopy units, or capsule robots. To bridge this gap, we present the UltraSeg family, operating in an extreme-compression regime (&lt;0.3 M parameters). UltraSeg-108K (0.108 M parameters) is optimized for single-center data, while UltraSeg-130K (0.13 M parameters) generalizes to multi-center, multi-modal images. By jointly optimizing encoder-decoder widths, incorporating constrained dilated convolutions to enlarge receptive fields, and integrating a cross-layer lightweight fusion module, the models achieve 90 FPS on a single CPU core without sacrificing accuracy. Evaluated on seven public datasets, UltraSeg retains &gt;94% of the Dice score of a 31 M-parameter U-Net while utilizing only 0.4% of its parameters, establishing a strong, clinically viable baseline for the extreme-compression domain and offering an immediately deployable solution for resource-constrained settings. This work provides not only a CPU-native solution for colonoscopy but also a reproducible blueprint for broader minimally invasive surgical vision applications. Source code is publicly available to ensure reproducibility and facilitate future benchmarking."
    },
    {
      "title": "Explicit Uncertainty Modeling for Active CLIP Adaptation with Dual Prompt Tuning",
      "url": "https://arxiv.org/abs/2602.04340",
      "date": "2026-02-04",
      "authors_text": "Qian-Wei Wang, Yaguang Song, Shu-Tao Xia",
      "is_highlight": false,
      "score": 71.0,
      "summary": "This paper presents a dual-prompt tuning framework for active CLIP adaptation, enhancing uncertainty modeling and improving sample selection in image classification tasks under limited annotations.",
      "teaser_image": "https://arxiv.org/html/2602.04340/data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==",
      "debug_abstract": "Pre-trained vision-language models such as CLIP exhibit strong transferability, yet adapting them to downstream image classification tasks under limited annotation budgets remains challenging. In active learning settings, the model must select the most informative samples for annotation from a large pool of unlabeled data. Existing approaches typically estimate uncertainty via entropy-based criteria or representation clustering, without explicitly modeling uncertainty from the model perspective. In this work, we propose a robust uncertainty modeling framework for active CLIP adaptation based on dual-prompt tuning. We introduce two learnable prompts in the textual branch of CLIP. The positive prompt enhances the discriminability of task-specific textual embeddings corresponding to light-weight tuned visual embeddings, improving classification reliability. Meanwhile, the negative prompt is trained in an reversed manner to explicitly model the probability that the predicted label is correct, providing a principled uncertainty signal for guiding active sample selection. Extensive experiments across different fine-tuning paradigms demonstrate that our method consistently outperforms existing active learning methods under the same annotation budget."
    },
    {
      "title": "Fine-tuning Pre-trained Vision-Language Models in a Human-Annotation-Free Manner",
      "url": "https://arxiv.org/abs/2602.04337",
      "date": "2026-02-04",
      "authors_text": "Qian-Wei Wang, Guanghao Meng, Ren Cai, Yaguang Song, Shu-Tao Xia",
      "is_highlight": false,
      "score": 68.0,
      "summary": "The paper presents Collaborative Fine-Tuning (CoFT), an unsupervised framework for adapting vision-language models without labeled data, enhancing performance through dual-prompt learning and collaborative filtering.",
      "teaser_image": "https://arxiv.org/html/2602.04337/pic/overall.png",
      "debug_abstract": "Large-scale vision-language models (VLMs) such as CLIP exhibit strong zero-shot generalization, but adapting them to downstream tasks typically requires costly labeled data. Existing unsupervised self-training methods rely on pseudo-labeling, yet often suffer from unreliable confidence filtering, confirmation bias, and underutilization of low-confidence samples. We propose Collaborative Fine-Tuning (CoFT), an unsupervised adaptation framework that leverages unlabeled data through a dual-model, cross-modal collaboration mechanism. CoFT introduces a dual-prompt learning strategy with positive and negative textual prompts to explicitly model pseudo-label cleanliness in a sample-dependent manner, removing the need for hand-crafted thresholds or noise assumptions. The negative prompt also regularizes lightweight visual adaptation modules, improving robustness under noisy supervision. CoFT employs a two-phase training scheme, transitioning from parameter-efficient fine-tuning on high-confidence samples to full fine-tuning guided by collaboratively filtered pseudo-labels. Building on CoFT, CoFT+ further enhances adaptation via iterative fine-tuning, momentum contrastive learning, and LLM-generated prompts. Extensive experiments demonstrate consistent gains over existing unsupervised methods and even few-shot supervised baselines."
    }
  ],
  "具身智能实验室 (TEA Lab)": [
    {
      "title": "Skin Tokens: A Learned Compact Representation for Unified Autoregressive Rigging",
      "url": "https://arxiv.org/abs/2602.04805",
      "date": "2026-02-04",
      "authors_text": "Jia-peng Zhang, Cheng-Feng Pu, Meng-Hao Guo, Yan-Pei Cao, Shi-Min Hu",
      "is_highlight": false,
      "score": 63.0,
      "summary": "The paper introduces SkinTokens, a learned representation for skinning weights that improves rigging accuracy and efficiency in 3D animation through a unified autoregressive framework.",
      "teaser_image": "https://arxiv.org/html/2602.04805/src/teaser-v2.png",
      "debug_abstract": "The rapid proliferation of generative 3D models has created a critical bottleneck in animation pipelines: rigging. Existing automated methods are fundamentally limited by their approach to skinning, treating it as an ill-posed, high-dimensional regression task that is inefficient to optimize and is typically decoupled from skeleton generation. We posit this is a representation problem and introduce SkinTokens: a learned, compact, and discrete representation for skinning weights. By leveraging an FSQ-CVAE to capture the intrinsic sparsity of skinning, we reframe the task from continuous regression to a more tractable token sequence prediction problem. This representation enables TokenRig, a unified autoregressive framework that models the entire rig as a single sequence of skeletal parameters and SkinTokens, learning the complicated dependencies between skeletons and skin deformations. The unified model is then amenable to a reinforcement learning stage, where tailored geometric and semantic rewards improve generalization to complex, out-of-distribution assets. Quantitatively, the SkinTokens representation leads to a 98%-133% percents improvement in skinning accuracy over state-of-the-art methods, while the full TokenRig framework, refined with RL, enhances bone prediction by 17%-22%. Our work presents a unified, generative approach to rigging that yields higher fidelity and robustness, offering a scalable solution to a long-standing challenge in 3D content creation."
    },
    {
      "title": "SynthVerse: A Large-Scale Diverse Synthetic Dataset for Point Tracking",
      "url": "https://arxiv.org/abs/2602.04441",
      "date": "2026-02-04",
      "authors_text": "Weiguang Zhao, Haoran Xu, Xingyu Miao, Qin Zhao, Rui Zhang",
      "is_highlight": false,
      "score": 69.0,
      "summary": "SynthVerse is a comprehensive synthetic dataset designed to enhance point tracking by offering diverse domains, high-quality annotations, and a robust benchmark for evaluating tracking methods.",
      "teaser_image": "https://arxiv.org/html/2602.04441/x2.png",
      "debug_abstract": "Point tracking aims to follow visual points through complex motion, occlusion, and viewpoint changes, and has advanced rapidly with modern foundation models. Yet progress toward general point tracking remains constrained by limited high-quality data, as existing datasets often provide insufficient diversity and imperfect trajectory annotations. To this end, we introduce SynthVerse, a large-scale, diverse synthetic dataset specifically designed for point tracking. SynthVerse includes several new domains and object types missing from existing synthetic datasets, such as animated-film-style content, embodied manipulation, scene navigation, and articulated objects. SynthVerse substantially expands dataset diversity by covering a broader range of object categories and providing high-quality dynamic motions and interactions, enabling more robust training and evaluation for general point tracking. In addition, we establish a highly diverse point tracking benchmark to systematically evaluate state-of-the-art methods under broader domain shifts. Extensive experiments and analyses demonstrate that training with SynthVerse yields consistent improvements in generalization and reveal limitations of existing trackers under diverse settings."
    },
    {
      "title": "Self-evolving Embodied AI",
      "url": "https://arxiv.org/abs/2602.04411",
      "date": "2026-02-04",
      "authors_text": "Tongtong Feng, Xin Wang, Wenwu Zhu",
      "is_highlight": false,
      "score": 80.0,
      "summary": "This paper proposes self-evolving embodied AI, enabling agents to autonomously adapt and learn in dynamic environments through self-updating memory and evolving models.",
      "teaser_image": "https://arxiv.org/html/2602.04411/x2.png",
      "debug_abstract": "Embodied Artificial Intelligence (AI) is an intelligent system formed by agents and their environment through active perception, embodied cognition, and action interaction. Existing embodied AI remains confined to human-crafted setting, in which agents are trained on given memory and construct models for given tasks, enabling fixed embodiments to interact with relatively static environments. Such methods fail in in-the-wild setting characterized by variable embodiments and dynamic open environments. This paper introduces self-evolving embodied AI, a new paradigm in which agents operate based on their changing state and environment with memory self-updating, task self-switching, environment self-prediction, embodiment self-adaptation, and model self-evolution, aiming to achieve continually adaptive intelligence with autonomous evolution. Specifically, we present the definition, framework, components, and mechanisms of self-evolving embodied AI, systematically review state-of-the-art works for realized components, discuss practical applications, and point out future research directions. We believe that self-evolving embodied AI enables agents to autonomously learn and interact with environments in a human-like manner and provide a new perspective toward general artificial intelligence."
    },
    {
      "title": "Enabling Real-Time Colonoscopic Polyp Segmentation on Commodity CPUs via Ultra-Lightweight Architecture",
      "url": "https://arxiv.org/abs/2602.04381",
      "date": "2026-02-04",
      "authors_text": "Weihao Gao, Zhuo Deng, Zheng Gong, Lan Ma",
      "is_highlight": false,
      "score": 66.0,
      "summary": "The UltraSeg models enable real-time polyp segmentation on standard CPUs with extreme parameter compression, achieving high accuracy and speed for practical clinical deployment.",
      "teaser_image": "https://arxiv.org/html/2602.04381/x1.png",
      "debug_abstract": "Early detection of colorectal cancer hinges on real-time, accurate polyp identification and resection. Yet current high-precision segmentation models rely on GPUs, making them impractical to deploy in primary hospitals, mobile endoscopy units, or capsule robots. To bridge this gap, we present the UltraSeg family, operating in an extreme-compression regime (&lt;0.3 M parameters). UltraSeg-108K (0.108 M parameters) is optimized for single-center data, while UltraSeg-130K (0.13 M parameters) generalizes to multi-center, multi-modal images. By jointly optimizing encoder-decoder widths, incorporating constrained dilated convolutions to enlarge receptive fields, and integrating a cross-layer lightweight fusion module, the models achieve 90 FPS on a single CPU core without sacrificing accuracy. Evaluated on seven public datasets, UltraSeg retains &gt;94% of the Dice score of a 31 M-parameter U-Net while utilizing only 0.4% of its parameters, establishing a strong, clinically viable baseline for the extreme-compression domain and offering an immediately deployable solution for resource-constrained settings. This work provides not only a CPU-native solution for colonoscopy but also a reproducible blueprint for broader minimally invasive surgical vision applications. Source code is publicly available to ensure reproducibility and facilitate future benchmarking."
    },
    {
      "title": "Explicit Uncertainty Modeling for Active CLIP Adaptation with Dual Prompt Tuning",
      "url": "https://arxiv.org/abs/2602.04340",
      "date": "2026-02-04",
      "authors_text": "Qian-Wei Wang, Yaguang Song, Shu-Tao Xia",
      "is_highlight": false,
      "score": 71.0,
      "summary": "This paper presents a dual-prompt tuning framework for active CLIP adaptation, enhancing uncertainty modeling and improving sample selection in image classification tasks under limited annotations.",
      "teaser_image": "https://arxiv.org/html/2602.04340/data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==",
      "debug_abstract": "Pre-trained vision-language models such as CLIP exhibit strong transferability, yet adapting them to downstream image classification tasks under limited annotation budgets remains challenging. In active learning settings, the model must select the most informative samples for annotation from a large pool of unlabeled data. Existing approaches typically estimate uncertainty via entropy-based criteria or representation clustering, without explicitly modeling uncertainty from the model perspective. In this work, we propose a robust uncertainty modeling framework for active CLIP adaptation based on dual-prompt tuning. We introduce two learnable prompts in the textual branch of CLIP. The positive prompt enhances the discriminability of task-specific textual embeddings corresponding to light-weight tuned visual embeddings, improving classification reliability. Meanwhile, the negative prompt is trained in an reversed manner to explicitly model the probability that the predicted label is correct, providing a principled uncertainty signal for guiding active sample selection. Extensive experiments across different fine-tuning paradigms demonstrate that our method consistently outperforms existing active learning methods under the same annotation budget."
    },
    {
      "title": "Fine-tuning Pre-trained Vision-Language Models in a Human-Annotation-Free Manner",
      "url": "https://arxiv.org/abs/2602.04337",
      "date": "2026-02-04",
      "authors_text": "Qian-Wei Wang, Guanghao Meng, Ren Cai, Yaguang Song, Shu-Tao Xia",
      "is_highlight": false,
      "score": 68.0,
      "summary": "The paper presents Collaborative Fine-Tuning (CoFT), an unsupervised framework for adapting vision-language models without labeled data, enhancing performance through dual-prompt learning and collaborative filtering.",
      "teaser_image": "https://arxiv.org/html/2602.04337/pic/overall.png",
      "debug_abstract": "Large-scale vision-language models (VLMs) such as CLIP exhibit strong zero-shot generalization, but adapting them to downstream tasks typically requires costly labeled data. Existing unsupervised self-training methods rely on pseudo-labeling, yet often suffer from unreliable confidence filtering, confirmation bias, and underutilization of low-confidence samples. We propose Collaborative Fine-Tuning (CoFT), an unsupervised adaptation framework that leverages unlabeled data through a dual-model, cross-modal collaboration mechanism. CoFT introduces a dual-prompt learning strategy with positive and negative textual prompts to explicitly model pseudo-label cleanliness in a sample-dependent manner, removing the need for hand-crafted thresholds or noise assumptions. The negative prompt also regularizes lightweight visual adaptation modules, improving robustness under noisy supervision. CoFT employs a two-phase training scheme, transitioning from parameter-efficient fine-tuning on high-confidence samples to full fine-tuning guided by collaboratively filtered pseudo-labels. Building on CoFT, CoFT+ further enhances adaptation via iterative fine-tuning, momentum contrastive learning, and LLM-generated prompts. Extensive experiments demonstrate consistent gains over existing unsupervised methods and even few-shot supervised baselines."
    }
  ],
  "具身视觉与机器人实验室 (EVAR Lab)": [
    {
      "title": "Skin Tokens: A Learned Compact Representation for Unified Autoregressive Rigging",
      "url": "https://arxiv.org/abs/2602.04805",
      "date": "2026-02-04",
      "authors_text": "Jia-peng Zhang, Cheng-Feng Pu, Meng-Hao Guo, Yan-Pei Cao, Shi-Min Hu",
      "is_highlight": false,
      "score": 63.0,
      "summary": "The paper introduces SkinTokens, a learned representation for skinning weights that improves rigging accuracy and efficiency in 3D animation through a unified autoregressive framework.",
      "teaser_image": "https://arxiv.org/html/2602.04805/src/teaser-v2.png",
      "debug_abstract": "The rapid proliferation of generative 3D models has created a critical bottleneck in animation pipelines: rigging. Existing automated methods are fundamentally limited by their approach to skinning, treating it as an ill-posed, high-dimensional regression task that is inefficient to optimize and is typically decoupled from skeleton generation. We posit this is a representation problem and introduce SkinTokens: a learned, compact, and discrete representation for skinning weights. By leveraging an FSQ-CVAE to capture the intrinsic sparsity of skinning, we reframe the task from continuous regression to a more tractable token sequence prediction problem. This representation enables TokenRig, a unified autoregressive framework that models the entire rig as a single sequence of skeletal parameters and SkinTokens, learning the complicated dependencies between skeletons and skin deformations. The unified model is then amenable to a reinforcement learning stage, where tailored geometric and semantic rewards improve generalization to complex, out-of-distribution assets. Quantitatively, the SkinTokens representation leads to a 98%-133% percents improvement in skinning accuracy over state-of-the-art methods, while the full TokenRig framework, refined with RL, enhances bone prediction by 17%-22%. Our work presents a unified, generative approach to rigging that yields higher fidelity and robustness, offering a scalable solution to a long-standing challenge in 3D content creation."
    },
    {
      "title": "SynthVerse: A Large-Scale Diverse Synthetic Dataset for Point Tracking",
      "url": "https://arxiv.org/abs/2602.04441",
      "date": "2026-02-04",
      "authors_text": "Weiguang Zhao, Haoran Xu, Xingyu Miao, Qin Zhao, Rui Zhang",
      "is_highlight": false,
      "score": 69.0,
      "summary": "SynthVerse is a comprehensive synthetic dataset designed to enhance point tracking by offering diverse domains, high-quality annotations, and a robust benchmark for evaluating tracking methods.",
      "teaser_image": "https://arxiv.org/html/2602.04441/x2.png",
      "debug_abstract": "Point tracking aims to follow visual points through complex motion, occlusion, and viewpoint changes, and has advanced rapidly with modern foundation models. Yet progress toward general point tracking remains constrained by limited high-quality data, as existing datasets often provide insufficient diversity and imperfect trajectory annotations. To this end, we introduce SynthVerse, a large-scale, diverse synthetic dataset specifically designed for point tracking. SynthVerse includes several new domains and object types missing from existing synthetic datasets, such as animated-film-style content, embodied manipulation, scene navigation, and articulated objects. SynthVerse substantially expands dataset diversity by covering a broader range of object categories and providing high-quality dynamic motions and interactions, enabling more robust training and evaluation for general point tracking. In addition, we establish a highly diverse point tracking benchmark to systematically evaluate state-of-the-art methods under broader domain shifts. Extensive experiments and analyses demonstrate that training with SynthVerse yields consistent improvements in generalization and reveal limitations of existing trackers under diverse settings."
    },
    {
      "title": "Self-evolving Embodied AI",
      "url": "https://arxiv.org/abs/2602.04411",
      "date": "2026-02-04",
      "authors_text": "Tongtong Feng, Xin Wang, Wenwu Zhu",
      "is_highlight": false,
      "score": 80.0,
      "summary": "This paper proposes self-evolving embodied AI, enabling agents to autonomously adapt and learn in dynamic environments through self-updating memory and evolving models.",
      "teaser_image": "https://arxiv.org/html/2602.04411/x2.png",
      "debug_abstract": "Embodied Artificial Intelligence (AI) is an intelligent system formed by agents and their environment through active perception, embodied cognition, and action interaction. Existing embodied AI remains confined to human-crafted setting, in which agents are trained on given memory and construct models for given tasks, enabling fixed embodiments to interact with relatively static environments. Such methods fail in in-the-wild setting characterized by variable embodiments and dynamic open environments. This paper introduces self-evolving embodied AI, a new paradigm in which agents operate based on their changing state and environment with memory self-updating, task self-switching, environment self-prediction, embodiment self-adaptation, and model self-evolution, aiming to achieve continually adaptive intelligence with autonomous evolution. Specifically, we present the definition, framework, components, and mechanisms of self-evolving embodied AI, systematically review state-of-the-art works for realized components, discuss practical applications, and point out future research directions. We believe that self-evolving embodied AI enables agents to autonomously learn and interact with environments in a human-like manner and provide a new perspective toward general artificial intelligence."
    },
    {
      "title": "Enabling Real-Time Colonoscopic Polyp Segmentation on Commodity CPUs via Ultra-Lightweight Architecture",
      "url": "https://arxiv.org/abs/2602.04381",
      "date": "2026-02-04",
      "authors_text": "Weihao Gao, Zhuo Deng, Zheng Gong, Lan Ma",
      "is_highlight": false,
      "score": 66.0,
      "summary": "The UltraSeg models enable real-time polyp segmentation on standard CPUs with extreme parameter compression, achieving high accuracy and speed for practical clinical deployment.",
      "teaser_image": "https://arxiv.org/html/2602.04381/x1.png",
      "debug_abstract": "Early detection of colorectal cancer hinges on real-time, accurate polyp identification and resection. Yet current high-precision segmentation models rely on GPUs, making them impractical to deploy in primary hospitals, mobile endoscopy units, or capsule robots. To bridge this gap, we present the UltraSeg family, operating in an extreme-compression regime (&lt;0.3 M parameters). UltraSeg-108K (0.108 M parameters) is optimized for single-center data, while UltraSeg-130K (0.13 M parameters) generalizes to multi-center, multi-modal images. By jointly optimizing encoder-decoder widths, incorporating constrained dilated convolutions to enlarge receptive fields, and integrating a cross-layer lightweight fusion module, the models achieve 90 FPS on a single CPU core without sacrificing accuracy. Evaluated on seven public datasets, UltraSeg retains &gt;94% of the Dice score of a 31 M-parameter U-Net while utilizing only 0.4% of its parameters, establishing a strong, clinically viable baseline for the extreme-compression domain and offering an immediately deployable solution for resource-constrained settings. This work provides not only a CPU-native solution for colonoscopy but also a reproducible blueprint for broader minimally invasive surgical vision applications. Source code is publicly available to ensure reproducibility and facilitate future benchmarking."
    },
    {
      "title": "Explicit Uncertainty Modeling for Active CLIP Adaptation with Dual Prompt Tuning",
      "url": "https://arxiv.org/abs/2602.04340",
      "date": "2026-02-04",
      "authors_text": "Qian-Wei Wang, Yaguang Song, Shu-Tao Xia",
      "is_highlight": false,
      "score": 71.0,
      "summary": "This paper presents a dual-prompt tuning framework for active CLIP adaptation, enhancing uncertainty modeling and improving sample selection in image classification tasks under limited annotations.",
      "teaser_image": "https://arxiv.org/html/2602.04340/data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==",
      "debug_abstract": "Pre-trained vision-language models such as CLIP exhibit strong transferability, yet adapting them to downstream image classification tasks under limited annotation budgets remains challenging. In active learning settings, the model must select the most informative samples for annotation from a large pool of unlabeled data. Existing approaches typically estimate uncertainty via entropy-based criteria or representation clustering, without explicitly modeling uncertainty from the model perspective. In this work, we propose a robust uncertainty modeling framework for active CLIP adaptation based on dual-prompt tuning. We introduce two learnable prompts in the textual branch of CLIP. The positive prompt enhances the discriminability of task-specific textual embeddings corresponding to light-weight tuned visual embeddings, improving classification reliability. Meanwhile, the negative prompt is trained in an reversed manner to explicitly model the probability that the predicted label is correct, providing a principled uncertainty signal for guiding active sample selection. Extensive experiments across different fine-tuning paradigms demonstrate that our method consistently outperforms existing active learning methods under the same annotation budget."
    },
    {
      "title": "Fine-tuning Pre-trained Vision-Language Models in a Human-Annotation-Free Manner",
      "url": "https://arxiv.org/abs/2602.04337",
      "date": "2026-02-04",
      "authors_text": "Qian-Wei Wang, Guanghao Meng, Ren Cai, Yaguang Song, Shu-Tao Xia",
      "is_highlight": false,
      "score": 68.0,
      "summary": "The paper presents Collaborative Fine-Tuning (CoFT), an unsupervised framework for adapting vision-language models without labeled data, enhancing performance through dual-prompt learning and collaborative filtering.",
      "teaser_image": "https://arxiv.org/html/2602.04337/pic/overall.png",
      "debug_abstract": "Large-scale vision-language models (VLMs) such as CLIP exhibit strong zero-shot generalization, but adapting them to downstream tasks typically requires costly labeled data. Existing unsupervised self-training methods rely on pseudo-labeling, yet often suffer from unreliable confidence filtering, confirmation bias, and underutilization of low-confidence samples. We propose Collaborative Fine-Tuning (CoFT), an unsupervised adaptation framework that leverages unlabeled data through a dual-model, cross-modal collaboration mechanism. CoFT introduces a dual-prompt learning strategy with positive and negative textual prompts to explicitly model pseudo-label cleanliness in a sample-dependent manner, removing the need for hand-crafted thresholds or noise assumptions. The negative prompt also regularizes lightweight visual adaptation modules, improving robustness under noisy supervision. CoFT employs a two-phase training scheme, transitioning from parameter-efficient fine-tuning on high-confidence samples to full fine-tuning guided by collaboratively filtered pseudo-labels. Building on CoFT, CoFT+ further enhances adaptation via iterative fine-tuning, momentum contrastive learning, and LLM-generated prompts. Extensive experiments demonstrate consistent gains over existing unsupervised methods and even few-shot supervised baselines."
    }
  ],
  "MARS多模态学习实验室": [
    {
      "title": "Skin Tokens: A Learned Compact Representation for Unified Autoregressive Rigging",
      "url": "https://arxiv.org/abs/2602.04805",
      "date": "2026-02-04",
      "authors_text": "Jia-peng Zhang, Cheng-Feng Pu, Meng-Hao Guo, Yan-Pei Cao, Shi-Min Hu",
      "is_highlight": false,
      "score": 63.0,
      "summary": "The paper introduces SkinTokens, a learned representation for skinning weights that improves rigging accuracy and efficiency in 3D animation through a unified autoregressive framework.",
      "teaser_image": "https://arxiv.org/html/2602.04805/src/teaser-v2.png",
      "debug_abstract": "The rapid proliferation of generative 3D models has created a critical bottleneck in animation pipelines: rigging. Existing automated methods are fundamentally limited by their approach to skinning, treating it as an ill-posed, high-dimensional regression task that is inefficient to optimize and is typically decoupled from skeleton generation. We posit this is a representation problem and introduce SkinTokens: a learned, compact, and discrete representation for skinning weights. By leveraging an FSQ-CVAE to capture the intrinsic sparsity of skinning, we reframe the task from continuous regression to a more tractable token sequence prediction problem. This representation enables TokenRig, a unified autoregressive framework that models the entire rig as a single sequence of skeletal parameters and SkinTokens, learning the complicated dependencies between skeletons and skin deformations. The unified model is then amenable to a reinforcement learning stage, where tailored geometric and semantic rewards improve generalization to complex, out-of-distribution assets. Quantitatively, the SkinTokens representation leads to a 98%-133% percents improvement in skinning accuracy over state-of-the-art methods, while the full TokenRig framework, refined with RL, enhances bone prediction by 17%-22%. Our work presents a unified, generative approach to rigging that yields higher fidelity and robustness, offering a scalable solution to a long-standing challenge in 3D content creation."
    },
    {
      "title": "SynthVerse: A Large-Scale Diverse Synthetic Dataset for Point Tracking",
      "url": "https://arxiv.org/abs/2602.04441",
      "date": "2026-02-04",
      "authors_text": "Weiguang Zhao, Haoran Xu, Xingyu Miao, Qin Zhao, Rui Zhang",
      "is_highlight": false,
      "score": 69.0,
      "summary": "SynthVerse is a comprehensive synthetic dataset designed to enhance point tracking by offering diverse domains, high-quality annotations, and a robust benchmark for evaluating tracking methods.",
      "teaser_image": "https://arxiv.org/html/2602.04441/x2.png",
      "debug_abstract": "Point tracking aims to follow visual points through complex motion, occlusion, and viewpoint changes, and has advanced rapidly with modern foundation models. Yet progress toward general point tracking remains constrained by limited high-quality data, as existing datasets often provide insufficient diversity and imperfect trajectory annotations. To this end, we introduce SynthVerse, a large-scale, diverse synthetic dataset specifically designed for point tracking. SynthVerse includes several new domains and object types missing from existing synthetic datasets, such as animated-film-style content, embodied manipulation, scene navigation, and articulated objects. SynthVerse substantially expands dataset diversity by covering a broader range of object categories and providing high-quality dynamic motions and interactions, enabling more robust training and evaluation for general point tracking. In addition, we establish a highly diverse point tracking benchmark to systematically evaluate state-of-the-art methods under broader domain shifts. Extensive experiments and analyses demonstrate that training with SynthVerse yields consistent improvements in generalization and reveal limitations of existing trackers under diverse settings."
    },
    {
      "title": "Self-evolving Embodied AI",
      "url": "https://arxiv.org/abs/2602.04411",
      "date": "2026-02-04",
      "authors_text": "Tongtong Feng, Xin Wang, Wenwu Zhu",
      "is_highlight": false,
      "score": 80.0,
      "summary": "This paper proposes self-evolving embodied AI, enabling agents to autonomously adapt and learn in dynamic environments through self-updating memory and evolving models.",
      "teaser_image": "https://arxiv.org/html/2602.04411/x2.png",
      "debug_abstract": "Embodied Artificial Intelligence (AI) is an intelligent system formed by agents and their environment through active perception, embodied cognition, and action interaction. Existing embodied AI remains confined to human-crafted setting, in which agents are trained on given memory and construct models for given tasks, enabling fixed embodiments to interact with relatively static environments. Such methods fail in in-the-wild setting characterized by variable embodiments and dynamic open environments. This paper introduces self-evolving embodied AI, a new paradigm in which agents operate based on their changing state and environment with memory self-updating, task self-switching, environment self-prediction, embodiment self-adaptation, and model self-evolution, aiming to achieve continually adaptive intelligence with autonomous evolution. Specifically, we present the definition, framework, components, and mechanisms of self-evolving embodied AI, systematically review state-of-the-art works for realized components, discuss practical applications, and point out future research directions. We believe that self-evolving embodied AI enables agents to autonomously learn and interact with environments in a human-like manner and provide a new perspective toward general artificial intelligence."
    },
    {
      "title": "Enabling Real-Time Colonoscopic Polyp Segmentation on Commodity CPUs via Ultra-Lightweight Architecture",
      "url": "https://arxiv.org/abs/2602.04381",
      "date": "2026-02-04",
      "authors_text": "Weihao Gao, Zhuo Deng, Zheng Gong, Lan Ma",
      "is_highlight": false,
      "score": 66.0,
      "summary": "The UltraSeg models enable real-time polyp segmentation on standard CPUs with extreme parameter compression, achieving high accuracy and speed for practical clinical deployment.",
      "teaser_image": "https://arxiv.org/html/2602.04381/x1.png",
      "debug_abstract": "Early detection of colorectal cancer hinges on real-time, accurate polyp identification and resection. Yet current high-precision segmentation models rely on GPUs, making them impractical to deploy in primary hospitals, mobile endoscopy units, or capsule robots. To bridge this gap, we present the UltraSeg family, operating in an extreme-compression regime (&lt;0.3 M parameters). UltraSeg-108K (0.108 M parameters) is optimized for single-center data, while UltraSeg-130K (0.13 M parameters) generalizes to multi-center, multi-modal images. By jointly optimizing encoder-decoder widths, incorporating constrained dilated convolutions to enlarge receptive fields, and integrating a cross-layer lightweight fusion module, the models achieve 90 FPS on a single CPU core without sacrificing accuracy. Evaluated on seven public datasets, UltraSeg retains &gt;94% of the Dice score of a 31 M-parameter U-Net while utilizing only 0.4% of its parameters, establishing a strong, clinically viable baseline for the extreme-compression domain and offering an immediately deployable solution for resource-constrained settings. This work provides not only a CPU-native solution for colonoscopy but also a reproducible blueprint for broader minimally invasive surgical vision applications. Source code is publicly available to ensure reproducibility and facilitate future benchmarking."
    },
    {
      "title": "Explicit Uncertainty Modeling for Active CLIP Adaptation with Dual Prompt Tuning",
      "url": "https://arxiv.org/abs/2602.04340",
      "date": "2026-02-04",
      "authors_text": "Qian-Wei Wang, Yaguang Song, Shu-Tao Xia",
      "is_highlight": false,
      "score": 71.0,
      "summary": "This paper presents a dual-prompt tuning framework for active CLIP adaptation, enhancing uncertainty modeling and improving sample selection in image classification tasks under limited annotations.",
      "teaser_image": "https://arxiv.org/html/2602.04340/data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==",
      "debug_abstract": "Pre-trained vision-language models such as CLIP exhibit strong transferability, yet adapting them to downstream image classification tasks under limited annotation budgets remains challenging. In active learning settings, the model must select the most informative samples for annotation from a large pool of unlabeled data. Existing approaches typically estimate uncertainty via entropy-based criteria or representation clustering, without explicitly modeling uncertainty from the model perspective. In this work, we propose a robust uncertainty modeling framework for active CLIP adaptation based on dual-prompt tuning. We introduce two learnable prompts in the textual branch of CLIP. The positive prompt enhances the discriminability of task-specific textual embeddings corresponding to light-weight tuned visual embeddings, improving classification reliability. Meanwhile, the negative prompt is trained in an reversed manner to explicitly model the probability that the predicted label is correct, providing a principled uncertainty signal for guiding active sample selection. Extensive experiments across different fine-tuning paradigms demonstrate that our method consistently outperforms existing active learning methods under the same annotation budget."
    },
    {
      "title": "Fine-tuning Pre-trained Vision-Language Models in a Human-Annotation-Free Manner",
      "url": "https://arxiv.org/abs/2602.04337",
      "date": "2026-02-04",
      "authors_text": "Qian-Wei Wang, Guanghao Meng, Ren Cai, Yaguang Song, Shu-Tao Xia",
      "is_highlight": false,
      "score": 68.0,
      "summary": "The paper presents Collaborative Fine-Tuning (CoFT), an unsupervised framework for adapting vision-language models without labeled data, enhancing performance through dual-prompt learning and collaborative filtering.",
      "teaser_image": "https://arxiv.org/html/2602.04337/pic/overall.png",
      "debug_abstract": "Large-scale vision-language models (VLMs) such as CLIP exhibit strong zero-shot generalization, but adapting them to downstream tasks typically requires costly labeled data. Existing unsupervised self-training methods rely on pseudo-labeling, yet often suffer from unreliable confidence filtering, confirmation bias, and underutilization of low-confidence samples. We propose Collaborative Fine-Tuning (CoFT), an unsupervised adaptation framework that leverages unlabeled data through a dual-model, cross-modal collaboration mechanism. CoFT introduces a dual-prompt learning strategy with positive and negative textual prompts to explicitly model pseudo-label cleanliness in a sample-dependent manner, removing the need for hand-crafted thresholds or noise assumptions. The negative prompt also regularizes lightweight visual adaptation modules, improving robustness under noisy supervision. CoFT employs a two-phase training scheme, transitioning from parameter-efficient fine-tuning on high-confidence samples to full fine-tuning guided by collaboratively filtered pseudo-labels. Building on CoFT, CoFT+ further enhances adaptation via iterative fine-tuning, momentum contrastive learning, and LLM-generated prompts. Extensive experiments demonstrate consistent gains over existing unsupervised methods and even few-shot supervised baselines."
    }
  ],
  "BIGAI": [
    {
      "title": "Integrated Exploration and Sequential Manipulation on Scene Graph with LLM-based Situated Replanning",
      "url": "https://arxiv.org/abs/2602.04419",
      "date": "2026-02-04",
      "authors_text": "Heqing Yang, Ziyuan Jiao, Shu Wang, Yida Niu, Si Liu",
      "is_highlight": false,
      "score": 91.0,
      "summary": "EPoG integrates exploration and sequential manipulation planning using scene graphs and LLMs, achieving a 91.3% success rate in complex tasks within dynamic environments.",
      "teaser_image": "https://arxiv.org/html/2602.04419/figures/fig2-structure.png",
      "debug_abstract": "In partially known environments, robots must combine exploration to gather information with task planning for efficient execution. To address this challenge, we propose EPoG, an Exploration-based sequential manipulation Planning framework on Scene Graphs. EPoG integrates a graph-based global planner with a Large Language Model (LLM)-based situated local planner, continuously updating a belief graph using observations and LLM predictions to represent known and unknown objects. Action sequences are generated by computing graph edit operations between the goal and belief graphs, ordered by temporal dependencies and movement costs. This approach seamlessly combines exploration and sequential manipulation planning. In ablation studies across 46 realistic household scenes and 5 long-horizon daily object transportation tasks, EPoG achieved a success rate of 91.3%, reducing travel distance by 36.1% on average. Furthermore, a physical mobile manipulator successfully executed complex tasks in unknown and dynamic environments, demonstrating EPoG&#39;s potential for real-world applications."
    }
  ],
  "北京航空航天大学": [
    {
      "title": "SynthVerse: A Large-Scale Diverse Synthetic Dataset for Point Tracking",
      "url": "https://arxiv.org/abs/2602.04441",
      "date": "2026-02-04",
      "authors_text": "Weiguang Zhao, Haoran Xu, Xingyu Miao, Qin Zhao, Rui Zhang",
      "is_highlight": false,
      "score": 69.0,
      "summary": "SynthVerse is a comprehensive synthetic dataset designed to enhance point tracking by offering diverse domains, high-quality annotations, and a robust benchmark for evaluating tracking methods.",
      "teaser_image": "https://arxiv.org/html/2602.04441/x2.png",
      "debug_abstract": "Point tracking aims to follow visual points through complex motion, occlusion, and viewpoint changes, and has advanced rapidly with modern foundation models. Yet progress toward general point tracking remains constrained by limited high-quality data, as existing datasets often provide insufficient diversity and imperfect trajectory annotations. To this end, we introduce SynthVerse, a large-scale, diverse synthetic dataset specifically designed for point tracking. SynthVerse includes several new domains and object types missing from existing synthetic datasets, such as animated-film-style content, embodied manipulation, scene navigation, and articulated objects. SynthVerse substantially expands dataset diversity by covering a broader range of object categories and providing high-quality dynamic motions and interactions, enabling more robust training and evaluation for general point tracking. In addition, we establish a highly diverse point tracking benchmark to systematically evaluate state-of-the-art methods under broader domain shifts. Extensive experiments and analyses demonstrate that training with SynthVerse yields consistent improvements in generalization and reveal limitations of existing trackers under diverse settings."
    },
    {
      "title": "Integrated Exploration and Sequential Manipulation on Scene Graph with LLM-based Situated Replanning",
      "url": "https://arxiv.org/abs/2602.04419",
      "date": "2026-02-04",
      "authors_text": "Heqing Yang, Ziyuan Jiao, Shu Wang, Yida Niu, Si Liu",
      "is_highlight": false,
      "score": 91.0,
      "summary": "EPoG integrates exploration and sequential manipulation planning using scene graphs and LLMs, achieving a 91.3% success rate in complex tasks within dynamic environments.",
      "teaser_image": "https://arxiv.org/html/2602.04419/figures/fig2-structure.png",
      "debug_abstract": "In partially known environments, robots must combine exploration to gather information with task planning for efficient execution. To address this challenge, we propose EPoG, an Exploration-based sequential manipulation Planning framework on Scene Graphs. EPoG integrates a graph-based global planner with a Large Language Model (LLM)-based situated local planner, continuously updating a belief graph using observations and LLM predictions to represent known and unknown objects. Action sequences are generated by computing graph edit operations between the goal and belief graphs, ordered by temporal dependencies and movement costs. This approach seamlessly combines exploration and sequential manipulation planning. In ablation studies across 46 realistic household scenes and 5 long-horizon daily object transportation tasks, EPoG achieved a success rate of 91.3%, reducing travel distance by 36.1% on average. Furthermore, a physical mobile manipulator successfully executed complex tasks in unknown and dynamic environments, demonstrating EPoG&#39;s potential for real-world applications."
    }
  ],
  "Multimedia Lab (MMLab)": [
    {
      "title": "SynthVerse: A Large-Scale Diverse Synthetic Dataset for Point Tracking",
      "url": "https://arxiv.org/abs/2602.04441",
      "date": "2026-02-04",
      "authors_text": "Weiguang Zhao, Haoran Xu, Xingyu Miao, Qin Zhao, Rui Zhang",
      "is_highlight": false,
      "score": 69.0,
      "summary": "SynthVerse is a comprehensive synthetic dataset designed to enhance point tracking by offering diverse domains, high-quality annotations, and a robust benchmark for evaluating tracking methods.",
      "teaser_image": "https://arxiv.org/html/2602.04441/x2.png",
      "debug_abstract": "Point tracking aims to follow visual points through complex motion, occlusion, and viewpoint changes, and has advanced rapidly with modern foundation models. Yet progress toward general point tracking remains constrained by limited high-quality data, as existing datasets often provide insufficient diversity and imperfect trajectory annotations. To this end, we introduce SynthVerse, a large-scale, diverse synthetic dataset specifically designed for point tracking. SynthVerse includes several new domains and object types missing from existing synthetic datasets, such as animated-film-style content, embodied manipulation, scene navigation, and articulated objects. SynthVerse substantially expands dataset diversity by covering a broader range of object categories and providing high-quality dynamic motions and interactions, enabling more robust training and evaluation for general point tracking. In addition, we establish a highly diverse point tracking benchmark to systematically evaluate state-of-the-art methods under broader domain shifts. Extensive experiments and analyses demonstrate that training with SynthVerse yields consistent improvements in generalization and reveal limitations of existing trackers under diverse settings."
    }
  ],
  "深圳市人工智能与机器人研究院 (AIRS)": [
    {
      "title": "SynthVerse: A Large-Scale Diverse Synthetic Dataset for Point Tracking",
      "url": "https://arxiv.org/abs/2602.04441",
      "date": "2026-02-04",
      "authors_text": "Weiguang Zhao, Haoran Xu, Xingyu Miao, Qin Zhao, Rui Zhang",
      "is_highlight": false,
      "score": 69.0,
      "summary": "SynthVerse is a comprehensive synthetic dataset designed to enhance point tracking by offering diverse domains, high-quality annotations, and a robust benchmark for evaluating tracking methods.",
      "teaser_image": "https://arxiv.org/html/2602.04441/x2.png",
      "debug_abstract": "Point tracking aims to follow visual points through complex motion, occlusion, and viewpoint changes, and has advanced rapidly with modern foundation models. Yet progress toward general point tracking remains constrained by limited high-quality data, as existing datasets often provide insufficient diversity and imperfect trajectory annotations. To this end, we introduce SynthVerse, a large-scale, diverse synthetic dataset specifically designed for point tracking. SynthVerse includes several new domains and object types missing from existing synthetic datasets, such as animated-film-style content, embodied manipulation, scene navigation, and articulated objects. SynthVerse substantially expands dataset diversity by covering a broader range of object categories and providing high-quality dynamic motions and interactions, enabling more robust training and evaluation for general point tracking. In addition, we establish a highly diverse point tracking benchmark to systematically evaluate state-of-the-art methods under broader domain shifts. Extensive experiments and analyses demonstrate that training with SynthVerse yields consistent improvements in generalization and reveal limitations of existing trackers under diverse settings."
    }
  ],
  "机器人与自动化研究中心": [
    {
      "title": "SynthVerse: A Large-Scale Diverse Synthetic Dataset for Point Tracking",
      "url": "https://arxiv.org/abs/2602.04441",
      "date": "2026-02-04",
      "authors_text": "Weiguang Zhao, Haoran Xu, Xingyu Miao, Qin Zhao, Rui Zhang",
      "is_highlight": false,
      "score": 69.0,
      "summary": "SynthVerse is a comprehensive synthetic dataset designed to enhance point tracking by offering diverse domains, high-quality annotations, and a robust benchmark for evaluating tracking methods.",
      "teaser_image": "https://arxiv.org/html/2602.04441/x2.png",
      "debug_abstract": "Point tracking aims to follow visual points through complex motion, occlusion, and viewpoint changes, and has advanced rapidly with modern foundation models. Yet progress toward general point tracking remains constrained by limited high-quality data, as existing datasets often provide insufficient diversity and imperfect trajectory annotations. To this end, we introduce SynthVerse, a large-scale, diverse synthetic dataset specifically designed for point tracking. SynthVerse includes several new domains and object types missing from existing synthetic datasets, such as animated-film-style content, embodied manipulation, scene navigation, and articulated objects. SynthVerse substantially expands dataset diversity by covering a broader range of object categories and providing high-quality dynamic motions and interactions, enabling more robust training and evaluation for general point tracking. In addition, we establish a highly diverse point tracking benchmark to systematically evaluate state-of-the-art methods under broader domain shifts. Extensive experiments and analyses demonstrate that training with SynthVerse yields consistent improvements in generalization and reveal limitations of existing trackers under diverse settings."
    }
  ],
  "浙江大学": [
    {
      "title": "From Data to Behavior: Predicting Unintended Model Behaviors Before Training",
      "url": "https://arxiv.org/abs/2602.04735",
      "date": "2026-02-04",
      "authors_text": "Mengru Wang, Zhenqian Xu, Junfeng Fang, Yunzhi Yao, Shumin Deng",
      "is_highlight": false,
      "score": 62.0,
      "summary": "The paper presents Data2Behavior and the Manipulating Data Features (MDF) method to predict unintended biases in Large Language Models before training, enhancing efficiency and safety.",
      "teaser_image": "https://arxiv.org/html/2602.04735/x2.png",
      "debug_abstract": "Large Language Models (LLMs) can acquire unintended biases from seemingly benign training data even without explicit cues or malicious content. Existing methods struggle to detect such risks before fine-tuning, making post hoc evaluation costly and inefficient. To address this challenge, we introduce Data2Behavior, a new task for predicting unintended model behaviors prior to training. We also propose Manipulating Data Features (MDF), a lightweight approach that summarizes candidate data through their mean representations and injects them into the forward pass of a base model, allowing latent statistical signals in the data to shape model activations and reveal potential biases and safety risks without updating any parameters. MDF achieves reliable prediction while consuming only about 20% of the GPU resources required for fine-tuning. Experiments on Qwen3-14B, Qwen2.5-32B-Instruct, and Gemma-3-12b-it confirm that MDF can anticipate unintended behaviors and provide insight into pre-training vulnerabilities."
    },
    {
      "title": "AGILE: Hand-Object Interaction Reconstruction from Video via Agentic Generation",
      "url": "https://arxiv.org/abs/2602.04672",
      "date": "2026-02-04",
      "authors_text": "Jin-Chuan Shi, Binhong Ye, Tao Liu, Junzhe He, Yangjinhui Xu",
      "is_highlight": false,
      "score": 87.0,
      "summary": "AGILE introduces a novel framework for reconstructing hand-object interactions from monocular videos, enhancing robustness and accuracy through agentic generation and contact-aware optimization.",
      "teaser_image": "https://arxiv.org/html/2602.04672/x2.png",
      "debug_abstract": "Reconstructing dynamic hand-object interactions from monocular videos is critical for dexterous manipulation data collection and creating realistic digital twins for robotics and VR. However, current methods face two prohibitive barriers: (1) reliance on neural rendering often yields fragmented, non-simulation-ready geometries under heavy occlusion, and (2) dependence on brittle Structure-from-Motion (SfM) initialization leads to frequent failures on in-the-wild footage. To overcome these limitations, we introduce AGILE, a robust framework that shifts the paradigm from reconstruction to agentic generation for interaction learning. First, we employ an agentic pipeline where a Vision-Language Model (VLM) guides a generative model to synthesize a complete, watertight object mesh with high-fidelity texture, independent of video occlusions. Second, bypassing fragile SfM entirely, we propose a robust anchor-and-track strategy. We initialize the object pose at a single interaction onset frame using a foundation model and propagate it temporally by leveraging the strong visual similarity between our generated asset and video observations. Finally, a contact-aware optimization integrates semantic, geometric, and interaction stability constraints to enforce physical plausibility. Extensive experiments on HO3D, DexYCB, and in-the-wild videos reveal that AGILE outperforms baselines in global geometric accuracy while demonstrating exceptional robustness on challenging sequences where prior art frequently collapses. By prioritizing physical validity, our method produces simulation-ready assets validated via real-to-sim retargeting for robotic applications."
    },
    {
      "title": "SynthVerse: A Large-Scale Diverse Synthetic Dataset for Point Tracking",
      "url": "https://arxiv.org/abs/2602.04441",
      "date": "2026-02-04",
      "authors_text": "Weiguang Zhao, Haoran Xu, Xingyu Miao, Qin Zhao, Rui Zhang",
      "is_highlight": false,
      "score": 69.0,
      "summary": "SynthVerse is a comprehensive synthetic dataset designed to enhance point tracking by offering diverse domains, high-quality annotations, and a robust benchmark for evaluating tracking methods.",
      "teaser_image": "https://arxiv.org/html/2602.04441/x2.png",
      "debug_abstract": "Point tracking aims to follow visual points through complex motion, occlusion, and viewpoint changes, and has advanced rapidly with modern foundation models. Yet progress toward general point tracking remains constrained by limited high-quality data, as existing datasets often provide insufficient diversity and imperfect trajectory annotations. To this end, we introduce SynthVerse, a large-scale, diverse synthetic dataset specifically designed for point tracking. SynthVerse includes several new domains and object types missing from existing synthetic datasets, such as animated-film-style content, embodied manipulation, scene navigation, and articulated objects. SynthVerse substantially expands dataset diversity by covering a broader range of object categories and providing high-quality dynamic motions and interactions, enabling more robust training and evaluation for general point tracking. In addition, we establish a highly diverse point tracking benchmark to systematically evaluate state-of-the-art methods under broader domain shifts. Extensive experiments and analyses demonstrate that training with SynthVerse yields consistent improvements in generalization and reveal limitations of existing trackers under diverse settings."
    }
  ],
  "机器人与人工智能实验室": [
    {
      "title": "SynthVerse: A Large-Scale Diverse Synthetic Dataset for Point Tracking",
      "url": "https://arxiv.org/abs/2602.04441",
      "date": "2026-02-04",
      "authors_text": "Weiguang Zhao, Haoran Xu, Xingyu Miao, Qin Zhao, Rui Zhang",
      "is_highlight": false,
      "score": 69.0,
      "summary": "SynthVerse is a comprehensive synthetic dataset designed to enhance point tracking by offering diverse domains, high-quality annotations, and a robust benchmark for evaluating tracking methods.",
      "teaser_image": "https://arxiv.org/html/2602.04441/x2.png",
      "debug_abstract": "Point tracking aims to follow visual points through complex motion, occlusion, and viewpoint changes, and has advanced rapidly with modern foundation models. Yet progress toward general point tracking remains constrained by limited high-quality data, as existing datasets often provide insufficient diversity and imperfect trajectory annotations. To this end, we introduce SynthVerse, a large-scale, diverse synthetic dataset specifically designed for point tracking. SynthVerse includes several new domains and object types missing from existing synthetic datasets, such as animated-film-style content, embodied manipulation, scene navigation, and articulated objects. SynthVerse substantially expands dataset diversity by covering a broader range of object categories and providing high-quality dynamic motions and interactions, enabling more robust training and evaluation for general point tracking. In addition, we establish a highly diverse point tracking benchmark to systematically evaluate state-of-the-art methods under broader domain shifts. Extensive experiments and analyses demonstrate that training with SynthVerse yields consistent improvements in generalization and reveal limitations of existing trackers under diverse settings."
    }
  ],
  "机器人与人工智能实验室 (RAIL)": [
    {
      "title": "When Silence Is Golden: Can LLMs Learn to Abstain in Temporal QA and Beyond?",
      "url": "https://arxiv.org/abs/2602.04755",
      "date": "2026-02-04",
      "authors_text": "Xinyu Zhou, Chang Jin, Carsten Eickhoff, Zhijiang Guo, Seyed Ali Bahrainian",
      "is_highlight": false,
      "score": 64.0,
      "summary": "This paper explores training large language models to abstain from answering uncertain temporal questions, demonstrating that reinforcement learning enhances reasoning accuracy and reduces overconfidence.",
      "teaser_image": "https://arxiv.org/html/2602.04755/cot_RL_page-0001.jpg",
      "debug_abstract": "Large language models (LLMs) rarely admit uncertainty, often producing fluent but misleading answers, rather than abstaining (i.e., refusing to answer). This weakness is even evident in temporal question answering, where models frequently ignore time-sensitive evidence and conflate facts across different time-periods. In this paper, we present the first empirical study of training LLMs with an abstention ability while reasoning about temporal QA. Existing approaches such as calibration might be unreliable in capturing uncertainty in complex reasoning. We instead frame abstention as a teachable skill and introduce a pipeline that couples Chain-of-Thought (CoT) supervision with Reinforcement Learning (RL) guided by abstention-aware rewards. Our goal is to systematically analyze how different information types and training techniques affect temporal reasoning with abstention behavior in LLMs. Through extensive experiments studying various methods, we find that RL yields strong empirical gains on reasoning: a model initialized by Qwen2.5-1.5B-Instruct surpasses GPT-4o by $3.46\\%$ and $5.80\\%$ in Exact Match on TimeQA-Easy and Hard, respectively. Moreover, it improves the True Positive rate on unanswerable questions by $20\\%$ over a pure supervised fine-tuned (SFT) variant. Beyond performance, our analysis shows that SFT induces overconfidence and harms reliability, while RL improves prediction accuracy but exhibits similar risks. Finally, by comparing implicit reasoning cues (e.g., original context, temporal sub-context, knowledge graphs) with explicit CoT supervision, we find that implicit information provides limited benefit for reasoning with abstention. Our study provides new insights into how abstention and reasoning can be jointly optimized, providing a foundation for building more reliable LLMs."
    },
    {
      "title": "SynthVerse: A Large-Scale Diverse Synthetic Dataset for Point Tracking",
      "url": "https://arxiv.org/abs/2602.04441",
      "date": "2026-02-04",
      "authors_text": "Weiguang Zhao, Haoran Xu, Xingyu Miao, Qin Zhao, Rui Zhang",
      "is_highlight": false,
      "score": 69.0,
      "summary": "SynthVerse is a comprehensive synthetic dataset designed to enhance point tracking by offering diverse domains, high-quality annotations, and a robust benchmark for evaluating tracking methods.",
      "teaser_image": "https://arxiv.org/html/2602.04441/x2.png",
      "debug_abstract": "Point tracking aims to follow visual points through complex motion, occlusion, and viewpoint changes, and has advanced rapidly with modern foundation models. Yet progress toward general point tracking remains constrained by limited high-quality data, as existing datasets often provide insufficient diversity and imperfect trajectory annotations. To this end, we introduce SynthVerse, a large-scale, diverse synthetic dataset specifically designed for point tracking. SynthVerse includes several new domains and object types missing from existing synthetic datasets, such as animated-film-style content, embodied manipulation, scene navigation, and articulated objects. SynthVerse substantially expands dataset diversity by covering a broader range of object categories and providing high-quality dynamic motions and interactions, enabling more robust training and evaluation for general point tracking. In addition, we establish a highly diverse point tracking benchmark to systematically evaluate state-of-the-art methods under broader domain shifts. Extensive experiments and analyses demonstrate that training with SynthVerse yields consistent improvements in generalization and reveal limitations of existing trackers under diverse settings."
    }
  ],
  "上海人工智能实验室": [
    {
      "title": "SynthVerse: A Large-Scale Diverse Synthetic Dataset for Point Tracking",
      "url": "https://arxiv.org/abs/2602.04441",
      "date": "2026-02-04",
      "authors_text": "Weiguang Zhao, Haoran Xu, Xingyu Miao, Qin Zhao, Rui Zhang",
      "is_highlight": false,
      "score": 69.0,
      "summary": "SynthVerse is a comprehensive synthetic dataset designed to enhance point tracking by offering diverse domains, high-quality annotations, and a robust benchmark for evaluating tracking methods.",
      "teaser_image": "https://arxiv.org/html/2602.04441/x2.png",
      "debug_abstract": "Point tracking aims to follow visual points through complex motion, occlusion, and viewpoint changes, and has advanced rapidly with modern foundation models. Yet progress toward general point tracking remains constrained by limited high-quality data, as existing datasets often provide insufficient diversity and imperfect trajectory annotations. To this end, we introduce SynthVerse, a large-scale, diverse synthetic dataset specifically designed for point tracking. SynthVerse includes several new domains and object types missing from existing synthetic datasets, such as animated-film-style content, embodied manipulation, scene navigation, and articulated objects. SynthVerse substantially expands dataset diversity by covering a broader range of object categories and providing high-quality dynamic motions and interactions, enabling more robust training and evaluation for general point tracking. In addition, we establish a highly diverse point tracking benchmark to systematically evaluate state-of-the-art methods under broader domain shifts. Extensive experiments and analyses demonstrate that training with SynthVerse yields consistent improvements in generalization and reveal limitations of existing trackers under diverse settings."
    }
  ],
  "IWIN-FINS实验室": [
    {
      "title": "Topology-Aware Revival for Efficient Sparse Training",
      "url": "https://arxiv.org/abs/2602.04166",
      "date": "2026-02-04",
      "authors_text": "Meiling Jin, Fei Wang, Xiaoyun Yuan, Chen Qian, Yuan Cheng",
      "is_highlight": false,
      "score": 70.0,
      "summary": "Topology-Aware Revival (TAR) enhances static sparse training in deep reinforcement learning by selectively reactivating pruned connections, yielding significant performance improvements over static and dynamic baselines.",
      "teaser_image": "https://arxiv.org/html/2602.04166/x2.png",
      "debug_abstract": "Static sparse training is a promising route to efficient learning by committing to a fixed mask pattern, yet the constrained structure reduces robustness. Early pruning decisions can lock the network into a brittle structure that is difficult to escape, especially in deep reinforcement learning (RL) where the evolving policy continually shifts the training distribution. We propose Topology-Aware Revival (TAR), a lightweight one-shot post-pruning procedure that improves static sparsity without dynamic rewiring. After static pruning, TAR performs a single revival step by allocating a small reserve budget across layers according to topology needs, randomly uniformly reactivating a few previously pruned connections within each layer, and then keeping the resulting connectivity fixed for the remainder of training. Across multiple continuous-control tasks with SAC and TD3, TAR improves final return over static sparse baselines by up to +37.9% and also outperforms dynamic sparse training baselines with a median gain of +13.5%."
    },
    {
      "title": "Act, Sense, Act: Learning Non-Markovian Active Perception Strategies from Large-Scale Egocentric Human Data",
      "url": "https://arxiv.org/abs/2602.04600",
      "date": "2026-02-04",
      "authors_text": "Jialiang Li, Yi Qiao, Yunhan Guo, Changwen Chen, Wenzhao Lian",
      "is_highlight": false,
      "score": 84.0,
      "summary": "The paper presents CoMe-VLA, a framework that utilizes large-scale egocentric data to enhance robots' active perception and manipulation in complex environments through non-Markovian strategies.",
      "teaser_image": "https://arxiv.org/html/2602.04600/x3.png",
      "debug_abstract": "Achieving generalizable manipulation in unconstrained environments requires the robot to proactively resolve information uncertainty, i.e., the capability of active perception. However, existing methods are often confined in limited types of sensing behaviors, restricting their applicability to complex environments. In this work, we formalize active perception as a non-Markovian process driven by information gain and decision branching, providing a structured categorization of visual active perception paradigms. Building on this perspective, we introduce CoMe-VLA, a cognitive and memory-aware vision-language-action (VLA) framework that leverages large-scale human egocentric data to learn versatile exploration and manipulation priors. Our framework integrates a cognitive auxiliary head for autonomous sub-task transitions and a dual-track memory system to maintain consistent self and environmental awareness by fusing proprioceptive and visual temporal contexts. By aligning human and robot hand-eye coordination behaviors in a unified egocentric action space, we train the model progressively in three stages. Extensive experiments on a wheel-based humanoid have demonstrated strong robustness and adaptability of our proposed method across diverse long-horizon tasks spanning multiple active perception scenarios."
    }
  ],
  "智能机器人与机器视觉(IRMV)实验室": [
    {
      "title": "Topology-Aware Revival for Efficient Sparse Training",
      "url": "https://arxiv.org/abs/2602.04166",
      "date": "2026-02-04",
      "authors_text": "Meiling Jin, Fei Wang, Xiaoyun Yuan, Chen Qian, Yuan Cheng",
      "is_highlight": false,
      "score": 70.0,
      "summary": "Topology-Aware Revival (TAR) enhances static sparse training in deep reinforcement learning by selectively reactivating pruned connections, yielding significant performance improvements over static and dynamic baselines.",
      "teaser_image": "https://arxiv.org/html/2602.04166/x2.png",
      "debug_abstract": "Static sparse training is a promising route to efficient learning by committing to a fixed mask pattern, yet the constrained structure reduces robustness. Early pruning decisions can lock the network into a brittle structure that is difficult to escape, especially in deep reinforcement learning (RL) where the evolving policy continually shifts the training distribution. We propose Topology-Aware Revival (TAR), a lightweight one-shot post-pruning procedure that improves static sparsity without dynamic rewiring. After static pruning, TAR performs a single revival step by allocating a small reserve budget across layers according to topology needs, randomly uniformly reactivating a few previously pruned connections within each layer, and then keeping the resulting connectivity fixed for the remainder of training. Across multiple continuous-control tasks with SAC and TD3, TAR improves final return over static sparse baselines by up to +37.9% and also outperforms dynamic sparse training baselines with a median gain of +13.5%."
    },
    {
      "title": "Act, Sense, Act: Learning Non-Markovian Active Perception Strategies from Large-Scale Egocentric Human Data",
      "url": "https://arxiv.org/abs/2602.04600",
      "date": "2026-02-04",
      "authors_text": "Jialiang Li, Yi Qiao, Yunhan Guo, Changwen Chen, Wenzhao Lian",
      "is_highlight": false,
      "score": 84.0,
      "summary": "The paper presents CoMe-VLA, a framework that utilizes large-scale egocentric data to enhance robots' active perception and manipulation in complex environments through non-Markovian strategies.",
      "teaser_image": "https://arxiv.org/html/2602.04600/x3.png",
      "debug_abstract": "Achieving generalizable manipulation in unconstrained environments requires the robot to proactively resolve information uncertainty, i.e., the capability of active perception. However, existing methods are often confined in limited types of sensing behaviors, restricting their applicability to complex environments. In this work, we formalize active perception as a non-Markovian process driven by information gain and decision branching, providing a structured categorization of visual active perception paradigms. Building on this perspective, we introduce CoMe-VLA, a cognitive and memory-aware vision-language-action (VLA) framework that leverages large-scale human egocentric data to learn versatile exploration and manipulation priors. Our framework integrates a cognitive auxiliary head for autonomous sub-task transitions and a dual-track memory system to maintain consistent self and environmental awareness by fusing proprioceptive and visual temporal contexts. By aligning human and robot hand-eye coordination behaviors in a unified egocentric action space, we train the model progressively in three stages. Extensive experiments on a wheel-based humanoid have demonstrated strong robustness and adaptability of our proposed method across diverse long-horizon tasks spanning multiple active perception scenarios."
    }
  ],
  "数字媒体与计算机视觉实验室": [
    {
      "title": "Topology-Aware Revival for Efficient Sparse Training",
      "url": "https://arxiv.org/abs/2602.04166",
      "date": "2026-02-04",
      "authors_text": "Meiling Jin, Fei Wang, Xiaoyun Yuan, Chen Qian, Yuan Cheng",
      "is_highlight": false,
      "score": 70.0,
      "summary": "Topology-Aware Revival (TAR) enhances static sparse training in deep reinforcement learning by selectively reactivating pruned connections, yielding significant performance improvements over static and dynamic baselines.",
      "teaser_image": "https://arxiv.org/html/2602.04166/x2.png",
      "debug_abstract": "Static sparse training is a promising route to efficient learning by committing to a fixed mask pattern, yet the constrained structure reduces robustness. Early pruning decisions can lock the network into a brittle structure that is difficult to escape, especially in deep reinforcement learning (RL) where the evolving policy continually shifts the training distribution. We propose Topology-Aware Revival (TAR), a lightweight one-shot post-pruning procedure that improves static sparsity without dynamic rewiring. After static pruning, TAR performs a single revival step by allocating a small reserve budget across layers according to topology needs, randomly uniformly reactivating a few previously pruned connections within each layer, and then keeping the resulting connectivity fixed for the remainder of training. Across multiple continuous-control tasks with SAC and TD3, TAR improves final return over static sparse baselines by up to +37.9% and also outperforms dynamic sparse training baselines with a median gain of +13.5%."
    },
    {
      "title": "Act, Sense, Act: Learning Non-Markovian Active Perception Strategies from Large-Scale Egocentric Human Data",
      "url": "https://arxiv.org/abs/2602.04600",
      "date": "2026-02-04",
      "authors_text": "Jialiang Li, Yi Qiao, Yunhan Guo, Changwen Chen, Wenzhao Lian",
      "is_highlight": false,
      "score": 84.0,
      "summary": "The paper presents CoMe-VLA, a framework that utilizes large-scale egocentric data to enhance robots' active perception and manipulation in complex environments through non-Markovian strategies.",
      "teaser_image": "https://arxiv.org/html/2602.04600/x3.png",
      "debug_abstract": "Achieving generalizable manipulation in unconstrained environments requires the robot to proactively resolve information uncertainty, i.e., the capability of active perception. However, existing methods are often confined in limited types of sensing behaviors, restricting their applicability to complex environments. In this work, we formalize active perception as a non-Markovian process driven by information gain and decision branching, providing a structured categorization of visual active perception paradigms. Building on this perspective, we introduce CoMe-VLA, a cognitive and memory-aware vision-language-action (VLA) framework that leverages large-scale human egocentric data to learn versatile exploration and manipulation priors. Our framework integrates a cognitive auxiliary head for autonomous sub-task transitions and a dual-track memory system to maintain consistent self and environmental awareness by fusing proprioceptive and visual temporal contexts. By aligning human and robot hand-eye coordination behaviors in a unified egocentric action space, we train the model progressively in three stages. Extensive experiments on a wheel-based humanoid have demonstrated strong robustness and adaptability of our proposed method across diverse long-horizon tasks spanning multiple active perception scenarios."
    }
  ],
  "机器视觉与智能学习实验室 (MVIG)": [
    {
      "title": "Topology-Aware Revival for Efficient Sparse Training",
      "url": "https://arxiv.org/abs/2602.04166",
      "date": "2026-02-04",
      "authors_text": "Meiling Jin, Fei Wang, Xiaoyun Yuan, Chen Qian, Yuan Cheng",
      "is_highlight": false,
      "score": 70.0,
      "summary": "Topology-Aware Revival (TAR) enhances static sparse training in deep reinforcement learning by selectively reactivating pruned connections, yielding significant performance improvements over static and dynamic baselines.",
      "teaser_image": "https://arxiv.org/html/2602.04166/x2.png",
      "debug_abstract": "Static sparse training is a promising route to efficient learning by committing to a fixed mask pattern, yet the constrained structure reduces robustness. Early pruning decisions can lock the network into a brittle structure that is difficult to escape, especially in deep reinforcement learning (RL) where the evolving policy continually shifts the training distribution. We propose Topology-Aware Revival (TAR), a lightweight one-shot post-pruning procedure that improves static sparsity without dynamic rewiring. After static pruning, TAR performs a single revival step by allocating a small reserve budget across layers according to topology needs, randomly uniformly reactivating a few previously pruned connections within each layer, and then keeping the resulting connectivity fixed for the remainder of training. Across multiple continuous-control tasks with SAC and TD3, TAR improves final return over static sparse baselines by up to +37.9% and also outperforms dynamic sparse training baselines with a median gain of +13.5%."
    },
    {
      "title": "Act, Sense, Act: Learning Non-Markovian Active Perception Strategies from Large-Scale Egocentric Human Data",
      "url": "https://arxiv.org/abs/2602.04600",
      "date": "2026-02-04",
      "authors_text": "Jialiang Li, Yi Qiao, Yunhan Guo, Changwen Chen, Wenzhao Lian",
      "is_highlight": false,
      "score": 84.0,
      "summary": "The paper presents CoMe-VLA, a framework that utilizes large-scale egocentric data to enhance robots' active perception and manipulation in complex environments through non-Markovian strategies.",
      "teaser_image": "https://arxiv.org/html/2602.04600/x3.png",
      "debug_abstract": "Achieving generalizable manipulation in unconstrained environments requires the robot to proactively resolve information uncertainty, i.e., the capability of active perception. However, existing methods are often confined in limited types of sensing behaviors, restricting their applicability to complex environments. In this work, we formalize active perception as a non-Markovian process driven by information gain and decision branching, providing a structured categorization of visual active perception paradigms. Building on this perspective, we introduce CoMe-VLA, a cognitive and memory-aware vision-language-action (VLA) framework that leverages large-scale human egocentric data to learn versatile exploration and manipulation priors. Our framework integrates a cognitive auxiliary head for autonomous sub-task transitions and a dual-track memory system to maintain consistent self and environmental awareness by fusing proprioceptive and visual temporal contexts. By aligning human and robot hand-eye coordination behaviors in a unified egocentric action space, we train the model progressively in three stages. Extensive experiments on a wheel-based humanoid have demonstrated strong robustness and adaptability of our proposed method across diverse long-horizon tasks spanning multiple active perception scenarios."
    }
  ],
  "赵波老师实验室": [
    {
      "title": "Topology-Aware Revival for Efficient Sparse Training",
      "url": "https://arxiv.org/abs/2602.04166",
      "date": "2026-02-04",
      "authors_text": "Meiling Jin, Fei Wang, Xiaoyun Yuan, Chen Qian, Yuan Cheng",
      "is_highlight": false,
      "score": 70.0,
      "summary": "Topology-Aware Revival (TAR) enhances static sparse training in deep reinforcement learning by selectively reactivating pruned connections, yielding significant performance improvements over static and dynamic baselines.",
      "teaser_image": "https://arxiv.org/html/2602.04166/x2.png",
      "debug_abstract": "Static sparse training is a promising route to efficient learning by committing to a fixed mask pattern, yet the constrained structure reduces robustness. Early pruning decisions can lock the network into a brittle structure that is difficult to escape, especially in deep reinforcement learning (RL) where the evolving policy continually shifts the training distribution. We propose Topology-Aware Revival (TAR), a lightweight one-shot post-pruning procedure that improves static sparsity without dynamic rewiring. After static pruning, TAR performs a single revival step by allocating a small reserve budget across layers according to topology needs, randomly uniformly reactivating a few previously pruned connections within each layer, and then keeping the resulting connectivity fixed for the remainder of training. Across multiple continuous-control tasks with SAC and TD3, TAR improves final return over static sparse baselines by up to +37.9% and also outperforms dynamic sparse training baselines with a median gain of +13.5%."
    },
    {
      "title": "Act, Sense, Act: Learning Non-Markovian Active Perception Strategies from Large-Scale Egocentric Human Data",
      "url": "https://arxiv.org/abs/2602.04600",
      "date": "2026-02-04",
      "authors_text": "Jialiang Li, Yi Qiao, Yunhan Guo, Changwen Chen, Wenzhao Lian",
      "is_highlight": false,
      "score": 84.0,
      "summary": "The paper presents CoMe-VLA, a framework that utilizes large-scale egocentric data to enhance robots' active perception and manipulation in complex environments through non-Markovian strategies.",
      "teaser_image": "https://arxiv.org/html/2602.04600/x3.png",
      "debug_abstract": "Achieving generalizable manipulation in unconstrained environments requires the robot to proactively resolve information uncertainty, i.e., the capability of active perception. However, existing methods are often confined in limited types of sensing behaviors, restricting their applicability to complex environments. In this work, we formalize active perception as a non-Markovian process driven by information gain and decision branching, providing a structured categorization of visual active perception paradigms. Building on this perspective, we introduce CoMe-VLA, a cognitive and memory-aware vision-language-action (VLA) framework that leverages large-scale human egocentric data to learn versatile exploration and manipulation priors. Our framework integrates a cognitive auxiliary head for autonomous sub-task transitions and a dual-track memory system to maintain consistent self and environmental awareness by fusing proprioceptive and visual temporal contexts. By aligning human and robot hand-eye coordination behaviors in a unified egocentric action space, we train the model progressively in three stages. Extensive experiments on a wheel-based humanoid have demonstrated strong robustness and adaptability of our proposed method across diverse long-horizon tasks spanning multiple active perception scenarios."
    }
  ],
  "ReThinkLab": [
    {
      "title": "Topology-Aware Revival for Efficient Sparse Training",
      "url": "https://arxiv.org/abs/2602.04166",
      "date": "2026-02-04",
      "authors_text": "Meiling Jin, Fei Wang, Xiaoyun Yuan, Chen Qian, Yuan Cheng",
      "is_highlight": false,
      "score": 70.0,
      "summary": "Topology-Aware Revival (TAR) enhances static sparse training in deep reinforcement learning by selectively reactivating pruned connections, yielding significant performance improvements over static and dynamic baselines.",
      "teaser_image": "https://arxiv.org/html/2602.04166/x2.png",
      "debug_abstract": "Static sparse training is a promising route to efficient learning by committing to a fixed mask pattern, yet the constrained structure reduces robustness. Early pruning decisions can lock the network into a brittle structure that is difficult to escape, especially in deep reinforcement learning (RL) where the evolving policy continually shifts the training distribution. We propose Topology-Aware Revival (TAR), a lightweight one-shot post-pruning procedure that improves static sparsity without dynamic rewiring. After static pruning, TAR performs a single revival step by allocating a small reserve budget across layers according to topology needs, randomly uniformly reactivating a few previously pruned connections within each layer, and then keeping the resulting connectivity fixed for the remainder of training. Across multiple continuous-control tasks with SAC and TD3, TAR improves final return over static sparse baselines by up to +37.9% and also outperforms dynamic sparse training baselines with a median gain of +13.5%."
    },
    {
      "title": "Act, Sense, Act: Learning Non-Markovian Active Perception Strategies from Large-Scale Egocentric Human Data",
      "url": "https://arxiv.org/abs/2602.04600",
      "date": "2026-02-04",
      "authors_text": "Jialiang Li, Yi Qiao, Yunhan Guo, Changwen Chen, Wenzhao Lian",
      "is_highlight": false,
      "score": 84.0,
      "summary": "The paper presents CoMe-VLA, a framework that utilizes large-scale egocentric data to enhance robots' active perception and manipulation in complex environments through non-Markovian strategies.",
      "teaser_image": "https://arxiv.org/html/2602.04600/x3.png",
      "debug_abstract": "Achieving generalizable manipulation in unconstrained environments requires the robot to proactively resolve information uncertainty, i.e., the capability of active perception. However, existing methods are often confined in limited types of sensing behaviors, restricting their applicability to complex environments. In this work, we formalize active perception as a non-Markovian process driven by information gain and decision branching, providing a structured categorization of visual active perception paradigms. Building on this perspective, we introduce CoMe-VLA, a cognitive and memory-aware vision-language-action (VLA) framework that leverages large-scale human egocentric data to learn versatile exploration and manipulation priors. Our framework integrates a cognitive auxiliary head for autonomous sub-task transitions and a dual-track memory system to maintain consistent self and environmental awareness by fusing proprioceptive and visual temporal contexts. By aligning human and robot hand-eye coordination behaviors in a unified egocentric action space, we train the model progressively in three stages. Extensive experiments on a wheel-based humanoid have demonstrated strong robustness and adaptability of our proposed method across diverse long-horizon tasks spanning multiple active perception scenarios."
    }
  ],
  "NUS AI LAB": [
    {
      "title": "From Data to Behavior: Predicting Unintended Model Behaviors Before Training",
      "url": "https://arxiv.org/abs/2602.04735",
      "date": "2026-02-04",
      "authors_text": "Mengru Wang, Zhenqian Xu, Junfeng Fang, Yunzhi Yao, Shumin Deng",
      "is_highlight": false,
      "score": 62.0,
      "summary": "The paper presents Data2Behavior and the Manipulating Data Features (MDF) method to predict unintended biases in Large Language Models before training, enhancing efficiency and safety.",
      "teaser_image": "https://arxiv.org/html/2602.04735/x2.png",
      "debug_abstract": "Large Language Models (LLMs) can acquire unintended biases from seemingly benign training data even without explicit cues or malicious content. Existing methods struggle to detect such risks before fine-tuning, making post hoc evaluation costly and inefficient. To address this challenge, we introduce Data2Behavior, a new task for predicting unintended model behaviors prior to training. We also propose Manipulating Data Features (MDF), a lightweight approach that summarizes candidate data through their mean representations and injects them into the forward pass of a base model, allowing latent statistical signals in the data to shape model activations and reveal potential biases and safety risks without updating any parameters. MDF achieves reliable prediction while consuming only about 20% of the GPU resources required for fine-tuning. Experiments on Qwen3-14B, Qwen2.5-32B-Instruct, and Gemma-3-12b-it confirm that MDF can anticipate unintended behaviors and provide insight into pre-training vulnerabilities."
    }
  ],
  "Microsystem Engineering and Robotics": [
    {
      "title": "From Data to Behavior: Predicting Unintended Model Behaviors Before Training",
      "url": "https://arxiv.org/abs/2602.04735",
      "date": "2026-02-04",
      "authors_text": "Mengru Wang, Zhenqian Xu, Junfeng Fang, Yunzhi Yao, Shumin Deng",
      "is_highlight": false,
      "score": 62.0,
      "summary": "The paper presents Data2Behavior and the Manipulating Data Features (MDF) method to predict unintended biases in Large Language Models before training, enhancing efficiency and safety.",
      "teaser_image": "https://arxiv.org/html/2602.04735/x2.png",
      "debug_abstract": "Large Language Models (LLMs) can acquire unintended biases from seemingly benign training data even without explicit cues or malicious content. Existing methods struggle to detect such risks before fine-tuning, making post hoc evaluation costly and inefficient. To address this challenge, we introduce Data2Behavior, a new task for predicting unintended model behaviors prior to training. We also propose Manipulating Data Features (MDF), a lightweight approach that summarizes candidate data through their mean representations and injects them into the forward pass of a base model, allowing latent statistical signals in the data to shape model activations and reveal potential biases and safety risks without updating any parameters. MDF achieves reliable prediction while consuming only about 20% of the GPU resources required for fine-tuning. Experiments on Qwen3-14B, Qwen2.5-32B-Instruct, and Gemma-3-12b-it confirm that MDF can anticipate unintended behaviors and provide insight into pre-training vulnerabilities."
    }
  ],
  "Advanced Robotics Centre": [
    {
      "title": "From Data to Behavior: Predicting Unintended Model Behaviors Before Training",
      "url": "https://arxiv.org/abs/2602.04735",
      "date": "2026-02-04",
      "authors_text": "Mengru Wang, Zhenqian Xu, Junfeng Fang, Yunzhi Yao, Shumin Deng",
      "is_highlight": false,
      "score": 62.0,
      "summary": "The paper presents Data2Behavior and the Manipulating Data Features (MDF) method to predict unintended biases in Large Language Models before training, enhancing efficiency and safety.",
      "teaser_image": "https://arxiv.org/html/2602.04735/x2.png",
      "debug_abstract": "Large Language Models (LLMs) can acquire unintended biases from seemingly benign training data even without explicit cues or malicious content. Existing methods struggle to detect such risks before fine-tuning, making post hoc evaluation costly and inefficient. To address this challenge, we introduce Data2Behavior, a new task for predicting unintended model behaviors prior to training. We also propose Manipulating Data Features (MDF), a lightweight approach that summarizes candidate data through their mean representations and injects them into the forward pass of a base model, allowing latent statistical signals in the data to shape model activations and reveal potential biases and safety risks without updating any parameters. MDF achieves reliable prediction while consuming only about 20% of the GPU resources required for fine-tuning. Experiments on Qwen3-14B, Qwen2.5-32B-Instruct, and Gemma-3-12b-it confirm that MDF can anticipate unintended behaviors and provide insight into pre-training vulnerabilities."
    }
  ],
  "Synteraction Lab": [
    {
      "title": "From Data to Behavior: Predicting Unintended Model Behaviors Before Training",
      "url": "https://arxiv.org/abs/2602.04735",
      "date": "2026-02-04",
      "authors_text": "Mengru Wang, Zhenqian Xu, Junfeng Fang, Yunzhi Yao, Shumin Deng",
      "is_highlight": false,
      "score": 62.0,
      "summary": "The paper presents Data2Behavior and the Manipulating Data Features (MDF) method to predict unintended biases in Large Language Models before training, enhancing efficiency and safety.",
      "teaser_image": "https://arxiv.org/html/2602.04735/x2.png",
      "debug_abstract": "Large Language Models (LLMs) can acquire unintended biases from seemingly benign training data even without explicit cues or malicious content. Existing methods struggle to detect such risks before fine-tuning, making post hoc evaluation costly and inefficient. To address this challenge, we introduce Data2Behavior, a new task for predicting unintended model behaviors prior to training. We also propose Manipulating Data Features (MDF), a lightweight approach that summarizes candidate data through their mean representations and injects them into the forward pass of a base model, allowing latent statistical signals in the data to shape model activations and reveal potential biases and safety risks without updating any parameters. MDF achieves reliable prediction while consuming only about 20% of the GPU resources required for fine-tuning. Experiments on Qwen3-14B, Qwen2.5-32B-Instruct, and Gemma-3-12b-it confirm that MDF can anticipate unintended behaviors and provide insight into pre-training vulnerabilities."
    }
  ],
  "机器人系统实验室 (RSL)": [
    {
      "title": "Fluid Representations in Reasoning Models",
      "url": "https://arxiv.org/abs/2602.04843",
      "date": "2026-02-04",
      "authors_text": "Dmitrii Kharlapenko, Alessandro Stolfo, Arthur Conmy, Mrinmaya Sachan, Zhijing Jin",
      "is_highlight": false,
      "score": 61.0,
      "summary": "The paper analyzes how the QwQ-32B reasoning model enhances problem-solving through evolving abstract representations, termed Fluid Reasoning Representations, during reasoning tasks.",
      "teaser_image": "https://arxiv.org/html/2602.04843/x1.png",
      "debug_abstract": "Reasoning language models, which generate long chains of thought, dramatically outperform non-reasoning language models on abstract problems. However, the internal model mechanisms that allow this superior performance remain poorly understood. We present a mechanistic analysis of how QwQ-32B - a model specifically trained to produce extensive reasoning traces - process abstract structural information. On Mystery Blocksworld - a semantically obfuscated planning domain - we find that QwQ-32B gradually improves its internal representation of actions and concepts during reasoning. The model develops abstract encodings that focus on structure rather than specific action names. Through steering experiments, we establish causal evidence that these adaptations improve problem solving: injecting refined representations from successful traces boosts accuracy, while symbolic representations can replace many obfuscated encodings with minimal performance loss. We find that one of the factors driving reasoning model performance is in-context refinement of token representations, which we dub Fluid Reasoning Representations."
    }
  ],
  "Sensing, Interaction & Perception Lab (SIPLAB)": [
    {
      "title": "Fluid Representations in Reasoning Models",
      "url": "https://arxiv.org/abs/2602.04843",
      "date": "2026-02-04",
      "authors_text": "Dmitrii Kharlapenko, Alessandro Stolfo, Arthur Conmy, Mrinmaya Sachan, Zhijing Jin",
      "is_highlight": false,
      "score": 61.0,
      "summary": "The paper analyzes how the QwQ-32B reasoning model enhances problem-solving through evolving abstract representations, termed Fluid Reasoning Representations, during reasoning tasks.",
      "teaser_image": "https://arxiv.org/html/2602.04843/x1.png",
      "debug_abstract": "Reasoning language models, which generate long chains of thought, dramatically outperform non-reasoning language models on abstract problems. However, the internal model mechanisms that allow this superior performance remain poorly understood. We present a mechanistic analysis of how QwQ-32B - a model specifically trained to produce extensive reasoning traces - process abstract structural information. On Mystery Blocksworld - a semantically obfuscated planning domain - we find that QwQ-32B gradually improves its internal representation of actions and concepts during reasoning. The model develops abstract encodings that focus on structure rather than specific action names. Through steering experiments, we establish causal evidence that these adaptations improve problem solving: injecting refined representations from successful traces boosts accuracy, while symbolic representations can replace many obfuscated encodings with minimal performance loss. We find that one of the factors driving reasoning model performance is in-context refinement of token representations, which we dub Fluid Reasoning Representations."
    }
  ]
}