{
  "机器人技术与系统国家重点实验室": [
    {
      "title": "M2I2HA: Multi-modal Object Detection Based on Intra- and Inter-Modal Hypergraph Attention",
      "url": "https://arxiv.org/abs/2601.14776",
      "date": "2026-01-24",
      "authors_text": "Xiaofan Yang, Yubin Liu, Wei Pan, Guoqing Chu, Junming Zhang",
      "is_highlight": true,
      "score": 60.0,
      "summary": "The M2I2HA network utilizes hypergraph theory for effective intra- and inter-modal feature extraction and fusion, achieving state-of-the-art performance in multi-modal object detection.",
      "teaser_image": null,
      "debug_abstract": "Recent advances in multi-modal detection have significantly improved detection accuracy in challenging environments (e.g., low light, overexposure). By integrating RGB with modalities such as thermal and depth, multi-modal fusion increases data redundancy and system robustness. However, significant challenges remain in effectively extracting task-relevant information both within and across modalities, as well as in achieving precise cross-modal alignment. While CNNs excel at feature extraction, they are limited by constrained receptive fields, strong inductive biases, and difficulty in capturing long-range dependencies. Transformer-based models offer global context but suffer from quadratic computational complexity and are confined to pairwise correlation modeling. Mamba and other State Space Models (SSMs), on the other hand, are hindered by their sequential scanning mechanism, which flattens 2D spatial structures into 1D sequences, disrupting topological relationships and limiting the modeling of complex higher-order dependencies. To address these issues, we propose a multi-modal perception network based on hypergraph theory called M2I2HA. Our architecture includes an Intra-Hypergraph Enhancement module to capture global many-to-many high-order relationships within each modality, and an Inter-Hypergraph Fusion module to align, enhance, and fuse cross-modal features by bridging configuration and spatial gaps between data sources. We further introduce a M2-FullPAD module to enable adaptive multi-level fusion of multi-modal enhanced features within the network, meanwhile enhancing data distribution and flow across the architecture. Extensive object detection experiments on multiple public datasets against baselines demonstrate that M2I2HA achieves state-of-the-art performance in multi-modal object detection tasks."
    },
    {
      "title": "Orchestrating Intelligence: Confidence-Aware Routing for Efficient Multi-Agent Collaboration across Multi-Scale Models",
      "url": "https://arxiv.org/abs/2601.04861",
      "date": "2026-01-24",
      "authors_text": "Jingbo Wang, Sendong Zhao, Jiatong Liu, Haochun Wang, Wanting Li",
      "is_highlight": false,
      "score": 75.0,
      "summary": "The OI-MAS framework enhances multi-agent collaboration by dynamically routing tasks to appropriate model scales based on cognitive demands, improving accuracy and reducing computational costs.",
      "teaser_image": null,
      "debug_abstract": "While multi-agent systems (MAS) have demonstrated superior performance over single-agent approaches in complex reasoning tasks, they often suffer from significant computational inefficiencies. Existing frameworks typically deploy large language models (LLMs) uniformly across all agent roles, failing to account for the varying cognitive demands of different reasoning stages. We address this inefficiency by proposing OI-MAS framework, a novel multi-agent framework that implements an adaptive model-selection policy across a heterogeneous pool of multi-scale LLMs. Specifically, OI-MAS introduces a state-dependent routing mechanism that dynamically selects agent roles and model scales throughout the reasoning process. In addition, we introduce a confidence-aware mechanism that selects appropriate model scales conditioned on task complexity, thus reducing unnecessary reliance on large-scale models. Experimental results show that OI-MAS consistently outperforms baseline multi-agent systems, improving accuracy by up to 12.88\\% while reducing cost by up to 79.78\\%."
    }
  ],
  "集群机器人系统实验室 (MagicLab)": [
    {
      "title": "AdaReasoner: Dynamic Tool Orchestration for Iterative Visual Reasoning",
      "url": "https://arxiv.org/abs/2601.18631",
      "date": "2026-01-26",
      "authors_text": "Mingyang Song, Haoyu Sun, Jiawei Gu, Linjie Li, Luxin Xu",
      "is_highlight": false,
      "score": 55.0,
      "summary": "AdaReasoner enhances visual reasoning in multimodal large language models by dynamically orchestrating tool usage through a scalable data pipeline and reinforcement learning, achieving state-of-the-art performance.",
      "teaser_image": null,
      "debug_abstract": "When humans face problems beyond their immediate capabilities, they rely on tools, providing a promising paradigm for improving visual reasoning in multimodal large language models (MLLMs). Effective reasoning, therefore, hinges on knowing which tools to use, when to invoke them, and how to compose them over multiple steps, even when faced with new tools or new tasks. We introduce \\textbf{AdaReasoner}, a family of multimodal models that learn tool use as a general reasoning skill rather than as tool-specific or explicitly supervised behavior. AdaReasoner is enabled by (i) a scalable data curation pipeline exposing models to long-horizon, multi-step tool interactions; (ii) Tool-GRPO, a reinforcement learning algorithm that optimizes tool selection and sequencing based on end-task success; and (iii) an adaptive learning mechanism that dynamically regulates tool usage. Together, these components allow models to infer tool utility from task context and intermediate outcomes, enabling coordination of multiple tools and generalization to unseen tools. Empirically, AdaReasoner exhibits strong tool-adaptive and generalization behaviors: it autonomously adopts beneficial tools, suppresses irrelevant ones, and adjusts tool usage frequency based on task demands, despite never being explicitly trained to do so. These capabilities translate into state-of-the-art performance across challenging benchmarks, improving the 7B base model by +24.9\\% on average and surpassing strong proprietary systems such as GPT-5 on multiple tasks, including VSP and Jigsaw."
    },
    {
      "title": "GenAgent: Scaling Text-to-Image Generation via Agentic Multimodal Reasoning",
      "url": "https://arxiv.org/abs/2601.18543",
      "date": "2026-01-26",
      "authors_text": "Kaixun Jiang, Yuzheng Wang, Junjie Zhou, Pandeng Li, Zhihang Liu",
      "is_highlight": false,
      "score": 65.0,
      "summary": "GenAgent enhances text-to-image generation by decoupling understanding and generation through an agentic framework, enabling autonomous interactions and improved performance across tasks.",
      "teaser_image": null,
      "debug_abstract": "We introduce GenAgent, unifying visual understanding and generation through an agentic multimodal model. Unlike unified models that face expensive training costs and understanding-generation trade-offs, GenAgent decouples these capabilities through an agentic framework: understanding is handled by the multimodal model itself, while generation is achieved by treating image generation models as invokable tools. Crucially, unlike existing modular systems constrained by static pipelines, this design enables autonomous multi-turn interactions where the agent generates multimodal chains-of-thought encompassing reasoning, tool invocation, judgment, and reflection to iteratively refine outputs. We employ a two-stage training strategy: first, cold-start with supervised fine-tuning on high-quality tool invocation and reflection data to bootstrap agent behaviors; second, end-to-end agentic reinforcement learning combining pointwise rewards (final image quality) and pairwise rewards (reflection accuracy), with trajectory resampling for enhanced multi-turn exploration. GenAgent significantly boosts base generator(FLUX.1-dev) performance on GenEval++ (+23.6\\%) and WISE (+14\\%). Beyond performance gains, our framework demonstrates three key properties: 1) cross-tool generalization to generators with varying capabilities, 2) test-time scaling with consistent improvements across interaction rounds, and 3) task-adaptive reasoning that automatically adjusts to different tasks. Our code will be available at \\href{this https URL}{this url}."
    },
    {
      "title": "SwipeGen: Bridging the Execution Gap in GUI Agents via Human-like Swipe Synthesis",
      "url": "https://arxiv.org/abs/2601.18305",
      "date": "2026-01-26",
      "authors_text": "Xuan Wang, Siyuan Su, Quantong Fu, Yongxiang Hu, Yangfan Zhou",
      "is_highlight": false,
      "score": 85.0,
      "summary": "SwipeGen enhances GUI agents' swipe execution by synthesizing human-like gestures, resulting in the GUISwiper achieving 69.07% accuracy, a 214% improvement over existing methods.",
      "teaser_image": null,
      "debug_abstract": "With the widespread adoption of Graphical User Interface (GUI) agents for automating GUI interaction tasks, substantial research focused on improving GUI perception to ground task instructions into concrete action steps. However, the step execution capability of these agents has gradually emerged as a new bottleneck for task completion. In particular, existing GUI agents often adopt overly simplified strategies for handling swipe interactions, preventing them from accurately replicating human-like behavior. To address this limitation, we decompose human swipe gestures into multiple quantifiable dimensions and propose an automated pipeline SwipeGen to synthesize human-like swipe interactions through GUI exploration. Based on this pipeline, we construct and release the first benchmark for evaluating the swipe execution capability of GUI agents. Furthermore, leveraging the synthesized data, we propose GUISwiper, a GUI agent with enhanced interaction execution capabilities. Experimental results demonstrate that GUISwiper achieves a swipe execution accuracy of 69.07%, representing a 214% improvement over existing VLM baselines."
    },
    {
      "title": "MetaWorld: Skill Transfer and Composition in a Hierarchical World Model for Grounding High-Level Instructions",
      "url": "https://arxiv.org/abs/2601.17507",
      "date": "2026-01-24",
      "authors_text": "Yutong Shen, Hangxu Liu, Kailin Pei, Ruizhe Xia, Tongtong Feng",
      "is_highlight": false,
      "score": 85.0,
      "summary": "MetaWorld introduces a hierarchical model that enhances humanoid robot loco-manipulation by integrating semantic planning and physical control, improving task completion and motion coherence.",
      "teaser_image": null,
      "debug_abstract": "Humanoid robot loco-manipulation remains constrained by the semantic-physical gap. Current methods face three limitations: Low sample efficiency in reinforcement learning, poor generalization in imitation learning, and physical inconsistency in VLMs. We propose MetaWorld, a hierarchical world model that integrates semantic planning and physical control via expert policy transfer. The framework decouples tasks into a VLM-driven semantic layer and a latent dynamics model operating in a compact state space. Our dynamic expert selection and motion prior fusion mechanism leverages a pre-trained multi-expert policy library as transferable knowledge, enabling efficient online adaptation via a two-stage framework. VLMs serve as semantic interfaces, mapping instructions to executable skills and bypassing symbol grounding. Experiments on Humanoid-Bench show MetaWorld outperforms world model-based RL in task completion and motion coherence. Our code will be found at this https URL"
    },
    {
      "title": "MARO: Learning Stronger Reasoning from Social Interaction",
      "url": "https://arxiv.org/abs/2601.12323",
      "date": "2026-01-24",
      "authors_text": "Yin Cai, Zhouhong Gu, Juntao Zhang, Ping Chen",
      "is_highlight": false,
      "score": 70.0,
      "summary": "The MARO method enhances large language models' reasoning abilities through multi-agent social interactions, addressing learning signal sparsity, role imbalance, and environmental instability.",
      "teaser_image": null,
      "debug_abstract": "Humans face countless scenarios that require reasoning and judgment in daily life. However, existing large language model training methods primarily allow models to learn from existing textual content or solve predetermined problems, lacking experience in real scenarios involving interaction, negotiation, and competition with others. To address this, this paper proposes Multi-Agent Reward Optimization (MARO), a method that enables large language models (LLMs) to acquire stronger reasoning abilities by learning and practicing in multi-agent social environments. Specifically, MARO first addresses the sparse learning signal problem by decomposing final success or failure outcomes into each specific behavior during the interaction process; second, it handles the uneven role distribution problem by balancing the training sample weights of different roles; finally, it addresses environmental instability issues by directly evaluating the utility of each behavior. Experimental results demonstrate that MARO not only achieves significant improvements in social reasoning capabilities, but also that the abilities acquired through social simulation learning can effectively transfer to other tasks such as mathematical reasoning and instruction following. This reveals the tremendous potential of multi-agent social learning in enhancing the general reasoning capabilities of LLMs."
    }
  ],
  "智能感知与无人系统实验室": [
    {
      "title": "AdaReasoner: Dynamic Tool Orchestration for Iterative Visual Reasoning",
      "url": "https://arxiv.org/abs/2601.18631",
      "date": "2026-01-26",
      "authors_text": "Mingyang Song, Haoyu Sun, Jiawei Gu, Linjie Li, Luxin Xu",
      "is_highlight": false,
      "score": 55.0,
      "summary": "AdaReasoner enhances visual reasoning in multimodal large language models by dynamically orchestrating tool usage through a scalable data pipeline and reinforcement learning, achieving state-of-the-art performance.",
      "teaser_image": null,
      "debug_abstract": "When humans face problems beyond their immediate capabilities, they rely on tools, providing a promising paradigm for improving visual reasoning in multimodal large language models (MLLMs). Effective reasoning, therefore, hinges on knowing which tools to use, when to invoke them, and how to compose them over multiple steps, even when faced with new tools or new tasks. We introduce \\textbf{AdaReasoner}, a family of multimodal models that learn tool use as a general reasoning skill rather than as tool-specific or explicitly supervised behavior. AdaReasoner is enabled by (i) a scalable data curation pipeline exposing models to long-horizon, multi-step tool interactions; (ii) Tool-GRPO, a reinforcement learning algorithm that optimizes tool selection and sequencing based on end-task success; and (iii) an adaptive learning mechanism that dynamically regulates tool usage. Together, these components allow models to infer tool utility from task context and intermediate outcomes, enabling coordination of multiple tools and generalization to unseen tools. Empirically, AdaReasoner exhibits strong tool-adaptive and generalization behaviors: it autonomously adopts beneficial tools, suppresses irrelevant ones, and adjusts tool usage frequency based on task demands, despite never being explicitly trained to do so. These capabilities translate into state-of-the-art performance across challenging benchmarks, improving the 7B base model by +24.9\\% on average and surpassing strong proprietary systems such as GPT-5 on multiple tasks, including VSP and Jigsaw."
    },
    {
      "title": "GenAgent: Scaling Text-to-Image Generation via Agentic Multimodal Reasoning",
      "url": "https://arxiv.org/abs/2601.18543",
      "date": "2026-01-26",
      "authors_text": "Kaixun Jiang, Yuzheng Wang, Junjie Zhou, Pandeng Li, Zhihang Liu",
      "is_highlight": false,
      "score": 65.0,
      "summary": "GenAgent enhances text-to-image generation by decoupling understanding and generation through an agentic framework, enabling autonomous interactions and improved performance across tasks.",
      "teaser_image": null,
      "debug_abstract": "We introduce GenAgent, unifying visual understanding and generation through an agentic multimodal model. Unlike unified models that face expensive training costs and understanding-generation trade-offs, GenAgent decouples these capabilities through an agentic framework: understanding is handled by the multimodal model itself, while generation is achieved by treating image generation models as invokable tools. Crucially, unlike existing modular systems constrained by static pipelines, this design enables autonomous multi-turn interactions where the agent generates multimodal chains-of-thought encompassing reasoning, tool invocation, judgment, and reflection to iteratively refine outputs. We employ a two-stage training strategy: first, cold-start with supervised fine-tuning on high-quality tool invocation and reflection data to bootstrap agent behaviors; second, end-to-end agentic reinforcement learning combining pointwise rewards (final image quality) and pairwise rewards (reflection accuracy), with trajectory resampling for enhanced multi-turn exploration. GenAgent significantly boosts base generator(FLUX.1-dev) performance on GenEval++ (+23.6\\%) and WISE (+14\\%). Beyond performance gains, our framework demonstrates three key properties: 1) cross-tool generalization to generators with varying capabilities, 2) test-time scaling with consistent improvements across interaction rounds, and 3) task-adaptive reasoning that automatically adjusts to different tasks. Our code will be available at \\href{this https URL}{this url}."
    },
    {
      "title": "SwipeGen: Bridging the Execution Gap in GUI Agents via Human-like Swipe Synthesis",
      "url": "https://arxiv.org/abs/2601.18305",
      "date": "2026-01-26",
      "authors_text": "Xuan Wang, Siyuan Su, Quantong Fu, Yongxiang Hu, Yangfan Zhou",
      "is_highlight": false,
      "score": 85.0,
      "summary": "SwipeGen enhances GUI agents' swipe execution by synthesizing human-like gestures, resulting in the GUISwiper achieving 69.07% accuracy, a 214% improvement over existing methods.",
      "teaser_image": null,
      "debug_abstract": "With the widespread adoption of Graphical User Interface (GUI) agents for automating GUI interaction tasks, substantial research focused on improving GUI perception to ground task instructions into concrete action steps. However, the step execution capability of these agents has gradually emerged as a new bottleneck for task completion. In particular, existing GUI agents often adopt overly simplified strategies for handling swipe interactions, preventing them from accurately replicating human-like behavior. To address this limitation, we decompose human swipe gestures into multiple quantifiable dimensions and propose an automated pipeline SwipeGen to synthesize human-like swipe interactions through GUI exploration. Based on this pipeline, we construct and release the first benchmark for evaluating the swipe execution capability of GUI agents. Furthermore, leveraging the synthesized data, we propose GUISwiper, a GUI agent with enhanced interaction execution capabilities. Experimental results demonstrate that GUISwiper achieves a swipe execution accuracy of 69.07%, representing a 214% improvement over existing VLM baselines."
    },
    {
      "title": "MetaWorld: Skill Transfer and Composition in a Hierarchical World Model for Grounding High-Level Instructions",
      "url": "https://arxiv.org/abs/2601.17507",
      "date": "2026-01-24",
      "authors_text": "Yutong Shen, Hangxu Liu, Kailin Pei, Ruizhe Xia, Tongtong Feng",
      "is_highlight": false,
      "score": 85.0,
      "summary": "MetaWorld introduces a hierarchical model that enhances humanoid robot loco-manipulation by integrating semantic planning and physical control, improving task completion and motion coherence.",
      "teaser_image": null,
      "debug_abstract": "Humanoid robot loco-manipulation remains constrained by the semantic-physical gap. Current methods face three limitations: Low sample efficiency in reinforcement learning, poor generalization in imitation learning, and physical inconsistency in VLMs. We propose MetaWorld, a hierarchical world model that integrates semantic planning and physical control via expert policy transfer. The framework decouples tasks into a VLM-driven semantic layer and a latent dynamics model operating in a compact state space. Our dynamic expert selection and motion prior fusion mechanism leverages a pre-trained multi-expert policy library as transferable knowledge, enabling efficient online adaptation via a two-stage framework. VLMs serve as semantic interfaces, mapping instructions to executable skills and bypassing symbol grounding. Experiments on Humanoid-Bench show MetaWorld outperforms world model-based RL in task completion and motion coherence. Our code will be found at this https URL"
    },
    {
      "title": "MARO: Learning Stronger Reasoning from Social Interaction",
      "url": "https://arxiv.org/abs/2601.12323",
      "date": "2026-01-24",
      "authors_text": "Yin Cai, Zhouhong Gu, Juntao Zhang, Ping Chen",
      "is_highlight": false,
      "score": 70.0,
      "summary": "The MARO method enhances large language models' reasoning abilities through multi-agent social interactions, addressing learning signal sparsity, role imbalance, and environmental instability.",
      "teaser_image": null,
      "debug_abstract": "Humans face countless scenarios that require reasoning and judgment in daily life. However, existing large language model training methods primarily allow models to learn from existing textual content or solve predetermined problems, lacking experience in real scenarios involving interaction, negotiation, and competition with others. To address this, this paper proposes Multi-Agent Reward Optimization (MARO), a method that enables large language models (LLMs) to acquire stronger reasoning abilities by learning and practicing in multi-agent social environments. Specifically, MARO first addresses the sparse learning signal problem by decomposing final success or failure outcomes into each specific behavior during the interaction process; second, it handles the uneven role distribution problem by balancing the training sample weights of different roles; finally, it addresses environmental instability issues by directly evaluating the utility of each behavior. Experimental results demonstrate that MARO not only achieves significant improvements in social reasoning capabilities, but also that the abilities acquired through social simulation learning can effectively transfer to other tasks such as mathematical reasoning and instruction following. This reveals the tremendous potential of multi-agent social learning in enhancing the general reasoning capabilities of LLMs."
    }
  ],
  "智能人机交互实验室 (MemX)": [
    {
      "title": "AdaReasoner: Dynamic Tool Orchestration for Iterative Visual Reasoning",
      "url": "https://arxiv.org/abs/2601.18631",
      "date": "2026-01-26",
      "authors_text": "Mingyang Song, Haoyu Sun, Jiawei Gu, Linjie Li, Luxin Xu",
      "is_highlight": false,
      "score": 55.0,
      "summary": "AdaReasoner enhances visual reasoning in multimodal large language models by dynamically orchestrating tool usage through a scalable data pipeline and reinforcement learning, achieving state-of-the-art performance.",
      "teaser_image": null,
      "debug_abstract": "When humans face problems beyond their immediate capabilities, they rely on tools, providing a promising paradigm for improving visual reasoning in multimodal large language models (MLLMs). Effective reasoning, therefore, hinges on knowing which tools to use, when to invoke them, and how to compose them over multiple steps, even when faced with new tools or new tasks. We introduce \\textbf{AdaReasoner}, a family of multimodal models that learn tool use as a general reasoning skill rather than as tool-specific or explicitly supervised behavior. AdaReasoner is enabled by (i) a scalable data curation pipeline exposing models to long-horizon, multi-step tool interactions; (ii) Tool-GRPO, a reinforcement learning algorithm that optimizes tool selection and sequencing based on end-task success; and (iii) an adaptive learning mechanism that dynamically regulates tool usage. Together, these components allow models to infer tool utility from task context and intermediate outcomes, enabling coordination of multiple tools and generalization to unseen tools. Empirically, AdaReasoner exhibits strong tool-adaptive and generalization behaviors: it autonomously adopts beneficial tools, suppresses irrelevant ones, and adjusts tool usage frequency based on task demands, despite never being explicitly trained to do so. These capabilities translate into state-of-the-art performance across challenging benchmarks, improving the 7B base model by +24.9\\% on average and surpassing strong proprietary systems such as GPT-5 on multiple tasks, including VSP and Jigsaw."
    },
    {
      "title": "GenAgent: Scaling Text-to-Image Generation via Agentic Multimodal Reasoning",
      "url": "https://arxiv.org/abs/2601.18543",
      "date": "2026-01-26",
      "authors_text": "Kaixun Jiang, Yuzheng Wang, Junjie Zhou, Pandeng Li, Zhihang Liu",
      "is_highlight": false,
      "score": 65.0,
      "summary": "GenAgent enhances text-to-image generation by decoupling understanding and generation through an agentic framework, enabling autonomous interactions and improved performance across tasks.",
      "teaser_image": null,
      "debug_abstract": "We introduce GenAgent, unifying visual understanding and generation through an agentic multimodal model. Unlike unified models that face expensive training costs and understanding-generation trade-offs, GenAgent decouples these capabilities through an agentic framework: understanding is handled by the multimodal model itself, while generation is achieved by treating image generation models as invokable tools. Crucially, unlike existing modular systems constrained by static pipelines, this design enables autonomous multi-turn interactions where the agent generates multimodal chains-of-thought encompassing reasoning, tool invocation, judgment, and reflection to iteratively refine outputs. We employ a two-stage training strategy: first, cold-start with supervised fine-tuning on high-quality tool invocation and reflection data to bootstrap agent behaviors; second, end-to-end agentic reinforcement learning combining pointwise rewards (final image quality) and pairwise rewards (reflection accuracy), with trajectory resampling for enhanced multi-turn exploration. GenAgent significantly boosts base generator(FLUX.1-dev) performance on GenEval++ (+23.6\\%) and WISE (+14\\%). Beyond performance gains, our framework demonstrates three key properties: 1) cross-tool generalization to generators with varying capabilities, 2) test-time scaling with consistent improvements across interaction rounds, and 3) task-adaptive reasoning that automatically adjusts to different tasks. Our code will be available at \\href{this https URL}{this url}."
    },
    {
      "title": "SwipeGen: Bridging the Execution Gap in GUI Agents via Human-like Swipe Synthesis",
      "url": "https://arxiv.org/abs/2601.18305",
      "date": "2026-01-26",
      "authors_text": "Xuan Wang, Siyuan Su, Quantong Fu, Yongxiang Hu, Yangfan Zhou",
      "is_highlight": false,
      "score": 85.0,
      "summary": "SwipeGen enhances GUI agents' swipe execution by synthesizing human-like gestures, resulting in the GUISwiper achieving 69.07% accuracy, a 214% improvement over existing methods.",
      "teaser_image": null,
      "debug_abstract": "With the widespread adoption of Graphical User Interface (GUI) agents for automating GUI interaction tasks, substantial research focused on improving GUI perception to ground task instructions into concrete action steps. However, the step execution capability of these agents has gradually emerged as a new bottleneck for task completion. In particular, existing GUI agents often adopt overly simplified strategies for handling swipe interactions, preventing them from accurately replicating human-like behavior. To address this limitation, we decompose human swipe gestures into multiple quantifiable dimensions and propose an automated pipeline SwipeGen to synthesize human-like swipe interactions through GUI exploration. Based on this pipeline, we construct and release the first benchmark for evaluating the swipe execution capability of GUI agents. Furthermore, leveraging the synthesized data, we propose GUISwiper, a GUI agent with enhanced interaction execution capabilities. Experimental results demonstrate that GUISwiper achieves a swipe execution accuracy of 69.07%, representing a 214% improvement over existing VLM baselines."
    },
    {
      "title": "MetaWorld: Skill Transfer and Composition in a Hierarchical World Model for Grounding High-Level Instructions",
      "url": "https://arxiv.org/abs/2601.17507",
      "date": "2026-01-24",
      "authors_text": "Yutong Shen, Hangxu Liu, Kailin Pei, Ruizhe Xia, Tongtong Feng",
      "is_highlight": false,
      "score": 85.0,
      "summary": "MetaWorld introduces a hierarchical model that enhances humanoid robot loco-manipulation by integrating semantic planning and physical control, improving task completion and motion coherence.",
      "teaser_image": null,
      "debug_abstract": "Humanoid robot loco-manipulation remains constrained by the semantic-physical gap. Current methods face three limitations: Low sample efficiency in reinforcement learning, poor generalization in imitation learning, and physical inconsistency in VLMs. We propose MetaWorld, a hierarchical world model that integrates semantic planning and physical control via expert policy transfer. The framework decouples tasks into a VLM-driven semantic layer and a latent dynamics model operating in a compact state space. Our dynamic expert selection and motion prior fusion mechanism leverages a pre-trained multi-expert policy library as transferable knowledge, enabling efficient online adaptation via a two-stage framework. VLMs serve as semantic interfaces, mapping instructions to executable skills and bypassing symbol grounding. Experiments on Humanoid-Bench show MetaWorld outperforms world model-based RL in task completion and motion coherence. Our code will be found at this https URL"
    },
    {
      "title": "MARO: Learning Stronger Reasoning from Social Interaction",
      "url": "https://arxiv.org/abs/2601.12323",
      "date": "2026-01-24",
      "authors_text": "Yin Cai, Zhouhong Gu, Juntao Zhang, Ping Chen",
      "is_highlight": false,
      "score": 70.0,
      "summary": "The MARO method enhances large language models' reasoning abilities through multi-agent social interactions, addressing learning signal sparsity, role imbalance, and environmental instability.",
      "teaser_image": null,
      "debug_abstract": "Humans face countless scenarios that require reasoning and judgment in daily life. However, existing large language model training methods primarily allow models to learn from existing textual content or solve predetermined problems, lacking experience in real scenarios involving interaction, negotiation, and competition with others. To address this, this paper proposes Multi-Agent Reward Optimization (MARO), a method that enables large language models (LLMs) to acquire stronger reasoning abilities by learning and practicing in multi-agent social environments. Specifically, MARO first addresses the sparse learning signal problem by decomposing final success or failure outcomes into each specific behavior during the interaction process; second, it handles the uneven role distribution problem by balancing the training sample weights of different roles; finally, it addresses environmental instability issues by directly evaluating the utility of each behavior. Experimental results demonstrate that MARO not only achieves significant improvements in social reasoning capabilities, but also that the abilities acquired through social simulation learning can effectively transfer to other tasks such as mathematical reasoning and instruction following. This reveals the tremendous potential of multi-agent social learning in enhancing the general reasoning capabilities of LLMs."
    }
  ],
  "BIGAI": [
    {
      "title": "Hybrid Deep Feature Extraction and ML for Construction and Demolition Debris Classification",
      "url": "https://arxiv.org/abs/2601.17038",
      "date": "2026-01-20",
      "authors_text": "Obai Alashram, Nejad Alagha, Mahmoud AlKakuri, Zeeshan Swaveel, Abigail Copiaco",
      "is_highlight": false,
      "score": 65.0,
      "summary": "This study introduces a hybrid pipeline combining deep feature extraction and classical ML classifiers for high-accuracy automated classification of construction debris, achieving up to 99.5% accuracy.",
      "teaser_image": null,
      "debug_abstract": "The construction industry produces significant volumes of debris, making effective sorting and classification critical for sustainable waste management and resource recovery. This study presents a hybrid vision-based pipeline that integrates deep feature extraction with classical machine learning (ML) classifiers for automated construction and demolition (C\\&amp;D) debris classification. A novel dataset comprising 1,800 balanced, high-quality images representing four material categories, Ceramic/Tile, Concrete, Trash/Waste, and Wood was collected from real construction sites in the UAE, capturing diverse real-world conditions. Deep features were extracted using a pre-trained Xception network, and multiple ML classifiers, including SVM, kNN, Bagged Trees, LDA, and Logistic Regression, were systematically evaluated. The results demonstrate that hybrid pipelines using Xception features with simple classifiers such as Linear SVM, kNN, and Bagged Trees achieve state-of-the-art performance, with up to 99.5\\% accuracy and macro-F1 scores, surpassing more complex or end-to-end deep learning approaches. The analysis highlights the operational benefits of this approach for robust, field-deployable debris identification and provides pathways for future integration with robotics and onsite automation systems."
    }
  ],
  "智能技术与系统实验室": [
    {
      "title": "Dep-Search: Learning Dependency-Aware Reasoning Traces with Persistent Memory",
      "url": "https://arxiv.org/abs/2601.18771",
      "date": "2026-01-26",
      "authors_text": "Yanming Liu, Xinyue Peng, Zixuan Yan, Yanxin Shen, Wenjie Xu",
      "is_highlight": false,
      "score": 30.0,
      "summary": "Dep-Search is a dependency-aware framework that enhances LLMs' multi-hop reasoning by integrating structured reasoning, retrieval, and persistent memory, outperforming existing methods.",
      "teaser_image": null,
      "debug_abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in complex reasoning tasks, particularly when augmented with search mechanisms that enable systematic exploration of external knowledge bases. The field has evolved from traditional retrieval-augmented generation (RAG) frameworks to more sophisticated search-based frameworks that orchestrate multi-step reasoning through explicit search strategies. However, existing search frameworks still rely heavily on implicit natural language reasoning to determine search strategies and how to leverage retrieved information across reasoning steps. This reliance on implicit reasoning creates fundamental challenges for managing dependencies between sub-questions, efficiently reusing previously retrieved knowledge, and learning optimal search strategies through reinforcement learning. To address these limitations, we propose Dep-Search, a dependency-aware search framework that advances beyond existing search frameworks by integrating structured reasoning, retrieval, and persistent memory through GRPO. Dep-Search introduces explicit control mechanisms that enable the model to decompose questions with dependency relationships, retrieve information when needed, access previously stored knowledge from memory, and summarize long reasoning contexts into reusable memory entries. Through extensive experiments on seven diverse question answering datasets, we demonstrate that Dep-Search significantly enhances LLMs&#39; ability to tackle complex multi-hop reasoning tasks, achieving substantial improvements over strong baselines across different model scales."
    },
    {
      "title": "HomoFM: Deep Homography Estimation with Flow Matching",
      "url": "https://arxiv.org/abs/2601.18222",
      "date": "2026-01-26",
      "authors_text": "Mengfan He, Liangzheng Sun, Chunyu Li, Ziyang Meng",
      "is_highlight": false,
      "score": 55.0,
      "summary": "HomoFM introduces a flow matching approach for deep homography estimation, enhancing accuracy and robustness by modeling velocity fields and employing domain adaptation techniques.",
      "teaser_image": null,
      "debug_abstract": "Deep homography estimation has broad applications in computer vision and robotics. Remarkable progresses have been achieved while the existing methods typically treat it as a direct regression or iterative refinement problem and often struggling to capture complex geometric transformations or generalize across different domains. In this work, we propose HomoFM, a new framework that introduces the flow matching technique from generative modeling into the homography estimation task for the first time. Unlike the existing methods, we formulate homography estimation problem as a velocity field learning problem. By modeling a continuous and point-wise velocity field that transforms noisy distributions into registered coordinates, the proposed network recovers high-precision transformations through a conditional flow trajectory. Furthermore, to address the challenge of domain shifts issue, e.g., the cases of multimodal matching or varying illumination scenarios, we integrate a gradient reversal layer (GRL) into the feature extraction backbone. This domain adaptation strategy explicitly constrains the encoder to learn domain-invariant representations, significantly enhancing the network&#39;s robustness. Extensive experiments demonstrate the effectiveness of the proposed method, showing that HomoFM outperforms state-of-the-art methods in both estimation accuracy and robustness on standard benchmarks. Code and data resource are available at this https URL."
    },
    {
      "title": "AnchoredDream: Zero-Shot 360° Indoor Scene Generation from a Single View via Geometric Grounding",
      "url": "https://arxiv.org/abs/2601.16532",
      "date": "2026-01-26",
      "authors_text": "Runmao Yao, Junsheng Zhou, Zhen Dong, Yu-Shen Liu",
      "is_highlight": false,
      "score": 45.0,
      "summary": "AnchoredDream introduces a zero-shot method for generating 360° indoor scenes from a single image, enhancing appearance consistency and geometric plausibility through innovative geometry anchoring techniques.",
      "teaser_image": null,
      "debug_abstract": "Single-view indoor scene generation plays a crucial role in a range of real-world applications. However, generating a complete 360° scene from a single image remains a highly ill-posed and challenging problem. Recent approaches have made progress by leveraging diffusion models and depth estimation networks, yet they still struggle to maintain appearance consistency and geometric plausibility under large viewpoint changes, limiting their effectiveness in full-scene generation. To address this, we propose AnchoredDream, a novel zero-shot pipeline that anchors 360° scene generation on high-fidelity geometry via an appearance-geometry mutual boosting mechanism. Given a single-view image, our method first performs appearance-guided geometry generation to construct a reliable 3D scene layout. Then, we progressively generate the complete scene through a series of modules: warp-and-inpaint, warp-and-refine, post-optimization, and a novel Grouting Block, which ensures seamless transitions between the input view and generated regions. Extensive experiments demonstrate that AnchoredDream outperforms existing methods by a large margin in both appearance consistency and geometric plausibility--all in a zero-shot manner. Our results highlight the potential of geometric grounding for high-quality, zero-shot single-view scene generation."
    },
    {
      "title": "The Flexibility Trap: Why Arbitrary Order Limits Reasoning Potential in Diffusion Language Models",
      "url": "https://arxiv.org/abs/2601.15165",
      "date": "2026-01-26",
      "authors_text": "Zanlin Ni, Shenzhi Wang, Yang Yue, Tianyu Yu, Weilin Zhao",
      "is_highlight": false,
      "score": 40.0,
      "summary": "This paper argues that the arbitrary order generation in diffusion language models constrains reasoning potential, advocating for a simpler approach, JustGRPO, to enhance performance.",
      "teaser_image": null,
      "debug_abstract": "Diffusion Large Language Models (dLLMs) break the rigid left-to-right constraint of traditional LLMs, enabling token generation in arbitrary orders. Intuitively, this flexibility implies a solution space that strictly supersets the fixed autoregressive trajectory, theoretically unlocking superior reasoning potential for general tasks like mathematics and coding. Consequently, numerous works have leveraged reinforcement learning (RL) to elicit the reasoning capability of dLLMs. In this paper, we reveal a counter-intuitive reality: arbitrary order generation, in its current form, narrows rather than expands the reasoning boundary of dLLMs. We find that dLLMs tend to exploit this order flexibility to bypass high-uncertainty tokens that are crucial for exploration, leading to a premature collapse of the solution space. This observation motivates a rethink of RL approaches for dLLMs, where considerable complexities, such as handling combinatorial trajectories and intractable likelihoods, are often devoted to preserving this flexibility. We demonstrate that effective reasoning can be better elicited by intentionally forgoing arbitrary order and applying standard Group Relative Policy Optimization (GRPO) instead. Our approach, JustGRPO, is minimalist yet surprisingly effective (e.g., 89.1% accuracy on GSM8K) while fully retaining the parallel decoding ability of dLLMs. Project page: this https URL"
    },
    {
      "title": "MetaWorld: Skill Transfer and Composition in a Hierarchical World Model for Grounding High-Level Instructions",
      "url": "https://arxiv.org/abs/2601.17507",
      "date": "2026-01-24",
      "authors_text": "Yutong Shen, Hangxu Liu, Kailin Pei, Ruizhe Xia, Tongtong Feng",
      "is_highlight": false,
      "score": 85.0,
      "summary": "MetaWorld introduces a hierarchical model that enhances humanoid robot loco-manipulation by integrating semantic planning and physical control, improving task completion and motion coherence.",
      "teaser_image": null,
      "debug_abstract": "Humanoid robot loco-manipulation remains constrained by the semantic-physical gap. Current methods face three limitations: Low sample efficiency in reinforcement learning, poor generalization in imitation learning, and physical inconsistency in VLMs. We propose MetaWorld, a hierarchical world model that integrates semantic planning and physical control via expert policy transfer. The framework decouples tasks into a VLM-driven semantic layer and a latent dynamics model operating in a compact state space. Our dynamic expert selection and motion prior fusion mechanism leverages a pre-trained multi-expert policy library as transferable knowledge, enabling efficient online adaptation via a two-stage framework. VLMs serve as semantic interfaces, mapping instructions to executable skills and bypassing symbol grounding. Experiments on Humanoid-Bench show MetaWorld outperforms world model-based RL in task completion and motion coherence. Our code will be found at this https URL"
    },
    {
      "title": "Latent-Space Contrastive Reinforcement Learning for Stable and Efficient LLM Reasoning",
      "url": "https://arxiv.org/abs/2601.17275",
      "date": "2026-01-24",
      "authors_text": "Lianlei Shan, Han Chen, Yixuan Wang, Zhenjie Liu, Wei Li",
      "is_highlight": false,
      "score": 60.0,
      "summary": "The paper introduces DeepLatent Reasoning (DLR), a contrastive reinforcement learning framework that enhances LLM reasoning by optimizing latent space exploration while preventing catastrophic forgetting.",
      "teaser_image": null,
      "debug_abstract": "While Large Language Models (LLMs) demonstrate exceptional performance in surface-level text generation, their nature in handling complex multi-step reasoning tasks often remains one of ``statistical fitting&#39;&#39; rather than systematic logical deduction. Traditional Reinforcement Learning (RL) attempts to mitigate this by introducing a ``think-before-speak&#39;&#39; paradigm. However, applying RL directly in high-dimensional, discrete token spaces faces three inherent challenges: sample-inefficient rollouts, high gradient estimation variance, and the risk of catastrophic forgetting. To fundamentally address these structural bottlenecks, we propose \\textbf{DeepLatent Reasoning (DLR)}, a latent-space bidirectional contrastive reinforcement learning framework. This framework shifts the trial-and-error cost from expensive token-level full sequence generation to the continuous latent manifold. Specifically, we introduce a lightweight assistant model to efficiently sample $K$ reasoning chain encodings within the latent space. These encodings are filtered via a dual reward mechanism based on correctness and formatting; only high-value latent trajectories are fed into a \\textbf{frozen main model} for single-pass decoding. To maximize reasoning diversity while maintaining coherence, we design a contrastive learning objective to enable directed exploration within the latent space. Since the main model parameters remain frozen during optimization, this method mathematically eliminates catastrophic forgetting. Experiments demonstrate that under comparable GPU computational budgets, DLR achieves more stable training convergence, supports longer-horizon reasoning chains, and facilitates the sustainable accumulation of reasoning capabilities, providing a viable path toward reliable and scalable reinforcement learning for LLMs."
    }
  ],
  "MARS多模态学习实验室": [
    {
      "title": "Dep-Search: Learning Dependency-Aware Reasoning Traces with Persistent Memory",
      "url": "https://arxiv.org/abs/2601.18771",
      "date": "2026-01-26",
      "authors_text": "Yanming Liu, Xinyue Peng, Zixuan Yan, Yanxin Shen, Wenjie Xu",
      "is_highlight": false,
      "score": 30.0,
      "summary": "Dep-Search is a dependency-aware framework that enhances LLMs' multi-hop reasoning by integrating structured reasoning, retrieval, and persistent memory, outperforming existing methods.",
      "teaser_image": null,
      "debug_abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in complex reasoning tasks, particularly when augmented with search mechanisms that enable systematic exploration of external knowledge bases. The field has evolved from traditional retrieval-augmented generation (RAG) frameworks to more sophisticated search-based frameworks that orchestrate multi-step reasoning through explicit search strategies. However, existing search frameworks still rely heavily on implicit natural language reasoning to determine search strategies and how to leverage retrieved information across reasoning steps. This reliance on implicit reasoning creates fundamental challenges for managing dependencies between sub-questions, efficiently reusing previously retrieved knowledge, and learning optimal search strategies through reinforcement learning. To address these limitations, we propose Dep-Search, a dependency-aware search framework that advances beyond existing search frameworks by integrating structured reasoning, retrieval, and persistent memory through GRPO. Dep-Search introduces explicit control mechanisms that enable the model to decompose questions with dependency relationships, retrieve information when needed, access previously stored knowledge from memory, and summarize long reasoning contexts into reusable memory entries. Through extensive experiments on seven diverse question answering datasets, we demonstrate that Dep-Search significantly enhances LLMs&#39; ability to tackle complex multi-hop reasoning tasks, achieving substantial improvements over strong baselines across different model scales."
    },
    {
      "title": "HomoFM: Deep Homography Estimation with Flow Matching",
      "url": "https://arxiv.org/abs/2601.18222",
      "date": "2026-01-26",
      "authors_text": "Mengfan He, Liangzheng Sun, Chunyu Li, Ziyang Meng",
      "is_highlight": false,
      "score": 55.0,
      "summary": "HomoFM introduces a flow matching approach for deep homography estimation, enhancing accuracy and robustness by modeling velocity fields and employing domain adaptation techniques.",
      "teaser_image": null,
      "debug_abstract": "Deep homography estimation has broad applications in computer vision and robotics. Remarkable progresses have been achieved while the existing methods typically treat it as a direct regression or iterative refinement problem and often struggling to capture complex geometric transformations or generalize across different domains. In this work, we propose HomoFM, a new framework that introduces the flow matching technique from generative modeling into the homography estimation task for the first time. Unlike the existing methods, we formulate homography estimation problem as a velocity field learning problem. By modeling a continuous and point-wise velocity field that transforms noisy distributions into registered coordinates, the proposed network recovers high-precision transformations through a conditional flow trajectory. Furthermore, to address the challenge of domain shifts issue, e.g., the cases of multimodal matching or varying illumination scenarios, we integrate a gradient reversal layer (GRL) into the feature extraction backbone. This domain adaptation strategy explicitly constrains the encoder to learn domain-invariant representations, significantly enhancing the network&#39;s robustness. Extensive experiments demonstrate the effectiveness of the proposed method, showing that HomoFM outperforms state-of-the-art methods in both estimation accuracy and robustness on standard benchmarks. Code and data resource are available at this https URL."
    },
    {
      "title": "AnchoredDream: Zero-Shot 360° Indoor Scene Generation from a Single View via Geometric Grounding",
      "url": "https://arxiv.org/abs/2601.16532",
      "date": "2026-01-26",
      "authors_text": "Runmao Yao, Junsheng Zhou, Zhen Dong, Yu-Shen Liu",
      "is_highlight": false,
      "score": 45.0,
      "summary": "AnchoredDream introduces a zero-shot method for generating 360° indoor scenes from a single image, enhancing appearance consistency and geometric plausibility through innovative geometry anchoring techniques.",
      "teaser_image": null,
      "debug_abstract": "Single-view indoor scene generation plays a crucial role in a range of real-world applications. However, generating a complete 360° scene from a single image remains a highly ill-posed and challenging problem. Recent approaches have made progress by leveraging diffusion models and depth estimation networks, yet they still struggle to maintain appearance consistency and geometric plausibility under large viewpoint changes, limiting their effectiveness in full-scene generation. To address this, we propose AnchoredDream, a novel zero-shot pipeline that anchors 360° scene generation on high-fidelity geometry via an appearance-geometry mutual boosting mechanism. Given a single-view image, our method first performs appearance-guided geometry generation to construct a reliable 3D scene layout. Then, we progressively generate the complete scene through a series of modules: warp-and-inpaint, warp-and-refine, post-optimization, and a novel Grouting Block, which ensures seamless transitions between the input view and generated regions. Extensive experiments demonstrate that AnchoredDream outperforms existing methods by a large margin in both appearance consistency and geometric plausibility--all in a zero-shot manner. Our results highlight the potential of geometric grounding for high-quality, zero-shot single-view scene generation."
    },
    {
      "title": "The Flexibility Trap: Why Arbitrary Order Limits Reasoning Potential in Diffusion Language Models",
      "url": "https://arxiv.org/abs/2601.15165",
      "date": "2026-01-26",
      "authors_text": "Zanlin Ni, Shenzhi Wang, Yang Yue, Tianyu Yu, Weilin Zhao",
      "is_highlight": false,
      "score": 40.0,
      "summary": "This paper argues that the arbitrary order generation in diffusion language models constrains reasoning potential, advocating for a simpler approach, JustGRPO, to enhance performance.",
      "teaser_image": null,
      "debug_abstract": "Diffusion Large Language Models (dLLMs) break the rigid left-to-right constraint of traditional LLMs, enabling token generation in arbitrary orders. Intuitively, this flexibility implies a solution space that strictly supersets the fixed autoregressive trajectory, theoretically unlocking superior reasoning potential for general tasks like mathematics and coding. Consequently, numerous works have leveraged reinforcement learning (RL) to elicit the reasoning capability of dLLMs. In this paper, we reveal a counter-intuitive reality: arbitrary order generation, in its current form, narrows rather than expands the reasoning boundary of dLLMs. We find that dLLMs tend to exploit this order flexibility to bypass high-uncertainty tokens that are crucial for exploration, leading to a premature collapse of the solution space. This observation motivates a rethink of RL approaches for dLLMs, where considerable complexities, such as handling combinatorial trajectories and intractable likelihoods, are often devoted to preserving this flexibility. We demonstrate that effective reasoning can be better elicited by intentionally forgoing arbitrary order and applying standard Group Relative Policy Optimization (GRPO) instead. Our approach, JustGRPO, is minimalist yet surprisingly effective (e.g., 89.1% accuracy on GSM8K) while fully retaining the parallel decoding ability of dLLMs. Project page: this https URL"
    },
    {
      "title": "MetaWorld: Skill Transfer and Composition in a Hierarchical World Model for Grounding High-Level Instructions",
      "url": "https://arxiv.org/abs/2601.17507",
      "date": "2026-01-24",
      "authors_text": "Yutong Shen, Hangxu Liu, Kailin Pei, Ruizhe Xia, Tongtong Feng",
      "is_highlight": false,
      "score": 85.0,
      "summary": "MetaWorld introduces a hierarchical model that enhances humanoid robot loco-manipulation by integrating semantic planning and physical control, improving task completion and motion coherence.",
      "teaser_image": null,
      "debug_abstract": "Humanoid robot loco-manipulation remains constrained by the semantic-physical gap. Current methods face three limitations: Low sample efficiency in reinforcement learning, poor generalization in imitation learning, and physical inconsistency in VLMs. We propose MetaWorld, a hierarchical world model that integrates semantic planning and physical control via expert policy transfer. The framework decouples tasks into a VLM-driven semantic layer and a latent dynamics model operating in a compact state space. Our dynamic expert selection and motion prior fusion mechanism leverages a pre-trained multi-expert policy library as transferable knowledge, enabling efficient online adaptation via a two-stage framework. VLMs serve as semantic interfaces, mapping instructions to executable skills and bypassing symbol grounding. Experiments on Humanoid-Bench show MetaWorld outperforms world model-based RL in task completion and motion coherence. Our code will be found at this https URL"
    },
    {
      "title": "Latent-Space Contrastive Reinforcement Learning for Stable and Efficient LLM Reasoning",
      "url": "https://arxiv.org/abs/2601.17275",
      "date": "2026-01-24",
      "authors_text": "Lianlei Shan, Han Chen, Yixuan Wang, Zhenjie Liu, Wei Li",
      "is_highlight": false,
      "score": 60.0,
      "summary": "The paper introduces DeepLatent Reasoning (DLR), a contrastive reinforcement learning framework that enhances LLM reasoning by optimizing latent space exploration while preventing catastrophic forgetting.",
      "teaser_image": null,
      "debug_abstract": "While Large Language Models (LLMs) demonstrate exceptional performance in surface-level text generation, their nature in handling complex multi-step reasoning tasks often remains one of ``statistical fitting&#39;&#39; rather than systematic logical deduction. Traditional Reinforcement Learning (RL) attempts to mitigate this by introducing a ``think-before-speak&#39;&#39; paradigm. However, applying RL directly in high-dimensional, discrete token spaces faces three inherent challenges: sample-inefficient rollouts, high gradient estimation variance, and the risk of catastrophic forgetting. To fundamentally address these structural bottlenecks, we propose \\textbf{DeepLatent Reasoning (DLR)}, a latent-space bidirectional contrastive reinforcement learning framework. This framework shifts the trial-and-error cost from expensive token-level full sequence generation to the continuous latent manifold. Specifically, we introduce a lightweight assistant model to efficiently sample $K$ reasoning chain encodings within the latent space. These encodings are filtered via a dual reward mechanism based on correctness and formatting; only high-value latent trajectories are fed into a \\textbf{frozen main model} for single-pass decoding. To maximize reasoning diversity while maintaining coherence, we design a contrastive learning objective to enable directed exploration within the latent space. Since the main model parameters remain frozen during optimization, this method mathematically eliminates catastrophic forgetting. Experiments demonstrate that under comparable GPU computational budgets, DLR achieves more stable training convergence, supports longer-horizon reasoning chains, and facilitates the sustainable accumulation of reasoning capabilities, providing a viable path toward reliable and scalable reinforcement learning for LLMs."
    }
  ],
  "具身视觉与机器人实验室 (EVAR Lab)": [
    {
      "title": "Dep-Search: Learning Dependency-Aware Reasoning Traces with Persistent Memory",
      "url": "https://arxiv.org/abs/2601.18771",
      "date": "2026-01-26",
      "authors_text": "Yanming Liu, Xinyue Peng, Zixuan Yan, Yanxin Shen, Wenjie Xu",
      "is_highlight": false,
      "score": 30.0,
      "summary": "Dep-Search is a dependency-aware framework that enhances LLMs' multi-hop reasoning by integrating structured reasoning, retrieval, and persistent memory, outperforming existing methods.",
      "teaser_image": null,
      "debug_abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in complex reasoning tasks, particularly when augmented with search mechanisms that enable systematic exploration of external knowledge bases. The field has evolved from traditional retrieval-augmented generation (RAG) frameworks to more sophisticated search-based frameworks that orchestrate multi-step reasoning through explicit search strategies. However, existing search frameworks still rely heavily on implicit natural language reasoning to determine search strategies and how to leverage retrieved information across reasoning steps. This reliance on implicit reasoning creates fundamental challenges for managing dependencies between sub-questions, efficiently reusing previously retrieved knowledge, and learning optimal search strategies through reinforcement learning. To address these limitations, we propose Dep-Search, a dependency-aware search framework that advances beyond existing search frameworks by integrating structured reasoning, retrieval, and persistent memory through GRPO. Dep-Search introduces explicit control mechanisms that enable the model to decompose questions with dependency relationships, retrieve information when needed, access previously stored knowledge from memory, and summarize long reasoning contexts into reusable memory entries. Through extensive experiments on seven diverse question answering datasets, we demonstrate that Dep-Search significantly enhances LLMs&#39; ability to tackle complex multi-hop reasoning tasks, achieving substantial improvements over strong baselines across different model scales."
    },
    {
      "title": "HomoFM: Deep Homography Estimation with Flow Matching",
      "url": "https://arxiv.org/abs/2601.18222",
      "date": "2026-01-26",
      "authors_text": "Mengfan He, Liangzheng Sun, Chunyu Li, Ziyang Meng",
      "is_highlight": false,
      "score": 55.0,
      "summary": "HomoFM introduces a flow matching approach for deep homography estimation, enhancing accuracy and robustness by modeling velocity fields and employing domain adaptation techniques.",
      "teaser_image": null,
      "debug_abstract": "Deep homography estimation has broad applications in computer vision and robotics. Remarkable progresses have been achieved while the existing methods typically treat it as a direct regression or iterative refinement problem and often struggling to capture complex geometric transformations or generalize across different domains. In this work, we propose HomoFM, a new framework that introduces the flow matching technique from generative modeling into the homography estimation task for the first time. Unlike the existing methods, we formulate homography estimation problem as a velocity field learning problem. By modeling a continuous and point-wise velocity field that transforms noisy distributions into registered coordinates, the proposed network recovers high-precision transformations through a conditional flow trajectory. Furthermore, to address the challenge of domain shifts issue, e.g., the cases of multimodal matching or varying illumination scenarios, we integrate a gradient reversal layer (GRL) into the feature extraction backbone. This domain adaptation strategy explicitly constrains the encoder to learn domain-invariant representations, significantly enhancing the network&#39;s robustness. Extensive experiments demonstrate the effectiveness of the proposed method, showing that HomoFM outperforms state-of-the-art methods in both estimation accuracy and robustness on standard benchmarks. Code and data resource are available at this https URL."
    },
    {
      "title": "AnchoredDream: Zero-Shot 360° Indoor Scene Generation from a Single View via Geometric Grounding",
      "url": "https://arxiv.org/abs/2601.16532",
      "date": "2026-01-26",
      "authors_text": "Runmao Yao, Junsheng Zhou, Zhen Dong, Yu-Shen Liu",
      "is_highlight": false,
      "score": 45.0,
      "summary": "AnchoredDream introduces a zero-shot method for generating 360° indoor scenes from a single image, enhancing appearance consistency and geometric plausibility through innovative geometry anchoring techniques.",
      "teaser_image": null,
      "debug_abstract": "Single-view indoor scene generation plays a crucial role in a range of real-world applications. However, generating a complete 360° scene from a single image remains a highly ill-posed and challenging problem. Recent approaches have made progress by leveraging diffusion models and depth estimation networks, yet they still struggle to maintain appearance consistency and geometric plausibility under large viewpoint changes, limiting their effectiveness in full-scene generation. To address this, we propose AnchoredDream, a novel zero-shot pipeline that anchors 360° scene generation on high-fidelity geometry via an appearance-geometry mutual boosting mechanism. Given a single-view image, our method first performs appearance-guided geometry generation to construct a reliable 3D scene layout. Then, we progressively generate the complete scene through a series of modules: warp-and-inpaint, warp-and-refine, post-optimization, and a novel Grouting Block, which ensures seamless transitions between the input view and generated regions. Extensive experiments demonstrate that AnchoredDream outperforms existing methods by a large margin in both appearance consistency and geometric plausibility--all in a zero-shot manner. Our results highlight the potential of geometric grounding for high-quality, zero-shot single-view scene generation."
    },
    {
      "title": "The Flexibility Trap: Why Arbitrary Order Limits Reasoning Potential in Diffusion Language Models",
      "url": "https://arxiv.org/abs/2601.15165",
      "date": "2026-01-26",
      "authors_text": "Zanlin Ni, Shenzhi Wang, Yang Yue, Tianyu Yu, Weilin Zhao",
      "is_highlight": false,
      "score": 40.0,
      "summary": "This paper argues that the arbitrary order generation in diffusion language models constrains reasoning potential, advocating for a simpler approach, JustGRPO, to enhance performance.",
      "teaser_image": null,
      "debug_abstract": "Diffusion Large Language Models (dLLMs) break the rigid left-to-right constraint of traditional LLMs, enabling token generation in arbitrary orders. Intuitively, this flexibility implies a solution space that strictly supersets the fixed autoregressive trajectory, theoretically unlocking superior reasoning potential for general tasks like mathematics and coding. Consequently, numerous works have leveraged reinforcement learning (RL) to elicit the reasoning capability of dLLMs. In this paper, we reveal a counter-intuitive reality: arbitrary order generation, in its current form, narrows rather than expands the reasoning boundary of dLLMs. We find that dLLMs tend to exploit this order flexibility to bypass high-uncertainty tokens that are crucial for exploration, leading to a premature collapse of the solution space. This observation motivates a rethink of RL approaches for dLLMs, where considerable complexities, such as handling combinatorial trajectories and intractable likelihoods, are often devoted to preserving this flexibility. We demonstrate that effective reasoning can be better elicited by intentionally forgoing arbitrary order and applying standard Group Relative Policy Optimization (GRPO) instead. Our approach, JustGRPO, is minimalist yet surprisingly effective (e.g., 89.1% accuracy on GSM8K) while fully retaining the parallel decoding ability of dLLMs. Project page: this https URL"
    },
    {
      "title": "MetaWorld: Skill Transfer and Composition in a Hierarchical World Model for Grounding High-Level Instructions",
      "url": "https://arxiv.org/abs/2601.17507",
      "date": "2026-01-24",
      "authors_text": "Yutong Shen, Hangxu Liu, Kailin Pei, Ruizhe Xia, Tongtong Feng",
      "is_highlight": false,
      "score": 85.0,
      "summary": "MetaWorld introduces a hierarchical model that enhances humanoid robot loco-manipulation by integrating semantic planning and physical control, improving task completion and motion coherence.",
      "teaser_image": null,
      "debug_abstract": "Humanoid robot loco-manipulation remains constrained by the semantic-physical gap. Current methods face three limitations: Low sample efficiency in reinforcement learning, poor generalization in imitation learning, and physical inconsistency in VLMs. We propose MetaWorld, a hierarchical world model that integrates semantic planning and physical control via expert policy transfer. The framework decouples tasks into a VLM-driven semantic layer and a latent dynamics model operating in a compact state space. Our dynamic expert selection and motion prior fusion mechanism leverages a pre-trained multi-expert policy library as transferable knowledge, enabling efficient online adaptation via a two-stage framework. VLMs serve as semantic interfaces, mapping instructions to executable skills and bypassing symbol grounding. Experiments on Humanoid-Bench show MetaWorld outperforms world model-based RL in task completion and motion coherence. Our code will be found at this https URL"
    },
    {
      "title": "Latent-Space Contrastive Reinforcement Learning for Stable and Efficient LLM Reasoning",
      "url": "https://arxiv.org/abs/2601.17275",
      "date": "2026-01-24",
      "authors_text": "Lianlei Shan, Han Chen, Yixuan Wang, Zhenjie Liu, Wei Li",
      "is_highlight": false,
      "score": 60.0,
      "summary": "The paper introduces DeepLatent Reasoning (DLR), a contrastive reinforcement learning framework that enhances LLM reasoning by optimizing latent space exploration while preventing catastrophic forgetting.",
      "teaser_image": null,
      "debug_abstract": "While Large Language Models (LLMs) demonstrate exceptional performance in surface-level text generation, their nature in handling complex multi-step reasoning tasks often remains one of ``statistical fitting&#39;&#39; rather than systematic logical deduction. Traditional Reinforcement Learning (RL) attempts to mitigate this by introducing a ``think-before-speak&#39;&#39; paradigm. However, applying RL directly in high-dimensional, discrete token spaces faces three inherent challenges: sample-inefficient rollouts, high gradient estimation variance, and the risk of catastrophic forgetting. To fundamentally address these structural bottlenecks, we propose \\textbf{DeepLatent Reasoning (DLR)}, a latent-space bidirectional contrastive reinforcement learning framework. This framework shifts the trial-and-error cost from expensive token-level full sequence generation to the continuous latent manifold. Specifically, we introduce a lightweight assistant model to efficiently sample $K$ reasoning chain encodings within the latent space. These encodings are filtered via a dual reward mechanism based on correctness and formatting; only high-value latent trajectories are fed into a \\textbf{frozen main model} for single-pass decoding. To maximize reasoning diversity while maintaining coherence, we design a contrastive learning objective to enable directed exploration within the latent space. Since the main model parameters remain frozen during optimization, this method mathematically eliminates catastrophic forgetting. Experiments demonstrate that under comparable GPU computational budgets, DLR achieves more stable training convergence, supports longer-horizon reasoning chains, and facilitates the sustainable accumulation of reasoning capabilities, providing a viable path toward reliable and scalable reinforcement learning for LLMs."
    }
  ],
  "机器人控制实验室": [
    {
      "title": "Dep-Search: Learning Dependency-Aware Reasoning Traces with Persistent Memory",
      "url": "https://arxiv.org/abs/2601.18771",
      "date": "2026-01-26",
      "authors_text": "Yanming Liu, Xinyue Peng, Zixuan Yan, Yanxin Shen, Wenjie Xu",
      "is_highlight": false,
      "score": 30.0,
      "summary": "Dep-Search is a dependency-aware framework that enhances LLMs' multi-hop reasoning by integrating structured reasoning, retrieval, and persistent memory, outperforming existing methods.",
      "teaser_image": null,
      "debug_abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in complex reasoning tasks, particularly when augmented with search mechanisms that enable systematic exploration of external knowledge bases. The field has evolved from traditional retrieval-augmented generation (RAG) frameworks to more sophisticated search-based frameworks that orchestrate multi-step reasoning through explicit search strategies. However, existing search frameworks still rely heavily on implicit natural language reasoning to determine search strategies and how to leverage retrieved information across reasoning steps. This reliance on implicit reasoning creates fundamental challenges for managing dependencies between sub-questions, efficiently reusing previously retrieved knowledge, and learning optimal search strategies through reinforcement learning. To address these limitations, we propose Dep-Search, a dependency-aware search framework that advances beyond existing search frameworks by integrating structured reasoning, retrieval, and persistent memory through GRPO. Dep-Search introduces explicit control mechanisms that enable the model to decompose questions with dependency relationships, retrieve information when needed, access previously stored knowledge from memory, and summarize long reasoning contexts into reusable memory entries. Through extensive experiments on seven diverse question answering datasets, we demonstrate that Dep-Search significantly enhances LLMs&#39; ability to tackle complex multi-hop reasoning tasks, achieving substantial improvements over strong baselines across different model scales."
    },
    {
      "title": "HomoFM: Deep Homography Estimation with Flow Matching",
      "url": "https://arxiv.org/abs/2601.18222",
      "date": "2026-01-26",
      "authors_text": "Mengfan He, Liangzheng Sun, Chunyu Li, Ziyang Meng",
      "is_highlight": false,
      "score": 55.0,
      "summary": "HomoFM introduces a flow matching approach for deep homography estimation, enhancing accuracy and robustness by modeling velocity fields and employing domain adaptation techniques.",
      "teaser_image": null,
      "debug_abstract": "Deep homography estimation has broad applications in computer vision and robotics. Remarkable progresses have been achieved while the existing methods typically treat it as a direct regression or iterative refinement problem and often struggling to capture complex geometric transformations or generalize across different domains. In this work, we propose HomoFM, a new framework that introduces the flow matching technique from generative modeling into the homography estimation task for the first time. Unlike the existing methods, we formulate homography estimation problem as a velocity field learning problem. By modeling a continuous and point-wise velocity field that transforms noisy distributions into registered coordinates, the proposed network recovers high-precision transformations through a conditional flow trajectory. Furthermore, to address the challenge of domain shifts issue, e.g., the cases of multimodal matching or varying illumination scenarios, we integrate a gradient reversal layer (GRL) into the feature extraction backbone. This domain adaptation strategy explicitly constrains the encoder to learn domain-invariant representations, significantly enhancing the network&#39;s robustness. Extensive experiments demonstrate the effectiveness of the proposed method, showing that HomoFM outperforms state-of-the-art methods in both estimation accuracy and robustness on standard benchmarks. Code and data resource are available at this https URL."
    },
    {
      "title": "AnchoredDream: Zero-Shot 360° Indoor Scene Generation from a Single View via Geometric Grounding",
      "url": "https://arxiv.org/abs/2601.16532",
      "date": "2026-01-26",
      "authors_text": "Runmao Yao, Junsheng Zhou, Zhen Dong, Yu-Shen Liu",
      "is_highlight": false,
      "score": 45.0,
      "summary": "AnchoredDream introduces a zero-shot method for generating 360° indoor scenes from a single image, enhancing appearance consistency and geometric plausibility through innovative geometry anchoring techniques.",
      "teaser_image": null,
      "debug_abstract": "Single-view indoor scene generation plays a crucial role in a range of real-world applications. However, generating a complete 360° scene from a single image remains a highly ill-posed and challenging problem. Recent approaches have made progress by leveraging diffusion models and depth estimation networks, yet they still struggle to maintain appearance consistency and geometric plausibility under large viewpoint changes, limiting their effectiveness in full-scene generation. To address this, we propose AnchoredDream, a novel zero-shot pipeline that anchors 360° scene generation on high-fidelity geometry via an appearance-geometry mutual boosting mechanism. Given a single-view image, our method first performs appearance-guided geometry generation to construct a reliable 3D scene layout. Then, we progressively generate the complete scene through a series of modules: warp-and-inpaint, warp-and-refine, post-optimization, and a novel Grouting Block, which ensures seamless transitions between the input view and generated regions. Extensive experiments demonstrate that AnchoredDream outperforms existing methods by a large margin in both appearance consistency and geometric plausibility--all in a zero-shot manner. Our results highlight the potential of geometric grounding for high-quality, zero-shot single-view scene generation."
    },
    {
      "title": "The Flexibility Trap: Why Arbitrary Order Limits Reasoning Potential in Diffusion Language Models",
      "url": "https://arxiv.org/abs/2601.15165",
      "date": "2026-01-26",
      "authors_text": "Zanlin Ni, Shenzhi Wang, Yang Yue, Tianyu Yu, Weilin Zhao",
      "is_highlight": false,
      "score": 40.0,
      "summary": "This paper argues that the arbitrary order generation in diffusion language models constrains reasoning potential, advocating for a simpler approach, JustGRPO, to enhance performance.",
      "teaser_image": null,
      "debug_abstract": "Diffusion Large Language Models (dLLMs) break the rigid left-to-right constraint of traditional LLMs, enabling token generation in arbitrary orders. Intuitively, this flexibility implies a solution space that strictly supersets the fixed autoregressive trajectory, theoretically unlocking superior reasoning potential for general tasks like mathematics and coding. Consequently, numerous works have leveraged reinforcement learning (RL) to elicit the reasoning capability of dLLMs. In this paper, we reveal a counter-intuitive reality: arbitrary order generation, in its current form, narrows rather than expands the reasoning boundary of dLLMs. We find that dLLMs tend to exploit this order flexibility to bypass high-uncertainty tokens that are crucial for exploration, leading to a premature collapse of the solution space. This observation motivates a rethink of RL approaches for dLLMs, where considerable complexities, such as handling combinatorial trajectories and intractable likelihoods, are often devoted to preserving this flexibility. We demonstrate that effective reasoning can be better elicited by intentionally forgoing arbitrary order and applying standard Group Relative Policy Optimization (GRPO) instead. Our approach, JustGRPO, is minimalist yet surprisingly effective (e.g., 89.1% accuracy on GSM8K) while fully retaining the parallel decoding ability of dLLMs. Project page: this https URL"
    },
    {
      "title": "MetaWorld: Skill Transfer and Composition in a Hierarchical World Model for Grounding High-Level Instructions",
      "url": "https://arxiv.org/abs/2601.17507",
      "date": "2026-01-24",
      "authors_text": "Yutong Shen, Hangxu Liu, Kailin Pei, Ruizhe Xia, Tongtong Feng",
      "is_highlight": false,
      "score": 85.0,
      "summary": "MetaWorld introduces a hierarchical model that enhances humanoid robot loco-manipulation by integrating semantic planning and physical control, improving task completion and motion coherence.",
      "teaser_image": null,
      "debug_abstract": "Humanoid robot loco-manipulation remains constrained by the semantic-physical gap. Current methods face three limitations: Low sample efficiency in reinforcement learning, poor generalization in imitation learning, and physical inconsistency in VLMs. We propose MetaWorld, a hierarchical world model that integrates semantic planning and physical control via expert policy transfer. The framework decouples tasks into a VLM-driven semantic layer and a latent dynamics model operating in a compact state space. Our dynamic expert selection and motion prior fusion mechanism leverages a pre-trained multi-expert policy library as transferable knowledge, enabling efficient online adaptation via a two-stage framework. VLMs serve as semantic interfaces, mapping instructions to executable skills and bypassing symbol grounding. Experiments on Humanoid-Bench show MetaWorld outperforms world model-based RL in task completion and motion coherence. Our code will be found at this https URL"
    },
    {
      "title": "Latent-Space Contrastive Reinforcement Learning for Stable and Efficient LLM Reasoning",
      "url": "https://arxiv.org/abs/2601.17275",
      "date": "2026-01-24",
      "authors_text": "Lianlei Shan, Han Chen, Yixuan Wang, Zhenjie Liu, Wei Li",
      "is_highlight": false,
      "score": 60.0,
      "summary": "The paper introduces DeepLatent Reasoning (DLR), a contrastive reinforcement learning framework that enhances LLM reasoning by optimizing latent space exploration while preventing catastrophic forgetting.",
      "teaser_image": null,
      "debug_abstract": "While Large Language Models (LLMs) demonstrate exceptional performance in surface-level text generation, their nature in handling complex multi-step reasoning tasks often remains one of ``statistical fitting&#39;&#39; rather than systematic logical deduction. Traditional Reinforcement Learning (RL) attempts to mitigate this by introducing a ``think-before-speak&#39;&#39; paradigm. However, applying RL directly in high-dimensional, discrete token spaces faces three inherent challenges: sample-inefficient rollouts, high gradient estimation variance, and the risk of catastrophic forgetting. To fundamentally address these structural bottlenecks, we propose \\textbf{DeepLatent Reasoning (DLR)}, a latent-space bidirectional contrastive reinforcement learning framework. This framework shifts the trial-and-error cost from expensive token-level full sequence generation to the continuous latent manifold. Specifically, we introduce a lightweight assistant model to efficiently sample $K$ reasoning chain encodings within the latent space. These encodings are filtered via a dual reward mechanism based on correctness and formatting; only high-value latent trajectories are fed into a \\textbf{frozen main model} for single-pass decoding. To maximize reasoning diversity while maintaining coherence, we design a contrastive learning objective to enable directed exploration within the latent space. Since the main model parameters remain frozen during optimization, this method mathematically eliminates catastrophic forgetting. Experiments demonstrate that under comparable GPU computational budgets, DLR achieves more stable training convergence, supports longer-horizon reasoning chains, and facilitates the sustainable accumulation of reasoning capabilities, providing a viable path toward reliable and scalable reinforcement learning for LLMs."
    }
  ],
  "智能系统与机器人实验室 (ISR Lab)": [
    {
      "title": "Dep-Search: Learning Dependency-Aware Reasoning Traces with Persistent Memory",
      "url": "https://arxiv.org/abs/2601.18771",
      "date": "2026-01-26",
      "authors_text": "Yanming Liu, Xinyue Peng, Zixuan Yan, Yanxin Shen, Wenjie Xu",
      "is_highlight": false,
      "score": 30.0,
      "summary": "Dep-Search is a dependency-aware framework that enhances LLMs' multi-hop reasoning by integrating structured reasoning, retrieval, and persistent memory, outperforming existing methods.",
      "teaser_image": null,
      "debug_abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in complex reasoning tasks, particularly when augmented with search mechanisms that enable systematic exploration of external knowledge bases. The field has evolved from traditional retrieval-augmented generation (RAG) frameworks to more sophisticated search-based frameworks that orchestrate multi-step reasoning through explicit search strategies. However, existing search frameworks still rely heavily on implicit natural language reasoning to determine search strategies and how to leverage retrieved information across reasoning steps. This reliance on implicit reasoning creates fundamental challenges for managing dependencies between sub-questions, efficiently reusing previously retrieved knowledge, and learning optimal search strategies through reinforcement learning. To address these limitations, we propose Dep-Search, a dependency-aware search framework that advances beyond existing search frameworks by integrating structured reasoning, retrieval, and persistent memory through GRPO. Dep-Search introduces explicit control mechanisms that enable the model to decompose questions with dependency relationships, retrieve information when needed, access previously stored knowledge from memory, and summarize long reasoning contexts into reusable memory entries. Through extensive experiments on seven diverse question answering datasets, we demonstrate that Dep-Search significantly enhances LLMs&#39; ability to tackle complex multi-hop reasoning tasks, achieving substantial improvements over strong baselines across different model scales."
    },
    {
      "title": "HomoFM: Deep Homography Estimation with Flow Matching",
      "url": "https://arxiv.org/abs/2601.18222",
      "date": "2026-01-26",
      "authors_text": "Mengfan He, Liangzheng Sun, Chunyu Li, Ziyang Meng",
      "is_highlight": false,
      "score": 55.0,
      "summary": "HomoFM introduces a flow matching approach for deep homography estimation, enhancing accuracy and robustness by modeling velocity fields and employing domain adaptation techniques.",
      "teaser_image": null,
      "debug_abstract": "Deep homography estimation has broad applications in computer vision and robotics. Remarkable progresses have been achieved while the existing methods typically treat it as a direct regression or iterative refinement problem and often struggling to capture complex geometric transformations or generalize across different domains. In this work, we propose HomoFM, a new framework that introduces the flow matching technique from generative modeling into the homography estimation task for the first time. Unlike the existing methods, we formulate homography estimation problem as a velocity field learning problem. By modeling a continuous and point-wise velocity field that transforms noisy distributions into registered coordinates, the proposed network recovers high-precision transformations through a conditional flow trajectory. Furthermore, to address the challenge of domain shifts issue, e.g., the cases of multimodal matching or varying illumination scenarios, we integrate a gradient reversal layer (GRL) into the feature extraction backbone. This domain adaptation strategy explicitly constrains the encoder to learn domain-invariant representations, significantly enhancing the network&#39;s robustness. Extensive experiments demonstrate the effectiveness of the proposed method, showing that HomoFM outperforms state-of-the-art methods in both estimation accuracy and robustness on standard benchmarks. Code and data resource are available at this https URL."
    },
    {
      "title": "AnchoredDream: Zero-Shot 360° Indoor Scene Generation from a Single View via Geometric Grounding",
      "url": "https://arxiv.org/abs/2601.16532",
      "date": "2026-01-26",
      "authors_text": "Runmao Yao, Junsheng Zhou, Zhen Dong, Yu-Shen Liu",
      "is_highlight": false,
      "score": 45.0,
      "summary": "AnchoredDream introduces a zero-shot method for generating 360° indoor scenes from a single image, enhancing appearance consistency and geometric plausibility through innovative geometry anchoring techniques.",
      "teaser_image": null,
      "debug_abstract": "Single-view indoor scene generation plays a crucial role in a range of real-world applications. However, generating a complete 360° scene from a single image remains a highly ill-posed and challenging problem. Recent approaches have made progress by leveraging diffusion models and depth estimation networks, yet they still struggle to maintain appearance consistency and geometric plausibility under large viewpoint changes, limiting their effectiveness in full-scene generation. To address this, we propose AnchoredDream, a novel zero-shot pipeline that anchors 360° scene generation on high-fidelity geometry via an appearance-geometry mutual boosting mechanism. Given a single-view image, our method first performs appearance-guided geometry generation to construct a reliable 3D scene layout. Then, we progressively generate the complete scene through a series of modules: warp-and-inpaint, warp-and-refine, post-optimization, and a novel Grouting Block, which ensures seamless transitions between the input view and generated regions. Extensive experiments demonstrate that AnchoredDream outperforms existing methods by a large margin in both appearance consistency and geometric plausibility--all in a zero-shot manner. Our results highlight the potential of geometric grounding for high-quality, zero-shot single-view scene generation."
    },
    {
      "title": "The Flexibility Trap: Why Arbitrary Order Limits Reasoning Potential in Diffusion Language Models",
      "url": "https://arxiv.org/abs/2601.15165",
      "date": "2026-01-26",
      "authors_text": "Zanlin Ni, Shenzhi Wang, Yang Yue, Tianyu Yu, Weilin Zhao",
      "is_highlight": false,
      "score": 40.0,
      "summary": "This paper argues that the arbitrary order generation in diffusion language models constrains reasoning potential, advocating for a simpler approach, JustGRPO, to enhance performance.",
      "teaser_image": null,
      "debug_abstract": "Diffusion Large Language Models (dLLMs) break the rigid left-to-right constraint of traditional LLMs, enabling token generation in arbitrary orders. Intuitively, this flexibility implies a solution space that strictly supersets the fixed autoregressive trajectory, theoretically unlocking superior reasoning potential for general tasks like mathematics and coding. Consequently, numerous works have leveraged reinforcement learning (RL) to elicit the reasoning capability of dLLMs. In this paper, we reveal a counter-intuitive reality: arbitrary order generation, in its current form, narrows rather than expands the reasoning boundary of dLLMs. We find that dLLMs tend to exploit this order flexibility to bypass high-uncertainty tokens that are crucial for exploration, leading to a premature collapse of the solution space. This observation motivates a rethink of RL approaches for dLLMs, where considerable complexities, such as handling combinatorial trajectories and intractable likelihoods, are often devoted to preserving this flexibility. We demonstrate that effective reasoning can be better elicited by intentionally forgoing arbitrary order and applying standard Group Relative Policy Optimization (GRPO) instead. Our approach, JustGRPO, is minimalist yet surprisingly effective (e.g., 89.1% accuracy on GSM8K) while fully retaining the parallel decoding ability of dLLMs. Project page: this https URL"
    },
    {
      "title": "MetaWorld: Skill Transfer and Composition in a Hierarchical World Model for Grounding High-Level Instructions",
      "url": "https://arxiv.org/abs/2601.17507",
      "date": "2026-01-24",
      "authors_text": "Yutong Shen, Hangxu Liu, Kailin Pei, Ruizhe Xia, Tongtong Feng",
      "is_highlight": false,
      "score": 85.0,
      "summary": "MetaWorld introduces a hierarchical model that enhances humanoid robot loco-manipulation by integrating semantic planning and physical control, improving task completion and motion coherence.",
      "teaser_image": null,
      "debug_abstract": "Humanoid robot loco-manipulation remains constrained by the semantic-physical gap. Current methods face three limitations: Low sample efficiency in reinforcement learning, poor generalization in imitation learning, and physical inconsistency in VLMs. We propose MetaWorld, a hierarchical world model that integrates semantic planning and physical control via expert policy transfer. The framework decouples tasks into a VLM-driven semantic layer and a latent dynamics model operating in a compact state space. Our dynamic expert selection and motion prior fusion mechanism leverages a pre-trained multi-expert policy library as transferable knowledge, enabling efficient online adaptation via a two-stage framework. VLMs serve as semantic interfaces, mapping instructions to executable skills and bypassing symbol grounding. Experiments on Humanoid-Bench show MetaWorld outperforms world model-based RL in task completion and motion coherence. Our code will be found at this https URL"
    },
    {
      "title": "Latent-Space Contrastive Reinforcement Learning for Stable and Efficient LLM Reasoning",
      "url": "https://arxiv.org/abs/2601.17275",
      "date": "2026-01-24",
      "authors_text": "Lianlei Shan, Han Chen, Yixuan Wang, Zhenjie Liu, Wei Li",
      "is_highlight": false,
      "score": 60.0,
      "summary": "The paper introduces DeepLatent Reasoning (DLR), a contrastive reinforcement learning framework that enhances LLM reasoning by optimizing latent space exploration while preventing catastrophic forgetting.",
      "teaser_image": null,
      "debug_abstract": "While Large Language Models (LLMs) demonstrate exceptional performance in surface-level text generation, their nature in handling complex multi-step reasoning tasks often remains one of ``statistical fitting&#39;&#39; rather than systematic logical deduction. Traditional Reinforcement Learning (RL) attempts to mitigate this by introducing a ``think-before-speak&#39;&#39; paradigm. However, applying RL directly in high-dimensional, discrete token spaces faces three inherent challenges: sample-inefficient rollouts, high gradient estimation variance, and the risk of catastrophic forgetting. To fundamentally address these structural bottlenecks, we propose \\textbf{DeepLatent Reasoning (DLR)}, a latent-space bidirectional contrastive reinforcement learning framework. This framework shifts the trial-and-error cost from expensive token-level full sequence generation to the continuous latent manifold. Specifically, we introduce a lightweight assistant model to efficiently sample $K$ reasoning chain encodings within the latent space. These encodings are filtered via a dual reward mechanism based on correctness and formatting; only high-value latent trajectories are fed into a \\textbf{frozen main model} for single-pass decoding. To maximize reasoning diversity while maintaining coherence, we design a contrastive learning objective to enable directed exploration within the latent space. Since the main model parameters remain frozen during optimization, this method mathematically eliminates catastrophic forgetting. Experiments demonstrate that under comparable GPU computational budgets, DLR achieves more stable training convergence, supports longer-horizon reasoning chains, and facilitates the sustainable accumulation of reasoning capabilities, providing a viable path toward reliable and scalable reinforcement learning for LLMs."
    }
  ],
  "智能产业研究院 (AIR)": [
    {
      "title": "Dep-Search: Learning Dependency-Aware Reasoning Traces with Persistent Memory",
      "url": "https://arxiv.org/abs/2601.18771",
      "date": "2026-01-26",
      "authors_text": "Yanming Liu, Xinyue Peng, Zixuan Yan, Yanxin Shen, Wenjie Xu",
      "is_highlight": false,
      "score": 30.0,
      "summary": "Dep-Search is a dependency-aware framework that enhances LLMs' multi-hop reasoning by integrating structured reasoning, retrieval, and persistent memory, outperforming existing methods.",
      "teaser_image": null,
      "debug_abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in complex reasoning tasks, particularly when augmented with search mechanisms that enable systematic exploration of external knowledge bases. The field has evolved from traditional retrieval-augmented generation (RAG) frameworks to more sophisticated search-based frameworks that orchestrate multi-step reasoning through explicit search strategies. However, existing search frameworks still rely heavily on implicit natural language reasoning to determine search strategies and how to leverage retrieved information across reasoning steps. This reliance on implicit reasoning creates fundamental challenges for managing dependencies between sub-questions, efficiently reusing previously retrieved knowledge, and learning optimal search strategies through reinforcement learning. To address these limitations, we propose Dep-Search, a dependency-aware search framework that advances beyond existing search frameworks by integrating structured reasoning, retrieval, and persistent memory through GRPO. Dep-Search introduces explicit control mechanisms that enable the model to decompose questions with dependency relationships, retrieve information when needed, access previously stored knowledge from memory, and summarize long reasoning contexts into reusable memory entries. Through extensive experiments on seven diverse question answering datasets, we demonstrate that Dep-Search significantly enhances LLMs&#39; ability to tackle complex multi-hop reasoning tasks, achieving substantial improvements over strong baselines across different model scales."
    },
    {
      "title": "HomoFM: Deep Homography Estimation with Flow Matching",
      "url": "https://arxiv.org/abs/2601.18222",
      "date": "2026-01-26",
      "authors_text": "Mengfan He, Liangzheng Sun, Chunyu Li, Ziyang Meng",
      "is_highlight": false,
      "score": 55.0,
      "summary": "HomoFM introduces a flow matching approach for deep homography estimation, enhancing accuracy and robustness by modeling velocity fields and employing domain adaptation techniques.",
      "teaser_image": null,
      "debug_abstract": "Deep homography estimation has broad applications in computer vision and robotics. Remarkable progresses have been achieved while the existing methods typically treat it as a direct regression or iterative refinement problem and often struggling to capture complex geometric transformations or generalize across different domains. In this work, we propose HomoFM, a new framework that introduces the flow matching technique from generative modeling into the homography estimation task for the first time. Unlike the existing methods, we formulate homography estimation problem as a velocity field learning problem. By modeling a continuous and point-wise velocity field that transforms noisy distributions into registered coordinates, the proposed network recovers high-precision transformations through a conditional flow trajectory. Furthermore, to address the challenge of domain shifts issue, e.g., the cases of multimodal matching or varying illumination scenarios, we integrate a gradient reversal layer (GRL) into the feature extraction backbone. This domain adaptation strategy explicitly constrains the encoder to learn domain-invariant representations, significantly enhancing the network&#39;s robustness. Extensive experiments demonstrate the effectiveness of the proposed method, showing that HomoFM outperforms state-of-the-art methods in both estimation accuracy and robustness on standard benchmarks. Code and data resource are available at this https URL."
    },
    {
      "title": "AnchoredDream: Zero-Shot 360° Indoor Scene Generation from a Single View via Geometric Grounding",
      "url": "https://arxiv.org/abs/2601.16532",
      "date": "2026-01-26",
      "authors_text": "Runmao Yao, Junsheng Zhou, Zhen Dong, Yu-Shen Liu",
      "is_highlight": false,
      "score": 45.0,
      "summary": "AnchoredDream introduces a zero-shot method for generating 360° indoor scenes from a single image, enhancing appearance consistency and geometric plausibility through innovative geometry anchoring techniques.",
      "teaser_image": null,
      "debug_abstract": "Single-view indoor scene generation plays a crucial role in a range of real-world applications. However, generating a complete 360° scene from a single image remains a highly ill-posed and challenging problem. Recent approaches have made progress by leveraging diffusion models and depth estimation networks, yet they still struggle to maintain appearance consistency and geometric plausibility under large viewpoint changes, limiting their effectiveness in full-scene generation. To address this, we propose AnchoredDream, a novel zero-shot pipeline that anchors 360° scene generation on high-fidelity geometry via an appearance-geometry mutual boosting mechanism. Given a single-view image, our method first performs appearance-guided geometry generation to construct a reliable 3D scene layout. Then, we progressively generate the complete scene through a series of modules: warp-and-inpaint, warp-and-refine, post-optimization, and a novel Grouting Block, which ensures seamless transitions between the input view and generated regions. Extensive experiments demonstrate that AnchoredDream outperforms existing methods by a large margin in both appearance consistency and geometric plausibility--all in a zero-shot manner. Our results highlight the potential of geometric grounding for high-quality, zero-shot single-view scene generation."
    },
    {
      "title": "The Flexibility Trap: Why Arbitrary Order Limits Reasoning Potential in Diffusion Language Models",
      "url": "https://arxiv.org/abs/2601.15165",
      "date": "2026-01-26",
      "authors_text": "Zanlin Ni, Shenzhi Wang, Yang Yue, Tianyu Yu, Weilin Zhao",
      "is_highlight": false,
      "score": 40.0,
      "summary": "This paper argues that the arbitrary order generation in diffusion language models constrains reasoning potential, advocating for a simpler approach, JustGRPO, to enhance performance.",
      "teaser_image": null,
      "debug_abstract": "Diffusion Large Language Models (dLLMs) break the rigid left-to-right constraint of traditional LLMs, enabling token generation in arbitrary orders. Intuitively, this flexibility implies a solution space that strictly supersets the fixed autoregressive trajectory, theoretically unlocking superior reasoning potential for general tasks like mathematics and coding. Consequently, numerous works have leveraged reinforcement learning (RL) to elicit the reasoning capability of dLLMs. In this paper, we reveal a counter-intuitive reality: arbitrary order generation, in its current form, narrows rather than expands the reasoning boundary of dLLMs. We find that dLLMs tend to exploit this order flexibility to bypass high-uncertainty tokens that are crucial for exploration, leading to a premature collapse of the solution space. This observation motivates a rethink of RL approaches for dLLMs, where considerable complexities, such as handling combinatorial trajectories and intractable likelihoods, are often devoted to preserving this flexibility. We demonstrate that effective reasoning can be better elicited by intentionally forgoing arbitrary order and applying standard Group Relative Policy Optimization (GRPO) instead. Our approach, JustGRPO, is minimalist yet surprisingly effective (e.g., 89.1% accuracy on GSM8K) while fully retaining the parallel decoding ability of dLLMs. Project page: this https URL"
    },
    {
      "title": "MetaWorld: Skill Transfer and Composition in a Hierarchical World Model for Grounding High-Level Instructions",
      "url": "https://arxiv.org/abs/2601.17507",
      "date": "2026-01-24",
      "authors_text": "Yutong Shen, Hangxu Liu, Kailin Pei, Ruizhe Xia, Tongtong Feng",
      "is_highlight": false,
      "score": 85.0,
      "summary": "MetaWorld introduces a hierarchical model that enhances humanoid robot loco-manipulation by integrating semantic planning and physical control, improving task completion and motion coherence.",
      "teaser_image": null,
      "debug_abstract": "Humanoid robot loco-manipulation remains constrained by the semantic-physical gap. Current methods face three limitations: Low sample efficiency in reinforcement learning, poor generalization in imitation learning, and physical inconsistency in VLMs. We propose MetaWorld, a hierarchical world model that integrates semantic planning and physical control via expert policy transfer. The framework decouples tasks into a VLM-driven semantic layer and a latent dynamics model operating in a compact state space. Our dynamic expert selection and motion prior fusion mechanism leverages a pre-trained multi-expert policy library as transferable knowledge, enabling efficient online adaptation via a two-stage framework. VLMs serve as semantic interfaces, mapping instructions to executable skills and bypassing symbol grounding. Experiments on Humanoid-Bench show MetaWorld outperforms world model-based RL in task completion and motion coherence. Our code will be found at this https URL"
    },
    {
      "title": "Latent-Space Contrastive Reinforcement Learning for Stable and Efficient LLM Reasoning",
      "url": "https://arxiv.org/abs/2601.17275",
      "date": "2026-01-24",
      "authors_text": "Lianlei Shan, Han Chen, Yixuan Wang, Zhenjie Liu, Wei Li",
      "is_highlight": false,
      "score": 60.0,
      "summary": "The paper introduces DeepLatent Reasoning (DLR), a contrastive reinforcement learning framework that enhances LLM reasoning by optimizing latent space exploration while preventing catastrophic forgetting.",
      "teaser_image": null,
      "debug_abstract": "While Large Language Models (LLMs) demonstrate exceptional performance in surface-level text generation, their nature in handling complex multi-step reasoning tasks often remains one of ``statistical fitting&#39;&#39; rather than systematic logical deduction. Traditional Reinforcement Learning (RL) attempts to mitigate this by introducing a ``think-before-speak&#39;&#39; paradigm. However, applying RL directly in high-dimensional, discrete token spaces faces three inherent challenges: sample-inefficient rollouts, high gradient estimation variance, and the risk of catastrophic forgetting. To fundamentally address these structural bottlenecks, we propose \\textbf{DeepLatent Reasoning (DLR)}, a latent-space bidirectional contrastive reinforcement learning framework. This framework shifts the trial-and-error cost from expensive token-level full sequence generation to the continuous latent manifold. Specifically, we introduce a lightweight assistant model to efficiently sample $K$ reasoning chain encodings within the latent space. These encodings are filtered via a dual reward mechanism based on correctness and formatting; only high-value latent trajectories are fed into a \\textbf{frozen main model} for single-pass decoding. To maximize reasoning diversity while maintaining coherence, we design a contrastive learning objective to enable directed exploration within the latent space. Since the main model parameters remain frozen during optimization, this method mathematically eliminates catastrophic forgetting. Experiments demonstrate that under comparable GPU computational budgets, DLR achieves more stable training convergence, supports longer-horizon reasoning chains, and facilitates the sustainable accumulation of reasoning capabilities, providing a viable path toward reliable and scalable reinforcement learning for LLMs."
    }
  ],
  "具身智能实验室 (TEA Lab)": [
    {
      "title": "Dep-Search: Learning Dependency-Aware Reasoning Traces with Persistent Memory",
      "url": "https://arxiv.org/abs/2601.18771",
      "date": "2026-01-26",
      "authors_text": "Yanming Liu, Xinyue Peng, Zixuan Yan, Yanxin Shen, Wenjie Xu",
      "is_highlight": false,
      "score": 30.0,
      "summary": "Dep-Search is a dependency-aware framework that enhances LLMs' multi-hop reasoning by integrating structured reasoning, retrieval, and persistent memory, outperforming existing methods.",
      "teaser_image": null,
      "debug_abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in complex reasoning tasks, particularly when augmented with search mechanisms that enable systematic exploration of external knowledge bases. The field has evolved from traditional retrieval-augmented generation (RAG) frameworks to more sophisticated search-based frameworks that orchestrate multi-step reasoning through explicit search strategies. However, existing search frameworks still rely heavily on implicit natural language reasoning to determine search strategies and how to leverage retrieved information across reasoning steps. This reliance on implicit reasoning creates fundamental challenges for managing dependencies between sub-questions, efficiently reusing previously retrieved knowledge, and learning optimal search strategies through reinforcement learning. To address these limitations, we propose Dep-Search, a dependency-aware search framework that advances beyond existing search frameworks by integrating structured reasoning, retrieval, and persistent memory through GRPO. Dep-Search introduces explicit control mechanisms that enable the model to decompose questions with dependency relationships, retrieve information when needed, access previously stored knowledge from memory, and summarize long reasoning contexts into reusable memory entries. Through extensive experiments on seven diverse question answering datasets, we demonstrate that Dep-Search significantly enhances LLMs&#39; ability to tackle complex multi-hop reasoning tasks, achieving substantial improvements over strong baselines across different model scales."
    },
    {
      "title": "HomoFM: Deep Homography Estimation with Flow Matching",
      "url": "https://arxiv.org/abs/2601.18222",
      "date": "2026-01-26",
      "authors_text": "Mengfan He, Liangzheng Sun, Chunyu Li, Ziyang Meng",
      "is_highlight": false,
      "score": 55.0,
      "summary": "HomoFM introduces a flow matching approach for deep homography estimation, enhancing accuracy and robustness by modeling velocity fields and employing domain adaptation techniques.",
      "teaser_image": null,
      "debug_abstract": "Deep homography estimation has broad applications in computer vision and robotics. Remarkable progresses have been achieved while the existing methods typically treat it as a direct regression or iterative refinement problem and often struggling to capture complex geometric transformations or generalize across different domains. In this work, we propose HomoFM, a new framework that introduces the flow matching technique from generative modeling into the homography estimation task for the first time. Unlike the existing methods, we formulate homography estimation problem as a velocity field learning problem. By modeling a continuous and point-wise velocity field that transforms noisy distributions into registered coordinates, the proposed network recovers high-precision transformations through a conditional flow trajectory. Furthermore, to address the challenge of domain shifts issue, e.g., the cases of multimodal matching or varying illumination scenarios, we integrate a gradient reversal layer (GRL) into the feature extraction backbone. This domain adaptation strategy explicitly constrains the encoder to learn domain-invariant representations, significantly enhancing the network&#39;s robustness. Extensive experiments demonstrate the effectiveness of the proposed method, showing that HomoFM outperforms state-of-the-art methods in both estimation accuracy and robustness on standard benchmarks. Code and data resource are available at this https URL."
    },
    {
      "title": "AnchoredDream: Zero-Shot 360° Indoor Scene Generation from a Single View via Geometric Grounding",
      "url": "https://arxiv.org/abs/2601.16532",
      "date": "2026-01-26",
      "authors_text": "Runmao Yao, Junsheng Zhou, Zhen Dong, Yu-Shen Liu",
      "is_highlight": false,
      "score": 45.0,
      "summary": "AnchoredDream introduces a zero-shot method for generating 360° indoor scenes from a single image, enhancing appearance consistency and geometric plausibility through innovative geometry anchoring techniques.",
      "teaser_image": null,
      "debug_abstract": "Single-view indoor scene generation plays a crucial role in a range of real-world applications. However, generating a complete 360° scene from a single image remains a highly ill-posed and challenging problem. Recent approaches have made progress by leveraging diffusion models and depth estimation networks, yet they still struggle to maintain appearance consistency and geometric plausibility under large viewpoint changes, limiting their effectiveness in full-scene generation. To address this, we propose AnchoredDream, a novel zero-shot pipeline that anchors 360° scene generation on high-fidelity geometry via an appearance-geometry mutual boosting mechanism. Given a single-view image, our method first performs appearance-guided geometry generation to construct a reliable 3D scene layout. Then, we progressively generate the complete scene through a series of modules: warp-and-inpaint, warp-and-refine, post-optimization, and a novel Grouting Block, which ensures seamless transitions between the input view and generated regions. Extensive experiments demonstrate that AnchoredDream outperforms existing methods by a large margin in both appearance consistency and geometric plausibility--all in a zero-shot manner. Our results highlight the potential of geometric grounding for high-quality, zero-shot single-view scene generation."
    },
    {
      "title": "The Flexibility Trap: Why Arbitrary Order Limits Reasoning Potential in Diffusion Language Models",
      "url": "https://arxiv.org/abs/2601.15165",
      "date": "2026-01-26",
      "authors_text": "Zanlin Ni, Shenzhi Wang, Yang Yue, Tianyu Yu, Weilin Zhao",
      "is_highlight": false,
      "score": 40.0,
      "summary": "This paper argues that the arbitrary order generation in diffusion language models constrains reasoning potential, advocating for a simpler approach, JustGRPO, to enhance performance.",
      "teaser_image": null,
      "debug_abstract": "Diffusion Large Language Models (dLLMs) break the rigid left-to-right constraint of traditional LLMs, enabling token generation in arbitrary orders. Intuitively, this flexibility implies a solution space that strictly supersets the fixed autoregressive trajectory, theoretically unlocking superior reasoning potential for general tasks like mathematics and coding. Consequently, numerous works have leveraged reinforcement learning (RL) to elicit the reasoning capability of dLLMs. In this paper, we reveal a counter-intuitive reality: arbitrary order generation, in its current form, narrows rather than expands the reasoning boundary of dLLMs. We find that dLLMs tend to exploit this order flexibility to bypass high-uncertainty tokens that are crucial for exploration, leading to a premature collapse of the solution space. This observation motivates a rethink of RL approaches for dLLMs, where considerable complexities, such as handling combinatorial trajectories and intractable likelihoods, are often devoted to preserving this flexibility. We demonstrate that effective reasoning can be better elicited by intentionally forgoing arbitrary order and applying standard Group Relative Policy Optimization (GRPO) instead. Our approach, JustGRPO, is minimalist yet surprisingly effective (e.g., 89.1% accuracy on GSM8K) while fully retaining the parallel decoding ability of dLLMs. Project page: this https URL"
    },
    {
      "title": "MetaWorld: Skill Transfer and Composition in a Hierarchical World Model for Grounding High-Level Instructions",
      "url": "https://arxiv.org/abs/2601.17507",
      "date": "2026-01-24",
      "authors_text": "Yutong Shen, Hangxu Liu, Kailin Pei, Ruizhe Xia, Tongtong Feng",
      "is_highlight": false,
      "score": 85.0,
      "summary": "MetaWorld introduces a hierarchical model that enhances humanoid robot loco-manipulation by integrating semantic planning and physical control, improving task completion and motion coherence.",
      "teaser_image": null,
      "debug_abstract": "Humanoid robot loco-manipulation remains constrained by the semantic-physical gap. Current methods face three limitations: Low sample efficiency in reinforcement learning, poor generalization in imitation learning, and physical inconsistency in VLMs. We propose MetaWorld, a hierarchical world model that integrates semantic planning and physical control via expert policy transfer. The framework decouples tasks into a VLM-driven semantic layer and a latent dynamics model operating in a compact state space. Our dynamic expert selection and motion prior fusion mechanism leverages a pre-trained multi-expert policy library as transferable knowledge, enabling efficient online adaptation via a two-stage framework. VLMs serve as semantic interfaces, mapping instructions to executable skills and bypassing symbol grounding. Experiments on Humanoid-Bench show MetaWorld outperforms world model-based RL in task completion and motion coherence. Our code will be found at this https URL"
    },
    {
      "title": "Latent-Space Contrastive Reinforcement Learning for Stable and Efficient LLM Reasoning",
      "url": "https://arxiv.org/abs/2601.17275",
      "date": "2026-01-24",
      "authors_text": "Lianlei Shan, Han Chen, Yixuan Wang, Zhenjie Liu, Wei Li",
      "is_highlight": false,
      "score": 60.0,
      "summary": "The paper introduces DeepLatent Reasoning (DLR), a contrastive reinforcement learning framework that enhances LLM reasoning by optimizing latent space exploration while preventing catastrophic forgetting.",
      "teaser_image": null,
      "debug_abstract": "While Large Language Models (LLMs) demonstrate exceptional performance in surface-level text generation, their nature in handling complex multi-step reasoning tasks often remains one of ``statistical fitting&#39;&#39; rather than systematic logical deduction. Traditional Reinforcement Learning (RL) attempts to mitigate this by introducing a ``think-before-speak&#39;&#39; paradigm. However, applying RL directly in high-dimensional, discrete token spaces faces three inherent challenges: sample-inefficient rollouts, high gradient estimation variance, and the risk of catastrophic forgetting. To fundamentally address these structural bottlenecks, we propose \\textbf{DeepLatent Reasoning (DLR)}, a latent-space bidirectional contrastive reinforcement learning framework. This framework shifts the trial-and-error cost from expensive token-level full sequence generation to the continuous latent manifold. Specifically, we introduce a lightweight assistant model to efficiently sample $K$ reasoning chain encodings within the latent space. These encodings are filtered via a dual reward mechanism based on correctness and formatting; only high-value latent trajectories are fed into a \\textbf{frozen main model} for single-pass decoding. To maximize reasoning diversity while maintaining coherence, we design a contrastive learning objective to enable directed exploration within the latent space. Since the main model parameters remain frozen during optimization, this method mathematically eliminates catastrophic forgetting. Experiments demonstrate that under comparable GPU computational budgets, DLR achieves more stable training convergence, supports longer-horizon reasoning chains, and facilitates the sustainable accumulation of reasoning capabilities, providing a viable path toward reliable and scalable reinforcement learning for LLMs."
    }
  ],
  "PINE Lab": [
    {
      "title": "Accurate Calibration and Robust LiDAR-Inertial Odometry for Spinning Actuated LiDAR Systems",
      "url": "https://arxiv.org/abs/2601.15946",
      "date": "2026-01-24",
      "authors_text": "Zijie Chen, Xiaowei Liu, Yong Xu, Shenghai Yuan, Jianping Li",
      "is_highlight": false,
      "score": 70.0,
      "summary": "The paper introduces LM-Calibr for targetless LiDAR-motor calibration and EVA-LIO for adaptive LiDAR-inertial odometry, enhancing accuracy and robustness in spinning actuated LiDAR systems.",
      "teaser_image": null,
      "debug_abstract": "Accurate calibration and robust localization are fundamental for downstream tasks in spinning actuated LiDAR applications. Existing methods, however, require parameterizing extrinsic parameters based on different mounting configurations, limiting their generalizability. Additionally, spinning actuated LiDAR inevitably scans featureless regions, which complicates the balance between scanning coverage and localization robustness. To address these challenges, this letter presents a targetless LiDAR-motor calibration (LM-Calibr) on the basis of the Denavit-Hartenberg convention and an environmental adaptive LiDAR-inertial odometry (EVA-LIO). LM-Calibr supports calibration of LiDAR-motor systems with various mounting configurations. Extensive experiments demonstrate its accuracy and convergence across different scenarios, mounting angles, and initial values. Additionally, EVA-LIO adaptively selects downsample rates and map resolutions according to spatial scale. This adaptivity enables the actuator to operate at maximum speed, thereby enhancing scanning completeness while ensuring robust localization, even when LiDAR briefly scans featureless areas. The source code and hardware design are available on GitHub: \\textcolor{blue}{\\href{this https URL}{this http URL\\_calibr}}. The video is available at \\textcolor{blue}{\\href{this https URL}{this http URL}}"
    },
    {
      "title": "Resisting Manipulative Bots in Meme Coin Copy Trading: A Multi-Agent Approach with Chain-of-Thought Reasoning",
      "url": "https://arxiv.org/abs/2601.08641",
      "date": "2026-01-26",
      "authors_text": "Yichen Luo, Yebo Feng, Jiahua Xu, Yang Liu",
      "is_highlight": false,
      "score": 40.0,
      "summary": "This paper presents a multi-agent system utilizing a large language model to enhance copy trading in meme coin markets, effectively resisting manipulative bots and improving trader profitability.",
      "teaser_image": null,
      "debug_abstract": "Copy trading has become the dominant entry strategy in meme coin markets. However, due to the market&#39;s extreme illiquid and volatile nature, the strategy exposes an exploitable attack surface: adversaries deploy manipulative bots to front-run trades, conceal positions, and fabricate sentiment, systematically extracting value from naïve copiers at scale. Despite its prevalence, bot-driven manipulation remains largely unexplored, and no robust defensive framework exists. We propose a manipulation-resistant copy-trading system based on a multi-agent architecture powered by a multi-modal, explainable large language model (LLM). Our system decomposes copy trading into three specialized agents for coin evaluation, wallet selection, and timing assessment. Evaluated on historical data from over 6,000 meme coins, our approach outperforms zero-shot and most statistic-driven baselines in prediction accuracy as well as all baselines in economic performance, achieving an average return of 14% for identified smart-money trades and an estimated copier return of 3% per trade under realistic market frictions. Overall, our results demonstrate the effectiveness of agent-based defenses and predictability of trader profitability in adversarial meme coin markets, providing a practical foundation for robust copy trading."
    },
    {
      "title": "ORION: Option-Regularized Deep Reinforcement Learning for Cooperative Multi-Agent Online Navigation",
      "url": "https://arxiv.org/abs/2601.01155",
      "date": "2026-01-26",
      "authors_text": "Shizhe Zhang, Jingsong Liang, Zhitao Zhou, Shuhan Ye, Yizhuo Wang",
      "is_highlight": false,
      "score": 90.0,
      "summary": "ORION is a deep reinforcement learning framework that enables decentralized, cooperative navigation in partially known environments, enhancing agent coordination and map uncertainty reduction.",
      "teaser_image": null,
      "debug_abstract": "Existing methods for multi-agent navigation typically assume fully known environments, offering limited support for partially known scenarios such as warehouses or factory floors. There, agents may need to plan trajectories that balance their own path optimality with their ability to collect and share information about the environment that can help their teammates reach their own goals. To these ends, we propose ORION, a novel deep reinforcement learning framework for cooperative multi-agent online navigation in partially known environments. Starting from an imperfect prior map, ORION trains agents to make decentralized decisions, coordinate to reach their individual targets, and actively reduce map uncertainty by sharing online observations in a closed perception-action loop. We first design a shared graph encoder that fuses prior map with online perception into a unified representation, providing robust state embeddings under dynamic map discrepancies. At the core of ORION is an option-critic framework that learns to reason about a set of high-level cooperative modes that translate into sequences of low-level actions, allowing agents to switch between individual navigation and team-level exploration adaptively. We further introduce a dual-stage cooperation strategy that enables agents to assist teammates under map uncertainty, thereby reducing the overall makespan. Across extensive maze-like maps and large-scale warehouse environments, our simulation results show that ORION achieves high-quality, real-time decentralized cooperation over varying team sizes, outperforming state-of-the-art classical and learning-based baselines. Finally, we validate ORION on physical robot teams, demonstrating its robustness and practicality for real-world cooperative navigation."
    },
    {
      "title": "Delay-Compensated Stiffness Estimation for Robot-Mediated Dyadic Interaction",
      "url": "https://arxiv.org/abs/2601.17812",
      "date": "2026-01-25",
      "authors_text": "Mingtian Du, Suhas Raghavendra Kulkarni, Bernardo Noronha, Domenico Campolo",
      "is_highlight": false,
      "score": 80.0,
      "summary": "The paper presents a delay-compensated stiffness estimation framework for robot-mediated dyadic interactions, improving accuracy in remote physical therapy despite network-induced haptic delays.",
      "teaser_image": null,
      "debug_abstract": "Robot-mediated human-human (dyadic) interactions enable therapists to provide physical therapy remotely, yet an accurate perception of patient stiffness remains challenging due to network-induced haptic delays. Conventional stiffness estimation methods, which neglect delay, suffer from temporal misalignment between force and position signals, leading to significant estimation errors as delays increase. To address this, we propose a robust, delay-compensated stiffness estimation framework by deriving an algebraic estimator based on quasi-static equilibrium that explicitly accounts for temporally aligning the expert&#39;s input with the novice&#39;s response. A Normalised Weighted Least Squares (NWLS) implementation is then introduced to robustly filter dynamic bias resulting from the algebraic derivation. Experiments using commercial rehabilitation robots (H-MAN) as the platform demonstrate that the proposed method significantly outperforms the standard estimator, maintaining consistent tracking accuracy under multiple introduced delays. These findings offer a promising solution for achieving high-fidelity haptic perception in remote dyadic interaction, potentially facilitating reliable stiffness assessment in therapeutic settings across networks."
    },
    {
      "title": "Physical Prompt Injection Attacks on Large Vision-Language Models",
      "url": "https://arxiv.org/abs/2601.17383",
      "date": "2026-01-24",
      "authors_text": "Chen Ling, Kai Hu, Hangcheng Liu, Xingshuo Han, Tianwei Zhang",
      "is_highlight": false,
      "score": 50.0,
      "summary": "The paper introduces Physical Prompt Injection Attacks (PPIA), a novel, query-agnostic method that manipulates Large Vision-Language Models using visual prompts embedded in physical objects, achieving up to 98% success.",
      "teaser_image": null,
      "debug_abstract": "Large Vision-Language Models (LVLMs) are increasingly deployed in real-world intelligent systems for perception and reasoning in open physical environments. While LVLMs are known to be vulnerable to prompt injection attacks, existing methods either require access to input channels or depend on knowledge of user queries, assumptions that rarely hold in practical deployments. We propose the first Physical Prompt Injection Attack (PPIA), a black-box, query-agnostic attack that embeds malicious typographic instructions into physical objects perceivable by the LVLM. PPIA requires no access to the model, its inputs, or internal pipeline, and operates solely through visual observation. It combines offline selection of highly recognizable and semantically effective visual prompts with strategic environment-aware placement guided by spatiotemporal attention, ensuring that the injected prompts are both perceivable and influential on model behavior. We evaluate PPIA across 10 state-of-the-art LVLMs in both simulated and real-world settings on tasks including visual question answering, planning, and navigation, PPIA achieves attack success rates up to 98%, with strong robustness under varying physical conditions such as distance, viewpoint, and illumination. Our code is publicly available at this https URL."
    }
  ],
  "S-Lab for Advanced Intelligence": [
    {
      "title": "Accurate Calibration and Robust LiDAR-Inertial Odometry for Spinning Actuated LiDAR Systems",
      "url": "https://arxiv.org/abs/2601.15946",
      "date": "2026-01-24",
      "authors_text": "Zijie Chen, Xiaowei Liu, Yong Xu, Shenghai Yuan, Jianping Li",
      "is_highlight": false,
      "score": 70.0,
      "summary": "The paper introduces LM-Calibr for targetless LiDAR-motor calibration and EVA-LIO for adaptive LiDAR-inertial odometry, enhancing accuracy and robustness in spinning actuated LiDAR systems.",
      "teaser_image": null,
      "debug_abstract": "Accurate calibration and robust localization are fundamental for downstream tasks in spinning actuated LiDAR applications. Existing methods, however, require parameterizing extrinsic parameters based on different mounting configurations, limiting their generalizability. Additionally, spinning actuated LiDAR inevitably scans featureless regions, which complicates the balance between scanning coverage and localization robustness. To address these challenges, this letter presents a targetless LiDAR-motor calibration (LM-Calibr) on the basis of the Denavit-Hartenberg convention and an environmental adaptive LiDAR-inertial odometry (EVA-LIO). LM-Calibr supports calibration of LiDAR-motor systems with various mounting configurations. Extensive experiments demonstrate its accuracy and convergence across different scenarios, mounting angles, and initial values. Additionally, EVA-LIO adaptively selects downsample rates and map resolutions according to spatial scale. This adaptivity enables the actuator to operate at maximum speed, thereby enhancing scanning completeness while ensuring robust localization, even when LiDAR briefly scans featureless areas. The source code and hardware design are available on GitHub: \\textcolor{blue}{\\href{this https URL}{this http URL\\_calibr}}. The video is available at \\textcolor{blue}{\\href{this https URL}{this http URL}}"
    },
    {
      "title": "Resisting Manipulative Bots in Meme Coin Copy Trading: A Multi-Agent Approach with Chain-of-Thought Reasoning",
      "url": "https://arxiv.org/abs/2601.08641",
      "date": "2026-01-26",
      "authors_text": "Yichen Luo, Yebo Feng, Jiahua Xu, Yang Liu",
      "is_highlight": false,
      "score": 40.0,
      "summary": "This paper presents a multi-agent system utilizing a large language model to enhance copy trading in meme coin markets, effectively resisting manipulative bots and improving trader profitability.",
      "teaser_image": null,
      "debug_abstract": "Copy trading has become the dominant entry strategy in meme coin markets. However, due to the market&#39;s extreme illiquid and volatile nature, the strategy exposes an exploitable attack surface: adversaries deploy manipulative bots to front-run trades, conceal positions, and fabricate sentiment, systematically extracting value from naïve copiers at scale. Despite its prevalence, bot-driven manipulation remains largely unexplored, and no robust defensive framework exists. We propose a manipulation-resistant copy-trading system based on a multi-agent architecture powered by a multi-modal, explainable large language model (LLM). Our system decomposes copy trading into three specialized agents for coin evaluation, wallet selection, and timing assessment. Evaluated on historical data from over 6,000 meme coins, our approach outperforms zero-shot and most statistic-driven baselines in prediction accuracy as well as all baselines in economic performance, achieving an average return of 14% for identified smart-money trades and an estimated copier return of 3% per trade under realistic market frictions. Overall, our results demonstrate the effectiveness of agent-based defenses and predictability of trader profitability in adversarial meme coin markets, providing a practical foundation for robust copy trading."
    },
    {
      "title": "ORION: Option-Regularized Deep Reinforcement Learning for Cooperative Multi-Agent Online Navigation",
      "url": "https://arxiv.org/abs/2601.01155",
      "date": "2026-01-26",
      "authors_text": "Shizhe Zhang, Jingsong Liang, Zhitao Zhou, Shuhan Ye, Yizhuo Wang",
      "is_highlight": false,
      "score": 90.0,
      "summary": "ORION is a deep reinforcement learning framework that enables decentralized, cooperative navigation in partially known environments, enhancing agent coordination and map uncertainty reduction.",
      "teaser_image": null,
      "debug_abstract": "Existing methods for multi-agent navigation typically assume fully known environments, offering limited support for partially known scenarios such as warehouses or factory floors. There, agents may need to plan trajectories that balance their own path optimality with their ability to collect and share information about the environment that can help their teammates reach their own goals. To these ends, we propose ORION, a novel deep reinforcement learning framework for cooperative multi-agent online navigation in partially known environments. Starting from an imperfect prior map, ORION trains agents to make decentralized decisions, coordinate to reach their individual targets, and actively reduce map uncertainty by sharing online observations in a closed perception-action loop. We first design a shared graph encoder that fuses prior map with online perception into a unified representation, providing robust state embeddings under dynamic map discrepancies. At the core of ORION is an option-critic framework that learns to reason about a set of high-level cooperative modes that translate into sequences of low-level actions, allowing agents to switch between individual navigation and team-level exploration adaptively. We further introduce a dual-stage cooperation strategy that enables agents to assist teammates under map uncertainty, thereby reducing the overall makespan. Across extensive maze-like maps and large-scale warehouse environments, our simulation results show that ORION achieves high-quality, real-time decentralized cooperation over varying team sizes, outperforming state-of-the-art classical and learning-based baselines. Finally, we validate ORION on physical robot teams, demonstrating its robustness and practicality for real-world cooperative navigation."
    },
    {
      "title": "Delay-Compensated Stiffness Estimation for Robot-Mediated Dyadic Interaction",
      "url": "https://arxiv.org/abs/2601.17812",
      "date": "2026-01-25",
      "authors_text": "Mingtian Du, Suhas Raghavendra Kulkarni, Bernardo Noronha, Domenico Campolo",
      "is_highlight": false,
      "score": 80.0,
      "summary": "The paper presents a delay-compensated stiffness estimation framework for robot-mediated dyadic interactions, improving accuracy in remote physical therapy despite network-induced haptic delays.",
      "teaser_image": null,
      "debug_abstract": "Robot-mediated human-human (dyadic) interactions enable therapists to provide physical therapy remotely, yet an accurate perception of patient stiffness remains challenging due to network-induced haptic delays. Conventional stiffness estimation methods, which neglect delay, suffer from temporal misalignment between force and position signals, leading to significant estimation errors as delays increase. To address this, we propose a robust, delay-compensated stiffness estimation framework by deriving an algebraic estimator based on quasi-static equilibrium that explicitly accounts for temporally aligning the expert&#39;s input with the novice&#39;s response. A Normalised Weighted Least Squares (NWLS) implementation is then introduced to robustly filter dynamic bias resulting from the algebraic derivation. Experiments using commercial rehabilitation robots (H-MAN) as the platform demonstrate that the proposed method significantly outperforms the standard estimator, maintaining consistent tracking accuracy under multiple introduced delays. These findings offer a promising solution for achieving high-fidelity haptic perception in remote dyadic interaction, potentially facilitating reliable stiffness assessment in therapeutic settings across networks."
    },
    {
      "title": "Physical Prompt Injection Attacks on Large Vision-Language Models",
      "url": "https://arxiv.org/abs/2601.17383",
      "date": "2026-01-24",
      "authors_text": "Chen Ling, Kai Hu, Hangcheng Liu, Xingshuo Han, Tianwei Zhang",
      "is_highlight": false,
      "score": 50.0,
      "summary": "The paper introduces Physical Prompt Injection Attacks (PPIA), a novel, query-agnostic method that manipulates Large Vision-Language Models using visual prompts embedded in physical objects, achieving up to 98% success.",
      "teaser_image": null,
      "debug_abstract": "Large Vision-Language Models (LVLMs) are increasingly deployed in real-world intelligent systems for perception and reasoning in open physical environments. While LVLMs are known to be vulnerable to prompt injection attacks, existing methods either require access to input channels or depend on knowledge of user queries, assumptions that rarely hold in practical deployments. We propose the first Physical Prompt Injection Attack (PPIA), a black-box, query-agnostic attack that embeds malicious typographic instructions into physical objects perceivable by the LVLM. PPIA requires no access to the model, its inputs, or internal pipeline, and operates solely through visual observation. It combines offline selection of highly recognizable and semantically effective visual prompts with strategic environment-aware placement guided by spatiotemporal attention, ensuring that the injected prompts are both perceivable and influential on model behavior. We evaluate PPIA across 10 state-of-the-art LVLMs in both simulated and real-world settings on tasks including visual question answering, planning, and navigation, PPIA achieves attack success rates up to 98%, with strong robustness under varying physical conditions such as distance, viewpoint, and illumination. Our code is publicly available at this https URL."
    }
  ],
  "MReaLLab": [
    {
      "title": "Accurate Calibration and Robust LiDAR-Inertial Odometry for Spinning Actuated LiDAR Systems",
      "url": "https://arxiv.org/abs/2601.15946",
      "date": "2026-01-24",
      "authors_text": "Zijie Chen, Xiaowei Liu, Yong Xu, Shenghai Yuan, Jianping Li",
      "is_highlight": false,
      "score": 70.0,
      "summary": "The paper introduces LM-Calibr for targetless LiDAR-motor calibration and EVA-LIO for adaptive LiDAR-inertial odometry, enhancing accuracy and robustness in spinning actuated LiDAR systems.",
      "teaser_image": null,
      "debug_abstract": "Accurate calibration and robust localization are fundamental for downstream tasks in spinning actuated LiDAR applications. Existing methods, however, require parameterizing extrinsic parameters based on different mounting configurations, limiting their generalizability. Additionally, spinning actuated LiDAR inevitably scans featureless regions, which complicates the balance between scanning coverage and localization robustness. To address these challenges, this letter presents a targetless LiDAR-motor calibration (LM-Calibr) on the basis of the Denavit-Hartenberg convention and an environmental adaptive LiDAR-inertial odometry (EVA-LIO). LM-Calibr supports calibration of LiDAR-motor systems with various mounting configurations. Extensive experiments demonstrate its accuracy and convergence across different scenarios, mounting angles, and initial values. Additionally, EVA-LIO adaptively selects downsample rates and map resolutions according to spatial scale. This adaptivity enables the actuator to operate at maximum speed, thereby enhancing scanning completeness while ensuring robust localization, even when LiDAR briefly scans featureless areas. The source code and hardware design are available on GitHub: \\textcolor{blue}{\\href{this https URL}{this http URL\\_calibr}}. The video is available at \\textcolor{blue}{\\href{this https URL}{this http URL}}"
    },
    {
      "title": "Resisting Manipulative Bots in Meme Coin Copy Trading: A Multi-Agent Approach with Chain-of-Thought Reasoning",
      "url": "https://arxiv.org/abs/2601.08641",
      "date": "2026-01-26",
      "authors_text": "Yichen Luo, Yebo Feng, Jiahua Xu, Yang Liu",
      "is_highlight": false,
      "score": 40.0,
      "summary": "This paper presents a multi-agent system utilizing a large language model to enhance copy trading in meme coin markets, effectively resisting manipulative bots and improving trader profitability.",
      "teaser_image": null,
      "debug_abstract": "Copy trading has become the dominant entry strategy in meme coin markets. However, due to the market&#39;s extreme illiquid and volatile nature, the strategy exposes an exploitable attack surface: adversaries deploy manipulative bots to front-run trades, conceal positions, and fabricate sentiment, systematically extracting value from naïve copiers at scale. Despite its prevalence, bot-driven manipulation remains largely unexplored, and no robust defensive framework exists. We propose a manipulation-resistant copy-trading system based on a multi-agent architecture powered by a multi-modal, explainable large language model (LLM). Our system decomposes copy trading into three specialized agents for coin evaluation, wallet selection, and timing assessment. Evaluated on historical data from over 6,000 meme coins, our approach outperforms zero-shot and most statistic-driven baselines in prediction accuracy as well as all baselines in economic performance, achieving an average return of 14% for identified smart-money trades and an estimated copier return of 3% per trade under realistic market frictions. Overall, our results demonstrate the effectiveness of agent-based defenses and predictability of trader profitability in adversarial meme coin markets, providing a practical foundation for robust copy trading."
    },
    {
      "title": "ORION: Option-Regularized Deep Reinforcement Learning for Cooperative Multi-Agent Online Navigation",
      "url": "https://arxiv.org/abs/2601.01155",
      "date": "2026-01-26",
      "authors_text": "Shizhe Zhang, Jingsong Liang, Zhitao Zhou, Shuhan Ye, Yizhuo Wang",
      "is_highlight": false,
      "score": 90.0,
      "summary": "ORION is a deep reinforcement learning framework that enables decentralized, cooperative navigation in partially known environments, enhancing agent coordination and map uncertainty reduction.",
      "teaser_image": null,
      "debug_abstract": "Existing methods for multi-agent navigation typically assume fully known environments, offering limited support for partially known scenarios such as warehouses or factory floors. There, agents may need to plan trajectories that balance their own path optimality with their ability to collect and share information about the environment that can help their teammates reach their own goals. To these ends, we propose ORION, a novel deep reinforcement learning framework for cooperative multi-agent online navigation in partially known environments. Starting from an imperfect prior map, ORION trains agents to make decentralized decisions, coordinate to reach their individual targets, and actively reduce map uncertainty by sharing online observations in a closed perception-action loop. We first design a shared graph encoder that fuses prior map with online perception into a unified representation, providing robust state embeddings under dynamic map discrepancies. At the core of ORION is an option-critic framework that learns to reason about a set of high-level cooperative modes that translate into sequences of low-level actions, allowing agents to switch between individual navigation and team-level exploration adaptively. We further introduce a dual-stage cooperation strategy that enables agents to assist teammates under map uncertainty, thereby reducing the overall makespan. Across extensive maze-like maps and large-scale warehouse environments, our simulation results show that ORION achieves high-quality, real-time decentralized cooperation over varying team sizes, outperforming state-of-the-art classical and learning-based baselines. Finally, we validate ORION on physical robot teams, demonstrating its robustness and practicality for real-world cooperative navigation."
    },
    {
      "title": "Delay-Compensated Stiffness Estimation for Robot-Mediated Dyadic Interaction",
      "url": "https://arxiv.org/abs/2601.17812",
      "date": "2026-01-25",
      "authors_text": "Mingtian Du, Suhas Raghavendra Kulkarni, Bernardo Noronha, Domenico Campolo",
      "is_highlight": false,
      "score": 80.0,
      "summary": "The paper presents a delay-compensated stiffness estimation framework for robot-mediated dyadic interactions, improving accuracy in remote physical therapy despite network-induced haptic delays.",
      "teaser_image": null,
      "debug_abstract": "Robot-mediated human-human (dyadic) interactions enable therapists to provide physical therapy remotely, yet an accurate perception of patient stiffness remains challenging due to network-induced haptic delays. Conventional stiffness estimation methods, which neglect delay, suffer from temporal misalignment between force and position signals, leading to significant estimation errors as delays increase. To address this, we propose a robust, delay-compensated stiffness estimation framework by deriving an algebraic estimator based on quasi-static equilibrium that explicitly accounts for temporally aligning the expert&#39;s input with the novice&#39;s response. A Normalised Weighted Least Squares (NWLS) implementation is then introduced to robustly filter dynamic bias resulting from the algebraic derivation. Experiments using commercial rehabilitation robots (H-MAN) as the platform demonstrate that the proposed method significantly outperforms the standard estimator, maintaining consistent tracking accuracy under multiple introduced delays. These findings offer a promising solution for achieving high-fidelity haptic perception in remote dyadic interaction, potentially facilitating reliable stiffness assessment in therapeutic settings across networks."
    },
    {
      "title": "Physical Prompt Injection Attacks on Large Vision-Language Models",
      "url": "https://arxiv.org/abs/2601.17383",
      "date": "2026-01-24",
      "authors_text": "Chen Ling, Kai Hu, Hangcheng Liu, Xingshuo Han, Tianwei Zhang",
      "is_highlight": false,
      "score": 50.0,
      "summary": "The paper introduces Physical Prompt Injection Attacks (PPIA), a novel, query-agnostic method that manipulates Large Vision-Language Models using visual prompts embedded in physical objects, achieving up to 98% success.",
      "teaser_image": null,
      "debug_abstract": "Large Vision-Language Models (LVLMs) are increasingly deployed in real-world intelligent systems for perception and reasoning in open physical environments. While LVLMs are known to be vulnerable to prompt injection attacks, existing methods either require access to input channels or depend on knowledge of user queries, assumptions that rarely hold in practical deployments. We propose the first Physical Prompt Injection Attack (PPIA), a black-box, query-agnostic attack that embeds malicious typographic instructions into physical objects perceivable by the LVLM. PPIA requires no access to the model, its inputs, or internal pipeline, and operates solely through visual observation. It combines offline selection of highly recognizable and semantically effective visual prompts with strategic environment-aware placement guided by spatiotemporal attention, ensuring that the injected prompts are both perceivable and influential on model behavior. We evaluate PPIA across 10 state-of-the-art LVLMs in both simulated and real-world settings on tasks including visual question answering, planning, and navigation, PPIA achieves attack success rates up to 98%, with strong robustness under varying physical conditions such as distance, viewpoint, and illumination. Our code is publicly available at this https URL."
    }
  ],
  "ROSE Lab": [
    {
      "title": "Accurate Calibration and Robust LiDAR-Inertial Odometry for Spinning Actuated LiDAR Systems",
      "url": "https://arxiv.org/abs/2601.15946",
      "date": "2026-01-24",
      "authors_text": "Zijie Chen, Xiaowei Liu, Yong Xu, Shenghai Yuan, Jianping Li",
      "is_highlight": false,
      "score": 70.0,
      "summary": "The paper introduces LM-Calibr for targetless LiDAR-motor calibration and EVA-LIO for adaptive LiDAR-inertial odometry, enhancing accuracy and robustness in spinning actuated LiDAR systems.",
      "teaser_image": null,
      "debug_abstract": "Accurate calibration and robust localization are fundamental for downstream tasks in spinning actuated LiDAR applications. Existing methods, however, require parameterizing extrinsic parameters based on different mounting configurations, limiting their generalizability. Additionally, spinning actuated LiDAR inevitably scans featureless regions, which complicates the balance between scanning coverage and localization robustness. To address these challenges, this letter presents a targetless LiDAR-motor calibration (LM-Calibr) on the basis of the Denavit-Hartenberg convention and an environmental adaptive LiDAR-inertial odometry (EVA-LIO). LM-Calibr supports calibration of LiDAR-motor systems with various mounting configurations. Extensive experiments demonstrate its accuracy and convergence across different scenarios, mounting angles, and initial values. Additionally, EVA-LIO adaptively selects downsample rates and map resolutions according to spatial scale. This adaptivity enables the actuator to operate at maximum speed, thereby enhancing scanning completeness while ensuring robust localization, even when LiDAR briefly scans featureless areas. The source code and hardware design are available on GitHub: \\textcolor{blue}{\\href{this https URL}{this http URL\\_calibr}}. The video is available at \\textcolor{blue}{\\href{this https URL}{this http URL}}"
    },
    {
      "title": "Resisting Manipulative Bots in Meme Coin Copy Trading: A Multi-Agent Approach with Chain-of-Thought Reasoning",
      "url": "https://arxiv.org/abs/2601.08641",
      "date": "2026-01-26",
      "authors_text": "Yichen Luo, Yebo Feng, Jiahua Xu, Yang Liu",
      "is_highlight": false,
      "score": 40.0,
      "summary": "This paper presents a multi-agent system utilizing a large language model to enhance copy trading in meme coin markets, effectively resisting manipulative bots and improving trader profitability.",
      "teaser_image": null,
      "debug_abstract": "Copy trading has become the dominant entry strategy in meme coin markets. However, due to the market&#39;s extreme illiquid and volatile nature, the strategy exposes an exploitable attack surface: adversaries deploy manipulative bots to front-run trades, conceal positions, and fabricate sentiment, systematically extracting value from naïve copiers at scale. Despite its prevalence, bot-driven manipulation remains largely unexplored, and no robust defensive framework exists. We propose a manipulation-resistant copy-trading system based on a multi-agent architecture powered by a multi-modal, explainable large language model (LLM). Our system decomposes copy trading into three specialized agents for coin evaluation, wallet selection, and timing assessment. Evaluated on historical data from over 6,000 meme coins, our approach outperforms zero-shot and most statistic-driven baselines in prediction accuracy as well as all baselines in economic performance, achieving an average return of 14% for identified smart-money trades and an estimated copier return of 3% per trade under realistic market frictions. Overall, our results demonstrate the effectiveness of agent-based defenses and predictability of trader profitability in adversarial meme coin markets, providing a practical foundation for robust copy trading."
    },
    {
      "title": "ORION: Option-Regularized Deep Reinforcement Learning for Cooperative Multi-Agent Online Navigation",
      "url": "https://arxiv.org/abs/2601.01155",
      "date": "2026-01-26",
      "authors_text": "Shizhe Zhang, Jingsong Liang, Zhitao Zhou, Shuhan Ye, Yizhuo Wang",
      "is_highlight": false,
      "score": 90.0,
      "summary": "ORION is a deep reinforcement learning framework that enables decentralized, cooperative navigation in partially known environments, enhancing agent coordination and map uncertainty reduction.",
      "teaser_image": null,
      "debug_abstract": "Existing methods for multi-agent navigation typically assume fully known environments, offering limited support for partially known scenarios such as warehouses or factory floors. There, agents may need to plan trajectories that balance their own path optimality with their ability to collect and share information about the environment that can help their teammates reach their own goals. To these ends, we propose ORION, a novel deep reinforcement learning framework for cooperative multi-agent online navigation in partially known environments. Starting from an imperfect prior map, ORION trains agents to make decentralized decisions, coordinate to reach their individual targets, and actively reduce map uncertainty by sharing online observations in a closed perception-action loop. We first design a shared graph encoder that fuses prior map with online perception into a unified representation, providing robust state embeddings under dynamic map discrepancies. At the core of ORION is an option-critic framework that learns to reason about a set of high-level cooperative modes that translate into sequences of low-level actions, allowing agents to switch between individual navigation and team-level exploration adaptively. We further introduce a dual-stage cooperation strategy that enables agents to assist teammates under map uncertainty, thereby reducing the overall makespan. Across extensive maze-like maps and large-scale warehouse environments, our simulation results show that ORION achieves high-quality, real-time decentralized cooperation over varying team sizes, outperforming state-of-the-art classical and learning-based baselines. Finally, we validate ORION on physical robot teams, demonstrating its robustness and practicality for real-world cooperative navigation."
    },
    {
      "title": "Delay-Compensated Stiffness Estimation for Robot-Mediated Dyadic Interaction",
      "url": "https://arxiv.org/abs/2601.17812",
      "date": "2026-01-25",
      "authors_text": "Mingtian Du, Suhas Raghavendra Kulkarni, Bernardo Noronha, Domenico Campolo",
      "is_highlight": false,
      "score": 80.0,
      "summary": "The paper presents a delay-compensated stiffness estimation framework for robot-mediated dyadic interactions, improving accuracy in remote physical therapy despite network-induced haptic delays.",
      "teaser_image": null,
      "debug_abstract": "Robot-mediated human-human (dyadic) interactions enable therapists to provide physical therapy remotely, yet an accurate perception of patient stiffness remains challenging due to network-induced haptic delays. Conventional stiffness estimation methods, which neglect delay, suffer from temporal misalignment between force and position signals, leading to significant estimation errors as delays increase. To address this, we propose a robust, delay-compensated stiffness estimation framework by deriving an algebraic estimator based on quasi-static equilibrium that explicitly accounts for temporally aligning the expert&#39;s input with the novice&#39;s response. A Normalised Weighted Least Squares (NWLS) implementation is then introduced to robustly filter dynamic bias resulting from the algebraic derivation. Experiments using commercial rehabilitation robots (H-MAN) as the platform demonstrate that the proposed method significantly outperforms the standard estimator, maintaining consistent tracking accuracy under multiple introduced delays. These findings offer a promising solution for achieving high-fidelity haptic perception in remote dyadic interaction, potentially facilitating reliable stiffness assessment in therapeutic settings across networks."
    },
    {
      "title": "Physical Prompt Injection Attacks on Large Vision-Language Models",
      "url": "https://arxiv.org/abs/2601.17383",
      "date": "2026-01-24",
      "authors_text": "Chen Ling, Kai Hu, Hangcheng Liu, Xingshuo Han, Tianwei Zhang",
      "is_highlight": false,
      "score": 50.0,
      "summary": "The paper introduces Physical Prompt Injection Attacks (PPIA), a novel, query-agnostic method that manipulates Large Vision-Language Models using visual prompts embedded in physical objects, achieving up to 98% success.",
      "teaser_image": null,
      "debug_abstract": "Large Vision-Language Models (LVLMs) are increasingly deployed in real-world intelligent systems for perception and reasoning in open physical environments. While LVLMs are known to be vulnerable to prompt injection attacks, existing methods either require access to input channels or depend on knowledge of user queries, assumptions that rarely hold in practical deployments. We propose the first Physical Prompt Injection Attack (PPIA), a black-box, query-agnostic attack that embeds malicious typographic instructions into physical objects perceivable by the LVLM. PPIA requires no access to the model, its inputs, or internal pipeline, and operates solely through visual observation. It combines offline selection of highly recognizable and semantically effective visual prompts with strategic environment-aware placement guided by spatiotemporal attention, ensuring that the injected prompts are both perceivable and influential on model behavior. We evaluate PPIA across 10 state-of-the-art LVLMs in both simulated and real-world settings on tasks including visual question answering, planning, and navigation, PPIA achieves attack success rates up to 98%, with strong robustness under varying physical conditions such as distance, viewpoint, and illumination. Our code is publicly available at this https URL."
    }
  ],
  "MARS Lab": [
    {
      "title": "Accurate Calibration and Robust LiDAR-Inertial Odometry for Spinning Actuated LiDAR Systems",
      "url": "https://arxiv.org/abs/2601.15946",
      "date": "2026-01-24",
      "authors_text": "Zijie Chen, Xiaowei Liu, Yong Xu, Shenghai Yuan, Jianping Li",
      "is_highlight": false,
      "score": 70.0,
      "summary": "The paper introduces LM-Calibr for targetless LiDAR-motor calibration and EVA-LIO for adaptive LiDAR-inertial odometry, enhancing accuracy and robustness in spinning actuated LiDAR systems.",
      "teaser_image": null,
      "debug_abstract": "Accurate calibration and robust localization are fundamental for downstream tasks in spinning actuated LiDAR applications. Existing methods, however, require parameterizing extrinsic parameters based on different mounting configurations, limiting their generalizability. Additionally, spinning actuated LiDAR inevitably scans featureless regions, which complicates the balance between scanning coverage and localization robustness. To address these challenges, this letter presents a targetless LiDAR-motor calibration (LM-Calibr) on the basis of the Denavit-Hartenberg convention and an environmental adaptive LiDAR-inertial odometry (EVA-LIO). LM-Calibr supports calibration of LiDAR-motor systems with various mounting configurations. Extensive experiments demonstrate its accuracy and convergence across different scenarios, mounting angles, and initial values. Additionally, EVA-LIO adaptively selects downsample rates and map resolutions according to spatial scale. This adaptivity enables the actuator to operate at maximum speed, thereby enhancing scanning completeness while ensuring robust localization, even when LiDAR briefly scans featureless areas. The source code and hardware design are available on GitHub: \\textcolor{blue}{\\href{this https URL}{this http URL\\_calibr}}. The video is available at \\textcolor{blue}{\\href{this https URL}{this http URL}}"
    },
    {
      "title": "Resisting Manipulative Bots in Meme Coin Copy Trading: A Multi-Agent Approach with Chain-of-Thought Reasoning",
      "url": "https://arxiv.org/abs/2601.08641",
      "date": "2026-01-26",
      "authors_text": "Yichen Luo, Yebo Feng, Jiahua Xu, Yang Liu",
      "is_highlight": false,
      "score": 40.0,
      "summary": "This paper presents a multi-agent system utilizing a large language model to enhance copy trading in meme coin markets, effectively resisting manipulative bots and improving trader profitability.",
      "teaser_image": null,
      "debug_abstract": "Copy trading has become the dominant entry strategy in meme coin markets. However, due to the market&#39;s extreme illiquid and volatile nature, the strategy exposes an exploitable attack surface: adversaries deploy manipulative bots to front-run trades, conceal positions, and fabricate sentiment, systematically extracting value from naïve copiers at scale. Despite its prevalence, bot-driven manipulation remains largely unexplored, and no robust defensive framework exists. We propose a manipulation-resistant copy-trading system based on a multi-agent architecture powered by a multi-modal, explainable large language model (LLM). Our system decomposes copy trading into three specialized agents for coin evaluation, wallet selection, and timing assessment. Evaluated on historical data from over 6,000 meme coins, our approach outperforms zero-shot and most statistic-driven baselines in prediction accuracy as well as all baselines in economic performance, achieving an average return of 14% for identified smart-money trades and an estimated copier return of 3% per trade under realistic market frictions. Overall, our results demonstrate the effectiveness of agent-based defenses and predictability of trader profitability in adversarial meme coin markets, providing a practical foundation for robust copy trading."
    },
    {
      "title": "ORION: Option-Regularized Deep Reinforcement Learning for Cooperative Multi-Agent Online Navigation",
      "url": "https://arxiv.org/abs/2601.01155",
      "date": "2026-01-26",
      "authors_text": "Shizhe Zhang, Jingsong Liang, Zhitao Zhou, Shuhan Ye, Yizhuo Wang",
      "is_highlight": false,
      "score": 90.0,
      "summary": "ORION is a deep reinforcement learning framework that enables decentralized, cooperative navigation in partially known environments, enhancing agent coordination and map uncertainty reduction.",
      "teaser_image": null,
      "debug_abstract": "Existing methods for multi-agent navigation typically assume fully known environments, offering limited support for partially known scenarios such as warehouses or factory floors. There, agents may need to plan trajectories that balance their own path optimality with their ability to collect and share information about the environment that can help their teammates reach their own goals. To these ends, we propose ORION, a novel deep reinforcement learning framework for cooperative multi-agent online navigation in partially known environments. Starting from an imperfect prior map, ORION trains agents to make decentralized decisions, coordinate to reach their individual targets, and actively reduce map uncertainty by sharing online observations in a closed perception-action loop. We first design a shared graph encoder that fuses prior map with online perception into a unified representation, providing robust state embeddings under dynamic map discrepancies. At the core of ORION is an option-critic framework that learns to reason about a set of high-level cooperative modes that translate into sequences of low-level actions, allowing agents to switch between individual navigation and team-level exploration adaptively. We further introduce a dual-stage cooperation strategy that enables agents to assist teammates under map uncertainty, thereby reducing the overall makespan. Across extensive maze-like maps and large-scale warehouse environments, our simulation results show that ORION achieves high-quality, real-time decentralized cooperation over varying team sizes, outperforming state-of-the-art classical and learning-based baselines. Finally, we validate ORION on physical robot teams, demonstrating its robustness and practicality for real-world cooperative navigation."
    },
    {
      "title": "Delay-Compensated Stiffness Estimation for Robot-Mediated Dyadic Interaction",
      "url": "https://arxiv.org/abs/2601.17812",
      "date": "2026-01-25",
      "authors_text": "Mingtian Du, Suhas Raghavendra Kulkarni, Bernardo Noronha, Domenico Campolo",
      "is_highlight": false,
      "score": 80.0,
      "summary": "The paper presents a delay-compensated stiffness estimation framework for robot-mediated dyadic interactions, improving accuracy in remote physical therapy despite network-induced haptic delays.",
      "teaser_image": null,
      "debug_abstract": "Robot-mediated human-human (dyadic) interactions enable therapists to provide physical therapy remotely, yet an accurate perception of patient stiffness remains challenging due to network-induced haptic delays. Conventional stiffness estimation methods, which neglect delay, suffer from temporal misalignment between force and position signals, leading to significant estimation errors as delays increase. To address this, we propose a robust, delay-compensated stiffness estimation framework by deriving an algebraic estimator based on quasi-static equilibrium that explicitly accounts for temporally aligning the expert&#39;s input with the novice&#39;s response. A Normalised Weighted Least Squares (NWLS) implementation is then introduced to robustly filter dynamic bias resulting from the algebraic derivation. Experiments using commercial rehabilitation robots (H-MAN) as the platform demonstrate that the proposed method significantly outperforms the standard estimator, maintaining consistent tracking accuracy under multiple introduced delays. These findings offer a promising solution for achieving high-fidelity haptic perception in remote dyadic interaction, potentially facilitating reliable stiffness assessment in therapeutic settings across networks."
    },
    {
      "title": "Physical Prompt Injection Attacks on Large Vision-Language Models",
      "url": "https://arxiv.org/abs/2601.17383",
      "date": "2026-01-24",
      "authors_text": "Chen Ling, Kai Hu, Hangcheng Liu, Xingshuo Han, Tianwei Zhang",
      "is_highlight": false,
      "score": 50.0,
      "summary": "The paper introduces Physical Prompt Injection Attacks (PPIA), a novel, query-agnostic method that manipulates Large Vision-Language Models using visual prompts embedded in physical objects, achieving up to 98% success.",
      "teaser_image": null,
      "debug_abstract": "Large Vision-Language Models (LVLMs) are increasingly deployed in real-world intelligent systems for perception and reasoning in open physical environments. While LVLMs are known to be vulnerable to prompt injection attacks, existing methods either require access to input channels or depend on knowledge of user queries, assumptions that rarely hold in practical deployments. We propose the first Physical Prompt Injection Attack (PPIA), a black-box, query-agnostic attack that embeds malicious typographic instructions into physical objects perceivable by the LVLM. PPIA requires no access to the model, its inputs, or internal pipeline, and operates solely through visual observation. It combines offline selection of highly recognizable and semantically effective visual prompts with strategic environment-aware placement guided by spatiotemporal attention, ensuring that the injected prompts are both perceivable and influential on model behavior. We evaluate PPIA across 10 state-of-the-art LVLMs in both simulated and real-world settings on tasks including visual question answering, planning, and navigation, PPIA achieves attack success rates up to 98%, with strong robustness under varying physical conditions such as distance, viewpoint, and illumination. Our code is publicly available at this https URL."
    }
  ],
  "MMLab@NTU": [
    {
      "title": "Accurate Calibration and Robust LiDAR-Inertial Odometry for Spinning Actuated LiDAR Systems",
      "url": "https://arxiv.org/abs/2601.15946",
      "date": "2026-01-24",
      "authors_text": "Zijie Chen, Xiaowei Liu, Yong Xu, Shenghai Yuan, Jianping Li",
      "is_highlight": false,
      "score": 70.0,
      "summary": "The paper introduces LM-Calibr for targetless LiDAR-motor calibration and EVA-LIO for adaptive LiDAR-inertial odometry, enhancing accuracy and robustness in spinning actuated LiDAR systems.",
      "teaser_image": null,
      "debug_abstract": "Accurate calibration and robust localization are fundamental for downstream tasks in spinning actuated LiDAR applications. Existing methods, however, require parameterizing extrinsic parameters based on different mounting configurations, limiting their generalizability. Additionally, spinning actuated LiDAR inevitably scans featureless regions, which complicates the balance between scanning coverage and localization robustness. To address these challenges, this letter presents a targetless LiDAR-motor calibration (LM-Calibr) on the basis of the Denavit-Hartenberg convention and an environmental adaptive LiDAR-inertial odometry (EVA-LIO). LM-Calibr supports calibration of LiDAR-motor systems with various mounting configurations. Extensive experiments demonstrate its accuracy and convergence across different scenarios, mounting angles, and initial values. Additionally, EVA-LIO adaptively selects downsample rates and map resolutions according to spatial scale. This adaptivity enables the actuator to operate at maximum speed, thereby enhancing scanning completeness while ensuring robust localization, even when LiDAR briefly scans featureless areas. The source code and hardware design are available on GitHub: \\textcolor{blue}{\\href{this https URL}{this http URL\\_calibr}}. The video is available at \\textcolor{blue}{\\href{this https URL}{this http URL}}"
    },
    {
      "title": "Resisting Manipulative Bots in Meme Coin Copy Trading: A Multi-Agent Approach with Chain-of-Thought Reasoning",
      "url": "https://arxiv.org/abs/2601.08641",
      "date": "2026-01-26",
      "authors_text": "Yichen Luo, Yebo Feng, Jiahua Xu, Yang Liu",
      "is_highlight": false,
      "score": 40.0,
      "summary": "This paper presents a multi-agent system utilizing a large language model to enhance copy trading in meme coin markets, effectively resisting manipulative bots and improving trader profitability.",
      "teaser_image": null,
      "debug_abstract": "Copy trading has become the dominant entry strategy in meme coin markets. However, due to the market&#39;s extreme illiquid and volatile nature, the strategy exposes an exploitable attack surface: adversaries deploy manipulative bots to front-run trades, conceal positions, and fabricate sentiment, systematically extracting value from naïve copiers at scale. Despite its prevalence, bot-driven manipulation remains largely unexplored, and no robust defensive framework exists. We propose a manipulation-resistant copy-trading system based on a multi-agent architecture powered by a multi-modal, explainable large language model (LLM). Our system decomposes copy trading into three specialized agents for coin evaluation, wallet selection, and timing assessment. Evaluated on historical data from over 6,000 meme coins, our approach outperforms zero-shot and most statistic-driven baselines in prediction accuracy as well as all baselines in economic performance, achieving an average return of 14% for identified smart-money trades and an estimated copier return of 3% per trade under realistic market frictions. Overall, our results demonstrate the effectiveness of agent-based defenses and predictability of trader profitability in adversarial meme coin markets, providing a practical foundation for robust copy trading."
    },
    {
      "title": "ORION: Option-Regularized Deep Reinforcement Learning for Cooperative Multi-Agent Online Navigation",
      "url": "https://arxiv.org/abs/2601.01155",
      "date": "2026-01-26",
      "authors_text": "Shizhe Zhang, Jingsong Liang, Zhitao Zhou, Shuhan Ye, Yizhuo Wang",
      "is_highlight": false,
      "score": 90.0,
      "summary": "ORION is a deep reinforcement learning framework that enables decentralized, cooperative navigation in partially known environments, enhancing agent coordination and map uncertainty reduction.",
      "teaser_image": null,
      "debug_abstract": "Existing methods for multi-agent navigation typically assume fully known environments, offering limited support for partially known scenarios such as warehouses or factory floors. There, agents may need to plan trajectories that balance their own path optimality with their ability to collect and share information about the environment that can help their teammates reach their own goals. To these ends, we propose ORION, a novel deep reinforcement learning framework for cooperative multi-agent online navigation in partially known environments. Starting from an imperfect prior map, ORION trains agents to make decentralized decisions, coordinate to reach their individual targets, and actively reduce map uncertainty by sharing online observations in a closed perception-action loop. We first design a shared graph encoder that fuses prior map with online perception into a unified representation, providing robust state embeddings under dynamic map discrepancies. At the core of ORION is an option-critic framework that learns to reason about a set of high-level cooperative modes that translate into sequences of low-level actions, allowing agents to switch between individual navigation and team-level exploration adaptively. We further introduce a dual-stage cooperation strategy that enables agents to assist teammates under map uncertainty, thereby reducing the overall makespan. Across extensive maze-like maps and large-scale warehouse environments, our simulation results show that ORION achieves high-quality, real-time decentralized cooperation over varying team sizes, outperforming state-of-the-art classical and learning-based baselines. Finally, we validate ORION on physical robot teams, demonstrating its robustness and practicality for real-world cooperative navigation."
    },
    {
      "title": "Delay-Compensated Stiffness Estimation for Robot-Mediated Dyadic Interaction",
      "url": "https://arxiv.org/abs/2601.17812",
      "date": "2026-01-25",
      "authors_text": "Mingtian Du, Suhas Raghavendra Kulkarni, Bernardo Noronha, Domenico Campolo",
      "is_highlight": false,
      "score": 80.0,
      "summary": "The paper presents a delay-compensated stiffness estimation framework for robot-mediated dyadic interactions, improving accuracy in remote physical therapy despite network-induced haptic delays.",
      "teaser_image": null,
      "debug_abstract": "Robot-mediated human-human (dyadic) interactions enable therapists to provide physical therapy remotely, yet an accurate perception of patient stiffness remains challenging due to network-induced haptic delays. Conventional stiffness estimation methods, which neglect delay, suffer from temporal misalignment between force and position signals, leading to significant estimation errors as delays increase. To address this, we propose a robust, delay-compensated stiffness estimation framework by deriving an algebraic estimator based on quasi-static equilibrium that explicitly accounts for temporally aligning the expert&#39;s input with the novice&#39;s response. A Normalised Weighted Least Squares (NWLS) implementation is then introduced to robustly filter dynamic bias resulting from the algebraic derivation. Experiments using commercial rehabilitation robots (H-MAN) as the platform demonstrate that the proposed method significantly outperforms the standard estimator, maintaining consistent tracking accuracy under multiple introduced delays. These findings offer a promising solution for achieving high-fidelity haptic perception in remote dyadic interaction, potentially facilitating reliable stiffness assessment in therapeutic settings across networks."
    },
    {
      "title": "Physical Prompt Injection Attacks on Large Vision-Language Models",
      "url": "https://arxiv.org/abs/2601.17383",
      "date": "2026-01-24",
      "authors_text": "Chen Ling, Kai Hu, Hangcheng Liu, Xingshuo Han, Tianwei Zhang",
      "is_highlight": false,
      "score": 50.0,
      "summary": "The paper introduces Physical Prompt Injection Attacks (PPIA), a novel, query-agnostic method that manipulates Large Vision-Language Models using visual prompts embedded in physical objects, achieving up to 98% success.",
      "teaser_image": null,
      "debug_abstract": "Large Vision-Language Models (LVLMs) are increasingly deployed in real-world intelligent systems for perception and reasoning in open physical environments. While LVLMs are known to be vulnerable to prompt injection attacks, existing methods either require access to input channels or depend on knowledge of user queries, assumptions that rarely hold in practical deployments. We propose the first Physical Prompt Injection Attack (PPIA), a black-box, query-agnostic attack that embeds malicious typographic instructions into physical objects perceivable by the LVLM. PPIA requires no access to the model, its inputs, or internal pipeline, and operates solely through visual observation. It combines offline selection of highly recognizable and semantically effective visual prompts with strategic environment-aware placement guided by spatiotemporal attention, ensuring that the injected prompts are both perceivable and influential on model behavior. We evaluate PPIA across 10 state-of-the-art LVLMs in both simulated and real-world settings on tasks including visual question answering, planning, and navigation, PPIA achieves attack success rates up to 98%, with strong robustness under varying physical conditions such as distance, viewpoint, and illumination. Our code is publicly available at this https URL."
    }
  ],
  "南京大学": [
    {
      "title": "GenAgent: Scaling Text-to-Image Generation via Agentic Multimodal Reasoning",
      "url": "https://arxiv.org/abs/2601.18543",
      "date": "2026-01-26",
      "authors_text": "Kaixun Jiang, Yuzheng Wang, Junjie Zhou, Pandeng Li, Zhihang Liu",
      "is_highlight": false,
      "score": 65.0,
      "summary": "GenAgent enhances text-to-image generation by decoupling understanding and generation through an agentic framework, enabling autonomous interactions and improved performance across tasks.",
      "teaser_image": null,
      "debug_abstract": "We introduce GenAgent, unifying visual understanding and generation through an agentic multimodal model. Unlike unified models that face expensive training costs and understanding-generation trade-offs, GenAgent decouples these capabilities through an agentic framework: understanding is handled by the multimodal model itself, while generation is achieved by treating image generation models as invokable tools. Crucially, unlike existing modular systems constrained by static pipelines, this design enables autonomous multi-turn interactions where the agent generates multimodal chains-of-thought encompassing reasoning, tool invocation, judgment, and reflection to iteratively refine outputs. We employ a two-stage training strategy: first, cold-start with supervised fine-tuning on high-quality tool invocation and reflection data to bootstrap agent behaviors; second, end-to-end agentic reinforcement learning combining pointwise rewards (final image quality) and pairwise rewards (reflection accuracy), with trajectory resampling for enhanced multi-turn exploration. GenAgent significantly boosts base generator(FLUX.1-dev) performance on GenEval++ (+23.6\\%) and WISE (+14\\%). Beyond performance gains, our framework demonstrates three key properties: 1) cross-tool generalization to generators with varying capabilities, 2) test-time scaling with consistent improvements across interaction rounds, and 3) task-adaptive reasoning that automatically adjusts to different tasks. Our code will be available at \\href{this https URL}{this url}."
    },
    {
      "title": "PEAfowl: Perception-Enhanced Multi-View Vision-Language-Action for Bimanual Manipulation",
      "url": "https://arxiv.org/abs/2601.17885",
      "date": "2026-01-25",
      "authors_text": "Qingyu Fan, Zhaoxiang Li, Yi Lu, Wang Chen, Qiu Shen",
      "is_highlight": false,
      "score": 95.0,
      "summary": "PEAfowl enhances bimanual manipulation by integrating perception-driven multi-view features and improved instruction grounding, achieving significant performance gains in both simulated and real environments.",
      "teaser_image": null,
      "debug_abstract": "Bimanual manipulation in cluttered scenes requires policies that remain stable under occlusions, viewpoint and scene variations. Existing vision-language-action models often fail to generalize because (i) multi-view features are fused via view-agnostic token concatenation, yielding weak 3D-consistent spatial understanding, and (ii) language is injected as global conditioning, resulting in coarse instruction grounding. In this paper, we introduce PEAfowl, a perception-enhanced multi-view VLA policy for bimanual manipulation. For spatial reasoning, PEAfowl predicts per-token depth distributions, performs differentiable 3D lifting, and aggregates local cross-view neighbors to form geometrically grounded, cross-view consistent representations. For instruction grounding, we propose to replace global conditioning with a Perceiver-style text-aware readout over frozen CLIP visual features, enabling iterative evidence accumulation. To overcome noisy and incomplete commodity depth without adding inference overhead, we apply training-only depth distillation from a pretrained depth teacher to supervise the depth-distribution head, providing perception front-end with geometry-aware priors. On RoboTwin 2.0 under domain-randomized setting, PEAfowl improves the strongest baseline by 23.0 pp in success rate, and real-robot experiments further demonstrate reliable sim-to-real transfer and consistent improvements from depth distillation. Project website: this https URL."
    },
    {
      "title": "Flatten The Complex: Joint B-Rep Generation via Compositional $k$-Cell Particles",
      "url": "https://arxiv.org/abs/2601.17733",
      "date": "2026-01-25",
      "authors_text": "Junran Lu, Yuanqi Li, Hengji Li, Jie Guo, Yanwen Guo",
      "is_highlight": false,
      "score": 35.0,
      "summary": "This paper presents a novel method for generating Boundary Representations (B-Reps) using compositional $k$-cell particles, enhancing topology-geometry relationships and enabling improved CAD model synthesis.",
      "teaser_image": null,
      "debug_abstract": "Boundary Representation (B-Rep) is the widely adopted standard in Computer-Aided Design (CAD) and manufacturing. However, generative modeling of B-Reps remains a formidable challenge due to their inherent heterogeneity as geometric cell complexes, which entangles topology with geometry across cells of varying orders (i.e., $k$-cells such as vertices, edges, faces). Previous methods typically rely on cascaded sequences to handle this hierarchy, which fails to fully exploit the geometric relationships between cells, such as adjacency and sharing, limiting context awareness and error recovery. To fill this gap, we introduce a novel paradigm that reformulates B-Reps into sets of compositional $k$-cell particles. Our approach encodes each topological entity as a composition of particles, where adjacent cells share identical latents at their interfaces, thereby promoting geometric coupling along shared boundaries. By decoupling the rigid hierarchy, our representation unifies vertices, edges, and faces, enabling the joint generation of topology and geometry with global context awareness. We synthesize these particle sets using a multi-modal flow matching framework to handle unconditional generation as well as precise conditional tasks, such as 3D reconstruction from single-view or point cloud. Furthermore, the explicit and localized nature of our representation naturally extends to downstream tasks like local in-painting and enables the direct synthesis of non-manifold structures (e.g., wireframes). Extensive experiments demonstrate that our method produces high-fidelity CAD models with superior validity and editability compared to state-of-the-art methods."
    },
    {
      "title": "Physical Prompt Injection Attacks on Large Vision-Language Models",
      "url": "https://arxiv.org/abs/2601.17383",
      "date": "2026-01-24",
      "authors_text": "Chen Ling, Kai Hu, Hangcheng Liu, Xingshuo Han, Tianwei Zhang",
      "is_highlight": false,
      "score": 50.0,
      "summary": "The paper introduces Physical Prompt Injection Attacks (PPIA), a novel, query-agnostic method that manipulates Large Vision-Language Models using visual prompts embedded in physical objects, achieving up to 98% success.",
      "teaser_image": null,
      "debug_abstract": "Large Vision-Language Models (LVLMs) are increasingly deployed in real-world intelligent systems for perception and reasoning in open physical environments. While LVLMs are known to be vulnerable to prompt injection attacks, existing methods either require access to input channels or depend on knowledge of user queries, assumptions that rarely hold in practical deployments. We propose the first Physical Prompt Injection Attack (PPIA), a black-box, query-agnostic attack that embeds malicious typographic instructions into physical objects perceivable by the LVLM. PPIA requires no access to the model, its inputs, or internal pipeline, and operates solely through visual observation. It combines offline selection of highly recognizable and semantically effective visual prompts with strategic environment-aware placement guided by spatiotemporal attention, ensuring that the injected prompts are both perceivable and influential on model behavior. We evaluate PPIA across 10 state-of-the-art LVLMs in both simulated and real-world settings on tasks including visual question answering, planning, and navigation, PPIA achieves attack success rates up to 98%, with strong robustness under varying physical conditions such as distance, viewpoint, and illumination. Our code is publicly available at this https URL."
    }
  ],
  "武汉大学": [
    {
      "title": "EFSI-DETR: Efficient Frequency-Semantic Integration for Real-Time Small Object Detection in UAV Imagery",
      "url": "https://arxiv.org/abs/2601.18597",
      "date": "2026-01-26",
      "authors_text": "Yu Xia, Chang Liu, Tianqi Xiang, Zhigang Tu",
      "is_highlight": false,
      "score": 45.0,
      "summary": "EFSI-DETR enhances real-time small object detection in UAV imagery through dynamic frequency-spatial integration and efficient semantic feature extraction, achieving state-of-the-art performance with high inference speed.",
      "teaser_image": null,
      "debug_abstract": "Real-time small object detection in Unmanned Aerial Vehicle (UAV) imagery remains challenging due to limited feature representation and ineffective multi-scale fusion. Existing methods underutilize frequency information and rely on static convolutional operations, which constrain the capacity to obtain rich feature representations and hinder the effective exploitation of deep semantic features. To address these issues, we propose EFSI-DETR, a novel detection framework that integrates efficient semantic feature enhancement with dynamic frequency-spatial guidance. EFSI-DETR comprises two main components: (1) a Dynamic Frequency-Spatial Unified Synergy Network (DyFusNet) that jointly exploits frequency and spatial cues for robust multi-scale feature fusion, (2) an Efficient Semantic Feature Concentrator (ESFC) that enables deep semantic extraction with minimal computational cost. Furthermore, a Fine-grained Feature Retention (FFR) strategy is adopted to incorporate spatially rich shallow features during fusion to preserve fine-grained details, crucial for small object detection in UAV imagery. Extensive experiments on VisDrone and CODrone benchmarks demonstrate that our EFSI-DETR achieves the state-of-the-art performance with real-time efficiency, yielding improvement of \\textbf{1.6}\\% and \\textbf{5.8}\\% in AP and AP$_{s}$ on VisDrone, while obtaining \\textbf{188} FPS inference speed on a single RTX 4090 GPU."
    },
    {
      "title": "AnchoredDream: Zero-Shot 360° Indoor Scene Generation from a Single View via Geometric Grounding",
      "url": "https://arxiv.org/abs/2601.16532",
      "date": "2026-01-26",
      "authors_text": "Runmao Yao, Junsheng Zhou, Zhen Dong, Yu-Shen Liu",
      "is_highlight": false,
      "score": 45.0,
      "summary": "AnchoredDream introduces a zero-shot method for generating 360° indoor scenes from a single image, enhancing appearance consistency and geometric plausibility through innovative geometry anchoring techniques.",
      "teaser_image": null,
      "debug_abstract": "Single-view indoor scene generation plays a crucial role in a range of real-world applications. However, generating a complete 360° scene from a single image remains a highly ill-posed and challenging problem. Recent approaches have made progress by leveraging diffusion models and depth estimation networks, yet they still struggle to maintain appearance consistency and geometric plausibility under large viewpoint changes, limiting their effectiveness in full-scene generation. To address this, we propose AnchoredDream, a novel zero-shot pipeline that anchors 360° scene generation on high-fidelity geometry via an appearance-geometry mutual boosting mechanism. Given a single-view image, our method first performs appearance-guided geometry generation to construct a reliable 3D scene layout. Then, we progressively generate the complete scene through a series of modules: warp-and-inpaint, warp-and-refine, post-optimization, and a novel Grouting Block, which ensures seamless transitions between the input view and generated regions. Extensive experiments demonstrate that AnchoredDream outperforms existing methods by a large margin in both appearance consistency and geometric plausibility--all in a zero-shot manner. Our results highlight the potential of geometric grounding for high-quality, zero-shot single-view scene generation."
    },
    {
      "title": "Physical Prompt Injection Attacks on Large Vision-Language Models",
      "url": "https://arxiv.org/abs/2601.17383",
      "date": "2026-01-24",
      "authors_text": "Chen Ling, Kai Hu, Hangcheng Liu, Xingshuo Han, Tianwei Zhang",
      "is_highlight": false,
      "score": 50.0,
      "summary": "The paper introduces Physical Prompt Injection Attacks (PPIA), a novel, query-agnostic method that manipulates Large Vision-Language Models using visual prompts embedded in physical objects, achieving up to 98% success.",
      "teaser_image": null,
      "debug_abstract": "Large Vision-Language Models (LVLMs) are increasingly deployed in real-world intelligent systems for perception and reasoning in open physical environments. While LVLMs are known to be vulnerable to prompt injection attacks, existing methods either require access to input channels or depend on knowledge of user queries, assumptions that rarely hold in practical deployments. We propose the first Physical Prompt Injection Attack (PPIA), a black-box, query-agnostic attack that embeds malicious typographic instructions into physical objects perceivable by the LVLM. PPIA requires no access to the model, its inputs, or internal pipeline, and operates solely through visual observation. It combines offline selection of highly recognizable and semantically effective visual prompts with strategic environment-aware placement guided by spatiotemporal attention, ensuring that the injected prompts are both perceivable and influential on model behavior. We evaluate PPIA across 10 state-of-the-art LVLMs in both simulated and real-world settings on tasks including visual question answering, planning, and navigation, PPIA achieves attack success rates up to 98%, with strong robustness under varying physical conditions such as distance, viewpoint, and illumination. Our code is publicly available at this https URL."
    }
  ],
  "范明明老师实验室": [
    {
      "title": "Fast and Safe Trajectory Optimization for Mobile Manipulators With Neural Configuration Space Distance Field",
      "url": "https://arxiv.org/abs/2601.18548",
      "date": "2026-01-26",
      "authors_text": "Yulin Li, Zhiyuan Song, Yiming Li, Zhicheng Song, Kai Chen",
      "is_highlight": false,
      "score": 88.0,
      "summary": "The paper presents Generalized Configuration Space Distance Fields (GCDF) for efficient trajectory optimization in mobile manipulators, enhancing collision reasoning and enabling agile motion in complex environments.",
      "teaser_image": null,
      "debug_abstract": "Mobile manipulators promise agile, long-horizon behavior by coordinating base and arm motion, yet whole-body trajectory optimization in cluttered, confined spaces remains difficult due to high-dimensional nonconvexity and the need for fast, accurate collision reasoning. Configuration Space Distance Fields (CDF) enable fixed-base manipulators to model collisions directly in configuration space via smooth, implicit distances. This representation holds strong potential to bypass the nonlinear configuration-to-workspace mapping while preserving accurate whole-body geometry and providing optimization-friendly collision costs. Yet, extending this capability to mobile manipulators is hindered by unbounded workspaces and tighter base-arm coupling. We lift this promise to mobile manipulation with Generalized Configuration Space Distance Fields (GCDF), extending CDF to robots with both translational and rotational joints in unbounded workspaces with tighter base-arm coupling. We prove that GCDF preserves Euclidean-like local distance structure and accurately encodes whole-body geometry in configuration space, and develop a data generation and training pipeline that yields continuous neural GCDFs with accurate values and gradients, supporting efficient GPU-batched queries. Building on this representation, we develop a high-performance sequential convex optimization framework centered on GCDF-based collision reasoning. The solver scales to large numbers of implicit constraints through (i) online specification of neural constraints, (ii) sparsity-aware active-set detection with parallel batched evaluation across thousands of constraints, and (iii) incremental constraint management for rapid replanning under scene changes."
    },
    {
      "title": "TC-IDM: Grounding Video Generation for Executable Zero-shot Robot Motion",
      "url": "https://arxiv.org/abs/2601.18323",
      "date": "2026-01-26",
      "authors_text": "Weishi Mi, Yong Bao, Xiaowei Chi, Xiaozhu Ju, Zhiyuan Qin",
      "is_highlight": false,
      "score": 90.0,
      "summary": "The TC-IDM framework enhances robotic control by bridging visual planning and physical execution through tool-centric trajectory modeling, achieving superior generalization and performance in diverse tasks.",
      "teaser_image": null,
      "debug_abstract": "The vision-language-action (VLA) paradigm has enabled powerful robotic control by leveraging vision-language models, but its reliance on large-scale, high-quality robot data limits its generalization. Generative world models offer a promising alternative for general-purpose embodied AI, yet a critical gap remains between their pixel-level plans and physically executable actions. To this end, we propose the Tool-Centric Inverse Dynamics Model (TC-IDM). By focusing on the tool&#39;s imagined trajectory as synthesized by the world model, TC-IDM establishes a robust intermediate representation that bridges the gap between visual planning and physical control. TC-IDM extracts the tool&#39;s point cloud trajectories via segmentation and 3D motion estimation from generated videos. Considering diverse tool attributes, our architecture employs decoupled action heads to project these planned trajectories into 6-DoF end-effector motions and corresponding control signals. This plan-and-translate paradigm not only supports a wide range of end-effectors but also significantly improves viewpoint invariance. Furthermore, it exhibits strong generalization capabilities across long-horizon and out-of-distribution tasks, including interacting with deformable objects. In real-world evaluations, the world model with TC-IDM achieves an average success rate of 61.11 percent, with 77.7 percent on simple tasks and 38.46 percent on zero-shot deformable object tasks. It substantially outperforms end-to-end VLA-style baselines and other inverse dynamics models."
    },
    {
      "title": "Uni-RS: A Spatially Faithful Unified Understanding and Generation Model for Remote Sensing",
      "url": "https://arxiv.org/abs/2601.17673",
      "date": "2026-01-25",
      "authors_text": "Weiyu Zhang, Yuan Hu, Yong Li, Yu Liu",
      "is_highlight": false,
      "score": 40.0,
      "summary": "Uni-RS is a novel multimodal model for remote sensing that enhances spatial fidelity in text-to-image generation by integrating spatial layout planning and query supervision.",
      "teaser_image": null,
      "debug_abstract": "Unified remote sensing multimodal models exhibit a pronounced spatial reversal curse: Although they can accurately recognize and describe object locations in images, they often fail to faithfully execute the same spatial relations during text-to-image generation, where such relations constitute core semantic information in remote sensing. Motivated by this observation, we propose Uni-RS, the first unified multimodal model tailored for remote sensing, to explicitly address the spatial asymmetry between understanding and generation. Specifically, we first introduce explicit Spatial-Layout Planning to transform textual instructions into spatial layout plans, decoupling geometric planning from visual synthesis. We then impose Spatial-Aware Query Supervision to bias learnable queries toward spatial relations explicitly specified in the instruction. Finally, we develop Image-Caption Spatial Layout Variation to expose the model to systematic geometry-consistent spatial transformations. Extensive experiments across multiple benchmarks show that our approach substantially improves spatial faithfulness in text-to-image generation, while maintaining strong performance on multimodal understanding tasks like image captioning, visual grounding, and VQA tasks."
    },
    {
      "title": "CoT-Seg: Rethinking Segmentation with Chain-of-Thought Reasoning and Self-Correction",
      "url": "https://arxiv.org/abs/2601.17420",
      "date": "2026-01-24",
      "authors_text": "Shiu-hong Kao, Chak Ho Huang, Huaiqian Liu, Yu-Wing Tai, Chi-Keung Tang",
      "is_highlight": false,
      "score": 55.0,
      "summary": "CoT-Seg is a novel framework that enhances segmentation by integrating chain-of-thought reasoning and self-correction, improving performance on complex queries and ambiguous images.",
      "teaser_image": null,
      "debug_abstract": "Existing works of reasoning segmentation often fall short in complex cases, particularly when addressing complicated queries and out-of-domain images. Inspired by the chain-of-thought reasoning, where harder problems require longer thinking steps/time, this paper aims to explore a system that can think step-by-step, look up information if needed, generate results, self-evaluate its own results, and refine the results, in the same way humans approach harder questions. We introduce CoT-Seg, a training-free framework that rethinks reasoning segmentation by combining chain-of-thought reasoning with self-correction. Instead of fine-tuning, CoT-Seg leverages the inherent reasoning ability of pre-trained MLLMs (GPT-4o) to decompose queries into meta-instructions, extract fine-grained semantics from images, and identify target objects even under implicit or complex prompts. Moreover, CoT-Seg incorporates a self-correction stage: the model evaluates its own segmentation against the original query and reasoning trace, identifies mismatches, and iteratively refines the mask. This tight integration of reasoning and correction significantly improves reliability and robustness, especially in ambiguous or error-prone cases. Furthermore, our CoT-Seg framework allows easy incorporation of retrieval-augmented reasoning, enabling the system to access external knowledge when the input lacks sufficient information. To showcase CoT-Seg&#39;s ability to handle very challenging cases ,we introduce a new dataset ReasonSeg-Hard. Our results highlight that combining chain-of-thought reasoning, self-correction, offers a powerful paradigm for vision-language integration driven segmentation."
    }
  ],
  "Precognition Lab": [
    {
      "title": "Fast and Safe Trajectory Optimization for Mobile Manipulators With Neural Configuration Space Distance Field",
      "url": "https://arxiv.org/abs/2601.18548",
      "date": "2026-01-26",
      "authors_text": "Yulin Li, Zhiyuan Song, Yiming Li, Zhicheng Song, Kai Chen",
      "is_highlight": false,
      "score": 88.0,
      "summary": "The paper presents Generalized Configuration Space Distance Fields (GCDF) for efficient trajectory optimization in mobile manipulators, enhancing collision reasoning and enabling agile motion in complex environments.",
      "teaser_image": null,
      "debug_abstract": "Mobile manipulators promise agile, long-horizon behavior by coordinating base and arm motion, yet whole-body trajectory optimization in cluttered, confined spaces remains difficult due to high-dimensional nonconvexity and the need for fast, accurate collision reasoning. Configuration Space Distance Fields (CDF) enable fixed-base manipulators to model collisions directly in configuration space via smooth, implicit distances. This representation holds strong potential to bypass the nonlinear configuration-to-workspace mapping while preserving accurate whole-body geometry and providing optimization-friendly collision costs. Yet, extending this capability to mobile manipulators is hindered by unbounded workspaces and tighter base-arm coupling. We lift this promise to mobile manipulation with Generalized Configuration Space Distance Fields (GCDF), extending CDF to robots with both translational and rotational joints in unbounded workspaces with tighter base-arm coupling. We prove that GCDF preserves Euclidean-like local distance structure and accurately encodes whole-body geometry in configuration space, and develop a data generation and training pipeline that yields continuous neural GCDFs with accurate values and gradients, supporting efficient GPU-batched queries. Building on this representation, we develop a high-performance sequential convex optimization framework centered on GCDF-based collision reasoning. The solver scales to large numbers of implicit constraints through (i) online specification of neural constraints, (ii) sparsity-aware active-set detection with parallel batched evaluation across thousands of constraints, and (iii) incremental constraint management for rapid replanning under scene changes."
    },
    {
      "title": "TC-IDM: Grounding Video Generation for Executable Zero-shot Robot Motion",
      "url": "https://arxiv.org/abs/2601.18323",
      "date": "2026-01-26",
      "authors_text": "Weishi Mi, Yong Bao, Xiaowei Chi, Xiaozhu Ju, Zhiyuan Qin",
      "is_highlight": false,
      "score": 90.0,
      "summary": "The TC-IDM framework enhances robotic control by bridging visual planning and physical execution through tool-centric trajectory modeling, achieving superior generalization and performance in diverse tasks.",
      "teaser_image": null,
      "debug_abstract": "The vision-language-action (VLA) paradigm has enabled powerful robotic control by leveraging vision-language models, but its reliance on large-scale, high-quality robot data limits its generalization. Generative world models offer a promising alternative for general-purpose embodied AI, yet a critical gap remains between their pixel-level plans and physically executable actions. To this end, we propose the Tool-Centric Inverse Dynamics Model (TC-IDM). By focusing on the tool&#39;s imagined trajectory as synthesized by the world model, TC-IDM establishes a robust intermediate representation that bridges the gap between visual planning and physical control. TC-IDM extracts the tool&#39;s point cloud trajectories via segmentation and 3D motion estimation from generated videos. Considering diverse tool attributes, our architecture employs decoupled action heads to project these planned trajectories into 6-DoF end-effector motions and corresponding control signals. This plan-and-translate paradigm not only supports a wide range of end-effectors but also significantly improves viewpoint invariance. Furthermore, it exhibits strong generalization capabilities across long-horizon and out-of-distribution tasks, including interacting with deformable objects. In real-world evaluations, the world model with TC-IDM achieves an average success rate of 61.11 percent, with 77.7 percent on simple tasks and 38.46 percent on zero-shot deformable object tasks. It substantially outperforms end-to-end VLA-style baselines and other inverse dynamics models."
    },
    {
      "title": "Uni-RS: A Spatially Faithful Unified Understanding and Generation Model for Remote Sensing",
      "url": "https://arxiv.org/abs/2601.17673",
      "date": "2026-01-25",
      "authors_text": "Weiyu Zhang, Yuan Hu, Yong Li, Yu Liu",
      "is_highlight": false,
      "score": 40.0,
      "summary": "Uni-RS is a novel multimodal model for remote sensing that enhances spatial fidelity in text-to-image generation by integrating spatial layout planning and query supervision.",
      "teaser_image": null,
      "debug_abstract": "Unified remote sensing multimodal models exhibit a pronounced spatial reversal curse: Although they can accurately recognize and describe object locations in images, they often fail to faithfully execute the same spatial relations during text-to-image generation, where such relations constitute core semantic information in remote sensing. Motivated by this observation, we propose Uni-RS, the first unified multimodal model tailored for remote sensing, to explicitly address the spatial asymmetry between understanding and generation. Specifically, we first introduce explicit Spatial-Layout Planning to transform textual instructions into spatial layout plans, decoupling geometric planning from visual synthesis. We then impose Spatial-Aware Query Supervision to bias learnable queries toward spatial relations explicitly specified in the instruction. Finally, we develop Image-Caption Spatial Layout Variation to expose the model to systematic geometry-consistent spatial transformations. Extensive experiments across multiple benchmarks show that our approach substantially improves spatial faithfulness in text-to-image generation, while maintaining strong performance on multimodal understanding tasks like image captioning, visual grounding, and VQA tasks."
    },
    {
      "title": "CoT-Seg: Rethinking Segmentation with Chain-of-Thought Reasoning and Self-Correction",
      "url": "https://arxiv.org/abs/2601.17420",
      "date": "2026-01-24",
      "authors_text": "Shiu-hong Kao, Chak Ho Huang, Huaiqian Liu, Yu-Wing Tai, Chi-Keung Tang",
      "is_highlight": false,
      "score": 55.0,
      "summary": "CoT-Seg is a novel framework that enhances segmentation by integrating chain-of-thought reasoning and self-correction, improving performance on complex queries and ambiguous images.",
      "teaser_image": null,
      "debug_abstract": "Existing works of reasoning segmentation often fall short in complex cases, particularly when addressing complicated queries and out-of-domain images. Inspired by the chain-of-thought reasoning, where harder problems require longer thinking steps/time, this paper aims to explore a system that can think step-by-step, look up information if needed, generate results, self-evaluate its own results, and refine the results, in the same way humans approach harder questions. We introduce CoT-Seg, a training-free framework that rethinks reasoning segmentation by combining chain-of-thought reasoning with self-correction. Instead of fine-tuning, CoT-Seg leverages the inherent reasoning ability of pre-trained MLLMs (GPT-4o) to decompose queries into meta-instructions, extract fine-grained semantics from images, and identify target objects even under implicit or complex prompts. Moreover, CoT-Seg incorporates a self-correction stage: the model evaluates its own segmentation against the original query and reasoning trace, identifies mismatches, and iteratively refines the mask. This tight integration of reasoning and correction significantly improves reliability and robustness, especially in ambiguous or error-prone cases. Furthermore, our CoT-Seg framework allows easy incorporation of retrieval-augmented reasoning, enabling the system to access external knowledge when the input lacks sufficient information. To showcase CoT-Seg&#39;s ability to handle very challenging cases ,we introduce a new dataset ReasonSeg-Hard. Our results highlight that combining chain-of-thought reasoning, self-correction, offers a powerful paradigm for vision-language integration driven segmentation."
    }
  ],
  "机器人研究所 (CKSRI)": [
    {
      "title": "Fast and Safe Trajectory Optimization for Mobile Manipulators With Neural Configuration Space Distance Field",
      "url": "https://arxiv.org/abs/2601.18548",
      "date": "2026-01-26",
      "authors_text": "Yulin Li, Zhiyuan Song, Yiming Li, Zhicheng Song, Kai Chen",
      "is_highlight": false,
      "score": 88.0,
      "summary": "The paper presents Generalized Configuration Space Distance Fields (GCDF) for efficient trajectory optimization in mobile manipulators, enhancing collision reasoning and enabling agile motion in complex environments.",
      "teaser_image": null,
      "debug_abstract": "Mobile manipulators promise agile, long-horizon behavior by coordinating base and arm motion, yet whole-body trajectory optimization in cluttered, confined spaces remains difficult due to high-dimensional nonconvexity and the need for fast, accurate collision reasoning. Configuration Space Distance Fields (CDF) enable fixed-base manipulators to model collisions directly in configuration space via smooth, implicit distances. This representation holds strong potential to bypass the nonlinear configuration-to-workspace mapping while preserving accurate whole-body geometry and providing optimization-friendly collision costs. Yet, extending this capability to mobile manipulators is hindered by unbounded workspaces and tighter base-arm coupling. We lift this promise to mobile manipulation with Generalized Configuration Space Distance Fields (GCDF), extending CDF to robots with both translational and rotational joints in unbounded workspaces with tighter base-arm coupling. We prove that GCDF preserves Euclidean-like local distance structure and accurately encodes whole-body geometry in configuration space, and develop a data generation and training pipeline that yields continuous neural GCDFs with accurate values and gradients, supporting efficient GPU-batched queries. Building on this representation, we develop a high-performance sequential convex optimization framework centered on GCDF-based collision reasoning. The solver scales to large numbers of implicit constraints through (i) online specification of neural constraints, (ii) sparsity-aware active-set detection with parallel batched evaluation across thousands of constraints, and (iii) incremental constraint management for rapid replanning under scene changes."
    },
    {
      "title": "TC-IDM: Grounding Video Generation for Executable Zero-shot Robot Motion",
      "url": "https://arxiv.org/abs/2601.18323",
      "date": "2026-01-26",
      "authors_text": "Weishi Mi, Yong Bao, Xiaowei Chi, Xiaozhu Ju, Zhiyuan Qin",
      "is_highlight": false,
      "score": 90.0,
      "summary": "The TC-IDM framework enhances robotic control by bridging visual planning and physical execution through tool-centric trajectory modeling, achieving superior generalization and performance in diverse tasks.",
      "teaser_image": null,
      "debug_abstract": "The vision-language-action (VLA) paradigm has enabled powerful robotic control by leveraging vision-language models, but its reliance on large-scale, high-quality robot data limits its generalization. Generative world models offer a promising alternative for general-purpose embodied AI, yet a critical gap remains between their pixel-level plans and physically executable actions. To this end, we propose the Tool-Centric Inverse Dynamics Model (TC-IDM). By focusing on the tool&#39;s imagined trajectory as synthesized by the world model, TC-IDM establishes a robust intermediate representation that bridges the gap between visual planning and physical control. TC-IDM extracts the tool&#39;s point cloud trajectories via segmentation and 3D motion estimation from generated videos. Considering diverse tool attributes, our architecture employs decoupled action heads to project these planned trajectories into 6-DoF end-effector motions and corresponding control signals. This plan-and-translate paradigm not only supports a wide range of end-effectors but also significantly improves viewpoint invariance. Furthermore, it exhibits strong generalization capabilities across long-horizon and out-of-distribution tasks, including interacting with deformable objects. In real-world evaluations, the world model with TC-IDM achieves an average success rate of 61.11 percent, with 77.7 percent on simple tasks and 38.46 percent on zero-shot deformable object tasks. It substantially outperforms end-to-end VLA-style baselines and other inverse dynamics models."
    },
    {
      "title": "Uni-RS: A Spatially Faithful Unified Understanding and Generation Model for Remote Sensing",
      "url": "https://arxiv.org/abs/2601.17673",
      "date": "2026-01-25",
      "authors_text": "Weiyu Zhang, Yuan Hu, Yong Li, Yu Liu",
      "is_highlight": false,
      "score": 40.0,
      "summary": "Uni-RS is a novel multimodal model for remote sensing that enhances spatial fidelity in text-to-image generation by integrating spatial layout planning and query supervision.",
      "teaser_image": null,
      "debug_abstract": "Unified remote sensing multimodal models exhibit a pronounced spatial reversal curse: Although they can accurately recognize and describe object locations in images, they often fail to faithfully execute the same spatial relations during text-to-image generation, where such relations constitute core semantic information in remote sensing. Motivated by this observation, we propose Uni-RS, the first unified multimodal model tailored for remote sensing, to explicitly address the spatial asymmetry between understanding and generation. Specifically, we first introduce explicit Spatial-Layout Planning to transform textual instructions into spatial layout plans, decoupling geometric planning from visual synthesis. We then impose Spatial-Aware Query Supervision to bias learnable queries toward spatial relations explicitly specified in the instruction. Finally, we develop Image-Caption Spatial Layout Variation to expose the model to systematic geometry-consistent spatial transformations. Extensive experiments across multiple benchmarks show that our approach substantially improves spatial faithfulness in text-to-image generation, while maintaining strong performance on multimodal understanding tasks like image captioning, visual grounding, and VQA tasks."
    },
    {
      "title": "CoT-Seg: Rethinking Segmentation with Chain-of-Thought Reasoning and Self-Correction",
      "url": "https://arxiv.org/abs/2601.17420",
      "date": "2026-01-24",
      "authors_text": "Shiu-hong Kao, Chak Ho Huang, Huaiqian Liu, Yu-Wing Tai, Chi-Keung Tang",
      "is_highlight": false,
      "score": 55.0,
      "summary": "CoT-Seg is a novel framework that enhances segmentation by integrating chain-of-thought reasoning and self-correction, improving performance on complex queries and ambiguous images.",
      "teaser_image": null,
      "debug_abstract": "Existing works of reasoning segmentation often fall short in complex cases, particularly when addressing complicated queries and out-of-domain images. Inspired by the chain-of-thought reasoning, where harder problems require longer thinking steps/time, this paper aims to explore a system that can think step-by-step, look up information if needed, generate results, self-evaluate its own results, and refine the results, in the same way humans approach harder questions. We introduce CoT-Seg, a training-free framework that rethinks reasoning segmentation by combining chain-of-thought reasoning with self-correction. Instead of fine-tuning, CoT-Seg leverages the inherent reasoning ability of pre-trained MLLMs (GPT-4o) to decompose queries into meta-instructions, extract fine-grained semantics from images, and identify target objects even under implicit or complex prompts. Moreover, CoT-Seg incorporates a self-correction stage: the model evaluates its own segmentation against the original query and reasoning trace, identifies mismatches, and iteratively refines the mask. This tight integration of reasoning and correction significantly improves reliability and robustness, especially in ambiguous or error-prone cases. Furthermore, our CoT-Seg framework allows easy incorporation of retrieval-augmented reasoning, enabling the system to access external knowledge when the input lacks sufficient information. To showcase CoT-Seg&#39;s ability to handle very challenging cases ,we introduce a new dataset ReasonSeg-Hard. Our results highlight that combining chain-of-thought reasoning, self-correction, offers a powerful paradigm for vision-language integration driven segmentation."
    }
  ],
  "机器人研究所": [
    {
      "title": "Fast and Safe Trajectory Optimization for Mobile Manipulators With Neural Configuration Space Distance Field",
      "url": "https://arxiv.org/abs/2601.18548",
      "date": "2026-01-26",
      "authors_text": "Yulin Li, Zhiyuan Song, Yiming Li, Zhicheng Song, Kai Chen",
      "is_highlight": false,
      "score": 88.0,
      "summary": "The paper presents Generalized Configuration Space Distance Fields (GCDF) for efficient trajectory optimization in mobile manipulators, enhancing collision reasoning and enabling agile motion in complex environments.",
      "teaser_image": null,
      "debug_abstract": "Mobile manipulators promise agile, long-horizon behavior by coordinating base and arm motion, yet whole-body trajectory optimization in cluttered, confined spaces remains difficult due to high-dimensional nonconvexity and the need for fast, accurate collision reasoning. Configuration Space Distance Fields (CDF) enable fixed-base manipulators to model collisions directly in configuration space via smooth, implicit distances. This representation holds strong potential to bypass the nonlinear configuration-to-workspace mapping while preserving accurate whole-body geometry and providing optimization-friendly collision costs. Yet, extending this capability to mobile manipulators is hindered by unbounded workspaces and tighter base-arm coupling. We lift this promise to mobile manipulation with Generalized Configuration Space Distance Fields (GCDF), extending CDF to robots with both translational and rotational joints in unbounded workspaces with tighter base-arm coupling. We prove that GCDF preserves Euclidean-like local distance structure and accurately encodes whole-body geometry in configuration space, and develop a data generation and training pipeline that yields continuous neural GCDFs with accurate values and gradients, supporting efficient GPU-batched queries. Building on this representation, we develop a high-performance sequential convex optimization framework centered on GCDF-based collision reasoning. The solver scales to large numbers of implicit constraints through (i) online specification of neural constraints, (ii) sparsity-aware active-set detection with parallel batched evaluation across thousands of constraints, and (iii) incremental constraint management for rapid replanning under scene changes."
    },
    {
      "title": "TC-IDM: Grounding Video Generation for Executable Zero-shot Robot Motion",
      "url": "https://arxiv.org/abs/2601.18323",
      "date": "2026-01-26",
      "authors_text": "Weishi Mi, Yong Bao, Xiaowei Chi, Xiaozhu Ju, Zhiyuan Qin",
      "is_highlight": false,
      "score": 90.0,
      "summary": "The TC-IDM framework enhances robotic control by bridging visual planning and physical execution through tool-centric trajectory modeling, achieving superior generalization and performance in diverse tasks.",
      "teaser_image": null,
      "debug_abstract": "The vision-language-action (VLA) paradigm has enabled powerful robotic control by leveraging vision-language models, but its reliance on large-scale, high-quality robot data limits its generalization. Generative world models offer a promising alternative for general-purpose embodied AI, yet a critical gap remains between their pixel-level plans and physically executable actions. To this end, we propose the Tool-Centric Inverse Dynamics Model (TC-IDM). By focusing on the tool&#39;s imagined trajectory as synthesized by the world model, TC-IDM establishes a robust intermediate representation that bridges the gap between visual planning and physical control. TC-IDM extracts the tool&#39;s point cloud trajectories via segmentation and 3D motion estimation from generated videos. Considering diverse tool attributes, our architecture employs decoupled action heads to project these planned trajectories into 6-DoF end-effector motions and corresponding control signals. This plan-and-translate paradigm not only supports a wide range of end-effectors but also significantly improves viewpoint invariance. Furthermore, it exhibits strong generalization capabilities across long-horizon and out-of-distribution tasks, including interacting with deformable objects. In real-world evaluations, the world model with TC-IDM achieves an average success rate of 61.11 percent, with 77.7 percent on simple tasks and 38.46 percent on zero-shot deformable object tasks. It substantially outperforms end-to-end VLA-style baselines and other inverse dynamics models."
    },
    {
      "title": "Uni-RS: A Spatially Faithful Unified Understanding and Generation Model for Remote Sensing",
      "url": "https://arxiv.org/abs/2601.17673",
      "date": "2026-01-25",
      "authors_text": "Weiyu Zhang, Yuan Hu, Yong Li, Yu Liu",
      "is_highlight": false,
      "score": 40.0,
      "summary": "Uni-RS is a novel multimodal model for remote sensing that enhances spatial fidelity in text-to-image generation by integrating spatial layout planning and query supervision.",
      "teaser_image": null,
      "debug_abstract": "Unified remote sensing multimodal models exhibit a pronounced spatial reversal curse: Although they can accurately recognize and describe object locations in images, they often fail to faithfully execute the same spatial relations during text-to-image generation, where such relations constitute core semantic information in remote sensing. Motivated by this observation, we propose Uni-RS, the first unified multimodal model tailored for remote sensing, to explicitly address the spatial asymmetry between understanding and generation. Specifically, we first introduce explicit Spatial-Layout Planning to transform textual instructions into spatial layout plans, decoupling geometric planning from visual synthesis. We then impose Spatial-Aware Query Supervision to bias learnable queries toward spatial relations explicitly specified in the instruction. Finally, we develop Image-Caption Spatial Layout Variation to expose the model to systematic geometry-consistent spatial transformations. Extensive experiments across multiple benchmarks show that our approach substantially improves spatial faithfulness in text-to-image generation, while maintaining strong performance on multimodal understanding tasks like image captioning, visual grounding, and VQA tasks."
    },
    {
      "title": "CoT-Seg: Rethinking Segmentation with Chain-of-Thought Reasoning and Self-Correction",
      "url": "https://arxiv.org/abs/2601.17420",
      "date": "2026-01-24",
      "authors_text": "Shiu-hong Kao, Chak Ho Huang, Huaiqian Liu, Yu-Wing Tai, Chi-Keung Tang",
      "is_highlight": false,
      "score": 55.0,
      "summary": "CoT-Seg is a novel framework that enhances segmentation by integrating chain-of-thought reasoning and self-correction, improving performance on complex queries and ambiguous images.",
      "teaser_image": null,
      "debug_abstract": "Existing works of reasoning segmentation often fall short in complex cases, particularly when addressing complicated queries and out-of-domain images. Inspired by the chain-of-thought reasoning, where harder problems require longer thinking steps/time, this paper aims to explore a system that can think step-by-step, look up information if needed, generate results, self-evaluate its own results, and refine the results, in the same way humans approach harder questions. We introduce CoT-Seg, a training-free framework that rethinks reasoning segmentation by combining chain-of-thought reasoning with self-correction. Instead of fine-tuning, CoT-Seg leverages the inherent reasoning ability of pre-trained MLLMs (GPT-4o) to decompose queries into meta-instructions, extract fine-grained semantics from images, and identify target objects even under implicit or complex prompts. Moreover, CoT-Seg incorporates a self-correction stage: the model evaluates its own segmentation against the original query and reasoning trace, identifies mismatches, and iteratively refines the mask. This tight integration of reasoning and correction significantly improves reliability and robustness, especially in ambiguous or error-prone cases. Furthermore, our CoT-Seg framework allows easy incorporation of retrieval-augmented reasoning, enabling the system to access external knowledge when the input lacks sufficient information. To showcase CoT-Seg&#39;s ability to handle very challenging cases ,we introduce a new dataset ReasonSeg-Hard. Our results highlight that combining chain-of-thought reasoning, self-correction, offers a powerful paradigm for vision-language integration driven segmentation."
    }
  ],
  "Jun MA老师实验室": [
    {
      "title": "Fast and Safe Trajectory Optimization for Mobile Manipulators With Neural Configuration Space Distance Field",
      "url": "https://arxiv.org/abs/2601.18548",
      "date": "2026-01-26",
      "authors_text": "Yulin Li, Zhiyuan Song, Yiming Li, Zhicheng Song, Kai Chen",
      "is_highlight": false,
      "score": 88.0,
      "summary": "The paper presents Generalized Configuration Space Distance Fields (GCDF) for efficient trajectory optimization in mobile manipulators, enhancing collision reasoning and enabling agile motion in complex environments.",
      "teaser_image": null,
      "debug_abstract": "Mobile manipulators promise agile, long-horizon behavior by coordinating base and arm motion, yet whole-body trajectory optimization in cluttered, confined spaces remains difficult due to high-dimensional nonconvexity and the need for fast, accurate collision reasoning. Configuration Space Distance Fields (CDF) enable fixed-base manipulators to model collisions directly in configuration space via smooth, implicit distances. This representation holds strong potential to bypass the nonlinear configuration-to-workspace mapping while preserving accurate whole-body geometry and providing optimization-friendly collision costs. Yet, extending this capability to mobile manipulators is hindered by unbounded workspaces and tighter base-arm coupling. We lift this promise to mobile manipulation with Generalized Configuration Space Distance Fields (GCDF), extending CDF to robots with both translational and rotational joints in unbounded workspaces with tighter base-arm coupling. We prove that GCDF preserves Euclidean-like local distance structure and accurately encodes whole-body geometry in configuration space, and develop a data generation and training pipeline that yields continuous neural GCDFs with accurate values and gradients, supporting efficient GPU-batched queries. Building on this representation, we develop a high-performance sequential convex optimization framework centered on GCDF-based collision reasoning. The solver scales to large numbers of implicit constraints through (i) online specification of neural constraints, (ii) sparsity-aware active-set detection with parallel batched evaluation across thousands of constraints, and (iii) incremental constraint management for rapid replanning under scene changes."
    },
    {
      "title": "TC-IDM: Grounding Video Generation for Executable Zero-shot Robot Motion",
      "url": "https://arxiv.org/abs/2601.18323",
      "date": "2026-01-26",
      "authors_text": "Weishi Mi, Yong Bao, Xiaowei Chi, Xiaozhu Ju, Zhiyuan Qin",
      "is_highlight": false,
      "score": 90.0,
      "summary": "The TC-IDM framework enhances robotic control by bridging visual planning and physical execution through tool-centric trajectory modeling, achieving superior generalization and performance in diverse tasks.",
      "teaser_image": null,
      "debug_abstract": "The vision-language-action (VLA) paradigm has enabled powerful robotic control by leveraging vision-language models, but its reliance on large-scale, high-quality robot data limits its generalization. Generative world models offer a promising alternative for general-purpose embodied AI, yet a critical gap remains between their pixel-level plans and physically executable actions. To this end, we propose the Tool-Centric Inverse Dynamics Model (TC-IDM). By focusing on the tool&#39;s imagined trajectory as synthesized by the world model, TC-IDM establishes a robust intermediate representation that bridges the gap between visual planning and physical control. TC-IDM extracts the tool&#39;s point cloud trajectories via segmentation and 3D motion estimation from generated videos. Considering diverse tool attributes, our architecture employs decoupled action heads to project these planned trajectories into 6-DoF end-effector motions and corresponding control signals. This plan-and-translate paradigm not only supports a wide range of end-effectors but also significantly improves viewpoint invariance. Furthermore, it exhibits strong generalization capabilities across long-horizon and out-of-distribution tasks, including interacting with deformable objects. In real-world evaluations, the world model with TC-IDM achieves an average success rate of 61.11 percent, with 77.7 percent on simple tasks and 38.46 percent on zero-shot deformable object tasks. It substantially outperforms end-to-end VLA-style baselines and other inverse dynamics models."
    },
    {
      "title": "Uni-RS: A Spatially Faithful Unified Understanding and Generation Model for Remote Sensing",
      "url": "https://arxiv.org/abs/2601.17673",
      "date": "2026-01-25",
      "authors_text": "Weiyu Zhang, Yuan Hu, Yong Li, Yu Liu",
      "is_highlight": false,
      "score": 40.0,
      "summary": "Uni-RS is a novel multimodal model for remote sensing that enhances spatial fidelity in text-to-image generation by integrating spatial layout planning and query supervision.",
      "teaser_image": null,
      "debug_abstract": "Unified remote sensing multimodal models exhibit a pronounced spatial reversal curse: Although they can accurately recognize and describe object locations in images, they often fail to faithfully execute the same spatial relations during text-to-image generation, where such relations constitute core semantic information in remote sensing. Motivated by this observation, we propose Uni-RS, the first unified multimodal model tailored for remote sensing, to explicitly address the spatial asymmetry between understanding and generation. Specifically, we first introduce explicit Spatial-Layout Planning to transform textual instructions into spatial layout plans, decoupling geometric planning from visual synthesis. We then impose Spatial-Aware Query Supervision to bias learnable queries toward spatial relations explicitly specified in the instruction. Finally, we develop Image-Caption Spatial Layout Variation to expose the model to systematic geometry-consistent spatial transformations. Extensive experiments across multiple benchmarks show that our approach substantially improves spatial faithfulness in text-to-image generation, while maintaining strong performance on multimodal understanding tasks like image captioning, visual grounding, and VQA tasks."
    },
    {
      "title": "CoT-Seg: Rethinking Segmentation with Chain-of-Thought Reasoning and Self-Correction",
      "url": "https://arxiv.org/abs/2601.17420",
      "date": "2026-01-24",
      "authors_text": "Shiu-hong Kao, Chak Ho Huang, Huaiqian Liu, Yu-Wing Tai, Chi-Keung Tang",
      "is_highlight": false,
      "score": 55.0,
      "summary": "CoT-Seg is a novel framework that enhances segmentation by integrating chain-of-thought reasoning and self-correction, improving performance on complex queries and ambiguous images.",
      "teaser_image": null,
      "debug_abstract": "Existing works of reasoning segmentation often fall short in complex cases, particularly when addressing complicated queries and out-of-domain images. Inspired by the chain-of-thought reasoning, where harder problems require longer thinking steps/time, this paper aims to explore a system that can think step-by-step, look up information if needed, generate results, self-evaluate its own results, and refine the results, in the same way humans approach harder questions. We introduce CoT-Seg, a training-free framework that rethinks reasoning segmentation by combining chain-of-thought reasoning with self-correction. Instead of fine-tuning, CoT-Seg leverages the inherent reasoning ability of pre-trained MLLMs (GPT-4o) to decompose queries into meta-instructions, extract fine-grained semantics from images, and identify target objects even under implicit or complex prompts. Moreover, CoT-Seg incorporates a self-correction stage: the model evaluates its own segmentation against the original query and reasoning trace, identifies mismatches, and iteratively refines the mask. This tight integration of reasoning and correction significantly improves reliability and robustness, especially in ambiguous or error-prone cases. Furthermore, our CoT-Seg framework allows easy incorporation of retrieval-augmented reasoning, enabling the system to access external knowledge when the input lacks sufficient information. To showcase CoT-Seg&#39;s ability to handle very challenging cases ,we introduce a new dataset ReasonSeg-Hard. Our results highlight that combining chain-of-thought reasoning, self-correction, offers a powerful paradigm for vision-language integration driven segmentation."
    }
  ],
  "Cheng Kar-Shun Robotics Institute (CKSRI)": [
    {
      "title": "Fast and Safe Trajectory Optimization for Mobile Manipulators With Neural Configuration Space Distance Field",
      "url": "https://arxiv.org/abs/2601.18548",
      "date": "2026-01-26",
      "authors_text": "Yulin Li, Zhiyuan Song, Yiming Li, Zhicheng Song, Kai Chen",
      "is_highlight": false,
      "score": 88.0,
      "summary": "The paper presents Generalized Configuration Space Distance Fields (GCDF) for efficient trajectory optimization in mobile manipulators, enhancing collision reasoning and enabling agile motion in complex environments.",
      "teaser_image": null,
      "debug_abstract": "Mobile manipulators promise agile, long-horizon behavior by coordinating base and arm motion, yet whole-body trajectory optimization in cluttered, confined spaces remains difficult due to high-dimensional nonconvexity and the need for fast, accurate collision reasoning. Configuration Space Distance Fields (CDF) enable fixed-base manipulators to model collisions directly in configuration space via smooth, implicit distances. This representation holds strong potential to bypass the nonlinear configuration-to-workspace mapping while preserving accurate whole-body geometry and providing optimization-friendly collision costs. Yet, extending this capability to mobile manipulators is hindered by unbounded workspaces and tighter base-arm coupling. We lift this promise to mobile manipulation with Generalized Configuration Space Distance Fields (GCDF), extending CDF to robots with both translational and rotational joints in unbounded workspaces with tighter base-arm coupling. We prove that GCDF preserves Euclidean-like local distance structure and accurately encodes whole-body geometry in configuration space, and develop a data generation and training pipeline that yields continuous neural GCDFs with accurate values and gradients, supporting efficient GPU-batched queries. Building on this representation, we develop a high-performance sequential convex optimization framework centered on GCDF-based collision reasoning. The solver scales to large numbers of implicit constraints through (i) online specification of neural constraints, (ii) sparsity-aware active-set detection with parallel batched evaluation across thousands of constraints, and (iii) incremental constraint management for rapid replanning under scene changes."
    },
    {
      "title": "TC-IDM: Grounding Video Generation for Executable Zero-shot Robot Motion",
      "url": "https://arxiv.org/abs/2601.18323",
      "date": "2026-01-26",
      "authors_text": "Weishi Mi, Yong Bao, Xiaowei Chi, Xiaozhu Ju, Zhiyuan Qin",
      "is_highlight": false,
      "score": 90.0,
      "summary": "The TC-IDM framework enhances robotic control by bridging visual planning and physical execution through tool-centric trajectory modeling, achieving superior generalization and performance in diverse tasks.",
      "teaser_image": null,
      "debug_abstract": "The vision-language-action (VLA) paradigm has enabled powerful robotic control by leveraging vision-language models, but its reliance on large-scale, high-quality robot data limits its generalization. Generative world models offer a promising alternative for general-purpose embodied AI, yet a critical gap remains between their pixel-level plans and physically executable actions. To this end, we propose the Tool-Centric Inverse Dynamics Model (TC-IDM). By focusing on the tool&#39;s imagined trajectory as synthesized by the world model, TC-IDM establishes a robust intermediate representation that bridges the gap between visual planning and physical control. TC-IDM extracts the tool&#39;s point cloud trajectories via segmentation and 3D motion estimation from generated videos. Considering diverse tool attributes, our architecture employs decoupled action heads to project these planned trajectories into 6-DoF end-effector motions and corresponding control signals. This plan-and-translate paradigm not only supports a wide range of end-effectors but also significantly improves viewpoint invariance. Furthermore, it exhibits strong generalization capabilities across long-horizon and out-of-distribution tasks, including interacting with deformable objects. In real-world evaluations, the world model with TC-IDM achieves an average success rate of 61.11 percent, with 77.7 percent on simple tasks and 38.46 percent on zero-shot deformable object tasks. It substantially outperforms end-to-end VLA-style baselines and other inverse dynamics models."
    },
    {
      "title": "Uni-RS: A Spatially Faithful Unified Understanding and Generation Model for Remote Sensing",
      "url": "https://arxiv.org/abs/2601.17673",
      "date": "2026-01-25",
      "authors_text": "Weiyu Zhang, Yuan Hu, Yong Li, Yu Liu",
      "is_highlight": false,
      "score": 40.0,
      "summary": "Uni-RS is a novel multimodal model for remote sensing that enhances spatial fidelity in text-to-image generation by integrating spatial layout planning and query supervision.",
      "teaser_image": null,
      "debug_abstract": "Unified remote sensing multimodal models exhibit a pronounced spatial reversal curse: Although they can accurately recognize and describe object locations in images, they often fail to faithfully execute the same spatial relations during text-to-image generation, where such relations constitute core semantic information in remote sensing. Motivated by this observation, we propose Uni-RS, the first unified multimodal model tailored for remote sensing, to explicitly address the spatial asymmetry between understanding and generation. Specifically, we first introduce explicit Spatial-Layout Planning to transform textual instructions into spatial layout plans, decoupling geometric planning from visual synthesis. We then impose Spatial-Aware Query Supervision to bias learnable queries toward spatial relations explicitly specified in the instruction. Finally, we develop Image-Caption Spatial Layout Variation to expose the model to systematic geometry-consistent spatial transformations. Extensive experiments across multiple benchmarks show that our approach substantially improves spatial faithfulness in text-to-image generation, while maintaining strong performance on multimodal understanding tasks like image captioning, visual grounding, and VQA tasks."
    },
    {
      "title": "CoT-Seg: Rethinking Segmentation with Chain-of-Thought Reasoning and Self-Correction",
      "url": "https://arxiv.org/abs/2601.17420",
      "date": "2026-01-24",
      "authors_text": "Shiu-hong Kao, Chak Ho Huang, Huaiqian Liu, Yu-Wing Tai, Chi-Keung Tang",
      "is_highlight": false,
      "score": 55.0,
      "summary": "CoT-Seg is a novel framework that enhances segmentation by integrating chain-of-thought reasoning and self-correction, improving performance on complex queries and ambiguous images.",
      "teaser_image": null,
      "debug_abstract": "Existing works of reasoning segmentation often fall short in complex cases, particularly when addressing complicated queries and out-of-domain images. Inspired by the chain-of-thought reasoning, where harder problems require longer thinking steps/time, this paper aims to explore a system that can think step-by-step, look up information if needed, generate results, self-evaluate its own results, and refine the results, in the same way humans approach harder questions. We introduce CoT-Seg, a training-free framework that rethinks reasoning segmentation by combining chain-of-thought reasoning with self-correction. Instead of fine-tuning, CoT-Seg leverages the inherent reasoning ability of pre-trained MLLMs (GPT-4o) to decompose queries into meta-instructions, extract fine-grained semantics from images, and identify target objects even under implicit or complex prompts. Moreover, CoT-Seg incorporates a self-correction stage: the model evaluates its own segmentation against the original query and reasoning trace, identifies mismatches, and iteratively refines the mask. This tight integration of reasoning and correction significantly improves reliability and robustness, especially in ambiguous or error-prone cases. Furthermore, our CoT-Seg framework allows easy incorporation of retrieval-augmented reasoning, enabling the system to access external knowledge when the input lacks sufficient information. To showcase CoT-Seg&#39;s ability to handle very challenging cases ,we introduce a new dataset ReasonSeg-Hard. Our results highlight that combining chain-of-thought reasoning, self-correction, offers a powerful paradigm for vision-language integration driven segmentation."
    }
  ],
  "机器人系统实验室 (RSL)": [
    {
      "title": "Scaling Rough Terrain Locomotion with Automatic Curriculum Reinforcement Learning",
      "url": "https://arxiv.org/abs/2601.17428",
      "date": "2026-01-24",
      "authors_text": "Ziming Li, Chenhao Li, Marco Hutter",
      "is_highlight": true,
      "score": 88.0,
      "summary": "The LP-ACRL framework enables quadruped robots to achieve stable high-speed locomotion across diverse terrains by adaptively generating curricula based on real-time learning progress.",
      "teaser_image": null,
      "debug_abstract": "Curriculum learning has demonstrated substantial effectiveness in robot learning. However, it still faces limitations when scaling to complex, wide-ranging task spaces. Such task spaces often lack a well-defined difficulty structure, making the difficulty ordering required by previous methods challenging to define. We propose a Learning Progress-based Automatic Curriculum Reinforcement Learning (LP-ACRL) framework, which estimates the agent&#39;s learning progress online and adaptively adjusts the task-sampling distribution, thereby enabling automatic curriculum generation without prior knowledge of the difficulty distribution over the task space. Policies trained with LP-ACRL enable the ANYmal D quadruped to achieve and maintain stable, high-speed locomotion at 2.5 m/s linear velocity and 3.0 rad/s angular velocity across diverse terrains, including stairs, slopes, gravel, and low-friction flat surfaces--whereas previous methods have generally been limited to high speeds on flat terrain or low speeds on complex terrain. Experimental results demonstrate that LP-ACRL exhibits strong scalability and real-world applicability, providing a robust baseline for future research on curriculum generation in complex, wide-ranging robotic learning task spaces."
    }
  ],
  "Sensing, Interaction & Perception Lab (SIPLAB)": [
    {
      "title": "Scaling Rough Terrain Locomotion with Automatic Curriculum Reinforcement Learning",
      "url": "https://arxiv.org/abs/2601.17428",
      "date": "2026-01-24",
      "authors_text": "Ziming Li, Chenhao Li, Marco Hutter",
      "is_highlight": true,
      "score": 88.0,
      "summary": "The LP-ACRL framework enables quadruped robots to achieve stable high-speed locomotion across diverse terrains by adaptively generating curricula based on real-time learning progress.",
      "teaser_image": null,
      "debug_abstract": "Curriculum learning has demonstrated substantial effectiveness in robot learning. However, it still faces limitations when scaling to complex, wide-ranging task spaces. Such task spaces often lack a well-defined difficulty structure, making the difficulty ordering required by previous methods challenging to define. We propose a Learning Progress-based Automatic Curriculum Reinforcement Learning (LP-ACRL) framework, which estimates the agent&#39;s learning progress online and adaptively adjusts the task-sampling distribution, thereby enabling automatic curriculum generation without prior knowledge of the difficulty distribution over the task space. Policies trained with LP-ACRL enable the ANYmal D quadruped to achieve and maintain stable, high-speed locomotion at 2.5 m/s linear velocity and 3.0 rad/s angular velocity across diverse terrains, including stairs, slopes, gravel, and low-friction flat surfaces--whereas previous methods have generally been limited to high speeds on flat terrain or low speeds on complex terrain. Experimental results demonstrate that LP-ACRL exhibits strong scalability and real-world applicability, providing a robust baseline for future research on curriculum generation in complex, wide-ranging robotic learning task spaces."
    }
  ],
  "IWIN-FINS实验室": [
    {
      "title": "Advances and Innovations in the Multi-Agent Robotic System (MARS) Challenge",
      "url": "https://arxiv.org/abs/2601.18733",
      "date": "2026-01-26",
      "authors_text": "Li Kang, Heng Zhou, Xiufeng Song, Rui Li, Bruno N. Y. Chen",
      "is_highlight": false,
      "score": 75.0,
      "summary": "The MARS Challenge at NeurIPS 2025 promotes advancements in multi-agent systems by exploring vision-language models for collaborative robotic planning and manipulation in dynamic environments.",
      "teaser_image": null,
      "debug_abstract": "Recent advancements in multimodal large language models and vision-languageaction models have significantly driven progress in Embodied AI. As the field transitions toward more complex task scenarios, multi-agent system frameworks are becoming essential for achieving scalable, efficient, and collaborative solutions. This shift is fueled by three primary factors: increasing agent capabilities, enhancing system efficiency through task delegation, and enabling advanced human-agent interactions. To address the challenges posed by multi-agent collaboration, we propose the Multi-Agent Robotic System (MARS) Challenge, held at the NeurIPS 2025 Workshop on SpaVLE. The competition focuses on two critical areas: planning and control, where participants explore multi-agent embodied planning using vision-language models (VLMs) to coordinate tasks and policy execution to perform robotic manipulation in dynamic environments. By evaluating solutions submitted by participants, the challenge provides valuable insights into the design and coordination of embodied multi-agent systems, contributing to the future development of advanced collaborative AI systems."
    },
    {
      "title": "ExoGS: A 4D Real-to-Sim-to-Real Framework for Scalable Manipulation Data Collection",
      "url": "https://arxiv.org/abs/2601.18629",
      "date": "2026-01-26",
      "authors_text": "Yiming Wang, Ruogu Zhang, Minyang Li, Hao Shi, Junbo Wang",
      "is_highlight": false,
      "score": 94.0,
      "summary": "ExoGS is a novel 4D framework that enhances scalable robotic manipulation data collection by effectively transferring real-world interactions to simulation using a passive exoskeleton.",
      "teaser_image": null,
      "debug_abstract": "Real-to-Sim-to-Real technique is gaining increasing interest for robotic manipulation, as it can generate scalable data in simulation while having narrower sim-to-real gap. However, previous methods mainly focused on environment-level visual real-to-sim transfer, ignoring the transfer of interactions, which could be challenging and inefficient to obtain purely in simulation especially for contact-rich tasks. We propose ExoGS, a robot-free 4D Real-to-Sim-to-Real framework that captures both static environments and dynamic interactions in the real world and transfers them seamlessly to a simulated environment. It provides a new solution for scalable manipulation data collection and policy learning. ExoGS employs a self-designed robot-isomorphic passive exoskeleton AirExo-3 to capture kinematically consistent trajectories with millimeter-level accuracy and synchronized RGB observations during direct human demonstrations. The robot, objects, and environment are reconstructed as editable 3D Gaussian Splatting assets, enabling geometry-consistent replay and large-scale data augmentation. Additionally, a lightweight Mask Adapter injects instance-level semantics into the policy to enhance robustness under visual domain shifts. Real-world experiments demonstrate that ExoGS significantly improves data efficiency and policy generalization compared to teleoperation-based baselines. Code and hardware files have been released on this https URL."
    },
    {
      "title": "QualiRAG: Retrieval-Augmented Generation for Visual Quality Understanding",
      "url": "https://arxiv.org/abs/2601.18195",
      "date": "2026-01-26",
      "authors_text": "Linhan Cao, Wei Sun, Weixia Zhang, Xiangyang Zhu, Kaiwei Zhang",
      "is_highlight": false,
      "score": 35.0,
      "summary": "QualiRAG introduces a training-free framework for visual quality understanding that utilizes dynamic retrieval and generation of contextual knowledge, outperforming existing models without task-specific training.",
      "teaser_image": null,
      "debug_abstract": "Visual quality assessment (VQA) is increasingly shifting from scalar score prediction toward interpretable quality understanding -- a paradigm that demands \\textit{fine-grained spatiotemporal perception} and \\textit{auxiliary contextual information}. Current approaches rely on supervised fine-tuning or reinforcement learning on curated instruction datasets, which involve labor-intensive annotation and are prone to dataset-specific biases. To address these challenges, we propose \\textbf{QualiRAG}, a \\textit{training-free} \\textbf{R}etrieval-\\textbf{A}ugmented \\textbf{G}eneration \\textbf{(RAG)} framework that systematically leverages the latent perceptual knowledge of large multimodal models (LMMs) for visual quality perception. Unlike conventional RAG that retrieves from static corpora, QualiRAG dynamically generates auxiliary knowledge by decomposing questions into structured requests and constructing four complementary knowledge sources: \\textit{visual metadata}, \\textit{subject localization}, \\textit{global quality summaries}, and \\textit{local quality descriptions}, followed by relevance-aware retrieval for evidence-grounded reasoning. Extensive experiments show that QualiRAG achieves substantial improvements over open-source general-purpose LMMs and VQA-finetuned LMMs on visual quality understanding tasks, and delivers competitive performance on visual quality comparison tasks, demonstrating robust quality assessment capabilities without any task-specific training. The code will be publicly available at this https URL."
    },
    {
      "title": "BMDS-Net: A Bayesian Multi-Modal Deep Supervision Network for Robust Brain Tumor Segmentation",
      "url": "https://arxiv.org/abs/2601.17504",
      "date": "2026-01-24",
      "authors_text": "Yan Zhou, Zhen Huang, Yingqiu Li, Yue Ouyang, Suncheng Xiang",
      "is_highlight": false,
      "score": 45.0,
      "summary": "BMDS-Net enhances brain tumor segmentation by integrating robust multimodal fusion and Bayesian fine-tuning for improved accuracy and uncertainty estimation in clinical MRI applications.",
      "teaser_image": null,
      "debug_abstract": "Accurate brain tumor segmentation from multi-modal magnetic resonance imaging (MRI) is a prerequisite for precise radiotherapy planning and surgical navigation. While recent Transformer-based models such as Swin UNETR have achieved impressive benchmark performance, their clinical utility is often compromised by two critical issues: sensitivity to missing modalities (common in clinical practice) and a lack of confidence calibration. Merely chasing higher Dice scores on idealized data fails to meet the safety requirements of real-world medical deployment. In this work, we propose BMDS-Net, a unified framework that prioritizes clinical robustness and trustworthiness over simple metric maximization. Our contribution is three-fold. First, we construct a robust deterministic backbone by integrating a Zero-Init Multimodal Contextual Fusion (MMCF) module and a Residual-Gated Deep Decoder Supervision (DDS) mechanism, enabling stable feature learning and precise boundary delineation with significantly reduced Hausdorff Distance, even under modality corruption. Second, and most importantly, we introduce a memory-efficient Bayesian fine-tuning strategy that transforms the network into a probabilistic predictor, providing voxel-wise uncertainty maps to highlight potential errors for clinicians. Third, comprehensive experiments on the BraTS 2021 dataset demonstrate that BMDS-Net not only maintains competitive accuracy but, more importantly, exhibits superior stability in missing-modality scenarios where baseline models fail. The source code is publicly available at this https URL."
    },
    {
      "title": "PILOT: A Perceptive Integrated Low-level Controller for Loco-manipulation over Unstructured Scenes",
      "url": "https://arxiv.org/abs/2601.17440",
      "date": "2026-01-24",
      "authors_text": "Xinru Cui, Linxi Feng, Yixuan Zhou, Haoqi Han, Zhe Liu",
      "is_highlight": true,
      "score": 92.0,
      "summary": "PILOT is a unified reinforcement learning framework that enhances humanoid robot loco-manipulation by integrating perceptive locomotion and whole-body control for improved task execution in unstructured environments.",
      "teaser_image": null,
      "debug_abstract": "Humanoid robots hold great potential for diverse interactions and daily service tasks within human-centered environments, necessitating controllers that seamlessly integrate precise locomotion with dexterous manipulation. However, most existing whole-body controllers lack exteroceptive awareness of the surrounding environment, rendering them insufficient for stable task execution in complex, unstructured this http URL address this challenge, we propose PILOT, a unified single-stage reinforcement learning (RL) framework tailored for perceptive loco-manipulation, which synergizes perceptive locomotion and expansive whole-body control within a single policy. To enhance terrain awareness and ensure precise foot placement, we design a cross-modal context encoder that fuses prediction-based proprioceptive features with attention-based perceptive representations. Furthermore, we introduce a Mixture-of-Experts (MoE) policy architecture to coordinate diverse motor skills, facilitating better specialization across distinct motion patterns. Extensive experiments in both simulation and on the physical Unitree G1 humanoid robot validate the efficacy of our framework. PILOT demonstrates superior stability, command tracking precision, and terrain traversability compared to existing baselines. These results highlight its potential to serve as a robust, foundational low-level controller for loco-manipulation in unstructured scenes."
    }
  ],
  "智能机器人与机器视觉(IRMV)实验室": [
    {
      "title": "Advances and Innovations in the Multi-Agent Robotic System (MARS) Challenge",
      "url": "https://arxiv.org/abs/2601.18733",
      "date": "2026-01-26",
      "authors_text": "Li Kang, Heng Zhou, Xiufeng Song, Rui Li, Bruno N. Y. Chen",
      "is_highlight": false,
      "score": 75.0,
      "summary": "The MARS Challenge at NeurIPS 2025 promotes advancements in multi-agent systems by exploring vision-language models for collaborative robotic planning and manipulation in dynamic environments.",
      "teaser_image": null,
      "debug_abstract": "Recent advancements in multimodal large language models and vision-languageaction models have significantly driven progress in Embodied AI. As the field transitions toward more complex task scenarios, multi-agent system frameworks are becoming essential for achieving scalable, efficient, and collaborative solutions. This shift is fueled by three primary factors: increasing agent capabilities, enhancing system efficiency through task delegation, and enabling advanced human-agent interactions. To address the challenges posed by multi-agent collaboration, we propose the Multi-Agent Robotic System (MARS) Challenge, held at the NeurIPS 2025 Workshop on SpaVLE. The competition focuses on two critical areas: planning and control, where participants explore multi-agent embodied planning using vision-language models (VLMs) to coordinate tasks and policy execution to perform robotic manipulation in dynamic environments. By evaluating solutions submitted by participants, the challenge provides valuable insights into the design and coordination of embodied multi-agent systems, contributing to the future development of advanced collaborative AI systems."
    },
    {
      "title": "ExoGS: A 4D Real-to-Sim-to-Real Framework for Scalable Manipulation Data Collection",
      "url": "https://arxiv.org/abs/2601.18629",
      "date": "2026-01-26",
      "authors_text": "Yiming Wang, Ruogu Zhang, Minyang Li, Hao Shi, Junbo Wang",
      "is_highlight": false,
      "score": 94.0,
      "summary": "ExoGS is a novel 4D framework that enhances scalable robotic manipulation data collection by effectively transferring real-world interactions to simulation using a passive exoskeleton.",
      "teaser_image": null,
      "debug_abstract": "Real-to-Sim-to-Real technique is gaining increasing interest for robotic manipulation, as it can generate scalable data in simulation while having narrower sim-to-real gap. However, previous methods mainly focused on environment-level visual real-to-sim transfer, ignoring the transfer of interactions, which could be challenging and inefficient to obtain purely in simulation especially for contact-rich tasks. We propose ExoGS, a robot-free 4D Real-to-Sim-to-Real framework that captures both static environments and dynamic interactions in the real world and transfers them seamlessly to a simulated environment. It provides a new solution for scalable manipulation data collection and policy learning. ExoGS employs a self-designed robot-isomorphic passive exoskeleton AirExo-3 to capture kinematically consistent trajectories with millimeter-level accuracy and synchronized RGB observations during direct human demonstrations. The robot, objects, and environment are reconstructed as editable 3D Gaussian Splatting assets, enabling geometry-consistent replay and large-scale data augmentation. Additionally, a lightweight Mask Adapter injects instance-level semantics into the policy to enhance robustness under visual domain shifts. Real-world experiments demonstrate that ExoGS significantly improves data efficiency and policy generalization compared to teleoperation-based baselines. Code and hardware files have been released on this https URL."
    },
    {
      "title": "QualiRAG: Retrieval-Augmented Generation for Visual Quality Understanding",
      "url": "https://arxiv.org/abs/2601.18195",
      "date": "2026-01-26",
      "authors_text": "Linhan Cao, Wei Sun, Weixia Zhang, Xiangyang Zhu, Kaiwei Zhang",
      "is_highlight": false,
      "score": 35.0,
      "summary": "QualiRAG introduces a training-free framework for visual quality understanding that utilizes dynamic retrieval and generation of contextual knowledge, outperforming existing models without task-specific training.",
      "teaser_image": null,
      "debug_abstract": "Visual quality assessment (VQA) is increasingly shifting from scalar score prediction toward interpretable quality understanding -- a paradigm that demands \\textit{fine-grained spatiotemporal perception} and \\textit{auxiliary contextual information}. Current approaches rely on supervised fine-tuning or reinforcement learning on curated instruction datasets, which involve labor-intensive annotation and are prone to dataset-specific biases. To address these challenges, we propose \\textbf{QualiRAG}, a \\textit{training-free} \\textbf{R}etrieval-\\textbf{A}ugmented \\textbf{G}eneration \\textbf{(RAG)} framework that systematically leverages the latent perceptual knowledge of large multimodal models (LMMs) for visual quality perception. Unlike conventional RAG that retrieves from static corpora, QualiRAG dynamically generates auxiliary knowledge by decomposing questions into structured requests and constructing four complementary knowledge sources: \\textit{visual metadata}, \\textit{subject localization}, \\textit{global quality summaries}, and \\textit{local quality descriptions}, followed by relevance-aware retrieval for evidence-grounded reasoning. Extensive experiments show that QualiRAG achieves substantial improvements over open-source general-purpose LMMs and VQA-finetuned LMMs on visual quality understanding tasks, and delivers competitive performance on visual quality comparison tasks, demonstrating robust quality assessment capabilities without any task-specific training. The code will be publicly available at this https URL."
    },
    {
      "title": "BMDS-Net: A Bayesian Multi-Modal Deep Supervision Network for Robust Brain Tumor Segmentation",
      "url": "https://arxiv.org/abs/2601.17504",
      "date": "2026-01-24",
      "authors_text": "Yan Zhou, Zhen Huang, Yingqiu Li, Yue Ouyang, Suncheng Xiang",
      "is_highlight": false,
      "score": 45.0,
      "summary": "BMDS-Net enhances brain tumor segmentation by integrating robust multimodal fusion and Bayesian fine-tuning for improved accuracy and uncertainty estimation in clinical MRI applications.",
      "teaser_image": null,
      "debug_abstract": "Accurate brain tumor segmentation from multi-modal magnetic resonance imaging (MRI) is a prerequisite for precise radiotherapy planning and surgical navigation. While recent Transformer-based models such as Swin UNETR have achieved impressive benchmark performance, their clinical utility is often compromised by two critical issues: sensitivity to missing modalities (common in clinical practice) and a lack of confidence calibration. Merely chasing higher Dice scores on idealized data fails to meet the safety requirements of real-world medical deployment. In this work, we propose BMDS-Net, a unified framework that prioritizes clinical robustness and trustworthiness over simple metric maximization. Our contribution is three-fold. First, we construct a robust deterministic backbone by integrating a Zero-Init Multimodal Contextual Fusion (MMCF) module and a Residual-Gated Deep Decoder Supervision (DDS) mechanism, enabling stable feature learning and precise boundary delineation with significantly reduced Hausdorff Distance, even under modality corruption. Second, and most importantly, we introduce a memory-efficient Bayesian fine-tuning strategy that transforms the network into a probabilistic predictor, providing voxel-wise uncertainty maps to highlight potential errors for clinicians. Third, comprehensive experiments on the BraTS 2021 dataset demonstrate that BMDS-Net not only maintains competitive accuracy but, more importantly, exhibits superior stability in missing-modality scenarios where baseline models fail. The source code is publicly available at this https URL."
    },
    {
      "title": "PILOT: A Perceptive Integrated Low-level Controller for Loco-manipulation over Unstructured Scenes",
      "url": "https://arxiv.org/abs/2601.17440",
      "date": "2026-01-24",
      "authors_text": "Xinru Cui, Linxi Feng, Yixuan Zhou, Haoqi Han, Zhe Liu",
      "is_highlight": true,
      "score": 92.0,
      "summary": "PILOT is a unified reinforcement learning framework that enhances humanoid robot loco-manipulation by integrating perceptive locomotion and whole-body control for improved task execution in unstructured environments.",
      "teaser_image": null,
      "debug_abstract": "Humanoid robots hold great potential for diverse interactions and daily service tasks within human-centered environments, necessitating controllers that seamlessly integrate precise locomotion with dexterous manipulation. However, most existing whole-body controllers lack exteroceptive awareness of the surrounding environment, rendering them insufficient for stable task execution in complex, unstructured this http URL address this challenge, we propose PILOT, a unified single-stage reinforcement learning (RL) framework tailored for perceptive loco-manipulation, which synergizes perceptive locomotion and expansive whole-body control within a single policy. To enhance terrain awareness and ensure precise foot placement, we design a cross-modal context encoder that fuses prediction-based proprioceptive features with attention-based perceptive representations. Furthermore, we introduce a Mixture-of-Experts (MoE) policy architecture to coordinate diverse motor skills, facilitating better specialization across distinct motion patterns. Extensive experiments in both simulation and on the physical Unitree G1 humanoid robot validate the efficacy of our framework. PILOT demonstrates superior stability, command tracking precision, and terrain traversability compared to existing baselines. These results highlight its potential to serve as a robust, foundational low-level controller for loco-manipulation in unstructured scenes."
    }
  ],
  "ReThinkLab": [
    {
      "title": "Advances and Innovations in the Multi-Agent Robotic System (MARS) Challenge",
      "url": "https://arxiv.org/abs/2601.18733",
      "date": "2026-01-26",
      "authors_text": "Li Kang, Heng Zhou, Xiufeng Song, Rui Li, Bruno N. Y. Chen",
      "is_highlight": false,
      "score": 75.0,
      "summary": "The MARS Challenge at NeurIPS 2025 promotes advancements in multi-agent systems by exploring vision-language models for collaborative robotic planning and manipulation in dynamic environments.",
      "teaser_image": null,
      "debug_abstract": "Recent advancements in multimodal large language models and vision-languageaction models have significantly driven progress in Embodied AI. As the field transitions toward more complex task scenarios, multi-agent system frameworks are becoming essential for achieving scalable, efficient, and collaborative solutions. This shift is fueled by three primary factors: increasing agent capabilities, enhancing system efficiency through task delegation, and enabling advanced human-agent interactions. To address the challenges posed by multi-agent collaboration, we propose the Multi-Agent Robotic System (MARS) Challenge, held at the NeurIPS 2025 Workshop on SpaVLE. The competition focuses on two critical areas: planning and control, where participants explore multi-agent embodied planning using vision-language models (VLMs) to coordinate tasks and policy execution to perform robotic manipulation in dynamic environments. By evaluating solutions submitted by participants, the challenge provides valuable insights into the design and coordination of embodied multi-agent systems, contributing to the future development of advanced collaborative AI systems."
    },
    {
      "title": "ExoGS: A 4D Real-to-Sim-to-Real Framework for Scalable Manipulation Data Collection",
      "url": "https://arxiv.org/abs/2601.18629",
      "date": "2026-01-26",
      "authors_text": "Yiming Wang, Ruogu Zhang, Minyang Li, Hao Shi, Junbo Wang",
      "is_highlight": false,
      "score": 94.0,
      "summary": "ExoGS is a novel 4D framework that enhances scalable robotic manipulation data collection by effectively transferring real-world interactions to simulation using a passive exoskeleton.",
      "teaser_image": null,
      "debug_abstract": "Real-to-Sim-to-Real technique is gaining increasing interest for robotic manipulation, as it can generate scalable data in simulation while having narrower sim-to-real gap. However, previous methods mainly focused on environment-level visual real-to-sim transfer, ignoring the transfer of interactions, which could be challenging and inefficient to obtain purely in simulation especially for contact-rich tasks. We propose ExoGS, a robot-free 4D Real-to-Sim-to-Real framework that captures both static environments and dynamic interactions in the real world and transfers them seamlessly to a simulated environment. It provides a new solution for scalable manipulation data collection and policy learning. ExoGS employs a self-designed robot-isomorphic passive exoskeleton AirExo-3 to capture kinematically consistent trajectories with millimeter-level accuracy and synchronized RGB observations during direct human demonstrations. The robot, objects, and environment are reconstructed as editable 3D Gaussian Splatting assets, enabling geometry-consistent replay and large-scale data augmentation. Additionally, a lightweight Mask Adapter injects instance-level semantics into the policy to enhance robustness under visual domain shifts. Real-world experiments demonstrate that ExoGS significantly improves data efficiency and policy generalization compared to teleoperation-based baselines. Code and hardware files have been released on this https URL."
    },
    {
      "title": "QualiRAG: Retrieval-Augmented Generation for Visual Quality Understanding",
      "url": "https://arxiv.org/abs/2601.18195",
      "date": "2026-01-26",
      "authors_text": "Linhan Cao, Wei Sun, Weixia Zhang, Xiangyang Zhu, Kaiwei Zhang",
      "is_highlight": false,
      "score": 35.0,
      "summary": "QualiRAG introduces a training-free framework for visual quality understanding that utilizes dynamic retrieval and generation of contextual knowledge, outperforming existing models without task-specific training.",
      "teaser_image": null,
      "debug_abstract": "Visual quality assessment (VQA) is increasingly shifting from scalar score prediction toward interpretable quality understanding -- a paradigm that demands \\textit{fine-grained spatiotemporal perception} and \\textit{auxiliary contextual information}. Current approaches rely on supervised fine-tuning or reinforcement learning on curated instruction datasets, which involve labor-intensive annotation and are prone to dataset-specific biases. To address these challenges, we propose \\textbf{QualiRAG}, a \\textit{training-free} \\textbf{R}etrieval-\\textbf{A}ugmented \\textbf{G}eneration \\textbf{(RAG)} framework that systematically leverages the latent perceptual knowledge of large multimodal models (LMMs) for visual quality perception. Unlike conventional RAG that retrieves from static corpora, QualiRAG dynamically generates auxiliary knowledge by decomposing questions into structured requests and constructing four complementary knowledge sources: \\textit{visual metadata}, \\textit{subject localization}, \\textit{global quality summaries}, and \\textit{local quality descriptions}, followed by relevance-aware retrieval for evidence-grounded reasoning. Extensive experiments show that QualiRAG achieves substantial improvements over open-source general-purpose LMMs and VQA-finetuned LMMs on visual quality understanding tasks, and delivers competitive performance on visual quality comparison tasks, demonstrating robust quality assessment capabilities without any task-specific training. The code will be publicly available at this https URL."
    },
    {
      "title": "BMDS-Net: A Bayesian Multi-Modal Deep Supervision Network for Robust Brain Tumor Segmentation",
      "url": "https://arxiv.org/abs/2601.17504",
      "date": "2026-01-24",
      "authors_text": "Yan Zhou, Zhen Huang, Yingqiu Li, Yue Ouyang, Suncheng Xiang",
      "is_highlight": false,
      "score": 45.0,
      "summary": "BMDS-Net enhances brain tumor segmentation by integrating robust multimodal fusion and Bayesian fine-tuning for improved accuracy and uncertainty estimation in clinical MRI applications.",
      "teaser_image": null,
      "debug_abstract": "Accurate brain tumor segmentation from multi-modal magnetic resonance imaging (MRI) is a prerequisite for precise radiotherapy planning and surgical navigation. While recent Transformer-based models such as Swin UNETR have achieved impressive benchmark performance, their clinical utility is often compromised by two critical issues: sensitivity to missing modalities (common in clinical practice) and a lack of confidence calibration. Merely chasing higher Dice scores on idealized data fails to meet the safety requirements of real-world medical deployment. In this work, we propose BMDS-Net, a unified framework that prioritizes clinical robustness and trustworthiness over simple metric maximization. Our contribution is three-fold. First, we construct a robust deterministic backbone by integrating a Zero-Init Multimodal Contextual Fusion (MMCF) module and a Residual-Gated Deep Decoder Supervision (DDS) mechanism, enabling stable feature learning and precise boundary delineation with significantly reduced Hausdorff Distance, even under modality corruption. Second, and most importantly, we introduce a memory-efficient Bayesian fine-tuning strategy that transforms the network into a probabilistic predictor, providing voxel-wise uncertainty maps to highlight potential errors for clinicians. Third, comprehensive experiments on the BraTS 2021 dataset demonstrate that BMDS-Net not only maintains competitive accuracy but, more importantly, exhibits superior stability in missing-modality scenarios where baseline models fail. The source code is publicly available at this https URL."
    },
    {
      "title": "PILOT: A Perceptive Integrated Low-level Controller for Loco-manipulation over Unstructured Scenes",
      "url": "https://arxiv.org/abs/2601.17440",
      "date": "2026-01-24",
      "authors_text": "Xinru Cui, Linxi Feng, Yixuan Zhou, Haoqi Han, Zhe Liu",
      "is_highlight": true,
      "score": 92.0,
      "summary": "PILOT is a unified reinforcement learning framework that enhances humanoid robot loco-manipulation by integrating perceptive locomotion and whole-body control for improved task execution in unstructured environments.",
      "teaser_image": null,
      "debug_abstract": "Humanoid robots hold great potential for diverse interactions and daily service tasks within human-centered environments, necessitating controllers that seamlessly integrate precise locomotion with dexterous manipulation. However, most existing whole-body controllers lack exteroceptive awareness of the surrounding environment, rendering them insufficient for stable task execution in complex, unstructured this http URL address this challenge, we propose PILOT, a unified single-stage reinforcement learning (RL) framework tailored for perceptive loco-manipulation, which synergizes perceptive locomotion and expansive whole-body control within a single policy. To enhance terrain awareness and ensure precise foot placement, we design a cross-modal context encoder that fuses prediction-based proprioceptive features with attention-based perceptive representations. Furthermore, we introduce a Mixture-of-Experts (MoE) policy architecture to coordinate diverse motor skills, facilitating better specialization across distinct motion patterns. Extensive experiments in both simulation and on the physical Unitree G1 humanoid robot validate the efficacy of our framework. PILOT demonstrates superior stability, command tracking precision, and terrain traversability compared to existing baselines. These results highlight its potential to serve as a robust, foundational low-level controller for loco-manipulation in unstructured scenes."
    }
  ],
  "机器视觉与智能学习实验室 (MVIG)": [
    {
      "title": "Advances and Innovations in the Multi-Agent Robotic System (MARS) Challenge",
      "url": "https://arxiv.org/abs/2601.18733",
      "date": "2026-01-26",
      "authors_text": "Li Kang, Heng Zhou, Xiufeng Song, Rui Li, Bruno N. Y. Chen",
      "is_highlight": false,
      "score": 75.0,
      "summary": "The MARS Challenge at NeurIPS 2025 promotes advancements in multi-agent systems by exploring vision-language models for collaborative robotic planning and manipulation in dynamic environments.",
      "teaser_image": null,
      "debug_abstract": "Recent advancements in multimodal large language models and vision-languageaction models have significantly driven progress in Embodied AI. As the field transitions toward more complex task scenarios, multi-agent system frameworks are becoming essential for achieving scalable, efficient, and collaborative solutions. This shift is fueled by three primary factors: increasing agent capabilities, enhancing system efficiency through task delegation, and enabling advanced human-agent interactions. To address the challenges posed by multi-agent collaboration, we propose the Multi-Agent Robotic System (MARS) Challenge, held at the NeurIPS 2025 Workshop on SpaVLE. The competition focuses on two critical areas: planning and control, where participants explore multi-agent embodied planning using vision-language models (VLMs) to coordinate tasks and policy execution to perform robotic manipulation in dynamic environments. By evaluating solutions submitted by participants, the challenge provides valuable insights into the design and coordination of embodied multi-agent systems, contributing to the future development of advanced collaborative AI systems."
    },
    {
      "title": "ExoGS: A 4D Real-to-Sim-to-Real Framework for Scalable Manipulation Data Collection",
      "url": "https://arxiv.org/abs/2601.18629",
      "date": "2026-01-26",
      "authors_text": "Yiming Wang, Ruogu Zhang, Minyang Li, Hao Shi, Junbo Wang",
      "is_highlight": false,
      "score": 94.0,
      "summary": "ExoGS is a novel 4D framework that enhances scalable robotic manipulation data collection by effectively transferring real-world interactions to simulation using a passive exoskeleton.",
      "teaser_image": null,
      "debug_abstract": "Real-to-Sim-to-Real technique is gaining increasing interest for robotic manipulation, as it can generate scalable data in simulation while having narrower sim-to-real gap. However, previous methods mainly focused on environment-level visual real-to-sim transfer, ignoring the transfer of interactions, which could be challenging and inefficient to obtain purely in simulation especially for contact-rich tasks. We propose ExoGS, a robot-free 4D Real-to-Sim-to-Real framework that captures both static environments and dynamic interactions in the real world and transfers them seamlessly to a simulated environment. It provides a new solution for scalable manipulation data collection and policy learning. ExoGS employs a self-designed robot-isomorphic passive exoskeleton AirExo-3 to capture kinematically consistent trajectories with millimeter-level accuracy and synchronized RGB observations during direct human demonstrations. The robot, objects, and environment are reconstructed as editable 3D Gaussian Splatting assets, enabling geometry-consistent replay and large-scale data augmentation. Additionally, a lightweight Mask Adapter injects instance-level semantics into the policy to enhance robustness under visual domain shifts. Real-world experiments demonstrate that ExoGS significantly improves data efficiency and policy generalization compared to teleoperation-based baselines. Code and hardware files have been released on this https URL."
    },
    {
      "title": "QualiRAG: Retrieval-Augmented Generation for Visual Quality Understanding",
      "url": "https://arxiv.org/abs/2601.18195",
      "date": "2026-01-26",
      "authors_text": "Linhan Cao, Wei Sun, Weixia Zhang, Xiangyang Zhu, Kaiwei Zhang",
      "is_highlight": false,
      "score": 35.0,
      "summary": "QualiRAG introduces a training-free framework for visual quality understanding that utilizes dynamic retrieval and generation of contextual knowledge, outperforming existing models without task-specific training.",
      "teaser_image": null,
      "debug_abstract": "Visual quality assessment (VQA) is increasingly shifting from scalar score prediction toward interpretable quality understanding -- a paradigm that demands \\textit{fine-grained spatiotemporal perception} and \\textit{auxiliary contextual information}. Current approaches rely on supervised fine-tuning or reinforcement learning on curated instruction datasets, which involve labor-intensive annotation and are prone to dataset-specific biases. To address these challenges, we propose \\textbf{QualiRAG}, a \\textit{training-free} \\textbf{R}etrieval-\\textbf{A}ugmented \\textbf{G}eneration \\textbf{(RAG)} framework that systematically leverages the latent perceptual knowledge of large multimodal models (LMMs) for visual quality perception. Unlike conventional RAG that retrieves from static corpora, QualiRAG dynamically generates auxiliary knowledge by decomposing questions into structured requests and constructing four complementary knowledge sources: \\textit{visual metadata}, \\textit{subject localization}, \\textit{global quality summaries}, and \\textit{local quality descriptions}, followed by relevance-aware retrieval for evidence-grounded reasoning. Extensive experiments show that QualiRAG achieves substantial improvements over open-source general-purpose LMMs and VQA-finetuned LMMs on visual quality understanding tasks, and delivers competitive performance on visual quality comparison tasks, demonstrating robust quality assessment capabilities without any task-specific training. The code will be publicly available at this https URL."
    },
    {
      "title": "BMDS-Net: A Bayesian Multi-Modal Deep Supervision Network for Robust Brain Tumor Segmentation",
      "url": "https://arxiv.org/abs/2601.17504",
      "date": "2026-01-24",
      "authors_text": "Yan Zhou, Zhen Huang, Yingqiu Li, Yue Ouyang, Suncheng Xiang",
      "is_highlight": false,
      "score": 45.0,
      "summary": "BMDS-Net enhances brain tumor segmentation by integrating robust multimodal fusion and Bayesian fine-tuning for improved accuracy and uncertainty estimation in clinical MRI applications.",
      "teaser_image": null,
      "debug_abstract": "Accurate brain tumor segmentation from multi-modal magnetic resonance imaging (MRI) is a prerequisite for precise radiotherapy planning and surgical navigation. While recent Transformer-based models such as Swin UNETR have achieved impressive benchmark performance, their clinical utility is often compromised by two critical issues: sensitivity to missing modalities (common in clinical practice) and a lack of confidence calibration. Merely chasing higher Dice scores on idealized data fails to meet the safety requirements of real-world medical deployment. In this work, we propose BMDS-Net, a unified framework that prioritizes clinical robustness and trustworthiness over simple metric maximization. Our contribution is three-fold. First, we construct a robust deterministic backbone by integrating a Zero-Init Multimodal Contextual Fusion (MMCF) module and a Residual-Gated Deep Decoder Supervision (DDS) mechanism, enabling stable feature learning and precise boundary delineation with significantly reduced Hausdorff Distance, even under modality corruption. Second, and most importantly, we introduce a memory-efficient Bayesian fine-tuning strategy that transforms the network into a probabilistic predictor, providing voxel-wise uncertainty maps to highlight potential errors for clinicians. Third, comprehensive experiments on the BraTS 2021 dataset demonstrate that BMDS-Net not only maintains competitive accuracy but, more importantly, exhibits superior stability in missing-modality scenarios where baseline models fail. The source code is publicly available at this https URL."
    },
    {
      "title": "PILOT: A Perceptive Integrated Low-level Controller for Loco-manipulation over Unstructured Scenes",
      "url": "https://arxiv.org/abs/2601.17440",
      "date": "2026-01-24",
      "authors_text": "Xinru Cui, Linxi Feng, Yixuan Zhou, Haoqi Han, Zhe Liu",
      "is_highlight": true,
      "score": 92.0,
      "summary": "PILOT is a unified reinforcement learning framework that enhances humanoid robot loco-manipulation by integrating perceptive locomotion and whole-body control for improved task execution in unstructured environments.",
      "teaser_image": null,
      "debug_abstract": "Humanoid robots hold great potential for diverse interactions and daily service tasks within human-centered environments, necessitating controllers that seamlessly integrate precise locomotion with dexterous manipulation. However, most existing whole-body controllers lack exteroceptive awareness of the surrounding environment, rendering them insufficient for stable task execution in complex, unstructured this http URL address this challenge, we propose PILOT, a unified single-stage reinforcement learning (RL) framework tailored for perceptive loco-manipulation, which synergizes perceptive locomotion and expansive whole-body control within a single policy. To enhance terrain awareness and ensure precise foot placement, we design a cross-modal context encoder that fuses prediction-based proprioceptive features with attention-based perceptive representations. Furthermore, we introduce a Mixture-of-Experts (MoE) policy architecture to coordinate diverse motor skills, facilitating better specialization across distinct motion patterns. Extensive experiments in both simulation and on the physical Unitree G1 humanoid robot validate the efficacy of our framework. PILOT demonstrates superior stability, command tracking precision, and terrain traversability compared to existing baselines. These results highlight its potential to serve as a robust, foundational low-level controller for loco-manipulation in unstructured scenes."
    }
  ],
  "赵波老师实验室": [
    {
      "title": "Advances and Innovations in the Multi-Agent Robotic System (MARS) Challenge",
      "url": "https://arxiv.org/abs/2601.18733",
      "date": "2026-01-26",
      "authors_text": "Li Kang, Heng Zhou, Xiufeng Song, Rui Li, Bruno N. Y. Chen",
      "is_highlight": false,
      "score": 75.0,
      "summary": "The MARS Challenge at NeurIPS 2025 promotes advancements in multi-agent systems by exploring vision-language models for collaborative robotic planning and manipulation in dynamic environments.",
      "teaser_image": null,
      "debug_abstract": "Recent advancements in multimodal large language models and vision-languageaction models have significantly driven progress in Embodied AI. As the field transitions toward more complex task scenarios, multi-agent system frameworks are becoming essential for achieving scalable, efficient, and collaborative solutions. This shift is fueled by three primary factors: increasing agent capabilities, enhancing system efficiency through task delegation, and enabling advanced human-agent interactions. To address the challenges posed by multi-agent collaboration, we propose the Multi-Agent Robotic System (MARS) Challenge, held at the NeurIPS 2025 Workshop on SpaVLE. The competition focuses on two critical areas: planning and control, where participants explore multi-agent embodied planning using vision-language models (VLMs) to coordinate tasks and policy execution to perform robotic manipulation in dynamic environments. By evaluating solutions submitted by participants, the challenge provides valuable insights into the design and coordination of embodied multi-agent systems, contributing to the future development of advanced collaborative AI systems."
    },
    {
      "title": "ExoGS: A 4D Real-to-Sim-to-Real Framework for Scalable Manipulation Data Collection",
      "url": "https://arxiv.org/abs/2601.18629",
      "date": "2026-01-26",
      "authors_text": "Yiming Wang, Ruogu Zhang, Minyang Li, Hao Shi, Junbo Wang",
      "is_highlight": false,
      "score": 94.0,
      "summary": "ExoGS is a novel 4D framework that enhances scalable robotic manipulation data collection by effectively transferring real-world interactions to simulation using a passive exoskeleton.",
      "teaser_image": null,
      "debug_abstract": "Real-to-Sim-to-Real technique is gaining increasing interest for robotic manipulation, as it can generate scalable data in simulation while having narrower sim-to-real gap. However, previous methods mainly focused on environment-level visual real-to-sim transfer, ignoring the transfer of interactions, which could be challenging and inefficient to obtain purely in simulation especially for contact-rich tasks. We propose ExoGS, a robot-free 4D Real-to-Sim-to-Real framework that captures both static environments and dynamic interactions in the real world and transfers them seamlessly to a simulated environment. It provides a new solution for scalable manipulation data collection and policy learning. ExoGS employs a self-designed robot-isomorphic passive exoskeleton AirExo-3 to capture kinematically consistent trajectories with millimeter-level accuracy and synchronized RGB observations during direct human demonstrations. The robot, objects, and environment are reconstructed as editable 3D Gaussian Splatting assets, enabling geometry-consistent replay and large-scale data augmentation. Additionally, a lightweight Mask Adapter injects instance-level semantics into the policy to enhance robustness under visual domain shifts. Real-world experiments demonstrate that ExoGS significantly improves data efficiency and policy generalization compared to teleoperation-based baselines. Code and hardware files have been released on this https URL."
    },
    {
      "title": "QualiRAG: Retrieval-Augmented Generation for Visual Quality Understanding",
      "url": "https://arxiv.org/abs/2601.18195",
      "date": "2026-01-26",
      "authors_text": "Linhan Cao, Wei Sun, Weixia Zhang, Xiangyang Zhu, Kaiwei Zhang",
      "is_highlight": false,
      "score": 35.0,
      "summary": "QualiRAG introduces a training-free framework for visual quality understanding that utilizes dynamic retrieval and generation of contextual knowledge, outperforming existing models without task-specific training.",
      "teaser_image": null,
      "debug_abstract": "Visual quality assessment (VQA) is increasingly shifting from scalar score prediction toward interpretable quality understanding -- a paradigm that demands \\textit{fine-grained spatiotemporal perception} and \\textit{auxiliary contextual information}. Current approaches rely on supervised fine-tuning or reinforcement learning on curated instruction datasets, which involve labor-intensive annotation and are prone to dataset-specific biases. To address these challenges, we propose \\textbf{QualiRAG}, a \\textit{training-free} \\textbf{R}etrieval-\\textbf{A}ugmented \\textbf{G}eneration \\textbf{(RAG)} framework that systematically leverages the latent perceptual knowledge of large multimodal models (LMMs) for visual quality perception. Unlike conventional RAG that retrieves from static corpora, QualiRAG dynamically generates auxiliary knowledge by decomposing questions into structured requests and constructing four complementary knowledge sources: \\textit{visual metadata}, \\textit{subject localization}, \\textit{global quality summaries}, and \\textit{local quality descriptions}, followed by relevance-aware retrieval for evidence-grounded reasoning. Extensive experiments show that QualiRAG achieves substantial improvements over open-source general-purpose LMMs and VQA-finetuned LMMs on visual quality understanding tasks, and delivers competitive performance on visual quality comparison tasks, demonstrating robust quality assessment capabilities without any task-specific training. The code will be publicly available at this https URL."
    },
    {
      "title": "BMDS-Net: A Bayesian Multi-Modal Deep Supervision Network for Robust Brain Tumor Segmentation",
      "url": "https://arxiv.org/abs/2601.17504",
      "date": "2026-01-24",
      "authors_text": "Yan Zhou, Zhen Huang, Yingqiu Li, Yue Ouyang, Suncheng Xiang",
      "is_highlight": false,
      "score": 45.0,
      "summary": "BMDS-Net enhances brain tumor segmentation by integrating robust multimodal fusion and Bayesian fine-tuning for improved accuracy and uncertainty estimation in clinical MRI applications.",
      "teaser_image": null,
      "debug_abstract": "Accurate brain tumor segmentation from multi-modal magnetic resonance imaging (MRI) is a prerequisite for precise radiotherapy planning and surgical navigation. While recent Transformer-based models such as Swin UNETR have achieved impressive benchmark performance, their clinical utility is often compromised by two critical issues: sensitivity to missing modalities (common in clinical practice) and a lack of confidence calibration. Merely chasing higher Dice scores on idealized data fails to meet the safety requirements of real-world medical deployment. In this work, we propose BMDS-Net, a unified framework that prioritizes clinical robustness and trustworthiness over simple metric maximization. Our contribution is three-fold. First, we construct a robust deterministic backbone by integrating a Zero-Init Multimodal Contextual Fusion (MMCF) module and a Residual-Gated Deep Decoder Supervision (DDS) mechanism, enabling stable feature learning and precise boundary delineation with significantly reduced Hausdorff Distance, even under modality corruption. Second, and most importantly, we introduce a memory-efficient Bayesian fine-tuning strategy that transforms the network into a probabilistic predictor, providing voxel-wise uncertainty maps to highlight potential errors for clinicians. Third, comprehensive experiments on the BraTS 2021 dataset demonstrate that BMDS-Net not only maintains competitive accuracy but, more importantly, exhibits superior stability in missing-modality scenarios where baseline models fail. The source code is publicly available at this https URL."
    },
    {
      "title": "PILOT: A Perceptive Integrated Low-level Controller for Loco-manipulation over Unstructured Scenes",
      "url": "https://arxiv.org/abs/2601.17440",
      "date": "2026-01-24",
      "authors_text": "Xinru Cui, Linxi Feng, Yixuan Zhou, Haoqi Han, Zhe Liu",
      "is_highlight": true,
      "score": 92.0,
      "summary": "PILOT is a unified reinforcement learning framework that enhances humanoid robot loco-manipulation by integrating perceptive locomotion and whole-body control for improved task execution in unstructured environments.",
      "teaser_image": null,
      "debug_abstract": "Humanoid robots hold great potential for diverse interactions and daily service tasks within human-centered environments, necessitating controllers that seamlessly integrate precise locomotion with dexterous manipulation. However, most existing whole-body controllers lack exteroceptive awareness of the surrounding environment, rendering them insufficient for stable task execution in complex, unstructured this http URL address this challenge, we propose PILOT, a unified single-stage reinforcement learning (RL) framework tailored for perceptive loco-manipulation, which synergizes perceptive locomotion and expansive whole-body control within a single policy. To enhance terrain awareness and ensure precise foot placement, we design a cross-modal context encoder that fuses prediction-based proprioceptive features with attention-based perceptive representations. Furthermore, we introduce a Mixture-of-Experts (MoE) policy architecture to coordinate diverse motor skills, facilitating better specialization across distinct motion patterns. Extensive experiments in both simulation and on the physical Unitree G1 humanoid robot validate the efficacy of our framework. PILOT demonstrates superior stability, command tracking precision, and terrain traversability compared to existing baselines. These results highlight its potential to serve as a robust, foundational low-level controller for loco-manipulation in unstructured scenes."
    }
  ],
  "数字媒体与计算机视觉实验室": [
    {
      "title": "Advances and Innovations in the Multi-Agent Robotic System (MARS) Challenge",
      "url": "https://arxiv.org/abs/2601.18733",
      "date": "2026-01-26",
      "authors_text": "Li Kang, Heng Zhou, Xiufeng Song, Rui Li, Bruno N. Y. Chen",
      "is_highlight": false,
      "score": 75.0,
      "summary": "The MARS Challenge at NeurIPS 2025 promotes advancements in multi-agent systems by exploring vision-language models for collaborative robotic planning and manipulation in dynamic environments.",
      "teaser_image": null,
      "debug_abstract": "Recent advancements in multimodal large language models and vision-languageaction models have significantly driven progress in Embodied AI. As the field transitions toward more complex task scenarios, multi-agent system frameworks are becoming essential for achieving scalable, efficient, and collaborative solutions. This shift is fueled by three primary factors: increasing agent capabilities, enhancing system efficiency through task delegation, and enabling advanced human-agent interactions. To address the challenges posed by multi-agent collaboration, we propose the Multi-Agent Robotic System (MARS) Challenge, held at the NeurIPS 2025 Workshop on SpaVLE. The competition focuses on two critical areas: planning and control, where participants explore multi-agent embodied planning using vision-language models (VLMs) to coordinate tasks and policy execution to perform robotic manipulation in dynamic environments. By evaluating solutions submitted by participants, the challenge provides valuable insights into the design and coordination of embodied multi-agent systems, contributing to the future development of advanced collaborative AI systems."
    },
    {
      "title": "ExoGS: A 4D Real-to-Sim-to-Real Framework for Scalable Manipulation Data Collection",
      "url": "https://arxiv.org/abs/2601.18629",
      "date": "2026-01-26",
      "authors_text": "Yiming Wang, Ruogu Zhang, Minyang Li, Hao Shi, Junbo Wang",
      "is_highlight": false,
      "score": 94.0,
      "summary": "ExoGS is a novel 4D framework that enhances scalable robotic manipulation data collection by effectively transferring real-world interactions to simulation using a passive exoskeleton.",
      "teaser_image": null,
      "debug_abstract": "Real-to-Sim-to-Real technique is gaining increasing interest for robotic manipulation, as it can generate scalable data in simulation while having narrower sim-to-real gap. However, previous methods mainly focused on environment-level visual real-to-sim transfer, ignoring the transfer of interactions, which could be challenging and inefficient to obtain purely in simulation especially for contact-rich tasks. We propose ExoGS, a robot-free 4D Real-to-Sim-to-Real framework that captures both static environments and dynamic interactions in the real world and transfers them seamlessly to a simulated environment. It provides a new solution for scalable manipulation data collection and policy learning. ExoGS employs a self-designed robot-isomorphic passive exoskeleton AirExo-3 to capture kinematically consistent trajectories with millimeter-level accuracy and synchronized RGB observations during direct human demonstrations. The robot, objects, and environment are reconstructed as editable 3D Gaussian Splatting assets, enabling geometry-consistent replay and large-scale data augmentation. Additionally, a lightweight Mask Adapter injects instance-level semantics into the policy to enhance robustness under visual domain shifts. Real-world experiments demonstrate that ExoGS significantly improves data efficiency and policy generalization compared to teleoperation-based baselines. Code and hardware files have been released on this https URL."
    },
    {
      "title": "QualiRAG: Retrieval-Augmented Generation for Visual Quality Understanding",
      "url": "https://arxiv.org/abs/2601.18195",
      "date": "2026-01-26",
      "authors_text": "Linhan Cao, Wei Sun, Weixia Zhang, Xiangyang Zhu, Kaiwei Zhang",
      "is_highlight": false,
      "score": 35.0,
      "summary": "QualiRAG introduces a training-free framework for visual quality understanding that utilizes dynamic retrieval and generation of contextual knowledge, outperforming existing models without task-specific training.",
      "teaser_image": null,
      "debug_abstract": "Visual quality assessment (VQA) is increasingly shifting from scalar score prediction toward interpretable quality understanding -- a paradigm that demands \\textit{fine-grained spatiotemporal perception} and \\textit{auxiliary contextual information}. Current approaches rely on supervised fine-tuning or reinforcement learning on curated instruction datasets, which involve labor-intensive annotation and are prone to dataset-specific biases. To address these challenges, we propose \\textbf{QualiRAG}, a \\textit{training-free} \\textbf{R}etrieval-\\textbf{A}ugmented \\textbf{G}eneration \\textbf{(RAG)} framework that systematically leverages the latent perceptual knowledge of large multimodal models (LMMs) for visual quality perception. Unlike conventional RAG that retrieves from static corpora, QualiRAG dynamically generates auxiliary knowledge by decomposing questions into structured requests and constructing four complementary knowledge sources: \\textit{visual metadata}, \\textit{subject localization}, \\textit{global quality summaries}, and \\textit{local quality descriptions}, followed by relevance-aware retrieval for evidence-grounded reasoning. Extensive experiments show that QualiRAG achieves substantial improvements over open-source general-purpose LMMs and VQA-finetuned LMMs on visual quality understanding tasks, and delivers competitive performance on visual quality comparison tasks, demonstrating robust quality assessment capabilities without any task-specific training. The code will be publicly available at this https URL."
    },
    {
      "title": "BMDS-Net: A Bayesian Multi-Modal Deep Supervision Network for Robust Brain Tumor Segmentation",
      "url": "https://arxiv.org/abs/2601.17504",
      "date": "2026-01-24",
      "authors_text": "Yan Zhou, Zhen Huang, Yingqiu Li, Yue Ouyang, Suncheng Xiang",
      "is_highlight": false,
      "score": 45.0,
      "summary": "BMDS-Net enhances brain tumor segmentation by integrating robust multimodal fusion and Bayesian fine-tuning for improved accuracy and uncertainty estimation in clinical MRI applications.",
      "teaser_image": null,
      "debug_abstract": "Accurate brain tumor segmentation from multi-modal magnetic resonance imaging (MRI) is a prerequisite for precise radiotherapy planning and surgical navigation. While recent Transformer-based models such as Swin UNETR have achieved impressive benchmark performance, their clinical utility is often compromised by two critical issues: sensitivity to missing modalities (common in clinical practice) and a lack of confidence calibration. Merely chasing higher Dice scores on idealized data fails to meet the safety requirements of real-world medical deployment. In this work, we propose BMDS-Net, a unified framework that prioritizes clinical robustness and trustworthiness over simple metric maximization. Our contribution is three-fold. First, we construct a robust deterministic backbone by integrating a Zero-Init Multimodal Contextual Fusion (MMCF) module and a Residual-Gated Deep Decoder Supervision (DDS) mechanism, enabling stable feature learning and precise boundary delineation with significantly reduced Hausdorff Distance, even under modality corruption. Second, and most importantly, we introduce a memory-efficient Bayesian fine-tuning strategy that transforms the network into a probabilistic predictor, providing voxel-wise uncertainty maps to highlight potential errors for clinicians. Third, comprehensive experiments on the BraTS 2021 dataset demonstrate that BMDS-Net not only maintains competitive accuracy but, more importantly, exhibits superior stability in missing-modality scenarios where baseline models fail. The source code is publicly available at this https URL."
    },
    {
      "title": "PILOT: A Perceptive Integrated Low-level Controller for Loco-manipulation over Unstructured Scenes",
      "url": "https://arxiv.org/abs/2601.17440",
      "date": "2026-01-24",
      "authors_text": "Xinru Cui, Linxi Feng, Yixuan Zhou, Haoqi Han, Zhe Liu",
      "is_highlight": true,
      "score": 92.0,
      "summary": "PILOT is a unified reinforcement learning framework that enhances humanoid robot loco-manipulation by integrating perceptive locomotion and whole-body control for improved task execution in unstructured environments.",
      "teaser_image": null,
      "debug_abstract": "Humanoid robots hold great potential for diverse interactions and daily service tasks within human-centered environments, necessitating controllers that seamlessly integrate precise locomotion with dexterous manipulation. However, most existing whole-body controllers lack exteroceptive awareness of the surrounding environment, rendering them insufficient for stable task execution in complex, unstructured this http URL address this challenge, we propose PILOT, a unified single-stage reinforcement learning (RL) framework tailored for perceptive loco-manipulation, which synergizes perceptive locomotion and expansive whole-body control within a single policy. To enhance terrain awareness and ensure precise foot placement, we design a cross-modal context encoder that fuses prediction-based proprioceptive features with attention-based perceptive representations. Furthermore, we introduce a Mixture-of-Experts (MoE) policy architecture to coordinate diverse motor skills, facilitating better specialization across distinct motion patterns. Extensive experiments in both simulation and on the physical Unitree G1 humanoid robot validate the efficacy of our framework. PILOT demonstrates superior stability, command tracking precision, and terrain traversability compared to existing baselines. These results highlight its potential to serve as a robust, foundational low-level controller for loco-manipulation in unstructured scenes."
    }
  ],
  "具身感知与交互实验室 (EPIC Lab)": [
    {
      "title": "TC-IDM: Grounding Video Generation for Executable Zero-shot Robot Motion",
      "url": "https://arxiv.org/abs/2601.18323",
      "date": "2026-01-26",
      "authors_text": "Weishi Mi, Yong Bao, Xiaowei Chi, Xiaozhu Ju, Zhiyuan Qin",
      "is_highlight": false,
      "score": 90.0,
      "summary": "The TC-IDM framework enhances robotic control by bridging visual planning and physical execution through tool-centric trajectory modeling, achieving superior generalization and performance in diverse tasks.",
      "teaser_image": null,
      "debug_abstract": "The vision-language-action (VLA) paradigm has enabled powerful robotic control by leveraging vision-language models, but its reliance on large-scale, high-quality robot data limits its generalization. Generative world models offer a promising alternative for general-purpose embodied AI, yet a critical gap remains between their pixel-level plans and physically executable actions. To this end, we propose the Tool-Centric Inverse Dynamics Model (TC-IDM). By focusing on the tool&#39;s imagined trajectory as synthesized by the world model, TC-IDM establishes a robust intermediate representation that bridges the gap between visual planning and physical control. TC-IDM extracts the tool&#39;s point cloud trajectories via segmentation and 3D motion estimation from generated videos. Considering diverse tool attributes, our architecture employs decoupled action heads to project these planned trajectories into 6-DoF end-effector motions and corresponding control signals. This plan-and-translate paradigm not only supports a wide range of end-effectors but also significantly improves viewpoint invariance. Furthermore, it exhibits strong generalization capabilities across long-horizon and out-of-distribution tasks, including interacting with deformable objects. In real-world evaluations, the world model with TC-IDM achieves an average success rate of 61.11 percent, with 77.7 percent on simple tasks and 38.46 percent on zero-shot deformable object tasks. It substantially outperforms end-to-end VLA-style baselines and other inverse dynamics models."
    },
    {
      "title": "Agentic reinforcement learning empowers next-generation chemical language models for molecular design and synthesis",
      "url": "https://arxiv.org/abs/2601.17687",
      "date": "2026-01-25",
      "authors_text": "Hao Li, He Cao, Shenyao Peng, Zijing Liu, Bin Feng",
      "is_highlight": false,
      "score": 50.0,
      "summary": "ChemCRAFT utilizes agentic reinforcement learning to enhance small chemical language models for efficient, privacy-preserving molecular design and synthesis through external knowledge retrieval.",
      "teaser_image": null,
      "debug_abstract": "Language models are revolutionizing the biochemistry domain, assisting scientists in drug design and chemical synthesis with high efficiency. Yet current approaches struggle between small language models prone to hallucination and limited knowledge retention, and large cloud-based language models plagued by privacy risks and high inference costs. To bridge this gap, we introduce ChemCRAFT, a novel framework leveraging agentic reinforcement learning to decouple chemical reasoning from knowledge storage. Instead of forcing the model to memorize vast chemical data, our approach empowers the language model to interact with a sandbox for precise information retrieval. This externalization of knowledge allows a locally deployable small model to achieve superior performance with minimal inference costs. To enable small language models for agent-calling ability, we build an agentic trajectory construction pipeline and a comprehensive chemical-agent sandbox. Based on sandbox interactions, we constructed ChemToolDataset, the first large-scale chemical tool trajectory dataset. Simultaneously, we propose SMILES-GRPO to build a dense chemical reward function, promoting the model&#39;s ability to call chemical agents. Evaluations across diverse aspects of drug design show that ChemCRAFT outperforms current cloud-based LLMs in molecular structure analysis, molecular optimization, and synthesis pathway prediction, demonstrating that scientific reasoning is not solely an emergent ability of model scale, but a learnable policy of tool orchestration. This work establishes a cost-effective and privacy-preserving paradigm for AI-aided chemistry, opening new avenues for accelerating molecular discovery with locally deployable agents. Code available at this https URL."
    },
    {
      "title": "Uni-RS: A Spatially Faithful Unified Understanding and Generation Model for Remote Sensing",
      "url": "https://arxiv.org/abs/2601.17673",
      "date": "2026-01-25",
      "authors_text": "Weiyu Zhang, Yuan Hu, Yong Li, Yu Liu",
      "is_highlight": false,
      "score": 40.0,
      "summary": "Uni-RS is a novel multimodal model for remote sensing that enhances spatial fidelity in text-to-image generation by integrating spatial layout planning and query supervision.",
      "teaser_image": null,
      "debug_abstract": "Unified remote sensing multimodal models exhibit a pronounced spatial reversal curse: Although they can accurately recognize and describe object locations in images, they often fail to faithfully execute the same spatial relations during text-to-image generation, where such relations constitute core semantic information in remote sensing. Motivated by this observation, we propose Uni-RS, the first unified multimodal model tailored for remote sensing, to explicitly address the spatial asymmetry between understanding and generation. Specifically, we first introduce explicit Spatial-Layout Planning to transform textual instructions into spatial layout plans, decoupling geometric planning from visual synthesis. We then impose Spatial-Aware Query Supervision to bias learnable queries toward spatial relations explicitly specified in the instruction. Finally, we develop Image-Caption Spatial Layout Variation to expose the model to systematic geometry-consistent spatial transformations. Extensive experiments across multiple benchmarks show that our approach substantially improves spatial faithfulness in text-to-image generation, while maintaining strong performance on multimodal understanding tasks like image captioning, visual grounding, and VQA tasks."
    }
  ],
  "智元机器人联合实验室 (PKU-Agibot)": [
    {
      "title": "TC-IDM: Grounding Video Generation for Executable Zero-shot Robot Motion",
      "url": "https://arxiv.org/abs/2601.18323",
      "date": "2026-01-26",
      "authors_text": "Weishi Mi, Yong Bao, Xiaowei Chi, Xiaozhu Ju, Zhiyuan Qin",
      "is_highlight": false,
      "score": 90.0,
      "summary": "The TC-IDM framework enhances robotic control by bridging visual planning and physical execution through tool-centric trajectory modeling, achieving superior generalization and performance in diverse tasks.",
      "teaser_image": null,
      "debug_abstract": "The vision-language-action (VLA) paradigm has enabled powerful robotic control by leveraging vision-language models, but its reliance on large-scale, high-quality robot data limits its generalization. Generative world models offer a promising alternative for general-purpose embodied AI, yet a critical gap remains between their pixel-level plans and physically executable actions. To this end, we propose the Tool-Centric Inverse Dynamics Model (TC-IDM). By focusing on the tool&#39;s imagined trajectory as synthesized by the world model, TC-IDM establishes a robust intermediate representation that bridges the gap between visual planning and physical control. TC-IDM extracts the tool&#39;s point cloud trajectories via segmentation and 3D motion estimation from generated videos. Considering diverse tool attributes, our architecture employs decoupled action heads to project these planned trajectories into 6-DoF end-effector motions and corresponding control signals. This plan-and-translate paradigm not only supports a wide range of end-effectors but also significantly improves viewpoint invariance. Furthermore, it exhibits strong generalization capabilities across long-horizon and out-of-distribution tasks, including interacting with deformable objects. In real-world evaluations, the world model with TC-IDM achieves an average success rate of 61.11 percent, with 77.7 percent on simple tasks and 38.46 percent on zero-shot deformable object tasks. It substantially outperforms end-to-end VLA-style baselines and other inverse dynamics models."
    },
    {
      "title": "Agentic reinforcement learning empowers next-generation chemical language models for molecular design and synthesis",
      "url": "https://arxiv.org/abs/2601.17687",
      "date": "2026-01-25",
      "authors_text": "Hao Li, He Cao, Shenyao Peng, Zijing Liu, Bin Feng",
      "is_highlight": false,
      "score": 50.0,
      "summary": "ChemCRAFT utilizes agentic reinforcement learning to enhance small chemical language models for efficient, privacy-preserving molecular design and synthesis through external knowledge retrieval.",
      "teaser_image": null,
      "debug_abstract": "Language models are revolutionizing the biochemistry domain, assisting scientists in drug design and chemical synthesis with high efficiency. Yet current approaches struggle between small language models prone to hallucination and limited knowledge retention, and large cloud-based language models plagued by privacy risks and high inference costs. To bridge this gap, we introduce ChemCRAFT, a novel framework leveraging agentic reinforcement learning to decouple chemical reasoning from knowledge storage. Instead of forcing the model to memorize vast chemical data, our approach empowers the language model to interact with a sandbox for precise information retrieval. This externalization of knowledge allows a locally deployable small model to achieve superior performance with minimal inference costs. To enable small language models for agent-calling ability, we build an agentic trajectory construction pipeline and a comprehensive chemical-agent sandbox. Based on sandbox interactions, we constructed ChemToolDataset, the first large-scale chemical tool trajectory dataset. Simultaneously, we propose SMILES-GRPO to build a dense chemical reward function, promoting the model&#39;s ability to call chemical agents. Evaluations across diverse aspects of drug design show that ChemCRAFT outperforms current cloud-based LLMs in molecular structure analysis, molecular optimization, and synthesis pathway prediction, demonstrating that scientific reasoning is not solely an emergent ability of model scale, but a learnable policy of tool orchestration. This work establishes a cost-effective and privacy-preserving paradigm for AI-aided chemistry, opening new avenues for accelerating molecular discovery with locally deployable agents. Code available at this https URL."
    },
    {
      "title": "Uni-RS: A Spatially Faithful Unified Understanding and Generation Model for Remote Sensing",
      "url": "https://arxiv.org/abs/2601.17673",
      "date": "2026-01-25",
      "authors_text": "Weiyu Zhang, Yuan Hu, Yong Li, Yu Liu",
      "is_highlight": false,
      "score": 40.0,
      "summary": "Uni-RS is a novel multimodal model for remote sensing that enhances spatial fidelity in text-to-image generation by integrating spatial layout planning and query supervision.",
      "teaser_image": null,
      "debug_abstract": "Unified remote sensing multimodal models exhibit a pronounced spatial reversal curse: Although they can accurately recognize and describe object locations in images, they often fail to faithfully execute the same spatial relations during text-to-image generation, where such relations constitute core semantic information in remote sensing. Motivated by this observation, we propose Uni-RS, the first unified multimodal model tailored for remote sensing, to explicitly address the spatial asymmetry between understanding and generation. Specifically, we first introduce explicit Spatial-Layout Planning to transform textual instructions into spatial layout plans, decoupling geometric planning from visual synthesis. We then impose Spatial-Aware Query Supervision to bias learnable queries toward spatial relations explicitly specified in the instruction. Finally, we develop Image-Caption Spatial Layout Variation to expose the model to systematic geometry-consistent spatial transformations. Extensive experiments across multiple benchmarks show that our approach substantially improves spatial faithfulness in text-to-image generation, while maintaining strong performance on multimodal understanding tasks like image captioning, visual grounding, and VQA tasks."
    }
  ],
  "情感与认知智能机器人实验室 (ACIR)": [
    {
      "title": "TC-IDM: Grounding Video Generation for Executable Zero-shot Robot Motion",
      "url": "https://arxiv.org/abs/2601.18323",
      "date": "2026-01-26",
      "authors_text": "Weishi Mi, Yong Bao, Xiaowei Chi, Xiaozhu Ju, Zhiyuan Qin",
      "is_highlight": false,
      "score": 90.0,
      "summary": "The TC-IDM framework enhances robotic control by bridging visual planning and physical execution through tool-centric trajectory modeling, achieving superior generalization and performance in diverse tasks.",
      "teaser_image": null,
      "debug_abstract": "The vision-language-action (VLA) paradigm has enabled powerful robotic control by leveraging vision-language models, but its reliance on large-scale, high-quality robot data limits its generalization. Generative world models offer a promising alternative for general-purpose embodied AI, yet a critical gap remains between their pixel-level plans and physically executable actions. To this end, we propose the Tool-Centric Inverse Dynamics Model (TC-IDM). By focusing on the tool&#39;s imagined trajectory as synthesized by the world model, TC-IDM establishes a robust intermediate representation that bridges the gap between visual planning and physical control. TC-IDM extracts the tool&#39;s point cloud trajectories via segmentation and 3D motion estimation from generated videos. Considering diverse tool attributes, our architecture employs decoupled action heads to project these planned trajectories into 6-DoF end-effector motions and corresponding control signals. This plan-and-translate paradigm not only supports a wide range of end-effectors but also significantly improves viewpoint invariance. Furthermore, it exhibits strong generalization capabilities across long-horizon and out-of-distribution tasks, including interacting with deformable objects. In real-world evaluations, the world model with TC-IDM achieves an average success rate of 61.11 percent, with 77.7 percent on simple tasks and 38.46 percent on zero-shot deformable object tasks. It substantially outperforms end-to-end VLA-style baselines and other inverse dynamics models."
    },
    {
      "title": "Agentic reinforcement learning empowers next-generation chemical language models for molecular design and synthesis",
      "url": "https://arxiv.org/abs/2601.17687",
      "date": "2026-01-25",
      "authors_text": "Hao Li, He Cao, Shenyao Peng, Zijing Liu, Bin Feng",
      "is_highlight": false,
      "score": 50.0,
      "summary": "ChemCRAFT utilizes agentic reinforcement learning to enhance small chemical language models for efficient, privacy-preserving molecular design and synthesis through external knowledge retrieval.",
      "teaser_image": null,
      "debug_abstract": "Language models are revolutionizing the biochemistry domain, assisting scientists in drug design and chemical synthesis with high efficiency. Yet current approaches struggle between small language models prone to hallucination and limited knowledge retention, and large cloud-based language models plagued by privacy risks and high inference costs. To bridge this gap, we introduce ChemCRAFT, a novel framework leveraging agentic reinforcement learning to decouple chemical reasoning from knowledge storage. Instead of forcing the model to memorize vast chemical data, our approach empowers the language model to interact with a sandbox for precise information retrieval. This externalization of knowledge allows a locally deployable small model to achieve superior performance with minimal inference costs. To enable small language models for agent-calling ability, we build an agentic trajectory construction pipeline and a comprehensive chemical-agent sandbox. Based on sandbox interactions, we constructed ChemToolDataset, the first large-scale chemical tool trajectory dataset. Simultaneously, we propose SMILES-GRPO to build a dense chemical reward function, promoting the model&#39;s ability to call chemical agents. Evaluations across diverse aspects of drug design show that ChemCRAFT outperforms current cloud-based LLMs in molecular structure analysis, molecular optimization, and synthesis pathway prediction, demonstrating that scientific reasoning is not solely an emergent ability of model scale, but a learnable policy of tool orchestration. This work establishes a cost-effective and privacy-preserving paradigm for AI-aided chemistry, opening new avenues for accelerating molecular discovery with locally deployable agents. Code available at this https URL."
    },
    {
      "title": "Uni-RS: A Spatially Faithful Unified Understanding and Generation Model for Remote Sensing",
      "url": "https://arxiv.org/abs/2601.17673",
      "date": "2026-01-25",
      "authors_text": "Weiyu Zhang, Yuan Hu, Yong Li, Yu Liu",
      "is_highlight": false,
      "score": 40.0,
      "summary": "Uni-RS is a novel multimodal model for remote sensing that enhances spatial fidelity in text-to-image generation by integrating spatial layout planning and query supervision.",
      "teaser_image": null,
      "debug_abstract": "Unified remote sensing multimodal models exhibit a pronounced spatial reversal curse: Although they can accurately recognize and describe object locations in images, they often fail to faithfully execute the same spatial relations during text-to-image generation, where such relations constitute core semantic information in remote sensing. Motivated by this observation, we propose Uni-RS, the first unified multimodal model tailored for remote sensing, to explicitly address the spatial asymmetry between understanding and generation. Specifically, we first introduce explicit Spatial-Layout Planning to transform textual instructions into spatial layout plans, decoupling geometric planning from visual synthesis. We then impose Spatial-Aware Query Supervision to bias learnable queries toward spatial relations explicitly specified in the instruction. Finally, we develop Image-Caption Spatial Layout Variation to expose the model to systematic geometry-consistent spatial transformations. Extensive experiments across multiple benchmarks show that our approach substantially improves spatial faithfulness in text-to-image generation, while maintaining strong performance on multimodal understanding tasks like image captioning, visual grounding, and VQA tasks."
    }
  ],
  "HMI Lab": [
    {
      "title": "TC-IDM: Grounding Video Generation for Executable Zero-shot Robot Motion",
      "url": "https://arxiv.org/abs/2601.18323",
      "date": "2026-01-26",
      "authors_text": "Weishi Mi, Yong Bao, Xiaowei Chi, Xiaozhu Ju, Zhiyuan Qin",
      "is_highlight": false,
      "score": 90.0,
      "summary": "The TC-IDM framework enhances robotic control by bridging visual planning and physical execution through tool-centric trajectory modeling, achieving superior generalization and performance in diverse tasks.",
      "teaser_image": null,
      "debug_abstract": "The vision-language-action (VLA) paradigm has enabled powerful robotic control by leveraging vision-language models, but its reliance on large-scale, high-quality robot data limits its generalization. Generative world models offer a promising alternative for general-purpose embodied AI, yet a critical gap remains between their pixel-level plans and physically executable actions. To this end, we propose the Tool-Centric Inverse Dynamics Model (TC-IDM). By focusing on the tool&#39;s imagined trajectory as synthesized by the world model, TC-IDM establishes a robust intermediate representation that bridges the gap between visual planning and physical control. TC-IDM extracts the tool&#39;s point cloud trajectories via segmentation and 3D motion estimation from generated videos. Considering diverse tool attributes, our architecture employs decoupled action heads to project these planned trajectories into 6-DoF end-effector motions and corresponding control signals. This plan-and-translate paradigm not only supports a wide range of end-effectors but also significantly improves viewpoint invariance. Furthermore, it exhibits strong generalization capabilities across long-horizon and out-of-distribution tasks, including interacting with deformable objects. In real-world evaluations, the world model with TC-IDM achieves an average success rate of 61.11 percent, with 77.7 percent on simple tasks and 38.46 percent on zero-shot deformable object tasks. It substantially outperforms end-to-end VLA-style baselines and other inverse dynamics models."
    },
    {
      "title": "Agentic reinforcement learning empowers next-generation chemical language models for molecular design and synthesis",
      "url": "https://arxiv.org/abs/2601.17687",
      "date": "2026-01-25",
      "authors_text": "Hao Li, He Cao, Shenyao Peng, Zijing Liu, Bin Feng",
      "is_highlight": false,
      "score": 50.0,
      "summary": "ChemCRAFT utilizes agentic reinforcement learning to enhance small chemical language models for efficient, privacy-preserving molecular design and synthesis through external knowledge retrieval.",
      "teaser_image": null,
      "debug_abstract": "Language models are revolutionizing the biochemistry domain, assisting scientists in drug design and chemical synthesis with high efficiency. Yet current approaches struggle between small language models prone to hallucination and limited knowledge retention, and large cloud-based language models plagued by privacy risks and high inference costs. To bridge this gap, we introduce ChemCRAFT, a novel framework leveraging agentic reinforcement learning to decouple chemical reasoning from knowledge storage. Instead of forcing the model to memorize vast chemical data, our approach empowers the language model to interact with a sandbox for precise information retrieval. This externalization of knowledge allows a locally deployable small model to achieve superior performance with minimal inference costs. To enable small language models for agent-calling ability, we build an agentic trajectory construction pipeline and a comprehensive chemical-agent sandbox. Based on sandbox interactions, we constructed ChemToolDataset, the first large-scale chemical tool trajectory dataset. Simultaneously, we propose SMILES-GRPO to build a dense chemical reward function, promoting the model&#39;s ability to call chemical agents. Evaluations across diverse aspects of drug design show that ChemCRAFT outperforms current cloud-based LLMs in molecular structure analysis, molecular optimization, and synthesis pathway prediction, demonstrating that scientific reasoning is not solely an emergent ability of model scale, but a learnable policy of tool orchestration. This work establishes a cost-effective and privacy-preserving paradigm for AI-aided chemistry, opening new avenues for accelerating molecular discovery with locally deployable agents. Code available at this https URL."
    },
    {
      "title": "Uni-RS: A Spatially Faithful Unified Understanding and Generation Model for Remote Sensing",
      "url": "https://arxiv.org/abs/2601.17673",
      "date": "2026-01-25",
      "authors_text": "Weiyu Zhang, Yuan Hu, Yong Li, Yu Liu",
      "is_highlight": false,
      "score": 40.0,
      "summary": "Uni-RS is a novel multimodal model for remote sensing that enhances spatial fidelity in text-to-image generation by integrating spatial layout planning and query supervision.",
      "teaser_image": null,
      "debug_abstract": "Unified remote sensing multimodal models exhibit a pronounced spatial reversal curse: Although they can accurately recognize and describe object locations in images, they often fail to faithfully execute the same spatial relations during text-to-image generation, where such relations constitute core semantic information in remote sensing. Motivated by this observation, we propose Uni-RS, the first unified multimodal model tailored for remote sensing, to explicitly address the spatial asymmetry between understanding and generation. Specifically, we first introduce explicit Spatial-Layout Planning to transform textual instructions into spatial layout plans, decoupling geometric planning from visual synthesis. We then impose Spatial-Aware Query Supervision to bias learnable queries toward spatial relations explicitly specified in the instruction. Finally, we develop Image-Caption Spatial Layout Variation to expose the model to systematic geometry-consistent spatial transformations. Extensive experiments across multiple benchmarks show that our approach substantially improves spatial faithfulness in text-to-image generation, while maintaining strong performance on multimodal understanding tasks like image captioning, visual grounding, and VQA tasks."
    }
  ],
  "多智能体与具身智能研究所": [
    {
      "title": "Agentic reinforcement learning empowers next-generation chemical language models for molecular design and synthesis",
      "url": "https://arxiv.org/abs/2601.17687",
      "date": "2026-01-25",
      "authors_text": "Hao Li, He Cao, Shenyao Peng, Zijing Liu, Bin Feng",
      "is_highlight": false,
      "score": 50.0,
      "summary": "ChemCRAFT utilizes agentic reinforcement learning to enhance small chemical language models for efficient, privacy-preserving molecular design and synthesis through external knowledge retrieval.",
      "teaser_image": null,
      "debug_abstract": "Language models are revolutionizing the biochemistry domain, assisting scientists in drug design and chemical synthesis with high efficiency. Yet current approaches struggle between small language models prone to hallucination and limited knowledge retention, and large cloud-based language models plagued by privacy risks and high inference costs. To bridge this gap, we introduce ChemCRAFT, a novel framework leveraging agentic reinforcement learning to decouple chemical reasoning from knowledge storage. Instead of forcing the model to memorize vast chemical data, our approach empowers the language model to interact with a sandbox for precise information retrieval. This externalization of knowledge allows a locally deployable small model to achieve superior performance with minimal inference costs. To enable small language models for agent-calling ability, we build an agentic trajectory construction pipeline and a comprehensive chemical-agent sandbox. Based on sandbox interactions, we constructed ChemToolDataset, the first large-scale chemical tool trajectory dataset. Simultaneously, we propose SMILES-GRPO to build a dense chemical reward function, promoting the model&#39;s ability to call chemical agents. Evaluations across diverse aspects of drug design show that ChemCRAFT outperforms current cloud-based LLMs in molecular structure analysis, molecular optimization, and synthesis pathway prediction, demonstrating that scientific reasoning is not solely an emergent ability of model scale, but a learnable policy of tool orchestration. This work establishes a cost-effective and privacy-preserving paradigm for AI-aided chemistry, opening new avenues for accelerating molecular discovery with locally deployable agents. Code available at this https URL."
    }
  ],
  "斯坦福-视觉与学习实验室 (SVL)": [
    {
      "title": "PaperSearchQA: Learning to Search and Reason over Scientific Papers with RLVR",
      "url": "https://arxiv.org/abs/2601.18207",
      "date": "2026-01-26",
      "authors_text": "James Burgess, Jan N. Hansen, Duo Peng, Yuhui Zhang, Alejandro Lozano",
      "is_highlight": false,
      "score": 25.0,
      "summary": "The paper introduces PaperSearchQA, a reinforcement learning framework for training search agents to effectively reason and answer questions using a vast corpus of biomedical paper abstracts.",
      "teaser_image": null,
      "debug_abstract": "Search agents are language models (LMs) that reason and search knowledge bases (or the web) to answer questions; recent methods supervise only the final answer accuracy using reinforcement learning with verifiable rewards (RLVR). Most RLVR search agents tackle general-domain QA, which limits their relevance to technical AI systems in science, engineering, and medicine. In this work we propose training agents to search and reason over scientific papers -- this tests technical question-answering, it is directly relevant to real scientists, and the capabilities will be crucial to future AI Scientist systems. Concretely, we release a search corpus of 16 million biomedical paper abstracts and construct a challenging factoid QA dataset called PaperSearchQA with 60k samples answerable from the corpus, along with benchmarks. We train search agents in this environment to outperform non-RL retrieval baselines; we also perform further quantitative analysis and observe interesting agent behaviors like planning, reasoning, and self-verification. Our corpus, datasets, and benchmarks are usable with the popular Search-R1 codebase for RLVR training and released on this https URL. Finally, our data creation methods are scalable and easily extendable to other scientific domains."
    },
    {
      "title": "Image2Garment: Simulation-ready Garment Generation from a Single Image",
      "url": "https://arxiv.org/abs/2601.09658",
      "date": "2026-01-25",
      "authors_text": "Selim Emir Can, Jan Ackermann, Kiyohiro Nakayama, Ruofan Liu, Tong Wu",
      "is_highlight": false,
      "score": 55.0,
      "summary": "The paper presents Image2Garment, a framework that generates simulation-ready garments from a single image by inferring material properties and fabric attributes without iterative optimization.",
      "teaser_image": null,
      "debug_abstract": "Estimating physically accurate, simulation-ready garments from a single image is challenging due to the absence of image-to-physics datasets and the ill-posed nature of this problem. Prior methods either require multi-view capture and expensive differentiable simulation or predict only garment geometry without the material properties required for realistic simulation. We propose a feed-forward framework that sidesteps these limitations by first fine-tuning a vision-language model to infer material composition and fabric attributes from real images, and then training a lightweight predictor that maps these attributes to the corresponding physical fabric parameters using a small dataset of material-physics measurements. Our approach introduces two new datasets (FTAG and T2P) and delivers simulation-ready garments from a single image without iterative optimization. Experiments show that our estimator achieves superior accuracy in material composition estimation and fabric attribute prediction, and by passing them through our physics parameter estimator, we further achieve higher-fidelity simulations compared to state-of-the-art image-to-garment methods."
    },
    {
      "title": "SQL-Trail: Multi-Turn Reinforcement Learning with Interleaved Feedback for Text-to-SQL",
      "url": "https://arxiv.org/abs/2601.17699",
      "date": "2026-01-25",
      "authors_text": "Harper Hua, Zhen Han, Zhengyuan Shen, Jeremy Lee, Patrick Guan",
      "is_highlight": false,
      "score": 30.0,
      "summary": "SQL-Trail introduces a multi-turn reinforcement learning framework for Text-to-SQL, enhancing query generation through iterative feedback and adaptive interaction, achieving state-of-the-art performance.",
      "teaser_image": null,
      "debug_abstract": "While large language models (LLMs) have substantially improved Text-to-SQL generation, a pronounced gap remains between AI systems and human experts on challenging benchmarks such as BIRD-SQL. We argue this gap stems largely from the prevailing single-pass paradigm, which lacks the iterative reasoning, schema exploration, and error-correction behaviors that humans naturally employ. To address this limitation, we introduce SQL-Trail, a multi-turn reinforcement learning (RL) agentic framework for Text-to-SQL. Rather than producing a query in one shot, SQL-Trail interacts with the database environment and uses execution feedback to iteratively refine its predictions. Our approach centers on two key ideas: (i) an adaptive turn-budget allocation mechanism that scales the agent&#39;s interaction depth to match question difficulty, and (ii) a composite reward panel that jointly incentivizes SQL correctness and efficient exploration. Across benchmarks, SQL-Trail sets a new state of the art and delivers strong data efficiency--up to 18x higher than prior single-pass RL state-of-the-art methods. Notably, our 7B and 14B models outperform substantially larger proprietary systems by 5% on average, underscoring the effectiveness of interactive, agentic workflows for robust Text-to-SQL generation."
    }
  ],
  "Robotics and Embodied Artificial Intelligence Lab (REAL)": [
    {
      "title": "PaperSearchQA: Learning to Search and Reason over Scientific Papers with RLVR",
      "url": "https://arxiv.org/abs/2601.18207",
      "date": "2026-01-26",
      "authors_text": "James Burgess, Jan N. Hansen, Duo Peng, Yuhui Zhang, Alejandro Lozano",
      "is_highlight": false,
      "score": 25.0,
      "summary": "The paper introduces PaperSearchQA, a reinforcement learning framework for training search agents to effectively reason and answer questions using a vast corpus of biomedical paper abstracts.",
      "teaser_image": null,
      "debug_abstract": "Search agents are language models (LMs) that reason and search knowledge bases (or the web) to answer questions; recent methods supervise only the final answer accuracy using reinforcement learning with verifiable rewards (RLVR). Most RLVR search agents tackle general-domain QA, which limits their relevance to technical AI systems in science, engineering, and medicine. In this work we propose training agents to search and reason over scientific papers -- this tests technical question-answering, it is directly relevant to real scientists, and the capabilities will be crucial to future AI Scientist systems. Concretely, we release a search corpus of 16 million biomedical paper abstracts and construct a challenging factoid QA dataset called PaperSearchQA with 60k samples answerable from the corpus, along with benchmarks. We train search agents in this environment to outperform non-RL retrieval baselines; we also perform further quantitative analysis and observe interesting agent behaviors like planning, reasoning, and self-verification. Our corpus, datasets, and benchmarks are usable with the popular Search-R1 codebase for RLVR training and released on this https URL. Finally, our data creation methods are scalable and easily extendable to other scientific domains."
    },
    {
      "title": "Image2Garment: Simulation-ready Garment Generation from a Single Image",
      "url": "https://arxiv.org/abs/2601.09658",
      "date": "2026-01-25",
      "authors_text": "Selim Emir Can, Jan Ackermann, Kiyohiro Nakayama, Ruofan Liu, Tong Wu",
      "is_highlight": false,
      "score": 55.0,
      "summary": "The paper presents Image2Garment, a framework that generates simulation-ready garments from a single image by inferring material properties and fabric attributes without iterative optimization.",
      "teaser_image": null,
      "debug_abstract": "Estimating physically accurate, simulation-ready garments from a single image is challenging due to the absence of image-to-physics datasets and the ill-posed nature of this problem. Prior methods either require multi-view capture and expensive differentiable simulation or predict only garment geometry without the material properties required for realistic simulation. We propose a feed-forward framework that sidesteps these limitations by first fine-tuning a vision-language model to infer material composition and fabric attributes from real images, and then training a lightweight predictor that maps these attributes to the corresponding physical fabric parameters using a small dataset of material-physics measurements. Our approach introduces two new datasets (FTAG and T2P) and delivers simulation-ready garments from a single image without iterative optimization. Experiments show that our estimator achieves superior accuracy in material composition estimation and fabric attribute prediction, and by passing them through our physics parameter estimator, we further achieve higher-fidelity simulations compared to state-of-the-art image-to-garment methods."
    },
    {
      "title": "SQL-Trail: Multi-Turn Reinforcement Learning with Interleaved Feedback for Text-to-SQL",
      "url": "https://arxiv.org/abs/2601.17699",
      "date": "2026-01-25",
      "authors_text": "Harper Hua, Zhen Han, Zhengyuan Shen, Jeremy Lee, Patrick Guan",
      "is_highlight": false,
      "score": 30.0,
      "summary": "SQL-Trail introduces a multi-turn reinforcement learning framework for Text-to-SQL, enhancing query generation through iterative feedback and adaptive interaction, achieving state-of-the-art performance.",
      "teaser_image": null,
      "debug_abstract": "While large language models (LLMs) have substantially improved Text-to-SQL generation, a pronounced gap remains between AI systems and human experts on challenging benchmarks such as BIRD-SQL. We argue this gap stems largely from the prevailing single-pass paradigm, which lacks the iterative reasoning, schema exploration, and error-correction behaviors that humans naturally employ. To address this limitation, we introduce SQL-Trail, a multi-turn reinforcement learning (RL) agentic framework for Text-to-SQL. Rather than producing a query in one shot, SQL-Trail interacts with the database environment and uses execution feedback to iteratively refine its predictions. Our approach centers on two key ideas: (i) an adaptive turn-budget allocation mechanism that scales the agent&#39;s interaction depth to match question difficulty, and (ii) a composite reward panel that jointly incentivizes SQL correctness and efficient exploration. Across benchmarks, SQL-Trail sets a new state of the art and delivers strong data efficiency--up to 18x higher than prior single-pass RL state-of-the-art methods. Notably, our 7B and 14B models outperform substantially larger proprietary systems by 5% on average, underscoring the effectiveness of interactive, agentic workflows for robust Text-to-SQL generation."
    }
  ],
  "斯坦福-人工智能实验室 (SAIL)": [
    {
      "title": "PaperSearchQA: Learning to Search and Reason over Scientific Papers with RLVR",
      "url": "https://arxiv.org/abs/2601.18207",
      "date": "2026-01-26",
      "authors_text": "James Burgess, Jan N. Hansen, Duo Peng, Yuhui Zhang, Alejandro Lozano",
      "is_highlight": false,
      "score": 25.0,
      "summary": "The paper introduces PaperSearchQA, a reinforcement learning framework for training search agents to effectively reason and answer questions using a vast corpus of biomedical paper abstracts.",
      "teaser_image": null,
      "debug_abstract": "Search agents are language models (LMs) that reason and search knowledge bases (or the web) to answer questions; recent methods supervise only the final answer accuracy using reinforcement learning with verifiable rewards (RLVR). Most RLVR search agents tackle general-domain QA, which limits their relevance to technical AI systems in science, engineering, and medicine. In this work we propose training agents to search and reason over scientific papers -- this tests technical question-answering, it is directly relevant to real scientists, and the capabilities will be crucial to future AI Scientist systems. Concretely, we release a search corpus of 16 million biomedical paper abstracts and construct a challenging factoid QA dataset called PaperSearchQA with 60k samples answerable from the corpus, along with benchmarks. We train search agents in this environment to outperform non-RL retrieval baselines; we also perform further quantitative analysis and observe interesting agent behaviors like planning, reasoning, and self-verification. Our corpus, datasets, and benchmarks are usable with the popular Search-R1 codebase for RLVR training and released on this https URL. Finally, our data creation methods are scalable and easily extendable to other scientific domains."
    },
    {
      "title": "Image2Garment: Simulation-ready Garment Generation from a Single Image",
      "url": "https://arxiv.org/abs/2601.09658",
      "date": "2026-01-25",
      "authors_text": "Selim Emir Can, Jan Ackermann, Kiyohiro Nakayama, Ruofan Liu, Tong Wu",
      "is_highlight": false,
      "score": 55.0,
      "summary": "The paper presents Image2Garment, a framework that generates simulation-ready garments from a single image by inferring material properties and fabric attributes without iterative optimization.",
      "teaser_image": null,
      "debug_abstract": "Estimating physically accurate, simulation-ready garments from a single image is challenging due to the absence of image-to-physics datasets and the ill-posed nature of this problem. Prior methods either require multi-view capture and expensive differentiable simulation or predict only garment geometry without the material properties required for realistic simulation. We propose a feed-forward framework that sidesteps these limitations by first fine-tuning a vision-language model to infer material composition and fabric attributes from real images, and then training a lightweight predictor that maps these attributes to the corresponding physical fabric parameters using a small dataset of material-physics measurements. Our approach introduces two new datasets (FTAG and T2P) and delivers simulation-ready garments from a single image without iterative optimization. Experiments show that our estimator achieves superior accuracy in material composition estimation and fabric attribute prediction, and by passing them through our physics parameter estimator, we further achieve higher-fidelity simulations compared to state-of-the-art image-to-garment methods."
    },
    {
      "title": "SQL-Trail: Multi-Turn Reinforcement Learning with Interleaved Feedback for Text-to-SQL",
      "url": "https://arxiv.org/abs/2601.17699",
      "date": "2026-01-25",
      "authors_text": "Harper Hua, Zhen Han, Zhengyuan Shen, Jeremy Lee, Patrick Guan",
      "is_highlight": false,
      "score": 30.0,
      "summary": "SQL-Trail introduces a multi-turn reinforcement learning framework for Text-to-SQL, enhancing query generation through iterative feedback and adaptive interaction, achieving state-of-the-art performance.",
      "teaser_image": null,
      "debug_abstract": "While large language models (LLMs) have substantially improved Text-to-SQL generation, a pronounced gap remains between AI systems and human experts on challenging benchmarks such as BIRD-SQL. We argue this gap stems largely from the prevailing single-pass paradigm, which lacks the iterative reasoning, schema exploration, and error-correction behaviors that humans naturally employ. To address this limitation, we introduce SQL-Trail, a multi-turn reinforcement learning (RL) agentic framework for Text-to-SQL. Rather than producing a query in one shot, SQL-Trail interacts with the database environment and uses execution feedback to iteratively refine its predictions. Our approach centers on two key ideas: (i) an adaptive turn-budget allocation mechanism that scales the agent&#39;s interaction depth to match question difficulty, and (ii) a composite reward panel that jointly incentivizes SQL correctness and efficient exploration. Across benchmarks, SQL-Trail sets a new state of the art and delivers strong data efficiency--up to 18x higher than prior single-pass RL state-of-the-art methods. Notably, our 7B and 14B models outperform substantially larger proprietary systems by 5% on average, underscoring the effectiveness of interactive, agentic workflows for robust Text-to-SQL generation."
    }
  ],
  "斯坦福-机器人与具身智能实验室 (REAL)": [
    {
      "title": "PaperSearchQA: Learning to Search and Reason over Scientific Papers with RLVR",
      "url": "https://arxiv.org/abs/2601.18207",
      "date": "2026-01-26",
      "authors_text": "James Burgess, Jan N. Hansen, Duo Peng, Yuhui Zhang, Alejandro Lozano",
      "is_highlight": false,
      "score": 25.0,
      "summary": "The paper introduces PaperSearchQA, a reinforcement learning framework for training search agents to effectively reason and answer questions using a vast corpus of biomedical paper abstracts.",
      "teaser_image": null,
      "debug_abstract": "Search agents are language models (LMs) that reason and search knowledge bases (or the web) to answer questions; recent methods supervise only the final answer accuracy using reinforcement learning with verifiable rewards (RLVR). Most RLVR search agents tackle general-domain QA, which limits their relevance to technical AI systems in science, engineering, and medicine. In this work we propose training agents to search and reason over scientific papers -- this tests technical question-answering, it is directly relevant to real scientists, and the capabilities will be crucial to future AI Scientist systems. Concretely, we release a search corpus of 16 million biomedical paper abstracts and construct a challenging factoid QA dataset called PaperSearchQA with 60k samples answerable from the corpus, along with benchmarks. We train search agents in this environment to outperform non-RL retrieval baselines; we also perform further quantitative analysis and observe interesting agent behaviors like planning, reasoning, and self-verification. Our corpus, datasets, and benchmarks are usable with the popular Search-R1 codebase for RLVR training and released on this https URL. Finally, our data creation methods are scalable and easily extendable to other scientific domains."
    },
    {
      "title": "Image2Garment: Simulation-ready Garment Generation from a Single Image",
      "url": "https://arxiv.org/abs/2601.09658",
      "date": "2026-01-25",
      "authors_text": "Selim Emir Can, Jan Ackermann, Kiyohiro Nakayama, Ruofan Liu, Tong Wu",
      "is_highlight": false,
      "score": 55.0,
      "summary": "The paper presents Image2Garment, a framework that generates simulation-ready garments from a single image by inferring material properties and fabric attributes without iterative optimization.",
      "teaser_image": null,
      "debug_abstract": "Estimating physically accurate, simulation-ready garments from a single image is challenging due to the absence of image-to-physics datasets and the ill-posed nature of this problem. Prior methods either require multi-view capture and expensive differentiable simulation or predict only garment geometry without the material properties required for realistic simulation. We propose a feed-forward framework that sidesteps these limitations by first fine-tuning a vision-language model to infer material composition and fabric attributes from real images, and then training a lightweight predictor that maps these attributes to the corresponding physical fabric parameters using a small dataset of material-physics measurements. Our approach introduces two new datasets (FTAG and T2P) and delivers simulation-ready garments from a single image without iterative optimization. Experiments show that our estimator achieves superior accuracy in material composition estimation and fabric attribute prediction, and by passing them through our physics parameter estimator, we further achieve higher-fidelity simulations compared to state-of-the-art image-to-garment methods."
    },
    {
      "title": "SQL-Trail: Multi-Turn Reinforcement Learning with Interleaved Feedback for Text-to-SQL",
      "url": "https://arxiv.org/abs/2601.17699",
      "date": "2026-01-25",
      "authors_text": "Harper Hua, Zhen Han, Zhengyuan Shen, Jeremy Lee, Patrick Guan",
      "is_highlight": false,
      "score": 30.0,
      "summary": "SQL-Trail introduces a multi-turn reinforcement learning framework for Text-to-SQL, enhancing query generation through iterative feedback and adaptive interaction, achieving state-of-the-art performance.",
      "teaser_image": null,
      "debug_abstract": "While large language models (LLMs) have substantially improved Text-to-SQL generation, a pronounced gap remains between AI systems and human experts on challenging benchmarks such as BIRD-SQL. We argue this gap stems largely from the prevailing single-pass paradigm, which lacks the iterative reasoning, schema exploration, and error-correction behaviors that humans naturally employ. To address this limitation, we introduce SQL-Trail, a multi-turn reinforcement learning (RL) agentic framework for Text-to-SQL. Rather than producing a query in one shot, SQL-Trail interacts with the database environment and uses execution feedback to iteratively refine its predictions. Our approach centers on two key ideas: (i) an adaptive turn-budget allocation mechanism that scales the agent&#39;s interaction depth to match question difficulty, and (ii) a composite reward panel that jointly incentivizes SQL correctness and efficient exploration. Across benchmarks, SQL-Trail sets a new state of the art and delivers strong data efficiency--up to 18x higher than prior single-pass RL state-of-the-art methods. Notably, our 7B and 14B models outperform substantially larger proprietary systems by 5% on average, underscoring the effectiveness of interactive, agentic workflows for robust Text-to-SQL generation."
    }
  ],
  "Stanford AI Lab (SAIL)": [
    {
      "title": "PaperSearchQA: Learning to Search and Reason over Scientific Papers with RLVR",
      "url": "https://arxiv.org/abs/2601.18207",
      "date": "2026-01-26",
      "authors_text": "James Burgess, Jan N. Hansen, Duo Peng, Yuhui Zhang, Alejandro Lozano",
      "is_highlight": false,
      "score": 25.0,
      "summary": "The paper introduces PaperSearchQA, a reinforcement learning framework for training search agents to effectively reason and answer questions using a vast corpus of biomedical paper abstracts.",
      "teaser_image": null,
      "debug_abstract": "Search agents are language models (LMs) that reason and search knowledge bases (or the web) to answer questions; recent methods supervise only the final answer accuracy using reinforcement learning with verifiable rewards (RLVR). Most RLVR search agents tackle general-domain QA, which limits their relevance to technical AI systems in science, engineering, and medicine. In this work we propose training agents to search and reason over scientific papers -- this tests technical question-answering, it is directly relevant to real scientists, and the capabilities will be crucial to future AI Scientist systems. Concretely, we release a search corpus of 16 million biomedical paper abstracts and construct a challenging factoid QA dataset called PaperSearchQA with 60k samples answerable from the corpus, along with benchmarks. We train search agents in this environment to outperform non-RL retrieval baselines; we also perform further quantitative analysis and observe interesting agent behaviors like planning, reasoning, and self-verification. Our corpus, datasets, and benchmarks are usable with the popular Search-R1 codebase for RLVR training and released on this https URL. Finally, our data creation methods are scalable and easily extendable to other scientific domains."
    },
    {
      "title": "Image2Garment: Simulation-ready Garment Generation from a Single Image",
      "url": "https://arxiv.org/abs/2601.09658",
      "date": "2026-01-25",
      "authors_text": "Selim Emir Can, Jan Ackermann, Kiyohiro Nakayama, Ruofan Liu, Tong Wu",
      "is_highlight": false,
      "score": 55.0,
      "summary": "The paper presents Image2Garment, a framework that generates simulation-ready garments from a single image by inferring material properties and fabric attributes without iterative optimization.",
      "teaser_image": null,
      "debug_abstract": "Estimating physically accurate, simulation-ready garments from a single image is challenging due to the absence of image-to-physics datasets and the ill-posed nature of this problem. Prior methods either require multi-view capture and expensive differentiable simulation or predict only garment geometry without the material properties required for realistic simulation. We propose a feed-forward framework that sidesteps these limitations by first fine-tuning a vision-language model to infer material composition and fabric attributes from real images, and then training a lightweight predictor that maps these attributes to the corresponding physical fabric parameters using a small dataset of material-physics measurements. Our approach introduces two new datasets (FTAG and T2P) and delivers simulation-ready garments from a single image without iterative optimization. Experiments show that our estimator achieves superior accuracy in material composition estimation and fabric attribute prediction, and by passing them through our physics parameter estimator, we further achieve higher-fidelity simulations compared to state-of-the-art image-to-garment methods."
    },
    {
      "title": "SQL-Trail: Multi-Turn Reinforcement Learning with Interleaved Feedback for Text-to-SQL",
      "url": "https://arxiv.org/abs/2601.17699",
      "date": "2026-01-25",
      "authors_text": "Harper Hua, Zhen Han, Zhengyuan Shen, Jeremy Lee, Patrick Guan",
      "is_highlight": false,
      "score": 30.0,
      "summary": "SQL-Trail introduces a multi-turn reinforcement learning framework for Text-to-SQL, enhancing query generation through iterative feedback and adaptive interaction, achieving state-of-the-art performance.",
      "teaser_image": null,
      "debug_abstract": "While large language models (LLMs) have substantially improved Text-to-SQL generation, a pronounced gap remains between AI systems and human experts on challenging benchmarks such as BIRD-SQL. We argue this gap stems largely from the prevailing single-pass paradigm, which lacks the iterative reasoning, schema exploration, and error-correction behaviors that humans naturally employ. To address this limitation, we introduce SQL-Trail, a multi-turn reinforcement learning (RL) agentic framework for Text-to-SQL. Rather than producing a query in one shot, SQL-Trail interacts with the database environment and uses execution feedback to iteratively refine its predictions. Our approach centers on two key ideas: (i) an adaptive turn-budget allocation mechanism that scales the agent&#39;s interaction depth to match question difficulty, and (ii) a composite reward panel that jointly incentivizes SQL correctness and efficient exploration. Across benchmarks, SQL-Trail sets a new state of the art and delivers strong data efficiency--up to 18x higher than prior single-pass RL state-of-the-art methods. Notably, our 7B and 14B models outperform substantially larger proprietary systems by 5% on average, underscoring the effectiveness of interactive, agentic workflows for robust Text-to-SQL generation."
    }
  ],
  "Advanced Robotics Centre": [
    {
      "title": "AdaReasoner: Dynamic Tool Orchestration for Iterative Visual Reasoning",
      "url": "https://arxiv.org/abs/2601.18631",
      "date": "2026-01-26",
      "authors_text": "Mingyang Song, Haoyu Sun, Jiawei Gu, Linjie Li, Luxin Xu",
      "is_highlight": false,
      "score": 55.0,
      "summary": "AdaReasoner enhances visual reasoning in multimodal large language models by dynamically orchestrating tool usage through a scalable data pipeline and reinforcement learning, achieving state-of-the-art performance.",
      "teaser_image": null,
      "debug_abstract": "When humans face problems beyond their immediate capabilities, they rely on tools, providing a promising paradigm for improving visual reasoning in multimodal large language models (MLLMs). Effective reasoning, therefore, hinges on knowing which tools to use, when to invoke them, and how to compose them over multiple steps, even when faced with new tools or new tasks. We introduce \\textbf{AdaReasoner}, a family of multimodal models that learn tool use as a general reasoning skill rather than as tool-specific or explicitly supervised behavior. AdaReasoner is enabled by (i) a scalable data curation pipeline exposing models to long-horizon, multi-step tool interactions; (ii) Tool-GRPO, a reinforcement learning algorithm that optimizes tool selection and sequencing based on end-task success; and (iii) an adaptive learning mechanism that dynamically regulates tool usage. Together, these components allow models to infer tool utility from task context and intermediate outcomes, enabling coordination of multiple tools and generalization to unseen tools. Empirically, AdaReasoner exhibits strong tool-adaptive and generalization behaviors: it autonomously adopts beneficial tools, suppresses irrelevant ones, and adjusts tool usage frequency based on task demands, despite never being explicitly trained to do so. These capabilities translate into state-of-the-art performance across challenging benchmarks, improving the 7B base model by +24.9\\% on average and surpassing strong proprietary systems such as GPT-5 on multiple tasks, including VSP and Jigsaw."
    },
    {
      "title": "Fast and Safe Trajectory Optimization for Mobile Manipulators With Neural Configuration Space Distance Field",
      "url": "https://arxiv.org/abs/2601.18548",
      "date": "2026-01-26",
      "authors_text": "Yulin Li, Zhiyuan Song, Yiming Li, Zhicheng Song, Kai Chen",
      "is_highlight": false,
      "score": 88.0,
      "summary": "The paper presents Generalized Configuration Space Distance Fields (GCDF) for efficient trajectory optimization in mobile manipulators, enhancing collision reasoning and enabling agile motion in complex environments.",
      "teaser_image": null,
      "debug_abstract": "Mobile manipulators promise agile, long-horizon behavior by coordinating base and arm motion, yet whole-body trajectory optimization in cluttered, confined spaces remains difficult due to high-dimensional nonconvexity and the need for fast, accurate collision reasoning. Configuration Space Distance Fields (CDF) enable fixed-base manipulators to model collisions directly in configuration space via smooth, implicit distances. This representation holds strong potential to bypass the nonlinear configuration-to-workspace mapping while preserving accurate whole-body geometry and providing optimization-friendly collision costs. Yet, extending this capability to mobile manipulators is hindered by unbounded workspaces and tighter base-arm coupling. We lift this promise to mobile manipulation with Generalized Configuration Space Distance Fields (GCDF), extending CDF to robots with both translational and rotational joints in unbounded workspaces with tighter base-arm coupling. We prove that GCDF preserves Euclidean-like local distance structure and accurately encodes whole-body geometry in configuration space, and develop a data generation and training pipeline that yields continuous neural GCDFs with accurate values and gradients, supporting efficient GPU-batched queries. Building on this representation, we develop a high-performance sequential convex optimization framework centered on GCDF-based collision reasoning. The solver scales to large numbers of implicit constraints through (i) online specification of neural constraints, (ii) sparsity-aware active-set detection with parallel batched evaluation across thousands of constraints, and (iii) incremental constraint management for rapid replanning under scene changes."
    },
    {
      "title": "ORION: Option-Regularized Deep Reinforcement Learning for Cooperative Multi-Agent Online Navigation",
      "url": "https://arxiv.org/abs/2601.01155",
      "date": "2026-01-26",
      "authors_text": "Shizhe Zhang, Jingsong Liang, Zhitao Zhou, Shuhan Ye, Yizhuo Wang",
      "is_highlight": false,
      "score": 90.0,
      "summary": "ORION is a deep reinforcement learning framework that enables decentralized, cooperative navigation in partially known environments, enhancing agent coordination and map uncertainty reduction.",
      "teaser_image": null,
      "debug_abstract": "Existing methods for multi-agent navigation typically assume fully known environments, offering limited support for partially known scenarios such as warehouses or factory floors. There, agents may need to plan trajectories that balance their own path optimality with their ability to collect and share information about the environment that can help their teammates reach their own goals. To these ends, we propose ORION, a novel deep reinforcement learning framework for cooperative multi-agent online navigation in partially known environments. Starting from an imperfect prior map, ORION trains agents to make decentralized decisions, coordinate to reach their individual targets, and actively reduce map uncertainty by sharing online observations in a closed perception-action loop. We first design a shared graph encoder that fuses prior map with online perception into a unified representation, providing robust state embeddings under dynamic map discrepancies. At the core of ORION is an option-critic framework that learns to reason about a set of high-level cooperative modes that translate into sequences of low-level actions, allowing agents to switch between individual navigation and team-level exploration adaptively. We further introduce a dual-stage cooperation strategy that enables agents to assist teammates under map uncertainty, thereby reducing the overall makespan. Across extensive maze-like maps and large-scale warehouse environments, our simulation results show that ORION achieves high-quality, real-time decentralized cooperation over varying team sizes, outperforming state-of-the-art classical and learning-based baselines. Finally, we validate ORION on physical robot teams, demonstrating its robustness and practicality for real-world cooperative navigation."
    }
  ],
  "NUS AI LAB": [
    {
      "title": "AdaReasoner: Dynamic Tool Orchestration for Iterative Visual Reasoning",
      "url": "https://arxiv.org/abs/2601.18631",
      "date": "2026-01-26",
      "authors_text": "Mingyang Song, Haoyu Sun, Jiawei Gu, Linjie Li, Luxin Xu",
      "is_highlight": false,
      "score": 55.0,
      "summary": "AdaReasoner enhances visual reasoning in multimodal large language models by dynamically orchestrating tool usage through a scalable data pipeline and reinforcement learning, achieving state-of-the-art performance.",
      "teaser_image": null,
      "debug_abstract": "When humans face problems beyond their immediate capabilities, they rely on tools, providing a promising paradigm for improving visual reasoning in multimodal large language models (MLLMs). Effective reasoning, therefore, hinges on knowing which tools to use, when to invoke them, and how to compose them over multiple steps, even when faced with new tools or new tasks. We introduce \\textbf{AdaReasoner}, a family of multimodal models that learn tool use as a general reasoning skill rather than as tool-specific or explicitly supervised behavior. AdaReasoner is enabled by (i) a scalable data curation pipeline exposing models to long-horizon, multi-step tool interactions; (ii) Tool-GRPO, a reinforcement learning algorithm that optimizes tool selection and sequencing based on end-task success; and (iii) an adaptive learning mechanism that dynamically regulates tool usage. Together, these components allow models to infer tool utility from task context and intermediate outcomes, enabling coordination of multiple tools and generalization to unseen tools. Empirically, AdaReasoner exhibits strong tool-adaptive and generalization behaviors: it autonomously adopts beneficial tools, suppresses irrelevant ones, and adjusts tool usage frequency based on task demands, despite never being explicitly trained to do so. These capabilities translate into state-of-the-art performance across challenging benchmarks, improving the 7B base model by +24.9\\% on average and surpassing strong proprietary systems such as GPT-5 on multiple tasks, including VSP and Jigsaw."
    },
    {
      "title": "Fast and Safe Trajectory Optimization for Mobile Manipulators With Neural Configuration Space Distance Field",
      "url": "https://arxiv.org/abs/2601.18548",
      "date": "2026-01-26",
      "authors_text": "Yulin Li, Zhiyuan Song, Yiming Li, Zhicheng Song, Kai Chen",
      "is_highlight": false,
      "score": 88.0,
      "summary": "The paper presents Generalized Configuration Space Distance Fields (GCDF) for efficient trajectory optimization in mobile manipulators, enhancing collision reasoning and enabling agile motion in complex environments.",
      "teaser_image": null,
      "debug_abstract": "Mobile manipulators promise agile, long-horizon behavior by coordinating base and arm motion, yet whole-body trajectory optimization in cluttered, confined spaces remains difficult due to high-dimensional nonconvexity and the need for fast, accurate collision reasoning. Configuration Space Distance Fields (CDF) enable fixed-base manipulators to model collisions directly in configuration space via smooth, implicit distances. This representation holds strong potential to bypass the nonlinear configuration-to-workspace mapping while preserving accurate whole-body geometry and providing optimization-friendly collision costs. Yet, extending this capability to mobile manipulators is hindered by unbounded workspaces and tighter base-arm coupling. We lift this promise to mobile manipulation with Generalized Configuration Space Distance Fields (GCDF), extending CDF to robots with both translational and rotational joints in unbounded workspaces with tighter base-arm coupling. We prove that GCDF preserves Euclidean-like local distance structure and accurately encodes whole-body geometry in configuration space, and develop a data generation and training pipeline that yields continuous neural GCDFs with accurate values and gradients, supporting efficient GPU-batched queries. Building on this representation, we develop a high-performance sequential convex optimization framework centered on GCDF-based collision reasoning. The solver scales to large numbers of implicit constraints through (i) online specification of neural constraints, (ii) sparsity-aware active-set detection with parallel batched evaluation across thousands of constraints, and (iii) incremental constraint management for rapid replanning under scene changes."
    },
    {
      "title": "ORION: Option-Regularized Deep Reinforcement Learning for Cooperative Multi-Agent Online Navigation",
      "url": "https://arxiv.org/abs/2601.01155",
      "date": "2026-01-26",
      "authors_text": "Shizhe Zhang, Jingsong Liang, Zhitao Zhou, Shuhan Ye, Yizhuo Wang",
      "is_highlight": false,
      "score": 90.0,
      "summary": "ORION is a deep reinforcement learning framework that enables decentralized, cooperative navigation in partially known environments, enhancing agent coordination and map uncertainty reduction.",
      "teaser_image": null,
      "debug_abstract": "Existing methods for multi-agent navigation typically assume fully known environments, offering limited support for partially known scenarios such as warehouses or factory floors. There, agents may need to plan trajectories that balance their own path optimality with their ability to collect and share information about the environment that can help their teammates reach their own goals. To these ends, we propose ORION, a novel deep reinforcement learning framework for cooperative multi-agent online navigation in partially known environments. Starting from an imperfect prior map, ORION trains agents to make decentralized decisions, coordinate to reach their individual targets, and actively reduce map uncertainty by sharing online observations in a closed perception-action loop. We first design a shared graph encoder that fuses prior map with online perception into a unified representation, providing robust state embeddings under dynamic map discrepancies. At the core of ORION is an option-critic framework that learns to reason about a set of high-level cooperative modes that translate into sequences of low-level actions, allowing agents to switch between individual navigation and team-level exploration adaptively. We further introduce a dual-stage cooperation strategy that enables agents to assist teammates under map uncertainty, thereby reducing the overall makespan. Across extensive maze-like maps and large-scale warehouse environments, our simulation results show that ORION achieves high-quality, real-time decentralized cooperation over varying team sizes, outperforming state-of-the-art classical and learning-based baselines. Finally, we validate ORION on physical robot teams, demonstrating its robustness and practicality for real-world cooperative navigation."
    }
  ],
  "Synteraction Lab": [
    {
      "title": "AdaReasoner: Dynamic Tool Orchestration for Iterative Visual Reasoning",
      "url": "https://arxiv.org/abs/2601.18631",
      "date": "2026-01-26",
      "authors_text": "Mingyang Song, Haoyu Sun, Jiawei Gu, Linjie Li, Luxin Xu",
      "is_highlight": false,
      "score": 55.0,
      "summary": "AdaReasoner enhances visual reasoning in multimodal large language models by dynamically orchestrating tool usage through a scalable data pipeline and reinforcement learning, achieving state-of-the-art performance.",
      "teaser_image": null,
      "debug_abstract": "When humans face problems beyond their immediate capabilities, they rely on tools, providing a promising paradigm for improving visual reasoning in multimodal large language models (MLLMs). Effective reasoning, therefore, hinges on knowing which tools to use, when to invoke them, and how to compose them over multiple steps, even when faced with new tools or new tasks. We introduce \\textbf{AdaReasoner}, a family of multimodal models that learn tool use as a general reasoning skill rather than as tool-specific or explicitly supervised behavior. AdaReasoner is enabled by (i) a scalable data curation pipeline exposing models to long-horizon, multi-step tool interactions; (ii) Tool-GRPO, a reinforcement learning algorithm that optimizes tool selection and sequencing based on end-task success; and (iii) an adaptive learning mechanism that dynamically regulates tool usage. Together, these components allow models to infer tool utility from task context and intermediate outcomes, enabling coordination of multiple tools and generalization to unseen tools. Empirically, AdaReasoner exhibits strong tool-adaptive and generalization behaviors: it autonomously adopts beneficial tools, suppresses irrelevant ones, and adjusts tool usage frequency based on task demands, despite never being explicitly trained to do so. These capabilities translate into state-of-the-art performance across challenging benchmarks, improving the 7B base model by +24.9\\% on average and surpassing strong proprietary systems such as GPT-5 on multiple tasks, including VSP and Jigsaw."
    },
    {
      "title": "Fast and Safe Trajectory Optimization for Mobile Manipulators With Neural Configuration Space Distance Field",
      "url": "https://arxiv.org/abs/2601.18548",
      "date": "2026-01-26",
      "authors_text": "Yulin Li, Zhiyuan Song, Yiming Li, Zhicheng Song, Kai Chen",
      "is_highlight": false,
      "score": 88.0,
      "summary": "The paper presents Generalized Configuration Space Distance Fields (GCDF) for efficient trajectory optimization in mobile manipulators, enhancing collision reasoning and enabling agile motion in complex environments.",
      "teaser_image": null,
      "debug_abstract": "Mobile manipulators promise agile, long-horizon behavior by coordinating base and arm motion, yet whole-body trajectory optimization in cluttered, confined spaces remains difficult due to high-dimensional nonconvexity and the need for fast, accurate collision reasoning. Configuration Space Distance Fields (CDF) enable fixed-base manipulators to model collisions directly in configuration space via smooth, implicit distances. This representation holds strong potential to bypass the nonlinear configuration-to-workspace mapping while preserving accurate whole-body geometry and providing optimization-friendly collision costs. Yet, extending this capability to mobile manipulators is hindered by unbounded workspaces and tighter base-arm coupling. We lift this promise to mobile manipulation with Generalized Configuration Space Distance Fields (GCDF), extending CDF to robots with both translational and rotational joints in unbounded workspaces with tighter base-arm coupling. We prove that GCDF preserves Euclidean-like local distance structure and accurately encodes whole-body geometry in configuration space, and develop a data generation and training pipeline that yields continuous neural GCDFs with accurate values and gradients, supporting efficient GPU-batched queries. Building on this representation, we develop a high-performance sequential convex optimization framework centered on GCDF-based collision reasoning. The solver scales to large numbers of implicit constraints through (i) online specification of neural constraints, (ii) sparsity-aware active-set detection with parallel batched evaluation across thousands of constraints, and (iii) incremental constraint management for rapid replanning under scene changes."
    },
    {
      "title": "ORION: Option-Regularized Deep Reinforcement Learning for Cooperative Multi-Agent Online Navigation",
      "url": "https://arxiv.org/abs/2601.01155",
      "date": "2026-01-26",
      "authors_text": "Shizhe Zhang, Jingsong Liang, Zhitao Zhou, Shuhan Ye, Yizhuo Wang",
      "is_highlight": false,
      "score": 90.0,
      "summary": "ORION is a deep reinforcement learning framework that enables decentralized, cooperative navigation in partially known environments, enhancing agent coordination and map uncertainty reduction.",
      "teaser_image": null,
      "debug_abstract": "Existing methods for multi-agent navigation typically assume fully known environments, offering limited support for partially known scenarios such as warehouses or factory floors. There, agents may need to plan trajectories that balance their own path optimality with their ability to collect and share information about the environment that can help their teammates reach their own goals. To these ends, we propose ORION, a novel deep reinforcement learning framework for cooperative multi-agent online navigation in partially known environments. Starting from an imperfect prior map, ORION trains agents to make decentralized decisions, coordinate to reach their individual targets, and actively reduce map uncertainty by sharing online observations in a closed perception-action loop. We first design a shared graph encoder that fuses prior map with online perception into a unified representation, providing robust state embeddings under dynamic map discrepancies. At the core of ORION is an option-critic framework that learns to reason about a set of high-level cooperative modes that translate into sequences of low-level actions, allowing agents to switch between individual navigation and team-level exploration adaptively. We further introduce a dual-stage cooperation strategy that enables agents to assist teammates under map uncertainty, thereby reducing the overall makespan. Across extensive maze-like maps and large-scale warehouse environments, our simulation results show that ORION achieves high-quality, real-time decentralized cooperation over varying team sizes, outperforming state-of-the-art classical and learning-based baselines. Finally, we validate ORION on physical robot teams, demonstrating its robustness and practicality for real-world cooperative navigation."
    }
  ],
  "Microsystem Engineering and Robotics": [
    {
      "title": "AdaReasoner: Dynamic Tool Orchestration for Iterative Visual Reasoning",
      "url": "https://arxiv.org/abs/2601.18631",
      "date": "2026-01-26",
      "authors_text": "Mingyang Song, Haoyu Sun, Jiawei Gu, Linjie Li, Luxin Xu",
      "is_highlight": false,
      "score": 55.0,
      "summary": "AdaReasoner enhances visual reasoning in multimodal large language models by dynamically orchestrating tool usage through a scalable data pipeline and reinforcement learning, achieving state-of-the-art performance.",
      "teaser_image": null,
      "debug_abstract": "When humans face problems beyond their immediate capabilities, they rely on tools, providing a promising paradigm for improving visual reasoning in multimodal large language models (MLLMs). Effective reasoning, therefore, hinges on knowing which tools to use, when to invoke them, and how to compose them over multiple steps, even when faced with new tools or new tasks. We introduce \\textbf{AdaReasoner}, a family of multimodal models that learn tool use as a general reasoning skill rather than as tool-specific or explicitly supervised behavior. AdaReasoner is enabled by (i) a scalable data curation pipeline exposing models to long-horizon, multi-step tool interactions; (ii) Tool-GRPO, a reinforcement learning algorithm that optimizes tool selection and sequencing based on end-task success; and (iii) an adaptive learning mechanism that dynamically regulates tool usage. Together, these components allow models to infer tool utility from task context and intermediate outcomes, enabling coordination of multiple tools and generalization to unseen tools. Empirically, AdaReasoner exhibits strong tool-adaptive and generalization behaviors: it autonomously adopts beneficial tools, suppresses irrelevant ones, and adjusts tool usage frequency based on task demands, despite never being explicitly trained to do so. These capabilities translate into state-of-the-art performance across challenging benchmarks, improving the 7B base model by +24.9\\% on average and surpassing strong proprietary systems such as GPT-5 on multiple tasks, including VSP and Jigsaw."
    },
    {
      "title": "Fast and Safe Trajectory Optimization for Mobile Manipulators With Neural Configuration Space Distance Field",
      "url": "https://arxiv.org/abs/2601.18548",
      "date": "2026-01-26",
      "authors_text": "Yulin Li, Zhiyuan Song, Yiming Li, Zhicheng Song, Kai Chen",
      "is_highlight": false,
      "score": 88.0,
      "summary": "The paper presents Generalized Configuration Space Distance Fields (GCDF) for efficient trajectory optimization in mobile manipulators, enhancing collision reasoning and enabling agile motion in complex environments.",
      "teaser_image": null,
      "debug_abstract": "Mobile manipulators promise agile, long-horizon behavior by coordinating base and arm motion, yet whole-body trajectory optimization in cluttered, confined spaces remains difficult due to high-dimensional nonconvexity and the need for fast, accurate collision reasoning. Configuration Space Distance Fields (CDF) enable fixed-base manipulators to model collisions directly in configuration space via smooth, implicit distances. This representation holds strong potential to bypass the nonlinear configuration-to-workspace mapping while preserving accurate whole-body geometry and providing optimization-friendly collision costs. Yet, extending this capability to mobile manipulators is hindered by unbounded workspaces and tighter base-arm coupling. We lift this promise to mobile manipulation with Generalized Configuration Space Distance Fields (GCDF), extending CDF to robots with both translational and rotational joints in unbounded workspaces with tighter base-arm coupling. We prove that GCDF preserves Euclidean-like local distance structure and accurately encodes whole-body geometry in configuration space, and develop a data generation and training pipeline that yields continuous neural GCDFs with accurate values and gradients, supporting efficient GPU-batched queries. Building on this representation, we develop a high-performance sequential convex optimization framework centered on GCDF-based collision reasoning. The solver scales to large numbers of implicit constraints through (i) online specification of neural constraints, (ii) sparsity-aware active-set detection with parallel batched evaluation across thousands of constraints, and (iii) incremental constraint management for rapid replanning under scene changes."
    },
    {
      "title": "ORION: Option-Regularized Deep Reinforcement Learning for Cooperative Multi-Agent Online Navigation",
      "url": "https://arxiv.org/abs/2601.01155",
      "date": "2026-01-26",
      "authors_text": "Shizhe Zhang, Jingsong Liang, Zhitao Zhou, Shuhan Ye, Yizhuo Wang",
      "is_highlight": false,
      "score": 90.0,
      "summary": "ORION is a deep reinforcement learning framework that enables decentralized, cooperative navigation in partially known environments, enhancing agent coordination and map uncertainty reduction.",
      "teaser_image": null,
      "debug_abstract": "Existing methods for multi-agent navigation typically assume fully known environments, offering limited support for partially known scenarios such as warehouses or factory floors. There, agents may need to plan trajectories that balance their own path optimality with their ability to collect and share information about the environment that can help their teammates reach their own goals. To these ends, we propose ORION, a novel deep reinforcement learning framework for cooperative multi-agent online navigation in partially known environments. Starting from an imperfect prior map, ORION trains agents to make decentralized decisions, coordinate to reach their individual targets, and actively reduce map uncertainty by sharing online observations in a closed perception-action loop. We first design a shared graph encoder that fuses prior map with online perception into a unified representation, providing robust state embeddings under dynamic map discrepancies. At the core of ORION is an option-critic framework that learns to reason about a set of high-level cooperative modes that translate into sequences of low-level actions, allowing agents to switch between individual navigation and team-level exploration adaptively. We further introduce a dual-stage cooperation strategy that enables agents to assist teammates under map uncertainty, thereby reducing the overall makespan. Across extensive maze-like maps and large-scale warehouse environments, our simulation results show that ORION achieves high-quality, real-time decentralized cooperation over varying team sizes, outperforming state-of-the-art classical and learning-based baselines. Finally, we validate ORION on physical robot teams, demonstrating its robustness and practicality for real-world cooperative navigation."
    }
  ],
  "卡内基-机器人研究所": [
    {
      "title": "Reuse your FLOPs: Scaling RL on Hard Problems by Conditioning on Very Off-Policy Prefixes",
      "url": "https://arxiv.org/abs/2601.18795",
      "date": "2026-01-26",
      "authors_text": "Amrith Setlur, Zijian Wang, Andrew Cohen, Paria Rashidinejad, Sang Michael Xie",
      "is_highlight": false,
      "score": 30.0,
      "summary": "PrefixRL enhances reinforcement learning efficiency on challenging problems by utilizing off-policy traces to stabilize learning and improve sample efficiency, achieving faster and higher rewards.",
      "teaser_image": null,
      "debug_abstract": "Typical reinforcement learning (RL) methods for LLM reasoning waste compute on hard problems, where correct on-policy traces are rare, policy gradients vanish, and learning stalls. To bootstrap more efficient RL, we consider reusing old sampling FLOPs (from prior inference or RL training) in the form of off-policy traces. Standard off-policy methods supervise against off-policy data, causing instabilities during RL optimization. We introduce PrefixRL, where we condition on the prefix of successful off-policy traces and run on-policy RL to complete them, side-stepping off-policy instabilities. PrefixRL boosts the learning signal on hard problems by modulating the difficulty of the problem through the off-policy prefix length. We prove that the PrefixRL objective is not only consistent with the standard RL objective but also more sample efficient. Empirically, we discover back-generalization: training only on prefixed problems generalizes to out-of-distribution unprefixed performance, with learned strategies often differing from those in the prefix. In our experiments, we source the off-policy traces by rejection sampling with the base model, creating a self-improvement loop. On hard reasoning problems, PrefixRL reaches the same training reward 2x faster than the strongest baseline (SFT on off-policy data then RL), even after accounting for the compute spent on the initial rejection sampling, and increases the final reward by 3x. The gains transfer to held-out benchmarks, and PrefixRL is still effective when off-policy traces are derived from a different model family, validating its flexibility in practical settings."
    },
    {
      "title": "POPE: Learning to Reason on Hard Problems via Privileged On-Policy Exploration",
      "url": "https://arxiv.org/abs/2601.18779",
      "date": "2026-01-26",
      "authors_text": "Yuxiao Qu, Amrith Setlur, Virginia Smith, Ruslan Salakhutdinov, Aviral Kumar",
      "is_highlight": false,
      "score": 35.0,
      "summary": "POPE enhances reinforcement learning by using privileged oracle solutions to guide exploration on hard problems, significantly improving performance on challenging reasoning tasks.",
      "teaser_image": null,
      "debug_abstract": "Reinforcement learning (RL) has improved the reasoning abilities of large language models (LLMs), yet state-of-the-art methods still fail to learn on many training problems. On hard problems, on-policy RL rarely explores even a single correct rollout, yielding zero reward and no learning signal for driving improvement. We find that natural solutions to remedy this exploration problem from classical RL, such as entropy bonuses, more permissive clipping of the importance ratio, or direct optimization of pass@k objectives, do not resolve this issue and often destabilize optimization without improving solvability. A natural alternative is to leverage transfer from easier problems. However, we show that mixing easy and hard problems during RL training is counterproductive due to ray interference, where optimization focuses on already-solvable problems in a way that actively inhibits progress on harder ones. To address this challenge, we introduce Privileged On-Policy Exploration (POPE), an approach that leverages human- or other oracle solutions as privileged information to guide exploration on hard problems, unlike methods that use oracle solutions as training targets (e.g., off-policy RL methods or warmstarting from SFT). POPE augments hard problems with prefixes of oracle solutions, enabling RL to obtain non-zero rewards during guided rollouts. Crucially, the resulting behaviors transfer back to the original, unguided problems through a synergy between instruction-following and reasoning. Empirically, POPE expands the set of solvable problems and substantially improves performance on challenging reasoning benchmarks."
    },
    {
      "title": "Constraint-Aware Discrete-Time PID Gain Optimization for Robotic Joint Control Under Actuator Saturation",
      "url": "https://arxiv.org/abs/2601.18639",
      "date": "2026-01-26",
      "authors_text": "Ojasva Mishra, Xiaolong Wu, Min Xu",
      "is_highlight": false,
      "score": 80.0,
      "summary": "This paper presents a constraint-aware optimization method for PID gains in robotic joint control, enhancing stability and performance under actuator saturation and discrete-time execution.",
      "teaser_image": null,
      "debug_abstract": "The precise regulation of rotary actuation is fundamental in autonomous robotics, yet practical PID loops deviate from continuous-time theory due to discrete-time execution, actuator saturation, and small delays and measurement imperfections. We present an implementation-aware analysis and tuning workflow for saturated discrete-time joint control. We (i) derive PI stability regions under Euler and exact zero-order-hold (ZOH) discretizations using the Jury criterion, (ii) evaluate a discrete back-calculation anti-windup realization under saturation-dominant regimes, and (iii) propose a hybrid-certified Bayesian optimization workflow that screens analytically unstable candidates and behaviorally unsafe transients while optimizing a robust IAE objective with soft penalties on overshoot and saturation duty. Baseline sweeps ($\\tau=1.0$~s, $\\Delta t=0.01$~s, $u\\in[-10,10]$) quantify rise/settle trends for P/PI/PID. Under a randomized model family emulating uncertainty, delay, noise, quantization, and tighter saturation, robustness-oriented tuning improves median IAE from $0.843$ to $0.430$ while keeping median overshoot below $2\\%$. In simulation-only tuning, the certification screen rejects $11.6\\%$ of randomly sampled gains within bounds before full robust evaluation, improving sample efficiency without hardware experiments."
    },
    {
      "title": "Sentipolis: Emotion-Aware Agents for Social Simulations",
      "url": "https://arxiv.org/abs/2601.18027",
      "date": "2026-01-25",
      "authors_text": "Chiyuan Fu, Lyuhao Chen, Yunze Xiao, Weihao Xuan, Carlos Busso",
      "is_highlight": false,
      "score": 60.0,
      "summary": "Sentipolis introduces emotionally stateful agents using PAD representation and memory coupling, enhancing emotional continuity and communication in social simulations while revealing model-dependent behavior dynamics.",
      "teaser_image": null,
      "debug_abstract": "LLM agents are increasingly used for social simulation, yet emotion is often treated as a transient cue, causing emotional amnesia and weak long-horizon continuity. We present Sentipolis, a framework for emotionally stateful agents that integrates continuous Pleasure-Arousal-Dominance (PAD) representation, dual-speed emotion dynamics, and emotion--memory coupling. Across thousands of interactions over multiple base models and evaluators, Sentipolis improves emotionally grounded behavior, boosting communication, and emotional continuity. Gains are model-dependent: believability increases for higher-capacity models but can drop for smaller ones, and emotion-awareness can mildly reduce adherence to social norms, reflecting a human-like tension between emotion-driven behavior and rule compliance in social simulation. Network-level diagnostics show reciprocal, moderately clustered, and temporally stable relationship structures, supporting the study of cumulative social dynamics such as alliance formation and gradual relationship change."
    }
  ],
  "潘佳老师实验室": [
    {
      "title": "Advances and Innovations in the Multi-Agent Robotic System (MARS) Challenge",
      "url": "https://arxiv.org/abs/2601.18733",
      "date": "2026-01-26",
      "authors_text": "Li Kang, Heng Zhou, Xiufeng Song, Rui Li, Bruno N. Y. Chen",
      "is_highlight": false,
      "score": 75.0,
      "summary": "The MARS Challenge at NeurIPS 2025 promotes advancements in multi-agent systems by exploring vision-language models for collaborative robotic planning and manipulation in dynamic environments.",
      "teaser_image": null,
      "debug_abstract": "Recent advancements in multimodal large language models and vision-languageaction models have significantly driven progress in Embodied AI. As the field transitions toward more complex task scenarios, multi-agent system frameworks are becoming essential for achieving scalable, efficient, and collaborative solutions. This shift is fueled by three primary factors: increasing agent capabilities, enhancing system efficiency through task delegation, and enabling advanced human-agent interactions. To address the challenges posed by multi-agent collaboration, we propose the Multi-Agent Robotic System (MARS) Challenge, held at the NeurIPS 2025 Workshop on SpaVLE. The competition focuses on two critical areas: planning and control, where participants explore multi-agent embodied planning using vision-language models (VLMs) to coordinate tasks and policy execution to perform robotic manipulation in dynamic environments. By evaluating solutions submitted by participants, the challenge provides valuable insights into the design and coordination of embodied multi-agent systems, contributing to the future development of advanced collaborative AI systems."
    },
    {
      "title": "AdaReasoner: Dynamic Tool Orchestration for Iterative Visual Reasoning",
      "url": "https://arxiv.org/abs/2601.18631",
      "date": "2026-01-26",
      "authors_text": "Mingyang Song, Haoyu Sun, Jiawei Gu, Linjie Li, Luxin Xu",
      "is_highlight": false,
      "score": 55.0,
      "summary": "AdaReasoner enhances visual reasoning in multimodal large language models by dynamically orchestrating tool usage through a scalable data pipeline and reinforcement learning, achieving state-of-the-art performance.",
      "teaser_image": null,
      "debug_abstract": "When humans face problems beyond their immediate capabilities, they rely on tools, providing a promising paradigm for improving visual reasoning in multimodal large language models (MLLMs). Effective reasoning, therefore, hinges on knowing which tools to use, when to invoke them, and how to compose them over multiple steps, even when faced with new tools or new tasks. We introduce \\textbf{AdaReasoner}, a family of multimodal models that learn tool use as a general reasoning skill rather than as tool-specific or explicitly supervised behavior. AdaReasoner is enabled by (i) a scalable data curation pipeline exposing models to long-horizon, multi-step tool interactions; (ii) Tool-GRPO, a reinforcement learning algorithm that optimizes tool selection and sequencing based on end-task success; and (iii) an adaptive learning mechanism that dynamically regulates tool usage. Together, these components allow models to infer tool utility from task context and intermediate outcomes, enabling coordination of multiple tools and generalization to unseen tools. Empirically, AdaReasoner exhibits strong tool-adaptive and generalization behaviors: it autonomously adopts beneficial tools, suppresses irrelevant ones, and adjusts tool usage frequency based on task demands, despite never being explicitly trained to do so. These capabilities translate into state-of-the-art performance across challenging benchmarks, improving the 7B base model by +24.9\\% on average and surpassing strong proprietary systems such as GPT-5 on multiple tasks, including VSP and Jigsaw."
    },
    {
      "title": "Resisting Manipulative Bots in Meme Coin Copy Trading: A Multi-Agent Approach with Chain-of-Thought Reasoning",
      "url": "https://arxiv.org/abs/2601.08641",
      "date": "2026-01-26",
      "authors_text": "Yichen Luo, Yebo Feng, Jiahua Xu, Yang Liu",
      "is_highlight": false,
      "score": 40.0,
      "summary": "This paper presents a multi-agent system utilizing a large language model to enhance copy trading in meme coin markets, effectively resisting manipulative bots and improving trader profitability.",
      "teaser_image": null,
      "debug_abstract": "Copy trading has become the dominant entry strategy in meme coin markets. However, due to the market&#39;s extreme illiquid and volatile nature, the strategy exposes an exploitable attack surface: adversaries deploy manipulative bots to front-run trades, conceal positions, and fabricate sentiment, systematically extracting value from naïve copiers at scale. Despite its prevalence, bot-driven manipulation remains largely unexplored, and no robust defensive framework exists. We propose a manipulation-resistant copy-trading system based on a multi-agent architecture powered by a multi-modal, explainable large language model (LLM). Our system decomposes copy trading into three specialized agents for coin evaluation, wallet selection, and timing assessment. Evaluated on historical data from over 6,000 meme coins, our approach outperforms zero-shot and most statistic-driven baselines in prediction accuracy as well as all baselines in economic performance, achieving an average return of 14% for identified smart-money trades and an estimated copier return of 3% per trade under realistic market frictions. Overall, our results demonstrate the effectiveness of agent-based defenses and predictability of trader profitability in adversarial meme coin markets, providing a practical foundation for robust copy trading."
    },
    {
      "title": "Fast and Safe Trajectory Optimization for Mobile Manipulators With Neural Configuration Space Distance Field",
      "url": "https://arxiv.org/abs/2601.18548",
      "date": "2026-01-26",
      "authors_text": "Yulin Li, Zhiyuan Song, Yiming Li, Zhicheng Song, Kai Chen",
      "is_highlight": false,
      "score": 88.0,
      "summary": "The paper presents Generalized Configuration Space Distance Fields (GCDF) for efficient trajectory optimization in mobile manipulators, enhancing collision reasoning and enabling agile motion in complex environments.",
      "teaser_image": null,
      "debug_abstract": "Mobile manipulators promise agile, long-horizon behavior by coordinating base and arm motion, yet whole-body trajectory optimization in cluttered, confined spaces remains difficult due to high-dimensional nonconvexity and the need for fast, accurate collision reasoning. Configuration Space Distance Fields (CDF) enable fixed-base manipulators to model collisions directly in configuration space via smooth, implicit distances. This representation holds strong potential to bypass the nonlinear configuration-to-workspace mapping while preserving accurate whole-body geometry and providing optimization-friendly collision costs. Yet, extending this capability to mobile manipulators is hindered by unbounded workspaces and tighter base-arm coupling. We lift this promise to mobile manipulation with Generalized Configuration Space Distance Fields (GCDF), extending CDF to robots with both translational and rotational joints in unbounded workspaces with tighter base-arm coupling. We prove that GCDF preserves Euclidean-like local distance structure and accurately encodes whole-body geometry in configuration space, and develop a data generation and training pipeline that yields continuous neural GCDFs with accurate values and gradients, supporting efficient GPU-batched queries. Building on this representation, we develop a high-performance sequential convex optimization framework centered on GCDF-based collision reasoning. The solver scales to large numbers of implicit constraints through (i) online specification of neural constraints, (ii) sparsity-aware active-set detection with parallel batched evaluation across thousands of constraints, and (iii) incremental constraint management for rapid replanning under scene changes."
    },
    {
      "title": "Diffusion Model-based Reinforcement Learning for Version Age of Information Scheduling: Average and Tail-Risk-Sensitive Control",
      "url": "https://arxiv.org/abs/2601.18069",
      "date": "2026-01-26",
      "authors_text": "Haoyuan Pan, Sizhao Chen, Zhaorui Wang, Tse-Tin Chan",
      "is_highlight": false,
      "score": 50.0,
      "summary": "This paper presents a diffusion model-based reinforcement learning approach for optimizing Version Age of Information scheduling, balancing average performance with tail-risk sensitivity in wireless systems.",
      "teaser_image": null,
      "debug_abstract": "Ensuring timely and semantically accurate information delivery is critical in real-time wireless systems. While Age of Information (AoI) quantifies temporal freshness, Version Age of Information (VAoI) captures semantic staleness by accounting for version evolution between transmitters and receivers. Existing VAoI scheduling approaches primarily focus on minimizing average VAoI, overlooking rare but severe staleness events that can compromise reliability under stochastic packet arrivals and unreliable channels. This paper investigates both average-oriented and tail-risk-sensitive VAoI scheduling in a multi-user status update system with long-term transmission cost constraints. We first formulate the average VAoI minimization problem as a constrained Markov decision process and introduce a deep diffusion-based Soft Actor-Critic (D2SAC) algorithm. By generating actions through a diffusion-based denoising process, D2SAC enhances policy expressiveness and establishes a strong baseline for mean performance. Building on this foundation, we put forth RS-D3SAC, a risk-sensitive deep distributional diffusion-based Soft Actor-Critic algorithm. RS-D3SAC integrates a diffusion-based actor with a quantile-based distributional critic, explicitly modeling the full VAoI return distribution. This enables principled tail-risk optimization via Conditional Value-at-Risk (CVaR) while satisfying long-term transmission cost constraints. Extensive simulations show that, while D2SAC reduces average VAoI, RS-D3SAC consistently achieves substantial reductions in CVaR without sacrificing mean performance. The dominant gain in tail-risk reduction stems from the distributional critic, with the diffusion-based actor providing complementary refinement to stabilize and enrich policy decisions, highlighting their effectiveness for robust and risk-aware VAoI scheduling in multi-user wireless systems."
    }
  ],
  "Hengshuang Zhao老师实验室": [
    {
      "title": "Advances and Innovations in the Multi-Agent Robotic System (MARS) Challenge",
      "url": "https://arxiv.org/abs/2601.18733",
      "date": "2026-01-26",
      "authors_text": "Li Kang, Heng Zhou, Xiufeng Song, Rui Li, Bruno N. Y. Chen",
      "is_highlight": false,
      "score": 75.0,
      "summary": "The MARS Challenge at NeurIPS 2025 promotes advancements in multi-agent systems by exploring vision-language models for collaborative robotic planning and manipulation in dynamic environments.",
      "teaser_image": null,
      "debug_abstract": "Recent advancements in multimodal large language models and vision-languageaction models have significantly driven progress in Embodied AI. As the field transitions toward more complex task scenarios, multi-agent system frameworks are becoming essential for achieving scalable, efficient, and collaborative solutions. This shift is fueled by three primary factors: increasing agent capabilities, enhancing system efficiency through task delegation, and enabling advanced human-agent interactions. To address the challenges posed by multi-agent collaboration, we propose the Multi-Agent Robotic System (MARS) Challenge, held at the NeurIPS 2025 Workshop on SpaVLE. The competition focuses on two critical areas: planning and control, where participants explore multi-agent embodied planning using vision-language models (VLMs) to coordinate tasks and policy execution to perform robotic manipulation in dynamic environments. By evaluating solutions submitted by participants, the challenge provides valuable insights into the design and coordination of embodied multi-agent systems, contributing to the future development of advanced collaborative AI systems."
    },
    {
      "title": "AdaReasoner: Dynamic Tool Orchestration for Iterative Visual Reasoning",
      "url": "https://arxiv.org/abs/2601.18631",
      "date": "2026-01-26",
      "authors_text": "Mingyang Song, Haoyu Sun, Jiawei Gu, Linjie Li, Luxin Xu",
      "is_highlight": false,
      "score": 55.0,
      "summary": "AdaReasoner enhances visual reasoning in multimodal large language models by dynamically orchestrating tool usage through a scalable data pipeline and reinforcement learning, achieving state-of-the-art performance.",
      "teaser_image": null,
      "debug_abstract": "When humans face problems beyond their immediate capabilities, they rely on tools, providing a promising paradigm for improving visual reasoning in multimodal large language models (MLLMs). Effective reasoning, therefore, hinges on knowing which tools to use, when to invoke them, and how to compose them over multiple steps, even when faced with new tools or new tasks. We introduce \\textbf{AdaReasoner}, a family of multimodal models that learn tool use as a general reasoning skill rather than as tool-specific or explicitly supervised behavior. AdaReasoner is enabled by (i) a scalable data curation pipeline exposing models to long-horizon, multi-step tool interactions; (ii) Tool-GRPO, a reinforcement learning algorithm that optimizes tool selection and sequencing based on end-task success; and (iii) an adaptive learning mechanism that dynamically regulates tool usage. Together, these components allow models to infer tool utility from task context and intermediate outcomes, enabling coordination of multiple tools and generalization to unseen tools. Empirically, AdaReasoner exhibits strong tool-adaptive and generalization behaviors: it autonomously adopts beneficial tools, suppresses irrelevant ones, and adjusts tool usage frequency based on task demands, despite never being explicitly trained to do so. These capabilities translate into state-of-the-art performance across challenging benchmarks, improving the 7B base model by +24.9\\% on average and surpassing strong proprietary systems such as GPT-5 on multiple tasks, including VSP and Jigsaw."
    },
    {
      "title": "Resisting Manipulative Bots in Meme Coin Copy Trading: A Multi-Agent Approach with Chain-of-Thought Reasoning",
      "url": "https://arxiv.org/abs/2601.08641",
      "date": "2026-01-26",
      "authors_text": "Yichen Luo, Yebo Feng, Jiahua Xu, Yang Liu",
      "is_highlight": false,
      "score": 40.0,
      "summary": "This paper presents a multi-agent system utilizing a large language model to enhance copy trading in meme coin markets, effectively resisting manipulative bots and improving trader profitability.",
      "teaser_image": null,
      "debug_abstract": "Copy trading has become the dominant entry strategy in meme coin markets. However, due to the market&#39;s extreme illiquid and volatile nature, the strategy exposes an exploitable attack surface: adversaries deploy manipulative bots to front-run trades, conceal positions, and fabricate sentiment, systematically extracting value from naïve copiers at scale. Despite its prevalence, bot-driven manipulation remains largely unexplored, and no robust defensive framework exists. We propose a manipulation-resistant copy-trading system based on a multi-agent architecture powered by a multi-modal, explainable large language model (LLM). Our system decomposes copy trading into three specialized agents for coin evaluation, wallet selection, and timing assessment. Evaluated on historical data from over 6,000 meme coins, our approach outperforms zero-shot and most statistic-driven baselines in prediction accuracy as well as all baselines in economic performance, achieving an average return of 14% for identified smart-money trades and an estimated copier return of 3% per trade under realistic market frictions. Overall, our results demonstrate the effectiveness of agent-based defenses and predictability of trader profitability in adversarial meme coin markets, providing a practical foundation for robust copy trading."
    },
    {
      "title": "Fast and Safe Trajectory Optimization for Mobile Manipulators With Neural Configuration Space Distance Field",
      "url": "https://arxiv.org/abs/2601.18548",
      "date": "2026-01-26",
      "authors_text": "Yulin Li, Zhiyuan Song, Yiming Li, Zhicheng Song, Kai Chen",
      "is_highlight": false,
      "score": 88.0,
      "summary": "The paper presents Generalized Configuration Space Distance Fields (GCDF) for efficient trajectory optimization in mobile manipulators, enhancing collision reasoning and enabling agile motion in complex environments.",
      "teaser_image": null,
      "debug_abstract": "Mobile manipulators promise agile, long-horizon behavior by coordinating base and arm motion, yet whole-body trajectory optimization in cluttered, confined spaces remains difficult due to high-dimensional nonconvexity and the need for fast, accurate collision reasoning. Configuration Space Distance Fields (CDF) enable fixed-base manipulators to model collisions directly in configuration space via smooth, implicit distances. This representation holds strong potential to bypass the nonlinear configuration-to-workspace mapping while preserving accurate whole-body geometry and providing optimization-friendly collision costs. Yet, extending this capability to mobile manipulators is hindered by unbounded workspaces and tighter base-arm coupling. We lift this promise to mobile manipulation with Generalized Configuration Space Distance Fields (GCDF), extending CDF to robots with both translational and rotational joints in unbounded workspaces with tighter base-arm coupling. We prove that GCDF preserves Euclidean-like local distance structure and accurately encodes whole-body geometry in configuration space, and develop a data generation and training pipeline that yields continuous neural GCDFs with accurate values and gradients, supporting efficient GPU-batched queries. Building on this representation, we develop a high-performance sequential convex optimization framework centered on GCDF-based collision reasoning. The solver scales to large numbers of implicit constraints through (i) online specification of neural constraints, (ii) sparsity-aware active-set detection with parallel batched evaluation across thousands of constraints, and (iii) incremental constraint management for rapid replanning under scene changes."
    },
    {
      "title": "Diffusion Model-based Reinforcement Learning for Version Age of Information Scheduling: Average and Tail-Risk-Sensitive Control",
      "url": "https://arxiv.org/abs/2601.18069",
      "date": "2026-01-26",
      "authors_text": "Haoyuan Pan, Sizhao Chen, Zhaorui Wang, Tse-Tin Chan",
      "is_highlight": false,
      "score": 50.0,
      "summary": "This paper presents a diffusion model-based reinforcement learning approach for optimizing Version Age of Information scheduling, balancing average performance with tail-risk sensitivity in wireless systems.",
      "teaser_image": null,
      "debug_abstract": "Ensuring timely and semantically accurate information delivery is critical in real-time wireless systems. While Age of Information (AoI) quantifies temporal freshness, Version Age of Information (VAoI) captures semantic staleness by accounting for version evolution between transmitters and receivers. Existing VAoI scheduling approaches primarily focus on minimizing average VAoI, overlooking rare but severe staleness events that can compromise reliability under stochastic packet arrivals and unreliable channels. This paper investigates both average-oriented and tail-risk-sensitive VAoI scheduling in a multi-user status update system with long-term transmission cost constraints. We first formulate the average VAoI minimization problem as a constrained Markov decision process and introduce a deep diffusion-based Soft Actor-Critic (D2SAC) algorithm. By generating actions through a diffusion-based denoising process, D2SAC enhances policy expressiveness and establishes a strong baseline for mean performance. Building on this foundation, we put forth RS-D3SAC, a risk-sensitive deep distributional diffusion-based Soft Actor-Critic algorithm. RS-D3SAC integrates a diffusion-based actor with a quantile-based distributional critic, explicitly modeling the full VAoI return distribution. This enables principled tail-risk optimization via Conditional Value-at-Risk (CVaR) while satisfying long-term transmission cost constraints. Extensive simulations show that, while D2SAC reduces average VAoI, RS-D3SAC consistently achieves substantial reductions in CVaR without sacrificing mean performance. The dominant gain in tail-risk reduction stems from the distributional critic, with the diffusion-based actor providing complementary refinement to stabilize and enrich policy decisions, highlighting their effectiveness for robust and risk-aware VAoI scheduling in multi-user wireless systems."
    }
  ],
  "OpenDriveLab": [
    {
      "title": "Advances and Innovations in the Multi-Agent Robotic System (MARS) Challenge",
      "url": "https://arxiv.org/abs/2601.18733",
      "date": "2026-01-26",
      "authors_text": "Li Kang, Heng Zhou, Xiufeng Song, Rui Li, Bruno N. Y. Chen",
      "is_highlight": false,
      "score": 75.0,
      "summary": "The MARS Challenge at NeurIPS 2025 promotes advancements in multi-agent systems by exploring vision-language models for collaborative robotic planning and manipulation in dynamic environments.",
      "teaser_image": null,
      "debug_abstract": "Recent advancements in multimodal large language models and vision-languageaction models have significantly driven progress in Embodied AI. As the field transitions toward more complex task scenarios, multi-agent system frameworks are becoming essential for achieving scalable, efficient, and collaborative solutions. This shift is fueled by three primary factors: increasing agent capabilities, enhancing system efficiency through task delegation, and enabling advanced human-agent interactions. To address the challenges posed by multi-agent collaboration, we propose the Multi-Agent Robotic System (MARS) Challenge, held at the NeurIPS 2025 Workshop on SpaVLE. The competition focuses on two critical areas: planning and control, where participants explore multi-agent embodied planning using vision-language models (VLMs) to coordinate tasks and policy execution to perform robotic manipulation in dynamic environments. By evaluating solutions submitted by participants, the challenge provides valuable insights into the design and coordination of embodied multi-agent systems, contributing to the future development of advanced collaborative AI systems."
    },
    {
      "title": "AdaReasoner: Dynamic Tool Orchestration for Iterative Visual Reasoning",
      "url": "https://arxiv.org/abs/2601.18631",
      "date": "2026-01-26",
      "authors_text": "Mingyang Song, Haoyu Sun, Jiawei Gu, Linjie Li, Luxin Xu",
      "is_highlight": false,
      "score": 55.0,
      "summary": "AdaReasoner enhances visual reasoning in multimodal large language models by dynamically orchestrating tool usage through a scalable data pipeline and reinforcement learning, achieving state-of-the-art performance.",
      "teaser_image": null,
      "debug_abstract": "When humans face problems beyond their immediate capabilities, they rely on tools, providing a promising paradigm for improving visual reasoning in multimodal large language models (MLLMs). Effective reasoning, therefore, hinges on knowing which tools to use, when to invoke them, and how to compose them over multiple steps, even when faced with new tools or new tasks. We introduce \\textbf{AdaReasoner}, a family of multimodal models that learn tool use as a general reasoning skill rather than as tool-specific or explicitly supervised behavior. AdaReasoner is enabled by (i) a scalable data curation pipeline exposing models to long-horizon, multi-step tool interactions; (ii) Tool-GRPO, a reinforcement learning algorithm that optimizes tool selection and sequencing based on end-task success; and (iii) an adaptive learning mechanism that dynamically regulates tool usage. Together, these components allow models to infer tool utility from task context and intermediate outcomes, enabling coordination of multiple tools and generalization to unseen tools. Empirically, AdaReasoner exhibits strong tool-adaptive and generalization behaviors: it autonomously adopts beneficial tools, suppresses irrelevant ones, and adjusts tool usage frequency based on task demands, despite never being explicitly trained to do so. These capabilities translate into state-of-the-art performance across challenging benchmarks, improving the 7B base model by +24.9\\% on average and surpassing strong proprietary systems such as GPT-5 on multiple tasks, including VSP and Jigsaw."
    },
    {
      "title": "Resisting Manipulative Bots in Meme Coin Copy Trading: A Multi-Agent Approach with Chain-of-Thought Reasoning",
      "url": "https://arxiv.org/abs/2601.08641",
      "date": "2026-01-26",
      "authors_text": "Yichen Luo, Yebo Feng, Jiahua Xu, Yang Liu",
      "is_highlight": false,
      "score": 40.0,
      "summary": "This paper presents a multi-agent system utilizing a large language model to enhance copy trading in meme coin markets, effectively resisting manipulative bots and improving trader profitability.",
      "teaser_image": null,
      "debug_abstract": "Copy trading has become the dominant entry strategy in meme coin markets. However, due to the market&#39;s extreme illiquid and volatile nature, the strategy exposes an exploitable attack surface: adversaries deploy manipulative bots to front-run trades, conceal positions, and fabricate sentiment, systematically extracting value from naïve copiers at scale. Despite its prevalence, bot-driven manipulation remains largely unexplored, and no robust defensive framework exists. We propose a manipulation-resistant copy-trading system based on a multi-agent architecture powered by a multi-modal, explainable large language model (LLM). Our system decomposes copy trading into three specialized agents for coin evaluation, wallet selection, and timing assessment. Evaluated on historical data from over 6,000 meme coins, our approach outperforms zero-shot and most statistic-driven baselines in prediction accuracy as well as all baselines in economic performance, achieving an average return of 14% for identified smart-money trades and an estimated copier return of 3% per trade under realistic market frictions. Overall, our results demonstrate the effectiveness of agent-based defenses and predictability of trader profitability in adversarial meme coin markets, providing a practical foundation for robust copy trading."
    },
    {
      "title": "Fast and Safe Trajectory Optimization for Mobile Manipulators With Neural Configuration Space Distance Field",
      "url": "https://arxiv.org/abs/2601.18548",
      "date": "2026-01-26",
      "authors_text": "Yulin Li, Zhiyuan Song, Yiming Li, Zhicheng Song, Kai Chen",
      "is_highlight": false,
      "score": 88.0,
      "summary": "The paper presents Generalized Configuration Space Distance Fields (GCDF) for efficient trajectory optimization in mobile manipulators, enhancing collision reasoning and enabling agile motion in complex environments.",
      "teaser_image": null,
      "debug_abstract": "Mobile manipulators promise agile, long-horizon behavior by coordinating base and arm motion, yet whole-body trajectory optimization in cluttered, confined spaces remains difficult due to high-dimensional nonconvexity and the need for fast, accurate collision reasoning. Configuration Space Distance Fields (CDF) enable fixed-base manipulators to model collisions directly in configuration space via smooth, implicit distances. This representation holds strong potential to bypass the nonlinear configuration-to-workspace mapping while preserving accurate whole-body geometry and providing optimization-friendly collision costs. Yet, extending this capability to mobile manipulators is hindered by unbounded workspaces and tighter base-arm coupling. We lift this promise to mobile manipulation with Generalized Configuration Space Distance Fields (GCDF), extending CDF to robots with both translational and rotational joints in unbounded workspaces with tighter base-arm coupling. We prove that GCDF preserves Euclidean-like local distance structure and accurately encodes whole-body geometry in configuration space, and develop a data generation and training pipeline that yields continuous neural GCDFs with accurate values and gradients, supporting efficient GPU-batched queries. Building on this representation, we develop a high-performance sequential convex optimization framework centered on GCDF-based collision reasoning. The solver scales to large numbers of implicit constraints through (i) online specification of neural constraints, (ii) sparsity-aware active-set detection with parallel batched evaluation across thousands of constraints, and (iii) incremental constraint management for rapid replanning under scene changes."
    },
    {
      "title": "Diffusion Model-based Reinforcement Learning for Version Age of Information Scheduling: Average and Tail-Risk-Sensitive Control",
      "url": "https://arxiv.org/abs/2601.18069",
      "date": "2026-01-26",
      "authors_text": "Haoyuan Pan, Sizhao Chen, Zhaorui Wang, Tse-Tin Chan",
      "is_highlight": false,
      "score": 50.0,
      "summary": "This paper presents a diffusion model-based reinforcement learning approach for optimizing Version Age of Information scheduling, balancing average performance with tail-risk sensitivity in wireless systems.",
      "teaser_image": null,
      "debug_abstract": "Ensuring timely and semantically accurate information delivery is critical in real-time wireless systems. While Age of Information (AoI) quantifies temporal freshness, Version Age of Information (VAoI) captures semantic staleness by accounting for version evolution between transmitters and receivers. Existing VAoI scheduling approaches primarily focus on minimizing average VAoI, overlooking rare but severe staleness events that can compromise reliability under stochastic packet arrivals and unreliable channels. This paper investigates both average-oriented and tail-risk-sensitive VAoI scheduling in a multi-user status update system with long-term transmission cost constraints. We first formulate the average VAoI minimization problem as a constrained Markov decision process and introduce a deep diffusion-based Soft Actor-Critic (D2SAC) algorithm. By generating actions through a diffusion-based denoising process, D2SAC enhances policy expressiveness and establishes a strong baseline for mean performance. Building on this foundation, we put forth RS-D3SAC, a risk-sensitive deep distributional diffusion-based Soft Actor-Critic algorithm. RS-D3SAC integrates a diffusion-based actor with a quantile-based distributional critic, explicitly modeling the full VAoI return distribution. This enables principled tail-risk optimization via Conditional Value-at-Risk (CVaR) while satisfying long-term transmission cost constraints. Extensive simulations show that, while D2SAC reduces average VAoI, RS-D3SAC consistently achieves substantial reductions in CVaR without sacrificing mean performance. The dominant gain in tail-risk reduction stems from the distributional critic, with the diffusion-based actor providing complementary refinement to stabilize and enrich policy decisions, highlighting their effectiveness for robust and risk-aware VAoI scheduling in multi-user wireless systems."
    }
  ],
  "深圳大学": [
    {
      "title": "YOLO-DS: Fine-Grained Feature Decoupling via Dual-Statistic Synergy Operator for Object Detection",
      "url": "https://arxiv.org/abs/2601.18172",
      "date": "2026-01-26",
      "authors_text": "Lin Huang, Yujuan Tan, Weisheng Li, Shitai Shan, Liu Liu",
      "is_highlight": false,
      "score": 45.0,
      "summary": "YOLO-DS enhances object detection by introducing a Dual-Statistic Synergy Operator for improved feature decoupling, achieving significant performance gains over YOLOv8 with minimal latency increase.",
      "teaser_image": null,
      "debug_abstract": "One-stage object detection, particularly the YOLO series, strikes a favorable balance between accuracy and efficiency. However, existing YOLO detectors lack explicit modeling of heterogeneous object responses within shared feature channels, which limits further performance gains. To address this, we propose YOLO-DS, a framework built around a novel Dual-Statistic Synergy Operator (DSO). The DSO decouples object features by jointly modeling the channel-wise mean and the peak-to-mean difference. Building upon the DSO, we design two lightweight gating modules: the Dual-Statistic Synergy Gating (DSG) module for adaptive channel-wise feature selection, and the Multi-Path Segmented Gating (MSG) module for depth-wise feature weighting. On the MS-COCO benchmark, YOLO-DS consistently outperforms YOLOv8 across five model scales (N, S, M, L, X), achieving AP gains of 1.1% to 1.7% with only a minimal increase in inference latency. Extensive visualization, ablation, and comparative studies validate the effectiveness of our approach, demonstrating its superior capability in discriminating heterogeneous objects with high efficiency."
    },
    {
      "title": "Diffusion Model-based Reinforcement Learning for Version Age of Information Scheduling: Average and Tail-Risk-Sensitive Control",
      "url": "https://arxiv.org/abs/2601.18069",
      "date": "2026-01-26",
      "authors_text": "Haoyuan Pan, Sizhao Chen, Zhaorui Wang, Tse-Tin Chan",
      "is_highlight": false,
      "score": 50.0,
      "summary": "This paper presents a diffusion model-based reinforcement learning approach for optimizing Version Age of Information scheduling, balancing average performance with tail-risk sensitivity in wireless systems.",
      "teaser_image": null,
      "debug_abstract": "Ensuring timely and semantically accurate information delivery is critical in real-time wireless systems. While Age of Information (AoI) quantifies temporal freshness, Version Age of Information (VAoI) captures semantic staleness by accounting for version evolution between transmitters and receivers. Existing VAoI scheduling approaches primarily focus on minimizing average VAoI, overlooking rare but severe staleness events that can compromise reliability under stochastic packet arrivals and unreliable channels. This paper investigates both average-oriented and tail-risk-sensitive VAoI scheduling in a multi-user status update system with long-term transmission cost constraints. We first formulate the average VAoI minimization problem as a constrained Markov decision process and introduce a deep diffusion-based Soft Actor-Critic (D2SAC) algorithm. By generating actions through a diffusion-based denoising process, D2SAC enhances policy expressiveness and establishes a strong baseline for mean performance. Building on this foundation, we put forth RS-D3SAC, a risk-sensitive deep distributional diffusion-based Soft Actor-Critic algorithm. RS-D3SAC integrates a diffusion-based actor with a quantile-based distributional critic, explicitly modeling the full VAoI return distribution. This enables principled tail-risk optimization via Conditional Value-at-Risk (CVaR) while satisfying long-term transmission cost constraints. Extensive simulations show that, while D2SAC reduces average VAoI, RS-D3SAC consistently achieves substantial reductions in CVaR without sacrificing mean performance. The dominant gain in tail-risk reduction stems from the distributional critic, with the diffusion-based actor providing complementary refinement to stabilize and enrich policy decisions, highlighting their effectiveness for robust and risk-aware VAoI scheduling in multi-user wireless systems."
    }
  ],
  "Language and Vision (LaVi) Lab": [
    {
      "title": "Advances and Innovations in the Multi-Agent Robotic System (MARS) Challenge",
      "url": "https://arxiv.org/abs/2601.18733",
      "date": "2026-01-26",
      "authors_text": "Li Kang, Heng Zhou, Xiufeng Song, Rui Li, Bruno N. Y. Chen",
      "is_highlight": false,
      "score": 75.0,
      "summary": "The MARS Challenge at NeurIPS 2025 promotes advancements in multi-agent systems by exploring vision-language models for collaborative robotic planning and manipulation in dynamic environments.",
      "teaser_image": null,
      "debug_abstract": "Recent advancements in multimodal large language models and vision-languageaction models have significantly driven progress in Embodied AI. As the field transitions toward more complex task scenarios, multi-agent system frameworks are becoming essential for achieving scalable, efficient, and collaborative solutions. This shift is fueled by three primary factors: increasing agent capabilities, enhancing system efficiency through task delegation, and enabling advanced human-agent interactions. To address the challenges posed by multi-agent collaboration, we propose the Multi-Agent Robotic System (MARS) Challenge, held at the NeurIPS 2025 Workshop on SpaVLE. The competition focuses on two critical areas: planning and control, where participants explore multi-agent embodied planning using vision-language models (VLMs) to coordinate tasks and policy execution to perform robotic manipulation in dynamic environments. By evaluating solutions submitted by participants, the challenge provides valuable insights into the design and coordination of embodied multi-agent systems, contributing to the future development of advanced collaborative AI systems."
    },
    {
      "title": "AdaReasoner: Dynamic Tool Orchestration for Iterative Visual Reasoning",
      "url": "https://arxiv.org/abs/2601.18631",
      "date": "2026-01-26",
      "authors_text": "Mingyang Song, Haoyu Sun, Jiawei Gu, Linjie Li, Luxin Xu",
      "is_highlight": false,
      "score": 55.0,
      "summary": "AdaReasoner enhances visual reasoning in multimodal large language models by dynamically orchestrating tool usage through a scalable data pipeline and reinforcement learning, achieving state-of-the-art performance.",
      "teaser_image": null,
      "debug_abstract": "When humans face problems beyond their immediate capabilities, they rely on tools, providing a promising paradigm for improving visual reasoning in multimodal large language models (MLLMs). Effective reasoning, therefore, hinges on knowing which tools to use, when to invoke them, and how to compose them over multiple steps, even when faced with new tools or new tasks. We introduce \\textbf{AdaReasoner}, a family of multimodal models that learn tool use as a general reasoning skill rather than as tool-specific or explicitly supervised behavior. AdaReasoner is enabled by (i) a scalable data curation pipeline exposing models to long-horizon, multi-step tool interactions; (ii) Tool-GRPO, a reinforcement learning algorithm that optimizes tool selection and sequencing based on end-task success; and (iii) an adaptive learning mechanism that dynamically regulates tool usage. Together, these components allow models to infer tool utility from task context and intermediate outcomes, enabling coordination of multiple tools and generalization to unseen tools. Empirically, AdaReasoner exhibits strong tool-adaptive and generalization behaviors: it autonomously adopts beneficial tools, suppresses irrelevant ones, and adjusts tool usage frequency based on task demands, despite never being explicitly trained to do so. These capabilities translate into state-of-the-art performance across challenging benchmarks, improving the 7B base model by +24.9\\% on average and surpassing strong proprietary systems such as GPT-5 on multiple tasks, including VSP and Jigsaw."
    },
    {
      "title": "Resisting Manipulative Bots in Meme Coin Copy Trading: A Multi-Agent Approach with Chain-of-Thought Reasoning",
      "url": "https://arxiv.org/abs/2601.08641",
      "date": "2026-01-26",
      "authors_text": "Yichen Luo, Yebo Feng, Jiahua Xu, Yang Liu",
      "is_highlight": false,
      "score": 40.0,
      "summary": "This paper presents a multi-agent system utilizing a large language model to enhance copy trading in meme coin markets, effectively resisting manipulative bots and improving trader profitability.",
      "teaser_image": null,
      "debug_abstract": "Copy trading has become the dominant entry strategy in meme coin markets. However, due to the market&#39;s extreme illiquid and volatile nature, the strategy exposes an exploitable attack surface: adversaries deploy manipulative bots to front-run trades, conceal positions, and fabricate sentiment, systematically extracting value from naïve copiers at scale. Despite its prevalence, bot-driven manipulation remains largely unexplored, and no robust defensive framework exists. We propose a manipulation-resistant copy-trading system based on a multi-agent architecture powered by a multi-modal, explainable large language model (LLM). Our system decomposes copy trading into three specialized agents for coin evaluation, wallet selection, and timing assessment. Evaluated on historical data from over 6,000 meme coins, our approach outperforms zero-shot and most statistic-driven baselines in prediction accuracy as well as all baselines in economic performance, achieving an average return of 14% for identified smart-money trades and an estimated copier return of 3% per trade under realistic market frictions. Overall, our results demonstrate the effectiveness of agent-based defenses and predictability of trader profitability in adversarial meme coin markets, providing a practical foundation for robust copy trading."
    },
    {
      "title": "Fast and Safe Trajectory Optimization for Mobile Manipulators With Neural Configuration Space Distance Field",
      "url": "https://arxiv.org/abs/2601.18548",
      "date": "2026-01-26",
      "authors_text": "Yulin Li, Zhiyuan Song, Yiming Li, Zhicheng Song, Kai Chen",
      "is_highlight": false,
      "score": 88.0,
      "summary": "The paper presents Generalized Configuration Space Distance Fields (GCDF) for efficient trajectory optimization in mobile manipulators, enhancing collision reasoning and enabling agile motion in complex environments.",
      "teaser_image": null,
      "debug_abstract": "Mobile manipulators promise agile, long-horizon behavior by coordinating base and arm motion, yet whole-body trajectory optimization in cluttered, confined spaces remains difficult due to high-dimensional nonconvexity and the need for fast, accurate collision reasoning. Configuration Space Distance Fields (CDF) enable fixed-base manipulators to model collisions directly in configuration space via smooth, implicit distances. This representation holds strong potential to bypass the nonlinear configuration-to-workspace mapping while preserving accurate whole-body geometry and providing optimization-friendly collision costs. Yet, extending this capability to mobile manipulators is hindered by unbounded workspaces and tighter base-arm coupling. We lift this promise to mobile manipulation with Generalized Configuration Space Distance Fields (GCDF), extending CDF to robots with both translational and rotational joints in unbounded workspaces with tighter base-arm coupling. We prove that GCDF preserves Euclidean-like local distance structure and accurately encodes whole-body geometry in configuration space, and develop a data generation and training pipeline that yields continuous neural GCDFs with accurate values and gradients, supporting efficient GPU-batched queries. Building on this representation, we develop a high-performance sequential convex optimization framework centered on GCDF-based collision reasoning. The solver scales to large numbers of implicit constraints through (i) online specification of neural constraints, (ii) sparsity-aware active-set detection with parallel batched evaluation across thousands of constraints, and (iii) incremental constraint management for rapid replanning under scene changes."
    },
    {
      "title": "Diffusion Model-based Reinforcement Learning for Version Age of Information Scheduling: Average and Tail-Risk-Sensitive Control",
      "url": "https://arxiv.org/abs/2601.18069",
      "date": "2026-01-26",
      "authors_text": "Haoyuan Pan, Sizhao Chen, Zhaorui Wang, Tse-Tin Chan",
      "is_highlight": false,
      "score": 50.0,
      "summary": "This paper presents a diffusion model-based reinforcement learning approach for optimizing Version Age of Information scheduling, balancing average performance with tail-risk sensitivity in wireless systems.",
      "teaser_image": null,
      "debug_abstract": "Ensuring timely and semantically accurate information delivery is critical in real-time wireless systems. While Age of Information (AoI) quantifies temporal freshness, Version Age of Information (VAoI) captures semantic staleness by accounting for version evolution between transmitters and receivers. Existing VAoI scheduling approaches primarily focus on minimizing average VAoI, overlooking rare but severe staleness events that can compromise reliability under stochastic packet arrivals and unreliable channels. This paper investigates both average-oriented and tail-risk-sensitive VAoI scheduling in a multi-user status update system with long-term transmission cost constraints. We first formulate the average VAoI minimization problem as a constrained Markov decision process and introduce a deep diffusion-based Soft Actor-Critic (D2SAC) algorithm. By generating actions through a diffusion-based denoising process, D2SAC enhances policy expressiveness and establishes a strong baseline for mean performance. Building on this foundation, we put forth RS-D3SAC, a risk-sensitive deep distributional diffusion-based Soft Actor-Critic algorithm. RS-D3SAC integrates a diffusion-based actor with a quantile-based distributional critic, explicitly modeling the full VAoI return distribution. This enables principled tail-risk optimization via Conditional Value-at-Risk (CVaR) while satisfying long-term transmission cost constraints. Extensive simulations show that, while D2SAC reduces average VAoI, RS-D3SAC consistently achieves substantial reductions in CVaR without sacrificing mean performance. The dominant gain in tail-risk reduction stems from the distributional critic, with the diffusion-based actor providing complementary refinement to stabilize and enrich policy decisions, highlighting their effectiveness for robust and risk-aware VAoI scheduling in multi-user wireless systems."
    }
  ],
  "香港大学机械工程系机器人实验室": [
    {
      "title": "Advances and Innovations in the Multi-Agent Robotic System (MARS) Challenge",
      "url": "https://arxiv.org/abs/2601.18733",
      "date": "2026-01-26",
      "authors_text": "Li Kang, Heng Zhou, Xiufeng Song, Rui Li, Bruno N. Y. Chen",
      "is_highlight": false,
      "score": 75.0,
      "summary": "The MARS Challenge at NeurIPS 2025 promotes advancements in multi-agent systems by exploring vision-language models for collaborative robotic planning and manipulation in dynamic environments.",
      "teaser_image": null,
      "debug_abstract": "Recent advancements in multimodal large language models and vision-languageaction models have significantly driven progress in Embodied AI. As the field transitions toward more complex task scenarios, multi-agent system frameworks are becoming essential for achieving scalable, efficient, and collaborative solutions. This shift is fueled by three primary factors: increasing agent capabilities, enhancing system efficiency through task delegation, and enabling advanced human-agent interactions. To address the challenges posed by multi-agent collaboration, we propose the Multi-Agent Robotic System (MARS) Challenge, held at the NeurIPS 2025 Workshop on SpaVLE. The competition focuses on two critical areas: planning and control, where participants explore multi-agent embodied planning using vision-language models (VLMs) to coordinate tasks and policy execution to perform robotic manipulation in dynamic environments. By evaluating solutions submitted by participants, the challenge provides valuable insights into the design and coordination of embodied multi-agent systems, contributing to the future development of advanced collaborative AI systems."
    },
    {
      "title": "AdaReasoner: Dynamic Tool Orchestration for Iterative Visual Reasoning",
      "url": "https://arxiv.org/abs/2601.18631",
      "date": "2026-01-26",
      "authors_text": "Mingyang Song, Haoyu Sun, Jiawei Gu, Linjie Li, Luxin Xu",
      "is_highlight": false,
      "score": 55.0,
      "summary": "AdaReasoner enhances visual reasoning in multimodal large language models by dynamically orchestrating tool usage through a scalable data pipeline and reinforcement learning, achieving state-of-the-art performance.",
      "teaser_image": null,
      "debug_abstract": "When humans face problems beyond their immediate capabilities, they rely on tools, providing a promising paradigm for improving visual reasoning in multimodal large language models (MLLMs). Effective reasoning, therefore, hinges on knowing which tools to use, when to invoke them, and how to compose them over multiple steps, even when faced with new tools or new tasks. We introduce \\textbf{AdaReasoner}, a family of multimodal models that learn tool use as a general reasoning skill rather than as tool-specific or explicitly supervised behavior. AdaReasoner is enabled by (i) a scalable data curation pipeline exposing models to long-horizon, multi-step tool interactions; (ii) Tool-GRPO, a reinforcement learning algorithm that optimizes tool selection and sequencing based on end-task success; and (iii) an adaptive learning mechanism that dynamically regulates tool usage. Together, these components allow models to infer tool utility from task context and intermediate outcomes, enabling coordination of multiple tools and generalization to unseen tools. Empirically, AdaReasoner exhibits strong tool-adaptive and generalization behaviors: it autonomously adopts beneficial tools, suppresses irrelevant ones, and adjusts tool usage frequency based on task demands, despite never being explicitly trained to do so. These capabilities translate into state-of-the-art performance across challenging benchmarks, improving the 7B base model by +24.9\\% on average and surpassing strong proprietary systems such as GPT-5 on multiple tasks, including VSP and Jigsaw."
    },
    {
      "title": "Resisting Manipulative Bots in Meme Coin Copy Trading: A Multi-Agent Approach with Chain-of-Thought Reasoning",
      "url": "https://arxiv.org/abs/2601.08641",
      "date": "2026-01-26",
      "authors_text": "Yichen Luo, Yebo Feng, Jiahua Xu, Yang Liu",
      "is_highlight": false,
      "score": 40.0,
      "summary": "This paper presents a multi-agent system utilizing a large language model to enhance copy trading in meme coin markets, effectively resisting manipulative bots and improving trader profitability.",
      "teaser_image": null,
      "debug_abstract": "Copy trading has become the dominant entry strategy in meme coin markets. However, due to the market&#39;s extreme illiquid and volatile nature, the strategy exposes an exploitable attack surface: adversaries deploy manipulative bots to front-run trades, conceal positions, and fabricate sentiment, systematically extracting value from naïve copiers at scale. Despite its prevalence, bot-driven manipulation remains largely unexplored, and no robust defensive framework exists. We propose a manipulation-resistant copy-trading system based on a multi-agent architecture powered by a multi-modal, explainable large language model (LLM). Our system decomposes copy trading into three specialized agents for coin evaluation, wallet selection, and timing assessment. Evaluated on historical data from over 6,000 meme coins, our approach outperforms zero-shot and most statistic-driven baselines in prediction accuracy as well as all baselines in economic performance, achieving an average return of 14% for identified smart-money trades and an estimated copier return of 3% per trade under realistic market frictions. Overall, our results demonstrate the effectiveness of agent-based defenses and predictability of trader profitability in adversarial meme coin markets, providing a practical foundation for robust copy trading."
    },
    {
      "title": "Fast and Safe Trajectory Optimization for Mobile Manipulators With Neural Configuration Space Distance Field",
      "url": "https://arxiv.org/abs/2601.18548",
      "date": "2026-01-26",
      "authors_text": "Yulin Li, Zhiyuan Song, Yiming Li, Zhicheng Song, Kai Chen",
      "is_highlight": false,
      "score": 88.0,
      "summary": "The paper presents Generalized Configuration Space Distance Fields (GCDF) for efficient trajectory optimization in mobile manipulators, enhancing collision reasoning and enabling agile motion in complex environments.",
      "teaser_image": null,
      "debug_abstract": "Mobile manipulators promise agile, long-horizon behavior by coordinating base and arm motion, yet whole-body trajectory optimization in cluttered, confined spaces remains difficult due to high-dimensional nonconvexity and the need for fast, accurate collision reasoning. Configuration Space Distance Fields (CDF) enable fixed-base manipulators to model collisions directly in configuration space via smooth, implicit distances. This representation holds strong potential to bypass the nonlinear configuration-to-workspace mapping while preserving accurate whole-body geometry and providing optimization-friendly collision costs. Yet, extending this capability to mobile manipulators is hindered by unbounded workspaces and tighter base-arm coupling. We lift this promise to mobile manipulation with Generalized Configuration Space Distance Fields (GCDF), extending CDF to robots with both translational and rotational joints in unbounded workspaces with tighter base-arm coupling. We prove that GCDF preserves Euclidean-like local distance structure and accurately encodes whole-body geometry in configuration space, and develop a data generation and training pipeline that yields continuous neural GCDFs with accurate values and gradients, supporting efficient GPU-batched queries. Building on this representation, we develop a high-performance sequential convex optimization framework centered on GCDF-based collision reasoning. The solver scales to large numbers of implicit constraints through (i) online specification of neural constraints, (ii) sparsity-aware active-set detection with parallel batched evaluation across thousands of constraints, and (iii) incremental constraint management for rapid replanning under scene changes."
    },
    {
      "title": "Diffusion Model-based Reinforcement Learning for Version Age of Information Scheduling: Average and Tail-Risk-Sensitive Control",
      "url": "https://arxiv.org/abs/2601.18069",
      "date": "2026-01-26",
      "authors_text": "Haoyuan Pan, Sizhao Chen, Zhaorui Wang, Tse-Tin Chan",
      "is_highlight": false,
      "score": 50.0,
      "summary": "This paper presents a diffusion model-based reinforcement learning approach for optimizing Version Age of Information scheduling, balancing average performance with tail-risk sensitivity in wireless systems.",
      "teaser_image": null,
      "debug_abstract": "Ensuring timely and semantically accurate information delivery is critical in real-time wireless systems. While Age of Information (AoI) quantifies temporal freshness, Version Age of Information (VAoI) captures semantic staleness by accounting for version evolution between transmitters and receivers. Existing VAoI scheduling approaches primarily focus on minimizing average VAoI, overlooking rare but severe staleness events that can compromise reliability under stochastic packet arrivals and unreliable channels. This paper investigates both average-oriented and tail-risk-sensitive VAoI scheduling in a multi-user status update system with long-term transmission cost constraints. We first formulate the average VAoI minimization problem as a constrained Markov decision process and introduce a deep diffusion-based Soft Actor-Critic (D2SAC) algorithm. By generating actions through a diffusion-based denoising process, D2SAC enhances policy expressiveness and establishes a strong baseline for mean performance. Building on this foundation, we put forth RS-D3SAC, a risk-sensitive deep distributional diffusion-based Soft Actor-Critic algorithm. RS-D3SAC integrates a diffusion-based actor with a quantile-based distributional critic, explicitly modeling the full VAoI return distribution. This enables principled tail-risk optimization via Conditional Value-at-Risk (CVaR) while satisfying long-term transmission cost constraints. Extensive simulations show that, while D2SAC reduces average VAoI, RS-D3SAC consistently achieves substantial reductions in CVaR without sacrificing mean performance. The dominant gain in tail-risk reduction stems from the distributional critic, with the diffusion-based actor providing complementary refinement to stabilize and enrich policy decisions, highlighting their effectiveness for robust and risk-aware VAoI scheduling in multi-user wireless systems."
    }
  ],
  "具身智能实验室": [
    {
      "title": "Text-Pass Filter: An Efficient Scene Text Detector",
      "url": "https://arxiv.org/abs/2601.18098",
      "date": "2026-01-26",
      "authors_text": "Chuang Yang, Haozhao Ma, Xu Han, Yuan Yuan, Qi Wang",
      "is_highlight": false,
      "score": 30.0,
      "summary": "The Text-Pass Filter (TPF) enhances arbitrary-shaped text detection by directly segmenting text, improving recognition through unique feature-filter pairs and reinforcement mechanisms.",
      "teaser_image": null,
      "debug_abstract": "To pursue an efficient text assembling process, existing methods detect texts via the shrink-mask expansion strategy. However, the shrinking operation loses the visual features of text margins and confuses the foreground and background difference, which brings intrinsic limitations to recognize text features. We follow this issue and design Text-Pass Filter (TPF) for arbitrary-shaped text detection. It segments the whole text directly, which avoids the intrinsic limitations. It is noteworthy that different from previous whole text region-based methods, TPF can separate adhesive texts naturally without complex decoding or post-processing processes, which makes it possible for real-time text detection. Concretely, we find that the band-pass filter allows through components in a specified band of frequencies, called its passband but blocks components with frequencies above or below this band. It provides a natural idea for extracting whole texts separately. By simulating the band-pass filter, TPF constructs a unique feature-filter pair for each text. In the inference stage, every filter extracts the corresponding matched text by passing its pass-feature and blocking other features. Meanwhile, considering the large aspect ratio problem of ribbon-like texts makes it hard to recognize texts wholly, a Reinforcement Ensemble Unit (REU) is designed to enhance the feature consistency of the same text and to enlarge the filter&#39;s recognition field to help recognize whole texts. Furthermore, a Foreground Prior Unit (FPU) is introduced to encourage TPF to discriminate the difference between the foreground and background, which improves the feature-filter pair quality. Experiments demonstrate the effectiveness of REU and FPU while showing the TPF&#39;s superiority."
    }
  ],
  "通用机器人、自动化、传感和感知实验室 (GRASP)": [
    {
      "title": "Paying Less Generalization Tax: A Cross-Domain Generalization Study of RL Training for LLM Agents",
      "url": "https://arxiv.org/abs/2601.18217",
      "date": "2026-01-26",
      "authors_text": "Zhihan Liu, Lin Guan, Yixin Nie, Kai Zhang, Zhuoqun Hao",
      "is_highlight": false,
      "score": 70.0,
      "summary": "This study explores how state information richness and planning complexity in RL environments influence cross-domain generalization for LLM agents, proposing techniques to enhance robustness.",
      "teaser_image": null,
      "debug_abstract": "Generalist LLM agents are often post-trained on a narrow set of environments but deployed across far broader, unseen domains. In this work, we investigate the challenge of agentic post-training when the eventual test domains are unknown. Specifically, we analyze which properties of reinforcement learning (RL) environments and modeling choices have the greatest influence on out-of-domain performance. First, we identify two environment axes that strongly correlate with cross-domain generalization: (i) state information richness, i.e., the amount of information for the agent to process from the state, and (ii) planning complexity, estimated via goal reachability and trajectory length under a base policy. Notably, domain realism and text-level similarity are not the primary factors; for instance, the simple grid-world domain Sokoban leads to even stronger generalization in SciWorld than the more realistic ALFWorld. Motivated by these findings, we further show that increasing state information richness alone can already effectively improve cross-domain robustness. We propose a randomization technique, which is low-overhead and broadly applicable: add small amounts of distractive goal-irrelevant features to the state to make it richer without altering the task. Beyond environment-side properties, we also examine several modeling choices: (a) SFT warmup or mid-training helps prevent catastrophic forgetting during RL but undermines generalization to domains that are not included in the mid-training datamix; and (b) turning on step-by-step thinking during RL, while not always improving in-domain performance, plays a crucial role in preserving generalization."
    }
  ],
  "四川大学": [
    {
      "title": "Co-PLNet: A Collaborative Point-Line Network for Prompt-Guided Wireframe Parsing",
      "url": "https://arxiv.org/abs/2601.18252",
      "date": "2026-01-26",
      "authors_text": "Chao Wang, Xuanying Li, Cheng Dai, Jinglei Feng, Yuxiang Luo",
      "is_highlight": false,
      "score": 50.0,
      "summary": "Co-PLNet introduces a collaborative framework for wireframe parsing that enhances line and junction detection through spatial cue exchange, improving accuracy and efficiency in geometric representation.",
      "teaser_image": null,
      "debug_abstract": "Wireframe parsing aims to recover line segments and their junctions to form a structured geometric representation useful for downstream tasks such as Simultaneous Localization and Mapping (SLAM). Existing methods predict lines and junctions separately and reconcile them post-hoc, causing mismatches and reduced robustness. We present Co-PLNet, a point-line collaborative framework that exchanges spatial cues between the two tasks, where early detections are converted into spatial prompts via a Point-Line Prompt Encoder (PLP-Encoder), which encodes geometric attributes into compact and spatially aligned maps. A Cross-Guidance Line Decoder (CGL-Decoder) then refines predictions with sparse attention conditioned on complementary prompts, enforcing point-line consistency and efficiency. Experiments on Wireframe and YorkUrban show consistent improvements in accuracy and robustness, together with favorable real-time efficiency, demonstrating our effectiveness for structured geometry perception."
    }
  ],
  "浙江大学": [
    {
      "title": "Dep-Search: Learning Dependency-Aware Reasoning Traces with Persistent Memory",
      "url": "https://arxiv.org/abs/2601.18771",
      "date": "2026-01-26",
      "authors_text": "Yanming Liu, Xinyue Peng, Zixuan Yan, Yanxin Shen, Wenjie Xu",
      "is_highlight": false,
      "score": 30.0,
      "summary": "Dep-Search is a dependency-aware framework that enhances LLMs' multi-hop reasoning by integrating structured reasoning, retrieval, and persistent memory, outperforming existing methods.",
      "teaser_image": null,
      "debug_abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in complex reasoning tasks, particularly when augmented with search mechanisms that enable systematic exploration of external knowledge bases. The field has evolved from traditional retrieval-augmented generation (RAG) frameworks to more sophisticated search-based frameworks that orchestrate multi-step reasoning through explicit search strategies. However, existing search frameworks still rely heavily on implicit natural language reasoning to determine search strategies and how to leverage retrieved information across reasoning steps. This reliance on implicit reasoning creates fundamental challenges for managing dependencies between sub-questions, efficiently reusing previously retrieved knowledge, and learning optimal search strategies through reinforcement learning. To address these limitations, we propose Dep-Search, a dependency-aware search framework that advances beyond existing search frameworks by integrating structured reasoning, retrieval, and persistent memory through GRPO. Dep-Search introduces explicit control mechanisms that enable the model to decompose questions with dependency relationships, retrieve information when needed, access previously stored knowledge from memory, and summarize long reasoning contexts into reusable memory entries. Through extensive experiments on seven diverse question answering datasets, we demonstrate that Dep-Search significantly enhances LLMs&#39; ability to tackle complex multi-hop reasoning tasks, achieving substantial improvements over strong baselines across different model scales."
    },
    {
      "title": "Temp-R1: A Unified Autonomous Agent for Complex Temporal KGQA via Reverse Curriculum Reinforcement Learning",
      "url": "https://arxiv.org/abs/2601.18296",
      "date": "2026-01-26",
      "authors_text": "Zhaoyan Gong, Zhiqiang Liu, Songze Li, Xiaoke Guo, Yuanxiang Liu",
      "is_highlight": false,
      "score": 80.0,
      "summary": "Temp-R1 is a novel autonomous agent for Temporal Knowledge Graph Question Answering that utilizes reverse curriculum reinforcement learning to enhance reasoning capabilities and achieve state-of-the-art performance.",
      "teaser_image": null,
      "debug_abstract": "Temporal Knowledge Graph Question Answering (TKGQA) is inherently challenging, as it requires sophisticated reasoning over dynamic facts with multi-hop dependencies and complex temporal constraints. Existing methods rely on fixed workflows and expensive closed-source APIs, limiting flexibility and scalability. We propose Temp-R1, the first autonomous end-to-end agent for TKGQA trained through reinforcement learning. To address cognitive overload in single-action reasoning, we expand the action space with specialized internal actions alongside external action. To prevent shortcut learning on simple questions, we introduce reverse curriculum learning that trains on difficult questions first, forcing the development of sophisticated reasoning before transferring to easier cases. Our 8B-parameter Temp-R1 achieves state-of-the-art performance on MultiTQ and TimelineKGQA, improving 19.8% over strong baselines on complex questions. Our work establishes a new paradigm for autonomous temporal reasoning agents. Our code will be publicly available soon at this https URL."
    }
  ],
  "中国科学技术大学": [
    {
      "title": "GenAgent: Scaling Text-to-Image Generation via Agentic Multimodal Reasoning",
      "url": "https://arxiv.org/abs/2601.18543",
      "date": "2026-01-26",
      "authors_text": "Kaixun Jiang, Yuzheng Wang, Junjie Zhou, Pandeng Li, Zhihang Liu",
      "is_highlight": false,
      "score": 65.0,
      "summary": "GenAgent enhances text-to-image generation by decoupling understanding and generation through an agentic framework, enabling autonomous interactions and improved performance across tasks.",
      "teaser_image": null,
      "debug_abstract": "We introduce GenAgent, unifying visual understanding and generation through an agentic multimodal model. Unlike unified models that face expensive training costs and understanding-generation trade-offs, GenAgent decouples these capabilities through an agentic framework: understanding is handled by the multimodal model itself, while generation is achieved by treating image generation models as invokable tools. Crucially, unlike existing modular systems constrained by static pipelines, this design enables autonomous multi-turn interactions where the agent generates multimodal chains-of-thought encompassing reasoning, tool invocation, judgment, and reflection to iteratively refine outputs. We employ a two-stage training strategy: first, cold-start with supervised fine-tuning on high-quality tool invocation and reflection data to bootstrap agent behaviors; second, end-to-end agentic reinforcement learning combining pointwise rewards (final image quality) and pairwise rewards (reflection accuracy), with trajectory resampling for enhanced multi-turn exploration. GenAgent significantly boosts base generator(FLUX.1-dev) performance on GenEval++ (+23.6\\%) and WISE (+14\\%). Beyond performance gains, our framework demonstrates three key properties: 1) cross-tool generalization to generators with varying capabilities, 2) test-time scaling with consistent improvements across interaction rounds, and 3) task-adaptive reasoning that automatically adjusts to different tasks. Our code will be available at \\href{this https URL}{this url}."
    }
  ],
  "Multimedia Lab (MMLab)": [
    {
      "title": "AdaReasoner: Dynamic Tool Orchestration for Iterative Visual Reasoning",
      "url": "https://arxiv.org/abs/2601.18631",
      "date": "2026-01-26",
      "authors_text": "Mingyang Song, Haoyu Sun, Jiawei Gu, Linjie Li, Luxin Xu",
      "is_highlight": false,
      "score": 55.0,
      "summary": "AdaReasoner enhances visual reasoning in multimodal large language models by dynamically orchestrating tool usage through a scalable data pipeline and reinforcement learning, achieving state-of-the-art performance.",
      "teaser_image": null,
      "debug_abstract": "When humans face problems beyond their immediate capabilities, they rely on tools, providing a promising paradigm for improving visual reasoning in multimodal large language models (MLLMs). Effective reasoning, therefore, hinges on knowing which tools to use, when to invoke them, and how to compose them over multiple steps, even when faced with new tools or new tasks. We introduce \\textbf{AdaReasoner}, a family of multimodal models that learn tool use as a general reasoning skill rather than as tool-specific or explicitly supervised behavior. AdaReasoner is enabled by (i) a scalable data curation pipeline exposing models to long-horizon, multi-step tool interactions; (ii) Tool-GRPO, a reinforcement learning algorithm that optimizes tool selection and sequencing based on end-task success; and (iii) an adaptive learning mechanism that dynamically regulates tool usage. Together, these components allow models to infer tool utility from task context and intermediate outcomes, enabling coordination of multiple tools and generalization to unseen tools. Empirically, AdaReasoner exhibits strong tool-adaptive and generalization behaviors: it autonomously adopts beneficial tools, suppresses irrelevant ones, and adjusts tool usage frequency based on task demands, despite never being explicitly trained to do so. These capabilities translate into state-of-the-art performance across challenging benchmarks, improving the 7B base model by +24.9\\% on average and surpassing strong proprietary systems such as GPT-5 on multiple tasks, including VSP and Jigsaw."
    }
  ],
  "机器人与自动化研究中心": [
    {
      "title": "Advances and Innovations in the Multi-Agent Robotic System (MARS) Challenge",
      "url": "https://arxiv.org/abs/2601.18733",
      "date": "2026-01-26",
      "authors_text": "Li Kang, Heng Zhou, Xiufeng Song, Rui Li, Bruno N. Y. Chen",
      "is_highlight": false,
      "score": 75.0,
      "summary": "The MARS Challenge at NeurIPS 2025 promotes advancements in multi-agent systems by exploring vision-language models for collaborative robotic planning and manipulation in dynamic environments.",
      "teaser_image": null,
      "debug_abstract": "Recent advancements in multimodal large language models and vision-languageaction models have significantly driven progress in Embodied AI. As the field transitions toward more complex task scenarios, multi-agent system frameworks are becoming essential for achieving scalable, efficient, and collaborative solutions. This shift is fueled by three primary factors: increasing agent capabilities, enhancing system efficiency through task delegation, and enabling advanced human-agent interactions. To address the challenges posed by multi-agent collaboration, we propose the Multi-Agent Robotic System (MARS) Challenge, held at the NeurIPS 2025 Workshop on SpaVLE. The competition focuses on two critical areas: planning and control, where participants explore multi-agent embodied planning using vision-language models (VLMs) to coordinate tasks and policy execution to perform robotic manipulation in dynamic environments. By evaluating solutions submitted by participants, the challenge provides valuable insights into the design and coordination of embodied multi-agent systems, contributing to the future development of advanced collaborative AI systems."
    },
    {
      "title": "AdaReasoner: Dynamic Tool Orchestration for Iterative Visual Reasoning",
      "url": "https://arxiv.org/abs/2601.18631",
      "date": "2026-01-26",
      "authors_text": "Mingyang Song, Haoyu Sun, Jiawei Gu, Linjie Li, Luxin Xu",
      "is_highlight": false,
      "score": 55.0,
      "summary": "AdaReasoner enhances visual reasoning in multimodal large language models by dynamically orchestrating tool usage through a scalable data pipeline and reinforcement learning, achieving state-of-the-art performance.",
      "teaser_image": null,
      "debug_abstract": "When humans face problems beyond their immediate capabilities, they rely on tools, providing a promising paradigm for improving visual reasoning in multimodal large language models (MLLMs). Effective reasoning, therefore, hinges on knowing which tools to use, when to invoke them, and how to compose them over multiple steps, even when faced with new tools or new tasks. We introduce \\textbf{AdaReasoner}, a family of multimodal models that learn tool use as a general reasoning skill rather than as tool-specific or explicitly supervised behavior. AdaReasoner is enabled by (i) a scalable data curation pipeline exposing models to long-horizon, multi-step tool interactions; (ii) Tool-GRPO, a reinforcement learning algorithm that optimizes tool selection and sequencing based on end-task success; and (iii) an adaptive learning mechanism that dynamically regulates tool usage. Together, these components allow models to infer tool utility from task context and intermediate outcomes, enabling coordination of multiple tools and generalization to unseen tools. Empirically, AdaReasoner exhibits strong tool-adaptive and generalization behaviors: it autonomously adopts beneficial tools, suppresses irrelevant ones, and adjusts tool usage frequency based on task demands, despite never being explicitly trained to do so. These capabilities translate into state-of-the-art performance across challenging benchmarks, improving the 7B base model by +24.9\\% on average and surpassing strong proprietary systems such as GPT-5 on multiple tasks, including VSP and Jigsaw."
    }
  ],
  "机器人与人工智能实验室": [
    {
      "title": "AdaReasoner: Dynamic Tool Orchestration for Iterative Visual Reasoning",
      "url": "https://arxiv.org/abs/2601.18631",
      "date": "2026-01-26",
      "authors_text": "Mingyang Song, Haoyu Sun, Jiawei Gu, Linjie Li, Luxin Xu",
      "is_highlight": false,
      "score": 55.0,
      "summary": "AdaReasoner enhances visual reasoning in multimodal large language models by dynamically orchestrating tool usage through a scalable data pipeline and reinforcement learning, achieving state-of-the-art performance.",
      "teaser_image": null,
      "debug_abstract": "When humans face problems beyond their immediate capabilities, they rely on tools, providing a promising paradigm for improving visual reasoning in multimodal large language models (MLLMs). Effective reasoning, therefore, hinges on knowing which tools to use, when to invoke them, and how to compose them over multiple steps, even when faced with new tools or new tasks. We introduce \\textbf{AdaReasoner}, a family of multimodal models that learn tool use as a general reasoning skill rather than as tool-specific or explicitly supervised behavior. AdaReasoner is enabled by (i) a scalable data curation pipeline exposing models to long-horizon, multi-step tool interactions; (ii) Tool-GRPO, a reinforcement learning algorithm that optimizes tool selection and sequencing based on end-task success; and (iii) an adaptive learning mechanism that dynamically regulates tool usage. Together, these components allow models to infer tool utility from task context and intermediate outcomes, enabling coordination of multiple tools and generalization to unseen tools. Empirically, AdaReasoner exhibits strong tool-adaptive and generalization behaviors: it autonomously adopts beneficial tools, suppresses irrelevant ones, and adjusts tool usage frequency based on task demands, despite never being explicitly trained to do so. These capabilities translate into state-of-the-art performance across challenging benchmarks, improving the 7B base model by +24.9\\% on average and surpassing strong proprietary systems such as GPT-5 on multiple tasks, including VSP and Jigsaw."
    }
  ],
  "电子科技大学": [
    {
      "title": "AdaReasoner: Dynamic Tool Orchestration for Iterative Visual Reasoning",
      "url": "https://arxiv.org/abs/2601.18631",
      "date": "2026-01-26",
      "authors_text": "Mingyang Song, Haoyu Sun, Jiawei Gu, Linjie Li, Luxin Xu",
      "is_highlight": false,
      "score": 55.0,
      "summary": "AdaReasoner enhances visual reasoning in multimodal large language models by dynamically orchestrating tool usage through a scalable data pipeline and reinforcement learning, achieving state-of-the-art performance.",
      "teaser_image": null,
      "debug_abstract": "When humans face problems beyond their immediate capabilities, they rely on tools, providing a promising paradigm for improving visual reasoning in multimodal large language models (MLLMs). Effective reasoning, therefore, hinges on knowing which tools to use, when to invoke them, and how to compose them over multiple steps, even when faced with new tools or new tasks. We introduce \\textbf{AdaReasoner}, a family of multimodal models that learn tool use as a general reasoning skill rather than as tool-specific or explicitly supervised behavior. AdaReasoner is enabled by (i) a scalable data curation pipeline exposing models to long-horizon, multi-step tool interactions; (ii) Tool-GRPO, a reinforcement learning algorithm that optimizes tool selection and sequencing based on end-task success; and (iii) an adaptive learning mechanism that dynamically regulates tool usage. Together, these components allow models to infer tool utility from task context and intermediate outcomes, enabling coordination of multiple tools and generalization to unseen tools. Empirically, AdaReasoner exhibits strong tool-adaptive and generalization behaviors: it autonomously adopts beneficial tools, suppresses irrelevant ones, and adjusts tool usage frequency based on task demands, despite never being explicitly trained to do so. These capabilities translate into state-of-the-art performance across challenging benchmarks, improving the 7B base model by +24.9\\% on average and surpassing strong proprietary systems such as GPT-5 on multiple tasks, including VSP and Jigsaw."
    }
  ],
  "机器人与人工智能实验室 (RAIL)": [
    {
      "title": "AdaReasoner: Dynamic Tool Orchestration for Iterative Visual Reasoning",
      "url": "https://arxiv.org/abs/2601.18631",
      "date": "2026-01-26",
      "authors_text": "Mingyang Song, Haoyu Sun, Jiawei Gu, Linjie Li, Luxin Xu",
      "is_highlight": false,
      "score": 55.0,
      "summary": "AdaReasoner enhances visual reasoning in multimodal large language models by dynamically orchestrating tool usage through a scalable data pipeline and reinforcement learning, achieving state-of-the-art performance.",
      "teaser_image": null,
      "debug_abstract": "When humans face problems beyond their immediate capabilities, they rely on tools, providing a promising paradigm for improving visual reasoning in multimodal large language models (MLLMs). Effective reasoning, therefore, hinges on knowing which tools to use, when to invoke them, and how to compose them over multiple steps, even when faced with new tools or new tasks. We introduce \\textbf{AdaReasoner}, a family of multimodal models that learn tool use as a general reasoning skill rather than as tool-specific or explicitly supervised behavior. AdaReasoner is enabled by (i) a scalable data curation pipeline exposing models to long-horizon, multi-step tool interactions; (ii) Tool-GRPO, a reinforcement learning algorithm that optimizes tool selection and sequencing based on end-task success; and (iii) an adaptive learning mechanism that dynamically regulates tool usage. Together, these components allow models to infer tool utility from task context and intermediate outcomes, enabling coordination of multiple tools and generalization to unseen tools. Empirically, AdaReasoner exhibits strong tool-adaptive and generalization behaviors: it autonomously adopts beneficial tools, suppresses irrelevant ones, and adjusts tool usage frequency based on task demands, despite never being explicitly trained to do so. These capabilities translate into state-of-the-art performance across challenging benchmarks, improving the 7B base model by +24.9\\% on average and surpassing strong proprietary systems such as GPT-5 on multiple tasks, including VSP and Jigsaw."
    }
  ],
  "深圳市人工智能与机器人研究院 (AIRS)": [
    {
      "title": "AdaReasoner: Dynamic Tool Orchestration for Iterative Visual Reasoning",
      "url": "https://arxiv.org/abs/2601.18631",
      "date": "2026-01-26",
      "authors_text": "Mingyang Song, Haoyu Sun, Jiawei Gu, Linjie Li, Luxin Xu",
      "is_highlight": false,
      "score": 55.0,
      "summary": "AdaReasoner enhances visual reasoning in multimodal large language models by dynamically orchestrating tool usage through a scalable data pipeline and reinforcement learning, achieving state-of-the-art performance.",
      "teaser_image": null,
      "debug_abstract": "When humans face problems beyond their immediate capabilities, they rely on tools, providing a promising paradigm for improving visual reasoning in multimodal large language models (MLLMs). Effective reasoning, therefore, hinges on knowing which tools to use, when to invoke them, and how to compose them over multiple steps, even when faced with new tools or new tasks. We introduce \\textbf{AdaReasoner}, a family of multimodal models that learn tool use as a general reasoning skill rather than as tool-specific or explicitly supervised behavior. AdaReasoner is enabled by (i) a scalable data curation pipeline exposing models to long-horizon, multi-step tool interactions; (ii) Tool-GRPO, a reinforcement learning algorithm that optimizes tool selection and sequencing based on end-task success; and (iii) an adaptive learning mechanism that dynamically regulates tool usage. Together, these components allow models to infer tool utility from task context and intermediate outcomes, enabling coordination of multiple tools and generalization to unseen tools. Empirically, AdaReasoner exhibits strong tool-adaptive and generalization behaviors: it autonomously adopts beneficial tools, suppresses irrelevant ones, and adjusts tool usage frequency based on task demands, despite never being explicitly trained to do so. These capabilities translate into state-of-the-art performance across challenging benchmarks, improving the 7B base model by +24.9\\% on average and surpassing strong proprietary systems such as GPT-5 on multiple tasks, including VSP and Jigsaw."
    }
  ],
  "人机物智能融合实验室 (HCP)": [
    {
      "title": "Why Keep Your Doubts to Yourself? Trading Visual Uncertainties in Multi-Agent Bandit Systems",
      "url": "https://arxiv.org/abs/2601.18735",
      "date": "2026-01-26",
      "authors_text": "Jusheng Zhang, Yijia Fan, Kaitong Cai, Jing Yang, Jiawei Yao",
      "is_highlight": false,
      "score": 50.0,
      "summary": "The paper introduces Agora, a decentralized market framework for trading visual uncertainties in multi-agent systems, enhancing coordination efficiency and reducing costs significantly.",
      "teaser_image": null,
      "debug_abstract": "Vision-Language Models (VLMs) enable powerful multi-agent systems, but scaling them is economically unsustainable: coordinating heterogeneous agents under information asymmetry often spirals costs. Existing paradigms, such as Mixture-of-Agents and knowledge-based routers, rely on heuristic proxies that ignore costs and collapse uncertainty structure, leading to provably suboptimal coordination. We introduce Agora, a framework that reframes coordination as a decentralized market for uncertainty. Agora formalizes epistemic uncertainty into a structured, tradable asset (perceptual, semantic, inferential), and enforces profitability-driven trading among agents based on rational economic rules. A market-aware broker, extending Thompson Sampling, initiates collaboration and guides the system toward cost-efficient equilibria. Experiments on five multimodal benchmarks (MMMU, MMBench, MathVision, InfoVQA, CC-OCR) show that Agora outperforms strong VLMs and heuristic multi-agent strategies, e.g., achieving +8.5% accuracy over the best baseline on MMMU while reducing cost by over 3x. These results establish market-based coordination as a principled and scalable paradigm for building economically viable multi-agent visual intelligence systems."
    }
  ],
  "Quest for Intelligence": [
    {
      "title": "Dep-Search: Learning Dependency-Aware Reasoning Traces with Persistent Memory",
      "url": "https://arxiv.org/abs/2601.18771",
      "date": "2026-01-26",
      "authors_text": "Yanming Liu, Xinyue Peng, Zixuan Yan, Yanxin Shen, Wenjie Xu",
      "is_highlight": false,
      "score": 30.0,
      "summary": "Dep-Search is a dependency-aware framework that enhances LLMs' multi-hop reasoning by integrating structured reasoning, retrieval, and persistent memory, outperforming existing methods.",
      "teaser_image": null,
      "debug_abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in complex reasoning tasks, particularly when augmented with search mechanisms that enable systematic exploration of external knowledge bases. The field has evolved from traditional retrieval-augmented generation (RAG) frameworks to more sophisticated search-based frameworks that orchestrate multi-step reasoning through explicit search strategies. However, existing search frameworks still rely heavily on implicit natural language reasoning to determine search strategies and how to leverage retrieved information across reasoning steps. This reliance on implicit reasoning creates fundamental challenges for managing dependencies between sub-questions, efficiently reusing previously retrieved knowledge, and learning optimal search strategies through reinforcement learning. To address these limitations, we propose Dep-Search, a dependency-aware search framework that advances beyond existing search frameworks by integrating structured reasoning, retrieval, and persistent memory through GRPO. Dep-Search introduces explicit control mechanisms that enable the model to decompose questions with dependency relationships, retrieve information when needed, access previously stored knowledge from memory, and summarize long reasoning contexts into reusable memory entries. Through extensive experiments on seven diverse question answering datasets, we demonstrate that Dep-Search significantly enhances LLMs&#39; ability to tackle complex multi-hop reasoning tasks, achieving substantial improvements over strong baselines across different model scales."
    }
  ],
  "CSAIL Embodied Intelligence Labs": [
    {
      "title": "Dep-Search: Learning Dependency-Aware Reasoning Traces with Persistent Memory",
      "url": "https://arxiv.org/abs/2601.18771",
      "date": "2026-01-26",
      "authors_text": "Yanming Liu, Xinyue Peng, Zixuan Yan, Yanxin Shen, Wenjie Xu",
      "is_highlight": false,
      "score": 30.0,
      "summary": "Dep-Search is a dependency-aware framework that enhances LLMs' multi-hop reasoning by integrating structured reasoning, retrieval, and persistent memory, outperforming existing methods.",
      "teaser_image": null,
      "debug_abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in complex reasoning tasks, particularly when augmented with search mechanisms that enable systematic exploration of external knowledge bases. The field has evolved from traditional retrieval-augmented generation (RAG) frameworks to more sophisticated search-based frameworks that orchestrate multi-step reasoning through explicit search strategies. However, existing search frameworks still rely heavily on implicit natural language reasoning to determine search strategies and how to leverage retrieved information across reasoning steps. This reliance on implicit reasoning creates fundamental challenges for managing dependencies between sub-questions, efficiently reusing previously retrieved knowledge, and learning optimal search strategies through reinforcement learning. To address these limitations, we propose Dep-Search, a dependency-aware search framework that advances beyond existing search frameworks by integrating structured reasoning, retrieval, and persistent memory through GRPO. Dep-Search introduces explicit control mechanisms that enable the model to decompose questions with dependency relationships, retrieve information when needed, access previously stored knowledge from memory, and summarize long reasoning contexts into reusable memory entries. Through extensive experiments on seven diverse question answering datasets, we demonstrate that Dep-Search significantly enhances LLMs&#39; ability to tackle complex multi-hop reasoning tasks, achieving substantial improvements over strong baselines across different model scales."
    }
  ],
  "机器人与机械智能实验室 (ROMI Lab)": [
    {
      "title": "DV-VLN: Dual Verification for Reliable LLM-Based Vision-and-Language Navigation",
      "url": "https://arxiv.org/abs/2601.18492",
      "date": "2026-01-26",
      "authors_text": "Zijun Li, Shijie Li, Zhenxi Zhang, Bin Li, Shoujun Zhou",
      "is_highlight": false,
      "score": 85.0,
      "summary": "DV-VLN enhances vision-and-language navigation by implementing a generate-then-verify approach with dual verification channels, improving decision reliability and performance in complex environments.",
      "teaser_image": null,
      "debug_abstract": "Vision-and-Language Navigation (VLN) requires an embodied agent to navigate in a complex 3D environment according to natural language instructions. Recent progress in large language models (LLMs) has enabled language-driven navigation with improved interpretability. However, most LLM-based agents still rely on single-shot action decisions, where the model must choose one option from noisy, textualized multi-perspective observations. Due to local mismatches and imperfect intermediate reasoning, such decisions can easily deviate from the correct path, leading to error accumulation and reduced reliability in unseen environments. In this paper, we propose DV-VLN, a new VLN framework that follows a generate-then-verify paradigm. DV-VLN first performs parameter-efficient in-domain adaptation of an open-source LLaMA-2 backbone to produce a structured navigational chain-of-thought, and then verifies candidate actions with two complementary channels: True-False Verification (TFV) and Masked-Entity Verification (MEV). DV-VLN selects actions by aggregating verification successes across multiple samples, yielding interpretable scores for reranking. Experiments on R2R, RxR (English subset), and REVERIE show that DV-VLN consistently improves over direct prediction and sampling-only baselines, achieving competitive performance among language-only VLN agents and promising results compared with several cross-modal this http URL is available at this https URL."
    }
  ]
}