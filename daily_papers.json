{
  "中国人民大学": [
    {
      "title": "MapDream: Task-Driven Map Learning for Vision-Language Navigation",
      "url": "https://arxiv.org/abs/2602.00222",
      "date": "2026-02-03",
      "authors_text": "Guoxin Lian, Shuo Wang, Yucheng Wang, Yongcai Wang, Maiyue Chen",
      "is_highlight": false,
      "score": 85.0,
      "summary": "MapDream introduces a task-driven framework for Vision-Language Navigation that learns compact, navigation-focused maps through autoregressive synthesis, achieving state-of-the-art performance in relevant benchmarks.",
      "teaser_image": "https://arxiv.org/html/2602.00222/x2.png",
      "debug_abstract": "Vision-Language Navigation (VLN) requires agents to follow natural language instructions in partially observed 3D environments, motivating map representations that aggregate spatial context beyond local perception. However, most existing approaches rely on hand-crafted maps constructed independently of the navigation policy. We argue that maps should instead be learned representations shaped directly by navigation objectives rather than exhaustive reconstructions. Based on this insight, we propose MapDream, a map-in-the-loop framework that formulates map construction as autoregressive bird&#39;s-eye-view (BEV) image synthesis. The framework jointly learns map generation and action prediction, distilling environmental context into a compact three-channel BEV map that preserves only navigation-critical affordances. Supervised pre-training bootstraps a reliable mapping-to-control interface, while the autoregressive design enables end-to-end joint optimization through reinforcement fine-tuning. Experiments on R2R-CE and RxR-CE achieve state-of-the-art monocular performance, validating task-driven generative map learning."
    }
  ],
  "北京航空航天大学": [
    {
      "title": "USS-Nav: Unified Spatio-Semantic Scene Graph for Lightweight UAV Zero-Shot Object Navigation",
      "url": "https://arxiv.org/abs/2602.00708",
      "date": "2026-02-03",
      "authors_text": "Weiqi Gai, Yuman Gao, Yuan Zhou, Yufan Xie, Zhiyang Liu",
      "is_highlight": false,
      "score": 82.0,
      "summary": "USS-Nav is a lightweight UAV framework that utilizes a Unified Spatio-Semantic scene graph for efficient zero-shot object navigation in unknown environments, enhancing computational efficiency and real-time performance.",
      "teaser_image": "https://arxiv.org/html/2602.00708/figs/skeleton_gen.png",
      "debug_abstract": "Zero-Shot Object Navigation in unknown environments poses significant challenges for Unmanned Aerial Vehicles (UAVs) due to the conflict between high-level semantic reasoning requirements and limited onboard computational resources. To address this, we present USS-Nav, a lightweight framework that incrementally constructs a Unified Spatio-Semantic scene graph and enables efficient Large Language Model (LLM)-augmented Zero-Shot Object Navigation in unknown environments. Specifically, we introduce an incremental Spatial Connectivity Graph generation method utilizing polyhedral expansion to capture global geometric topology, which is dynamically partitioned into semantic regions via graph clustering. Concurrently, open-vocabulary object semantics are instantiated and anchored to this topology to form a hierarchical environmental representation. Leveraging this hierarchical structure, we present a coarse-to-fine exploration strategy: LLM grounded in the scene graph&#39;s semantics to determine global target regions, while a local planner optimizes frontier coverage based on information gain. Experimental results demonstrate that our framework outperforms state-of-the-art methods in terms of computational efficiency and real-time update frequency (15 Hz) on a resource-constrained platform. Furthermore, ablation studies confirm the effectiveness of our framework, showing substantial improvements in Success weighted by Path Length (SPL). The source code will be made publicly available to foster further research."
    }
  ],
  "浙江大学": [
    {
      "title": "Hand3R: Online 4D Hand-Scene Reconstruction in the Wild",
      "url": "https://arxiv.org/abs/2602.03200",
      "date": "2026-02-03",
      "authors_text": "Wendi Hu, Haonan Zhou, Wenhao Hu, Gaoang Wang",
      "is_highlight": false,
      "score": 91.0,
      "summary": "Hand3R is an innovative online framework for simultaneous 4D hand and scene reconstruction from monocular video, integrating hand priors with scene context for enhanced accuracy.",
      "teaser_image": "https://arxiv.org/html/2602.03200/x1.png",
      "debug_abstract": "For Embodied AI, jointly reconstructing dynamic hands and the dense scene context is crucial for understanding physical interaction. However, most existing methods recover isolated hands in local coordinates, overlooking the surrounding 3D environment. To address this, we present Hand3R, the first online framework for joint 4D hand-scene reconstruction from monocular video. Hand3R synergizes a pre-trained hand expert with a 4D scene foundation model via a scene-aware visual prompting mechanism. By injecting high-fidelity hand priors into a persistent scene memory, our approach enables simultaneous reconstruction of accurate hand meshes and dense metric-scale scene geometry in a single forward pass. Experiments demonstrate that Hand3R bypasses the reliance on offline optimization and delivers competitive performance in both local hand reconstruction and global positioning."
    },
    {
      "title": "From Frames to Sequences: Temporally Consistent Human-Centric Dense Prediction",
      "url": "https://arxiv.org/abs/2602.01661",
      "date": "2026-02-03",
      "authors_text": "Xingyu Miao, Junting Dong, Qin Zhao, Yuhang Yang, Junhao Chen",
      "is_highlight": false,
      "score": 72.0,
      "summary": "This paper presents a novel synthetic data pipeline and a ViT-based model for achieving temporally consistent human-centric dense prediction in video sequences, outperforming existing methods.",
      "teaser_image": "https://arxiv.org/html/2602.01661/x2.png",
      "debug_abstract": "In this work, we focus on the challenge of temporally consistent human-centric dense prediction across video sequences. Existing models achieve strong per-frame accuracy but often flicker under motion, occlusion, and lighting changes, and they rarely have paired human video supervision for multiple dense tasks. We address this gap with a scalable synthetic data pipeline that generates photorealistic human frames and motion-aligned sequences with pixel-accurate depth, normals, and masks. Unlike prior static data synthetic pipelines, our pipeline provides both frame-level labels for spatial learning and sequence-level supervision for temporal learning. Building on this, we train a unified ViT-based dense predictor that (i) injects an explicit human geometric prior via CSE embeddings and (ii) improves geometry-feature reliability with a lightweight channel reweighting module after feature fusion. Our two-stage training strategy, combining static pretraining with dynamic sequence supervision, enables the model first to acquire robust spatial representations and then refine temporal consistency across motion-aligned sequences. Extensive experiments show that we achieve state-of-the-art performance on THuman2.1 and Hi4D and generalize effectively to in-the-wild videos."
    },
    {
      "title": "USS-Nav: Unified Spatio-Semantic Scene Graph for Lightweight UAV Zero-Shot Object Navigation",
      "url": "https://arxiv.org/abs/2602.00708",
      "date": "2026-02-03",
      "authors_text": "Weiqi Gai, Yuman Gao, Yuan Zhou, Yufan Xie, Zhiyang Liu",
      "is_highlight": false,
      "score": 82.0,
      "summary": "USS-Nav is a lightweight UAV framework that utilizes a Unified Spatio-Semantic scene graph for efficient zero-shot object navigation in unknown environments, enhancing computational efficiency and real-time performance.",
      "teaser_image": "https://arxiv.org/html/2602.00708/figs/skeleton_gen.png",
      "debug_abstract": "Zero-Shot Object Navigation in unknown environments poses significant challenges for Unmanned Aerial Vehicles (UAVs) due to the conflict between high-level semantic reasoning requirements and limited onboard computational resources. To address this, we present USS-Nav, a lightweight framework that incrementally constructs a Unified Spatio-Semantic scene graph and enables efficient Large Language Model (LLM)-augmented Zero-Shot Object Navigation in unknown environments. Specifically, we introduce an incremental Spatial Connectivity Graph generation method utilizing polyhedral expansion to capture global geometric topology, which is dynamically partitioned into semantic regions via graph clustering. Concurrently, open-vocabulary object semantics are instantiated and anchored to this topology to form a hierarchical environmental representation. Leveraging this hierarchical structure, we present a coarse-to-fine exploration strategy: LLM grounded in the scene graph&#39;s semantics to determine global target regions, while a local planner optimizes frontier coverage based on information gain. Experimental results demonstrate that our framework outperforms state-of-the-art methods in terms of computational efficiency and real-time update frequency (15 Hz) on a resource-constrained platform. Furthermore, ablation studies confirm the effectiveness of our framework, showing substantial improvements in Success weighted by Path Length (SPL). The source code will be made publicly available to foster further research."
    }
  ],
  "Oxford Robotics Institute (ORI)": [
    {
      "title": "TreeLoc: 6-DoF LiDAR Global Localization in Forests via Inter-Tree Geometric Matching",
      "url": "https://arxiv.org/abs/2602.01501",
      "date": "2026-02-03",
      "authors_text": "Minwoo Jung, Nived Chebrolu, Lucas Carvalho de Lima, Haedam Oh, Maurice Fallon",
      "is_highlight": false,
      "score": 78.0,
      "summary": "TreeLoc is a robust LiDAR-based localization framework for forests that utilizes tree geometry for accurate 6-DoF pose estimation, outperforming traditional methods.",
      "teaser_image": "https://arxiv.org/html/2602.01501/x2.png",
      "debug_abstract": "Reliable localization is crucial for navigation in forests, where GPS is often degraded and LiDAR measurements are repetitive, occluded, and structurally complex. These conditions weaken the assumptions of traditional urban-centric localization methods, which assume that consistent features arise from unique structural patterns, necessitating forest-centric solutions to achieve robustness in these environments. To address these challenges, we propose TreeLoc, a LiDAR-based global localization framework for forests that handles place recognition and 6-DoF pose estimation. We represent scenes using tree stems and their Diameter at Breast Height (DBH), which are aligned to a common reference frame via their axes and summarized using the tree distribution histogram (TDH) for coarse matching, followed by fine matching with a 2D triangle descriptor. Finally, pose estimation is achieved through a two-step geometric verification. On diverse forest benchmarks, TreeLoc outperforms baselines, achieving precise localization. Ablation studies validate the contribution of each component. We also propose applications for long-term forest management using descriptors from a compact global tree database. TreeLoc is open-sourced for the robotics community at this https URL."
    }
  ],
  "具身智能实验室 (TEA Lab)": [
    {
      "title": "InstaDrive: Instance-Aware Driving World Models for Realistic and Consistent Video Generation",
      "url": "https://arxiv.org/abs/2602.03242",
      "date": "2026-02-03",
      "authors_text": "Zhuoran Yang, Xi Guo, Chenjing Ding, Chiyu Wang, Wei Wu",
      "is_highlight": false,
      "score": 84.0,
      "summary": "InstaDrive introduces a framework that enhances realistic driving video generation by ensuring instance-level temporal consistency and spatial geometric fidelity through innovative feature extraction and alignment techniques.",
      "teaser_image": "https://arxiv.org/html/2602.03242/x2.png",
      "debug_abstract": "Autonomous driving relies on robust models trained on high-quality, large-scale multi-view driving videos. While world models offer a cost-effective solution for generating realistic driving videos, they struggle to maintain instance-level temporal consistency and spatial geometric fidelity. To address these challenges, we propose InstaDrive, a novel framework that enhances driving video realism through two key advancements: (1) Instance Flow Guider, which extracts and propagates instance features across frames to enforce temporal consistency, preserving instance identity over time. (2) Spatial Geometric Aligner, which improves spatial reasoning, ensures precise instance positioning, and explicitly models occlusion hierarchies. By incorporating these instance-aware mechanisms, InstaDrive achieves state-of-the-art video generation quality and enhances downstream autonomous driving tasks on the nuScenes dataset. Additionally, we utilize CARLA&#39;s autopilot to procedurally and stochastically simulate rare but safety-critical driving scenarios across diverse maps and regions, enabling rigorous safety evaluation for autonomous systems. Our project page is this https URL."
    },
    {
      "title": "EventFlash: Towards Efficient MLLMs for Event-Based Vision",
      "url": "https://arxiv.org/abs/2602.03230",
      "date": "2026-02-03",
      "authors_text": "Shaoyu Liu, Jianing Li, Guanghui Zhao, Yunjian Zhang, Wen Jiang",
      "is_highlight": false,
      "score": 63.0,
      "summary": "EventFlash introduces an efficient MLLM for event-based vision, leveraging spatiotemporal sparsification and adaptive sampling to enhance performance and reduce computational costs.",
      "teaser_image": "https://arxiv.org/html/2602.03230/x2.png",
      "debug_abstract": "Event-based multimodal large language models (MLLMs) enable robust perception in high-speed and low-light scenarios, addressing key limitations of frame-based MLLMs. However, current event-based MLLMs often rely on dense image-like processing paradigms, overlooking the spatiotemporal sparsity of event streams and resulting in high computational cost. In this paper, we propose EventFlash, a novel and efficient MLLM to explore spatiotemporal token sparsification for reducing data redundancy and accelerating inference. Technically, we build EventMind, a large-scale and scene-diverse dataset with over 500k instruction sets, providing both short and long event stream sequences to support our curriculum training strategy. We then present an adaptive temporal window aggregation module for efficient temporal sampling, which adaptively compresses temporal tokens while retaining key temporal cues. Finally, a sparse density-guided attention module is designed to improve spatial token efficiency by selecting informative regions and suppressing empty or sparse areas. Experimental results show that EventFlash achieves a $12.4\\times$ throughput improvement over the baseline (EventFlash-Zero) while maintaining comparable performance. It supports long-range event stream processing with up to 1,000 bins, significantly outperforming the 5-bin limit of EventGPT. We believe EventFlash serves as an efficient foundation model for event-based vision."
    },
    {
      "title": "From Frames to Sequences: Temporally Consistent Human-Centric Dense Prediction",
      "url": "https://arxiv.org/abs/2602.01661",
      "date": "2026-02-03",
      "authors_text": "Xingyu Miao, Junting Dong, Qin Zhao, Yuhang Yang, Junhao Chen",
      "is_highlight": false,
      "score": 72.0,
      "summary": "This paper presents a novel synthetic data pipeline and a ViT-based model for achieving temporally consistent human-centric dense prediction in video sequences, outperforming existing methods.",
      "teaser_image": "https://arxiv.org/html/2602.01661/x2.png",
      "debug_abstract": "In this work, we focus on the challenge of temporally consistent human-centric dense prediction across video sequences. Existing models achieve strong per-frame accuracy but often flicker under motion, occlusion, and lighting changes, and they rarely have paired human video supervision for multiple dense tasks. We address this gap with a scalable synthetic data pipeline that generates photorealistic human frames and motion-aligned sequences with pixel-accurate depth, normals, and masks. Unlike prior static data synthetic pipelines, our pipeline provides both frame-level labels for spatial learning and sequence-level supervision for temporal learning. Building on this, we train a unified ViT-based dense predictor that (i) injects an explicit human geometric prior via CSE embeddings and (ii) improves geometry-feature reliability with a lightweight channel reweighting module after feature fusion. Our two-stage training strategy, combining static pretraining with dynamic sequence supervision, enables the model first to acquire robust spatial representations and then refine temporal consistency across motion-aligned sequences. Extensive experiments show that we achieve state-of-the-art performance on THuman2.1 and Hi4D and generalize effectively to in-the-wild videos."
    },
    {
      "title": "Making Avatars Interact: Towards Text-Driven Human-Object Interaction for Controllable Talking Avatars",
      "url": "https://arxiv.org/abs/2602.01538",
      "date": "2026-02-02",
      "authors_text": "Youliang Zhang, Zhengguang Zhou, Zhentao Yu, Ziyao Huang, Teng Hu",
      "is_highlight": false,
      "score": 75.0,
      "summary": "The paper presents InteractAvatar, a dual-stream framework for generating text-driven human-object interactions in talking avatars, enhancing perception and control in video synthesis.",
      "teaser_image": "https://arxiv.org/html/2602.01538/x1.png",
      "debug_abstract": "Generating talking avatars is a fundamental task in video generation. Although existing methods can generate full-body talking avatars with simple human motion, extending this task to grounded human-object interaction (GHOI) remains an open challenge, requiring the avatar to perform text-aligned interactions with surrounding objects. This challenge stems from the need for environmental perception and the control-quality dilemma in GHOI generation. To address this, we propose a novel dual-stream framework, InteractAvatar, which decouples perception and planning from video synthesis for grounded human-object interaction. Leveraging detection to enhance environmental perception, we introduce a Perception and Interaction Module (PIM) to generate text-aligned interaction motions. Additionally, an Audio-Interaction Aware Generation Module (AIM) is proposed to synthesize vivid talking avatars performing object interactions. With a specially designed motion-to-video aligner, PIM and AIM share a similar network structure and enable parallel co-generation of motions and plausible videos, effectively mitigating the control-quality dilemma. Finally, we establish a benchmark, GroundedInter, for evaluating GHOI video generation. Extensive experiments and comparisons demonstrate the effectiveness of our method in generating grounded human-object interactions for talking avatars. Project page: this https URL"
    }
  ],
  "机器人控制实验室": [
    {
      "title": "InstaDrive: Instance-Aware Driving World Models for Realistic and Consistent Video Generation",
      "url": "https://arxiv.org/abs/2602.03242",
      "date": "2026-02-03",
      "authors_text": "Zhuoran Yang, Xi Guo, Chenjing Ding, Chiyu Wang, Wei Wu",
      "is_highlight": false,
      "score": 84.0,
      "summary": "InstaDrive introduces a framework that enhances realistic driving video generation by ensuring instance-level temporal consistency and spatial geometric fidelity through innovative feature extraction and alignment techniques.",
      "teaser_image": "https://arxiv.org/html/2602.03242/x2.png",
      "debug_abstract": "Autonomous driving relies on robust models trained on high-quality, large-scale multi-view driving videos. While world models offer a cost-effective solution for generating realistic driving videos, they struggle to maintain instance-level temporal consistency and spatial geometric fidelity. To address these challenges, we propose InstaDrive, a novel framework that enhances driving video realism through two key advancements: (1) Instance Flow Guider, which extracts and propagates instance features across frames to enforce temporal consistency, preserving instance identity over time. (2) Spatial Geometric Aligner, which improves spatial reasoning, ensures precise instance positioning, and explicitly models occlusion hierarchies. By incorporating these instance-aware mechanisms, InstaDrive achieves state-of-the-art video generation quality and enhances downstream autonomous driving tasks on the nuScenes dataset. Additionally, we utilize CARLA&#39;s autopilot to procedurally and stochastically simulate rare but safety-critical driving scenarios across diverse maps and regions, enabling rigorous safety evaluation for autonomous systems. Our project page is this https URL."
    },
    {
      "title": "EventFlash: Towards Efficient MLLMs for Event-Based Vision",
      "url": "https://arxiv.org/abs/2602.03230",
      "date": "2026-02-03",
      "authors_text": "Shaoyu Liu, Jianing Li, Guanghui Zhao, Yunjian Zhang, Wen Jiang",
      "is_highlight": false,
      "score": 63.0,
      "summary": "EventFlash introduces an efficient MLLM for event-based vision, leveraging spatiotemporal sparsification and adaptive sampling to enhance performance and reduce computational costs.",
      "teaser_image": "https://arxiv.org/html/2602.03230/x2.png",
      "debug_abstract": "Event-based multimodal large language models (MLLMs) enable robust perception in high-speed and low-light scenarios, addressing key limitations of frame-based MLLMs. However, current event-based MLLMs often rely on dense image-like processing paradigms, overlooking the spatiotemporal sparsity of event streams and resulting in high computational cost. In this paper, we propose EventFlash, a novel and efficient MLLM to explore spatiotemporal token sparsification for reducing data redundancy and accelerating inference. Technically, we build EventMind, a large-scale and scene-diverse dataset with over 500k instruction sets, providing both short and long event stream sequences to support our curriculum training strategy. We then present an adaptive temporal window aggregation module for efficient temporal sampling, which adaptively compresses temporal tokens while retaining key temporal cues. Finally, a sparse density-guided attention module is designed to improve spatial token efficiency by selecting informative regions and suppressing empty or sparse areas. Experimental results show that EventFlash achieves a $12.4\\times$ throughput improvement over the baseline (EventFlash-Zero) while maintaining comparable performance. It supports long-range event stream processing with up to 1,000 bins, significantly outperforming the 5-bin limit of EventGPT. We believe EventFlash serves as an efficient foundation model for event-based vision."
    },
    {
      "title": "From Frames to Sequences: Temporally Consistent Human-Centric Dense Prediction",
      "url": "https://arxiv.org/abs/2602.01661",
      "date": "2026-02-03",
      "authors_text": "Xingyu Miao, Junting Dong, Qin Zhao, Yuhang Yang, Junhao Chen",
      "is_highlight": false,
      "score": 72.0,
      "summary": "This paper presents a novel synthetic data pipeline and a ViT-based model for achieving temporally consistent human-centric dense prediction in video sequences, outperforming existing methods.",
      "teaser_image": "https://arxiv.org/html/2602.01661/x2.png",
      "debug_abstract": "In this work, we focus on the challenge of temporally consistent human-centric dense prediction across video sequences. Existing models achieve strong per-frame accuracy but often flicker under motion, occlusion, and lighting changes, and they rarely have paired human video supervision for multiple dense tasks. We address this gap with a scalable synthetic data pipeline that generates photorealistic human frames and motion-aligned sequences with pixel-accurate depth, normals, and masks. Unlike prior static data synthetic pipelines, our pipeline provides both frame-level labels for spatial learning and sequence-level supervision for temporal learning. Building on this, we train a unified ViT-based dense predictor that (i) injects an explicit human geometric prior via CSE embeddings and (ii) improves geometry-feature reliability with a lightweight channel reweighting module after feature fusion. Our two-stage training strategy, combining static pretraining with dynamic sequence supervision, enables the model first to acquire robust spatial representations and then refine temporal consistency across motion-aligned sequences. Extensive experiments show that we achieve state-of-the-art performance on THuman2.1 and Hi4D and generalize effectively to in-the-wild videos."
    },
    {
      "title": "Making Avatars Interact: Towards Text-Driven Human-Object Interaction for Controllable Talking Avatars",
      "url": "https://arxiv.org/abs/2602.01538",
      "date": "2026-02-02",
      "authors_text": "Youliang Zhang, Zhengguang Zhou, Zhentao Yu, Ziyao Huang, Teng Hu",
      "is_highlight": false,
      "score": 75.0,
      "summary": "The paper presents InteractAvatar, a dual-stream framework for generating text-driven human-object interactions in talking avatars, enhancing perception and control in video synthesis.",
      "teaser_image": "https://arxiv.org/html/2602.01538/x1.png",
      "debug_abstract": "Generating talking avatars is a fundamental task in video generation. Although existing methods can generate full-body talking avatars with simple human motion, extending this task to grounded human-object interaction (GHOI) remains an open challenge, requiring the avatar to perform text-aligned interactions with surrounding objects. This challenge stems from the need for environmental perception and the control-quality dilemma in GHOI generation. To address this, we propose a novel dual-stream framework, InteractAvatar, which decouples perception and planning from video synthesis for grounded human-object interaction. Leveraging detection to enhance environmental perception, we introduce a Perception and Interaction Module (PIM) to generate text-aligned interaction motions. Additionally, an Audio-Interaction Aware Generation Module (AIM) is proposed to synthesize vivid talking avatars performing object interactions. With a specially designed motion-to-video aligner, PIM and AIM share a similar network structure and enable parallel co-generation of motions and plausible videos, effectively mitigating the control-quality dilemma. Finally, we establish a benchmark, GroundedInter, for evaluating GHOI video generation. Extensive experiments and comparisons demonstrate the effectiveness of our method in generating grounded human-object interactions for talking avatars. Project page: this https URL"
    }
  ],
  "智能系统与机器人实验室 (ISR Lab)": [
    {
      "title": "InstaDrive: Instance-Aware Driving World Models for Realistic and Consistent Video Generation",
      "url": "https://arxiv.org/abs/2602.03242",
      "date": "2026-02-03",
      "authors_text": "Zhuoran Yang, Xi Guo, Chenjing Ding, Chiyu Wang, Wei Wu",
      "is_highlight": false,
      "score": 84.0,
      "summary": "InstaDrive introduces a framework that enhances realistic driving video generation by ensuring instance-level temporal consistency and spatial geometric fidelity through innovative feature extraction and alignment techniques.",
      "teaser_image": "https://arxiv.org/html/2602.03242/x2.png",
      "debug_abstract": "Autonomous driving relies on robust models trained on high-quality, large-scale multi-view driving videos. While world models offer a cost-effective solution for generating realistic driving videos, they struggle to maintain instance-level temporal consistency and spatial geometric fidelity. To address these challenges, we propose InstaDrive, a novel framework that enhances driving video realism through two key advancements: (1) Instance Flow Guider, which extracts and propagates instance features across frames to enforce temporal consistency, preserving instance identity over time. (2) Spatial Geometric Aligner, which improves spatial reasoning, ensures precise instance positioning, and explicitly models occlusion hierarchies. By incorporating these instance-aware mechanisms, InstaDrive achieves state-of-the-art video generation quality and enhances downstream autonomous driving tasks on the nuScenes dataset. Additionally, we utilize CARLA&#39;s autopilot to procedurally and stochastically simulate rare but safety-critical driving scenarios across diverse maps and regions, enabling rigorous safety evaluation for autonomous systems. Our project page is this https URL."
    },
    {
      "title": "EventFlash: Towards Efficient MLLMs for Event-Based Vision",
      "url": "https://arxiv.org/abs/2602.03230",
      "date": "2026-02-03",
      "authors_text": "Shaoyu Liu, Jianing Li, Guanghui Zhao, Yunjian Zhang, Wen Jiang",
      "is_highlight": false,
      "score": 63.0,
      "summary": "EventFlash introduces an efficient MLLM for event-based vision, leveraging spatiotemporal sparsification and adaptive sampling to enhance performance and reduce computational costs.",
      "teaser_image": "https://arxiv.org/html/2602.03230/x2.png",
      "debug_abstract": "Event-based multimodal large language models (MLLMs) enable robust perception in high-speed and low-light scenarios, addressing key limitations of frame-based MLLMs. However, current event-based MLLMs often rely on dense image-like processing paradigms, overlooking the spatiotemporal sparsity of event streams and resulting in high computational cost. In this paper, we propose EventFlash, a novel and efficient MLLM to explore spatiotemporal token sparsification for reducing data redundancy and accelerating inference. Technically, we build EventMind, a large-scale and scene-diverse dataset with over 500k instruction sets, providing both short and long event stream sequences to support our curriculum training strategy. We then present an adaptive temporal window aggregation module for efficient temporal sampling, which adaptively compresses temporal tokens while retaining key temporal cues. Finally, a sparse density-guided attention module is designed to improve spatial token efficiency by selecting informative regions and suppressing empty or sparse areas. Experimental results show that EventFlash achieves a $12.4\\times$ throughput improvement over the baseline (EventFlash-Zero) while maintaining comparable performance. It supports long-range event stream processing with up to 1,000 bins, significantly outperforming the 5-bin limit of EventGPT. We believe EventFlash serves as an efficient foundation model for event-based vision."
    },
    {
      "title": "From Frames to Sequences: Temporally Consistent Human-Centric Dense Prediction",
      "url": "https://arxiv.org/abs/2602.01661",
      "date": "2026-02-03",
      "authors_text": "Xingyu Miao, Junting Dong, Qin Zhao, Yuhang Yang, Junhao Chen",
      "is_highlight": false,
      "score": 72.0,
      "summary": "This paper presents a novel synthetic data pipeline and a ViT-based model for achieving temporally consistent human-centric dense prediction in video sequences, outperforming existing methods.",
      "teaser_image": "https://arxiv.org/html/2602.01661/x2.png",
      "debug_abstract": "In this work, we focus on the challenge of temporally consistent human-centric dense prediction across video sequences. Existing models achieve strong per-frame accuracy but often flicker under motion, occlusion, and lighting changes, and they rarely have paired human video supervision for multiple dense tasks. We address this gap with a scalable synthetic data pipeline that generates photorealistic human frames and motion-aligned sequences with pixel-accurate depth, normals, and masks. Unlike prior static data synthetic pipelines, our pipeline provides both frame-level labels for spatial learning and sequence-level supervision for temporal learning. Building on this, we train a unified ViT-based dense predictor that (i) injects an explicit human geometric prior via CSE embeddings and (ii) improves geometry-feature reliability with a lightweight channel reweighting module after feature fusion. Our two-stage training strategy, combining static pretraining with dynamic sequence supervision, enables the model first to acquire robust spatial representations and then refine temporal consistency across motion-aligned sequences. Extensive experiments show that we achieve state-of-the-art performance on THuman2.1 and Hi4D and generalize effectively to in-the-wild videos."
    },
    {
      "title": "Making Avatars Interact: Towards Text-Driven Human-Object Interaction for Controllable Talking Avatars",
      "url": "https://arxiv.org/abs/2602.01538",
      "date": "2026-02-02",
      "authors_text": "Youliang Zhang, Zhengguang Zhou, Zhentao Yu, Ziyao Huang, Teng Hu",
      "is_highlight": false,
      "score": 75.0,
      "summary": "The paper presents InteractAvatar, a dual-stream framework for generating text-driven human-object interactions in talking avatars, enhancing perception and control in video synthesis.",
      "teaser_image": "https://arxiv.org/html/2602.01538/x1.png",
      "debug_abstract": "Generating talking avatars is a fundamental task in video generation. Although existing methods can generate full-body talking avatars with simple human motion, extending this task to grounded human-object interaction (GHOI) remains an open challenge, requiring the avatar to perform text-aligned interactions with surrounding objects. This challenge stems from the need for environmental perception and the control-quality dilemma in GHOI generation. To address this, we propose a novel dual-stream framework, InteractAvatar, which decouples perception and planning from video synthesis for grounded human-object interaction. Leveraging detection to enhance environmental perception, we introduce a Perception and Interaction Module (PIM) to generate text-aligned interaction motions. Additionally, an Audio-Interaction Aware Generation Module (AIM) is proposed to synthesize vivid talking avatars performing object interactions. With a specially designed motion-to-video aligner, PIM and AIM share a similar network structure and enable parallel co-generation of motions and plausible videos, effectively mitigating the control-quality dilemma. Finally, we establish a benchmark, GroundedInter, for evaluating GHOI video generation. Extensive experiments and comparisons demonstrate the effectiveness of our method in generating grounded human-object interactions for talking avatars. Project page: this https URL"
    }
  ],
  "智能技术与系统实验室": [
    {
      "title": "InstaDrive: Instance-Aware Driving World Models for Realistic and Consistent Video Generation",
      "url": "https://arxiv.org/abs/2602.03242",
      "date": "2026-02-03",
      "authors_text": "Zhuoran Yang, Xi Guo, Chenjing Ding, Chiyu Wang, Wei Wu",
      "is_highlight": false,
      "score": 84.0,
      "summary": "InstaDrive introduces a framework that enhances realistic driving video generation by ensuring instance-level temporal consistency and spatial geometric fidelity through innovative feature extraction and alignment techniques.",
      "teaser_image": "https://arxiv.org/html/2602.03242/x2.png",
      "debug_abstract": "Autonomous driving relies on robust models trained on high-quality, large-scale multi-view driving videos. While world models offer a cost-effective solution for generating realistic driving videos, they struggle to maintain instance-level temporal consistency and spatial geometric fidelity. To address these challenges, we propose InstaDrive, a novel framework that enhances driving video realism through two key advancements: (1) Instance Flow Guider, which extracts and propagates instance features across frames to enforce temporal consistency, preserving instance identity over time. (2) Spatial Geometric Aligner, which improves spatial reasoning, ensures precise instance positioning, and explicitly models occlusion hierarchies. By incorporating these instance-aware mechanisms, InstaDrive achieves state-of-the-art video generation quality and enhances downstream autonomous driving tasks on the nuScenes dataset. Additionally, we utilize CARLA&#39;s autopilot to procedurally and stochastically simulate rare but safety-critical driving scenarios across diverse maps and regions, enabling rigorous safety evaluation for autonomous systems. Our project page is this https URL."
    },
    {
      "title": "EventFlash: Towards Efficient MLLMs for Event-Based Vision",
      "url": "https://arxiv.org/abs/2602.03230",
      "date": "2026-02-03",
      "authors_text": "Shaoyu Liu, Jianing Li, Guanghui Zhao, Yunjian Zhang, Wen Jiang",
      "is_highlight": false,
      "score": 63.0,
      "summary": "EventFlash introduces an efficient MLLM for event-based vision, leveraging spatiotemporal sparsification and adaptive sampling to enhance performance and reduce computational costs.",
      "teaser_image": "https://arxiv.org/html/2602.03230/x2.png",
      "debug_abstract": "Event-based multimodal large language models (MLLMs) enable robust perception in high-speed and low-light scenarios, addressing key limitations of frame-based MLLMs. However, current event-based MLLMs often rely on dense image-like processing paradigms, overlooking the spatiotemporal sparsity of event streams and resulting in high computational cost. In this paper, we propose EventFlash, a novel and efficient MLLM to explore spatiotemporal token sparsification for reducing data redundancy and accelerating inference. Technically, we build EventMind, a large-scale and scene-diverse dataset with over 500k instruction sets, providing both short and long event stream sequences to support our curriculum training strategy. We then present an adaptive temporal window aggregation module for efficient temporal sampling, which adaptively compresses temporal tokens while retaining key temporal cues. Finally, a sparse density-guided attention module is designed to improve spatial token efficiency by selecting informative regions and suppressing empty or sparse areas. Experimental results show that EventFlash achieves a $12.4\\times$ throughput improvement over the baseline (EventFlash-Zero) while maintaining comparable performance. It supports long-range event stream processing with up to 1,000 bins, significantly outperforming the 5-bin limit of EventGPT. We believe EventFlash serves as an efficient foundation model for event-based vision."
    },
    {
      "title": "From Frames to Sequences: Temporally Consistent Human-Centric Dense Prediction",
      "url": "https://arxiv.org/abs/2602.01661",
      "date": "2026-02-03",
      "authors_text": "Xingyu Miao, Junting Dong, Qin Zhao, Yuhang Yang, Junhao Chen",
      "is_highlight": false,
      "score": 72.0,
      "summary": "This paper presents a novel synthetic data pipeline and a ViT-based model for achieving temporally consistent human-centric dense prediction in video sequences, outperforming existing methods.",
      "teaser_image": "https://arxiv.org/html/2602.01661/x2.png",
      "debug_abstract": "In this work, we focus on the challenge of temporally consistent human-centric dense prediction across video sequences. Existing models achieve strong per-frame accuracy but often flicker under motion, occlusion, and lighting changes, and they rarely have paired human video supervision for multiple dense tasks. We address this gap with a scalable synthetic data pipeline that generates photorealistic human frames and motion-aligned sequences with pixel-accurate depth, normals, and masks. Unlike prior static data synthetic pipelines, our pipeline provides both frame-level labels for spatial learning and sequence-level supervision for temporal learning. Building on this, we train a unified ViT-based dense predictor that (i) injects an explicit human geometric prior via CSE embeddings and (ii) improves geometry-feature reliability with a lightweight channel reweighting module after feature fusion. Our two-stage training strategy, combining static pretraining with dynamic sequence supervision, enables the model first to acquire robust spatial representations and then refine temporal consistency across motion-aligned sequences. Extensive experiments show that we achieve state-of-the-art performance on THuman2.1 and Hi4D and generalize effectively to in-the-wild videos."
    },
    {
      "title": "Making Avatars Interact: Towards Text-Driven Human-Object Interaction for Controllable Talking Avatars",
      "url": "https://arxiv.org/abs/2602.01538",
      "date": "2026-02-02",
      "authors_text": "Youliang Zhang, Zhengguang Zhou, Zhentao Yu, Ziyao Huang, Teng Hu",
      "is_highlight": false,
      "score": 75.0,
      "summary": "The paper presents InteractAvatar, a dual-stream framework for generating text-driven human-object interactions in talking avatars, enhancing perception and control in video synthesis.",
      "teaser_image": "https://arxiv.org/html/2602.01538/x1.png",
      "debug_abstract": "Generating talking avatars is a fundamental task in video generation. Although existing methods can generate full-body talking avatars with simple human motion, extending this task to grounded human-object interaction (GHOI) remains an open challenge, requiring the avatar to perform text-aligned interactions with surrounding objects. This challenge stems from the need for environmental perception and the control-quality dilemma in GHOI generation. To address this, we propose a novel dual-stream framework, InteractAvatar, which decouples perception and planning from video synthesis for grounded human-object interaction. Leveraging detection to enhance environmental perception, we introduce a Perception and Interaction Module (PIM) to generate text-aligned interaction motions. Additionally, an Audio-Interaction Aware Generation Module (AIM) is proposed to synthesize vivid talking avatars performing object interactions. With a specially designed motion-to-video aligner, PIM and AIM share a similar network structure and enable parallel co-generation of motions and plausible videos, effectively mitigating the control-quality dilemma. Finally, we establish a benchmark, GroundedInter, for evaluating GHOI video generation. Extensive experiments and comparisons demonstrate the effectiveness of our method in generating grounded human-object interactions for talking avatars. Project page: this https URL"
    }
  ],
  "具身视觉与机器人实验室 (EVAR Lab)": [
    {
      "title": "InstaDrive: Instance-Aware Driving World Models for Realistic and Consistent Video Generation",
      "url": "https://arxiv.org/abs/2602.03242",
      "date": "2026-02-03",
      "authors_text": "Zhuoran Yang, Xi Guo, Chenjing Ding, Chiyu Wang, Wei Wu",
      "is_highlight": false,
      "score": 84.0,
      "summary": "InstaDrive introduces a framework that enhances realistic driving video generation by ensuring instance-level temporal consistency and spatial geometric fidelity through innovative feature extraction and alignment techniques.",
      "teaser_image": "https://arxiv.org/html/2602.03242/x2.png",
      "debug_abstract": "Autonomous driving relies on robust models trained on high-quality, large-scale multi-view driving videos. While world models offer a cost-effective solution for generating realistic driving videos, they struggle to maintain instance-level temporal consistency and spatial geometric fidelity. To address these challenges, we propose InstaDrive, a novel framework that enhances driving video realism through two key advancements: (1) Instance Flow Guider, which extracts and propagates instance features across frames to enforce temporal consistency, preserving instance identity over time. (2) Spatial Geometric Aligner, which improves spatial reasoning, ensures precise instance positioning, and explicitly models occlusion hierarchies. By incorporating these instance-aware mechanisms, InstaDrive achieves state-of-the-art video generation quality and enhances downstream autonomous driving tasks on the nuScenes dataset. Additionally, we utilize CARLA&#39;s autopilot to procedurally and stochastically simulate rare but safety-critical driving scenarios across diverse maps and regions, enabling rigorous safety evaluation for autonomous systems. Our project page is this https URL."
    },
    {
      "title": "EventFlash: Towards Efficient MLLMs for Event-Based Vision",
      "url": "https://arxiv.org/abs/2602.03230",
      "date": "2026-02-03",
      "authors_text": "Shaoyu Liu, Jianing Li, Guanghui Zhao, Yunjian Zhang, Wen Jiang",
      "is_highlight": false,
      "score": 63.0,
      "summary": "EventFlash introduces an efficient MLLM for event-based vision, leveraging spatiotemporal sparsification and adaptive sampling to enhance performance and reduce computational costs.",
      "teaser_image": "https://arxiv.org/html/2602.03230/x2.png",
      "debug_abstract": "Event-based multimodal large language models (MLLMs) enable robust perception in high-speed and low-light scenarios, addressing key limitations of frame-based MLLMs. However, current event-based MLLMs often rely on dense image-like processing paradigms, overlooking the spatiotemporal sparsity of event streams and resulting in high computational cost. In this paper, we propose EventFlash, a novel and efficient MLLM to explore spatiotemporal token sparsification for reducing data redundancy and accelerating inference. Technically, we build EventMind, a large-scale and scene-diverse dataset with over 500k instruction sets, providing both short and long event stream sequences to support our curriculum training strategy. We then present an adaptive temporal window aggregation module for efficient temporal sampling, which adaptively compresses temporal tokens while retaining key temporal cues. Finally, a sparse density-guided attention module is designed to improve spatial token efficiency by selecting informative regions and suppressing empty or sparse areas. Experimental results show that EventFlash achieves a $12.4\\times$ throughput improvement over the baseline (EventFlash-Zero) while maintaining comparable performance. It supports long-range event stream processing with up to 1,000 bins, significantly outperforming the 5-bin limit of EventGPT. We believe EventFlash serves as an efficient foundation model for event-based vision."
    },
    {
      "title": "From Frames to Sequences: Temporally Consistent Human-Centric Dense Prediction",
      "url": "https://arxiv.org/abs/2602.01661",
      "date": "2026-02-03",
      "authors_text": "Xingyu Miao, Junting Dong, Qin Zhao, Yuhang Yang, Junhao Chen",
      "is_highlight": false,
      "score": 72.0,
      "summary": "This paper presents a novel synthetic data pipeline and a ViT-based model for achieving temporally consistent human-centric dense prediction in video sequences, outperforming existing methods.",
      "teaser_image": "https://arxiv.org/html/2602.01661/x2.png",
      "debug_abstract": "In this work, we focus on the challenge of temporally consistent human-centric dense prediction across video sequences. Existing models achieve strong per-frame accuracy but often flicker under motion, occlusion, and lighting changes, and they rarely have paired human video supervision for multiple dense tasks. We address this gap with a scalable synthetic data pipeline that generates photorealistic human frames and motion-aligned sequences with pixel-accurate depth, normals, and masks. Unlike prior static data synthetic pipelines, our pipeline provides both frame-level labels for spatial learning and sequence-level supervision for temporal learning. Building on this, we train a unified ViT-based dense predictor that (i) injects an explicit human geometric prior via CSE embeddings and (ii) improves geometry-feature reliability with a lightweight channel reweighting module after feature fusion. Our two-stage training strategy, combining static pretraining with dynamic sequence supervision, enables the model first to acquire robust spatial representations and then refine temporal consistency across motion-aligned sequences. Extensive experiments show that we achieve state-of-the-art performance on THuman2.1 and Hi4D and generalize effectively to in-the-wild videos."
    },
    {
      "title": "Making Avatars Interact: Towards Text-Driven Human-Object Interaction for Controllable Talking Avatars",
      "url": "https://arxiv.org/abs/2602.01538",
      "date": "2026-02-02",
      "authors_text": "Youliang Zhang, Zhengguang Zhou, Zhentao Yu, Ziyao Huang, Teng Hu",
      "is_highlight": false,
      "score": 75.0,
      "summary": "The paper presents InteractAvatar, a dual-stream framework for generating text-driven human-object interactions in talking avatars, enhancing perception and control in video synthesis.",
      "teaser_image": "https://arxiv.org/html/2602.01538/x1.png",
      "debug_abstract": "Generating talking avatars is a fundamental task in video generation. Although existing methods can generate full-body talking avatars with simple human motion, extending this task to grounded human-object interaction (GHOI) remains an open challenge, requiring the avatar to perform text-aligned interactions with surrounding objects. This challenge stems from the need for environmental perception and the control-quality dilemma in GHOI generation. To address this, we propose a novel dual-stream framework, InteractAvatar, which decouples perception and planning from video synthesis for grounded human-object interaction. Leveraging detection to enhance environmental perception, we introduce a Perception and Interaction Module (PIM) to generate text-aligned interaction motions. Additionally, an Audio-Interaction Aware Generation Module (AIM) is proposed to synthesize vivid talking avatars performing object interactions. With a specially designed motion-to-video aligner, PIM and AIM share a similar network structure and enable parallel co-generation of motions and plausible videos, effectively mitigating the control-quality dilemma. Finally, we establish a benchmark, GroundedInter, for evaluating GHOI video generation. Extensive experiments and comparisons demonstrate the effectiveness of our method in generating grounded human-object interactions for talking avatars. Project page: this https URL"
    }
  ],
  "MARS多模态学习实验室": [
    {
      "title": "InstaDrive: Instance-Aware Driving World Models for Realistic and Consistent Video Generation",
      "url": "https://arxiv.org/abs/2602.03242",
      "date": "2026-02-03",
      "authors_text": "Zhuoran Yang, Xi Guo, Chenjing Ding, Chiyu Wang, Wei Wu",
      "is_highlight": false,
      "score": 84.0,
      "summary": "InstaDrive introduces a framework that enhances realistic driving video generation by ensuring instance-level temporal consistency and spatial geometric fidelity through innovative feature extraction and alignment techniques.",
      "teaser_image": "https://arxiv.org/html/2602.03242/x2.png",
      "debug_abstract": "Autonomous driving relies on robust models trained on high-quality, large-scale multi-view driving videos. While world models offer a cost-effective solution for generating realistic driving videos, they struggle to maintain instance-level temporal consistency and spatial geometric fidelity. To address these challenges, we propose InstaDrive, a novel framework that enhances driving video realism through two key advancements: (1) Instance Flow Guider, which extracts and propagates instance features across frames to enforce temporal consistency, preserving instance identity over time. (2) Spatial Geometric Aligner, which improves spatial reasoning, ensures precise instance positioning, and explicitly models occlusion hierarchies. By incorporating these instance-aware mechanisms, InstaDrive achieves state-of-the-art video generation quality and enhances downstream autonomous driving tasks on the nuScenes dataset. Additionally, we utilize CARLA&#39;s autopilot to procedurally and stochastically simulate rare but safety-critical driving scenarios across diverse maps and regions, enabling rigorous safety evaluation for autonomous systems. Our project page is this https URL."
    },
    {
      "title": "EventFlash: Towards Efficient MLLMs for Event-Based Vision",
      "url": "https://arxiv.org/abs/2602.03230",
      "date": "2026-02-03",
      "authors_text": "Shaoyu Liu, Jianing Li, Guanghui Zhao, Yunjian Zhang, Wen Jiang",
      "is_highlight": false,
      "score": 63.0,
      "summary": "EventFlash introduces an efficient MLLM for event-based vision, leveraging spatiotemporal sparsification and adaptive sampling to enhance performance and reduce computational costs.",
      "teaser_image": "https://arxiv.org/html/2602.03230/x2.png",
      "debug_abstract": "Event-based multimodal large language models (MLLMs) enable robust perception in high-speed and low-light scenarios, addressing key limitations of frame-based MLLMs. However, current event-based MLLMs often rely on dense image-like processing paradigms, overlooking the spatiotemporal sparsity of event streams and resulting in high computational cost. In this paper, we propose EventFlash, a novel and efficient MLLM to explore spatiotemporal token sparsification for reducing data redundancy and accelerating inference. Technically, we build EventMind, a large-scale and scene-diverse dataset with over 500k instruction sets, providing both short and long event stream sequences to support our curriculum training strategy. We then present an adaptive temporal window aggregation module for efficient temporal sampling, which adaptively compresses temporal tokens while retaining key temporal cues. Finally, a sparse density-guided attention module is designed to improve spatial token efficiency by selecting informative regions and suppressing empty or sparse areas. Experimental results show that EventFlash achieves a $12.4\\times$ throughput improvement over the baseline (EventFlash-Zero) while maintaining comparable performance. It supports long-range event stream processing with up to 1,000 bins, significantly outperforming the 5-bin limit of EventGPT. We believe EventFlash serves as an efficient foundation model for event-based vision."
    },
    {
      "title": "From Frames to Sequences: Temporally Consistent Human-Centric Dense Prediction",
      "url": "https://arxiv.org/abs/2602.01661",
      "date": "2026-02-03",
      "authors_text": "Xingyu Miao, Junting Dong, Qin Zhao, Yuhang Yang, Junhao Chen",
      "is_highlight": false,
      "score": 72.0,
      "summary": "This paper presents a novel synthetic data pipeline and a ViT-based model for achieving temporally consistent human-centric dense prediction in video sequences, outperforming existing methods.",
      "teaser_image": "https://arxiv.org/html/2602.01661/x2.png",
      "debug_abstract": "In this work, we focus on the challenge of temporally consistent human-centric dense prediction across video sequences. Existing models achieve strong per-frame accuracy but often flicker under motion, occlusion, and lighting changes, and they rarely have paired human video supervision for multiple dense tasks. We address this gap with a scalable synthetic data pipeline that generates photorealistic human frames and motion-aligned sequences with pixel-accurate depth, normals, and masks. Unlike prior static data synthetic pipelines, our pipeline provides both frame-level labels for spatial learning and sequence-level supervision for temporal learning. Building on this, we train a unified ViT-based dense predictor that (i) injects an explicit human geometric prior via CSE embeddings and (ii) improves geometry-feature reliability with a lightweight channel reweighting module after feature fusion. Our two-stage training strategy, combining static pretraining with dynamic sequence supervision, enables the model first to acquire robust spatial representations and then refine temporal consistency across motion-aligned sequences. Extensive experiments show that we achieve state-of-the-art performance on THuman2.1 and Hi4D and generalize effectively to in-the-wild videos."
    },
    {
      "title": "Making Avatars Interact: Towards Text-Driven Human-Object Interaction for Controllable Talking Avatars",
      "url": "https://arxiv.org/abs/2602.01538",
      "date": "2026-02-02",
      "authors_text": "Youliang Zhang, Zhengguang Zhou, Zhentao Yu, Ziyao Huang, Teng Hu",
      "is_highlight": false,
      "score": 75.0,
      "summary": "The paper presents InteractAvatar, a dual-stream framework for generating text-driven human-object interactions in talking avatars, enhancing perception and control in video synthesis.",
      "teaser_image": "https://arxiv.org/html/2602.01538/x1.png",
      "debug_abstract": "Generating talking avatars is a fundamental task in video generation. Although existing methods can generate full-body talking avatars with simple human motion, extending this task to grounded human-object interaction (GHOI) remains an open challenge, requiring the avatar to perform text-aligned interactions with surrounding objects. This challenge stems from the need for environmental perception and the control-quality dilemma in GHOI generation. To address this, we propose a novel dual-stream framework, InteractAvatar, which decouples perception and planning from video synthesis for grounded human-object interaction. Leveraging detection to enhance environmental perception, we introduce a Perception and Interaction Module (PIM) to generate text-aligned interaction motions. Additionally, an Audio-Interaction Aware Generation Module (AIM) is proposed to synthesize vivid talking avatars performing object interactions. With a specially designed motion-to-video aligner, PIM and AIM share a similar network structure and enable parallel co-generation of motions and plausible videos, effectively mitigating the control-quality dilemma. Finally, we establish a benchmark, GroundedInter, for evaluating GHOI video generation. Extensive experiments and comparisons demonstrate the effectiveness of our method in generating grounded human-object interactions for talking avatars. Project page: this https URL"
    }
  ],
  "智能产业研究院 (AIR)": [
    {
      "title": "InstaDrive: Instance-Aware Driving World Models for Realistic and Consistent Video Generation",
      "url": "https://arxiv.org/abs/2602.03242",
      "date": "2026-02-03",
      "authors_text": "Zhuoran Yang, Xi Guo, Chenjing Ding, Chiyu Wang, Wei Wu",
      "is_highlight": false,
      "score": 84.0,
      "summary": "InstaDrive introduces a framework that enhances realistic driving video generation by ensuring instance-level temporal consistency and spatial geometric fidelity through innovative feature extraction and alignment techniques.",
      "teaser_image": "https://arxiv.org/html/2602.03242/x2.png",
      "debug_abstract": "Autonomous driving relies on robust models trained on high-quality, large-scale multi-view driving videos. While world models offer a cost-effective solution for generating realistic driving videos, they struggle to maintain instance-level temporal consistency and spatial geometric fidelity. To address these challenges, we propose InstaDrive, a novel framework that enhances driving video realism through two key advancements: (1) Instance Flow Guider, which extracts and propagates instance features across frames to enforce temporal consistency, preserving instance identity over time. (2) Spatial Geometric Aligner, which improves spatial reasoning, ensures precise instance positioning, and explicitly models occlusion hierarchies. By incorporating these instance-aware mechanisms, InstaDrive achieves state-of-the-art video generation quality and enhances downstream autonomous driving tasks on the nuScenes dataset. Additionally, we utilize CARLA&#39;s autopilot to procedurally and stochastically simulate rare but safety-critical driving scenarios across diverse maps and regions, enabling rigorous safety evaluation for autonomous systems. Our project page is this https URL."
    },
    {
      "title": "EventFlash: Towards Efficient MLLMs for Event-Based Vision",
      "url": "https://arxiv.org/abs/2602.03230",
      "date": "2026-02-03",
      "authors_text": "Shaoyu Liu, Jianing Li, Guanghui Zhao, Yunjian Zhang, Wen Jiang",
      "is_highlight": false,
      "score": 63.0,
      "summary": "EventFlash introduces an efficient MLLM for event-based vision, leveraging spatiotemporal sparsification and adaptive sampling to enhance performance and reduce computational costs.",
      "teaser_image": "https://arxiv.org/html/2602.03230/x2.png",
      "debug_abstract": "Event-based multimodal large language models (MLLMs) enable robust perception in high-speed and low-light scenarios, addressing key limitations of frame-based MLLMs. However, current event-based MLLMs often rely on dense image-like processing paradigms, overlooking the spatiotemporal sparsity of event streams and resulting in high computational cost. In this paper, we propose EventFlash, a novel and efficient MLLM to explore spatiotemporal token sparsification for reducing data redundancy and accelerating inference. Technically, we build EventMind, a large-scale and scene-diverse dataset with over 500k instruction sets, providing both short and long event stream sequences to support our curriculum training strategy. We then present an adaptive temporal window aggregation module for efficient temporal sampling, which adaptively compresses temporal tokens while retaining key temporal cues. Finally, a sparse density-guided attention module is designed to improve spatial token efficiency by selecting informative regions and suppressing empty or sparse areas. Experimental results show that EventFlash achieves a $12.4\\times$ throughput improvement over the baseline (EventFlash-Zero) while maintaining comparable performance. It supports long-range event stream processing with up to 1,000 bins, significantly outperforming the 5-bin limit of EventGPT. We believe EventFlash serves as an efficient foundation model for event-based vision."
    },
    {
      "title": "From Frames to Sequences: Temporally Consistent Human-Centric Dense Prediction",
      "url": "https://arxiv.org/abs/2602.01661",
      "date": "2026-02-03",
      "authors_text": "Xingyu Miao, Junting Dong, Qin Zhao, Yuhang Yang, Junhao Chen",
      "is_highlight": false,
      "score": 72.0,
      "summary": "This paper presents a novel synthetic data pipeline and a ViT-based model for achieving temporally consistent human-centric dense prediction in video sequences, outperforming existing methods.",
      "teaser_image": "https://arxiv.org/html/2602.01661/x2.png",
      "debug_abstract": "In this work, we focus on the challenge of temporally consistent human-centric dense prediction across video sequences. Existing models achieve strong per-frame accuracy but often flicker under motion, occlusion, and lighting changes, and they rarely have paired human video supervision for multiple dense tasks. We address this gap with a scalable synthetic data pipeline that generates photorealistic human frames and motion-aligned sequences with pixel-accurate depth, normals, and masks. Unlike prior static data synthetic pipelines, our pipeline provides both frame-level labels for spatial learning and sequence-level supervision for temporal learning. Building on this, we train a unified ViT-based dense predictor that (i) injects an explicit human geometric prior via CSE embeddings and (ii) improves geometry-feature reliability with a lightweight channel reweighting module after feature fusion. Our two-stage training strategy, combining static pretraining with dynamic sequence supervision, enables the model first to acquire robust spatial representations and then refine temporal consistency across motion-aligned sequences. Extensive experiments show that we achieve state-of-the-art performance on THuman2.1 and Hi4D and generalize effectively to in-the-wild videos."
    },
    {
      "title": "Making Avatars Interact: Towards Text-Driven Human-Object Interaction for Controllable Talking Avatars",
      "url": "https://arxiv.org/abs/2602.01538",
      "date": "2026-02-02",
      "authors_text": "Youliang Zhang, Zhengguang Zhou, Zhentao Yu, Ziyao Huang, Teng Hu",
      "is_highlight": false,
      "score": 75.0,
      "summary": "The paper presents InteractAvatar, a dual-stream framework for generating text-driven human-object interactions in talking avatars, enhancing perception and control in video synthesis.",
      "teaser_image": "https://arxiv.org/html/2602.01538/x1.png",
      "debug_abstract": "Generating talking avatars is a fundamental task in video generation. Although existing methods can generate full-body talking avatars with simple human motion, extending this task to grounded human-object interaction (GHOI) remains an open challenge, requiring the avatar to perform text-aligned interactions with surrounding objects. This challenge stems from the need for environmental perception and the control-quality dilemma in GHOI generation. To address this, we propose a novel dual-stream framework, InteractAvatar, which decouples perception and planning from video synthesis for grounded human-object interaction. Leveraging detection to enhance environmental perception, we introduce a Perception and Interaction Module (PIM) to generate text-aligned interaction motions. Additionally, an Audio-Interaction Aware Generation Module (AIM) is proposed to synthesize vivid talking avatars performing object interactions. With a specially designed motion-to-video aligner, PIM and AIM share a similar network structure and enable parallel co-generation of motions and plausible videos, effectively mitigating the control-quality dilemma. Finally, we establish a benchmark, GroundedInter, for evaluating GHOI video generation. Extensive experiments and comparisons demonstrate the effectiveness of our method in generating grounded human-object interactions for talking avatars. Project page: this https URL"
    }
  ],
  "潘佳老师实验室": [
    {
      "title": "Search-R2: Enhancing Search-Integrated Reasoning via Actor-Refiner Collaboration",
      "url": "https://arxiv.org/abs/2602.03647",
      "date": "2026-02-03",
      "authors_text": "Bowei He, Minda Hu, Zenan Xu, Hongru Wang, Licheng Zong",
      "is_highlight": false,
      "score": 52.0,
      "summary": "Search-R2 introduces an Actor-Refiner framework that enhances search-integrated reasoning in language agents through targeted intervention and fine-grained supervision, outperforming existing methods.",
      "teaser_image": "https://arxiv.org/html/2602.03647/x2.png",
      "debug_abstract": "Search-integrated reasoning enables language agents to transcend static parametric knowledge by actively querying external sources. However, training these agents via reinforcement learning is hindered by the multi-scale credit assignment problem: existing methods typically rely on sparse, trajectory-level rewards that fail to distinguish between high-quality reasoning and fortuitous guesses, leading to redundant or misleading search behaviors. To address this, we propose Search-R2, a novel Actor-Refiner collaboration framework that enhances reasoning through targeted intervention, with both components jointly optimized during training. Our approach decomposes the generation process into an Actor, which produces initial reasoning trajectories, and a Meta-Refiner, which selectively diagnoses and repairs flawed steps via a &#39;cut-and-regenerate&#39; mechanism. To provide fine-grained supervision, we introduce a hybrid reward design that couples outcome correctness with a dense process reward quantifying the information density of retrieved evidence. Theoretically, we formalize the Actor-Refiner interaction as a smoothed mixture policy, proving that selective correction yields strict performance gains over strong baselines. Extensive experiments across various general and multi-hop QA datasets demonstrate that Search-R2 consistently outperforms strong RAG and RL-based baselines across model scales, achieving superior reasoning accuracy with minimal overhead."
    },
    {
      "title": "Large Language Models Can Take False First Steps at Inference-time Planning",
      "url": "https://arxiv.org/abs/2602.02991",
      "date": "2026-02-03",
      "authors_text": "Haijiang Yan, Jian-Qiao Zhu, Adam Sanborn",
      "is_highlight": false,
      "score": 73.0,
      "summary": "The paper explains that large language models exhibit inconsistent planning at inference due to evolving self-generated context, supported by experiments demonstrating planning shifts and reduced biases.",
      "teaser_image": "https://arxiv.org/html/2602.02991/images/llama_regression.png",
      "debug_abstract": "Large language models (LLMs) have been shown to acquire sequence-level planning abilities during training, yet their planning behavior exhibited at inference time often appears short-sighted and inconsistent with these capabilities. We propose a Bayesian account for this gap by grounding planning behavior in the evolving generative context: given the subtle differences between natural language and the language internalized by LLMs, accumulated self-generated context drives a planning-shift during inference and thereby creates the appearance of compromised planning behavior. We further validate the proposed model through two controlled experiments: a random-generation task demonstrating constrained planning under human prompts and increasing planning strength as self-generated context accumulates, and a Gaussian-sampling task showing reduced initial bias when conditioning on self-generated sequences. These findings provide a theoretical explanation along with empirical evidence for characterizing how LLMs plan ahead during inference."
    },
    {
      "title": "TIDE: Trajectory-based Diagnostic Evaluation of Test-Time Improvement in LLM Agents",
      "url": "https://arxiv.org/abs/2602.02196",
      "date": "2026-02-03",
      "authors_text": "Hang Yan, Xinyu Che, Fangzhi Xu, Qiushi Sun, Zichen Ding",
      "is_highlight": false,
      "score": 68.0,
      "summary": "The TIDE framework evaluates Test-Time Improvement in LLM agents by analyzing task completion dynamics, recursive behaviors, and memory constraints to optimize agent-environment interactions.",
      "teaser_image": "https://arxiv.org/html/2602.02196/x1.png",
      "debug_abstract": "Recent advances in autonomous LLM agents demonstrate their ability to improve performance through iterative interaction with the environment. We define this paradigm as Test-Time Improvement (TTI). However, the mechanisms under how and why TTI succeed or fail remain poorly understood, and existing evaluation metrics fail to capture their task optimization efficiency, behavior adaptation after erroneous actions, and the specific utility of working memory for task completion. To address these gaps, we propose Test-time Improvement Diagnostic Evaluation (TIDE), an agent-agnostic and environment-agnostic framework that decomposes TTI into three comprehensive and interconnected dimensions. The framework measures (1) the overall temporal dynamics of task completion and (2) identifies whether performance is primarily constrained by recursive looping behaviors or (3) by burdensome accumulated memory. Through extensive experiments across diverse agents and environments, TIDE highlights that improving agent performance requires more than scaling internal reasoning, calling for explicitly optimizing the interaction dynamics between the agent and the environment."
    },
    {
      "title": "Adaptive Rollout Allocation for Online Reinforcement Learning with Verifiable Rewards",
      "url": "https://arxiv.org/abs/2602.01601",
      "date": "2026-02-03",
      "authors_text": "Hieu Trung Nguyen, Bao Nguyen, Wenao Ma, Yuzhi Zhao, Ruifeng She",
      "is_highlight": false,
      "score": 70.0,
      "summary": "The paper presents VIP, a novel rollout allocation strategy that optimizes sampling efficiency in reinforcement learning by minimizing expected gradient variance using predictive modeling.",
      "teaser_image": "https://arxiv.org/html/2602.01601/x2.png",
      "debug_abstract": "Sampling efficiency is a key bottleneck in reinforcement learning with verifiable rewards. Existing group-based policy optimization methods, such as GRPO, allocate a fixed number of rollouts for all training prompts. This uniform allocation implicitly treats all prompts as equally informative, and could lead to inefficient computational budget usage and impede training progress. We introduce VIP, a Variance-Informed Predictive allocation strategy that allocates a given rollout budget to the prompts in the incumbent batch to minimize the expected gradient variance of the policy update. At each iteration, VIP uses a lightweight Gaussian process model to predict per-prompt success probabilities based on recent rollouts. These probability predictions are translated into variance estimates, which are then fed into a convex optimization problem to determine the optimal rollout allocations under a hard compute budget constraint. Empirical results show that VIP consistently improves sampling efficiency and achieves higher performance than uniform or heuristic allocation strategies in multiple benchmarks."
    }
  ],
  "Multimedia Lab (MMLab)": [
    {
      "title": "Search-R2: Enhancing Search-Integrated Reasoning via Actor-Refiner Collaboration",
      "url": "https://arxiv.org/abs/2602.03647",
      "date": "2026-02-03",
      "authors_text": "Bowei He, Minda Hu, Zenan Xu, Hongru Wang, Licheng Zong",
      "is_highlight": false,
      "score": 52.0,
      "summary": "Search-R2 introduces an Actor-Refiner framework that enhances search-integrated reasoning in language agents through targeted intervention and fine-grained supervision, outperforming existing methods.",
      "teaser_image": "https://arxiv.org/html/2602.03647/x2.png",
      "debug_abstract": "Search-integrated reasoning enables language agents to transcend static parametric knowledge by actively querying external sources. However, training these agents via reinforcement learning is hindered by the multi-scale credit assignment problem: existing methods typically rely on sparse, trajectory-level rewards that fail to distinguish between high-quality reasoning and fortuitous guesses, leading to redundant or misleading search behaviors. To address this, we propose Search-R2, a novel Actor-Refiner collaboration framework that enhances reasoning through targeted intervention, with both components jointly optimized during training. Our approach decomposes the generation process into an Actor, which produces initial reasoning trajectories, and a Meta-Refiner, which selectively diagnoses and repairs flawed steps via a &#39;cut-and-regenerate&#39; mechanism. To provide fine-grained supervision, we introduce a hybrid reward design that couples outcome correctness with a dense process reward quantifying the information density of retrieved evidence. Theoretically, we formalize the Actor-Refiner interaction as a smoothed mixture policy, proving that selective correction yields strict performance gains over strong baselines. Extensive experiments across various general and multi-hop QA datasets demonstrate that Search-R2 consistently outperforms strong RAG and RL-based baselines across model scales, achieving superior reasoning accuracy with minimal overhead."
    },
    {
      "title": "Adaptive Rollout Allocation for Online Reinforcement Learning with Verifiable Rewards",
      "url": "https://arxiv.org/abs/2602.01601",
      "date": "2026-02-03",
      "authors_text": "Hieu Trung Nguyen, Bao Nguyen, Wenao Ma, Yuzhi Zhao, Ruifeng She",
      "is_highlight": false,
      "score": 70.0,
      "summary": "The paper presents VIP, a novel rollout allocation strategy that optimizes sampling efficiency in reinforcement learning by minimizing expected gradient variance using predictive modeling.",
      "teaser_image": "https://arxiv.org/html/2602.01601/x2.png",
      "debug_abstract": "Sampling efficiency is a key bottleneck in reinforcement learning with verifiable rewards. Existing group-based policy optimization methods, such as GRPO, allocate a fixed number of rollouts for all training prompts. This uniform allocation implicitly treats all prompts as equally informative, and could lead to inefficient computational budget usage and impede training progress. We introduce VIP, a Variance-Informed Predictive allocation strategy that allocates a given rollout budget to the prompts in the incumbent batch to minimize the expected gradient variance of the policy update. At each iteration, VIP uses a lightweight Gaussian process model to predict per-prompt success probabilities based on recent rollouts. These probability predictions are translated into variance estimates, which are then fed into a convex optimization problem to determine the optimal rollout allocations under a hard compute budget constraint. Empirical results show that VIP consistently improves sampling efficiency and achieves higher performance than uniform or heuristic allocation strategies in multiple benchmarks."
    }
  ],
  "OpenDriveLab": [
    {
      "title": "Search-R2: Enhancing Search-Integrated Reasoning via Actor-Refiner Collaboration",
      "url": "https://arxiv.org/abs/2602.03647",
      "date": "2026-02-03",
      "authors_text": "Bowei He, Minda Hu, Zenan Xu, Hongru Wang, Licheng Zong",
      "is_highlight": false,
      "score": 52.0,
      "summary": "Search-R2 introduces an Actor-Refiner framework that enhances search-integrated reasoning in language agents through targeted intervention and fine-grained supervision, outperforming existing methods.",
      "teaser_image": "https://arxiv.org/html/2602.03647/x2.png",
      "debug_abstract": "Search-integrated reasoning enables language agents to transcend static parametric knowledge by actively querying external sources. However, training these agents via reinforcement learning is hindered by the multi-scale credit assignment problem: existing methods typically rely on sparse, trajectory-level rewards that fail to distinguish between high-quality reasoning and fortuitous guesses, leading to redundant or misleading search behaviors. To address this, we propose Search-R2, a novel Actor-Refiner collaboration framework that enhances reasoning through targeted intervention, with both components jointly optimized during training. Our approach decomposes the generation process into an Actor, which produces initial reasoning trajectories, and a Meta-Refiner, which selectively diagnoses and repairs flawed steps via a &#39;cut-and-regenerate&#39; mechanism. To provide fine-grained supervision, we introduce a hybrid reward design that couples outcome correctness with a dense process reward quantifying the information density of retrieved evidence. Theoretically, we formalize the Actor-Refiner interaction as a smoothed mixture policy, proving that selective correction yields strict performance gains over strong baselines. Extensive experiments across various general and multi-hop QA datasets demonstrate that Search-R2 consistently outperforms strong RAG and RL-based baselines across model scales, achieving superior reasoning accuracy with minimal overhead."
    },
    {
      "title": "Large Language Models Can Take False First Steps at Inference-time Planning",
      "url": "https://arxiv.org/abs/2602.02991",
      "date": "2026-02-03",
      "authors_text": "Haijiang Yan, Jian-Qiao Zhu, Adam Sanborn",
      "is_highlight": false,
      "score": 73.0,
      "summary": "The paper explains that large language models exhibit inconsistent planning at inference due to evolving self-generated context, supported by experiments demonstrating planning shifts and reduced biases.",
      "teaser_image": "https://arxiv.org/html/2602.02991/images/llama_regression.png",
      "debug_abstract": "Large language models (LLMs) have been shown to acquire sequence-level planning abilities during training, yet their planning behavior exhibited at inference time often appears short-sighted and inconsistent with these capabilities. We propose a Bayesian account for this gap by grounding planning behavior in the evolving generative context: given the subtle differences between natural language and the language internalized by LLMs, accumulated self-generated context drives a planning-shift during inference and thereby creates the appearance of compromised planning behavior. We further validate the proposed model through two controlled experiments: a random-generation task demonstrating constrained planning under human prompts and increasing planning strength as self-generated context accumulates, and a Gaussian-sampling task showing reduced initial bias when conditioning on self-generated sequences. These findings provide a theoretical explanation along with empirical evidence for characterizing how LLMs plan ahead during inference."
    },
    {
      "title": "TIDE: Trajectory-based Diagnostic Evaluation of Test-Time Improvement in LLM Agents",
      "url": "https://arxiv.org/abs/2602.02196",
      "date": "2026-02-03",
      "authors_text": "Hang Yan, Xinyu Che, Fangzhi Xu, Qiushi Sun, Zichen Ding",
      "is_highlight": false,
      "score": 68.0,
      "summary": "The TIDE framework evaluates Test-Time Improvement in LLM agents by analyzing task completion dynamics, recursive behaviors, and memory constraints to optimize agent-environment interactions.",
      "teaser_image": "https://arxiv.org/html/2602.02196/x1.png",
      "debug_abstract": "Recent advances in autonomous LLM agents demonstrate their ability to improve performance through iterative interaction with the environment. We define this paradigm as Test-Time Improvement (TTI). However, the mechanisms under how and why TTI succeed or fail remain poorly understood, and existing evaluation metrics fail to capture their task optimization efficiency, behavior adaptation after erroneous actions, and the specific utility of working memory for task completion. To address these gaps, we propose Test-time Improvement Diagnostic Evaluation (TIDE), an agent-agnostic and environment-agnostic framework that decomposes TTI into three comprehensive and interconnected dimensions. The framework measures (1) the overall temporal dynamics of task completion and (2) identifies whether performance is primarily constrained by recursive looping behaviors or (3) by burdensome accumulated memory. Through extensive experiments across diverse agents and environments, TIDE highlights that improving agent performance requires more than scaling internal reasoning, calling for explicitly optimizing the interaction dynamics between the agent and the environment."
    },
    {
      "title": "Adaptive Rollout Allocation for Online Reinforcement Learning with Verifiable Rewards",
      "url": "https://arxiv.org/abs/2602.01601",
      "date": "2026-02-03",
      "authors_text": "Hieu Trung Nguyen, Bao Nguyen, Wenao Ma, Yuzhi Zhao, Ruifeng She",
      "is_highlight": false,
      "score": 70.0,
      "summary": "The paper presents VIP, a novel rollout allocation strategy that optimizes sampling efficiency in reinforcement learning by minimizing expected gradient variance using predictive modeling.",
      "teaser_image": "https://arxiv.org/html/2602.01601/x2.png",
      "debug_abstract": "Sampling efficiency is a key bottleneck in reinforcement learning with verifiable rewards. Existing group-based policy optimization methods, such as GRPO, allocate a fixed number of rollouts for all training prompts. This uniform allocation implicitly treats all prompts as equally informative, and could lead to inefficient computational budget usage and impede training progress. We introduce VIP, a Variance-Informed Predictive allocation strategy that allocates a given rollout budget to the prompts in the incumbent batch to minimize the expected gradient variance of the policy update. At each iteration, VIP uses a lightweight Gaussian process model to predict per-prompt success probabilities based on recent rollouts. These probability predictions are translated into variance estimates, which are then fed into a convex optimization problem to determine the optimal rollout allocations under a hard compute budget constraint. Empirical results show that VIP consistently improves sampling efficiency and achieves higher performance than uniform or heuristic allocation strategies in multiple benchmarks."
    }
  ],
  "Language and Vision (LaVi) Lab": [
    {
      "title": "Search-R2: Enhancing Search-Integrated Reasoning via Actor-Refiner Collaboration",
      "url": "https://arxiv.org/abs/2602.03647",
      "date": "2026-02-03",
      "authors_text": "Bowei He, Minda Hu, Zenan Xu, Hongru Wang, Licheng Zong",
      "is_highlight": false,
      "score": 52.0,
      "summary": "Search-R2 introduces an Actor-Refiner framework that enhances search-integrated reasoning in language agents through targeted intervention and fine-grained supervision, outperforming existing methods.",
      "teaser_image": "https://arxiv.org/html/2602.03647/x2.png",
      "debug_abstract": "Search-integrated reasoning enables language agents to transcend static parametric knowledge by actively querying external sources. However, training these agents via reinforcement learning is hindered by the multi-scale credit assignment problem: existing methods typically rely on sparse, trajectory-level rewards that fail to distinguish between high-quality reasoning and fortuitous guesses, leading to redundant or misleading search behaviors. To address this, we propose Search-R2, a novel Actor-Refiner collaboration framework that enhances reasoning through targeted intervention, with both components jointly optimized during training. Our approach decomposes the generation process into an Actor, which produces initial reasoning trajectories, and a Meta-Refiner, which selectively diagnoses and repairs flawed steps via a &#39;cut-and-regenerate&#39; mechanism. To provide fine-grained supervision, we introduce a hybrid reward design that couples outcome correctness with a dense process reward quantifying the information density of retrieved evidence. Theoretically, we formalize the Actor-Refiner interaction as a smoothed mixture policy, proving that selective correction yields strict performance gains over strong baselines. Extensive experiments across various general and multi-hop QA datasets demonstrate that Search-R2 consistently outperforms strong RAG and RL-based baselines across model scales, achieving superior reasoning accuracy with minimal overhead."
    },
    {
      "title": "Large Language Models Can Take False First Steps at Inference-time Planning",
      "url": "https://arxiv.org/abs/2602.02991",
      "date": "2026-02-03",
      "authors_text": "Haijiang Yan, Jian-Qiao Zhu, Adam Sanborn",
      "is_highlight": false,
      "score": 73.0,
      "summary": "The paper explains that large language models exhibit inconsistent planning at inference due to evolving self-generated context, supported by experiments demonstrating planning shifts and reduced biases.",
      "teaser_image": "https://arxiv.org/html/2602.02991/images/llama_regression.png",
      "debug_abstract": "Large language models (LLMs) have been shown to acquire sequence-level planning abilities during training, yet their planning behavior exhibited at inference time often appears short-sighted and inconsistent with these capabilities. We propose a Bayesian account for this gap by grounding planning behavior in the evolving generative context: given the subtle differences between natural language and the language internalized by LLMs, accumulated self-generated context drives a planning-shift during inference and thereby creates the appearance of compromised planning behavior. We further validate the proposed model through two controlled experiments: a random-generation task demonstrating constrained planning under human prompts and increasing planning strength as self-generated context accumulates, and a Gaussian-sampling task showing reduced initial bias when conditioning on self-generated sequences. These findings provide a theoretical explanation along with empirical evidence for characterizing how LLMs plan ahead during inference."
    },
    {
      "title": "TIDE: Trajectory-based Diagnostic Evaluation of Test-Time Improvement in LLM Agents",
      "url": "https://arxiv.org/abs/2602.02196",
      "date": "2026-02-03",
      "authors_text": "Hang Yan, Xinyu Che, Fangzhi Xu, Qiushi Sun, Zichen Ding",
      "is_highlight": false,
      "score": 68.0,
      "summary": "The TIDE framework evaluates Test-Time Improvement in LLM agents by analyzing task completion dynamics, recursive behaviors, and memory constraints to optimize agent-environment interactions.",
      "teaser_image": "https://arxiv.org/html/2602.02196/x1.png",
      "debug_abstract": "Recent advances in autonomous LLM agents demonstrate their ability to improve performance through iterative interaction with the environment. We define this paradigm as Test-Time Improvement (TTI). However, the mechanisms under how and why TTI succeed or fail remain poorly understood, and existing evaluation metrics fail to capture their task optimization efficiency, behavior adaptation after erroneous actions, and the specific utility of working memory for task completion. To address these gaps, we propose Test-time Improvement Diagnostic Evaluation (TIDE), an agent-agnostic and environment-agnostic framework that decomposes TTI into three comprehensive and interconnected dimensions. The framework measures (1) the overall temporal dynamics of task completion and (2) identifies whether performance is primarily constrained by recursive looping behaviors or (3) by burdensome accumulated memory. Through extensive experiments across diverse agents and environments, TIDE highlights that improving agent performance requires more than scaling internal reasoning, calling for explicitly optimizing the interaction dynamics between the agent and the environment."
    },
    {
      "title": "Adaptive Rollout Allocation for Online Reinforcement Learning with Verifiable Rewards",
      "url": "https://arxiv.org/abs/2602.01601",
      "date": "2026-02-03",
      "authors_text": "Hieu Trung Nguyen, Bao Nguyen, Wenao Ma, Yuzhi Zhao, Ruifeng She",
      "is_highlight": false,
      "score": 70.0,
      "summary": "The paper presents VIP, a novel rollout allocation strategy that optimizes sampling efficiency in reinforcement learning by minimizing expected gradient variance using predictive modeling.",
      "teaser_image": "https://arxiv.org/html/2602.01601/x2.png",
      "debug_abstract": "Sampling efficiency is a key bottleneck in reinforcement learning with verifiable rewards. Existing group-based policy optimization methods, such as GRPO, allocate a fixed number of rollouts for all training prompts. This uniform allocation implicitly treats all prompts as equally informative, and could lead to inefficient computational budget usage and impede training progress. We introduce VIP, a Variance-Informed Predictive allocation strategy that allocates a given rollout budget to the prompts in the incumbent batch to minimize the expected gradient variance of the policy update. At each iteration, VIP uses a lightweight Gaussian process model to predict per-prompt success probabilities based on recent rollouts. These probability predictions are translated into variance estimates, which are then fed into a convex optimization problem to determine the optimal rollout allocations under a hard compute budget constraint. Empirical results show that VIP consistently improves sampling efficiency and achieves higher performance than uniform or heuristic allocation strategies in multiple benchmarks."
    }
  ],
  "深圳市人工智能与机器人研究院 (AIRS)": [
    {
      "title": "Search-R2: Enhancing Search-Integrated Reasoning via Actor-Refiner Collaboration",
      "url": "https://arxiv.org/abs/2602.03647",
      "date": "2026-02-03",
      "authors_text": "Bowei He, Minda Hu, Zenan Xu, Hongru Wang, Licheng Zong",
      "is_highlight": false,
      "score": 52.0,
      "summary": "Search-R2 introduces an Actor-Refiner framework that enhances search-integrated reasoning in language agents through targeted intervention and fine-grained supervision, outperforming existing methods.",
      "teaser_image": "https://arxiv.org/html/2602.03647/x2.png",
      "debug_abstract": "Search-integrated reasoning enables language agents to transcend static parametric knowledge by actively querying external sources. However, training these agents via reinforcement learning is hindered by the multi-scale credit assignment problem: existing methods typically rely on sparse, trajectory-level rewards that fail to distinguish between high-quality reasoning and fortuitous guesses, leading to redundant or misleading search behaviors. To address this, we propose Search-R2, a novel Actor-Refiner collaboration framework that enhances reasoning through targeted intervention, with both components jointly optimized during training. Our approach decomposes the generation process into an Actor, which produces initial reasoning trajectories, and a Meta-Refiner, which selectively diagnoses and repairs flawed steps via a &#39;cut-and-regenerate&#39; mechanism. To provide fine-grained supervision, we introduce a hybrid reward design that couples outcome correctness with a dense process reward quantifying the information density of retrieved evidence. Theoretically, we formalize the Actor-Refiner interaction as a smoothed mixture policy, proving that selective correction yields strict performance gains over strong baselines. Extensive experiments across various general and multi-hop QA datasets demonstrate that Search-R2 consistently outperforms strong RAG and RL-based baselines across model scales, achieving superior reasoning accuracy with minimal overhead."
    },
    {
      "title": "Adaptive Rollout Allocation for Online Reinforcement Learning with Verifiable Rewards",
      "url": "https://arxiv.org/abs/2602.01601",
      "date": "2026-02-03",
      "authors_text": "Hieu Trung Nguyen, Bao Nguyen, Wenao Ma, Yuzhi Zhao, Ruifeng She",
      "is_highlight": false,
      "score": 70.0,
      "summary": "The paper presents VIP, a novel rollout allocation strategy that optimizes sampling efficiency in reinforcement learning by minimizing expected gradient variance using predictive modeling.",
      "teaser_image": "https://arxiv.org/html/2602.01601/x2.png",
      "debug_abstract": "Sampling efficiency is a key bottleneck in reinforcement learning with verifiable rewards. Existing group-based policy optimization methods, such as GRPO, allocate a fixed number of rollouts for all training prompts. This uniform allocation implicitly treats all prompts as equally informative, and could lead to inefficient computational budget usage and impede training progress. We introduce VIP, a Variance-Informed Predictive allocation strategy that allocates a given rollout budget to the prompts in the incumbent batch to minimize the expected gradient variance of the policy update. At each iteration, VIP uses a lightweight Gaussian process model to predict per-prompt success probabilities based on recent rollouts. These probability predictions are translated into variance estimates, which are then fed into a convex optimization problem to determine the optimal rollout allocations under a hard compute budget constraint. Empirical results show that VIP consistently improves sampling efficiency and achieves higher performance than uniform or heuristic allocation strategies in multiple benchmarks."
    }
  ],
  "机器人与人工智能实验室": [
    {
      "title": "Search-R2: Enhancing Search-Integrated Reasoning via Actor-Refiner Collaboration",
      "url": "https://arxiv.org/abs/2602.03647",
      "date": "2026-02-03",
      "authors_text": "Bowei He, Minda Hu, Zenan Xu, Hongru Wang, Licheng Zong",
      "is_highlight": false,
      "score": 52.0,
      "summary": "Search-R2 introduces an Actor-Refiner framework that enhances search-integrated reasoning in language agents through targeted intervention and fine-grained supervision, outperforming existing methods.",
      "teaser_image": "https://arxiv.org/html/2602.03647/x2.png",
      "debug_abstract": "Search-integrated reasoning enables language agents to transcend static parametric knowledge by actively querying external sources. However, training these agents via reinforcement learning is hindered by the multi-scale credit assignment problem: existing methods typically rely on sparse, trajectory-level rewards that fail to distinguish between high-quality reasoning and fortuitous guesses, leading to redundant or misleading search behaviors. To address this, we propose Search-R2, a novel Actor-Refiner collaboration framework that enhances reasoning through targeted intervention, with both components jointly optimized during training. Our approach decomposes the generation process into an Actor, which produces initial reasoning trajectories, and a Meta-Refiner, which selectively diagnoses and repairs flawed steps via a &#39;cut-and-regenerate&#39; mechanism. To provide fine-grained supervision, we introduce a hybrid reward design that couples outcome correctness with a dense process reward quantifying the information density of retrieved evidence. Theoretically, we formalize the Actor-Refiner interaction as a smoothed mixture policy, proving that selective correction yields strict performance gains over strong baselines. Extensive experiments across various general and multi-hop QA datasets demonstrate that Search-R2 consistently outperforms strong RAG and RL-based baselines across model scales, achieving superior reasoning accuracy with minimal overhead."
    },
    {
      "title": "Adaptive Rollout Allocation for Online Reinforcement Learning with Verifiable Rewards",
      "url": "https://arxiv.org/abs/2602.01601",
      "date": "2026-02-03",
      "authors_text": "Hieu Trung Nguyen, Bao Nguyen, Wenao Ma, Yuzhi Zhao, Ruifeng She",
      "is_highlight": false,
      "score": 70.0,
      "summary": "The paper presents VIP, a novel rollout allocation strategy that optimizes sampling efficiency in reinforcement learning by minimizing expected gradient variance using predictive modeling.",
      "teaser_image": "https://arxiv.org/html/2602.01601/x2.png",
      "debug_abstract": "Sampling efficiency is a key bottleneck in reinforcement learning with verifiable rewards. Existing group-based policy optimization methods, such as GRPO, allocate a fixed number of rollouts for all training prompts. This uniform allocation implicitly treats all prompts as equally informative, and could lead to inefficient computational budget usage and impede training progress. We introduce VIP, a Variance-Informed Predictive allocation strategy that allocates a given rollout budget to the prompts in the incumbent batch to minimize the expected gradient variance of the policy update. At each iteration, VIP uses a lightweight Gaussian process model to predict per-prompt success probabilities based on recent rollouts. These probability predictions are translated into variance estimates, which are then fed into a convex optimization problem to determine the optimal rollout allocations under a hard compute budget constraint. Empirical results show that VIP consistently improves sampling efficiency and achieves higher performance than uniform or heuristic allocation strategies in multiple benchmarks."
    }
  ],
  "机器人与自动化研究中心": [
    {
      "title": "Search-R2: Enhancing Search-Integrated Reasoning via Actor-Refiner Collaboration",
      "url": "https://arxiv.org/abs/2602.03647",
      "date": "2026-02-03",
      "authors_text": "Bowei He, Minda Hu, Zenan Xu, Hongru Wang, Licheng Zong",
      "is_highlight": false,
      "score": 52.0,
      "summary": "Search-R2 introduces an Actor-Refiner framework that enhances search-integrated reasoning in language agents through targeted intervention and fine-grained supervision, outperforming existing methods.",
      "teaser_image": "https://arxiv.org/html/2602.03647/x2.png",
      "debug_abstract": "Search-integrated reasoning enables language agents to transcend static parametric knowledge by actively querying external sources. However, training these agents via reinforcement learning is hindered by the multi-scale credit assignment problem: existing methods typically rely on sparse, trajectory-level rewards that fail to distinguish between high-quality reasoning and fortuitous guesses, leading to redundant or misleading search behaviors. To address this, we propose Search-R2, a novel Actor-Refiner collaboration framework that enhances reasoning through targeted intervention, with both components jointly optimized during training. Our approach decomposes the generation process into an Actor, which produces initial reasoning trajectories, and a Meta-Refiner, which selectively diagnoses and repairs flawed steps via a &#39;cut-and-regenerate&#39; mechanism. To provide fine-grained supervision, we introduce a hybrid reward design that couples outcome correctness with a dense process reward quantifying the information density of retrieved evidence. Theoretically, we formalize the Actor-Refiner interaction as a smoothed mixture policy, proving that selective correction yields strict performance gains over strong baselines. Extensive experiments across various general and multi-hop QA datasets demonstrate that Search-R2 consistently outperforms strong RAG and RL-based baselines across model scales, achieving superior reasoning accuracy with minimal overhead."
    },
    {
      "title": "Adaptive Rollout Allocation for Online Reinforcement Learning with Verifiable Rewards",
      "url": "https://arxiv.org/abs/2602.01601",
      "date": "2026-02-03",
      "authors_text": "Hieu Trung Nguyen, Bao Nguyen, Wenao Ma, Yuzhi Zhao, Ruifeng She",
      "is_highlight": false,
      "score": 70.0,
      "summary": "The paper presents VIP, a novel rollout allocation strategy that optimizes sampling efficiency in reinforcement learning by minimizing expected gradient variance using predictive modeling.",
      "teaser_image": "https://arxiv.org/html/2602.01601/x2.png",
      "debug_abstract": "Sampling efficiency is a key bottleneck in reinforcement learning with verifiable rewards. Existing group-based policy optimization methods, such as GRPO, allocate a fixed number of rollouts for all training prompts. This uniform allocation implicitly treats all prompts as equally informative, and could lead to inefficient computational budget usage and impede training progress. We introduce VIP, a Variance-Informed Predictive allocation strategy that allocates a given rollout budget to the prompts in the incumbent batch to minimize the expected gradient variance of the policy update. At each iteration, VIP uses a lightweight Gaussian process model to predict per-prompt success probabilities based on recent rollouts. These probability predictions are translated into variance estimates, which are then fed into a convex optimization problem to determine the optimal rollout allocations under a hard compute budget constraint. Empirical results show that VIP consistently improves sampling efficiency and achieves higher performance than uniform or heuristic allocation strategies in multiple benchmarks."
    }
  ],
  "香港大学机械工程系机器人实验室": [
    {
      "title": "Search-R2: Enhancing Search-Integrated Reasoning via Actor-Refiner Collaboration",
      "url": "https://arxiv.org/abs/2602.03647",
      "date": "2026-02-03",
      "authors_text": "Bowei He, Minda Hu, Zenan Xu, Hongru Wang, Licheng Zong",
      "is_highlight": false,
      "score": 52.0,
      "summary": "Search-R2 introduces an Actor-Refiner framework that enhances search-integrated reasoning in language agents through targeted intervention and fine-grained supervision, outperforming existing methods.",
      "teaser_image": "https://arxiv.org/html/2602.03647/x2.png",
      "debug_abstract": "Search-integrated reasoning enables language agents to transcend static parametric knowledge by actively querying external sources. However, training these agents via reinforcement learning is hindered by the multi-scale credit assignment problem: existing methods typically rely on sparse, trajectory-level rewards that fail to distinguish between high-quality reasoning and fortuitous guesses, leading to redundant or misleading search behaviors. To address this, we propose Search-R2, a novel Actor-Refiner collaboration framework that enhances reasoning through targeted intervention, with both components jointly optimized during training. Our approach decomposes the generation process into an Actor, which produces initial reasoning trajectories, and a Meta-Refiner, which selectively diagnoses and repairs flawed steps via a &#39;cut-and-regenerate&#39; mechanism. To provide fine-grained supervision, we introduce a hybrid reward design that couples outcome correctness with a dense process reward quantifying the information density of retrieved evidence. Theoretically, we formalize the Actor-Refiner interaction as a smoothed mixture policy, proving that selective correction yields strict performance gains over strong baselines. Extensive experiments across various general and multi-hop QA datasets demonstrate that Search-R2 consistently outperforms strong RAG and RL-based baselines across model scales, achieving superior reasoning accuracy with minimal overhead."
    },
    {
      "title": "Large Language Models Can Take False First Steps at Inference-time Planning",
      "url": "https://arxiv.org/abs/2602.02991",
      "date": "2026-02-03",
      "authors_text": "Haijiang Yan, Jian-Qiao Zhu, Adam Sanborn",
      "is_highlight": false,
      "score": 73.0,
      "summary": "The paper explains that large language models exhibit inconsistent planning at inference due to evolving self-generated context, supported by experiments demonstrating planning shifts and reduced biases.",
      "teaser_image": "https://arxiv.org/html/2602.02991/images/llama_regression.png",
      "debug_abstract": "Large language models (LLMs) have been shown to acquire sequence-level planning abilities during training, yet their planning behavior exhibited at inference time often appears short-sighted and inconsistent with these capabilities. We propose a Bayesian account for this gap by grounding planning behavior in the evolving generative context: given the subtle differences between natural language and the language internalized by LLMs, accumulated self-generated context drives a planning-shift during inference and thereby creates the appearance of compromised planning behavior. We further validate the proposed model through two controlled experiments: a random-generation task demonstrating constrained planning under human prompts and increasing planning strength as self-generated context accumulates, and a Gaussian-sampling task showing reduced initial bias when conditioning on self-generated sequences. These findings provide a theoretical explanation along with empirical evidence for characterizing how LLMs plan ahead during inference."
    },
    {
      "title": "TIDE: Trajectory-based Diagnostic Evaluation of Test-Time Improvement in LLM Agents",
      "url": "https://arxiv.org/abs/2602.02196",
      "date": "2026-02-03",
      "authors_text": "Hang Yan, Xinyu Che, Fangzhi Xu, Qiushi Sun, Zichen Ding",
      "is_highlight": false,
      "score": 68.0,
      "summary": "The TIDE framework evaluates Test-Time Improvement in LLM agents by analyzing task completion dynamics, recursive behaviors, and memory constraints to optimize agent-environment interactions.",
      "teaser_image": "https://arxiv.org/html/2602.02196/x1.png",
      "debug_abstract": "Recent advances in autonomous LLM agents demonstrate their ability to improve performance through iterative interaction with the environment. We define this paradigm as Test-Time Improvement (TTI). However, the mechanisms under how and why TTI succeed or fail remain poorly understood, and existing evaluation metrics fail to capture their task optimization efficiency, behavior adaptation after erroneous actions, and the specific utility of working memory for task completion. To address these gaps, we propose Test-time Improvement Diagnostic Evaluation (TIDE), an agent-agnostic and environment-agnostic framework that decomposes TTI into three comprehensive and interconnected dimensions. The framework measures (1) the overall temporal dynamics of task completion and (2) identifies whether performance is primarily constrained by recursive looping behaviors or (3) by burdensome accumulated memory. Through extensive experiments across diverse agents and environments, TIDE highlights that improving agent performance requires more than scaling internal reasoning, calling for explicitly optimizing the interaction dynamics between the agent and the environment."
    },
    {
      "title": "Adaptive Rollout Allocation for Online Reinforcement Learning with Verifiable Rewards",
      "url": "https://arxiv.org/abs/2602.01601",
      "date": "2026-02-03",
      "authors_text": "Hieu Trung Nguyen, Bao Nguyen, Wenao Ma, Yuzhi Zhao, Ruifeng She",
      "is_highlight": false,
      "score": 70.0,
      "summary": "The paper presents VIP, a novel rollout allocation strategy that optimizes sampling efficiency in reinforcement learning by minimizing expected gradient variance using predictive modeling.",
      "teaser_image": "https://arxiv.org/html/2602.01601/x2.png",
      "debug_abstract": "Sampling efficiency is a key bottleneck in reinforcement learning with verifiable rewards. Existing group-based policy optimization methods, such as GRPO, allocate a fixed number of rollouts for all training prompts. This uniform allocation implicitly treats all prompts as equally informative, and could lead to inefficient computational budget usage and impede training progress. We introduce VIP, a Variance-Informed Predictive allocation strategy that allocates a given rollout budget to the prompts in the incumbent batch to minimize the expected gradient variance of the policy update. At each iteration, VIP uses a lightweight Gaussian process model to predict per-prompt success probabilities based on recent rollouts. These probability predictions are translated into variance estimates, which are then fed into a convex optimization problem to determine the optimal rollout allocations under a hard compute budget constraint. Empirical results show that VIP consistently improves sampling efficiency and achieves higher performance than uniform or heuristic allocation strategies in multiple benchmarks."
    }
  ],
  "Hengshuang Zhao老师实验室": [
    {
      "title": "Search-R2: Enhancing Search-Integrated Reasoning via Actor-Refiner Collaboration",
      "url": "https://arxiv.org/abs/2602.03647",
      "date": "2026-02-03",
      "authors_text": "Bowei He, Minda Hu, Zenan Xu, Hongru Wang, Licheng Zong",
      "is_highlight": false,
      "score": 52.0,
      "summary": "Search-R2 introduces an Actor-Refiner framework that enhances search-integrated reasoning in language agents through targeted intervention and fine-grained supervision, outperforming existing methods.",
      "teaser_image": "https://arxiv.org/html/2602.03647/x2.png",
      "debug_abstract": "Search-integrated reasoning enables language agents to transcend static parametric knowledge by actively querying external sources. However, training these agents via reinforcement learning is hindered by the multi-scale credit assignment problem: existing methods typically rely on sparse, trajectory-level rewards that fail to distinguish between high-quality reasoning and fortuitous guesses, leading to redundant or misleading search behaviors. To address this, we propose Search-R2, a novel Actor-Refiner collaboration framework that enhances reasoning through targeted intervention, with both components jointly optimized during training. Our approach decomposes the generation process into an Actor, which produces initial reasoning trajectories, and a Meta-Refiner, which selectively diagnoses and repairs flawed steps via a &#39;cut-and-regenerate&#39; mechanism. To provide fine-grained supervision, we introduce a hybrid reward design that couples outcome correctness with a dense process reward quantifying the information density of retrieved evidence. Theoretically, we formalize the Actor-Refiner interaction as a smoothed mixture policy, proving that selective correction yields strict performance gains over strong baselines. Extensive experiments across various general and multi-hop QA datasets demonstrate that Search-R2 consistently outperforms strong RAG and RL-based baselines across model scales, achieving superior reasoning accuracy with minimal overhead."
    },
    {
      "title": "Large Language Models Can Take False First Steps at Inference-time Planning",
      "url": "https://arxiv.org/abs/2602.02991",
      "date": "2026-02-03",
      "authors_text": "Haijiang Yan, Jian-Qiao Zhu, Adam Sanborn",
      "is_highlight": false,
      "score": 73.0,
      "summary": "The paper explains that large language models exhibit inconsistent planning at inference due to evolving self-generated context, supported by experiments demonstrating planning shifts and reduced biases.",
      "teaser_image": "https://arxiv.org/html/2602.02991/images/llama_regression.png",
      "debug_abstract": "Large language models (LLMs) have been shown to acquire sequence-level planning abilities during training, yet their planning behavior exhibited at inference time often appears short-sighted and inconsistent with these capabilities. We propose a Bayesian account for this gap by grounding planning behavior in the evolving generative context: given the subtle differences between natural language and the language internalized by LLMs, accumulated self-generated context drives a planning-shift during inference and thereby creates the appearance of compromised planning behavior. We further validate the proposed model through two controlled experiments: a random-generation task demonstrating constrained planning under human prompts and increasing planning strength as self-generated context accumulates, and a Gaussian-sampling task showing reduced initial bias when conditioning on self-generated sequences. These findings provide a theoretical explanation along with empirical evidence for characterizing how LLMs plan ahead during inference."
    },
    {
      "title": "TIDE: Trajectory-based Diagnostic Evaluation of Test-Time Improvement in LLM Agents",
      "url": "https://arxiv.org/abs/2602.02196",
      "date": "2026-02-03",
      "authors_text": "Hang Yan, Xinyu Che, Fangzhi Xu, Qiushi Sun, Zichen Ding",
      "is_highlight": false,
      "score": 68.0,
      "summary": "The TIDE framework evaluates Test-Time Improvement in LLM agents by analyzing task completion dynamics, recursive behaviors, and memory constraints to optimize agent-environment interactions.",
      "teaser_image": "https://arxiv.org/html/2602.02196/x1.png",
      "debug_abstract": "Recent advances in autonomous LLM agents demonstrate their ability to improve performance through iterative interaction with the environment. We define this paradigm as Test-Time Improvement (TTI). However, the mechanisms under how and why TTI succeed or fail remain poorly understood, and existing evaluation metrics fail to capture their task optimization efficiency, behavior adaptation after erroneous actions, and the specific utility of working memory for task completion. To address these gaps, we propose Test-time Improvement Diagnostic Evaluation (TIDE), an agent-agnostic and environment-agnostic framework that decomposes TTI into three comprehensive and interconnected dimensions. The framework measures (1) the overall temporal dynamics of task completion and (2) identifies whether performance is primarily constrained by recursive looping behaviors or (3) by burdensome accumulated memory. Through extensive experiments across diverse agents and environments, TIDE highlights that improving agent performance requires more than scaling internal reasoning, calling for explicitly optimizing the interaction dynamics between the agent and the environment."
    },
    {
      "title": "Adaptive Rollout Allocation for Online Reinforcement Learning with Verifiable Rewards",
      "url": "https://arxiv.org/abs/2602.01601",
      "date": "2026-02-03",
      "authors_text": "Hieu Trung Nguyen, Bao Nguyen, Wenao Ma, Yuzhi Zhao, Ruifeng She",
      "is_highlight": false,
      "score": 70.0,
      "summary": "The paper presents VIP, a novel rollout allocation strategy that optimizes sampling efficiency in reinforcement learning by minimizing expected gradient variance using predictive modeling.",
      "teaser_image": "https://arxiv.org/html/2602.01601/x2.png",
      "debug_abstract": "Sampling efficiency is a key bottleneck in reinforcement learning with verifiable rewards. Existing group-based policy optimization methods, such as GRPO, allocate a fixed number of rollouts for all training prompts. This uniform allocation implicitly treats all prompts as equally informative, and could lead to inefficient computational budget usage and impede training progress. We introduce VIP, a Variance-Informed Predictive allocation strategy that allocates a given rollout budget to the prompts in the incumbent batch to minimize the expected gradient variance of the policy update. At each iteration, VIP uses a lightweight Gaussian process model to predict per-prompt success probabilities based on recent rollouts. These probability predictions are translated into variance estimates, which are then fed into a convex optimization problem to determine the optimal rollout allocations under a hard compute budget constraint. Empirical results show that VIP consistently improves sampling efficiency and achieves higher performance than uniform or heuristic allocation strategies in multiple benchmarks."
    }
  ],
  "机器人与人工智能实验室 (RAIL)": [
    {
      "title": "Search-R2: Enhancing Search-Integrated Reasoning via Actor-Refiner Collaboration",
      "url": "https://arxiv.org/abs/2602.03647",
      "date": "2026-02-03",
      "authors_text": "Bowei He, Minda Hu, Zenan Xu, Hongru Wang, Licheng Zong",
      "is_highlight": false,
      "score": 52.0,
      "summary": "Search-R2 introduces an Actor-Refiner framework that enhances search-integrated reasoning in language agents through targeted intervention and fine-grained supervision, outperforming existing methods.",
      "teaser_image": "https://arxiv.org/html/2602.03647/x2.png",
      "debug_abstract": "Search-integrated reasoning enables language agents to transcend static parametric knowledge by actively querying external sources. However, training these agents via reinforcement learning is hindered by the multi-scale credit assignment problem: existing methods typically rely on sparse, trajectory-level rewards that fail to distinguish between high-quality reasoning and fortuitous guesses, leading to redundant or misleading search behaviors. To address this, we propose Search-R2, a novel Actor-Refiner collaboration framework that enhances reasoning through targeted intervention, with both components jointly optimized during training. Our approach decomposes the generation process into an Actor, which produces initial reasoning trajectories, and a Meta-Refiner, which selectively diagnoses and repairs flawed steps via a &#39;cut-and-regenerate&#39; mechanism. To provide fine-grained supervision, we introduce a hybrid reward design that couples outcome correctness with a dense process reward quantifying the information density of retrieved evidence. Theoretically, we formalize the Actor-Refiner interaction as a smoothed mixture policy, proving that selective correction yields strict performance gains over strong baselines. Extensive experiments across various general and multi-hop QA datasets demonstrate that Search-R2 consistently outperforms strong RAG and RL-based baselines across model scales, achieving superior reasoning accuracy with minimal overhead."
    },
    {
      "title": "Adaptive Rollout Allocation for Online Reinforcement Learning with Verifiable Rewards",
      "url": "https://arxiv.org/abs/2602.01601",
      "date": "2026-02-03",
      "authors_text": "Hieu Trung Nguyen, Bao Nguyen, Wenao Ma, Yuzhi Zhao, Ruifeng She",
      "is_highlight": false,
      "score": 70.0,
      "summary": "The paper presents VIP, a novel rollout allocation strategy that optimizes sampling efficiency in reinforcement learning by minimizing expected gradient variance using predictive modeling.",
      "teaser_image": "https://arxiv.org/html/2602.01601/x2.png",
      "debug_abstract": "Sampling efficiency is a key bottleneck in reinforcement learning with verifiable rewards. Existing group-based policy optimization methods, such as GRPO, allocate a fixed number of rollouts for all training prompts. This uniform allocation implicitly treats all prompts as equally informative, and could lead to inefficient computational budget usage and impede training progress. We introduce VIP, a Variance-Informed Predictive allocation strategy that allocates a given rollout budget to the prompts in the incumbent batch to minimize the expected gradient variance of the policy update. At each iteration, VIP uses a lightweight Gaussian process model to predict per-prompt success probabilities based on recent rollouts. These probability predictions are translated into variance estimates, which are then fed into a convex optimization problem to determine the optimal rollout allocations under a hard compute budget constraint. Empirical results show that VIP consistently improves sampling efficiency and achieves higher performance than uniform or heuristic allocation strategies in multiple benchmarks."
    }
  ],
  "数字媒体与计算机视觉实验室": [
    {
      "title": "QVLA: Not All Channels Are Equal in Vision-Language-Action Model's Quantization",
      "url": "https://arxiv.org/abs/2602.03782",
      "date": "2026-02-03",
      "authors_text": "Yuhao Xu, Yantai Yang, Zhenyang Fan, Yufan Liu, Yuming Li",
      "is_highlight": false,
      "score": 47.0,
      "summary": "QVLA introduces an action-centric quantization framework for Vision-Language-Action models, optimizing channel-wise bit allocation to enhance performance and reduce resource demands in robotics.",
      "teaser_image": "https://arxiv.org/html/2602.03782/x2.png",
      "debug_abstract": "The advent of Vision-Language-Action (VLA) models represents a significant leap for embodied intelligence, yet their immense computational demands critically hinder deployment on resource-constrained robotic platforms. Intuitively, low-bit quantization is a prevalent and preferred technique for large-scale model compression. However, we find that a systematic analysis of VLA model&#39;s quantization is fundamentally lacking. We argue that naively applying uniform-bit quantization from Large Language Models (LLMs) to robotics is flawed, as these methods prioritize passive data fidelity while ignoring how minor action deviations compound into catastrophic task failures. To bridge this gap, we introduce QVLA, the first action-centric quantization framework specifically designed for embodied control. In a sharp departure from the rigid, uniform-bit quantization of LLM-based methods, QVLA introduces a highly granular, channel-wise bit allocation strategy. Its core mechanism is to directly measure the final action-space sensitivity when quantizing each individual channel to various bit-widths. This process yields a precise, per-channel importance metric that guides a global optimization, which elegantly unifies quantization and pruning (0-bit) into a single, cohesive framework. Extensive evaluations on different baselines demonstrate the superiority of our approach. In the LIBERO, the quantization version of OpenVLA-OFT with our method requires only 29.2% of the original model&#39;s VRAM while maintaining 98.9% of its original performance and achieving a 1.49x speedup. This translates to a 22.6% performance improvement over the LLM-derived method SmoothQuant. Our work establishes a new, principled foundation for compressing VLA models in robotics, paving the way for deploying powerful, large-scale models on real-world hardware. Code will be released."
    },
    {
      "title": "Socratic-Geo: Synthetic Data Generation and Geometric Reasoning via Multi-Agent Interaction",
      "url": "https://arxiv.org/abs/2602.03414",
      "date": "2026-02-03",
      "authors_text": "Zhengbo Jiao, Shaobo Wang, Zifan Zhang, Wei Wang, Bing Zhao",
      "is_highlight": false,
      "score": 58.0,
      "summary": "Socratic-Geo introduces a multi-agent framework for autonomous data synthesis and geometric reasoning, significantly improving model performance with minimal seed problems and surpassing existing benchmarks.",
      "teaser_image": "https://arxiv.org/html/2602.03414/x1.png",
      "debug_abstract": "Multimodal Large Language Models (MLLMs) have significantly advanced vision-language understanding. However, even state-of-the-art models struggle with geometric reasoning, revealing a critical bottleneck: the extreme scarcity of high-quality image-text pairs. Human annotation is prohibitively expensive, while automated methods fail to ensure fidelity and training effectiveness. Existing approaches either passively adapt to available images or employ inefficient random exploration with filtering, decoupling generation from learning needs. We propose Socratic-Geo, a fully autonomous framework that dynamically couples data synthesis with model learning through multi-agent interaction. The Teacher agent generates parameterized Python scripts with reflective feedback (Reflect for solvability, RePI for visual validity), ensuring image-text pair purity. The Solver agent optimizes reasoning through preference learning, with failure paths guiding Teacher&#39;s targeted augmentation. Independently, the Generator learns image generation capabilities on accumulated &#34;image-code-instruction&#34; triplets, distilling programmatic drawing intelligence into visual generation. Starting from only 108 seed problems, Socratic-Solver achieves 49.11 on six benchmarks using one-quarter of baseline data, surpassing strong baselines by 2.43 points. Socratic-Generator achieves 42.4% on GenExam, establishing new state-of-the-art for open-source models, surpassing Seedream-4.0 (39.8%) and approaching Gemini-2.5-Flash-Image (43.1%)."
    },
    {
      "title": "Agentic Proposing: Enhancing Large Language Model Reasoning via Compositional Skill Synthesis",
      "url": "https://arxiv.org/abs/2602.03279",
      "date": "2026-02-03",
      "authors_text": "Zhengbo Jiao, Shaobo Wang, Zifan Zhang, Xuan Ren, Wei Wang",
      "is_highlight": false,
      "score": 71.0,
      "summary": "The paper introduces Agentic Proposing, a framework for generating high-quality synthetic training data that enhances reasoning in large language models, achieving superior performance with minimal human input.",
      "teaser_image": "https://arxiv.org/html/2602.03279/x2.png",
      "debug_abstract": "Advancing complex reasoning in large language models relies on high-quality, verifiable datasets, yet human annotation remains cost-prohibitive and difficult to scale. Current synthesis paradigms often face a recurring trade-off: maintaining structural validity typically restricts problem complexity, while relaxing constraints to increase difficulty frequently leads to inconsistent or unsolvable instances. To address this, we propose Agentic Proposing, a framework that models problem synthesis as a goal-driven sequential decision process where a specialized agent dynamically selects and composes modular reasoning skills. Through an iterative workflow of internal reflection and tool-use, we develop the Agentic-Proposer-4B using Multi-Granularity Policy Optimization (MGPO) to generate high-precision, verifiable training trajectories across mathematics, coding, and science. Empirical results demonstrate that downstream solvers trained on agent-synthesized data significantly outperform leading baselines and exhibit robust cross-domain generalization. Notably, a 30B solver trained on only 11,000 synthesized trajectories achieves a state-of-the-art 91.6% accuracy on AIME25, rivaling frontier-scale proprietary models such as GPT-5 and proving that a small volume of high-quality synthetic signals can effectively substitute for massive human-curated datasets."
    },
    {
      "title": "Bongards at the Boundary of Perception and Reasoning: Programs or Language?",
      "url": "https://arxiv.org/abs/2602.03038",
      "date": "2026-02-03",
      "authors_text": "Cassidy Langenfeld, Claas Beger, Gloria Geng, Wasu Top Piriyakulkij, Keya Hu",
      "is_highlight": false,
      "score": 60.0,
      "summary": "This paper introduces a neurosymbolic method utilizing LLMs and Bayesian optimization to tackle Bongard problems, enhancing visual reasoning beyond conventional VLM capabilities.",
      "teaser_image": "https://arxiv.org/html/2602.03038/x3.png",
      "debug_abstract": "Vision-Language Models (VLMs) have made great strides in everyday visual tasks, such as captioning a natural image, or answering commonsense questions about such images. But humans possess the puzzling ability to deploy their visual reasoning abilities in radically new situations, a skill rigorously tested by the classic set of visual reasoning challenges known as the Bongard problems. We present a neurosymbolic approach to solving these problems: given a hypothesized solution rule for a Bongard problem, we leverage LLMs to generate parameterized programmatic representations for the rule and perform parameter fitting using Bayesian optimization. We evaluate our method on classifying Bongard problem images given the ground truth rule, as well as on solving the problems from scratch."
    },
    {
      "title": "Embodiment-Aware Generalist Specialist Distillation for Unified Humanoid Whole-Body Control",
      "url": "https://arxiv.org/abs/2602.02960",
      "date": "2026-02-03",
      "authors_text": "Quanquan Peng, Yunfeng Lin, Yufei Xue, Jiangmiao Pang, Weinan Zhang",
      "is_highlight": false,
      "score": 90.0,
      "summary": "The EAGLE framework enables a unified policy for diverse humanoid robots, enhancing whole-body control through iterative generalist-specialist distillation without individual reward tuning.",
      "teaser_image": "https://arxiv.org/html/2602.02960/x1.png",
      "debug_abstract": "Humanoid Whole-Body Controllers trained with reinforcement learning (RL) have recently achieved remarkable performance, yet many target a single robot embodiment. Variations in dynamics, degrees of freedom (DoFs), and kinematic topology still hinder a single policy from commanding diverse humanoids. Moreover, obtaining a generalist policy that not only transfers across embodiments but also supports richer behaviors-beyond simple walking to squatting, leaning-remains especially challenging. In this work, we tackle these obstacles by introducing EAGLE, an iterative generalist-specialist distillation framework that produces a single unified policy that controls multiple heterogeneous humanoids without per-robot reward tuning. During each cycle, embodiment-specific specialists are forked from the current generalist, refined on their respective robots, and new skills are distilled back into the generalist by training on the pooled embodiment set. Repeating this loop until performance convergence produces a robust Whole-Body Controller validated on robots such as Unitree H1, G1, and Fourier N1. We conducted experiments on five different robots in simulation and four in real-world settings. Through quantitative evaluations, EAGLE achieves high tracking accuracy and robustness compared to other methods, marking a step toward scalable, fleet-level humanoid control. See more details at this https URL"
    },
    {
      "title": "SurfSplat: Conquering Feedforward 2D Gaussian Splatting with Surface Continuity Priors",
      "url": "https://arxiv.org/abs/2602.02000",
      "date": "2026-02-03",
      "authors_text": "Bing He, Jingnan Gao, Yunuo Chen, Ning Cao, Gang Chen",
      "is_highlight": false,
      "score": 65.0,
      "summary": "SurfSplat introduces a feedforward 2D Gaussian Splatting framework that enhances 3D scene reconstruction by ensuring surface continuity and high geometric precision, outperforming existing methods.",
      "teaser_image": "https://arxiv.org/html/2602.02000/x2.png",
      "debug_abstract": "Reconstructing 3D scenes from sparse images remains a challenging task due to the difficulty of recovering accurate geometry and texture without optimization. Recent approaches leverage generalizable models to generate 3D scenes using 3D Gaussian Splatting (3DGS) primitive. However, they often fail to produce continuous surfaces and instead yield discrete, color-biased point clouds that appear plausible at normal resolution but reveal severe artifacts under close-up views. To address this issue, we present SurfSplat, a feedforward framework based on 2D Gaussian Splatting (2DGS) primitive, which provides stronger anisotropy and higher geometric precision. By incorporating a surface continuity prior and a forced alpha blending strategy, SurfSplat reconstructs coherent geometry together with faithful textures. Furthermore, we introduce High-Resolution Rendering Consistency (HRRC), a new evaluation metric designed to evaluate high-resolution reconstruction quality. Extensive experiments on RealEstate10K, DL3DV, and ScanNet demonstrate that SurfSplat consistently outperforms prior methods on both standard metrics and HRRC, establishing a robust solution for high-fidelity 3D reconstruction from sparse inputs. Project page: this https URL"
    }
  ],
  "机器视觉与智能学习实验室 (MVIG)": [
    {
      "title": "QVLA: Not All Channels Are Equal in Vision-Language-Action Model's Quantization",
      "url": "https://arxiv.org/abs/2602.03782",
      "date": "2026-02-03",
      "authors_text": "Yuhao Xu, Yantai Yang, Zhenyang Fan, Yufan Liu, Yuming Li",
      "is_highlight": false,
      "score": 47.0,
      "summary": "QVLA introduces an action-centric quantization framework for Vision-Language-Action models, optimizing channel-wise bit allocation to enhance performance and reduce resource demands in robotics.",
      "teaser_image": "https://arxiv.org/html/2602.03782/x2.png",
      "debug_abstract": "The advent of Vision-Language-Action (VLA) models represents a significant leap for embodied intelligence, yet their immense computational demands critically hinder deployment on resource-constrained robotic platforms. Intuitively, low-bit quantization is a prevalent and preferred technique for large-scale model compression. However, we find that a systematic analysis of VLA model&#39;s quantization is fundamentally lacking. We argue that naively applying uniform-bit quantization from Large Language Models (LLMs) to robotics is flawed, as these methods prioritize passive data fidelity while ignoring how minor action deviations compound into catastrophic task failures. To bridge this gap, we introduce QVLA, the first action-centric quantization framework specifically designed for embodied control. In a sharp departure from the rigid, uniform-bit quantization of LLM-based methods, QVLA introduces a highly granular, channel-wise bit allocation strategy. Its core mechanism is to directly measure the final action-space sensitivity when quantizing each individual channel to various bit-widths. This process yields a precise, per-channel importance metric that guides a global optimization, which elegantly unifies quantization and pruning (0-bit) into a single, cohesive framework. Extensive evaluations on different baselines demonstrate the superiority of our approach. In the LIBERO, the quantization version of OpenVLA-OFT with our method requires only 29.2% of the original model&#39;s VRAM while maintaining 98.9% of its original performance and achieving a 1.49x speedup. This translates to a 22.6% performance improvement over the LLM-derived method SmoothQuant. Our work establishes a new, principled foundation for compressing VLA models in robotics, paving the way for deploying powerful, large-scale models on real-world hardware. Code will be released."
    },
    {
      "title": "Socratic-Geo: Synthetic Data Generation and Geometric Reasoning via Multi-Agent Interaction",
      "url": "https://arxiv.org/abs/2602.03414",
      "date": "2026-02-03",
      "authors_text": "Zhengbo Jiao, Shaobo Wang, Zifan Zhang, Wei Wang, Bing Zhao",
      "is_highlight": false,
      "score": 58.0,
      "summary": "Socratic-Geo introduces a multi-agent framework for autonomous data synthesis and geometric reasoning, significantly improving model performance with minimal seed problems and surpassing existing benchmarks.",
      "teaser_image": "https://arxiv.org/html/2602.03414/x1.png",
      "debug_abstract": "Multimodal Large Language Models (MLLMs) have significantly advanced vision-language understanding. However, even state-of-the-art models struggle with geometric reasoning, revealing a critical bottleneck: the extreme scarcity of high-quality image-text pairs. Human annotation is prohibitively expensive, while automated methods fail to ensure fidelity and training effectiveness. Existing approaches either passively adapt to available images or employ inefficient random exploration with filtering, decoupling generation from learning needs. We propose Socratic-Geo, a fully autonomous framework that dynamically couples data synthesis with model learning through multi-agent interaction. The Teacher agent generates parameterized Python scripts with reflective feedback (Reflect for solvability, RePI for visual validity), ensuring image-text pair purity. The Solver agent optimizes reasoning through preference learning, with failure paths guiding Teacher&#39;s targeted augmentation. Independently, the Generator learns image generation capabilities on accumulated &#34;image-code-instruction&#34; triplets, distilling programmatic drawing intelligence into visual generation. Starting from only 108 seed problems, Socratic-Solver achieves 49.11 on six benchmarks using one-quarter of baseline data, surpassing strong baselines by 2.43 points. Socratic-Generator achieves 42.4% on GenExam, establishing new state-of-the-art for open-source models, surpassing Seedream-4.0 (39.8%) and approaching Gemini-2.5-Flash-Image (43.1%)."
    },
    {
      "title": "Agentic Proposing: Enhancing Large Language Model Reasoning via Compositional Skill Synthesis",
      "url": "https://arxiv.org/abs/2602.03279",
      "date": "2026-02-03",
      "authors_text": "Zhengbo Jiao, Shaobo Wang, Zifan Zhang, Xuan Ren, Wei Wang",
      "is_highlight": false,
      "score": 71.0,
      "summary": "The paper introduces Agentic Proposing, a framework for generating high-quality synthetic training data that enhances reasoning in large language models, achieving superior performance with minimal human input.",
      "teaser_image": "https://arxiv.org/html/2602.03279/x2.png",
      "debug_abstract": "Advancing complex reasoning in large language models relies on high-quality, verifiable datasets, yet human annotation remains cost-prohibitive and difficult to scale. Current synthesis paradigms often face a recurring trade-off: maintaining structural validity typically restricts problem complexity, while relaxing constraints to increase difficulty frequently leads to inconsistent or unsolvable instances. To address this, we propose Agentic Proposing, a framework that models problem synthesis as a goal-driven sequential decision process where a specialized agent dynamically selects and composes modular reasoning skills. Through an iterative workflow of internal reflection and tool-use, we develop the Agentic-Proposer-4B using Multi-Granularity Policy Optimization (MGPO) to generate high-precision, verifiable training trajectories across mathematics, coding, and science. Empirical results demonstrate that downstream solvers trained on agent-synthesized data significantly outperform leading baselines and exhibit robust cross-domain generalization. Notably, a 30B solver trained on only 11,000 synthesized trajectories achieves a state-of-the-art 91.6% accuracy on AIME25, rivaling frontier-scale proprietary models such as GPT-5 and proving that a small volume of high-quality synthetic signals can effectively substitute for massive human-curated datasets."
    },
    {
      "title": "Bongards at the Boundary of Perception and Reasoning: Programs or Language?",
      "url": "https://arxiv.org/abs/2602.03038",
      "date": "2026-02-03",
      "authors_text": "Cassidy Langenfeld, Claas Beger, Gloria Geng, Wasu Top Piriyakulkij, Keya Hu",
      "is_highlight": false,
      "score": 60.0,
      "summary": "This paper introduces a neurosymbolic method utilizing LLMs and Bayesian optimization to tackle Bongard problems, enhancing visual reasoning beyond conventional VLM capabilities.",
      "teaser_image": "https://arxiv.org/html/2602.03038/x3.png",
      "debug_abstract": "Vision-Language Models (VLMs) have made great strides in everyday visual tasks, such as captioning a natural image, or answering commonsense questions about such images. But humans possess the puzzling ability to deploy their visual reasoning abilities in radically new situations, a skill rigorously tested by the classic set of visual reasoning challenges known as the Bongard problems. We present a neurosymbolic approach to solving these problems: given a hypothesized solution rule for a Bongard problem, we leverage LLMs to generate parameterized programmatic representations for the rule and perform parameter fitting using Bayesian optimization. We evaluate our method on classifying Bongard problem images given the ground truth rule, as well as on solving the problems from scratch."
    },
    {
      "title": "Embodiment-Aware Generalist Specialist Distillation for Unified Humanoid Whole-Body Control",
      "url": "https://arxiv.org/abs/2602.02960",
      "date": "2026-02-03",
      "authors_text": "Quanquan Peng, Yunfeng Lin, Yufei Xue, Jiangmiao Pang, Weinan Zhang",
      "is_highlight": false,
      "score": 90.0,
      "summary": "The EAGLE framework enables a unified policy for diverse humanoid robots, enhancing whole-body control through iterative generalist-specialist distillation without individual reward tuning.",
      "teaser_image": "https://arxiv.org/html/2602.02960/x1.png",
      "debug_abstract": "Humanoid Whole-Body Controllers trained with reinforcement learning (RL) have recently achieved remarkable performance, yet many target a single robot embodiment. Variations in dynamics, degrees of freedom (DoFs), and kinematic topology still hinder a single policy from commanding diverse humanoids. Moreover, obtaining a generalist policy that not only transfers across embodiments but also supports richer behaviors-beyond simple walking to squatting, leaning-remains especially challenging. In this work, we tackle these obstacles by introducing EAGLE, an iterative generalist-specialist distillation framework that produces a single unified policy that controls multiple heterogeneous humanoids without per-robot reward tuning. During each cycle, embodiment-specific specialists are forked from the current generalist, refined on their respective robots, and new skills are distilled back into the generalist by training on the pooled embodiment set. Repeating this loop until performance convergence produces a robust Whole-Body Controller validated on robots such as Unitree H1, G1, and Fourier N1. We conducted experiments on five different robots in simulation and four in real-world settings. Through quantitative evaluations, EAGLE achieves high tracking accuracy and robustness compared to other methods, marking a step toward scalable, fleet-level humanoid control. See more details at this https URL"
    },
    {
      "title": "SurfSplat: Conquering Feedforward 2D Gaussian Splatting with Surface Continuity Priors",
      "url": "https://arxiv.org/abs/2602.02000",
      "date": "2026-02-03",
      "authors_text": "Bing He, Jingnan Gao, Yunuo Chen, Ning Cao, Gang Chen",
      "is_highlight": false,
      "score": 65.0,
      "summary": "SurfSplat introduces a feedforward 2D Gaussian Splatting framework that enhances 3D scene reconstruction by ensuring surface continuity and high geometric precision, outperforming existing methods.",
      "teaser_image": "https://arxiv.org/html/2602.02000/x2.png",
      "debug_abstract": "Reconstructing 3D scenes from sparse images remains a challenging task due to the difficulty of recovering accurate geometry and texture without optimization. Recent approaches leverage generalizable models to generate 3D scenes using 3D Gaussian Splatting (3DGS) primitive. However, they often fail to produce continuous surfaces and instead yield discrete, color-biased point clouds that appear plausible at normal resolution but reveal severe artifacts under close-up views. To address this issue, we present SurfSplat, a feedforward framework based on 2D Gaussian Splatting (2DGS) primitive, which provides stronger anisotropy and higher geometric precision. By incorporating a surface continuity prior and a forced alpha blending strategy, SurfSplat reconstructs coherent geometry together with faithful textures. Furthermore, we introduce High-Resolution Rendering Consistency (HRRC), a new evaluation metric designed to evaluate high-resolution reconstruction quality. Extensive experiments on RealEstate10K, DL3DV, and ScanNet demonstrate that SurfSplat consistently outperforms prior methods on both standard metrics and HRRC, establishing a robust solution for high-fidelity 3D reconstruction from sparse inputs. Project page: this https URL"
    }
  ],
  "赵波老师实验室": [
    {
      "title": "QVLA: Not All Channels Are Equal in Vision-Language-Action Model's Quantization",
      "url": "https://arxiv.org/abs/2602.03782",
      "date": "2026-02-03",
      "authors_text": "Yuhao Xu, Yantai Yang, Zhenyang Fan, Yufan Liu, Yuming Li",
      "is_highlight": false,
      "score": 47.0,
      "summary": "QVLA introduces an action-centric quantization framework for Vision-Language-Action models, optimizing channel-wise bit allocation to enhance performance and reduce resource demands in robotics.",
      "teaser_image": "https://arxiv.org/html/2602.03782/x2.png",
      "debug_abstract": "The advent of Vision-Language-Action (VLA) models represents a significant leap for embodied intelligence, yet their immense computational demands critically hinder deployment on resource-constrained robotic platforms. Intuitively, low-bit quantization is a prevalent and preferred technique for large-scale model compression. However, we find that a systematic analysis of VLA model&#39;s quantization is fundamentally lacking. We argue that naively applying uniform-bit quantization from Large Language Models (LLMs) to robotics is flawed, as these methods prioritize passive data fidelity while ignoring how minor action deviations compound into catastrophic task failures. To bridge this gap, we introduce QVLA, the first action-centric quantization framework specifically designed for embodied control. In a sharp departure from the rigid, uniform-bit quantization of LLM-based methods, QVLA introduces a highly granular, channel-wise bit allocation strategy. Its core mechanism is to directly measure the final action-space sensitivity when quantizing each individual channel to various bit-widths. This process yields a precise, per-channel importance metric that guides a global optimization, which elegantly unifies quantization and pruning (0-bit) into a single, cohesive framework. Extensive evaluations on different baselines demonstrate the superiority of our approach. In the LIBERO, the quantization version of OpenVLA-OFT with our method requires only 29.2% of the original model&#39;s VRAM while maintaining 98.9% of its original performance and achieving a 1.49x speedup. This translates to a 22.6% performance improvement over the LLM-derived method SmoothQuant. Our work establishes a new, principled foundation for compressing VLA models in robotics, paving the way for deploying powerful, large-scale models on real-world hardware. Code will be released."
    },
    {
      "title": "Socratic-Geo: Synthetic Data Generation and Geometric Reasoning via Multi-Agent Interaction",
      "url": "https://arxiv.org/abs/2602.03414",
      "date": "2026-02-03",
      "authors_text": "Zhengbo Jiao, Shaobo Wang, Zifan Zhang, Wei Wang, Bing Zhao",
      "is_highlight": false,
      "score": 58.0,
      "summary": "Socratic-Geo introduces a multi-agent framework for autonomous data synthesis and geometric reasoning, significantly improving model performance with minimal seed problems and surpassing existing benchmarks.",
      "teaser_image": "https://arxiv.org/html/2602.03414/x1.png",
      "debug_abstract": "Multimodal Large Language Models (MLLMs) have significantly advanced vision-language understanding. However, even state-of-the-art models struggle with geometric reasoning, revealing a critical bottleneck: the extreme scarcity of high-quality image-text pairs. Human annotation is prohibitively expensive, while automated methods fail to ensure fidelity and training effectiveness. Existing approaches either passively adapt to available images or employ inefficient random exploration with filtering, decoupling generation from learning needs. We propose Socratic-Geo, a fully autonomous framework that dynamically couples data synthesis with model learning through multi-agent interaction. The Teacher agent generates parameterized Python scripts with reflective feedback (Reflect for solvability, RePI for visual validity), ensuring image-text pair purity. The Solver agent optimizes reasoning through preference learning, with failure paths guiding Teacher&#39;s targeted augmentation. Independently, the Generator learns image generation capabilities on accumulated &#34;image-code-instruction&#34; triplets, distilling programmatic drawing intelligence into visual generation. Starting from only 108 seed problems, Socratic-Solver achieves 49.11 on six benchmarks using one-quarter of baseline data, surpassing strong baselines by 2.43 points. Socratic-Generator achieves 42.4% on GenExam, establishing new state-of-the-art for open-source models, surpassing Seedream-4.0 (39.8%) and approaching Gemini-2.5-Flash-Image (43.1%)."
    },
    {
      "title": "Agentic Proposing: Enhancing Large Language Model Reasoning via Compositional Skill Synthesis",
      "url": "https://arxiv.org/abs/2602.03279",
      "date": "2026-02-03",
      "authors_text": "Zhengbo Jiao, Shaobo Wang, Zifan Zhang, Xuan Ren, Wei Wang",
      "is_highlight": false,
      "score": 71.0,
      "summary": "The paper introduces Agentic Proposing, a framework for generating high-quality synthetic training data that enhances reasoning in large language models, achieving superior performance with minimal human input.",
      "teaser_image": "https://arxiv.org/html/2602.03279/x2.png",
      "debug_abstract": "Advancing complex reasoning in large language models relies on high-quality, verifiable datasets, yet human annotation remains cost-prohibitive and difficult to scale. Current synthesis paradigms often face a recurring trade-off: maintaining structural validity typically restricts problem complexity, while relaxing constraints to increase difficulty frequently leads to inconsistent or unsolvable instances. To address this, we propose Agentic Proposing, a framework that models problem synthesis as a goal-driven sequential decision process where a specialized agent dynamically selects and composes modular reasoning skills. Through an iterative workflow of internal reflection and tool-use, we develop the Agentic-Proposer-4B using Multi-Granularity Policy Optimization (MGPO) to generate high-precision, verifiable training trajectories across mathematics, coding, and science. Empirical results demonstrate that downstream solvers trained on agent-synthesized data significantly outperform leading baselines and exhibit robust cross-domain generalization. Notably, a 30B solver trained on only 11,000 synthesized trajectories achieves a state-of-the-art 91.6% accuracy on AIME25, rivaling frontier-scale proprietary models such as GPT-5 and proving that a small volume of high-quality synthetic signals can effectively substitute for massive human-curated datasets."
    },
    {
      "title": "Bongards at the Boundary of Perception and Reasoning: Programs or Language?",
      "url": "https://arxiv.org/abs/2602.03038",
      "date": "2026-02-03",
      "authors_text": "Cassidy Langenfeld, Claas Beger, Gloria Geng, Wasu Top Piriyakulkij, Keya Hu",
      "is_highlight": false,
      "score": 60.0,
      "summary": "This paper introduces a neurosymbolic method utilizing LLMs and Bayesian optimization to tackle Bongard problems, enhancing visual reasoning beyond conventional VLM capabilities.",
      "teaser_image": "https://arxiv.org/html/2602.03038/x3.png",
      "debug_abstract": "Vision-Language Models (VLMs) have made great strides in everyday visual tasks, such as captioning a natural image, or answering commonsense questions about such images. But humans possess the puzzling ability to deploy their visual reasoning abilities in radically new situations, a skill rigorously tested by the classic set of visual reasoning challenges known as the Bongard problems. We present a neurosymbolic approach to solving these problems: given a hypothesized solution rule for a Bongard problem, we leverage LLMs to generate parameterized programmatic representations for the rule and perform parameter fitting using Bayesian optimization. We evaluate our method on classifying Bongard problem images given the ground truth rule, as well as on solving the problems from scratch."
    },
    {
      "title": "Embodiment-Aware Generalist Specialist Distillation for Unified Humanoid Whole-Body Control",
      "url": "https://arxiv.org/abs/2602.02960",
      "date": "2026-02-03",
      "authors_text": "Quanquan Peng, Yunfeng Lin, Yufei Xue, Jiangmiao Pang, Weinan Zhang",
      "is_highlight": false,
      "score": 90.0,
      "summary": "The EAGLE framework enables a unified policy for diverse humanoid robots, enhancing whole-body control through iterative generalist-specialist distillation without individual reward tuning.",
      "teaser_image": "https://arxiv.org/html/2602.02960/x1.png",
      "debug_abstract": "Humanoid Whole-Body Controllers trained with reinforcement learning (RL) have recently achieved remarkable performance, yet many target a single robot embodiment. Variations in dynamics, degrees of freedom (DoFs), and kinematic topology still hinder a single policy from commanding diverse humanoids. Moreover, obtaining a generalist policy that not only transfers across embodiments but also supports richer behaviors-beyond simple walking to squatting, leaning-remains especially challenging. In this work, we tackle these obstacles by introducing EAGLE, an iterative generalist-specialist distillation framework that produces a single unified policy that controls multiple heterogeneous humanoids without per-robot reward tuning. During each cycle, embodiment-specific specialists are forked from the current generalist, refined on their respective robots, and new skills are distilled back into the generalist by training on the pooled embodiment set. Repeating this loop until performance convergence produces a robust Whole-Body Controller validated on robots such as Unitree H1, G1, and Fourier N1. We conducted experiments on five different robots in simulation and four in real-world settings. Through quantitative evaluations, EAGLE achieves high tracking accuracy and robustness compared to other methods, marking a step toward scalable, fleet-level humanoid control. See more details at this https URL"
    },
    {
      "title": "SurfSplat: Conquering Feedforward 2D Gaussian Splatting with Surface Continuity Priors",
      "url": "https://arxiv.org/abs/2602.02000",
      "date": "2026-02-03",
      "authors_text": "Bing He, Jingnan Gao, Yunuo Chen, Ning Cao, Gang Chen",
      "is_highlight": false,
      "score": 65.0,
      "summary": "SurfSplat introduces a feedforward 2D Gaussian Splatting framework that enhances 3D scene reconstruction by ensuring surface continuity and high geometric precision, outperforming existing methods.",
      "teaser_image": "https://arxiv.org/html/2602.02000/x2.png",
      "debug_abstract": "Reconstructing 3D scenes from sparse images remains a challenging task due to the difficulty of recovering accurate geometry and texture without optimization. Recent approaches leverage generalizable models to generate 3D scenes using 3D Gaussian Splatting (3DGS) primitive. However, they often fail to produce continuous surfaces and instead yield discrete, color-biased point clouds that appear plausible at normal resolution but reveal severe artifacts under close-up views. To address this issue, we present SurfSplat, a feedforward framework based on 2D Gaussian Splatting (2DGS) primitive, which provides stronger anisotropy and higher geometric precision. By incorporating a surface continuity prior and a forced alpha blending strategy, SurfSplat reconstructs coherent geometry together with faithful textures. Furthermore, we introduce High-Resolution Rendering Consistency (HRRC), a new evaluation metric designed to evaluate high-resolution reconstruction quality. Extensive experiments on RealEstate10K, DL3DV, and ScanNet demonstrate that SurfSplat consistently outperforms prior methods on both standard metrics and HRRC, establishing a robust solution for high-fidelity 3D reconstruction from sparse inputs. Project page: this https URL"
    }
  ],
  "智能机器人与机器视觉(IRMV)实验室": [
    {
      "title": "QVLA: Not All Channels Are Equal in Vision-Language-Action Model's Quantization",
      "url": "https://arxiv.org/abs/2602.03782",
      "date": "2026-02-03",
      "authors_text": "Yuhao Xu, Yantai Yang, Zhenyang Fan, Yufan Liu, Yuming Li",
      "is_highlight": false,
      "score": 47.0,
      "summary": "QVLA introduces an action-centric quantization framework for Vision-Language-Action models, optimizing channel-wise bit allocation to enhance performance and reduce resource demands in robotics.",
      "teaser_image": "https://arxiv.org/html/2602.03782/x2.png",
      "debug_abstract": "The advent of Vision-Language-Action (VLA) models represents a significant leap for embodied intelligence, yet their immense computational demands critically hinder deployment on resource-constrained robotic platforms. Intuitively, low-bit quantization is a prevalent and preferred technique for large-scale model compression. However, we find that a systematic analysis of VLA model&#39;s quantization is fundamentally lacking. We argue that naively applying uniform-bit quantization from Large Language Models (LLMs) to robotics is flawed, as these methods prioritize passive data fidelity while ignoring how minor action deviations compound into catastrophic task failures. To bridge this gap, we introduce QVLA, the first action-centric quantization framework specifically designed for embodied control. In a sharp departure from the rigid, uniform-bit quantization of LLM-based methods, QVLA introduces a highly granular, channel-wise bit allocation strategy. Its core mechanism is to directly measure the final action-space sensitivity when quantizing each individual channel to various bit-widths. This process yields a precise, per-channel importance metric that guides a global optimization, which elegantly unifies quantization and pruning (0-bit) into a single, cohesive framework. Extensive evaluations on different baselines demonstrate the superiority of our approach. In the LIBERO, the quantization version of OpenVLA-OFT with our method requires only 29.2% of the original model&#39;s VRAM while maintaining 98.9% of its original performance and achieving a 1.49x speedup. This translates to a 22.6% performance improvement over the LLM-derived method SmoothQuant. Our work establishes a new, principled foundation for compressing VLA models in robotics, paving the way for deploying powerful, large-scale models on real-world hardware. Code will be released."
    },
    {
      "title": "Socratic-Geo: Synthetic Data Generation and Geometric Reasoning via Multi-Agent Interaction",
      "url": "https://arxiv.org/abs/2602.03414",
      "date": "2026-02-03",
      "authors_text": "Zhengbo Jiao, Shaobo Wang, Zifan Zhang, Wei Wang, Bing Zhao",
      "is_highlight": false,
      "score": 58.0,
      "summary": "Socratic-Geo introduces a multi-agent framework for autonomous data synthesis and geometric reasoning, significantly improving model performance with minimal seed problems and surpassing existing benchmarks.",
      "teaser_image": "https://arxiv.org/html/2602.03414/x1.png",
      "debug_abstract": "Multimodal Large Language Models (MLLMs) have significantly advanced vision-language understanding. However, even state-of-the-art models struggle with geometric reasoning, revealing a critical bottleneck: the extreme scarcity of high-quality image-text pairs. Human annotation is prohibitively expensive, while automated methods fail to ensure fidelity and training effectiveness. Existing approaches either passively adapt to available images or employ inefficient random exploration with filtering, decoupling generation from learning needs. We propose Socratic-Geo, a fully autonomous framework that dynamically couples data synthesis with model learning through multi-agent interaction. The Teacher agent generates parameterized Python scripts with reflective feedback (Reflect for solvability, RePI for visual validity), ensuring image-text pair purity. The Solver agent optimizes reasoning through preference learning, with failure paths guiding Teacher&#39;s targeted augmentation. Independently, the Generator learns image generation capabilities on accumulated &#34;image-code-instruction&#34; triplets, distilling programmatic drawing intelligence into visual generation. Starting from only 108 seed problems, Socratic-Solver achieves 49.11 on six benchmarks using one-quarter of baseline data, surpassing strong baselines by 2.43 points. Socratic-Generator achieves 42.4% on GenExam, establishing new state-of-the-art for open-source models, surpassing Seedream-4.0 (39.8%) and approaching Gemini-2.5-Flash-Image (43.1%)."
    },
    {
      "title": "Agentic Proposing: Enhancing Large Language Model Reasoning via Compositional Skill Synthesis",
      "url": "https://arxiv.org/abs/2602.03279",
      "date": "2026-02-03",
      "authors_text": "Zhengbo Jiao, Shaobo Wang, Zifan Zhang, Xuan Ren, Wei Wang",
      "is_highlight": false,
      "score": 71.0,
      "summary": "The paper introduces Agentic Proposing, a framework for generating high-quality synthetic training data that enhances reasoning in large language models, achieving superior performance with minimal human input.",
      "teaser_image": "https://arxiv.org/html/2602.03279/x2.png",
      "debug_abstract": "Advancing complex reasoning in large language models relies on high-quality, verifiable datasets, yet human annotation remains cost-prohibitive and difficult to scale. Current synthesis paradigms often face a recurring trade-off: maintaining structural validity typically restricts problem complexity, while relaxing constraints to increase difficulty frequently leads to inconsistent or unsolvable instances. To address this, we propose Agentic Proposing, a framework that models problem synthesis as a goal-driven sequential decision process where a specialized agent dynamically selects and composes modular reasoning skills. Through an iterative workflow of internal reflection and tool-use, we develop the Agentic-Proposer-4B using Multi-Granularity Policy Optimization (MGPO) to generate high-precision, verifiable training trajectories across mathematics, coding, and science. Empirical results demonstrate that downstream solvers trained on agent-synthesized data significantly outperform leading baselines and exhibit robust cross-domain generalization. Notably, a 30B solver trained on only 11,000 synthesized trajectories achieves a state-of-the-art 91.6% accuracy on AIME25, rivaling frontier-scale proprietary models such as GPT-5 and proving that a small volume of high-quality synthetic signals can effectively substitute for massive human-curated datasets."
    },
    {
      "title": "Bongards at the Boundary of Perception and Reasoning: Programs or Language?",
      "url": "https://arxiv.org/abs/2602.03038",
      "date": "2026-02-03",
      "authors_text": "Cassidy Langenfeld, Claas Beger, Gloria Geng, Wasu Top Piriyakulkij, Keya Hu",
      "is_highlight": false,
      "score": 60.0,
      "summary": "This paper introduces a neurosymbolic method utilizing LLMs and Bayesian optimization to tackle Bongard problems, enhancing visual reasoning beyond conventional VLM capabilities.",
      "teaser_image": "https://arxiv.org/html/2602.03038/x3.png",
      "debug_abstract": "Vision-Language Models (VLMs) have made great strides in everyday visual tasks, such as captioning a natural image, or answering commonsense questions about such images. But humans possess the puzzling ability to deploy their visual reasoning abilities in radically new situations, a skill rigorously tested by the classic set of visual reasoning challenges known as the Bongard problems. We present a neurosymbolic approach to solving these problems: given a hypothesized solution rule for a Bongard problem, we leverage LLMs to generate parameterized programmatic representations for the rule and perform parameter fitting using Bayesian optimization. We evaluate our method on classifying Bongard problem images given the ground truth rule, as well as on solving the problems from scratch."
    },
    {
      "title": "Embodiment-Aware Generalist Specialist Distillation for Unified Humanoid Whole-Body Control",
      "url": "https://arxiv.org/abs/2602.02960",
      "date": "2026-02-03",
      "authors_text": "Quanquan Peng, Yunfeng Lin, Yufei Xue, Jiangmiao Pang, Weinan Zhang",
      "is_highlight": false,
      "score": 90.0,
      "summary": "The EAGLE framework enables a unified policy for diverse humanoid robots, enhancing whole-body control through iterative generalist-specialist distillation without individual reward tuning.",
      "teaser_image": "https://arxiv.org/html/2602.02960/x1.png",
      "debug_abstract": "Humanoid Whole-Body Controllers trained with reinforcement learning (RL) have recently achieved remarkable performance, yet many target a single robot embodiment. Variations in dynamics, degrees of freedom (DoFs), and kinematic topology still hinder a single policy from commanding diverse humanoids. Moreover, obtaining a generalist policy that not only transfers across embodiments but also supports richer behaviors-beyond simple walking to squatting, leaning-remains especially challenging. In this work, we tackle these obstacles by introducing EAGLE, an iterative generalist-specialist distillation framework that produces a single unified policy that controls multiple heterogeneous humanoids without per-robot reward tuning. During each cycle, embodiment-specific specialists are forked from the current generalist, refined on their respective robots, and new skills are distilled back into the generalist by training on the pooled embodiment set. Repeating this loop until performance convergence produces a robust Whole-Body Controller validated on robots such as Unitree H1, G1, and Fourier N1. We conducted experiments on five different robots in simulation and four in real-world settings. Through quantitative evaluations, EAGLE achieves high tracking accuracy and robustness compared to other methods, marking a step toward scalable, fleet-level humanoid control. See more details at this https URL"
    },
    {
      "title": "SurfSplat: Conquering Feedforward 2D Gaussian Splatting with Surface Continuity Priors",
      "url": "https://arxiv.org/abs/2602.02000",
      "date": "2026-02-03",
      "authors_text": "Bing He, Jingnan Gao, Yunuo Chen, Ning Cao, Gang Chen",
      "is_highlight": false,
      "score": 65.0,
      "summary": "SurfSplat introduces a feedforward 2D Gaussian Splatting framework that enhances 3D scene reconstruction by ensuring surface continuity and high geometric precision, outperforming existing methods.",
      "teaser_image": "https://arxiv.org/html/2602.02000/x2.png",
      "debug_abstract": "Reconstructing 3D scenes from sparse images remains a challenging task due to the difficulty of recovering accurate geometry and texture without optimization. Recent approaches leverage generalizable models to generate 3D scenes using 3D Gaussian Splatting (3DGS) primitive. However, they often fail to produce continuous surfaces and instead yield discrete, color-biased point clouds that appear plausible at normal resolution but reveal severe artifacts under close-up views. To address this issue, we present SurfSplat, a feedforward framework based on 2D Gaussian Splatting (2DGS) primitive, which provides stronger anisotropy and higher geometric precision. By incorporating a surface continuity prior and a forced alpha blending strategy, SurfSplat reconstructs coherent geometry together with faithful textures. Furthermore, we introduce High-Resolution Rendering Consistency (HRRC), a new evaluation metric designed to evaluate high-resolution reconstruction quality. Extensive experiments on RealEstate10K, DL3DV, and ScanNet demonstrate that SurfSplat consistently outperforms prior methods on both standard metrics and HRRC, establishing a robust solution for high-fidelity 3D reconstruction from sparse inputs. Project page: this https URL"
    }
  ],
  "IWIN-FINS实验室": [
    {
      "title": "QVLA: Not All Channels Are Equal in Vision-Language-Action Model's Quantization",
      "url": "https://arxiv.org/abs/2602.03782",
      "date": "2026-02-03",
      "authors_text": "Yuhao Xu, Yantai Yang, Zhenyang Fan, Yufan Liu, Yuming Li",
      "is_highlight": false,
      "score": 47.0,
      "summary": "QVLA introduces an action-centric quantization framework for Vision-Language-Action models, optimizing channel-wise bit allocation to enhance performance and reduce resource demands in robotics.",
      "teaser_image": "https://arxiv.org/html/2602.03782/x2.png",
      "debug_abstract": "The advent of Vision-Language-Action (VLA) models represents a significant leap for embodied intelligence, yet their immense computational demands critically hinder deployment on resource-constrained robotic platforms. Intuitively, low-bit quantization is a prevalent and preferred technique for large-scale model compression. However, we find that a systematic analysis of VLA model&#39;s quantization is fundamentally lacking. We argue that naively applying uniform-bit quantization from Large Language Models (LLMs) to robotics is flawed, as these methods prioritize passive data fidelity while ignoring how minor action deviations compound into catastrophic task failures. To bridge this gap, we introduce QVLA, the first action-centric quantization framework specifically designed for embodied control. In a sharp departure from the rigid, uniform-bit quantization of LLM-based methods, QVLA introduces a highly granular, channel-wise bit allocation strategy. Its core mechanism is to directly measure the final action-space sensitivity when quantizing each individual channel to various bit-widths. This process yields a precise, per-channel importance metric that guides a global optimization, which elegantly unifies quantization and pruning (0-bit) into a single, cohesive framework. Extensive evaluations on different baselines demonstrate the superiority of our approach. In the LIBERO, the quantization version of OpenVLA-OFT with our method requires only 29.2% of the original model&#39;s VRAM while maintaining 98.9% of its original performance and achieving a 1.49x speedup. This translates to a 22.6% performance improvement over the LLM-derived method SmoothQuant. Our work establishes a new, principled foundation for compressing VLA models in robotics, paving the way for deploying powerful, large-scale models on real-world hardware. Code will be released."
    },
    {
      "title": "Socratic-Geo: Synthetic Data Generation and Geometric Reasoning via Multi-Agent Interaction",
      "url": "https://arxiv.org/abs/2602.03414",
      "date": "2026-02-03",
      "authors_text": "Zhengbo Jiao, Shaobo Wang, Zifan Zhang, Wei Wang, Bing Zhao",
      "is_highlight": false,
      "score": 58.0,
      "summary": "Socratic-Geo introduces a multi-agent framework for autonomous data synthesis and geometric reasoning, significantly improving model performance with minimal seed problems and surpassing existing benchmarks.",
      "teaser_image": "https://arxiv.org/html/2602.03414/x1.png",
      "debug_abstract": "Multimodal Large Language Models (MLLMs) have significantly advanced vision-language understanding. However, even state-of-the-art models struggle with geometric reasoning, revealing a critical bottleneck: the extreme scarcity of high-quality image-text pairs. Human annotation is prohibitively expensive, while automated methods fail to ensure fidelity and training effectiveness. Existing approaches either passively adapt to available images or employ inefficient random exploration with filtering, decoupling generation from learning needs. We propose Socratic-Geo, a fully autonomous framework that dynamically couples data synthesis with model learning through multi-agent interaction. The Teacher agent generates parameterized Python scripts with reflective feedback (Reflect for solvability, RePI for visual validity), ensuring image-text pair purity. The Solver agent optimizes reasoning through preference learning, with failure paths guiding Teacher&#39;s targeted augmentation. Independently, the Generator learns image generation capabilities on accumulated &#34;image-code-instruction&#34; triplets, distilling programmatic drawing intelligence into visual generation. Starting from only 108 seed problems, Socratic-Solver achieves 49.11 on six benchmarks using one-quarter of baseline data, surpassing strong baselines by 2.43 points. Socratic-Generator achieves 42.4% on GenExam, establishing new state-of-the-art for open-source models, surpassing Seedream-4.0 (39.8%) and approaching Gemini-2.5-Flash-Image (43.1%)."
    },
    {
      "title": "Agentic Proposing: Enhancing Large Language Model Reasoning via Compositional Skill Synthesis",
      "url": "https://arxiv.org/abs/2602.03279",
      "date": "2026-02-03",
      "authors_text": "Zhengbo Jiao, Shaobo Wang, Zifan Zhang, Xuan Ren, Wei Wang",
      "is_highlight": false,
      "score": 71.0,
      "summary": "The paper introduces Agentic Proposing, a framework for generating high-quality synthetic training data that enhances reasoning in large language models, achieving superior performance with minimal human input.",
      "teaser_image": "https://arxiv.org/html/2602.03279/x2.png",
      "debug_abstract": "Advancing complex reasoning in large language models relies on high-quality, verifiable datasets, yet human annotation remains cost-prohibitive and difficult to scale. Current synthesis paradigms often face a recurring trade-off: maintaining structural validity typically restricts problem complexity, while relaxing constraints to increase difficulty frequently leads to inconsistent or unsolvable instances. To address this, we propose Agentic Proposing, a framework that models problem synthesis as a goal-driven sequential decision process where a specialized agent dynamically selects and composes modular reasoning skills. Through an iterative workflow of internal reflection and tool-use, we develop the Agentic-Proposer-4B using Multi-Granularity Policy Optimization (MGPO) to generate high-precision, verifiable training trajectories across mathematics, coding, and science. Empirical results demonstrate that downstream solvers trained on agent-synthesized data significantly outperform leading baselines and exhibit robust cross-domain generalization. Notably, a 30B solver trained on only 11,000 synthesized trajectories achieves a state-of-the-art 91.6% accuracy on AIME25, rivaling frontier-scale proprietary models such as GPT-5 and proving that a small volume of high-quality synthetic signals can effectively substitute for massive human-curated datasets."
    },
    {
      "title": "Bongards at the Boundary of Perception and Reasoning: Programs or Language?",
      "url": "https://arxiv.org/abs/2602.03038",
      "date": "2026-02-03",
      "authors_text": "Cassidy Langenfeld, Claas Beger, Gloria Geng, Wasu Top Piriyakulkij, Keya Hu",
      "is_highlight": false,
      "score": 60.0,
      "summary": "This paper introduces a neurosymbolic method utilizing LLMs and Bayesian optimization to tackle Bongard problems, enhancing visual reasoning beyond conventional VLM capabilities.",
      "teaser_image": "https://arxiv.org/html/2602.03038/x3.png",
      "debug_abstract": "Vision-Language Models (VLMs) have made great strides in everyday visual tasks, such as captioning a natural image, or answering commonsense questions about such images. But humans possess the puzzling ability to deploy their visual reasoning abilities in radically new situations, a skill rigorously tested by the classic set of visual reasoning challenges known as the Bongard problems. We present a neurosymbolic approach to solving these problems: given a hypothesized solution rule for a Bongard problem, we leverage LLMs to generate parameterized programmatic representations for the rule and perform parameter fitting using Bayesian optimization. We evaluate our method on classifying Bongard problem images given the ground truth rule, as well as on solving the problems from scratch."
    },
    {
      "title": "Embodiment-Aware Generalist Specialist Distillation for Unified Humanoid Whole-Body Control",
      "url": "https://arxiv.org/abs/2602.02960",
      "date": "2026-02-03",
      "authors_text": "Quanquan Peng, Yunfeng Lin, Yufei Xue, Jiangmiao Pang, Weinan Zhang",
      "is_highlight": false,
      "score": 90.0,
      "summary": "The EAGLE framework enables a unified policy for diverse humanoid robots, enhancing whole-body control through iterative generalist-specialist distillation without individual reward tuning.",
      "teaser_image": "https://arxiv.org/html/2602.02960/x1.png",
      "debug_abstract": "Humanoid Whole-Body Controllers trained with reinforcement learning (RL) have recently achieved remarkable performance, yet many target a single robot embodiment. Variations in dynamics, degrees of freedom (DoFs), and kinematic topology still hinder a single policy from commanding diverse humanoids. Moreover, obtaining a generalist policy that not only transfers across embodiments but also supports richer behaviors-beyond simple walking to squatting, leaning-remains especially challenging. In this work, we tackle these obstacles by introducing EAGLE, an iterative generalist-specialist distillation framework that produces a single unified policy that controls multiple heterogeneous humanoids without per-robot reward tuning. During each cycle, embodiment-specific specialists are forked from the current generalist, refined on their respective robots, and new skills are distilled back into the generalist by training on the pooled embodiment set. Repeating this loop until performance convergence produces a robust Whole-Body Controller validated on robots such as Unitree H1, G1, and Fourier N1. We conducted experiments on five different robots in simulation and four in real-world settings. Through quantitative evaluations, EAGLE achieves high tracking accuracy and robustness compared to other methods, marking a step toward scalable, fleet-level humanoid control. See more details at this https URL"
    },
    {
      "title": "SurfSplat: Conquering Feedforward 2D Gaussian Splatting with Surface Continuity Priors",
      "url": "https://arxiv.org/abs/2602.02000",
      "date": "2026-02-03",
      "authors_text": "Bing He, Jingnan Gao, Yunuo Chen, Ning Cao, Gang Chen",
      "is_highlight": false,
      "score": 65.0,
      "summary": "SurfSplat introduces a feedforward 2D Gaussian Splatting framework that enhances 3D scene reconstruction by ensuring surface continuity and high geometric precision, outperforming existing methods.",
      "teaser_image": "https://arxiv.org/html/2602.02000/x2.png",
      "debug_abstract": "Reconstructing 3D scenes from sparse images remains a challenging task due to the difficulty of recovering accurate geometry and texture without optimization. Recent approaches leverage generalizable models to generate 3D scenes using 3D Gaussian Splatting (3DGS) primitive. However, they often fail to produce continuous surfaces and instead yield discrete, color-biased point clouds that appear plausible at normal resolution but reveal severe artifacts under close-up views. To address this issue, we present SurfSplat, a feedforward framework based on 2D Gaussian Splatting (2DGS) primitive, which provides stronger anisotropy and higher geometric precision. By incorporating a surface continuity prior and a forced alpha blending strategy, SurfSplat reconstructs coherent geometry together with faithful textures. Furthermore, we introduce High-Resolution Rendering Consistency (HRRC), a new evaluation metric designed to evaluate high-resolution reconstruction quality. Extensive experiments on RealEstate10K, DL3DV, and ScanNet demonstrate that SurfSplat consistently outperforms prior methods on both standard metrics and HRRC, establishing a robust solution for high-fidelity 3D reconstruction from sparse inputs. Project page: this https URL"
    }
  ],
  "ReThinkLab": [
    {
      "title": "QVLA: Not All Channels Are Equal in Vision-Language-Action Model's Quantization",
      "url": "https://arxiv.org/abs/2602.03782",
      "date": "2026-02-03",
      "authors_text": "Yuhao Xu, Yantai Yang, Zhenyang Fan, Yufan Liu, Yuming Li",
      "is_highlight": false,
      "score": 47.0,
      "summary": "QVLA introduces an action-centric quantization framework for Vision-Language-Action models, optimizing channel-wise bit allocation to enhance performance and reduce resource demands in robotics.",
      "teaser_image": "https://arxiv.org/html/2602.03782/x2.png",
      "debug_abstract": "The advent of Vision-Language-Action (VLA) models represents a significant leap for embodied intelligence, yet their immense computational demands critically hinder deployment on resource-constrained robotic platforms. Intuitively, low-bit quantization is a prevalent and preferred technique for large-scale model compression. However, we find that a systematic analysis of VLA model&#39;s quantization is fundamentally lacking. We argue that naively applying uniform-bit quantization from Large Language Models (LLMs) to robotics is flawed, as these methods prioritize passive data fidelity while ignoring how minor action deviations compound into catastrophic task failures. To bridge this gap, we introduce QVLA, the first action-centric quantization framework specifically designed for embodied control. In a sharp departure from the rigid, uniform-bit quantization of LLM-based methods, QVLA introduces a highly granular, channel-wise bit allocation strategy. Its core mechanism is to directly measure the final action-space sensitivity when quantizing each individual channel to various bit-widths. This process yields a precise, per-channel importance metric that guides a global optimization, which elegantly unifies quantization and pruning (0-bit) into a single, cohesive framework. Extensive evaluations on different baselines demonstrate the superiority of our approach. In the LIBERO, the quantization version of OpenVLA-OFT with our method requires only 29.2% of the original model&#39;s VRAM while maintaining 98.9% of its original performance and achieving a 1.49x speedup. This translates to a 22.6% performance improvement over the LLM-derived method SmoothQuant. Our work establishes a new, principled foundation for compressing VLA models in robotics, paving the way for deploying powerful, large-scale models on real-world hardware. Code will be released."
    },
    {
      "title": "Socratic-Geo: Synthetic Data Generation and Geometric Reasoning via Multi-Agent Interaction",
      "url": "https://arxiv.org/abs/2602.03414",
      "date": "2026-02-03",
      "authors_text": "Zhengbo Jiao, Shaobo Wang, Zifan Zhang, Wei Wang, Bing Zhao",
      "is_highlight": false,
      "score": 58.0,
      "summary": "Socratic-Geo introduces a multi-agent framework for autonomous data synthesis and geometric reasoning, significantly improving model performance with minimal seed problems and surpassing existing benchmarks.",
      "teaser_image": "https://arxiv.org/html/2602.03414/x1.png",
      "debug_abstract": "Multimodal Large Language Models (MLLMs) have significantly advanced vision-language understanding. However, even state-of-the-art models struggle with geometric reasoning, revealing a critical bottleneck: the extreme scarcity of high-quality image-text pairs. Human annotation is prohibitively expensive, while automated methods fail to ensure fidelity and training effectiveness. Existing approaches either passively adapt to available images or employ inefficient random exploration with filtering, decoupling generation from learning needs. We propose Socratic-Geo, a fully autonomous framework that dynamically couples data synthesis with model learning through multi-agent interaction. The Teacher agent generates parameterized Python scripts with reflective feedback (Reflect for solvability, RePI for visual validity), ensuring image-text pair purity. The Solver agent optimizes reasoning through preference learning, with failure paths guiding Teacher&#39;s targeted augmentation. Independently, the Generator learns image generation capabilities on accumulated &#34;image-code-instruction&#34; triplets, distilling programmatic drawing intelligence into visual generation. Starting from only 108 seed problems, Socratic-Solver achieves 49.11 on six benchmarks using one-quarter of baseline data, surpassing strong baselines by 2.43 points. Socratic-Generator achieves 42.4% on GenExam, establishing new state-of-the-art for open-source models, surpassing Seedream-4.0 (39.8%) and approaching Gemini-2.5-Flash-Image (43.1%)."
    },
    {
      "title": "Agentic Proposing: Enhancing Large Language Model Reasoning via Compositional Skill Synthesis",
      "url": "https://arxiv.org/abs/2602.03279",
      "date": "2026-02-03",
      "authors_text": "Zhengbo Jiao, Shaobo Wang, Zifan Zhang, Xuan Ren, Wei Wang",
      "is_highlight": false,
      "score": 71.0,
      "summary": "The paper introduces Agentic Proposing, a framework for generating high-quality synthetic training data that enhances reasoning in large language models, achieving superior performance with minimal human input.",
      "teaser_image": "https://arxiv.org/html/2602.03279/x2.png",
      "debug_abstract": "Advancing complex reasoning in large language models relies on high-quality, verifiable datasets, yet human annotation remains cost-prohibitive and difficult to scale. Current synthesis paradigms often face a recurring trade-off: maintaining structural validity typically restricts problem complexity, while relaxing constraints to increase difficulty frequently leads to inconsistent or unsolvable instances. To address this, we propose Agentic Proposing, a framework that models problem synthesis as a goal-driven sequential decision process where a specialized agent dynamically selects and composes modular reasoning skills. Through an iterative workflow of internal reflection and tool-use, we develop the Agentic-Proposer-4B using Multi-Granularity Policy Optimization (MGPO) to generate high-precision, verifiable training trajectories across mathematics, coding, and science. Empirical results demonstrate that downstream solvers trained on agent-synthesized data significantly outperform leading baselines and exhibit robust cross-domain generalization. Notably, a 30B solver trained on only 11,000 synthesized trajectories achieves a state-of-the-art 91.6% accuracy on AIME25, rivaling frontier-scale proprietary models such as GPT-5 and proving that a small volume of high-quality synthetic signals can effectively substitute for massive human-curated datasets."
    },
    {
      "title": "Bongards at the Boundary of Perception and Reasoning: Programs or Language?",
      "url": "https://arxiv.org/abs/2602.03038",
      "date": "2026-02-03",
      "authors_text": "Cassidy Langenfeld, Claas Beger, Gloria Geng, Wasu Top Piriyakulkij, Keya Hu",
      "is_highlight": false,
      "score": 60.0,
      "summary": "This paper introduces a neurosymbolic method utilizing LLMs and Bayesian optimization to tackle Bongard problems, enhancing visual reasoning beyond conventional VLM capabilities.",
      "teaser_image": "https://arxiv.org/html/2602.03038/x3.png",
      "debug_abstract": "Vision-Language Models (VLMs) have made great strides in everyday visual tasks, such as captioning a natural image, or answering commonsense questions about such images. But humans possess the puzzling ability to deploy their visual reasoning abilities in radically new situations, a skill rigorously tested by the classic set of visual reasoning challenges known as the Bongard problems. We present a neurosymbolic approach to solving these problems: given a hypothesized solution rule for a Bongard problem, we leverage LLMs to generate parameterized programmatic representations for the rule and perform parameter fitting using Bayesian optimization. We evaluate our method on classifying Bongard problem images given the ground truth rule, as well as on solving the problems from scratch."
    },
    {
      "title": "Embodiment-Aware Generalist Specialist Distillation for Unified Humanoid Whole-Body Control",
      "url": "https://arxiv.org/abs/2602.02960",
      "date": "2026-02-03",
      "authors_text": "Quanquan Peng, Yunfeng Lin, Yufei Xue, Jiangmiao Pang, Weinan Zhang",
      "is_highlight": false,
      "score": 90.0,
      "summary": "The EAGLE framework enables a unified policy for diverse humanoid robots, enhancing whole-body control through iterative generalist-specialist distillation without individual reward tuning.",
      "teaser_image": "https://arxiv.org/html/2602.02960/x1.png",
      "debug_abstract": "Humanoid Whole-Body Controllers trained with reinforcement learning (RL) have recently achieved remarkable performance, yet many target a single robot embodiment. Variations in dynamics, degrees of freedom (DoFs), and kinematic topology still hinder a single policy from commanding diverse humanoids. Moreover, obtaining a generalist policy that not only transfers across embodiments but also supports richer behaviors-beyond simple walking to squatting, leaning-remains especially challenging. In this work, we tackle these obstacles by introducing EAGLE, an iterative generalist-specialist distillation framework that produces a single unified policy that controls multiple heterogeneous humanoids without per-robot reward tuning. During each cycle, embodiment-specific specialists are forked from the current generalist, refined on their respective robots, and new skills are distilled back into the generalist by training on the pooled embodiment set. Repeating this loop until performance convergence produces a robust Whole-Body Controller validated on robots such as Unitree H1, G1, and Fourier N1. We conducted experiments on five different robots in simulation and four in real-world settings. Through quantitative evaluations, EAGLE achieves high tracking accuracy and robustness compared to other methods, marking a step toward scalable, fleet-level humanoid control. See more details at this https URL"
    },
    {
      "title": "SurfSplat: Conquering Feedforward 2D Gaussian Splatting with Surface Continuity Priors",
      "url": "https://arxiv.org/abs/2602.02000",
      "date": "2026-02-03",
      "authors_text": "Bing He, Jingnan Gao, Yunuo Chen, Ning Cao, Gang Chen",
      "is_highlight": false,
      "score": 65.0,
      "summary": "SurfSplat introduces a feedforward 2D Gaussian Splatting framework that enhances 3D scene reconstruction by ensuring surface continuity and high geometric precision, outperforming existing methods.",
      "teaser_image": "https://arxiv.org/html/2602.02000/x2.png",
      "debug_abstract": "Reconstructing 3D scenes from sparse images remains a challenging task due to the difficulty of recovering accurate geometry and texture without optimization. Recent approaches leverage generalizable models to generate 3D scenes using 3D Gaussian Splatting (3DGS) primitive. However, they often fail to produce continuous surfaces and instead yield discrete, color-biased point clouds that appear plausible at normal resolution but reveal severe artifacts under close-up views. To address this issue, we present SurfSplat, a feedforward framework based on 2D Gaussian Splatting (2DGS) primitive, which provides stronger anisotropy and higher geometric precision. By incorporating a surface continuity prior and a forced alpha blending strategy, SurfSplat reconstructs coherent geometry together with faithful textures. Furthermore, we introduce High-Resolution Rendering Consistency (HRRC), a new evaluation metric designed to evaluate high-resolution reconstruction quality. Extensive experiments on RealEstate10K, DL3DV, and ScanNet demonstrate that SurfSplat consistently outperforms prior methods on both standard metrics and HRRC, establishing a robust solution for high-fidelity 3D reconstruction from sparse inputs. Project page: this https URL"
    }
  ],
  "人机物智能融合实验室 (HCP)": [
    {
      "title": "Refer-Agent: A Collaborative Multi-Agent System with Reasoning and Reflection for Referring Video Object Segmentation",
      "url": "https://arxiv.org/abs/2602.03595",
      "date": "2026-02-03",
      "authors_text": "Haichao Jiang, Tianming Liang, Wei-Shi Zheng, Jian-Fang Hu",
      "is_highlight": false,
      "score": 53.0,
      "summary": "Refer-Agent is a collaborative multi-agent system for Referring Video Object Segmentation that enhances performance through reasoning-reflection mechanisms and flexible integration of new MLLMs.",
      "teaser_image": "https://arxiv.org/html/2602.03595/x2.png",
      "debug_abstract": "Referring Video Object Segmentation (RVOS) aims to segment objects in videos based on textual queries. Current methods mainly rely on large-scale supervised fine-tuning (SFT) of Multi-modal Large Language Models (MLLMs). However, this paradigm suffers from heavy data dependence and limited scalability against the rapid evolution of MLLMs. Although recent zero-shot approaches offer a flexible alternative, their performance remains significantly behind SFT-based methods, due to the straightforward workflow designs. To address these limitations, we propose \\textbf{Refer-Agent}, a collaborative multi-agent system with alternating reasoning-reflection mechanisms. This system decomposes RVOS into step-by-step reasoning process. During reasoning, we introduce a Coarse-to-Fine frame selection strategy to ensure the frame diversity and textual relevance, along with a Dynamic Focus Layout that adaptively adjusts the agent&#39;s visual focus. Furthermore, we propose a Chain-of-Reflection mechanism, which employs a Questioner-Responder pair to generate a self-reflection chain, enabling the system to verify intermediate results and generates feedback for next-round reasoning refinement. Extensive experiments on five challenging benchmarks demonstrate that Refer-Agent significantly outperforms state-of-the-art methods, including both SFT-based models and zero-shot approaches. Moreover, Refer-Agent is flexible and enables fast integration of new MLLMs without any additional fine-tuning costs. Code will be released."
    },
    {
      "title": "IntentRL: Training Proactive User-intent Agents for Open-ended Deep Research via Reinforcement Learning",
      "url": "https://arxiv.org/abs/2602.03468",
      "date": "2026-02-03",
      "authors_text": "Haohao Luo, Zexi Li, Yuexiang Xie, Wenhao Zhang, Yaliang Li",
      "is_highlight": false,
      "score": 74.0,
      "summary": "IntentRL trains proactive agents to clarify user intents before deep research, enhancing performance and efficiency in generating long-form reports from web data.",
      "teaser_image": "https://arxiv.org/html/2602.03468/x2.png",
      "debug_abstract": "Deep Research (DR) agents extend Large Language Models (LLMs) beyond parametric knowledge by autonomously retrieving and synthesizing evidence from large web corpora into long-form reports, enabling a long-horizon agentic paradigm. However, unlike real-time conversational assistants, DR is computationally expensive and time-consuming, creating an autonomy-interaction dilemma: high autonomy on ambiguous user queries often leads to prolonged execution with unsatisfactory outcomes. To address this, we propose IntentRL, a framework that trains proactive agents to clarify latent user intents before starting long-horizon research. To overcome the scarcity of open-ended research data, we introduce a scalable pipeline that expands a few seed samples into high-quality dialogue turns via a shallow-to-deep intent refinement graph. We further adopt a two-stage reinforcement learning (RL) strategy: Stage I applies RL on offline dialogues to efficiently learn general user-interaction behavior, while Stage II uses the trained agent and a user simulator for online rollouts to strengthen adaptation to diverse user feedback. Extensive experiments show that IntentRL significantly improves both intent hit rate and downstream task performance, outperforming the built-in clarify modules of closed-source DR agents and proactive LLM baselines."
    },
    {
      "title": "CIEC: Coupling Implicit and Explicit Cues for Multimodal Weakly Supervised Manipulation Localization",
      "url": "https://arxiv.org/abs/2602.02175",
      "date": "2026-02-03",
      "authors_text": "Xinquan Yu, Wei Lu, Xiangyang Luo, Rui Yang",
      "is_highlight": false,
      "score": 80.0,
      "summary": "The CIEC framework enables multimodal weakly-supervised manipulation localization using only coarse annotations, integrating image and text cues for improved accuracy and efficiency.",
      "teaser_image": "https://arxiv.org/html/2602.02175/x2.png",
      "debug_abstract": "To mitigate the threat of misinformation, multimodal manipulation localization has garnered growing attention. Consider that current methods rely on costly and time-consuming fine-grained annotations, such as patch/token-level annotations. This paper proposes a novel framework named Coupling Implicit and Explicit Cues (CIEC), which aims to achieve multimodal weakly-supervised manipulation localization for image-text pairs utilizing only coarse-grained image/sentence-level annotations. It comprises two branches, image-based and text-based weakly-supervised localization. For the former, we devise the Textual-guidance Refine Patch Selection (TRPS) module. It integrates forgery cues from both visual and textual perspectives to lock onto suspicious regions aided by spatial priors. Followed by the background silencing and spatial contrast constraints to suppress interference from irrelevant areas. For the latter, we devise the Visual-deviation Calibrated Token Grounding (VCTG) module. It focuses on meaningful content words and leverages relative visual bias to assist token localization. Followed by the asymmetric sparse and semantic consistency constraints to mitigate label noise and ensure reliability. Extensive experiments demonstrate the effectiveness of our CIEC, yielding results comparable to fully supervised methods on several evaluation metrics."
    }
  ],
  "南京大学": [
    {
      "title": "CMR: Contractive Mapping Embeddings for Robust Humanoid Locomotion on Unstructured Terrains",
      "url": "https://arxiv.org/abs/2602.03511",
      "date": "2026-02-03",
      "authors_text": "Qixin Zeng, Hongyin Zhang, Shangke Lyu, Junxi Jin, Donglin Wang",
      "is_highlight": true,
      "score": 87.0,
      "summary": "The CMR framework enhances humanoid locomotion robustness on unstructured terrains by mapping noisy observations to a stable latent space, improving performance against disturbances.",
      "teaser_image": "https://arxiv.org/html/2602.03511/figures/CMR_Framwork.png",
      "debug_abstract": "Robust disturbance rejection remains a longstanding challenge in humanoid locomotion, particularly on unstructured terrains where sensing is unreliable and model mismatch is pronounced. While perception information, such as height map, enhances terrain awareness, sensor noise and sim-to-real gaps can destabilize policies in practice. In this work, we provide theoretical analysis that bounds the return gap under observation noise, when the induced latent dynamics are contractive. Furthermore, we present Contractive Mapping for Robustness (CMR) framework that maps high-dimensional, disturbance-prone observations into a latent space, where local perturbations are attenuated over time. Specifically, this approach couples contrastive representation learning with Lipschitz regularization to preserve task-relevant geometry while explicitly controlling sensitivity. Notably, the formulation can be incorporated into modern deep reinforcement learning pipelines as an auxiliary loss term with minimal additional technical effort required. Further, our extensive humanoid experiments show that CMR potently outperforms other locomotion algorithms under increased noise."
    },
    {
      "title": "TIDE: Trajectory-based Diagnostic Evaluation of Test-Time Improvement in LLM Agents",
      "url": "https://arxiv.org/abs/2602.02196",
      "date": "2026-02-03",
      "authors_text": "Hang Yan, Xinyu Che, Fangzhi Xu, Qiushi Sun, Zichen Ding",
      "is_highlight": false,
      "score": 68.0,
      "summary": "The TIDE framework evaluates Test-Time Improvement in LLM agents by analyzing task completion dynamics, recursive behaviors, and memory constraints to optimize agent-environment interactions.",
      "teaser_image": "https://arxiv.org/html/2602.02196/x1.png",
      "debug_abstract": "Recent advances in autonomous LLM agents demonstrate their ability to improve performance through iterative interaction with the environment. We define this paradigm as Test-Time Improvement (TTI). However, the mechanisms under how and why TTI succeed or fail remain poorly understood, and existing evaluation metrics fail to capture their task optimization efficiency, behavior adaptation after erroneous actions, and the specific utility of working memory for task completion. To address these gaps, we propose Test-time Improvement Diagnostic Evaluation (TIDE), an agent-agnostic and environment-agnostic framework that decomposes TTI into three comprehensive and interconnected dimensions. The framework measures (1) the overall temporal dynamics of task completion and (2) identifies whether performance is primarily constrained by recursive looping behaviors or (3) by burdensome accumulated memory. Through extensive experiments across diverse agents and environments, TIDE highlights that improving agent performance requires more than scaling internal reasoning, calling for explicitly optimizing the interaction dynamics between the agent and the environment."
    }
  ],
  "Synteraction Lab": [
    {
      "title": "A Low-Cost Vision-Based Tactile Gripper with Pretraining Learning for Contact-Rich Manipulation",
      "url": "https://arxiv.org/abs/2602.00514",
      "date": "2026-02-03",
      "authors_text": "Yaohua Liu, Binkai Ou, Zicheng Qiu, Ce Hao, Hengjun Zhang",
      "is_highlight": false,
      "score": 93.0,
      "summary": "The LVTG gripper enhances contact-rich manipulation by integrating low-cost visuo-tactile sensing with pretraining, improving grasp stability and efficiency for larger objects.",
      "teaser_image": "https://arxiv.org/html/2602.00514/x1.png",
      "debug_abstract": "Robotic manipulation in contact-rich environments remains challenging, particularly when relying on conventional tactile sensors that suffer from limited sensing range, reliability, and cost-effectiveness. In this work, we present LVTG, a low-cost visuo-tactile gripper designed for stable, robust, and efficient physical interaction. Unlike existing visuo-tactile sensors, LVTG enables more effective and stable grasping of larger and heavier everyday objects, thanks to its enhanced tactile sensing area and greater opening angle. Its surface skin is made of highly wear-resistant material, significantly improving durability and extending operational lifespan. The integration of vision and tactile feedback allows LVTG to provide rich, high-fidelity sensory data, facilitating reliable perception during complex manipulation tasks. Furthermore, LVTG features a modular design that supports rapid maintenance and replacement. To effectively fuse vision and touch, We adopt a CLIP-inspired contrastive learning objective to align tactile embeddings with their corresponding visual observations, enabling a shared cross-modal representation space for visuo-tactile perception. This alignment improves the performance of an Action Chunking Transformer (ACT) policy in contact-rich manipulation, leading to more efficient data collection and more effective policy learning. Compared to the original ACT method, the proposed LVTG with pretraining achieves significantly higher success rates in manipulation tasks."
    },
    {
      "title": "PWAVEP: Purifying Imperceptible Adversarial Perturbations in 3D Point Clouds via Spectral Graph Wavelets",
      "url": "https://arxiv.org/abs/2602.03333",
      "date": "2026-02-03",
      "authors_text": "Haoran Li, Renyang Liu, Hongjia Liu, Chen Wang, Long Yin",
      "is_highlight": false,
      "score": 59.0,
      "summary": "PWAVEP introduces a non-invasive defense mechanism using spectral graph wavelets to purify adversarial perturbations in 3D point clouds, enhancing accuracy and robustness.",
      "teaser_image": "https://arxiv.org/html/2602.03333/x1.png",
      "debug_abstract": "Recent progress in adversarial attacks on 3D point clouds, particularly in achieving spatial imperceptibility and high attack performance, presents significant challenges for defenders. Current defensive approaches remain cumbersome, often requiring invasive model modifications, expensive training procedures or auxiliary data access. To address these threats, in this paper, we propose a plug-and-play and non-invasive defense mechanism in the spectral domain, grounded in a theoretical and empirical analysis of the relationship between imperceptible perturbations and high-frequency spectral components. Building upon these insights, we introduce a novel purification framework, termed PWAVEP, which begins by computing a spectral graph wavelet domain saliency score and local sparsity score for each point. Guided by these values, PWAVEP adopts a hierarchical strategy, it eliminates the most salient points, which are identified as hardly recoverable adversarial outliers. Simultaneously, it applies a spectral filtering process to a broader set of moderately salient points. This process leverages a graph wavelet transform to attenuate high-frequency coefficients associated with the targeted points, thereby effectively suppressing adversarial noise. Extensive evaluations demonstrate that the proposed PWAVEP achieves superior accuracy and robustness compared to existing approaches, advancing the state-of-the-art in 3D point cloud purification. Code and datasets are available at this https URL"
    },
    {
      "title": "FARTrack: Fast Autoregressive Visual Tracking with High Performance",
      "url": "https://arxiv.org/abs/2602.03214",
      "date": "2026-02-03",
      "authors_text": "Guijie Wang, Tong Lin, Yifan Bai, Anjia Cao, Shiyi Liang",
      "is_highlight": false,
      "score": 79.0,
      "summary": "FARTrack is a fast autoregressive visual tracking framework that enhances inference speed and performance through task-specific self-distillation and inter-frame autoregressive sparsification, achieving 343 FPS on GPU.",
      "teaser_image": "https://arxiv.org/html/2602.03214/x1.png",
      "debug_abstract": "Inference speed and tracking performance are two critical evaluation metrics in the field of visual tracking. However, high-performance trackers often suffer from slow processing speeds, making them impractical for deployment on resource-constrained devices. To alleviate this issue, we propose FARTrack, a Fast Auto-Regressive Tracking framework. Since autoregression emphasizes the temporal nature of the trajectory sequence, it can maintain high performance while achieving efficient execution across various devices. FARTrack introduces Task-Specific Self-Distillation and Inter-frame Autoregressive Sparsification, designed from the perspectives of shallow-yet-accurate distillation and redundant-to-essential token optimization, respectively. Task-Specific Self-Distillation achieves model compression by distilling task-specific tokens layer by layer, enhancing the model&#39;s inference speed while avoiding suboptimal manual teacher-student layer pairs assignments. Meanwhile, Inter-frame Autoregressive Sparsification sequentially condenses multiple templates, avoiding additional runtime overhead while learning a temporally-global optimal sparsification strategy. FARTrack demonstrates outstanding speed and competitive performance. It delivers an AO of 70.6% on GOT-10k in real-time. Beyond, our fastest model achieves a speed of 343 FPS on the GPU and 121 FPS on the CPU."
    },
    {
      "title": "TIDE: Trajectory-based Diagnostic Evaluation of Test-Time Improvement in LLM Agents",
      "url": "https://arxiv.org/abs/2602.02196",
      "date": "2026-02-03",
      "authors_text": "Hang Yan, Xinyu Che, Fangzhi Xu, Qiushi Sun, Zichen Ding",
      "is_highlight": false,
      "score": 68.0,
      "summary": "The TIDE framework evaluates Test-Time Improvement in LLM agents by analyzing task completion dynamics, recursive behaviors, and memory constraints to optimize agent-environment interactions.",
      "teaser_image": "https://arxiv.org/html/2602.02196/x1.png",
      "debug_abstract": "Recent advances in autonomous LLM agents demonstrate their ability to improve performance through iterative interaction with the environment. We define this paradigm as Test-Time Improvement (TTI). However, the mechanisms under how and why TTI succeed or fail remain poorly understood, and existing evaluation metrics fail to capture their task optimization efficiency, behavior adaptation after erroneous actions, and the specific utility of working memory for task completion. To address these gaps, we propose Test-time Improvement Diagnostic Evaluation (TIDE), an agent-agnostic and environment-agnostic framework that decomposes TTI into three comprehensive and interconnected dimensions. The framework measures (1) the overall temporal dynamics of task completion and (2) identifies whether performance is primarily constrained by recursive looping behaviors or (3) by burdensome accumulated memory. Through extensive experiments across diverse agents and environments, TIDE highlights that improving agent performance requires more than scaling internal reasoning, calling for explicitly optimizing the interaction dynamics between the agent and the environment."
    }
  ],
  "上海人工智能实验室": [
    {
      "title": "TIDE: Trajectory-based Diagnostic Evaluation of Test-Time Improvement in LLM Agents",
      "url": "https://arxiv.org/abs/2602.02196",
      "date": "2026-02-03",
      "authors_text": "Hang Yan, Xinyu Che, Fangzhi Xu, Qiushi Sun, Zichen Ding",
      "is_highlight": false,
      "score": 68.0,
      "summary": "The TIDE framework evaluates Test-Time Improvement in LLM agents by analyzing task completion dynamics, recursive behaviors, and memory constraints to optimize agent-environment interactions.",
      "teaser_image": "https://arxiv.org/html/2602.02196/x1.png",
      "debug_abstract": "Recent advances in autonomous LLM agents demonstrate their ability to improve performance through iterative interaction with the environment. We define this paradigm as Test-Time Improvement (TTI). However, the mechanisms under how and why TTI succeed or fail remain poorly understood, and existing evaluation metrics fail to capture their task optimization efficiency, behavior adaptation after erroneous actions, and the specific utility of working memory for task completion. To address these gaps, we propose Test-time Improvement Diagnostic Evaluation (TIDE), an agent-agnostic and environment-agnostic framework that decomposes TTI into three comprehensive and interconnected dimensions. The framework measures (1) the overall temporal dynamics of task completion and (2) identifies whether performance is primarily constrained by recursive looping behaviors or (3) by burdensome accumulated memory. Through extensive experiments across diverse agents and environments, TIDE highlights that improving agent performance requires more than scaling internal reasoning, calling for explicitly optimizing the interaction dynamics between the agent and the environment."
    }
  ],
  "Advanced Robotics Centre": [
    {
      "title": "A Low-Cost Vision-Based Tactile Gripper with Pretraining Learning for Contact-Rich Manipulation",
      "url": "https://arxiv.org/abs/2602.00514",
      "date": "2026-02-03",
      "authors_text": "Yaohua Liu, Binkai Ou, Zicheng Qiu, Ce Hao, Hengjun Zhang",
      "is_highlight": false,
      "score": 93.0,
      "summary": "The LVTG gripper enhances contact-rich manipulation by integrating low-cost visuo-tactile sensing with pretraining, improving grasp stability and efficiency for larger objects.",
      "teaser_image": "https://arxiv.org/html/2602.00514/x1.png",
      "debug_abstract": "Robotic manipulation in contact-rich environments remains challenging, particularly when relying on conventional tactile sensors that suffer from limited sensing range, reliability, and cost-effectiveness. In this work, we present LVTG, a low-cost visuo-tactile gripper designed for stable, robust, and efficient physical interaction. Unlike existing visuo-tactile sensors, LVTG enables more effective and stable grasping of larger and heavier everyday objects, thanks to its enhanced tactile sensing area and greater opening angle. Its surface skin is made of highly wear-resistant material, significantly improving durability and extending operational lifespan. The integration of vision and tactile feedback allows LVTG to provide rich, high-fidelity sensory data, facilitating reliable perception during complex manipulation tasks. Furthermore, LVTG features a modular design that supports rapid maintenance and replacement. To effectively fuse vision and touch, We adopt a CLIP-inspired contrastive learning objective to align tactile embeddings with their corresponding visual observations, enabling a shared cross-modal representation space for visuo-tactile perception. This alignment improves the performance of an Action Chunking Transformer (ACT) policy in contact-rich manipulation, leading to more efficient data collection and more effective policy learning. Compared to the original ACT method, the proposed LVTG with pretraining achieves significantly higher success rates in manipulation tasks."
    },
    {
      "title": "PWAVEP: Purifying Imperceptible Adversarial Perturbations in 3D Point Clouds via Spectral Graph Wavelets",
      "url": "https://arxiv.org/abs/2602.03333",
      "date": "2026-02-03",
      "authors_text": "Haoran Li, Renyang Liu, Hongjia Liu, Chen Wang, Long Yin",
      "is_highlight": false,
      "score": 59.0,
      "summary": "PWAVEP introduces a non-invasive defense mechanism using spectral graph wavelets to purify adversarial perturbations in 3D point clouds, enhancing accuracy and robustness.",
      "teaser_image": "https://arxiv.org/html/2602.03333/x1.png",
      "debug_abstract": "Recent progress in adversarial attacks on 3D point clouds, particularly in achieving spatial imperceptibility and high attack performance, presents significant challenges for defenders. Current defensive approaches remain cumbersome, often requiring invasive model modifications, expensive training procedures or auxiliary data access. To address these threats, in this paper, we propose a plug-and-play and non-invasive defense mechanism in the spectral domain, grounded in a theoretical and empirical analysis of the relationship between imperceptible perturbations and high-frequency spectral components. Building upon these insights, we introduce a novel purification framework, termed PWAVEP, which begins by computing a spectral graph wavelet domain saliency score and local sparsity score for each point. Guided by these values, PWAVEP adopts a hierarchical strategy, it eliminates the most salient points, which are identified as hardly recoverable adversarial outliers. Simultaneously, it applies a spectral filtering process to a broader set of moderately salient points. This process leverages a graph wavelet transform to attenuate high-frequency coefficients associated with the targeted points, thereby effectively suppressing adversarial noise. Extensive evaluations demonstrate that the proposed PWAVEP achieves superior accuracy and robustness compared to existing approaches, advancing the state-of-the-art in 3D point cloud purification. Code and datasets are available at this https URL"
    },
    {
      "title": "FARTrack: Fast Autoregressive Visual Tracking with High Performance",
      "url": "https://arxiv.org/abs/2602.03214",
      "date": "2026-02-03",
      "authors_text": "Guijie Wang, Tong Lin, Yifan Bai, Anjia Cao, Shiyi Liang",
      "is_highlight": false,
      "score": 79.0,
      "summary": "FARTrack is a fast autoregressive visual tracking framework that enhances inference speed and performance through task-specific self-distillation and inter-frame autoregressive sparsification, achieving 343 FPS on GPU.",
      "teaser_image": "https://arxiv.org/html/2602.03214/x1.png",
      "debug_abstract": "Inference speed and tracking performance are two critical evaluation metrics in the field of visual tracking. However, high-performance trackers often suffer from slow processing speeds, making them impractical for deployment on resource-constrained devices. To alleviate this issue, we propose FARTrack, a Fast Auto-Regressive Tracking framework. Since autoregression emphasizes the temporal nature of the trajectory sequence, it can maintain high performance while achieving efficient execution across various devices. FARTrack introduces Task-Specific Self-Distillation and Inter-frame Autoregressive Sparsification, designed from the perspectives of shallow-yet-accurate distillation and redundant-to-essential token optimization, respectively. Task-Specific Self-Distillation achieves model compression by distilling task-specific tokens layer by layer, enhancing the model&#39;s inference speed while avoiding suboptimal manual teacher-student layer pairs assignments. Meanwhile, Inter-frame Autoregressive Sparsification sequentially condenses multiple templates, avoiding additional runtime overhead while learning a temporally-global optimal sparsification strategy. FARTrack demonstrates outstanding speed and competitive performance. It delivers an AO of 70.6% on GOT-10k in real-time. Beyond, our fastest model achieves a speed of 343 FPS on the GPU and 121 FPS on the CPU."
    },
    {
      "title": "TIDE: Trajectory-based Diagnostic Evaluation of Test-Time Improvement in LLM Agents",
      "url": "https://arxiv.org/abs/2602.02196",
      "date": "2026-02-03",
      "authors_text": "Hang Yan, Xinyu Che, Fangzhi Xu, Qiushi Sun, Zichen Ding",
      "is_highlight": false,
      "score": 68.0,
      "summary": "The TIDE framework evaluates Test-Time Improvement in LLM agents by analyzing task completion dynamics, recursive behaviors, and memory constraints to optimize agent-environment interactions.",
      "teaser_image": "https://arxiv.org/html/2602.02196/x1.png",
      "debug_abstract": "Recent advances in autonomous LLM agents demonstrate their ability to improve performance through iterative interaction with the environment. We define this paradigm as Test-Time Improvement (TTI). However, the mechanisms under how and why TTI succeed or fail remain poorly understood, and existing evaluation metrics fail to capture their task optimization efficiency, behavior adaptation after erroneous actions, and the specific utility of working memory for task completion. To address these gaps, we propose Test-time Improvement Diagnostic Evaluation (TIDE), an agent-agnostic and environment-agnostic framework that decomposes TTI into three comprehensive and interconnected dimensions. The framework measures (1) the overall temporal dynamics of task completion and (2) identifies whether performance is primarily constrained by recursive looping behaviors or (3) by burdensome accumulated memory. Through extensive experiments across diverse agents and environments, TIDE highlights that improving agent performance requires more than scaling internal reasoning, calling for explicitly optimizing the interaction dynamics between the agent and the environment."
    }
  ],
  "Microsystem Engineering and Robotics": [
    {
      "title": "A Low-Cost Vision-Based Tactile Gripper with Pretraining Learning for Contact-Rich Manipulation",
      "url": "https://arxiv.org/abs/2602.00514",
      "date": "2026-02-03",
      "authors_text": "Yaohua Liu, Binkai Ou, Zicheng Qiu, Ce Hao, Hengjun Zhang",
      "is_highlight": false,
      "score": 93.0,
      "summary": "The LVTG gripper enhances contact-rich manipulation by integrating low-cost visuo-tactile sensing with pretraining, improving grasp stability and efficiency for larger objects.",
      "teaser_image": "https://arxiv.org/html/2602.00514/x1.png",
      "debug_abstract": "Robotic manipulation in contact-rich environments remains challenging, particularly when relying on conventional tactile sensors that suffer from limited sensing range, reliability, and cost-effectiveness. In this work, we present LVTG, a low-cost visuo-tactile gripper designed for stable, robust, and efficient physical interaction. Unlike existing visuo-tactile sensors, LVTG enables more effective and stable grasping of larger and heavier everyday objects, thanks to its enhanced tactile sensing area and greater opening angle. Its surface skin is made of highly wear-resistant material, significantly improving durability and extending operational lifespan. The integration of vision and tactile feedback allows LVTG to provide rich, high-fidelity sensory data, facilitating reliable perception during complex manipulation tasks. Furthermore, LVTG features a modular design that supports rapid maintenance and replacement. To effectively fuse vision and touch, We adopt a CLIP-inspired contrastive learning objective to align tactile embeddings with their corresponding visual observations, enabling a shared cross-modal representation space for visuo-tactile perception. This alignment improves the performance of an Action Chunking Transformer (ACT) policy in contact-rich manipulation, leading to more efficient data collection and more effective policy learning. Compared to the original ACT method, the proposed LVTG with pretraining achieves significantly higher success rates in manipulation tasks."
    },
    {
      "title": "PWAVEP: Purifying Imperceptible Adversarial Perturbations in 3D Point Clouds via Spectral Graph Wavelets",
      "url": "https://arxiv.org/abs/2602.03333",
      "date": "2026-02-03",
      "authors_text": "Haoran Li, Renyang Liu, Hongjia Liu, Chen Wang, Long Yin",
      "is_highlight": false,
      "score": 59.0,
      "summary": "PWAVEP introduces a non-invasive defense mechanism using spectral graph wavelets to purify adversarial perturbations in 3D point clouds, enhancing accuracy and robustness.",
      "teaser_image": "https://arxiv.org/html/2602.03333/x1.png",
      "debug_abstract": "Recent progress in adversarial attacks on 3D point clouds, particularly in achieving spatial imperceptibility and high attack performance, presents significant challenges for defenders. Current defensive approaches remain cumbersome, often requiring invasive model modifications, expensive training procedures or auxiliary data access. To address these threats, in this paper, we propose a plug-and-play and non-invasive defense mechanism in the spectral domain, grounded in a theoretical and empirical analysis of the relationship between imperceptible perturbations and high-frequency spectral components. Building upon these insights, we introduce a novel purification framework, termed PWAVEP, which begins by computing a spectral graph wavelet domain saliency score and local sparsity score for each point. Guided by these values, PWAVEP adopts a hierarchical strategy, it eliminates the most salient points, which are identified as hardly recoverable adversarial outliers. Simultaneously, it applies a spectral filtering process to a broader set of moderately salient points. This process leverages a graph wavelet transform to attenuate high-frequency coefficients associated with the targeted points, thereby effectively suppressing adversarial noise. Extensive evaluations demonstrate that the proposed PWAVEP achieves superior accuracy and robustness compared to existing approaches, advancing the state-of-the-art in 3D point cloud purification. Code and datasets are available at this https URL"
    },
    {
      "title": "FARTrack: Fast Autoregressive Visual Tracking with High Performance",
      "url": "https://arxiv.org/abs/2602.03214",
      "date": "2026-02-03",
      "authors_text": "Guijie Wang, Tong Lin, Yifan Bai, Anjia Cao, Shiyi Liang",
      "is_highlight": false,
      "score": 79.0,
      "summary": "FARTrack is a fast autoregressive visual tracking framework that enhances inference speed and performance through task-specific self-distillation and inter-frame autoregressive sparsification, achieving 343 FPS on GPU.",
      "teaser_image": "https://arxiv.org/html/2602.03214/x1.png",
      "debug_abstract": "Inference speed and tracking performance are two critical evaluation metrics in the field of visual tracking. However, high-performance trackers often suffer from slow processing speeds, making them impractical for deployment on resource-constrained devices. To alleviate this issue, we propose FARTrack, a Fast Auto-Regressive Tracking framework. Since autoregression emphasizes the temporal nature of the trajectory sequence, it can maintain high performance while achieving efficient execution across various devices. FARTrack introduces Task-Specific Self-Distillation and Inter-frame Autoregressive Sparsification, designed from the perspectives of shallow-yet-accurate distillation and redundant-to-essential token optimization, respectively. Task-Specific Self-Distillation achieves model compression by distilling task-specific tokens layer by layer, enhancing the model&#39;s inference speed while avoiding suboptimal manual teacher-student layer pairs assignments. Meanwhile, Inter-frame Autoregressive Sparsification sequentially condenses multiple templates, avoiding additional runtime overhead while learning a temporally-global optimal sparsification strategy. FARTrack demonstrates outstanding speed and competitive performance. It delivers an AO of 70.6% on GOT-10k in real-time. Beyond, our fastest model achieves a speed of 343 FPS on the GPU and 121 FPS on the CPU."
    },
    {
      "title": "TIDE: Trajectory-based Diagnostic Evaluation of Test-Time Improvement in LLM Agents",
      "url": "https://arxiv.org/abs/2602.02196",
      "date": "2026-02-03",
      "authors_text": "Hang Yan, Xinyu Che, Fangzhi Xu, Qiushi Sun, Zichen Ding",
      "is_highlight": false,
      "score": 68.0,
      "summary": "The TIDE framework evaluates Test-Time Improvement in LLM agents by analyzing task completion dynamics, recursive behaviors, and memory constraints to optimize agent-environment interactions.",
      "teaser_image": "https://arxiv.org/html/2602.02196/x1.png",
      "debug_abstract": "Recent advances in autonomous LLM agents demonstrate their ability to improve performance through iterative interaction with the environment. We define this paradigm as Test-Time Improvement (TTI). However, the mechanisms under how and why TTI succeed or fail remain poorly understood, and existing evaluation metrics fail to capture their task optimization efficiency, behavior adaptation after erroneous actions, and the specific utility of working memory for task completion. To address these gaps, we propose Test-time Improvement Diagnostic Evaluation (TIDE), an agent-agnostic and environment-agnostic framework that decomposes TTI into three comprehensive and interconnected dimensions. The framework measures (1) the overall temporal dynamics of task completion and (2) identifies whether performance is primarily constrained by recursive looping behaviors or (3) by burdensome accumulated memory. Through extensive experiments across diverse agents and environments, TIDE highlights that improving agent performance requires more than scaling internal reasoning, calling for explicitly optimizing the interaction dynamics between the agent and the environment."
    }
  ],
  "NUS AI LAB": [
    {
      "title": "A Low-Cost Vision-Based Tactile Gripper with Pretraining Learning for Contact-Rich Manipulation",
      "url": "https://arxiv.org/abs/2602.00514",
      "date": "2026-02-03",
      "authors_text": "Yaohua Liu, Binkai Ou, Zicheng Qiu, Ce Hao, Hengjun Zhang",
      "is_highlight": false,
      "score": 93.0,
      "summary": "The LVTG gripper enhances contact-rich manipulation by integrating low-cost visuo-tactile sensing with pretraining, improving grasp stability and efficiency for larger objects.",
      "teaser_image": "https://arxiv.org/html/2602.00514/x1.png",
      "debug_abstract": "Robotic manipulation in contact-rich environments remains challenging, particularly when relying on conventional tactile sensors that suffer from limited sensing range, reliability, and cost-effectiveness. In this work, we present LVTG, a low-cost visuo-tactile gripper designed for stable, robust, and efficient physical interaction. Unlike existing visuo-tactile sensors, LVTG enables more effective and stable grasping of larger and heavier everyday objects, thanks to its enhanced tactile sensing area and greater opening angle. Its surface skin is made of highly wear-resistant material, significantly improving durability and extending operational lifespan. The integration of vision and tactile feedback allows LVTG to provide rich, high-fidelity sensory data, facilitating reliable perception during complex manipulation tasks. Furthermore, LVTG features a modular design that supports rapid maintenance and replacement. To effectively fuse vision and touch, We adopt a CLIP-inspired contrastive learning objective to align tactile embeddings with their corresponding visual observations, enabling a shared cross-modal representation space for visuo-tactile perception. This alignment improves the performance of an Action Chunking Transformer (ACT) policy in contact-rich manipulation, leading to more efficient data collection and more effective policy learning. Compared to the original ACT method, the proposed LVTG with pretraining achieves significantly higher success rates in manipulation tasks."
    },
    {
      "title": "PWAVEP: Purifying Imperceptible Adversarial Perturbations in 3D Point Clouds via Spectral Graph Wavelets",
      "url": "https://arxiv.org/abs/2602.03333",
      "date": "2026-02-03",
      "authors_text": "Haoran Li, Renyang Liu, Hongjia Liu, Chen Wang, Long Yin",
      "is_highlight": false,
      "score": 59.0,
      "summary": "PWAVEP introduces a non-invasive defense mechanism using spectral graph wavelets to purify adversarial perturbations in 3D point clouds, enhancing accuracy and robustness.",
      "teaser_image": "https://arxiv.org/html/2602.03333/x1.png",
      "debug_abstract": "Recent progress in adversarial attacks on 3D point clouds, particularly in achieving spatial imperceptibility and high attack performance, presents significant challenges for defenders. Current defensive approaches remain cumbersome, often requiring invasive model modifications, expensive training procedures or auxiliary data access. To address these threats, in this paper, we propose a plug-and-play and non-invasive defense mechanism in the spectral domain, grounded in a theoretical and empirical analysis of the relationship between imperceptible perturbations and high-frequency spectral components. Building upon these insights, we introduce a novel purification framework, termed PWAVEP, which begins by computing a spectral graph wavelet domain saliency score and local sparsity score for each point. Guided by these values, PWAVEP adopts a hierarchical strategy, it eliminates the most salient points, which are identified as hardly recoverable adversarial outliers. Simultaneously, it applies a spectral filtering process to a broader set of moderately salient points. This process leverages a graph wavelet transform to attenuate high-frequency coefficients associated with the targeted points, thereby effectively suppressing adversarial noise. Extensive evaluations demonstrate that the proposed PWAVEP achieves superior accuracy and robustness compared to existing approaches, advancing the state-of-the-art in 3D point cloud purification. Code and datasets are available at this https URL"
    },
    {
      "title": "FARTrack: Fast Autoregressive Visual Tracking with High Performance",
      "url": "https://arxiv.org/abs/2602.03214",
      "date": "2026-02-03",
      "authors_text": "Guijie Wang, Tong Lin, Yifan Bai, Anjia Cao, Shiyi Liang",
      "is_highlight": false,
      "score": 79.0,
      "summary": "FARTrack is a fast autoregressive visual tracking framework that enhances inference speed and performance through task-specific self-distillation and inter-frame autoregressive sparsification, achieving 343 FPS on GPU.",
      "teaser_image": "https://arxiv.org/html/2602.03214/x1.png",
      "debug_abstract": "Inference speed and tracking performance are two critical evaluation metrics in the field of visual tracking. However, high-performance trackers often suffer from slow processing speeds, making them impractical for deployment on resource-constrained devices. To alleviate this issue, we propose FARTrack, a Fast Auto-Regressive Tracking framework. Since autoregression emphasizes the temporal nature of the trajectory sequence, it can maintain high performance while achieving efficient execution across various devices. FARTrack introduces Task-Specific Self-Distillation and Inter-frame Autoregressive Sparsification, designed from the perspectives of shallow-yet-accurate distillation and redundant-to-essential token optimization, respectively. Task-Specific Self-Distillation achieves model compression by distilling task-specific tokens layer by layer, enhancing the model&#39;s inference speed while avoiding suboptimal manual teacher-student layer pairs assignments. Meanwhile, Inter-frame Autoregressive Sparsification sequentially condenses multiple templates, avoiding additional runtime overhead while learning a temporally-global optimal sparsification strategy. FARTrack demonstrates outstanding speed and competitive performance. It delivers an AO of 70.6% on GOT-10k in real-time. Beyond, our fastest model achieves a speed of 343 FPS on the GPU and 121 FPS on the CPU."
    },
    {
      "title": "TIDE: Trajectory-based Diagnostic Evaluation of Test-Time Improvement in LLM Agents",
      "url": "https://arxiv.org/abs/2602.02196",
      "date": "2026-02-03",
      "authors_text": "Hang Yan, Xinyu Che, Fangzhi Xu, Qiushi Sun, Zichen Ding",
      "is_highlight": false,
      "score": 68.0,
      "summary": "The TIDE framework evaluates Test-Time Improvement in LLM agents by analyzing task completion dynamics, recursive behaviors, and memory constraints to optimize agent-environment interactions.",
      "teaser_image": "https://arxiv.org/html/2602.02196/x1.png",
      "debug_abstract": "Recent advances in autonomous LLM agents demonstrate their ability to improve performance through iterative interaction with the environment. We define this paradigm as Test-Time Improvement (TTI). However, the mechanisms under how and why TTI succeed or fail remain poorly understood, and existing evaluation metrics fail to capture their task optimization efficiency, behavior adaptation after erroneous actions, and the specific utility of working memory for task completion. To address these gaps, we propose Test-time Improvement Diagnostic Evaluation (TIDE), an agent-agnostic and environment-agnostic framework that decomposes TTI into three comprehensive and interconnected dimensions. The framework measures (1) the overall temporal dynamics of task completion and (2) identifies whether performance is primarily constrained by recursive looping behaviors or (3) by burdensome accumulated memory. Through extensive experiments across diverse agents and environments, TIDE highlights that improving agent performance requires more than scaling internal reasoning, calling for explicitly optimizing the interaction dynamics between the agent and the environment."
    }
  ],
  "卡内基-机器人研究所": [
    {
      "title": "Simulating Human Audiovisual Search Behavior",
      "url": "https://arxiv.org/abs/2602.02790",
      "date": "2026-02-02",
      "authors_text": "Hyunsung Cho, Xuejing Luo, Byungjoo Lee, David Lindlbauer, Antti Oulasvirta",
      "is_highlight": true,
      "score": 77.0,
      "summary": "The paper introduces Sensonaut, a computational model simulating human audiovisual search behavior by optimizing movement and sensory strategies under uncertainty to enhance target location efficiency.",
      "teaser_image": "https://arxiv.org/html/2602.02790/x2.png",
      "debug_abstract": "Locating a target based on auditory and visual cues$\\unicode{x2013}$such as finding a car in a crowded parking lot or identifying a speaker in a virtual meeting$\\unicode{x2013}$requires balancing effort, time, and accuracy under uncertainty. Existing models of audiovisual search often treat perception and action in isolation, overlooking how people adaptively coordinate movement and sensory strategies. We present Sensonaut, a computational model of embodied audiovisual search. The core assumption is that people deploy their body and sensory systems in ways they believe will most efficiently improve their chances of locating a target, trading off time and effort under perceptual constraints. Our model formulates this as a resource-rational decision-making problem under partial observability. We validate the model against newly collected human data, showing that it reproduces both adaptive scaling of search time and effort under task complexity, occlusion, and distraction, and characteristic human errors. Our simulation of human-like resource-rational search informs the design of audiovisual interfaces that minimize search cost and cognitive load."
    },
    {
      "title": "Hierarchical Entity-centric Reinforcement Learning with Factored Subgoal Diffusion",
      "url": "https://arxiv.org/abs/2602.02722",
      "date": "2026-02-02",
      "authors_text": "Dan Haramati, Carl Qi, Tal Daniel, Amy Zhang, Aviv Tamar",
      "is_highlight": false,
      "score": 74.0,
      "summary": "This paper presents a hierarchical entity-centric framework for offline Goal-Conditioned Reinforcement Learning that enhances performance in multi-entity long-horizon tasks through modular subgoal generation.",
      "teaser_image": "https://arxiv.org/html/2602.02722/figs/alg1_illustration_blue.png",
      "debug_abstract": "We propose a hierarchical entity-centric framework for offline Goal-Conditioned Reinforcement Learning (GCRL) that combines subgoal decomposition with factored structure to solve long-horizon tasks in domains with multiple entities. Achieving long-horizon goals in complex environments remains a core challenge in Reinforcement Learning (RL). Domains with multiple entities are particularly difficult due to their combinatorial complexity. GCRL facilitates generalization across goals and the use of subgoal structure, but struggles with high-dimensional observations and combinatorial state-spaces, especially under sparse reward. We employ a two-level hierarchy composed of a value-based GCRL agent and a factored subgoal-generating conditional diffusion model. The RL agent and subgoal generator are trained independently and composed post hoc through selective subgoal generation based on the value function, making the approach modular and compatible with existing GCRL algorithms. We introduce new variations to benchmark tasks that highlight the challenges of multi-entity domains, and show that our method consistently boosts performance of the underlying RL agent on image-based long-horizon tasks with sparse rewards, achieving over 150% higher success rates on the hardest task in our suite and generalizing to increasing horizons and numbers of entities. Rollout videos are provided at: this https URL"
    }
  ],
  "斯坦福-视觉与学习实验室 (SVL)": [
    {
      "title": "Accelerating Structured Chain-of-Thought in Autonomous Vehicles",
      "url": "https://arxiv.org/abs/2602.02864",
      "date": "2026-02-02",
      "authors_text": "Yi Gu, Yan Wang, Yuxiao Chen, Yurong You, Wenjie Luo",
      "is_highlight": false,
      "score": 76.0,
      "summary": "FastDriveCoT accelerates Chain-of-Thought reasoning in autonomous vehicles by enabling parallel decoding of sub-tasks, achieving 3-4× speedup while maintaining decision-making improvements.",
      "teaser_image": "https://arxiv.org/html/2602.02864/x2.png",
      "debug_abstract": "Chain-of-Thought (CoT) reasoning enhances the decision-making capabilities of vision-language-action models in autonomous driving, but its autoregressive nature introduces significant inference latency, making it impractical for real-time applications. To address this, we introduce FastDriveCoT, a novel parallel decoding method that accelerates template-structured CoT. Our approach decomposes the reasoning process into a dependency graph of distinct sub-tasks, such as identifying critical objects and summarizing traffic rules, some of which can be generated in parallel. By generating multiple independent reasoning steps concurrently within a single forward pass, we significantly reduce the number of sequential computations. Experiments demonstrate a 3-4$\\times$ speedup in CoT generation and a substantial reduction in end-to-end latency across various model architectures, all while preserving the original downstream task improvements brought by incorporating CoT reasoning."
    }
  ],
  "斯坦福-机器人与具身智能实验室 (REAL)": [
    {
      "title": "Accelerating Structured Chain-of-Thought in Autonomous Vehicles",
      "url": "https://arxiv.org/abs/2602.02864",
      "date": "2026-02-02",
      "authors_text": "Yi Gu, Yan Wang, Yuxiao Chen, Yurong You, Wenjie Luo",
      "is_highlight": false,
      "score": 76.0,
      "summary": "FastDriveCoT accelerates Chain-of-Thought reasoning in autonomous vehicles by enabling parallel decoding of sub-tasks, achieving 3-4× speedup while maintaining decision-making improvements.",
      "teaser_image": "https://arxiv.org/html/2602.02864/x2.png",
      "debug_abstract": "Chain-of-Thought (CoT) reasoning enhances the decision-making capabilities of vision-language-action models in autonomous driving, but its autoregressive nature introduces significant inference latency, making it impractical for real-time applications. To address this, we introduce FastDriveCoT, a novel parallel decoding method that accelerates template-structured CoT. Our approach decomposes the reasoning process into a dependency graph of distinct sub-tasks, such as identifying critical objects and summarizing traffic rules, some of which can be generated in parallel. By generating multiple independent reasoning steps concurrently within a single forward pass, we significantly reduce the number of sequential computations. Experiments demonstrate a 3-4$\\times$ speedup in CoT generation and a substantial reduction in end-to-end latency across various model architectures, all while preserving the original downstream task improvements brought by incorporating CoT reasoning."
    }
  ],
  "斯坦福-人工智能实验室 (SAIL)": [
    {
      "title": "Accelerating Structured Chain-of-Thought in Autonomous Vehicles",
      "url": "https://arxiv.org/abs/2602.02864",
      "date": "2026-02-02",
      "authors_text": "Yi Gu, Yan Wang, Yuxiao Chen, Yurong You, Wenjie Luo",
      "is_highlight": false,
      "score": 76.0,
      "summary": "FastDriveCoT accelerates Chain-of-Thought reasoning in autonomous vehicles by enabling parallel decoding of sub-tasks, achieving 3-4× speedup while maintaining decision-making improvements.",
      "teaser_image": "https://arxiv.org/html/2602.02864/x2.png",
      "debug_abstract": "Chain-of-Thought (CoT) reasoning enhances the decision-making capabilities of vision-language-action models in autonomous driving, but its autoregressive nature introduces significant inference latency, making it impractical for real-time applications. To address this, we introduce FastDriveCoT, a novel parallel decoding method that accelerates template-structured CoT. Our approach decomposes the reasoning process into a dependency graph of distinct sub-tasks, such as identifying critical objects and summarizing traffic rules, some of which can be generated in parallel. By generating multiple independent reasoning steps concurrently within a single forward pass, we significantly reduce the number of sequential computations. Experiments demonstrate a 3-4$\\times$ speedup in CoT generation and a substantial reduction in end-to-end latency across various model architectures, all while preserving the original downstream task improvements brought by incorporating CoT reasoning."
    }
  ],
  "Rajpurkar Lab": [
    {
      "title": "Accelerating Structured Chain-of-Thought in Autonomous Vehicles",
      "url": "https://arxiv.org/abs/2602.02864",
      "date": "2026-02-02",
      "authors_text": "Yi Gu, Yan Wang, Yuxiao Chen, Yurong You, Wenjie Luo",
      "is_highlight": false,
      "score": 76.0,
      "summary": "FastDriveCoT accelerates Chain-of-Thought reasoning in autonomous vehicles by enabling parallel decoding of sub-tasks, achieving 3-4× speedup while maintaining decision-making improvements.",
      "teaser_image": "https://arxiv.org/html/2602.02864/x2.png",
      "debug_abstract": "Chain-of-Thought (CoT) reasoning enhances the decision-making capabilities of vision-language-action models in autonomous driving, but its autoregressive nature introduces significant inference latency, making it impractical for real-time applications. To address this, we introduce FastDriveCoT, a novel parallel decoding method that accelerates template-structured CoT. Our approach decomposes the reasoning process into a dependency graph of distinct sub-tasks, such as identifying critical objects and summarizing traffic rules, some of which can be generated in parallel. By generating multiple independent reasoning steps concurrently within a single forward pass, we significantly reduce the number of sequential computations. Experiments demonstrate a 3-4$\\times$ speedup in CoT generation and a substantial reduction in end-to-end latency across various model architectures, all while preserving the original downstream task improvements brought by incorporating CoT reasoning."
    }
  ],
  "Harvard Microrobotics Laboratory": [
    {
      "title": "Accelerating Structured Chain-of-Thought in Autonomous Vehicles",
      "url": "https://arxiv.org/abs/2602.02864",
      "date": "2026-02-02",
      "authors_text": "Yi Gu, Yan Wang, Yuxiao Chen, Yurong You, Wenjie Luo",
      "is_highlight": false,
      "score": 76.0,
      "summary": "FastDriveCoT accelerates Chain-of-Thought reasoning in autonomous vehicles by enabling parallel decoding of sub-tasks, achieving 3-4× speedup while maintaining decision-making improvements.",
      "teaser_image": "https://arxiv.org/html/2602.02864/x2.png",
      "debug_abstract": "Chain-of-Thought (CoT) reasoning enhances the decision-making capabilities of vision-language-action models in autonomous driving, but its autoregressive nature introduces significant inference latency, making it impractical for real-time applications. To address this, we introduce FastDriveCoT, a novel parallel decoding method that accelerates template-structured CoT. Our approach decomposes the reasoning process into a dependency graph of distinct sub-tasks, such as identifying critical objects and summarizing traffic rules, some of which can be generated in parallel. By generating multiple independent reasoning steps concurrently within a single forward pass, we significantly reduce the number of sequential computations. Experiments demonstrate a 3-4$\\times$ speedup in CoT generation and a substantial reduction in end-to-end latency across various model architectures, all while preserving the original downstream task improvements brought by incorporating CoT reasoning."
    }
  ],
  "Robotics and Embodied Artificial Intelligence Lab (REAL)": [
    {
      "title": "Accelerating Structured Chain-of-Thought in Autonomous Vehicles",
      "url": "https://arxiv.org/abs/2602.02864",
      "date": "2026-02-02",
      "authors_text": "Yi Gu, Yan Wang, Yuxiao Chen, Yurong You, Wenjie Luo",
      "is_highlight": false,
      "score": 76.0,
      "summary": "FastDriveCoT accelerates Chain-of-Thought reasoning in autonomous vehicles by enabling parallel decoding of sub-tasks, achieving 3-4× speedup while maintaining decision-making improvements.",
      "teaser_image": "https://arxiv.org/html/2602.02864/x2.png",
      "debug_abstract": "Chain-of-Thought (CoT) reasoning enhances the decision-making capabilities of vision-language-action models in autonomous driving, but its autoregressive nature introduces significant inference latency, making it impractical for real-time applications. To address this, we introduce FastDriveCoT, a novel parallel decoding method that accelerates template-structured CoT. Our approach decomposes the reasoning process into a dependency graph of distinct sub-tasks, such as identifying critical objects and summarizing traffic rules, some of which can be generated in parallel. By generating multiple independent reasoning steps concurrently within a single forward pass, we significantly reduce the number of sequential computations. Experiments demonstrate a 3-4$\\times$ speedup in CoT generation and a substantial reduction in end-to-end latency across various model architectures, all while preserving the original downstream task improvements brought by incorporating CoT reasoning."
    }
  ],
  "Stanford AI Lab (SAIL)": [
    {
      "title": "Accelerating Structured Chain-of-Thought in Autonomous Vehicles",
      "url": "https://arxiv.org/abs/2602.02864",
      "date": "2026-02-02",
      "authors_text": "Yi Gu, Yan Wang, Yuxiao Chen, Yurong You, Wenjie Luo",
      "is_highlight": false,
      "score": 76.0,
      "summary": "FastDriveCoT accelerates Chain-of-Thought reasoning in autonomous vehicles by enabling parallel decoding of sub-tasks, achieving 3-4× speedup while maintaining decision-making improvements.",
      "teaser_image": "https://arxiv.org/html/2602.02864/x2.png",
      "debug_abstract": "Chain-of-Thought (CoT) reasoning enhances the decision-making capabilities of vision-language-action models in autonomous driving, but its autoregressive nature introduces significant inference latency, making it impractical for real-time applications. To address this, we introduce FastDriveCoT, a novel parallel decoding method that accelerates template-structured CoT. Our approach decomposes the reasoning process into a dependency graph of distinct sub-tasks, such as identifying critical objects and summarizing traffic rules, some of which can be generated in parallel. By generating multiple independent reasoning steps concurrently within a single forward pass, we significantly reduce the number of sequential computations. Experiments demonstrate a 3-4$\\times$ speedup in CoT generation and a substantial reduction in end-to-end latency across various model architectures, all while preserving the original downstream task improvements brought by incorporating CoT reasoning."
    }
  ],
  "Berkeley Artificial Intelligence Research Lab (BAIR)": [
    {
      "title": "HetroD: A High-Fidelity Drone Dataset and Benchmark for Autonomous Driving in Heterogeneous Traffic",
      "url": "https://arxiv.org/abs/2602.03447",
      "date": "2026-02-03",
      "authors_text": "Yu-Hsiang Chen, Wei-Jer Chang, Christian Kotulla, Thomas Keutgens, Steffen Runde",
      "is_highlight": false,
      "score": 92.0,
      "summary": "HetroD is a comprehensive drone-based dataset designed to enhance autonomous driving systems by addressing the complexities of heterogeneous traffic involving vulnerable road users.",
      "teaser_image": "https://arxiv.org/html/2602.03447/x3.png",
      "debug_abstract": "We present HetroD, a dataset and benchmark for developing autonomous driving systems in heterogeneous environments. HetroD targets the critical challenge of navi- gating real-world heterogeneous traffic dominated by vulner- able road users (VRUs), including pedestrians, cyclists, and motorcyclists that interact with vehicles. These mixed agent types exhibit complex behaviors such as hook turns, lane splitting, and informal right-of-way negotiation. Such behaviors pose significant challenges for autonomous vehicles but remain underrepresented in existing datasets focused on structured, lane-disciplined traffic. To bridge the gap, we collect a large- scale drone-based dataset to provide a holistic observation of traffic scenes with centimeter-accurate annotations, HD maps, and traffic signal states. We further develop a modular toolkit for extracting per-agent scenarios to support downstream task development. In total, the dataset comprises over 65.4k high- fidelity agent trajectories, 70% of which are from VRUs. HetroD supports modeling of VRU behaviors in dense, het- erogeneous traffic and provides standardized benchmarks for forecasting, planning, and simulation tasks. Evaluation results reveal that state-of-the-art prediction and planning models struggle with the challenges presented by our dataset: they fail to predict lateral VRU movements, cannot handle unstructured maneuvers, and exhibit limited performance in dense and multi-agent scenarios, highlighting the need for more robust approaches to heterogeneous traffic. See our project page for more examples: this https URL"
    },
    {
      "title": "Spatiotemporal Decision Transformer for Traffic Coordination",
      "url": "https://arxiv.org/abs/2602.02903",
      "date": "2026-02-02",
      "authors_text": "Haoran Su, Yandong Sun, Hanxiao Deng",
      "is_highlight": false,
      "score": 81.0,
      "summary": "The MADT framework enhances multi-agent traffic signal control using a sequence modeling approach, achieving state-of-the-art performance and improved coordination through graph attention and temporal dynamics.",
      "teaser_image": "https://arxiv.org/html/2602.02903/figures/method_comparison.png",
      "debug_abstract": "Traffic signal control is a critical challenge in urban transportation, requiring coordination among multiple intersections to optimize network-wide traffic flow. While reinforcement learning has shown promise for adaptive signal control, existing methods struggle with multi-agent coordination and sample efficiency. We introduce MADT (Multi-Agent Decision Transformer), a novel approach that reformulates multi-agent traffic signal control as a sequence modeling problem. MADT extends the Decision Transformer paradigm to multi-agent settings by incorporating: (1) a graph attention mechanism for modeling spatial dependencies between intersections, (2) a|temporal transformer encoder for capturing traffic dynamics, and (3) return-to-go conditioning for target performance specification. Our approach enables offline learning from historical traffic data, with architecture design that facilitates potential online fine-tuning. Experiments on synthetic grid networks and real-world traffic scenarios demonstrate that MADT achieves state-of-the-art performance, reducing average travel time by 5-6% compared to the strongest baseline while exhibiting superior coordination among adjacent intersections."
    }
  ],
  "深圳大学": [
    {
      "title": "Nüwa: Mending the Spatial Integrity Torn by VLM Token Pruning",
      "url": "https://arxiv.org/abs/2602.02951",
      "date": "2026-02-03",
      "authors_text": "Yihong Huang, Fei Ma, Yihua Shao, Jingcai Guo, Zitong Yu",
      "is_highlight": false,
      "score": 69.0,
      "summary": "Nüwa introduces a two-stage token pruning framework that enhances spatial integrity and significantly improves performance in visual grounding and question answering tasks in Vision Language Models.",
      "teaser_image": "https://arxiv.org/html/2602.02951/x2.png",
      "debug_abstract": "Vision token pruning has proven to be an effective acceleration technique for the efficient Vision Language Model (VLM). However, existing pruning methods demonstrate excellent performance preservation in visual question answering (VQA) and suffer substantial degradation on visual grounding (VG) tasks. Our analysis of the VLM&#39;s processing pipeline reveals that strategies utilizing global semantic similarity and attention scores lose the global spatial reference frame, which is derived from the interactions of tokens&#39; positional information. Motivated by these findings, we propose $\\text{Nüwa}$, a two-stage token pruning framework that enables efficient feature aggregation while maintaining spatial integrity. In the first stage, after the vision encoder, we apply three operations, namely separation, alignment, and aggregation, which are inspired by swarm intelligence algorithms to retain information-rich global spatial anchors. In the second stage, within the LLM, we perform text-guided pruning to retain task-relevant visual tokens. Extensive experiments demonstrate that $\\text{Nüwa}$ achieves SOTA performance on multiple VQA benchmarks (from 94% to 95%) and yields substantial improvements on visual grounding tasks (from 7% to 47%)."
    }
  ],
  "机器人与机械智能实验室 (ROMI Lab)": [
    {
      "title": "Nüwa: Mending the Spatial Integrity Torn by VLM Token Pruning",
      "url": "https://arxiv.org/abs/2602.02951",
      "date": "2026-02-03",
      "authors_text": "Yihong Huang, Fei Ma, Yihua Shao, Jingcai Guo, Zitong Yu",
      "is_highlight": false,
      "score": 69.0,
      "summary": "Nüwa introduces a two-stage token pruning framework that enhances spatial integrity and significantly improves performance in visual grounding and question answering tasks in Vision Language Models.",
      "teaser_image": "https://arxiv.org/html/2602.02951/x2.png",
      "debug_abstract": "Vision token pruning has proven to be an effective acceleration technique for the efficient Vision Language Model (VLM). However, existing pruning methods demonstrate excellent performance preservation in visual question answering (VQA) and suffer substantial degradation on visual grounding (VG) tasks. Our analysis of the VLM&#39;s processing pipeline reveals that strategies utilizing global semantic similarity and attention scores lose the global spatial reference frame, which is derived from the interactions of tokens&#39; positional information. Motivated by these findings, we propose $\\text{Nüwa}$, a two-stage token pruning framework that enables efficient feature aggregation while maintaining spatial integrity. In the first stage, after the vision encoder, we apply three operations, namely separation, alignment, and aggregation, which are inspired by swarm intelligence algorithms to retain information-rich global spatial anchors. In the second stage, within the LLM, we perform text-guided pruning to retain task-relevant visual tokens. Extensive experiments demonstrate that $\\text{Nüwa}$ achieves SOTA performance on multiple VQA benchmarks (from 94% to 95%) and yields substantial improvements on visual grounding tasks (from 7% to 47%)."
    }
  ],
  "西安电子科技大学": [
    {
      "title": "AdaptMMBench: Benchmarking Adaptive Multimodal Reasoning for Mode Selection and Reasoning Process",
      "url": "https://arxiv.org/abs/2602.02676",
      "date": "2026-02-02",
      "authors_text": "Xintong Zhang, Xiaowen Zhang, Jongrong Wu, Zhi Gao, Shilin Yan",
      "is_highlight": false,
      "score": 45.0,
      "summary": "AdaptMMBench introduces a dynamic benchmark for adaptive multimodal reasoning, evaluating mode selection and reasoning processes across diverse tasks while revealing inconsistencies in tool effectiveness and performance alignment.",
      "teaser_image": "https://arxiv.org/html/2602.02676/x3.png",
      "debug_abstract": "Adaptive multimodal reasoning has emerged as a promising frontier in Vision-Language Models (VLMs), aiming to dynamically modulate between tool-augmented visual reasoning and text reasoning to enhance both effectiveness and efficiency. However, existing evaluations rely on static difficulty labels and simplistic metrics, which fail to capture the dynamic nature of difficulty relative to varying model capacities. Consequently, they obscure the distinction between adaptive mode selection and general performance while neglecting fine-grained process analyses. In this paper, we propose AdaptMMBench, a comprehensive benchmark for adaptive multimodal reasoning across five domains: real-world, OCR, GUI, knowledge, and math, encompassing both direct perception and complex reasoning tasks. AdaptMMBench utilizes a Matthews Correlation Coefficient (MCC) metric to evaluate the selection rationality of different reasoning modes, isolating this meta-cognition ability by dynamically identifying task difficulties based on models&#39; capability boundaries. Moreover, AdaptMMBench facilitates multi-dimensional process evaluation across key step coverage, tool effectiveness, and computational efficiency. Our evaluation reveals that while adaptive mode selection scales with model capacity, it notably decouples from final accuracy. Conversely, key step coverage aligns with performance, though tool effectiveness remains highly inconsistent across model architectures."
    },
    {
      "title": "EventFlash: Towards Efficient MLLMs for Event-Based Vision",
      "url": "https://arxiv.org/abs/2602.03230",
      "date": "2026-02-03",
      "authors_text": "Shaoyu Liu, Jianing Li, Guanghui Zhao, Yunjian Zhang, Wen Jiang",
      "is_highlight": false,
      "score": 63.0,
      "summary": "EventFlash introduces an efficient MLLM for event-based vision, leveraging spatiotemporal sparsification and adaptive sampling to enhance performance and reduce computational costs.",
      "teaser_image": "https://arxiv.org/html/2602.03230/x2.png",
      "debug_abstract": "Event-based multimodal large language models (MLLMs) enable robust perception in high-speed and low-light scenarios, addressing key limitations of frame-based MLLMs. However, current event-based MLLMs often rely on dense image-like processing paradigms, overlooking the spatiotemporal sparsity of event streams and resulting in high computational cost. In this paper, we propose EventFlash, a novel and efficient MLLM to explore spatiotemporal token sparsification for reducing data redundancy and accelerating inference. Technically, we build EventMind, a large-scale and scene-diverse dataset with over 500k instruction sets, providing both short and long event stream sequences to support our curriculum training strategy. We then present an adaptive temporal window aggregation module for efficient temporal sampling, which adaptively compresses temporal tokens while retaining key temporal cues. Finally, a sparse density-guided attention module is designed to improve spatial token efficiency by selecting informative regions and suppressing empty or sparse areas. Experimental results show that EventFlash achieves a $12.4\\times$ throughput improvement over the baseline (EventFlash-Zero) while maintaining comparable performance. It supports long-range event stream processing with up to 1,000 bins, significantly outperforming the 5-bin limit of EventGPT. We believe EventFlash serves as an efficient foundation model for event-based vision."
    },
    {
      "title": "Nüwa: Mending the Spatial Integrity Torn by VLM Token Pruning",
      "url": "https://arxiv.org/abs/2602.02951",
      "date": "2026-02-03",
      "authors_text": "Yihong Huang, Fei Ma, Yihua Shao, Jingcai Guo, Zitong Yu",
      "is_highlight": false,
      "score": 69.0,
      "summary": "Nüwa introduces a two-stage token pruning framework that enhances spatial integrity and significantly improves performance in visual grounding and question answering tasks in Vision Language Models.",
      "teaser_image": "https://arxiv.org/html/2602.02951/x2.png",
      "debug_abstract": "Vision token pruning has proven to be an effective acceleration technique for the efficient Vision Language Model (VLM). However, existing pruning methods demonstrate excellent performance preservation in visual question answering (VQA) and suffer substantial degradation on visual grounding (VG) tasks. Our analysis of the VLM&#39;s processing pipeline reveals that strategies utilizing global semantic similarity and attention scores lose the global spatial reference frame, which is derived from the interactions of tokens&#39; positional information. Motivated by these findings, we propose $\\text{Nüwa}$, a two-stage token pruning framework that enables efficient feature aggregation while maintaining spatial integrity. In the first stage, after the vision encoder, we apply three operations, namely separation, alignment, and aggregation, which are inspired by swarm intelligence algorithms to retain information-rich global spatial anchors. In the second stage, within the LLM, we perform text-guided pruning to retain task-relevant visual tokens. Extensive experiments demonstrate that $\\text{Nüwa}$ achieves SOTA performance on multiple VQA benchmarks (from 94% to 95%) and yields substantial improvements on visual grounding tasks (from 7% to 47%)."
    }
  ],
  "Jun MA老师实验室": [
    {
      "title": "Neural Predictor-Corrector: Solving Homotopy Problems with Reinforcement Learning",
      "url": "https://arxiv.org/abs/2602.03086",
      "date": "2026-02-03",
      "authors_text": "Jiayao Mai, Bangyan Liao, Zhenjun Zhao, Yingping Zeng, Haoang Li",
      "is_highlight": false,
      "score": 64.0,
      "summary": "The Neural Predictor-Corrector (NPC) framework uses reinforcement learning to automate step size and termination decisions in solving diverse homotopy problems, outperforming traditional methods.",
      "teaser_image": "https://arxiv.org/html/2602.03086/x2.png",
      "debug_abstract": "The Homotopy paradigm, a general principle for solving challenging problems, appears across diverse domains such as robust optimization, global optimization, polynomial root-finding, and sampling. Practical solvers for these problems typically follow a predictor-corrector (PC) structure, but rely on hand-crafted heuristics for step sizes and iteration termination, which are often suboptimal and task-specific. To address this, we unify these problems under a single framework, which enables the design of a general neural solver. Building on this unified view, we propose Neural Predictor-Corrector (NPC), which replaces hand-crafted heuristics with automatically learned policies. NPC formulates policy selection as a sequential decision-making problem and leverages reinforcement learning to automatically discover efficient strategies. To further enhance generalization, we introduce an amortized training mechanism, enabling one-time offline training for a class of problems and efficient online inference on new instances. Experiments on four representative homotopy problems demonstrate that our method generalizes effectively to unseen instances. It consistently outperforms classical and specialized baselines in efficiency while demonstrating superior stability across tasks, highlighting the value of unifying homotopy methods into a single neural framework."
    },
    {
      "title": "Visual Reasoning over Time Series via Multi-Agent System",
      "url": "https://arxiv.org/abs/2602.03026",
      "date": "2026-02-03",
      "authors_text": "Weilin Ruan, Yuxuan Liang",
      "is_highlight": false,
      "score": 66.0,
      "summary": "MAS4TS is a multi-agent system that enhances time series analysis through visual reasoning and adaptive tool usage, achieving state-of-the-art performance across diverse tasks.",
      "teaser_image": "https://arxiv.org/html/2602.03026/x2.png",
      "debug_abstract": "Time series analysis underpins many real-world applications, yet existing time-series-specific methods and pretrained large-model-based approaches remain limited in integrating intuitive visual reasoning and generalizing across tasks with adaptive tool usage. To address these limitations, we propose MAS4TS, a tool-driven multi-agent system for general time series tasks, built upon an Analyzer-Reasoner-Executor paradigm that integrates agent communication, visual reasoning, and latent reconstruction within a unified framework. MAS4TS first performs visual reasoning over time series plots with structured priors using a Vision-Language Model to extract temporal structures, and subsequently reconstructs predictive trajectories in latent space. Three specialized agents coordinate via shared memory and gated communication, while a router selects task-specific tool chains for execution. Extensive experiments on multiple benchmarks demonstrate that MAS4TS achieves state-of-the-art performance across a wide range of time series tasks, while exhibiting strong generalization and efficient inference."
    }
  ],
  "机器人研究所": [
    {
      "title": "Neural Predictor-Corrector: Solving Homotopy Problems with Reinforcement Learning",
      "url": "https://arxiv.org/abs/2602.03086",
      "date": "2026-02-03",
      "authors_text": "Jiayao Mai, Bangyan Liao, Zhenjun Zhao, Yingping Zeng, Haoang Li",
      "is_highlight": false,
      "score": 64.0,
      "summary": "The Neural Predictor-Corrector (NPC) framework uses reinforcement learning to automate step size and termination decisions in solving diverse homotopy problems, outperforming traditional methods.",
      "teaser_image": "https://arxiv.org/html/2602.03086/x2.png",
      "debug_abstract": "The Homotopy paradigm, a general principle for solving challenging problems, appears across diverse domains such as robust optimization, global optimization, polynomial root-finding, and sampling. Practical solvers for these problems typically follow a predictor-corrector (PC) structure, but rely on hand-crafted heuristics for step sizes and iteration termination, which are often suboptimal and task-specific. To address this, we unify these problems under a single framework, which enables the design of a general neural solver. Building on this unified view, we propose Neural Predictor-Corrector (NPC), which replaces hand-crafted heuristics with automatically learned policies. NPC formulates policy selection as a sequential decision-making problem and leverages reinforcement learning to automatically discover efficient strategies. To further enhance generalization, we introduce an amortized training mechanism, enabling one-time offline training for a class of problems and efficient online inference on new instances. Experiments on four representative homotopy problems demonstrate that our method generalizes effectively to unseen instances. It consistently outperforms classical and specialized baselines in efficiency while demonstrating superior stability across tasks, highlighting the value of unifying homotopy methods into a single neural framework."
    },
    {
      "title": "Visual Reasoning over Time Series via Multi-Agent System",
      "url": "https://arxiv.org/abs/2602.03026",
      "date": "2026-02-03",
      "authors_text": "Weilin Ruan, Yuxuan Liang",
      "is_highlight": false,
      "score": 66.0,
      "summary": "MAS4TS is a multi-agent system that enhances time series analysis through visual reasoning and adaptive tool usage, achieving state-of-the-art performance across diverse tasks.",
      "teaser_image": "https://arxiv.org/html/2602.03026/x2.png",
      "debug_abstract": "Time series analysis underpins many real-world applications, yet existing time-series-specific methods and pretrained large-model-based approaches remain limited in integrating intuitive visual reasoning and generalizing across tasks with adaptive tool usage. To address these limitations, we propose MAS4TS, a tool-driven multi-agent system for general time series tasks, built upon an Analyzer-Reasoner-Executor paradigm that integrates agent communication, visual reasoning, and latent reconstruction within a unified framework. MAS4TS first performs visual reasoning over time series plots with structured priors using a Vision-Language Model to extract temporal structures, and subsequently reconstructs predictive trajectories in latent space. Three specialized agents coordinate via shared memory and gated communication, while a router selects task-specific tool chains for execution. Extensive experiments on multiple benchmarks demonstrate that MAS4TS achieves state-of-the-art performance across a wide range of time series tasks, while exhibiting strong generalization and efficient inference."
    }
  ],
  "Precognition Lab": [
    {
      "title": "Neural Predictor-Corrector: Solving Homotopy Problems with Reinforcement Learning",
      "url": "https://arxiv.org/abs/2602.03086",
      "date": "2026-02-03",
      "authors_text": "Jiayao Mai, Bangyan Liao, Zhenjun Zhao, Yingping Zeng, Haoang Li",
      "is_highlight": false,
      "score": 64.0,
      "summary": "The Neural Predictor-Corrector (NPC) framework uses reinforcement learning to automate step size and termination decisions in solving diverse homotopy problems, outperforming traditional methods.",
      "teaser_image": "https://arxiv.org/html/2602.03086/x2.png",
      "debug_abstract": "The Homotopy paradigm, a general principle for solving challenging problems, appears across diverse domains such as robust optimization, global optimization, polynomial root-finding, and sampling. Practical solvers for these problems typically follow a predictor-corrector (PC) structure, but rely on hand-crafted heuristics for step sizes and iteration termination, which are often suboptimal and task-specific. To address this, we unify these problems under a single framework, which enables the design of a general neural solver. Building on this unified view, we propose Neural Predictor-Corrector (NPC), which replaces hand-crafted heuristics with automatically learned policies. NPC formulates policy selection as a sequential decision-making problem and leverages reinforcement learning to automatically discover efficient strategies. To further enhance generalization, we introduce an amortized training mechanism, enabling one-time offline training for a class of problems and efficient online inference on new instances. Experiments on four representative homotopy problems demonstrate that our method generalizes effectively to unseen instances. It consistently outperforms classical and specialized baselines in efficiency while demonstrating superior stability across tasks, highlighting the value of unifying homotopy methods into a single neural framework."
    },
    {
      "title": "Visual Reasoning over Time Series via Multi-Agent System",
      "url": "https://arxiv.org/abs/2602.03026",
      "date": "2026-02-03",
      "authors_text": "Weilin Ruan, Yuxuan Liang",
      "is_highlight": false,
      "score": 66.0,
      "summary": "MAS4TS is a multi-agent system that enhances time series analysis through visual reasoning and adaptive tool usage, achieving state-of-the-art performance across diverse tasks.",
      "teaser_image": "https://arxiv.org/html/2602.03026/x2.png",
      "debug_abstract": "Time series analysis underpins many real-world applications, yet existing time-series-specific methods and pretrained large-model-based approaches remain limited in integrating intuitive visual reasoning and generalizing across tasks with adaptive tool usage. To address these limitations, we propose MAS4TS, a tool-driven multi-agent system for general time series tasks, built upon an Analyzer-Reasoner-Executor paradigm that integrates agent communication, visual reasoning, and latent reconstruction within a unified framework. MAS4TS first performs visual reasoning over time series plots with structured priors using a Vision-Language Model to extract temporal structures, and subsequently reconstructs predictive trajectories in latent space. Three specialized agents coordinate via shared memory and gated communication, while a router selects task-specific tool chains for execution. Extensive experiments on multiple benchmarks demonstrate that MAS4TS achieves state-of-the-art performance across a wide range of time series tasks, while exhibiting strong generalization and efficient inference."
    }
  ],
  "范明明老师实验室": [
    {
      "title": "Neural Predictor-Corrector: Solving Homotopy Problems with Reinforcement Learning",
      "url": "https://arxiv.org/abs/2602.03086",
      "date": "2026-02-03",
      "authors_text": "Jiayao Mai, Bangyan Liao, Zhenjun Zhao, Yingping Zeng, Haoang Li",
      "is_highlight": false,
      "score": 64.0,
      "summary": "The Neural Predictor-Corrector (NPC) framework uses reinforcement learning to automate step size and termination decisions in solving diverse homotopy problems, outperforming traditional methods.",
      "teaser_image": "https://arxiv.org/html/2602.03086/x2.png",
      "debug_abstract": "The Homotopy paradigm, a general principle for solving challenging problems, appears across diverse domains such as robust optimization, global optimization, polynomial root-finding, and sampling. Practical solvers for these problems typically follow a predictor-corrector (PC) structure, but rely on hand-crafted heuristics for step sizes and iteration termination, which are often suboptimal and task-specific. To address this, we unify these problems under a single framework, which enables the design of a general neural solver. Building on this unified view, we propose Neural Predictor-Corrector (NPC), which replaces hand-crafted heuristics with automatically learned policies. NPC formulates policy selection as a sequential decision-making problem and leverages reinforcement learning to automatically discover efficient strategies. To further enhance generalization, we introduce an amortized training mechanism, enabling one-time offline training for a class of problems and efficient online inference on new instances. Experiments on four representative homotopy problems demonstrate that our method generalizes effectively to unseen instances. It consistently outperforms classical and specialized baselines in efficiency while demonstrating superior stability across tasks, highlighting the value of unifying homotopy methods into a single neural framework."
    },
    {
      "title": "Visual Reasoning over Time Series via Multi-Agent System",
      "url": "https://arxiv.org/abs/2602.03026",
      "date": "2026-02-03",
      "authors_text": "Weilin Ruan, Yuxuan Liang",
      "is_highlight": false,
      "score": 66.0,
      "summary": "MAS4TS is a multi-agent system that enhances time series analysis through visual reasoning and adaptive tool usage, achieving state-of-the-art performance across diverse tasks.",
      "teaser_image": "https://arxiv.org/html/2602.03026/x2.png",
      "debug_abstract": "Time series analysis underpins many real-world applications, yet existing time-series-specific methods and pretrained large-model-based approaches remain limited in integrating intuitive visual reasoning and generalizing across tasks with adaptive tool usage. To address these limitations, we propose MAS4TS, a tool-driven multi-agent system for general time series tasks, built upon an Analyzer-Reasoner-Executor paradigm that integrates agent communication, visual reasoning, and latent reconstruction within a unified framework. MAS4TS first performs visual reasoning over time series plots with structured priors using a Vision-Language Model to extract temporal structures, and subsequently reconstructs predictive trajectories in latent space. Three specialized agents coordinate via shared memory and gated communication, while a router selects task-specific tool chains for execution. Extensive experiments on multiple benchmarks demonstrate that MAS4TS achieves state-of-the-art performance across a wide range of time series tasks, while exhibiting strong generalization and efficient inference."
    }
  ],
  "机器人研究所 (CKSRI)": [
    {
      "title": "Neural Predictor-Corrector: Solving Homotopy Problems with Reinforcement Learning",
      "url": "https://arxiv.org/abs/2602.03086",
      "date": "2026-02-03",
      "authors_text": "Jiayao Mai, Bangyan Liao, Zhenjun Zhao, Yingping Zeng, Haoang Li",
      "is_highlight": false,
      "score": 64.0,
      "summary": "The Neural Predictor-Corrector (NPC) framework uses reinforcement learning to automate step size and termination decisions in solving diverse homotopy problems, outperforming traditional methods.",
      "teaser_image": "https://arxiv.org/html/2602.03086/x2.png",
      "debug_abstract": "The Homotopy paradigm, a general principle for solving challenging problems, appears across diverse domains such as robust optimization, global optimization, polynomial root-finding, and sampling. Practical solvers for these problems typically follow a predictor-corrector (PC) structure, but rely on hand-crafted heuristics for step sizes and iteration termination, which are often suboptimal and task-specific. To address this, we unify these problems under a single framework, which enables the design of a general neural solver. Building on this unified view, we propose Neural Predictor-Corrector (NPC), which replaces hand-crafted heuristics with automatically learned policies. NPC formulates policy selection as a sequential decision-making problem and leverages reinforcement learning to automatically discover efficient strategies. To further enhance generalization, we introduce an amortized training mechanism, enabling one-time offline training for a class of problems and efficient online inference on new instances. Experiments on four representative homotopy problems demonstrate that our method generalizes effectively to unseen instances. It consistently outperforms classical and specialized baselines in efficiency while demonstrating superior stability across tasks, highlighting the value of unifying homotopy methods into a single neural framework."
    },
    {
      "title": "Visual Reasoning over Time Series via Multi-Agent System",
      "url": "https://arxiv.org/abs/2602.03026",
      "date": "2026-02-03",
      "authors_text": "Weilin Ruan, Yuxuan Liang",
      "is_highlight": false,
      "score": 66.0,
      "summary": "MAS4TS is a multi-agent system that enhances time series analysis through visual reasoning and adaptive tool usage, achieving state-of-the-art performance across diverse tasks.",
      "teaser_image": "https://arxiv.org/html/2602.03026/x2.png",
      "debug_abstract": "Time series analysis underpins many real-world applications, yet existing time-series-specific methods and pretrained large-model-based approaches remain limited in integrating intuitive visual reasoning and generalizing across tasks with adaptive tool usage. To address these limitations, we propose MAS4TS, a tool-driven multi-agent system for general time series tasks, built upon an Analyzer-Reasoner-Executor paradigm that integrates agent communication, visual reasoning, and latent reconstruction within a unified framework. MAS4TS first performs visual reasoning over time series plots with structured priors using a Vision-Language Model to extract temporal structures, and subsequently reconstructs predictive trajectories in latent space. Three specialized agents coordinate via shared memory and gated communication, while a router selects task-specific tool chains for execution. Extensive experiments on multiple benchmarks demonstrate that MAS4TS achieves state-of-the-art performance across a wide range of time series tasks, while exhibiting strong generalization and efficient inference."
    }
  ],
  "Cheng Kar-Shun Robotics Institute (CKSRI)": [
    {
      "title": "Neural Predictor-Corrector: Solving Homotopy Problems with Reinforcement Learning",
      "url": "https://arxiv.org/abs/2602.03086",
      "date": "2026-02-03",
      "authors_text": "Jiayao Mai, Bangyan Liao, Zhenjun Zhao, Yingping Zeng, Haoang Li",
      "is_highlight": false,
      "score": 64.0,
      "summary": "The Neural Predictor-Corrector (NPC) framework uses reinforcement learning to automate step size and termination decisions in solving diverse homotopy problems, outperforming traditional methods.",
      "teaser_image": "https://arxiv.org/html/2602.03086/x2.png",
      "debug_abstract": "The Homotopy paradigm, a general principle for solving challenging problems, appears across diverse domains such as robust optimization, global optimization, polynomial root-finding, and sampling. Practical solvers for these problems typically follow a predictor-corrector (PC) structure, but rely on hand-crafted heuristics for step sizes and iteration termination, which are often suboptimal and task-specific. To address this, we unify these problems under a single framework, which enables the design of a general neural solver. Building on this unified view, we propose Neural Predictor-Corrector (NPC), which replaces hand-crafted heuristics with automatically learned policies. NPC formulates policy selection as a sequential decision-making problem and leverages reinforcement learning to automatically discover efficient strategies. To further enhance generalization, we introduce an amortized training mechanism, enabling one-time offline training for a class of problems and efficient online inference on new instances. Experiments on four representative homotopy problems demonstrate that our method generalizes effectively to unseen instances. It consistently outperforms classical and specialized baselines in efficiency while demonstrating superior stability across tasks, highlighting the value of unifying homotopy methods into a single neural framework."
    },
    {
      "title": "Visual Reasoning over Time Series via Multi-Agent System",
      "url": "https://arxiv.org/abs/2602.03026",
      "date": "2026-02-03",
      "authors_text": "Weilin Ruan, Yuxuan Liang",
      "is_highlight": false,
      "score": 66.0,
      "summary": "MAS4TS is a multi-agent system that enhances time series analysis through visual reasoning and adaptive tool usage, achieving state-of-the-art performance across diverse tasks.",
      "teaser_image": "https://arxiv.org/html/2602.03026/x2.png",
      "debug_abstract": "Time series analysis underpins many real-world applications, yet existing time-series-specific methods and pretrained large-model-based approaches remain limited in integrating intuitive visual reasoning and generalizing across tasks with adaptive tool usage. To address these limitations, we propose MAS4TS, a tool-driven multi-agent system for general time series tasks, built upon an Analyzer-Reasoner-Executor paradigm that integrates agent communication, visual reasoning, and latent reconstruction within a unified framework. MAS4TS first performs visual reasoning over time series plots with structured priors using a Vision-Language Model to extract temporal structures, and subsequently reconstructs predictive trajectories in latent space. Three specialized agents coordinate via shared memory and gated communication, while a router selects task-specific tool chains for execution. Extensive experiments on multiple benchmarks demonstrate that MAS4TS achieves state-of-the-art performance across a wide range of time series tasks, while exhibiting strong generalization and efficient inference."
    }
  ],
  "MReaLLab": [
    {
      "title": "Estimation of Ground Reaction Forces from Kinematic Data during Locomotion",
      "url": "https://arxiv.org/abs/2602.03177",
      "date": "2026-02-03",
      "authors_text": "Gautami Golani, Dong Anh Khoa To, Ananda Sidarta, Arun-Kumar Kaliya-Perumal, Oliver Roberts",
      "is_highlight": false,
      "score": 44.0,
      "summary": "This study introduces a force-plate-free method for estimating ground reaction forces from kinematic data, enhancing clinical gait analysis without the need for specialized equipment.",
      "teaser_image": "https://arxiv.org/html/2602.03177/x2.png",
      "debug_abstract": "Ground reaction forces (GRFs) provide fundamental insight into human gait mechanics and are widely used to assess joint loading, limb symmetry, balance control, and motor function. Despite their clinical relevance, the use of GRF remains underutilised in clinical workflows due to the practical limitations of force plate systems. In this work, we present a force-plate-free approach for estimating GRFs using only marker-based motion capture data. This kinematics only method to estimate and decompose GRF makes it well suited for widespread clinical depolyment. By using kinematics from sixteen body segments, we estimate the centre of mass (CoM) and compute GRFs, which are subsequently decomposed into individual components through a minimization-based approach. Through this framework, we can identify gait stance phases and provide access to clinically meaningful kinetic measures without a dedicated force plate system. Experimental results demonstrate the viability of CoM and GRF estimation based solely on kinematic data, supporting force-plate-free gait analysis."
    },
    {
      "title": "Bongards at the Boundary of Perception and Reasoning: Programs or Language?",
      "url": "https://arxiv.org/abs/2602.03038",
      "date": "2026-02-03",
      "authors_text": "Cassidy Langenfeld, Claas Beger, Gloria Geng, Wasu Top Piriyakulkij, Keya Hu",
      "is_highlight": false,
      "score": 60.0,
      "summary": "This paper introduces a neurosymbolic method utilizing LLMs and Bayesian optimization to tackle Bongard problems, enhancing visual reasoning beyond conventional VLM capabilities.",
      "teaser_image": "https://arxiv.org/html/2602.03038/x3.png",
      "debug_abstract": "Vision-Language Models (VLMs) have made great strides in everyday visual tasks, such as captioning a natural image, or answering commonsense questions about such images. But humans possess the puzzling ability to deploy their visual reasoning abilities in radically new situations, a skill rigorously tested by the classic set of visual reasoning challenges known as the Bongard problems. We present a neurosymbolic approach to solving these problems: given a hypothesized solution rule for a Bongard problem, we leverage LLMs to generate parameterized programmatic representations for the rule and perform parameter fitting using Bayesian optimization. We evaluate our method on classifying Bongard problem images given the ground truth rule, as well as on solving the problems from scratch."
    }
  ],
  "MMLab@NTU": [
    {
      "title": "Estimation of Ground Reaction Forces from Kinematic Data during Locomotion",
      "url": "https://arxiv.org/abs/2602.03177",
      "date": "2026-02-03",
      "authors_text": "Gautami Golani, Dong Anh Khoa To, Ananda Sidarta, Arun-Kumar Kaliya-Perumal, Oliver Roberts",
      "is_highlight": false,
      "score": 44.0,
      "summary": "This study introduces a force-plate-free method for estimating ground reaction forces from kinematic data, enhancing clinical gait analysis without the need for specialized equipment.",
      "teaser_image": "https://arxiv.org/html/2602.03177/x2.png",
      "debug_abstract": "Ground reaction forces (GRFs) provide fundamental insight into human gait mechanics and are widely used to assess joint loading, limb symmetry, balance control, and motor function. Despite their clinical relevance, the use of GRF remains underutilised in clinical workflows due to the practical limitations of force plate systems. In this work, we present a force-plate-free approach for estimating GRFs using only marker-based motion capture data. This kinematics only method to estimate and decompose GRF makes it well suited for widespread clinical depolyment. By using kinematics from sixteen body segments, we estimate the centre of mass (CoM) and compute GRFs, which are subsequently decomposed into individual components through a minimization-based approach. Through this framework, we can identify gait stance phases and provide access to clinically meaningful kinetic measures without a dedicated force plate system. Experimental results demonstrate the viability of CoM and GRF estimation based solely on kinematic data, supporting force-plate-free gait analysis."
    },
    {
      "title": "Bongards at the Boundary of Perception and Reasoning: Programs or Language?",
      "url": "https://arxiv.org/abs/2602.03038",
      "date": "2026-02-03",
      "authors_text": "Cassidy Langenfeld, Claas Beger, Gloria Geng, Wasu Top Piriyakulkij, Keya Hu",
      "is_highlight": false,
      "score": 60.0,
      "summary": "This paper introduces a neurosymbolic method utilizing LLMs and Bayesian optimization to tackle Bongard problems, enhancing visual reasoning beyond conventional VLM capabilities.",
      "teaser_image": "https://arxiv.org/html/2602.03038/x3.png",
      "debug_abstract": "Vision-Language Models (VLMs) have made great strides in everyday visual tasks, such as captioning a natural image, or answering commonsense questions about such images. But humans possess the puzzling ability to deploy their visual reasoning abilities in radically new situations, a skill rigorously tested by the classic set of visual reasoning challenges known as the Bongard problems. We present a neurosymbolic approach to solving these problems: given a hypothesized solution rule for a Bongard problem, we leverage LLMs to generate parameterized programmatic representations for the rule and perform parameter fitting using Bayesian optimization. We evaluate our method on classifying Bongard problem images given the ground truth rule, as well as on solving the problems from scratch."
    }
  ],
  "MARS Lab": [
    {
      "title": "Estimation of Ground Reaction Forces from Kinematic Data during Locomotion",
      "url": "https://arxiv.org/abs/2602.03177",
      "date": "2026-02-03",
      "authors_text": "Gautami Golani, Dong Anh Khoa To, Ananda Sidarta, Arun-Kumar Kaliya-Perumal, Oliver Roberts",
      "is_highlight": false,
      "score": 44.0,
      "summary": "This study introduces a force-plate-free method for estimating ground reaction forces from kinematic data, enhancing clinical gait analysis without the need for specialized equipment.",
      "teaser_image": "https://arxiv.org/html/2602.03177/x2.png",
      "debug_abstract": "Ground reaction forces (GRFs) provide fundamental insight into human gait mechanics and are widely used to assess joint loading, limb symmetry, balance control, and motor function. Despite their clinical relevance, the use of GRF remains underutilised in clinical workflows due to the practical limitations of force plate systems. In this work, we present a force-plate-free approach for estimating GRFs using only marker-based motion capture data. This kinematics only method to estimate and decompose GRF makes it well suited for widespread clinical depolyment. By using kinematics from sixteen body segments, we estimate the centre of mass (CoM) and compute GRFs, which are subsequently decomposed into individual components through a minimization-based approach. Through this framework, we can identify gait stance phases and provide access to clinically meaningful kinetic measures without a dedicated force plate system. Experimental results demonstrate the viability of CoM and GRF estimation based solely on kinematic data, supporting force-plate-free gait analysis."
    },
    {
      "title": "Bongards at the Boundary of Perception and Reasoning: Programs or Language?",
      "url": "https://arxiv.org/abs/2602.03038",
      "date": "2026-02-03",
      "authors_text": "Cassidy Langenfeld, Claas Beger, Gloria Geng, Wasu Top Piriyakulkij, Keya Hu",
      "is_highlight": false,
      "score": 60.0,
      "summary": "This paper introduces a neurosymbolic method utilizing LLMs and Bayesian optimization to tackle Bongard problems, enhancing visual reasoning beyond conventional VLM capabilities.",
      "teaser_image": "https://arxiv.org/html/2602.03038/x3.png",
      "debug_abstract": "Vision-Language Models (VLMs) have made great strides in everyday visual tasks, such as captioning a natural image, or answering commonsense questions about such images. But humans possess the puzzling ability to deploy their visual reasoning abilities in radically new situations, a skill rigorously tested by the classic set of visual reasoning challenges known as the Bongard problems. We present a neurosymbolic approach to solving these problems: given a hypothesized solution rule for a Bongard problem, we leverage LLMs to generate parameterized programmatic representations for the rule and perform parameter fitting using Bayesian optimization. We evaluate our method on classifying Bongard problem images given the ground truth rule, as well as on solving the problems from scratch."
    }
  ],
  "S-Lab for Advanced Intelligence": [
    {
      "title": "Estimation of Ground Reaction Forces from Kinematic Data during Locomotion",
      "url": "https://arxiv.org/abs/2602.03177",
      "date": "2026-02-03",
      "authors_text": "Gautami Golani, Dong Anh Khoa To, Ananda Sidarta, Arun-Kumar Kaliya-Perumal, Oliver Roberts",
      "is_highlight": false,
      "score": 44.0,
      "summary": "This study introduces a force-plate-free method for estimating ground reaction forces from kinematic data, enhancing clinical gait analysis without the need for specialized equipment.",
      "teaser_image": "https://arxiv.org/html/2602.03177/x2.png",
      "debug_abstract": "Ground reaction forces (GRFs) provide fundamental insight into human gait mechanics and are widely used to assess joint loading, limb symmetry, balance control, and motor function. Despite their clinical relevance, the use of GRF remains underutilised in clinical workflows due to the practical limitations of force plate systems. In this work, we present a force-plate-free approach for estimating GRFs using only marker-based motion capture data. This kinematics only method to estimate and decompose GRF makes it well suited for widespread clinical depolyment. By using kinematics from sixteen body segments, we estimate the centre of mass (CoM) and compute GRFs, which are subsequently decomposed into individual components through a minimization-based approach. Through this framework, we can identify gait stance phases and provide access to clinically meaningful kinetic measures without a dedicated force plate system. Experimental results demonstrate the viability of CoM and GRF estimation based solely on kinematic data, supporting force-plate-free gait analysis."
    },
    {
      "title": "Bongards at the Boundary of Perception and Reasoning: Programs or Language?",
      "url": "https://arxiv.org/abs/2602.03038",
      "date": "2026-02-03",
      "authors_text": "Cassidy Langenfeld, Claas Beger, Gloria Geng, Wasu Top Piriyakulkij, Keya Hu",
      "is_highlight": false,
      "score": 60.0,
      "summary": "This paper introduces a neurosymbolic method utilizing LLMs and Bayesian optimization to tackle Bongard problems, enhancing visual reasoning beyond conventional VLM capabilities.",
      "teaser_image": "https://arxiv.org/html/2602.03038/x3.png",
      "debug_abstract": "Vision-Language Models (VLMs) have made great strides in everyday visual tasks, such as captioning a natural image, or answering commonsense questions about such images. But humans possess the puzzling ability to deploy their visual reasoning abilities in radically new situations, a skill rigorously tested by the classic set of visual reasoning challenges known as the Bongard problems. We present a neurosymbolic approach to solving these problems: given a hypothesized solution rule for a Bongard problem, we leverage LLMs to generate parameterized programmatic representations for the rule and perform parameter fitting using Bayesian optimization. We evaluate our method on classifying Bongard problem images given the ground truth rule, as well as on solving the problems from scratch."
    }
  ],
  "ROSE Lab": [
    {
      "title": "Estimation of Ground Reaction Forces from Kinematic Data during Locomotion",
      "url": "https://arxiv.org/abs/2602.03177",
      "date": "2026-02-03",
      "authors_text": "Gautami Golani, Dong Anh Khoa To, Ananda Sidarta, Arun-Kumar Kaliya-Perumal, Oliver Roberts",
      "is_highlight": false,
      "score": 44.0,
      "summary": "This study introduces a force-plate-free method for estimating ground reaction forces from kinematic data, enhancing clinical gait analysis without the need for specialized equipment.",
      "teaser_image": "https://arxiv.org/html/2602.03177/x2.png",
      "debug_abstract": "Ground reaction forces (GRFs) provide fundamental insight into human gait mechanics and are widely used to assess joint loading, limb symmetry, balance control, and motor function. Despite their clinical relevance, the use of GRF remains underutilised in clinical workflows due to the practical limitations of force plate systems. In this work, we present a force-plate-free approach for estimating GRFs using only marker-based motion capture data. This kinematics only method to estimate and decompose GRF makes it well suited for widespread clinical depolyment. By using kinematics from sixteen body segments, we estimate the centre of mass (CoM) and compute GRFs, which are subsequently decomposed into individual components through a minimization-based approach. Through this framework, we can identify gait stance phases and provide access to clinically meaningful kinetic measures without a dedicated force plate system. Experimental results demonstrate the viability of CoM and GRF estimation based solely on kinematic data, supporting force-plate-free gait analysis."
    },
    {
      "title": "Bongards at the Boundary of Perception and Reasoning: Programs or Language?",
      "url": "https://arxiv.org/abs/2602.03038",
      "date": "2026-02-03",
      "authors_text": "Cassidy Langenfeld, Claas Beger, Gloria Geng, Wasu Top Piriyakulkij, Keya Hu",
      "is_highlight": false,
      "score": 60.0,
      "summary": "This paper introduces a neurosymbolic method utilizing LLMs and Bayesian optimization to tackle Bongard problems, enhancing visual reasoning beyond conventional VLM capabilities.",
      "teaser_image": "https://arxiv.org/html/2602.03038/x3.png",
      "debug_abstract": "Vision-Language Models (VLMs) have made great strides in everyday visual tasks, such as captioning a natural image, or answering commonsense questions about such images. But humans possess the puzzling ability to deploy their visual reasoning abilities in radically new situations, a skill rigorously tested by the classic set of visual reasoning challenges known as the Bongard problems. We present a neurosymbolic approach to solving these problems: given a hypothesized solution rule for a Bongard problem, we leverage LLMs to generate parameterized programmatic representations for the rule and perform parameter fitting using Bayesian optimization. We evaluate our method on classifying Bongard problem images given the ground truth rule, as well as on solving the problems from scratch."
    }
  ],
  "PINE Lab": [
    {
      "title": "Estimation of Ground Reaction Forces from Kinematic Data during Locomotion",
      "url": "https://arxiv.org/abs/2602.03177",
      "date": "2026-02-03",
      "authors_text": "Gautami Golani, Dong Anh Khoa To, Ananda Sidarta, Arun-Kumar Kaliya-Perumal, Oliver Roberts",
      "is_highlight": false,
      "score": 44.0,
      "summary": "This study introduces a force-plate-free method for estimating ground reaction forces from kinematic data, enhancing clinical gait analysis without the need for specialized equipment.",
      "teaser_image": "https://arxiv.org/html/2602.03177/x2.png",
      "debug_abstract": "Ground reaction forces (GRFs) provide fundamental insight into human gait mechanics and are widely used to assess joint loading, limb symmetry, balance control, and motor function. Despite their clinical relevance, the use of GRF remains underutilised in clinical workflows due to the practical limitations of force plate systems. In this work, we present a force-plate-free approach for estimating GRFs using only marker-based motion capture data. This kinematics only method to estimate and decompose GRF makes it well suited for widespread clinical depolyment. By using kinematics from sixteen body segments, we estimate the centre of mass (CoM) and compute GRFs, which are subsequently decomposed into individual components through a minimization-based approach. Through this framework, we can identify gait stance phases and provide access to clinically meaningful kinetic measures without a dedicated force plate system. Experimental results demonstrate the viability of CoM and GRF estimation based solely on kinematic data, supporting force-plate-free gait analysis."
    },
    {
      "title": "Bongards at the Boundary of Perception and Reasoning: Programs or Language?",
      "url": "https://arxiv.org/abs/2602.03038",
      "date": "2026-02-03",
      "authors_text": "Cassidy Langenfeld, Claas Beger, Gloria Geng, Wasu Top Piriyakulkij, Keya Hu",
      "is_highlight": false,
      "score": 60.0,
      "summary": "This paper introduces a neurosymbolic method utilizing LLMs and Bayesian optimization to tackle Bongard problems, enhancing visual reasoning beyond conventional VLM capabilities.",
      "teaser_image": "https://arxiv.org/html/2602.03038/x3.png",
      "debug_abstract": "Vision-Language Models (VLMs) have made great strides in everyday visual tasks, such as captioning a natural image, or answering commonsense questions about such images. But humans possess the puzzling ability to deploy their visual reasoning abilities in radically new situations, a skill rigorously tested by the classic set of visual reasoning challenges known as the Bongard problems. We present a neurosymbolic approach to solving these problems: given a hypothesized solution rule for a Bongard problem, we leverage LLMs to generate parameterized programmatic representations for the rule and perform parameter fitting using Bayesian optimization. We evaluate our method on classifying Bongard problem images given the ground truth rule, as well as on solving the problems from scratch."
    }
  ],
  "武汉大学": [
    {
      "title": "Socratic-Geo: Synthetic Data Generation and Geometric Reasoning via Multi-Agent Interaction",
      "url": "https://arxiv.org/abs/2602.03414",
      "date": "2026-02-03",
      "authors_text": "Zhengbo Jiao, Shaobo Wang, Zifan Zhang, Wei Wang, Bing Zhao",
      "is_highlight": false,
      "score": 58.0,
      "summary": "Socratic-Geo introduces a multi-agent framework for autonomous data synthesis and geometric reasoning, significantly improving model performance with minimal seed problems and surpassing existing benchmarks.",
      "teaser_image": "https://arxiv.org/html/2602.03414/x1.png",
      "debug_abstract": "Multimodal Large Language Models (MLLMs) have significantly advanced vision-language understanding. However, even state-of-the-art models struggle with geometric reasoning, revealing a critical bottleneck: the extreme scarcity of high-quality image-text pairs. Human annotation is prohibitively expensive, while automated methods fail to ensure fidelity and training effectiveness. Existing approaches either passively adapt to available images or employ inefficient random exploration with filtering, decoupling generation from learning needs. We propose Socratic-Geo, a fully autonomous framework that dynamically couples data synthesis with model learning through multi-agent interaction. The Teacher agent generates parameterized Python scripts with reflective feedback (Reflect for solvability, RePI for visual validity), ensuring image-text pair purity. The Solver agent optimizes reasoning through preference learning, with failure paths guiding Teacher&#39;s targeted augmentation. Independently, the Generator learns image generation capabilities on accumulated &#34;image-code-instruction&#34; triplets, distilling programmatic drawing intelligence into visual generation. Starting from only 108 seed problems, Socratic-Solver achieves 49.11 on six benchmarks using one-quarter of baseline data, surpassing strong baselines by 2.43 points. Socratic-Generator achieves 42.4% on GenExam, establishing new state-of-the-art for open-source models, surpassing Seedream-4.0 (39.8%) and approaching Gemini-2.5-Flash-Image (43.1%)."
    },
    {
      "title": "Agentic Proposing: Enhancing Large Language Model Reasoning via Compositional Skill Synthesis",
      "url": "https://arxiv.org/abs/2602.03279",
      "date": "2026-02-03",
      "authors_text": "Zhengbo Jiao, Shaobo Wang, Zifan Zhang, Xuan Ren, Wei Wang",
      "is_highlight": false,
      "score": 71.0,
      "summary": "The paper introduces Agentic Proposing, a framework for generating high-quality synthetic training data that enhances reasoning in large language models, achieving superior performance with minimal human input.",
      "teaser_image": "https://arxiv.org/html/2602.03279/x2.png",
      "debug_abstract": "Advancing complex reasoning in large language models relies on high-quality, verifiable datasets, yet human annotation remains cost-prohibitive and difficult to scale. Current synthesis paradigms often face a recurring trade-off: maintaining structural validity typically restricts problem complexity, while relaxing constraints to increase difficulty frequently leads to inconsistent or unsolvable instances. To address this, we propose Agentic Proposing, a framework that models problem synthesis as a goal-driven sequential decision process where a specialized agent dynamically selects and composes modular reasoning skills. Through an iterative workflow of internal reflection and tool-use, we develop the Agentic-Proposer-4B using Multi-Granularity Policy Optimization (MGPO) to generate high-precision, verifiable training trajectories across mathematics, coding, and science. Empirical results demonstrate that downstream solvers trained on agent-synthesized data significantly outperform leading baselines and exhibit robust cross-domain generalization. Notably, a 30B solver trained on only 11,000 synthesized trajectories achieves a state-of-the-art 91.6% accuracy on AIME25, rivaling frontier-scale proprietary models such as GPT-5 and proving that a small volume of high-quality synthetic signals can effectively substitute for massive human-curated datasets."
    },
    {
      "title": "When Attention Betrays: Erasing Backdoor Attacks in Robotic Policies by Reconstructing Visual Tokens",
      "url": "https://arxiv.org/abs/2602.03153",
      "date": "2026-02-03",
      "authors_text": "Xuetao Li, Pinhan Fu, Wenke Huang, Nengyuan Pan, Songhua Yang",
      "is_highlight": false,
      "score": 67.0,
      "summary": "The paper presents Bera, a novel framework that detects and erases backdoor attacks in vision-language-action models without retraining, ensuring robust robotic behavior.",
      "teaser_image": "https://arxiv.org/html/2602.03153/x4.png",
      "debug_abstract": "Downstream fine-tuning of vision-language-action (VLA) models enhances robotics, yet exposes the pipeline to backdoor risks. Attackers can pretrain VLAs on poisoned data to implant backdoors that remain stealthy but can trigger harmful behavior during inference. However, existing defenses either lack mechanistic insight into multimodal backdoors or impose prohibitive computational costs via full-model retraining. To this end, we uncover a deep-layer attention grabbing mechanism: backdoors redirect late-stage attention and form compact embedding clusters near the clean manifold. Leveraging this insight, we introduce Bera, a test-time backdoor erasure framework that detects tokens with anomalous attention via latent-space localization, masks suspicious regions using deep-layer cues, and reconstructs a trigger-free image to break the trigger-unsafe-action mapping while restoring correct behavior. Unlike prior defenses, Bera requires neither retraining of VLAs nor any changes to the training pipeline. Extensive experiments across multiple embodied platforms and tasks show that Bera effectively maintains nominal performance, significantly reduces attack success rates, and consistently restores benign behavior from backdoored outputs, thereby offering a robust and practical defense mechanism for securing robotic systems."
    },
    {
      "title": "IVC-Prune: Revealing the Implicit Visual Coordinates in LVLMs for Vision Token Pruning",
      "url": "https://arxiv.org/abs/2602.03060",
      "date": "2026-02-03",
      "authors_text": "Zhichao Sun, Yidong Ma, Gang Liu, Yibo Chen, Xu Tang",
      "is_highlight": true,
      "score": 62.0,
      "summary": "IVC-Prune enhances visual token pruning in LVLMs by preserving essential implicit visual coordinates and semantically relevant tokens, achieving significant reductions in token count without performance loss.",
      "teaser_image": "https://arxiv.org/html/2602.03060/x2.png",
      "debug_abstract": "Large Vision-Language Models (LVLMs) achieve impressive performance across multiple tasks. A significant challenge, however, is their prohibitive inference cost when processing high-resolution visual inputs. While visual token pruning has emerged as a promising solution, existing methods that primarily focus on semantic relevance often discard tokens that are crucial for spatial reasoning. We address this gap through a novel insight into \\emph{how LVLMs process spatial reasoning}. Specifically, we reveal that LVLMs implicitly establish visual coordinate systems through Rotary Position Embeddings (RoPE), where specific token positions serve as \\textbf{implicit visual coordinates} (IVC tokens) that are essential for spatial reasoning. Based on this insight, we propose \\textbf{IVC-Prune}, a training-free, prompt-aware pruning strategy that retains both IVC tokens and semantically relevant foreground tokens. IVC tokens are identified by theoretically analyzing the mathematical properties of RoPE, targeting positions at which its rotation matrices approximate identity matrix or the $90^\\circ$ rotation matrix. Foreground tokens are identified through a robust two-stage process: semantic seed discovery followed by contextual refinement via value-vector similarity. Extensive evaluations across four representative LVLMs and twenty diverse benchmarks show that IVC-Prune reduces visual tokens by approximately 50\\% while maintaining $\\geq$ 99\\% of the original performance and even achieving improvements on several benchmarks. Source codes are available at this https URL."
    }
  ],
  "西湖机器人科技 / 机器智能实验室 (MiLAB)": [
    {
      "title": "RegionReasoner: Region-Grounded Multi-Round Visual Reasoning",
      "url": "https://arxiv.org/abs/2602.03733",
      "date": "2026-02-03",
      "authors_text": "Wenfang Sun, Hao Chen, Yingjun Du, Yefeng Zheng, Cees G. M. Snoek",
      "is_highlight": false,
      "score": 49.0,
      "summary": "RegionReasoner introduces a multi-round visual reasoning framework that enhances accuracy and consistency in detection and segmentation tasks through grounded, iterative reasoning and structured rewards.",
      "teaser_image": "https://arxiv.org/html/2602.03733/image/appendix_test.png",
      "debug_abstract": "Large vision-language models have achieved remarkable progress in visual reasoning, yet most existing systems rely on single-step or text-only reasoning, limiting their ability to iteratively refine understanding across multiple visual contexts. To address this limitation, we introduce a new multi-round visual reasoning benchmark with training and test sets spanning both detection and segmentation tasks, enabling systematic evaluation under iterative reasoning scenarios. We further propose RegionReasoner, a reinforcement learning framework that enforces grounded reasoning by requiring each reasoning trace to explicitly cite the corresponding reference bounding boxes, while maintaining semantic coherence via a global-local consistency reward. This reward extracts key objects and nouns from both global scene captions and region-level captions, aligning them with the reasoning trace to ensure consistency across reasoning steps. RegionReasoner is optimized with structured rewards combining grounding fidelity and global-local semantic alignment. Experiments on detection and segmentation tasks show that RegionReasoner-7B, together with our newly introduced benchmark RegionDial-Bench, considerably improves multi-round reasoning accuracy, spatial grounding precision, and global-local consistency, establishing a strong baseline for this emerging research direction."
    },
    {
      "title": "CMR: Contractive Mapping Embeddings for Robust Humanoid Locomotion on Unstructured Terrains",
      "url": "https://arxiv.org/abs/2602.03511",
      "date": "2026-02-03",
      "authors_text": "Qixin Zeng, Hongyin Zhang, Shangke Lyu, Junxi Jin, Donglin Wang",
      "is_highlight": true,
      "score": 87.0,
      "summary": "The CMR framework enhances humanoid locomotion robustness on unstructured terrains by mapping noisy observations to a stable latent space, improving performance against disturbances.",
      "teaser_image": "https://arxiv.org/html/2602.03511/figures/CMR_Framwork.png",
      "debug_abstract": "Robust disturbance rejection remains a longstanding challenge in humanoid locomotion, particularly on unstructured terrains where sensing is unreliable and model mismatch is pronounced. While perception information, such as height map, enhances terrain awareness, sensor noise and sim-to-real gaps can destabilize policies in practice. In this work, we provide theoretical analysis that bounds the return gap under observation noise, when the induced latent dynamics are contractive. Furthermore, we present Contractive Mapping for Robustness (CMR) framework that maps high-dimensional, disturbance-prone observations into a latent space, where local perturbations are attenuated over time. Specifically, this approach couples contrastive representation learning with Lipschitz regularization to preserve task-relevant geometry while explicitly controlling sensitivity. Notably, the formulation can be incorporated into modern deep reinforcement learning pipelines as an auxiliary loss term with minimal additional technical effort required. Further, our extensive humanoid experiments show that CMR potently outperforms other locomotion algorithms under increased noise."
    },
    {
      "title": "Neural Predictor-Corrector: Solving Homotopy Problems with Reinforcement Learning",
      "url": "https://arxiv.org/abs/2602.03086",
      "date": "2026-02-03",
      "authors_text": "Jiayao Mai, Bangyan Liao, Zhenjun Zhao, Yingping Zeng, Haoang Li",
      "is_highlight": false,
      "score": 64.0,
      "summary": "The Neural Predictor-Corrector (NPC) framework uses reinforcement learning to automate step size and termination decisions in solving diverse homotopy problems, outperforming traditional methods.",
      "teaser_image": "https://arxiv.org/html/2602.03086/x2.png",
      "debug_abstract": "The Homotopy paradigm, a general principle for solving challenging problems, appears across diverse domains such as robust optimization, global optimization, polynomial root-finding, and sampling. Practical solvers for these problems typically follow a predictor-corrector (PC) structure, but rely on hand-crafted heuristics for step sizes and iteration termination, which are often suboptimal and task-specific. To address this, we unify these problems under a single framework, which enables the design of a general neural solver. Building on this unified view, we propose Neural Predictor-Corrector (NPC), which replaces hand-crafted heuristics with automatically learned policies. NPC formulates policy selection as a sequential decision-making problem and leverages reinforcement learning to automatically discover efficient strategies. To further enhance generalization, we introduce an amortized training mechanism, enabling one-time offline training for a class of problems and efficient online inference on new instances. Experiments on four representative homotopy problems demonstrate that our method generalizes effectively to unseen instances. It consistently outperforms classical and specialized baselines in efficiency while demonstrating superior stability across tasks, highlighting the value of unifying homotopy methods into a single neural framework."
    }
  ],
  "四川大学": [
    {
      "title": "Multi-function Robotized Surgical Dissector for Endoscopic Pulmonary Thromboendarterectomy: Preclinical Study and Evaluation",
      "url": "https://arxiv.org/abs/2602.03147",
      "date": "2026-02-03",
      "authors_text": "Runfeng Zhu, Xin Zhong, Qingxiang Zhao, Jing Lin, Zhong Wu",
      "is_highlight": true,
      "score": 88.0,
      "summary": "This study introduces a novel robotized surgical dissector for endoscopic pulmonary thromboendarterectomy, enhancing dexterity and precision in accessing pulmonary artery branches.",
      "teaser_image": "https://arxiv.org/html/2602.03147/x2.png",
      "debug_abstract": "Patients suffering chronic severe pulmonary thromboembolism need Pulmonary Thromboendarterectomy (PTE) to remove the thromb and intima located inside pulmonary artery (PA). During the surgery, a surgeon holds tweezers and a dissector to delicately strip the blockage, but available tools for this surgery are rigid and straight, lacking distal dexterity to access into thin branches of PA. Therefore, this work presents a novel robotized dissector based on concentric push/pull robot (CPPR) structure, enabling entering deep thin branch of tortuous PA. Compared with conventional rigid dissectors, our design characterizes slenderness and dual-segment-bending dexterity. Owing to the hollow and thin-walled structure of the CPPR-based dissector as it has a slender body of 3.5mm in diameter, the central lumen accommodates two channels for irrigation and tip tool, and space for endoscopic camera&#39;s signal wire. To provide accurate surgical manipulation, optimization-based kinematics model was established, realizing a 2mm accuracy in positioning the tip tool (60mm length) under open-loop control strategy. As such, with the endoscopic camera, traditional PTE is possible to be upgraded as endoscopic PTE. Basic physic performance of the robotized dissector including stiffness, motion accuracy and maneuverability was evaluated through experiments. Surgery simulation on ex vivo porcine lung also demonstrates its dexterity and notable advantages in PTE."
    }
  ],
  "电子科技大学": [
    {
      "title": "Multi-function Robotized Surgical Dissector for Endoscopic Pulmonary Thromboendarterectomy: Preclinical Study and Evaluation",
      "url": "https://arxiv.org/abs/2602.03147",
      "date": "2026-02-03",
      "authors_text": "Runfeng Zhu, Xin Zhong, Qingxiang Zhao, Jing Lin, Zhong Wu",
      "is_highlight": true,
      "score": 88.0,
      "summary": "This study introduces a novel robotized surgical dissector for endoscopic pulmonary thromboendarterectomy, enhancing dexterity and precision in accessing pulmonary artery branches.",
      "teaser_image": "https://arxiv.org/html/2602.03147/x2.png",
      "debug_abstract": "Patients suffering chronic severe pulmonary thromboembolism need Pulmonary Thromboendarterectomy (PTE) to remove the thromb and intima located inside pulmonary artery (PA). During the surgery, a surgeon holds tweezers and a dissector to delicately strip the blockage, but available tools for this surgery are rigid and straight, lacking distal dexterity to access into thin branches of PA. Therefore, this work presents a novel robotized dissector based on concentric push/pull robot (CPPR) structure, enabling entering deep thin branch of tortuous PA. Compared with conventional rigid dissectors, our design characterizes slenderness and dual-segment-bending dexterity. Owing to the hollow and thin-walled structure of the CPPR-based dissector as it has a slender body of 3.5mm in diameter, the central lumen accommodates two channels for irrigation and tip tool, and space for endoscopic camera&#39;s signal wire. To provide accurate surgical manipulation, optimization-based kinematics model was established, realizing a 2mm accuracy in positioning the tip tool (60mm length) under open-loop control strategy. As such, with the endoscopic camera, traditional PTE is possible to be upgraded as endoscopic PTE. Basic physic performance of the robotized dissector including stiffness, motion accuracy and maneuverability was evaluated through experiments. Surgery simulation on ex vivo porcine lung also demonstrates its dexterity and notable advantages in PTE."
    }
  ],
  "中国科学技术大学": [
    {
      "title": "InstaDrive: Instance-Aware Driving World Models for Realistic and Consistent Video Generation",
      "url": "https://arxiv.org/abs/2602.03242",
      "date": "2026-02-03",
      "authors_text": "Zhuoran Yang, Xi Guo, Chenjing Ding, Chiyu Wang, Wei Wu",
      "is_highlight": false,
      "score": 84.0,
      "summary": "InstaDrive introduces a framework that enhances realistic driving video generation by ensuring instance-level temporal consistency and spatial geometric fidelity through innovative feature extraction and alignment techniques.",
      "teaser_image": "https://arxiv.org/html/2602.03242/x2.png",
      "debug_abstract": "Autonomous driving relies on robust models trained on high-quality, large-scale multi-view driving videos. While world models offer a cost-effective solution for generating realistic driving videos, they struggle to maintain instance-level temporal consistency and spatial geometric fidelity. To address these challenges, we propose InstaDrive, a novel framework that enhances driving video realism through two key advancements: (1) Instance Flow Guider, which extracts and propagates instance features across frames to enforce temporal consistency, preserving instance identity over time. (2) Spatial Geometric Aligner, which improves spatial reasoning, ensures precise instance positioning, and explicitly models occlusion hierarchies. By incorporating these instance-aware mechanisms, InstaDrive achieves state-of-the-art video generation quality and enhances downstream autonomous driving tasks on the nuScenes dataset. Additionally, we utilize CARLA&#39;s autopilot to procedurally and stochastically simulate rare but safety-critical driving scenarios across diverse maps and regions, enabling rigorous safety evaluation for autonomous systems. Our project page is this https URL."
    },
    {
      "title": "ConsisDrive: Identity-Preserving Driving World Models for Video Generation by Instance Mask",
      "url": "https://arxiv.org/abs/2602.03213",
      "date": "2026-02-03",
      "authors_text": "Zhuoran Yang, Yanyong Zhang",
      "is_highlight": false,
      "score": 83.0,
      "summary": "ConsisDrive introduces an identity-preserving world model for video generation in autonomous driving, enhancing temporal consistency and object identity through instance-masked attention and loss mechanisms.",
      "teaser_image": "https://arxiv.org/html/2602.03213/x2.png",
      "debug_abstract": "Autonomous driving relies on robust models trained on large-scale, high-quality multi-view driving videos. Although world models provide a cost-effective solution for generating realistic driving data, they often suffer from identity drift, where the same object changes its appearance or category across frames due to the absence of instance-level temporal constraints. We introduce ConsisDrive, an identity-preserving driving world model designed to enforce temporal consistency at the instance level. Our framework incorporates two key components: (1) Instance-Masked Attention, which applies instance identity masks and trajectory masks within attention blocks to ensure that visual tokens interact only with their corresponding instance features across spatial and temporal dimensions, thereby preserving object identity consistency; and (2) Instance-Masked Loss, which adaptively emphasizes foreground regions with probabilistic instance masking, reducing background noise while maintaining overall scene fidelity. By integrating these mechanisms, ConsisDrive achieves state-of-the-art driving video generation quality and demonstrates significant improvements in downstream autonomous driving tasks on the nuScenes dataset. Our project page is this https URL."
    }
  ],
  "机器人系统实验室 (RSL)": [
    {
      "title": "Depth Completion in Unseen Field Robotics Environments Using Extremely Sparse Depth Measurements",
      "url": "https://arxiv.org/abs/2602.03209",
      "date": "2026-02-03",
      "authors_text": "Marco Job, Thomas Stastny, Eleni Kelasidi, Roland Siegwart, Michael Pantic",
      "is_highlight": false,
      "score": 43.0,
      "summary": "The paper presents a depth completion model that utilizes synthetic data and sparse depth measurements to enhance depth perception for autonomous field robots in unstructured environments.",
      "teaser_image": "https://arxiv.org/html/2602.03209/x2.png",
      "debug_abstract": "Autonomous field robots operating in unstructured environments require robust perception to ensure safe and reliable operations. Recent advances in monocular depth estimation have demonstrated the potential of low-cost cameras as depth sensors; however, their adoption in field robotics remains limited due to the absence of reliable scale cues, ambiguous or low-texture conditions, and the scarcity of large-scale datasets. To address these challenges, we propose a depth completion model that trains on synthetic data and uses extremely sparse measurements from depth sensors to predict dense metric depth in unseen field robotics environments. A synthetic dataset generation pipeline tailored to field robotics enables the creation of multiple realistic datasets for training purposes. This dataset generation approach utilizes textured 3D meshes from Structure from Motion and photorealistic rendering with novel viewpoint synthesis to simulate diverse field robotics scenarios. Our approach achieves an end-to-end latency of 53 ms per frame on a Nvidia Jetson AGX Orin, enabling real-time deployment on embedded platforms. Extensive evaluation demonstrates competitive performance across diverse real-world field robotics scenarios."
    },
    {
      "title": "LEVIO: Lightweight Embedded Visual Inertial Odometry for Resource-Constrained Devices",
      "url": "https://arxiv.org/abs/2602.03294",
      "date": "2026-02-03",
      "authors_text": "Jonas Kühne, Christian Vogt, Michele Magno, Luca Benini",
      "is_highlight": false,
      "score": 86.0,
      "summary": "LEVIO is a lightweight visual-inertial odometry system optimized for low-power devices, achieving real-time performance with efficient algorithms and open-source implementation.",
      "teaser_image": "https://arxiv.org/html/2602.03294/x2.png",
      "debug_abstract": "Accurate, infrastructure-less sensor systems for motion tracking are essential for mobile robotics and augmented reality (AR) applications. The most popular state-of-the-art visual-inertial odometry (VIO) systems, however, are too computationally demanding for resource-constrained hardware, such as micro-drones and smart glasses. This work presents LEVIO, a fully featured VIO pipeline optimized for ultra-low-power compute platforms, allowing six-degrees-of-freedom (DoF) real-time sensing. LEVIO incorporates established VIO components such as Oriented FAST and Rotated BRIEF (ORB) feature tracking and bundle adjustment, while emphasizing a computationally efficient architecture with parallelization and low memory usage to suit embedded microcontrollers and low-power systems-on-chip (SoCs). The paper proposes and details the algorithmic design choices and the hardware-software co-optimization approach, and presents real-time performance on resource-constrained hardware. LEVIO is validated on a parallel-processing ultra-low-power RISC-V SoC, achieving 20 FPS while consuming less than 100 mW, and benchmarked against public VIO datasets, offering a compelling balance between efficiency and accuracy. To facilitate reproducibility and adoption, the complete implementation is released as open-source."
    }
  ],
  "Sensing, Interaction & Perception Lab (SIPLAB)": [
    {
      "title": "Depth Completion in Unseen Field Robotics Environments Using Extremely Sparse Depth Measurements",
      "url": "https://arxiv.org/abs/2602.03209",
      "date": "2026-02-03",
      "authors_text": "Marco Job, Thomas Stastny, Eleni Kelasidi, Roland Siegwart, Michael Pantic",
      "is_highlight": false,
      "score": 43.0,
      "summary": "The paper presents a depth completion model that utilizes synthetic data and sparse depth measurements to enhance depth perception for autonomous field robots in unstructured environments.",
      "teaser_image": "https://arxiv.org/html/2602.03209/x2.png",
      "debug_abstract": "Autonomous field robots operating in unstructured environments require robust perception to ensure safe and reliable operations. Recent advances in monocular depth estimation have demonstrated the potential of low-cost cameras as depth sensors; however, their adoption in field robotics remains limited due to the absence of reliable scale cues, ambiguous or low-texture conditions, and the scarcity of large-scale datasets. To address these challenges, we propose a depth completion model that trains on synthetic data and uses extremely sparse measurements from depth sensors to predict dense metric depth in unseen field robotics environments. A synthetic dataset generation pipeline tailored to field robotics enables the creation of multiple realistic datasets for training purposes. This dataset generation approach utilizes textured 3D meshes from Structure from Motion and photorealistic rendering with novel viewpoint synthesis to simulate diverse field robotics scenarios. Our approach achieves an end-to-end latency of 53 ms per frame on a Nvidia Jetson AGX Orin, enabling real-time deployment on embedded platforms. Extensive evaluation demonstrates competitive performance across diverse real-world field robotics scenarios."
    },
    {
      "title": "LEVIO: Lightweight Embedded Visual Inertial Odometry for Resource-Constrained Devices",
      "url": "https://arxiv.org/abs/2602.03294",
      "date": "2026-02-03",
      "authors_text": "Jonas Kühne, Christian Vogt, Michele Magno, Luca Benini",
      "is_highlight": false,
      "score": 86.0,
      "summary": "LEVIO is a lightweight visual-inertial odometry system optimized for low-power devices, achieving real-time performance with efficient algorithms and open-source implementation.",
      "teaser_image": "https://arxiv.org/html/2602.03294/x2.png",
      "debug_abstract": "Accurate, infrastructure-less sensor systems for motion tracking are essential for mobile robotics and augmented reality (AR) applications. The most popular state-of-the-art visual-inertial odometry (VIO) systems, however, are too computationally demanding for resource-constrained hardware, such as micro-drones and smart glasses. This work presents LEVIO, a fully featured VIO pipeline optimized for ultra-low-power compute platforms, allowing six-degrees-of-freedom (DoF) real-time sensing. LEVIO incorporates established VIO components such as Oriented FAST and Rotated BRIEF (ORB) feature tracking and bundle adjustment, while emphasizing a computationally efficient architecture with parallelization and low memory usage to suit embedded microcontrollers and low-power systems-on-chip (SoCs). The paper proposes and details the algorithmic design choices and the hardware-software co-optimization approach, and presents real-time performance on resource-constrained hardware. LEVIO is validated on a parallel-processing ultra-low-power RISC-V SoC, achieving 20 FPS while consuming less than 100 mW, and benchmarked against public VIO datasets, offering a compelling balance between efficiency and accuracy. To facilitate reproducibility and adoption, the complete implementation is released as open-source."
    }
  ],
  "智元机器人联合实验室 (PKU-Agibot)": [
    {
      "title": "Multi-Resolution Alignment for Voxel Sparsity in Camera-Based 3D Semantic Scene Completion",
      "url": "https://arxiv.org/abs/2602.03371",
      "date": "2026-02-03",
      "authors_text": "Zhiwen Yang, Yuxin Peng",
      "is_highlight": false,
      "score": 61.0,
      "summary": "The paper presents a Multi-Resolution Alignment approach to enhance camera-based 3D semantic scene completion by addressing voxel sparsity through scene and instance-level feature alignment.",
      "teaser_image": "https://arxiv.org/html/2602.03371/x2.png",
      "debug_abstract": "Camera-based 3D semantic scene completion (SSC) offers a cost-effective solution for assessing the geometric occupancy and semantic labels of each voxel in the surrounding 3D scene with image inputs, providing a voxel-level scene perception foundation for the perception-prediction-planning autonomous driving systems. Although significant progress has been made in existing methods, their optimization rely solely on the supervision from voxel labels and face the challenge of voxel sparsity as a large portion of voxels in autonomous driving scenarios are empty, which limits both optimization efficiency and model performance. To address this issue, we propose a \\textit{Multi-Resolution Alignment (MRA)} approach to mitigate voxel sparsity in camera-based 3D semantic scene completion, which exploits the scene and instance level alignment across multi-resolution 3D features as auxiliary supervision. Specifically, we first propose the Multi-resolution View Transformer module, which projects 2D image features into multi-resolution 3D features and aligns them at the scene level through fusing discriminative seed features. Furthermore, we design the Cubic Semantic Anisotropy module to identify the instance-level semantic significance of each voxel, accounting for the semantic differences of a specific voxel against its neighboring voxels within a cubic area. Finally, we devise a Critical Distribution Alignment module, which selects critical voxels as instance-level anchors with the guidance of cubic semantic anisotropy, and applies a circulated loss for auxiliary supervision on the critical feature distribution consistency across different resolutions. The code is available at this https URL."
    }
  ],
  "情感与认知智能机器人实验室 (ACIR)": [
    {
      "title": "Multi-Resolution Alignment for Voxel Sparsity in Camera-Based 3D Semantic Scene Completion",
      "url": "https://arxiv.org/abs/2602.03371",
      "date": "2026-02-03",
      "authors_text": "Zhiwen Yang, Yuxin Peng",
      "is_highlight": false,
      "score": 61.0,
      "summary": "The paper presents a Multi-Resolution Alignment approach to enhance camera-based 3D semantic scene completion by addressing voxel sparsity through scene and instance-level feature alignment.",
      "teaser_image": "https://arxiv.org/html/2602.03371/x2.png",
      "debug_abstract": "Camera-based 3D semantic scene completion (SSC) offers a cost-effective solution for assessing the geometric occupancy and semantic labels of each voxel in the surrounding 3D scene with image inputs, providing a voxel-level scene perception foundation for the perception-prediction-planning autonomous driving systems. Although significant progress has been made in existing methods, their optimization rely solely on the supervision from voxel labels and face the challenge of voxel sparsity as a large portion of voxels in autonomous driving scenarios are empty, which limits both optimization efficiency and model performance. To address this issue, we propose a \\textit{Multi-Resolution Alignment (MRA)} approach to mitigate voxel sparsity in camera-based 3D semantic scene completion, which exploits the scene and instance level alignment across multi-resolution 3D features as auxiliary supervision. Specifically, we first propose the Multi-resolution View Transformer module, which projects 2D image features into multi-resolution 3D features and aligns them at the scene level through fusing discriminative seed features. Furthermore, we design the Cubic Semantic Anisotropy module to identify the instance-level semantic significance of each voxel, accounting for the semantic differences of a specific voxel against its neighboring voxels within a cubic area. Finally, we devise a Critical Distribution Alignment module, which selects critical voxels as instance-level anchors with the guidance of cubic semantic anisotropy, and applies a circulated loss for auxiliary supervision on the critical feature distribution consistency across different resolutions. The code is available at this https URL."
    }
  ],
  "具身感知与交互实验室 (EPIC Lab)": [
    {
      "title": "Multi-Resolution Alignment for Voxel Sparsity in Camera-Based 3D Semantic Scene Completion",
      "url": "https://arxiv.org/abs/2602.03371",
      "date": "2026-02-03",
      "authors_text": "Zhiwen Yang, Yuxin Peng",
      "is_highlight": false,
      "score": 61.0,
      "summary": "The paper presents a Multi-Resolution Alignment approach to enhance camera-based 3D semantic scene completion by addressing voxel sparsity through scene and instance-level feature alignment.",
      "teaser_image": "https://arxiv.org/html/2602.03371/x2.png",
      "debug_abstract": "Camera-based 3D semantic scene completion (SSC) offers a cost-effective solution for assessing the geometric occupancy and semantic labels of each voxel in the surrounding 3D scene with image inputs, providing a voxel-level scene perception foundation for the perception-prediction-planning autonomous driving systems. Although significant progress has been made in existing methods, their optimization rely solely on the supervision from voxel labels and face the challenge of voxel sparsity as a large portion of voxels in autonomous driving scenarios are empty, which limits both optimization efficiency and model performance. To address this issue, we propose a \\textit{Multi-Resolution Alignment (MRA)} approach to mitigate voxel sparsity in camera-based 3D semantic scene completion, which exploits the scene and instance level alignment across multi-resolution 3D features as auxiliary supervision. Specifically, we first propose the Multi-resolution View Transformer module, which projects 2D image features into multi-resolution 3D features and aligns them at the scene level through fusing discriminative seed features. Furthermore, we design the Cubic Semantic Anisotropy module to identify the instance-level semantic significance of each voxel, accounting for the semantic differences of a specific voxel against its neighboring voxels within a cubic area. Finally, we devise a Critical Distribution Alignment module, which selects critical voxels as instance-level anchors with the guidance of cubic semantic anisotropy, and applies a circulated loss for auxiliary supervision on the critical feature distribution consistency across different resolutions. The code is available at this https URL."
    }
  ],
  "HMI Lab": [
    {
      "title": "Multi-Resolution Alignment for Voxel Sparsity in Camera-Based 3D Semantic Scene Completion",
      "url": "https://arxiv.org/abs/2602.03371",
      "date": "2026-02-03",
      "authors_text": "Zhiwen Yang, Yuxin Peng",
      "is_highlight": false,
      "score": 61.0,
      "summary": "The paper presents a Multi-Resolution Alignment approach to enhance camera-based 3D semantic scene completion by addressing voxel sparsity through scene and instance-level feature alignment.",
      "teaser_image": "https://arxiv.org/html/2602.03371/x2.png",
      "debug_abstract": "Camera-based 3D semantic scene completion (SSC) offers a cost-effective solution for assessing the geometric occupancy and semantic labels of each voxel in the surrounding 3D scene with image inputs, providing a voxel-level scene perception foundation for the perception-prediction-planning autonomous driving systems. Although significant progress has been made in existing methods, their optimization rely solely on the supervision from voxel labels and face the challenge of voxel sparsity as a large portion of voxels in autonomous driving scenarios are empty, which limits both optimization efficiency and model performance. To address this issue, we propose a \\textit{Multi-Resolution Alignment (MRA)} approach to mitigate voxel sparsity in camera-based 3D semantic scene completion, which exploits the scene and instance level alignment across multi-resolution 3D features as auxiliary supervision. Specifically, we first propose the Multi-resolution View Transformer module, which projects 2D image features into multi-resolution 3D features and aligns them at the scene level through fusing discriminative seed features. Furthermore, we design the Cubic Semantic Anisotropy module to identify the instance-level semantic significance of each voxel, accounting for the semantic differences of a specific voxel against its neighboring voxels within a cubic area. Finally, we devise a Critical Distribution Alignment module, which selects critical voxels as instance-level anchors with the guidance of cubic semantic anisotropy, and applies a circulated loss for auxiliary supervision on the critical feature distribution consistency across different resolutions. The code is available at this https URL."
    }
  ],
  "上海科技大学": [
    {
      "title": "Interpreting and Controlling LLM Reasoning through Integrated Policy Gradient",
      "url": "https://arxiv.org/abs/2602.02313",
      "date": "2026-02-03",
      "authors_text": "Changming Li, Kaixing Zhang, Haoyun Xu, Yingdong Shi, Zheng Zhang",
      "is_highlight": false,
      "score": 57.0,
      "summary": "The paper introduces Integrated Policy Gradient (IPG), a framework that enhances interpretability and control of LLM reasoning by attributing behaviors to model components through outcome-based signal propagation.",
      "teaser_image": "https://arxiv.org/html/2602.02313/x1.png",
      "debug_abstract": "Large language models (LLMs) demonstrate strong reasoning abilities in solving complex real-world problems. Yet, the internal mechanisms driving these complex reasoning behaviors remain opaque. Existing interpretability approaches targeting reasoning either identify components (e.g., neurons) correlated with special textual patterns, or rely on human-annotated contrastive pairs to derive control vectors. Consequently, current methods struggle to precisely localize complex reasoning mechanisms or capture sequential influence from model internal workings to the reasoning outputs. In this paper, built on outcome-oriented and sequential-influence-aware principles, we focus on identifying components that have sequential contribution to reasoning behavior where outcomes are cumulated by long-range effects. We propose Integrated Policy Gradient (IPG), a novel framework that attributes reasoning behaviors to model&#39;s inner components by propagating compound outcome-based signals such as post reasoning accuracy backward through model inference trajectories. Empirical evaluations demonstrate that our approach achieves more precise localization and enables reliable modulation of reasoning behaviors (e.g., reasoning capability, reasoning strength) across diverse reasoning models."
    }
  ],
  "南方科技大学": [
    {
      "title": "ConsistentRFT: Reducing Visual Hallucinations in Flow-based Reinforcement Fine-Tuning",
      "url": "https://arxiv.org/abs/2602.03425",
      "date": "2026-02-03",
      "authors_text": "Xiaofeng Tan, Jun Liu, Yuanting Fan, Bin-Bin Gao, Xi Jiang",
      "is_highlight": false,
      "score": 55.0,
      "summary": "The paper presents ConsistentRFT, a framework that reduces visual hallucinations in flow-based reinforcement fine-tuning by balancing exploration and exploiting policy consistency through innovative mechanisms.",
      "teaser_image": "https://arxiv.org/html/2602.03425/x5.png",
      "debug_abstract": "Reinforcement Fine-Tuning (RFT) on flow-based models is crucial for preference alignment. However, they often introduce visual hallucinations like over-optimized details and semantic misalignment. This work preliminarily explores why visual hallucinations arise and how to reduce them. We first investigate RFT methods from a unified perspective, and reveal the core problems stemming from two aspects, exploration and exploitation: (1) limited exploration during stochastic differential equation (SDE) rollouts, leading to an over-emphasis on local details at the expense of global semantics, and (2) trajectory imitation process inherent in policy gradient methods, distorting the model&#39;s foundational vector field and its cross-step consistency. Building on this, we propose ConsistentRFT, a general framework to mitigate these hallucinations. Specifically, we design a Dynamic Granularity Rollout (DGR) mechanism to balance exploration between global semantics and local details by dynamically scheduling different noise sources. We then introduce a Consistent Policy Gradient Optimization (CPGO) that preserves the model&#39;s consistency by aligning the current policy with a more stable prior. Extensive experiments demonstrate that ConsistentRFT significantly mitigates visual hallucinations, achieving average reductions of 49\\% for low-level and 38\\% for high-level perceptual hallucinations. Furthermore, ConsistentRFT outperforms other RFT methods on out-of-domain metrics, showing an improvement of 5.1\\% (v.s. the baseline&#39;s decrease of -0.4\\%) over this http URL. This is \\href{this https URL}{Project Page}."
    }
  ],
  "智能人机交互实验室 (MemX)": [
    {
      "title": "SlowFocus: Enhancing Fine-grained Temporal Understanding in Video LLM",
      "url": "https://arxiv.org/abs/2602.03589",
      "date": "2026-02-03",
      "authors_text": "Ming Nie, Dan Ding, Chunwei Wang, Yuanfan Guo, Jianhua Han",
      "is_highlight": false,
      "score": 54.0,
      "summary": "The SlowFocus mechanism enhances video LLMs' fine-grained temporal understanding by improving sampling frequency and integrating local high-frequency features with global contexts.",
      "teaser_image": "https://arxiv.org/html/2602.03589/x2.png",
      "debug_abstract": "Large language models (LLMs) have demonstrated exceptional capabilities in text understanding, which has paved the way for their expansion into video LLMs (Vid-LLMs) to analyze video data. However, current Vid-LLMs struggle to simultaneously retain high-quality frame-level semantic information (i.e., a sufficient number of tokens per frame) and comprehensive video-level temporal information (i.e., an adequate number of sampled frames per video). This limitation hinders the advancement of Vid-LLMs towards fine-grained video understanding. To address this issue, we introduce the SlowFocus mechanism, which significantly enhances the equivalent sampling frequency without compromising the quality of frame-level visual tokens. SlowFocus begins by identifying the query-related temporal segment based on the posed question, then performs dense sampling on this segment to extract local high-frequency features. A multi-frequency mixing attention module is further leveraged to aggregate these local high-frequency details with global low-frequency contexts for enhanced temporal comprehension. Additionally, to tailor Vid-LLMs to this innovative mechanism, we introduce a set of training strategies aimed at bolstering both temporal grounding and detailed temporal reasoning capabilities. Furthermore, we establish FineAction-CGR, a benchmark specifically devised to assess the ability of Vid-LLMs to process fine-grained temporal understanding tasks. Comprehensive experiments demonstrate the superiority of our mechanism across both existing public video understanding benchmarks and our proposed FineAction-CGR."
    }
  ],
  "智能感知与无人系统实验室": [
    {
      "title": "SlowFocus: Enhancing Fine-grained Temporal Understanding in Video LLM",
      "url": "https://arxiv.org/abs/2602.03589",
      "date": "2026-02-03",
      "authors_text": "Ming Nie, Dan Ding, Chunwei Wang, Yuanfan Guo, Jianhua Han",
      "is_highlight": false,
      "score": 54.0,
      "summary": "The SlowFocus mechanism enhances video LLMs' fine-grained temporal understanding by improving sampling frequency and integrating local high-frequency features with global contexts.",
      "teaser_image": "https://arxiv.org/html/2602.03589/x2.png",
      "debug_abstract": "Large language models (LLMs) have demonstrated exceptional capabilities in text understanding, which has paved the way for their expansion into video LLMs (Vid-LLMs) to analyze video data. However, current Vid-LLMs struggle to simultaneously retain high-quality frame-level semantic information (i.e., a sufficient number of tokens per frame) and comprehensive video-level temporal information (i.e., an adequate number of sampled frames per video). This limitation hinders the advancement of Vid-LLMs towards fine-grained video understanding. To address this issue, we introduce the SlowFocus mechanism, which significantly enhances the equivalent sampling frequency without compromising the quality of frame-level visual tokens. SlowFocus begins by identifying the query-related temporal segment based on the posed question, then performs dense sampling on this segment to extract local high-frequency features. A multi-frequency mixing attention module is further leveraged to aggregate these local high-frequency details with global low-frequency contexts for enhanced temporal comprehension. Additionally, to tailor Vid-LLMs to this innovative mechanism, we introduce a set of training strategies aimed at bolstering both temporal grounding and detailed temporal reasoning capabilities. Furthermore, we establish FineAction-CGR, a benchmark specifically devised to assess the ability of Vid-LLMs to process fine-grained temporal understanding tasks. Comprehensive experiments demonstrate the superiority of our mechanism across both existing public video understanding benchmarks and our proposed FineAction-CGR."
    }
  ],
  "集群机器人系统实验室 (MagicLab)": [
    {
      "title": "SlowFocus: Enhancing Fine-grained Temporal Understanding in Video LLM",
      "url": "https://arxiv.org/abs/2602.03589",
      "date": "2026-02-03",
      "authors_text": "Ming Nie, Dan Ding, Chunwei Wang, Yuanfan Guo, Jianhua Han",
      "is_highlight": false,
      "score": 54.0,
      "summary": "The SlowFocus mechanism enhances video LLMs' fine-grained temporal understanding by improving sampling frequency and integrating local high-frequency features with global contexts.",
      "teaser_image": "https://arxiv.org/html/2602.03589/x2.png",
      "debug_abstract": "Large language models (LLMs) have demonstrated exceptional capabilities in text understanding, which has paved the way for their expansion into video LLMs (Vid-LLMs) to analyze video data. However, current Vid-LLMs struggle to simultaneously retain high-quality frame-level semantic information (i.e., a sufficient number of tokens per frame) and comprehensive video-level temporal information (i.e., an adequate number of sampled frames per video). This limitation hinders the advancement of Vid-LLMs towards fine-grained video understanding. To address this issue, we introduce the SlowFocus mechanism, which significantly enhances the equivalent sampling frequency without compromising the quality of frame-level visual tokens. SlowFocus begins by identifying the query-related temporal segment based on the posed question, then performs dense sampling on this segment to extract local high-frequency features. A multi-frequency mixing attention module is further leveraged to aggregate these local high-frequency details with global low-frequency contexts for enhanced temporal comprehension. Additionally, to tailor Vid-LLMs to this innovative mechanism, we introduce a set of training strategies aimed at bolstering both temporal grounding and detailed temporal reasoning capabilities. Furthermore, we establish FineAction-CGR, a benchmark specifically devised to assess the ability of Vid-LLMs to process fine-grained temporal understanding tasks. Comprehensive experiments demonstrate the superiority of our mechanism across both existing public video understanding benchmarks and our proposed FineAction-CGR."
    }
  ],
  "加州理工-机器人实验室": [
    {
      "title": "Input-to-State Safe Backstepping: Robust Safety-Critical Control with Unmatched Uncertainties",
      "url": "https://arxiv.org/abs/2602.03691",
      "date": "2026-02-03",
      "authors_text": "Max H. Cohen, Pio Ong, Aaron D. Ames",
      "is_highlight": false,
      "score": 50.0,
      "summary": "This paper introduces a novel approach for ensuring safety in nonlinear systems with unmatched disturbances through an enhanced input-to-state safety framework using Optimal Decay CBFs.",
      "teaser_image": "https://arxiv.org/html/2602.03691/x2.png",
      "debug_abstract": "Guaranteeing safety in the presence of unmatched disturbances -- uncertainties that cannot be directly canceled by the control input -- remains a key challenge in nonlinear control. This paper presents a constructive approach to safety-critical control of nonlinear systems with unmatched disturbances. We first present a generalization of the input-to-state safety (ISSf) framework for systems with these uncertainties using the recently developed notion of an Optimal Decay CBF, which provides more flexibility for satisfying the associated Lyapunov-like conditions for safety. From there, we outline a procedure for constructing ISSf-CBFs for two relevant classes of systems with unmatched uncertainties: i) strict-feedback systems; ii) dual-relative-degree systems, which are similar to differentially flat systems. Our theoretical results are illustrated via numerical simulations of an inverted pendulum and planar quadrotor."
    }
  ],
  "通用机器人、自动化、传感和感知实验室 (GRASP)": [
    {
      "title": "See-through: Single-image Layer Decomposition for Anime Characters",
      "url": "https://arxiv.org/abs/2602.03749",
      "date": "2026-02-03",
      "authors_text": "Jian Lin, Chengze Li, Haoyun Qin, Kwun Wang Chan, Yanghua Jin",
      "is_highlight": false,
      "score": 48.0,
      "summary": "The paper presents a framework that transforms static anime illustrations into dynamic 2.5D models by automating layer decomposition and utilizing high-quality supervision from Live2D models.",
      "teaser_image": "https://arxiv.org/html/2602.03749/figures/Figure_Method.png",
      "debug_abstract": "We introduce a framework that automates the transformation of static anime illustrations into manipulatable 2.5D models. Current professional workflows require tedious manual segmentation and the artistic ``hallucination&#39;&#39; of occluded regions to enable motion. Our approach overcomes this by decomposing a single image into fully inpainted, semantically distinct layers with inferred drawing orders. To address the scarcity of training data, we introduce a scalable engine that bootstraps high-quality supervision from commercial Live2D models, capturing pixel-perfect semantics and hidden geometry. Our methodology couples a diffusion-based Body Part Consistency Module, which enforces global geometric coherence, with a pixel-level pseudo-depth inference mechanism. This combination resolves the intricate stratification of anime characters, e.g., interleaving hair strands, allowing for dynamic layer reconstruction. We demonstrate that our approach yields high-fidelity, manipulatable models suitable for professional, real-time animation applications."
    }
  ],
  "机器人技术与系统国家重点实验室": [
    {
      "title": "Cross-Modal Alignment and Fusion for RGB-D Transmission-Line Defect Detection",
      "url": "https://arxiv.org/abs/2602.01696",
      "date": "2026-02-03",
      "authors_text": "Jiaming Cui, Wenqiang Li, Shuai Zhou, Ruifeng Qin, Feng Shen",
      "is_highlight": false,
      "score": 46.0,
      "summary": "CMAFNet enhances RGB-D transmission line defect detection by integrating RGB and depth data through a novel purification and fusion approach, significantly outperforming existing methods.",
      "teaser_image": "https://arxiv.org/html/2602.01696/figures/Overall_Architecture.png",
      "debug_abstract": "Transmission line defect detection remains challenging for automated UAV inspection due to the dominance of small-scale defects, complex backgrounds, and illumination variations. Existing RGB-based detectors, despite recent progress, struggle to distinguish geometrically subtle defects from visually similar background structures under limited chromatic contrast. This paper proposes CMAFNet, a Cross-Modal Alignment and Fusion Network that integrates RGB appearance and depth geometry through a principled purify-then-fuse paradigm. CMAFNet consists of a Semantic Recomposition Module that performs dictionary-based feature purification via a learned codebook to suppress modality-specific noise while preserving defect-discriminative information, and a Contextual Semantic Integration Framework that captures global spatial dependencies using partial-channel attention to enhance structural semantic reasoning. Position-wise normalization within the purification stage enforces explicit reconstruction-driven cross-modal alignment, ensuring statistical compatibility between heterogeneous features prior to fusion. Extensive experiments on the TLRGBD benchmark, where 94.5% of instances are small objects, demonstrate that CMAFNet achieves 32.2% mAP@50 and 12.5% APs, outperforming the strongest baseline by 9.8 and 4.0 percentage points, respectively. A lightweight variant reaches 24.8% mAP50 at 228 FPS with only 4.9M parameters, surpassing all YOLO-based detectors while matching transformer-based methods at substantially lower computational cost."
    }
  ]
}