{
  "潘佳老师实验室": [
    {
      "title": "PLANING: A Loosely Coupled Triangle-Gaussian Framework for Streaming 3D Reconstruction",
      "url": "https://arxiv.org/abs/2601.22046",
      "date": "2026-01-29",
      "authors_text": "Changjian Jiang, Kerui Ren, Xudong Li, Kaiwen Song, Linning Xu",
      "is_highlight": false,
      "score": 66.0,
      "summary": "PLANING is a novel streaming 3D reconstruction framework that efficiently combines geometric primitives and neural Gaussians, enhancing quality and speed while reducing redundancy.",
      "teaser_image": "https://arxiv.org/html/2601.22046/x3.png",
      "debug_abstract": "Streaming reconstruction from monocular image sequences remains challenging, as existing methods typically favor either high-quality rendering or accurate geometry, but rarely both. We present PLANING, an efficient on-the-fly reconstruction framework built on a hybrid representation that loosely couples explicit geometric primitives with neural Gaussians, enabling geometry and appearance to be modeled in a decoupled manner. This decoupling supports an online initialization and optimization strategy that separates geometry and appearance updates, yielding stable streaming reconstruction with substantially reduced structural redundancy. PLANING improves dense mesh Chamfer-L2 by 18.52% over PGSR, surpasses ARTDECO by 1.31 dB PSNR, and reconstructs ScanNetV2 scenes in under 100 seconds, over 5x faster than 2D Gaussian Splatting, while matching the quality of offline per-scene optimization. Beyond reconstruction quality, the structural clarity and computational efficiency of \\modelname~make it well suited for a broad range of downstream applications, such as enabling large-scale scene modeling and simulation-ready environments for embodied AI. Project page: this https URL ."
    },
    {
      "title": "Dynamic Topology Awareness: Breaking the Granularity Rigidity in Vision-Language Navigation",
      "url": "https://arxiv.org/abs/2601.21751",
      "date": "2026-01-29",
      "authors_text": "Jiankun Peng, Jianyuan Guo, Ying Xu, Yue Liu, Jiashuang Yan",
      "is_highlight": false,
      "score": 84.0,
      "summary": "The paper presents DGNav, a dynamic framework for Vision-Language Navigation that adapts topological map density and connectivity to enhance navigation efficiency and safety in complex environments.",
      "teaser_image": "https://arxiv.org/html/2601.21751/x1.png",
      "debug_abstract": "Vision-Language Navigation in Continuous Environments (VLN-CE) presents a core challenge: grounding high-level linguistic instructions into precise, safe, and long-horizon spatial actions. Explicit topological maps have proven to be a vital solution for providing robust spatial memory in such tasks. However, existing topological planning methods suffer from a &#34;Granularity Rigidity&#34; problem. Specifically, these methods typically rely on fixed geometric thresholds to sample nodes, which fails to adapt to varying environmental complexities. This rigidity leads to a critical mismatch: the model tends to over-sample in simple areas, causing computational redundancy, while under-sampling in high-uncertainty regions, increasing collision risks and compromising precision. To address this, we propose DGNav, a framework for Dynamic Topological Navigation, introducing a context-aware mechanism to modulate map density and connectivity on-the-fly. Our approach comprises two core innovations: (1) A Scene-Aware Adaptive Strategy that dynamically modulates graph construction thresholds based on the dispersion of predicted waypoints, enabling &#34;densification on demand&#34; in challenging environments; (2) A Dynamic Graph Transformer that reconstructs graph connectivity by fusing visual, linguistic, and geometric cues into dynamic edge weights, enabling the agent to filter out topological noise and enhancing instruction adherence. Extensive experiments on the R2R-CE and RxR-CE benchmarks demonstrate DGNav exhibits superior navigation performance and strong generalization capabilities. Furthermore, ablation studies confirm that our framework achieves an optimal trade-off between navigation efficiency and safe exploration. The code is available at this https URL."
    },
    {
      "title": "Disentangling perception and reasoning for improving data efficiency in learning cloth manipulation without demonstrations",
      "url": "https://arxiv.org/abs/2601.21713",
      "date": "2026-01-29",
      "authors_text": "Donatien Delehelle, Fei Chen, Darwin Caldwell",
      "is_highlight": false,
      "score": 87.0,
      "summary": "This paper presents a modular reinforcement learning approach for cloth manipulation that enhances data efficiency and reduces model size and training time, achieving improved performance in simulation and real-world transfer.",
      "teaser_image": "https://arxiv.org/html/2601.21713/x1.png",
      "debug_abstract": "Cloth manipulation is a ubiquitous task in everyday life, but it remains an open challenge for robotics. The difficulties in developing cloth manipulation policies are attributed to the high-dimensional state space, complex dynamics, and high propensity to self-occlusion exhibited by fabrics. As analytical methods have not been able to provide robust and general manipulation policies, reinforcement learning (RL) is considered a promising approach to these problems. However, to address the large state space and complex dynamics, data-based methods usually rely on large models and long training times. The resulting computational cost significantly hampers the development and adoption of these methods. Additionally, due to the challenge of robust state estimation, garment manipulation policies often adopt an end-to-end learning approach with workspace images as input. While this approach enables a conceptually straightforward sim-to-real transfer via real-world fine-tuning, it also incurs a significant computational cost by training agents on a highly lossy representation of the environment state. This paper questions this common design choice by exploring an efficient and modular approach to RL for cloth manipulation. We show that, through careful design choices, model size and training time can be significantly reduced when learning in simulation. Furthermore, we demonstrate how the resulting simulation-trained model can be transferred to the real world. We evaluate our approach on the SoftGym benchmark and achieve significant performance improvements over available baselines on our task, while using a substantially smaller model."
    },
    {
      "title": "Heterogeneous Vertiport Selection Optimization for On-Demand Air Taxi Services: A Deep Reinforcement Learning Approach",
      "url": "https://arxiv.org/abs/2601.21316",
      "date": "2026-01-29",
      "authors_text": "Aoyu Pang, Maonan Wang, Zifan Sha, Wenwei Yue, Changle Li",
      "is_highlight": false,
      "score": 80.0,
      "summary": "The paper presents a deep reinforcement learning framework for optimizing vertiport selection and air taxi routing, achieving significant travel time reductions in urban air mobility systems.",
      "teaser_image": "https://arxiv.org/html/2601.21316/x2.png",
      "debug_abstract": "Urban Air Mobility (UAM) has emerged as a transformative solution to alleviate urban congestion by utilizing low-altitude airspace, thereby reducing pressure on ground transportation networks. To enable truly efficient and seamless door-to-door travel experiences, UAM requires close integration with existing ground transportation infrastructure. However, current research on optimal integrated routing strategies for passengers in air-ground mobility systems remains limited, with a lack of systematic this http URL address this gap, we first propose a unified optimization model that integrates strategy selection for both air and ground transportation. This model captures the dynamic characteristics of multimodal transport networks and incorporates real-time traffic conditions alongside passenger decision-making behavior. Building on this model, we propose a Unified Air-Ground Mobility Coordination (UAGMC) framework, which leverages deep reinforcement learning (RL) and Vehicle-to-Everything (V2X) communication to optimize vertiport selection and dynamically plan air taxi routes. Experimental results demonstrate that UAGMC achieves a 34\\% reduction in average travel time compared to conventional proportional allocation methods, enhancing overall travel efficiency and providing novel insights into the integration and optimization of multimodal transportation systems. This work lays a solid foundation for advancing intelligent urban mobility solutions through the coordination of air and ground transportation modes. The related code can be found at this https URL."
    },
    {
      "title": "Listen, Look, Drive: Coupling Audio Instructions for User-aware VLA-based Autonomous Driving",
      "url": "https://arxiv.org/abs/2601.12142",
      "date": "2026-01-29",
      "authors_text": "Ziang Guo, Feng Yang, Xuefeng Zhang, Jiaqi Guo, Kun Zhao",
      "is_highlight": false,
      "score": 88.0,
      "summary": "EchoVLA enhances autonomous driving by integrating audio instructions with visual inputs, allowing for user-specific, emotionally adaptive driving behaviors, significantly improving performance metrics.",
      "teaser_image": "https://arxiv.org/html/2601.12142/Figs/overview.png",
      "debug_abstract": "Vision Language Action (VLA) models promise an open-vocabulary interface that can translate perceptual ambiguity into semantically grounded driving decisions, yet they still treat language as a static prior fixed at inference time. As a result, the model must infer continuously shifting objectives from pixels alone, yielding delayed or overly conservative maneuvers. We argue that effective VLAs for autonomous driving need an online channel in which users can influence driving with specific intentions. To this end, we present EchoVLA, a user-aware VLA that couples camera streams with in situ audio instructions. We augment the nuScenes dataset with temporally aligned, intent-specific speech commands generated by converting ego-motion descriptions into synthetic audios. Further, we compose emotional speech-trajectory pairs into a multimodal Chain-of-Thought (CoT) for fine-tuning a Multimodal Large Model (MLM) based on Qwen2.5-Omni. Specifically, we synthesize the audio-augmented dataset with different emotion types paired with corresponding driving behaviors, leveraging the emotional cues embedded in tone, pitch, and speech tempo to reflect varying user states, such as urgent or hesitant intentions, thus enabling our EchoVLA to interpret not only the semantic content but also the emotional context of audio commands for more nuanced and emotionally adaptive driving behavior. In open-loop benchmarks, our approach reduces the average L2 error by $59.4\\%$ and the collision rate by $74.4\\%$ compared to the baseline of vision-only perception. More experiments on nuScenes dataset validate that EchoVLA not only steers the trajectory through audio instructions, but also modulates driving behavior in response to the emotions detected in the user&#39;s speech."
    }
  ],
  "机器人控制实验室": [
    {
      "title": "InspecSafe-V1: A Multimodal Benchmark for Safety Assessment in Industrial Inspection Scenarios",
      "url": "https://arxiv.org/abs/2601.21173",
      "date": "2026-01-29",
      "authors_text": "Zeyi Liu, Shuang Liu, Jihai Min, Zhaoheng Zhang, Jun Cen",
      "is_highlight": false,
      "score": 82.0,
      "summary": "InspecSafe-V1 is a multimodal benchmark dataset for safety assessment in industrial inspections, featuring real-world data from diverse scenarios and multiple sensing modalities.",
      "teaser_image": "https://arxiv.org/html/2601.21173/x1.png",
      "debug_abstract": "With the rapid development of industrial intelligence and unmanned inspection, reliable perception and safety assessment for AI systems in complex and dynamic industrial sites has become a key bottleneck for deploying predictive maintenance and autonomous inspection. Most public datasets remain limited by simulated data sources, single-modality sensing, or the absence of fine-grained object-level annotations, which prevents robust scene understanding and multimodal safety reasoning for industrial foundation models. To address these limitations, InspecSafe-V1 is released as the first multimodal benchmark dataset for industrial inspection safety assessment that is collected from routine operations of real inspection robots in real-world environments. InspecSafe-V1 covers five representative industrial scenarios, including tunnels, power facilities, sintering equipment, oil and gas petrochemical plants, and coal conveyor trestles. The dataset is constructed from 41 wheeled and rail-mounted inspection robots operating at 2,239 valid inspection sites, yielding 5,013 inspection instances. For each instance, pixel-level segmentation annotations are provided for key objects in visible-spectrum images. In addition, a semantic scene description and a corresponding safety level label are provided according to practical inspection tasks. Seven synchronized sensing modalities are further included, including infrared video, audio, depth point clouds, radar point clouds, gas measurements, temperature, and humidity, to support multimodal anomaly recognition, cross-modal fusion, and comprehensive safety assessment in industrial environments."
    },
    {
      "title": "Listen, Look, Drive: Coupling Audio Instructions for User-aware VLA-based Autonomous Driving",
      "url": "https://arxiv.org/abs/2601.12142",
      "date": "2026-01-29",
      "authors_text": "Ziang Guo, Feng Yang, Xuefeng Zhang, Jiaqi Guo, Kun Zhao",
      "is_highlight": false,
      "score": 88.0,
      "summary": "EchoVLA enhances autonomous driving by integrating audio instructions with visual inputs, allowing for user-specific, emotionally adaptive driving behaviors, significantly improving performance metrics.",
      "teaser_image": "https://arxiv.org/html/2601.12142/Figs/overview.png",
      "debug_abstract": "Vision Language Action (VLA) models promise an open-vocabulary interface that can translate perceptual ambiguity into semantically grounded driving decisions, yet they still treat language as a static prior fixed at inference time. As a result, the model must infer continuously shifting objectives from pixels alone, yielding delayed or overly conservative maneuvers. We argue that effective VLAs for autonomous driving need an online channel in which users can influence driving with specific intentions. To this end, we present EchoVLA, a user-aware VLA that couples camera streams with in situ audio instructions. We augment the nuScenes dataset with temporally aligned, intent-specific speech commands generated by converting ego-motion descriptions into synthetic audios. Further, we compose emotional speech-trajectory pairs into a multimodal Chain-of-Thought (CoT) for fine-tuning a Multimodal Large Model (MLM) based on Qwen2.5-Omni. Specifically, we synthesize the audio-augmented dataset with different emotion types paired with corresponding driving behaviors, leveraging the emotional cues embedded in tone, pitch, and speech tempo to reflect varying user states, such as urgent or hesitant intentions, thus enabling our EchoVLA to interpret not only the semantic content but also the emotional context of audio commands for more nuanced and emotionally adaptive driving behavior. In open-loop benchmarks, our approach reduces the average L2 error by $59.4\\%$ and the collision rate by $74.4\\%$ compared to the baseline of vision-only perception. More experiments on nuScenes dataset validate that EchoVLA not only steers the trajectory through audio instructions, but also modulates driving behavior in response to the emotions detected in the user&#39;s speech."
    }
  ],
  "智能产业研究院 (AIR)": [
    {
      "title": "InspecSafe-V1: A Multimodal Benchmark for Safety Assessment in Industrial Inspection Scenarios",
      "url": "https://arxiv.org/abs/2601.21173",
      "date": "2026-01-29",
      "authors_text": "Zeyi Liu, Shuang Liu, Jihai Min, Zhaoheng Zhang, Jun Cen",
      "is_highlight": false,
      "score": 82.0,
      "summary": "InspecSafe-V1 is a multimodal benchmark dataset for safety assessment in industrial inspections, featuring real-world data from diverse scenarios and multiple sensing modalities.",
      "teaser_image": "https://arxiv.org/html/2601.21173/x1.png",
      "debug_abstract": "With the rapid development of industrial intelligence and unmanned inspection, reliable perception and safety assessment for AI systems in complex and dynamic industrial sites has become a key bottleneck for deploying predictive maintenance and autonomous inspection. Most public datasets remain limited by simulated data sources, single-modality sensing, or the absence of fine-grained object-level annotations, which prevents robust scene understanding and multimodal safety reasoning for industrial foundation models. To address these limitations, InspecSafe-V1 is released as the first multimodal benchmark dataset for industrial inspection safety assessment that is collected from routine operations of real inspection robots in real-world environments. InspecSafe-V1 covers five representative industrial scenarios, including tunnels, power facilities, sintering equipment, oil and gas petrochemical plants, and coal conveyor trestles. The dataset is constructed from 41 wheeled and rail-mounted inspection robots operating at 2,239 valid inspection sites, yielding 5,013 inspection instances. For each instance, pixel-level segmentation annotations are provided for key objects in visible-spectrum images. In addition, a semantic scene description and a corresponding safety level label are provided according to practical inspection tasks. Seven synchronized sensing modalities are further included, including infrared video, audio, depth point clouds, radar point clouds, gas measurements, temperature, and humidity, to support multimodal anomaly recognition, cross-modal fusion, and comprehensive safety assessment in industrial environments."
    },
    {
      "title": "Listen, Look, Drive: Coupling Audio Instructions for User-aware VLA-based Autonomous Driving",
      "url": "https://arxiv.org/abs/2601.12142",
      "date": "2026-01-29",
      "authors_text": "Ziang Guo, Feng Yang, Xuefeng Zhang, Jiaqi Guo, Kun Zhao",
      "is_highlight": false,
      "score": 88.0,
      "summary": "EchoVLA enhances autonomous driving by integrating audio instructions with visual inputs, allowing for user-specific, emotionally adaptive driving behaviors, significantly improving performance metrics.",
      "teaser_image": "https://arxiv.org/html/2601.12142/Figs/overview.png",
      "debug_abstract": "Vision Language Action (VLA) models promise an open-vocabulary interface that can translate perceptual ambiguity into semantically grounded driving decisions, yet they still treat language as a static prior fixed at inference time. As a result, the model must infer continuously shifting objectives from pixels alone, yielding delayed or overly conservative maneuvers. We argue that effective VLAs for autonomous driving need an online channel in which users can influence driving with specific intentions. To this end, we present EchoVLA, a user-aware VLA that couples camera streams with in situ audio instructions. We augment the nuScenes dataset with temporally aligned, intent-specific speech commands generated by converting ego-motion descriptions into synthetic audios. Further, we compose emotional speech-trajectory pairs into a multimodal Chain-of-Thought (CoT) for fine-tuning a Multimodal Large Model (MLM) based on Qwen2.5-Omni. Specifically, we synthesize the audio-augmented dataset with different emotion types paired with corresponding driving behaviors, leveraging the emotional cues embedded in tone, pitch, and speech tempo to reflect varying user states, such as urgent or hesitant intentions, thus enabling our EchoVLA to interpret not only the semantic content but also the emotional context of audio commands for more nuanced and emotionally adaptive driving behavior. In open-loop benchmarks, our approach reduces the average L2 error by $59.4\\%$ and the collision rate by $74.4\\%$ compared to the baseline of vision-only perception. More experiments on nuScenes dataset validate that EchoVLA not only steers the trajectory through audio instructions, but also modulates driving behavior in response to the emotions detected in the user&#39;s speech."
    }
  ],
  "MARS多模态学习实验室": [
    {
      "title": "InspecSafe-V1: A Multimodal Benchmark for Safety Assessment in Industrial Inspection Scenarios",
      "url": "https://arxiv.org/abs/2601.21173",
      "date": "2026-01-29",
      "authors_text": "Zeyi Liu, Shuang Liu, Jihai Min, Zhaoheng Zhang, Jun Cen",
      "is_highlight": false,
      "score": 82.0,
      "summary": "InspecSafe-V1 is a multimodal benchmark dataset for safety assessment in industrial inspections, featuring real-world data from diverse scenarios and multiple sensing modalities.",
      "teaser_image": "https://arxiv.org/html/2601.21173/x1.png",
      "debug_abstract": "With the rapid development of industrial intelligence and unmanned inspection, reliable perception and safety assessment for AI systems in complex and dynamic industrial sites has become a key bottleneck for deploying predictive maintenance and autonomous inspection. Most public datasets remain limited by simulated data sources, single-modality sensing, or the absence of fine-grained object-level annotations, which prevents robust scene understanding and multimodal safety reasoning for industrial foundation models. To address these limitations, InspecSafe-V1 is released as the first multimodal benchmark dataset for industrial inspection safety assessment that is collected from routine operations of real inspection robots in real-world environments. InspecSafe-V1 covers five representative industrial scenarios, including tunnels, power facilities, sintering equipment, oil and gas petrochemical plants, and coal conveyor trestles. The dataset is constructed from 41 wheeled and rail-mounted inspection robots operating at 2,239 valid inspection sites, yielding 5,013 inspection instances. For each instance, pixel-level segmentation annotations are provided for key objects in visible-spectrum images. In addition, a semantic scene description and a corresponding safety level label are provided according to practical inspection tasks. Seven synchronized sensing modalities are further included, including infrared video, audio, depth point clouds, radar point clouds, gas measurements, temperature, and humidity, to support multimodal anomaly recognition, cross-modal fusion, and comprehensive safety assessment in industrial environments."
    },
    {
      "title": "Listen, Look, Drive: Coupling Audio Instructions for User-aware VLA-based Autonomous Driving",
      "url": "https://arxiv.org/abs/2601.12142",
      "date": "2026-01-29",
      "authors_text": "Ziang Guo, Feng Yang, Xuefeng Zhang, Jiaqi Guo, Kun Zhao",
      "is_highlight": false,
      "score": 88.0,
      "summary": "EchoVLA enhances autonomous driving by integrating audio instructions with visual inputs, allowing for user-specific, emotionally adaptive driving behaviors, significantly improving performance metrics.",
      "teaser_image": "https://arxiv.org/html/2601.12142/Figs/overview.png",
      "debug_abstract": "Vision Language Action (VLA) models promise an open-vocabulary interface that can translate perceptual ambiguity into semantically grounded driving decisions, yet they still treat language as a static prior fixed at inference time. As a result, the model must infer continuously shifting objectives from pixels alone, yielding delayed or overly conservative maneuvers. We argue that effective VLAs for autonomous driving need an online channel in which users can influence driving with specific intentions. To this end, we present EchoVLA, a user-aware VLA that couples camera streams with in situ audio instructions. We augment the nuScenes dataset with temporally aligned, intent-specific speech commands generated by converting ego-motion descriptions into synthetic audios. Further, we compose emotional speech-trajectory pairs into a multimodal Chain-of-Thought (CoT) for fine-tuning a Multimodal Large Model (MLM) based on Qwen2.5-Omni. Specifically, we synthesize the audio-augmented dataset with different emotion types paired with corresponding driving behaviors, leveraging the emotional cues embedded in tone, pitch, and speech tempo to reflect varying user states, such as urgent or hesitant intentions, thus enabling our EchoVLA to interpret not only the semantic content but also the emotional context of audio commands for more nuanced and emotionally adaptive driving behavior. In open-loop benchmarks, our approach reduces the average L2 error by $59.4\\%$ and the collision rate by $74.4\\%$ compared to the baseline of vision-only perception. More experiments on nuScenes dataset validate that EchoVLA not only steers the trajectory through audio instructions, but also modulates driving behavior in response to the emotions detected in the user&#39;s speech."
    }
  ],
  "具身智能实验室 (TEA Lab)": [
    {
      "title": "InspecSafe-V1: A Multimodal Benchmark for Safety Assessment in Industrial Inspection Scenarios",
      "url": "https://arxiv.org/abs/2601.21173",
      "date": "2026-01-29",
      "authors_text": "Zeyi Liu, Shuang Liu, Jihai Min, Zhaoheng Zhang, Jun Cen",
      "is_highlight": false,
      "score": 82.0,
      "summary": "InspecSafe-V1 is a multimodal benchmark dataset for safety assessment in industrial inspections, featuring real-world data from diverse scenarios and multiple sensing modalities.",
      "teaser_image": "https://arxiv.org/html/2601.21173/x1.png",
      "debug_abstract": "With the rapid development of industrial intelligence and unmanned inspection, reliable perception and safety assessment for AI systems in complex and dynamic industrial sites has become a key bottleneck for deploying predictive maintenance and autonomous inspection. Most public datasets remain limited by simulated data sources, single-modality sensing, or the absence of fine-grained object-level annotations, which prevents robust scene understanding and multimodal safety reasoning for industrial foundation models. To address these limitations, InspecSafe-V1 is released as the first multimodal benchmark dataset for industrial inspection safety assessment that is collected from routine operations of real inspection robots in real-world environments. InspecSafe-V1 covers five representative industrial scenarios, including tunnels, power facilities, sintering equipment, oil and gas petrochemical plants, and coal conveyor trestles. The dataset is constructed from 41 wheeled and rail-mounted inspection robots operating at 2,239 valid inspection sites, yielding 5,013 inspection instances. For each instance, pixel-level segmentation annotations are provided for key objects in visible-spectrum images. In addition, a semantic scene description and a corresponding safety level label are provided according to practical inspection tasks. Seven synchronized sensing modalities are further included, including infrared video, audio, depth point clouds, radar point clouds, gas measurements, temperature, and humidity, to support multimodal anomaly recognition, cross-modal fusion, and comprehensive safety assessment in industrial environments."
    },
    {
      "title": "Listen, Look, Drive: Coupling Audio Instructions for User-aware VLA-based Autonomous Driving",
      "url": "https://arxiv.org/abs/2601.12142",
      "date": "2026-01-29",
      "authors_text": "Ziang Guo, Feng Yang, Xuefeng Zhang, Jiaqi Guo, Kun Zhao",
      "is_highlight": false,
      "score": 88.0,
      "summary": "EchoVLA enhances autonomous driving by integrating audio instructions with visual inputs, allowing for user-specific, emotionally adaptive driving behaviors, significantly improving performance metrics.",
      "teaser_image": "https://arxiv.org/html/2601.12142/Figs/overview.png",
      "debug_abstract": "Vision Language Action (VLA) models promise an open-vocabulary interface that can translate perceptual ambiguity into semantically grounded driving decisions, yet they still treat language as a static prior fixed at inference time. As a result, the model must infer continuously shifting objectives from pixels alone, yielding delayed or overly conservative maneuvers. We argue that effective VLAs for autonomous driving need an online channel in which users can influence driving with specific intentions. To this end, we present EchoVLA, a user-aware VLA that couples camera streams with in situ audio instructions. We augment the nuScenes dataset with temporally aligned, intent-specific speech commands generated by converting ego-motion descriptions into synthetic audios. Further, we compose emotional speech-trajectory pairs into a multimodal Chain-of-Thought (CoT) for fine-tuning a Multimodal Large Model (MLM) based on Qwen2.5-Omni. Specifically, we synthesize the audio-augmented dataset with different emotion types paired with corresponding driving behaviors, leveraging the emotional cues embedded in tone, pitch, and speech tempo to reflect varying user states, such as urgent or hesitant intentions, thus enabling our EchoVLA to interpret not only the semantic content but also the emotional context of audio commands for more nuanced and emotionally adaptive driving behavior. In open-loop benchmarks, our approach reduces the average L2 error by $59.4\\%$ and the collision rate by $74.4\\%$ compared to the baseline of vision-only perception. More experiments on nuScenes dataset validate that EchoVLA not only steers the trajectory through audio instructions, but also modulates driving behavior in response to the emotions detected in the user&#39;s speech."
    }
  ],
  "香港大学机械工程系机器人实验室": [
    {
      "title": "PLANING: A Loosely Coupled Triangle-Gaussian Framework for Streaming 3D Reconstruction",
      "url": "https://arxiv.org/abs/2601.22046",
      "date": "2026-01-29",
      "authors_text": "Changjian Jiang, Kerui Ren, Xudong Li, Kaiwen Song, Linning Xu",
      "is_highlight": false,
      "score": 66.0,
      "summary": "PLANING is a novel streaming 3D reconstruction framework that efficiently combines geometric primitives and neural Gaussians, enhancing quality and speed while reducing redundancy.",
      "teaser_image": "https://arxiv.org/html/2601.22046/x3.png",
      "debug_abstract": "Streaming reconstruction from monocular image sequences remains challenging, as existing methods typically favor either high-quality rendering or accurate geometry, but rarely both. We present PLANING, an efficient on-the-fly reconstruction framework built on a hybrid representation that loosely couples explicit geometric primitives with neural Gaussians, enabling geometry and appearance to be modeled in a decoupled manner. This decoupling supports an online initialization and optimization strategy that separates geometry and appearance updates, yielding stable streaming reconstruction with substantially reduced structural redundancy. PLANING improves dense mesh Chamfer-L2 by 18.52% over PGSR, surpasses ARTDECO by 1.31 dB PSNR, and reconstructs ScanNetV2 scenes in under 100 seconds, over 5x faster than 2D Gaussian Splatting, while matching the quality of offline per-scene optimization. Beyond reconstruction quality, the structural clarity and computational efficiency of \\modelname~make it well suited for a broad range of downstream applications, such as enabling large-scale scene modeling and simulation-ready environments for embodied AI. Project page: this https URL ."
    },
    {
      "title": "Dynamic Topology Awareness: Breaking the Granularity Rigidity in Vision-Language Navigation",
      "url": "https://arxiv.org/abs/2601.21751",
      "date": "2026-01-29",
      "authors_text": "Jiankun Peng, Jianyuan Guo, Ying Xu, Yue Liu, Jiashuang Yan",
      "is_highlight": false,
      "score": 84.0,
      "summary": "The paper presents DGNav, a dynamic framework for Vision-Language Navigation that adapts topological map density and connectivity to enhance navigation efficiency and safety in complex environments.",
      "teaser_image": "https://arxiv.org/html/2601.21751/x1.png",
      "debug_abstract": "Vision-Language Navigation in Continuous Environments (VLN-CE) presents a core challenge: grounding high-level linguistic instructions into precise, safe, and long-horizon spatial actions. Explicit topological maps have proven to be a vital solution for providing robust spatial memory in such tasks. However, existing topological planning methods suffer from a &#34;Granularity Rigidity&#34; problem. Specifically, these methods typically rely on fixed geometric thresholds to sample nodes, which fails to adapt to varying environmental complexities. This rigidity leads to a critical mismatch: the model tends to over-sample in simple areas, causing computational redundancy, while under-sampling in high-uncertainty regions, increasing collision risks and compromising precision. To address this, we propose DGNav, a framework for Dynamic Topological Navigation, introducing a context-aware mechanism to modulate map density and connectivity on-the-fly. Our approach comprises two core innovations: (1) A Scene-Aware Adaptive Strategy that dynamically modulates graph construction thresholds based on the dispersion of predicted waypoints, enabling &#34;densification on demand&#34; in challenging environments; (2) A Dynamic Graph Transformer that reconstructs graph connectivity by fusing visual, linguistic, and geometric cues into dynamic edge weights, enabling the agent to filter out topological noise and enhancing instruction adherence. Extensive experiments on the R2R-CE and RxR-CE benchmarks demonstrate DGNav exhibits superior navigation performance and strong generalization capabilities. Furthermore, ablation studies confirm that our framework achieves an optimal trade-off between navigation efficiency and safe exploration. The code is available at this https URL."
    },
    {
      "title": "Disentangling perception and reasoning for improving data efficiency in learning cloth manipulation without demonstrations",
      "url": "https://arxiv.org/abs/2601.21713",
      "date": "2026-01-29",
      "authors_text": "Donatien Delehelle, Fei Chen, Darwin Caldwell",
      "is_highlight": false,
      "score": 87.0,
      "summary": "This paper presents a modular reinforcement learning approach for cloth manipulation that enhances data efficiency and reduces model size and training time, achieving improved performance in simulation and real-world transfer.",
      "teaser_image": "https://arxiv.org/html/2601.21713/x1.png",
      "debug_abstract": "Cloth manipulation is a ubiquitous task in everyday life, but it remains an open challenge for robotics. The difficulties in developing cloth manipulation policies are attributed to the high-dimensional state space, complex dynamics, and high propensity to self-occlusion exhibited by fabrics. As analytical methods have not been able to provide robust and general manipulation policies, reinforcement learning (RL) is considered a promising approach to these problems. However, to address the large state space and complex dynamics, data-based methods usually rely on large models and long training times. The resulting computational cost significantly hampers the development and adoption of these methods. Additionally, due to the challenge of robust state estimation, garment manipulation policies often adopt an end-to-end learning approach with workspace images as input. While this approach enables a conceptually straightforward sim-to-real transfer via real-world fine-tuning, it also incurs a significant computational cost by training agents on a highly lossy representation of the environment state. This paper questions this common design choice by exploring an efficient and modular approach to RL for cloth manipulation. We show that, through careful design choices, model size and training time can be significantly reduced when learning in simulation. Furthermore, we demonstrate how the resulting simulation-trained model can be transferred to the real world. We evaluate our approach on the SoftGym benchmark and achieve significant performance improvements over available baselines on our task, while using a substantially smaller model."
    },
    {
      "title": "Heterogeneous Vertiport Selection Optimization for On-Demand Air Taxi Services: A Deep Reinforcement Learning Approach",
      "url": "https://arxiv.org/abs/2601.21316",
      "date": "2026-01-29",
      "authors_text": "Aoyu Pang, Maonan Wang, Zifan Sha, Wenwei Yue, Changle Li",
      "is_highlight": false,
      "score": 80.0,
      "summary": "The paper presents a deep reinforcement learning framework for optimizing vertiport selection and air taxi routing, achieving significant travel time reductions in urban air mobility systems.",
      "teaser_image": "https://arxiv.org/html/2601.21316/x2.png",
      "debug_abstract": "Urban Air Mobility (UAM) has emerged as a transformative solution to alleviate urban congestion by utilizing low-altitude airspace, thereby reducing pressure on ground transportation networks. To enable truly efficient and seamless door-to-door travel experiences, UAM requires close integration with existing ground transportation infrastructure. However, current research on optimal integrated routing strategies for passengers in air-ground mobility systems remains limited, with a lack of systematic this http URL address this gap, we first propose a unified optimization model that integrates strategy selection for both air and ground transportation. This model captures the dynamic characteristics of multimodal transport networks and incorporates real-time traffic conditions alongside passenger decision-making behavior. Building on this model, we propose a Unified Air-Ground Mobility Coordination (UAGMC) framework, which leverages deep reinforcement learning (RL) and Vehicle-to-Everything (V2X) communication to optimize vertiport selection and dynamically plan air taxi routes. Experimental results demonstrate that UAGMC achieves a 34\\% reduction in average travel time compared to conventional proportional allocation methods, enhancing overall travel efficiency and providing novel insights into the integration and optimization of multimodal transportation systems. This work lays a solid foundation for advancing intelligent urban mobility solutions through the coordination of air and ground transportation modes. The related code can be found at this https URL."
    },
    {
      "title": "Listen, Look, Drive: Coupling Audio Instructions for User-aware VLA-based Autonomous Driving",
      "url": "https://arxiv.org/abs/2601.12142",
      "date": "2026-01-29",
      "authors_text": "Ziang Guo, Feng Yang, Xuefeng Zhang, Jiaqi Guo, Kun Zhao",
      "is_highlight": false,
      "score": 88.0,
      "summary": "EchoVLA enhances autonomous driving by integrating audio instructions with visual inputs, allowing for user-specific, emotionally adaptive driving behaviors, significantly improving performance metrics.",
      "teaser_image": "https://arxiv.org/html/2601.12142/Figs/overview.png",
      "debug_abstract": "Vision Language Action (VLA) models promise an open-vocabulary interface that can translate perceptual ambiguity into semantically grounded driving decisions, yet they still treat language as a static prior fixed at inference time. As a result, the model must infer continuously shifting objectives from pixels alone, yielding delayed or overly conservative maneuvers. We argue that effective VLAs for autonomous driving need an online channel in which users can influence driving with specific intentions. To this end, we present EchoVLA, a user-aware VLA that couples camera streams with in situ audio instructions. We augment the nuScenes dataset with temporally aligned, intent-specific speech commands generated by converting ego-motion descriptions into synthetic audios. Further, we compose emotional speech-trajectory pairs into a multimodal Chain-of-Thought (CoT) for fine-tuning a Multimodal Large Model (MLM) based on Qwen2.5-Omni. Specifically, we synthesize the audio-augmented dataset with different emotion types paired with corresponding driving behaviors, leveraging the emotional cues embedded in tone, pitch, and speech tempo to reflect varying user states, such as urgent or hesitant intentions, thus enabling our EchoVLA to interpret not only the semantic content but also the emotional context of audio commands for more nuanced and emotionally adaptive driving behavior. In open-loop benchmarks, our approach reduces the average L2 error by $59.4\\%$ and the collision rate by $74.4\\%$ compared to the baseline of vision-only perception. More experiments on nuScenes dataset validate that EchoVLA not only steers the trajectory through audio instructions, but also modulates driving behavior in response to the emotions detected in the user&#39;s speech."
    }
  ],
  "智能系统与机器人实验室 (ISR Lab)": [
    {
      "title": "InspecSafe-V1: A Multimodal Benchmark for Safety Assessment in Industrial Inspection Scenarios",
      "url": "https://arxiv.org/abs/2601.21173",
      "date": "2026-01-29",
      "authors_text": "Zeyi Liu, Shuang Liu, Jihai Min, Zhaoheng Zhang, Jun Cen",
      "is_highlight": false,
      "score": 82.0,
      "summary": "InspecSafe-V1 is a multimodal benchmark dataset for safety assessment in industrial inspections, featuring real-world data from diverse scenarios and multiple sensing modalities.",
      "teaser_image": "https://arxiv.org/html/2601.21173/x1.png",
      "debug_abstract": "With the rapid development of industrial intelligence and unmanned inspection, reliable perception and safety assessment for AI systems in complex and dynamic industrial sites has become a key bottleneck for deploying predictive maintenance and autonomous inspection. Most public datasets remain limited by simulated data sources, single-modality sensing, or the absence of fine-grained object-level annotations, which prevents robust scene understanding and multimodal safety reasoning for industrial foundation models. To address these limitations, InspecSafe-V1 is released as the first multimodal benchmark dataset for industrial inspection safety assessment that is collected from routine operations of real inspection robots in real-world environments. InspecSafe-V1 covers five representative industrial scenarios, including tunnels, power facilities, sintering equipment, oil and gas petrochemical plants, and coal conveyor trestles. The dataset is constructed from 41 wheeled and rail-mounted inspection robots operating at 2,239 valid inspection sites, yielding 5,013 inspection instances. For each instance, pixel-level segmentation annotations are provided for key objects in visible-spectrum images. In addition, a semantic scene description and a corresponding safety level label are provided according to practical inspection tasks. Seven synchronized sensing modalities are further included, including infrared video, audio, depth point clouds, radar point clouds, gas measurements, temperature, and humidity, to support multimodal anomaly recognition, cross-modal fusion, and comprehensive safety assessment in industrial environments."
    },
    {
      "title": "Listen, Look, Drive: Coupling Audio Instructions for User-aware VLA-based Autonomous Driving",
      "url": "https://arxiv.org/abs/2601.12142",
      "date": "2026-01-29",
      "authors_text": "Ziang Guo, Feng Yang, Xuefeng Zhang, Jiaqi Guo, Kun Zhao",
      "is_highlight": false,
      "score": 88.0,
      "summary": "EchoVLA enhances autonomous driving by integrating audio instructions with visual inputs, allowing for user-specific, emotionally adaptive driving behaviors, significantly improving performance metrics.",
      "teaser_image": "https://arxiv.org/html/2601.12142/Figs/overview.png",
      "debug_abstract": "Vision Language Action (VLA) models promise an open-vocabulary interface that can translate perceptual ambiguity into semantically grounded driving decisions, yet they still treat language as a static prior fixed at inference time. As a result, the model must infer continuously shifting objectives from pixels alone, yielding delayed or overly conservative maneuvers. We argue that effective VLAs for autonomous driving need an online channel in which users can influence driving with specific intentions. To this end, we present EchoVLA, a user-aware VLA that couples camera streams with in situ audio instructions. We augment the nuScenes dataset with temporally aligned, intent-specific speech commands generated by converting ego-motion descriptions into synthetic audios. Further, we compose emotional speech-trajectory pairs into a multimodal Chain-of-Thought (CoT) for fine-tuning a Multimodal Large Model (MLM) based on Qwen2.5-Omni. Specifically, we synthesize the audio-augmented dataset with different emotion types paired with corresponding driving behaviors, leveraging the emotional cues embedded in tone, pitch, and speech tempo to reflect varying user states, such as urgent or hesitant intentions, thus enabling our EchoVLA to interpret not only the semantic content but also the emotional context of audio commands for more nuanced and emotionally adaptive driving behavior. In open-loop benchmarks, our approach reduces the average L2 error by $59.4\\%$ and the collision rate by $74.4\\%$ compared to the baseline of vision-only perception. More experiments on nuScenes dataset validate that EchoVLA not only steers the trajectory through audio instructions, but also modulates driving behavior in response to the emotions detected in the user&#39;s speech."
    }
  ],
  "OpenDriveLab": [
    {
      "title": "PLANING: A Loosely Coupled Triangle-Gaussian Framework for Streaming 3D Reconstruction",
      "url": "https://arxiv.org/abs/2601.22046",
      "date": "2026-01-29",
      "authors_text": "Changjian Jiang, Kerui Ren, Xudong Li, Kaiwen Song, Linning Xu",
      "is_highlight": false,
      "score": 66.0,
      "summary": "PLANING is a novel streaming 3D reconstruction framework that efficiently combines geometric primitives and neural Gaussians, enhancing quality and speed while reducing redundancy.",
      "teaser_image": "https://arxiv.org/html/2601.22046/x3.png",
      "debug_abstract": "Streaming reconstruction from monocular image sequences remains challenging, as existing methods typically favor either high-quality rendering or accurate geometry, but rarely both. We present PLANING, an efficient on-the-fly reconstruction framework built on a hybrid representation that loosely couples explicit geometric primitives with neural Gaussians, enabling geometry and appearance to be modeled in a decoupled manner. This decoupling supports an online initialization and optimization strategy that separates geometry and appearance updates, yielding stable streaming reconstruction with substantially reduced structural redundancy. PLANING improves dense mesh Chamfer-L2 by 18.52% over PGSR, surpasses ARTDECO by 1.31 dB PSNR, and reconstructs ScanNetV2 scenes in under 100 seconds, over 5x faster than 2D Gaussian Splatting, while matching the quality of offline per-scene optimization. Beyond reconstruction quality, the structural clarity and computational efficiency of \\modelname~make it well suited for a broad range of downstream applications, such as enabling large-scale scene modeling and simulation-ready environments for embodied AI. Project page: this https URL ."
    },
    {
      "title": "Dynamic Topology Awareness: Breaking the Granularity Rigidity in Vision-Language Navigation",
      "url": "https://arxiv.org/abs/2601.21751",
      "date": "2026-01-29",
      "authors_text": "Jiankun Peng, Jianyuan Guo, Ying Xu, Yue Liu, Jiashuang Yan",
      "is_highlight": false,
      "score": 84.0,
      "summary": "The paper presents DGNav, a dynamic framework for Vision-Language Navigation that adapts topological map density and connectivity to enhance navigation efficiency and safety in complex environments.",
      "teaser_image": "https://arxiv.org/html/2601.21751/x1.png",
      "debug_abstract": "Vision-Language Navigation in Continuous Environments (VLN-CE) presents a core challenge: grounding high-level linguistic instructions into precise, safe, and long-horizon spatial actions. Explicit topological maps have proven to be a vital solution for providing robust spatial memory in such tasks. However, existing topological planning methods suffer from a &#34;Granularity Rigidity&#34; problem. Specifically, these methods typically rely on fixed geometric thresholds to sample nodes, which fails to adapt to varying environmental complexities. This rigidity leads to a critical mismatch: the model tends to over-sample in simple areas, causing computational redundancy, while under-sampling in high-uncertainty regions, increasing collision risks and compromising precision. To address this, we propose DGNav, a framework for Dynamic Topological Navigation, introducing a context-aware mechanism to modulate map density and connectivity on-the-fly. Our approach comprises two core innovations: (1) A Scene-Aware Adaptive Strategy that dynamically modulates graph construction thresholds based on the dispersion of predicted waypoints, enabling &#34;densification on demand&#34; in challenging environments; (2) A Dynamic Graph Transformer that reconstructs graph connectivity by fusing visual, linguistic, and geometric cues into dynamic edge weights, enabling the agent to filter out topological noise and enhancing instruction adherence. Extensive experiments on the R2R-CE and RxR-CE benchmarks demonstrate DGNav exhibits superior navigation performance and strong generalization capabilities. Furthermore, ablation studies confirm that our framework achieves an optimal trade-off between navigation efficiency and safe exploration. The code is available at this https URL."
    },
    {
      "title": "Disentangling perception and reasoning for improving data efficiency in learning cloth manipulation without demonstrations",
      "url": "https://arxiv.org/abs/2601.21713",
      "date": "2026-01-29",
      "authors_text": "Donatien Delehelle, Fei Chen, Darwin Caldwell",
      "is_highlight": false,
      "score": 87.0,
      "summary": "This paper presents a modular reinforcement learning approach for cloth manipulation that enhances data efficiency and reduces model size and training time, achieving improved performance in simulation and real-world transfer.",
      "teaser_image": "https://arxiv.org/html/2601.21713/x1.png",
      "debug_abstract": "Cloth manipulation is a ubiquitous task in everyday life, but it remains an open challenge for robotics. The difficulties in developing cloth manipulation policies are attributed to the high-dimensional state space, complex dynamics, and high propensity to self-occlusion exhibited by fabrics. As analytical methods have not been able to provide robust and general manipulation policies, reinforcement learning (RL) is considered a promising approach to these problems. However, to address the large state space and complex dynamics, data-based methods usually rely on large models and long training times. The resulting computational cost significantly hampers the development and adoption of these methods. Additionally, due to the challenge of robust state estimation, garment manipulation policies often adopt an end-to-end learning approach with workspace images as input. While this approach enables a conceptually straightforward sim-to-real transfer via real-world fine-tuning, it also incurs a significant computational cost by training agents on a highly lossy representation of the environment state. This paper questions this common design choice by exploring an efficient and modular approach to RL for cloth manipulation. We show that, through careful design choices, model size and training time can be significantly reduced when learning in simulation. Furthermore, we demonstrate how the resulting simulation-trained model can be transferred to the real world. We evaluate our approach on the SoftGym benchmark and achieve significant performance improvements over available baselines on our task, while using a substantially smaller model."
    },
    {
      "title": "Heterogeneous Vertiport Selection Optimization for On-Demand Air Taxi Services: A Deep Reinforcement Learning Approach",
      "url": "https://arxiv.org/abs/2601.21316",
      "date": "2026-01-29",
      "authors_text": "Aoyu Pang, Maonan Wang, Zifan Sha, Wenwei Yue, Changle Li",
      "is_highlight": false,
      "score": 80.0,
      "summary": "The paper presents a deep reinforcement learning framework for optimizing vertiport selection and air taxi routing, achieving significant travel time reductions in urban air mobility systems.",
      "teaser_image": "https://arxiv.org/html/2601.21316/x2.png",
      "debug_abstract": "Urban Air Mobility (UAM) has emerged as a transformative solution to alleviate urban congestion by utilizing low-altitude airspace, thereby reducing pressure on ground transportation networks. To enable truly efficient and seamless door-to-door travel experiences, UAM requires close integration with existing ground transportation infrastructure. However, current research on optimal integrated routing strategies for passengers in air-ground mobility systems remains limited, with a lack of systematic this http URL address this gap, we first propose a unified optimization model that integrates strategy selection for both air and ground transportation. This model captures the dynamic characteristics of multimodal transport networks and incorporates real-time traffic conditions alongside passenger decision-making behavior. Building on this model, we propose a Unified Air-Ground Mobility Coordination (UAGMC) framework, which leverages deep reinforcement learning (RL) and Vehicle-to-Everything (V2X) communication to optimize vertiport selection and dynamically plan air taxi routes. Experimental results demonstrate that UAGMC achieves a 34\\% reduction in average travel time compared to conventional proportional allocation methods, enhancing overall travel efficiency and providing novel insights into the integration and optimization of multimodal transportation systems. This work lays a solid foundation for advancing intelligent urban mobility solutions through the coordination of air and ground transportation modes. The related code can be found at this https URL."
    },
    {
      "title": "Listen, Look, Drive: Coupling Audio Instructions for User-aware VLA-based Autonomous Driving",
      "url": "https://arxiv.org/abs/2601.12142",
      "date": "2026-01-29",
      "authors_text": "Ziang Guo, Feng Yang, Xuefeng Zhang, Jiaqi Guo, Kun Zhao",
      "is_highlight": false,
      "score": 88.0,
      "summary": "EchoVLA enhances autonomous driving by integrating audio instructions with visual inputs, allowing for user-specific, emotionally adaptive driving behaviors, significantly improving performance metrics.",
      "teaser_image": "https://arxiv.org/html/2601.12142/Figs/overview.png",
      "debug_abstract": "Vision Language Action (VLA) models promise an open-vocabulary interface that can translate perceptual ambiguity into semantically grounded driving decisions, yet they still treat language as a static prior fixed at inference time. As a result, the model must infer continuously shifting objectives from pixels alone, yielding delayed or overly conservative maneuvers. We argue that effective VLAs for autonomous driving need an online channel in which users can influence driving with specific intentions. To this end, we present EchoVLA, a user-aware VLA that couples camera streams with in situ audio instructions. We augment the nuScenes dataset with temporally aligned, intent-specific speech commands generated by converting ego-motion descriptions into synthetic audios. Further, we compose emotional speech-trajectory pairs into a multimodal Chain-of-Thought (CoT) for fine-tuning a Multimodal Large Model (MLM) based on Qwen2.5-Omni. Specifically, we synthesize the audio-augmented dataset with different emotion types paired with corresponding driving behaviors, leveraging the emotional cues embedded in tone, pitch, and speech tempo to reflect varying user states, such as urgent or hesitant intentions, thus enabling our EchoVLA to interpret not only the semantic content but also the emotional context of audio commands for more nuanced and emotionally adaptive driving behavior. In open-loop benchmarks, our approach reduces the average L2 error by $59.4\\%$ and the collision rate by $74.4\\%$ compared to the baseline of vision-only perception. More experiments on nuScenes dataset validate that EchoVLA not only steers the trajectory through audio instructions, but also modulates driving behavior in response to the emotions detected in the user&#39;s speech."
    }
  ],
  "Hengshuang Zhao老师实验室": [
    {
      "title": "PLANING: A Loosely Coupled Triangle-Gaussian Framework for Streaming 3D Reconstruction",
      "url": "https://arxiv.org/abs/2601.22046",
      "date": "2026-01-29",
      "authors_text": "Changjian Jiang, Kerui Ren, Xudong Li, Kaiwen Song, Linning Xu",
      "is_highlight": false,
      "score": 66.0,
      "summary": "PLANING is a novel streaming 3D reconstruction framework that efficiently combines geometric primitives and neural Gaussians, enhancing quality and speed while reducing redundancy.",
      "teaser_image": "https://arxiv.org/html/2601.22046/x3.png",
      "debug_abstract": "Streaming reconstruction from monocular image sequences remains challenging, as existing methods typically favor either high-quality rendering or accurate geometry, but rarely both. We present PLANING, an efficient on-the-fly reconstruction framework built on a hybrid representation that loosely couples explicit geometric primitives with neural Gaussians, enabling geometry and appearance to be modeled in a decoupled manner. This decoupling supports an online initialization and optimization strategy that separates geometry and appearance updates, yielding stable streaming reconstruction with substantially reduced structural redundancy. PLANING improves dense mesh Chamfer-L2 by 18.52% over PGSR, surpasses ARTDECO by 1.31 dB PSNR, and reconstructs ScanNetV2 scenes in under 100 seconds, over 5x faster than 2D Gaussian Splatting, while matching the quality of offline per-scene optimization. Beyond reconstruction quality, the structural clarity and computational efficiency of \\modelname~make it well suited for a broad range of downstream applications, such as enabling large-scale scene modeling and simulation-ready environments for embodied AI. Project page: this https URL ."
    },
    {
      "title": "Dynamic Topology Awareness: Breaking the Granularity Rigidity in Vision-Language Navigation",
      "url": "https://arxiv.org/abs/2601.21751",
      "date": "2026-01-29",
      "authors_text": "Jiankun Peng, Jianyuan Guo, Ying Xu, Yue Liu, Jiashuang Yan",
      "is_highlight": false,
      "score": 84.0,
      "summary": "The paper presents DGNav, a dynamic framework for Vision-Language Navigation that adapts topological map density and connectivity to enhance navigation efficiency and safety in complex environments.",
      "teaser_image": "https://arxiv.org/html/2601.21751/x1.png",
      "debug_abstract": "Vision-Language Navigation in Continuous Environments (VLN-CE) presents a core challenge: grounding high-level linguistic instructions into precise, safe, and long-horizon spatial actions. Explicit topological maps have proven to be a vital solution for providing robust spatial memory in such tasks. However, existing topological planning methods suffer from a &#34;Granularity Rigidity&#34; problem. Specifically, these methods typically rely on fixed geometric thresholds to sample nodes, which fails to adapt to varying environmental complexities. This rigidity leads to a critical mismatch: the model tends to over-sample in simple areas, causing computational redundancy, while under-sampling in high-uncertainty regions, increasing collision risks and compromising precision. To address this, we propose DGNav, a framework for Dynamic Topological Navigation, introducing a context-aware mechanism to modulate map density and connectivity on-the-fly. Our approach comprises two core innovations: (1) A Scene-Aware Adaptive Strategy that dynamically modulates graph construction thresholds based on the dispersion of predicted waypoints, enabling &#34;densification on demand&#34; in challenging environments; (2) A Dynamic Graph Transformer that reconstructs graph connectivity by fusing visual, linguistic, and geometric cues into dynamic edge weights, enabling the agent to filter out topological noise and enhancing instruction adherence. Extensive experiments on the R2R-CE and RxR-CE benchmarks demonstrate DGNav exhibits superior navigation performance and strong generalization capabilities. Furthermore, ablation studies confirm that our framework achieves an optimal trade-off between navigation efficiency and safe exploration. The code is available at this https URL."
    },
    {
      "title": "Disentangling perception and reasoning for improving data efficiency in learning cloth manipulation without demonstrations",
      "url": "https://arxiv.org/abs/2601.21713",
      "date": "2026-01-29",
      "authors_text": "Donatien Delehelle, Fei Chen, Darwin Caldwell",
      "is_highlight": false,
      "score": 87.0,
      "summary": "This paper presents a modular reinforcement learning approach for cloth manipulation that enhances data efficiency and reduces model size and training time, achieving improved performance in simulation and real-world transfer.",
      "teaser_image": "https://arxiv.org/html/2601.21713/x1.png",
      "debug_abstract": "Cloth manipulation is a ubiquitous task in everyday life, but it remains an open challenge for robotics. The difficulties in developing cloth manipulation policies are attributed to the high-dimensional state space, complex dynamics, and high propensity to self-occlusion exhibited by fabrics. As analytical methods have not been able to provide robust and general manipulation policies, reinforcement learning (RL) is considered a promising approach to these problems. However, to address the large state space and complex dynamics, data-based methods usually rely on large models and long training times. The resulting computational cost significantly hampers the development and adoption of these methods. Additionally, due to the challenge of robust state estimation, garment manipulation policies often adopt an end-to-end learning approach with workspace images as input. While this approach enables a conceptually straightforward sim-to-real transfer via real-world fine-tuning, it also incurs a significant computational cost by training agents on a highly lossy representation of the environment state. This paper questions this common design choice by exploring an efficient and modular approach to RL for cloth manipulation. We show that, through careful design choices, model size and training time can be significantly reduced when learning in simulation. Furthermore, we demonstrate how the resulting simulation-trained model can be transferred to the real world. We evaluate our approach on the SoftGym benchmark and achieve significant performance improvements over available baselines on our task, while using a substantially smaller model."
    },
    {
      "title": "Heterogeneous Vertiport Selection Optimization for On-Demand Air Taxi Services: A Deep Reinforcement Learning Approach",
      "url": "https://arxiv.org/abs/2601.21316",
      "date": "2026-01-29",
      "authors_text": "Aoyu Pang, Maonan Wang, Zifan Sha, Wenwei Yue, Changle Li",
      "is_highlight": false,
      "score": 80.0,
      "summary": "The paper presents a deep reinforcement learning framework for optimizing vertiport selection and air taxi routing, achieving significant travel time reductions in urban air mobility systems.",
      "teaser_image": "https://arxiv.org/html/2601.21316/x2.png",
      "debug_abstract": "Urban Air Mobility (UAM) has emerged as a transformative solution to alleviate urban congestion by utilizing low-altitude airspace, thereby reducing pressure on ground transportation networks. To enable truly efficient and seamless door-to-door travel experiences, UAM requires close integration with existing ground transportation infrastructure. However, current research on optimal integrated routing strategies for passengers in air-ground mobility systems remains limited, with a lack of systematic this http URL address this gap, we first propose a unified optimization model that integrates strategy selection for both air and ground transportation. This model captures the dynamic characteristics of multimodal transport networks and incorporates real-time traffic conditions alongside passenger decision-making behavior. Building on this model, we propose a Unified Air-Ground Mobility Coordination (UAGMC) framework, which leverages deep reinforcement learning (RL) and Vehicle-to-Everything (V2X) communication to optimize vertiport selection and dynamically plan air taxi routes. Experimental results demonstrate that UAGMC achieves a 34\\% reduction in average travel time compared to conventional proportional allocation methods, enhancing overall travel efficiency and providing novel insights into the integration and optimization of multimodal transportation systems. This work lays a solid foundation for advancing intelligent urban mobility solutions through the coordination of air and ground transportation modes. The related code can be found at this https URL."
    },
    {
      "title": "Listen, Look, Drive: Coupling Audio Instructions for User-aware VLA-based Autonomous Driving",
      "url": "https://arxiv.org/abs/2601.12142",
      "date": "2026-01-29",
      "authors_text": "Ziang Guo, Feng Yang, Xuefeng Zhang, Jiaqi Guo, Kun Zhao",
      "is_highlight": false,
      "score": 88.0,
      "summary": "EchoVLA enhances autonomous driving by integrating audio instructions with visual inputs, allowing for user-specific, emotionally adaptive driving behaviors, significantly improving performance metrics.",
      "teaser_image": "https://arxiv.org/html/2601.12142/Figs/overview.png",
      "debug_abstract": "Vision Language Action (VLA) models promise an open-vocabulary interface that can translate perceptual ambiguity into semantically grounded driving decisions, yet they still treat language as a static prior fixed at inference time. As a result, the model must infer continuously shifting objectives from pixels alone, yielding delayed or overly conservative maneuvers. We argue that effective VLAs for autonomous driving need an online channel in which users can influence driving with specific intentions. To this end, we present EchoVLA, a user-aware VLA that couples camera streams with in situ audio instructions. We augment the nuScenes dataset with temporally aligned, intent-specific speech commands generated by converting ego-motion descriptions into synthetic audios. Further, we compose emotional speech-trajectory pairs into a multimodal Chain-of-Thought (CoT) for fine-tuning a Multimodal Large Model (MLM) based on Qwen2.5-Omni. Specifically, we synthesize the audio-augmented dataset with different emotion types paired with corresponding driving behaviors, leveraging the emotional cues embedded in tone, pitch, and speech tempo to reflect varying user states, such as urgent or hesitant intentions, thus enabling our EchoVLA to interpret not only the semantic content but also the emotional context of audio commands for more nuanced and emotionally adaptive driving behavior. In open-loop benchmarks, our approach reduces the average L2 error by $59.4\\%$ and the collision rate by $74.4\\%$ compared to the baseline of vision-only perception. More experiments on nuScenes dataset validate that EchoVLA not only steers the trajectory through audio instructions, but also modulates driving behavior in response to the emotions detected in the user&#39;s speech."
    }
  ],
  "智能技术与系统实验室": [
    {
      "title": "InspecSafe-V1: A Multimodal Benchmark for Safety Assessment in Industrial Inspection Scenarios",
      "url": "https://arxiv.org/abs/2601.21173",
      "date": "2026-01-29",
      "authors_text": "Zeyi Liu, Shuang Liu, Jihai Min, Zhaoheng Zhang, Jun Cen",
      "is_highlight": false,
      "score": 82.0,
      "summary": "InspecSafe-V1 is a multimodal benchmark dataset for safety assessment in industrial inspections, featuring real-world data from diverse scenarios and multiple sensing modalities.",
      "teaser_image": "https://arxiv.org/html/2601.21173/x1.png",
      "debug_abstract": "With the rapid development of industrial intelligence and unmanned inspection, reliable perception and safety assessment for AI systems in complex and dynamic industrial sites has become a key bottleneck for deploying predictive maintenance and autonomous inspection. Most public datasets remain limited by simulated data sources, single-modality sensing, or the absence of fine-grained object-level annotations, which prevents robust scene understanding and multimodal safety reasoning for industrial foundation models. To address these limitations, InspecSafe-V1 is released as the first multimodal benchmark dataset for industrial inspection safety assessment that is collected from routine operations of real inspection robots in real-world environments. InspecSafe-V1 covers five representative industrial scenarios, including tunnels, power facilities, sintering equipment, oil and gas petrochemical plants, and coal conveyor trestles. The dataset is constructed from 41 wheeled and rail-mounted inspection robots operating at 2,239 valid inspection sites, yielding 5,013 inspection instances. For each instance, pixel-level segmentation annotations are provided for key objects in visible-spectrum images. In addition, a semantic scene description and a corresponding safety level label are provided according to practical inspection tasks. Seven synchronized sensing modalities are further included, including infrared video, audio, depth point clouds, radar point clouds, gas measurements, temperature, and humidity, to support multimodal anomaly recognition, cross-modal fusion, and comprehensive safety assessment in industrial environments."
    },
    {
      "title": "Listen, Look, Drive: Coupling Audio Instructions for User-aware VLA-based Autonomous Driving",
      "url": "https://arxiv.org/abs/2601.12142",
      "date": "2026-01-29",
      "authors_text": "Ziang Guo, Feng Yang, Xuefeng Zhang, Jiaqi Guo, Kun Zhao",
      "is_highlight": false,
      "score": 88.0,
      "summary": "EchoVLA enhances autonomous driving by integrating audio instructions with visual inputs, allowing for user-specific, emotionally adaptive driving behaviors, significantly improving performance metrics.",
      "teaser_image": "https://arxiv.org/html/2601.12142/Figs/overview.png",
      "debug_abstract": "Vision Language Action (VLA) models promise an open-vocabulary interface that can translate perceptual ambiguity into semantically grounded driving decisions, yet they still treat language as a static prior fixed at inference time. As a result, the model must infer continuously shifting objectives from pixels alone, yielding delayed or overly conservative maneuvers. We argue that effective VLAs for autonomous driving need an online channel in which users can influence driving with specific intentions. To this end, we present EchoVLA, a user-aware VLA that couples camera streams with in situ audio instructions. We augment the nuScenes dataset with temporally aligned, intent-specific speech commands generated by converting ego-motion descriptions into synthetic audios. Further, we compose emotional speech-trajectory pairs into a multimodal Chain-of-Thought (CoT) for fine-tuning a Multimodal Large Model (MLM) based on Qwen2.5-Omni. Specifically, we synthesize the audio-augmented dataset with different emotion types paired with corresponding driving behaviors, leveraging the emotional cues embedded in tone, pitch, and speech tempo to reflect varying user states, such as urgent or hesitant intentions, thus enabling our EchoVLA to interpret not only the semantic content but also the emotional context of audio commands for more nuanced and emotionally adaptive driving behavior. In open-loop benchmarks, our approach reduces the average L2 error by $59.4\\%$ and the collision rate by $74.4\\%$ compared to the baseline of vision-only perception. More experiments on nuScenes dataset validate that EchoVLA not only steers the trajectory through audio instructions, but also modulates driving behavior in response to the emotions detected in the user&#39;s speech."
    }
  ],
  "Language and Vision (LaVi) Lab": [
    {
      "title": "PLANING: A Loosely Coupled Triangle-Gaussian Framework for Streaming 3D Reconstruction",
      "url": "https://arxiv.org/abs/2601.22046",
      "date": "2026-01-29",
      "authors_text": "Changjian Jiang, Kerui Ren, Xudong Li, Kaiwen Song, Linning Xu",
      "is_highlight": false,
      "score": 66.0,
      "summary": "PLANING is a novel streaming 3D reconstruction framework that efficiently combines geometric primitives and neural Gaussians, enhancing quality and speed while reducing redundancy.",
      "teaser_image": "https://arxiv.org/html/2601.22046/x3.png",
      "debug_abstract": "Streaming reconstruction from monocular image sequences remains challenging, as existing methods typically favor either high-quality rendering or accurate geometry, but rarely both. We present PLANING, an efficient on-the-fly reconstruction framework built on a hybrid representation that loosely couples explicit geometric primitives with neural Gaussians, enabling geometry and appearance to be modeled in a decoupled manner. This decoupling supports an online initialization and optimization strategy that separates geometry and appearance updates, yielding stable streaming reconstruction with substantially reduced structural redundancy. PLANING improves dense mesh Chamfer-L2 by 18.52% over PGSR, surpasses ARTDECO by 1.31 dB PSNR, and reconstructs ScanNetV2 scenes in under 100 seconds, over 5x faster than 2D Gaussian Splatting, while matching the quality of offline per-scene optimization. Beyond reconstruction quality, the structural clarity and computational efficiency of \\modelname~make it well suited for a broad range of downstream applications, such as enabling large-scale scene modeling and simulation-ready environments for embodied AI. Project page: this https URL ."
    },
    {
      "title": "Dynamic Topology Awareness: Breaking the Granularity Rigidity in Vision-Language Navigation",
      "url": "https://arxiv.org/abs/2601.21751",
      "date": "2026-01-29",
      "authors_text": "Jiankun Peng, Jianyuan Guo, Ying Xu, Yue Liu, Jiashuang Yan",
      "is_highlight": false,
      "score": 84.0,
      "summary": "The paper presents DGNav, a dynamic framework for Vision-Language Navigation that adapts topological map density and connectivity to enhance navigation efficiency and safety in complex environments.",
      "teaser_image": "https://arxiv.org/html/2601.21751/x1.png",
      "debug_abstract": "Vision-Language Navigation in Continuous Environments (VLN-CE) presents a core challenge: grounding high-level linguistic instructions into precise, safe, and long-horizon spatial actions. Explicit topological maps have proven to be a vital solution for providing robust spatial memory in such tasks. However, existing topological planning methods suffer from a &#34;Granularity Rigidity&#34; problem. Specifically, these methods typically rely on fixed geometric thresholds to sample nodes, which fails to adapt to varying environmental complexities. This rigidity leads to a critical mismatch: the model tends to over-sample in simple areas, causing computational redundancy, while under-sampling in high-uncertainty regions, increasing collision risks and compromising precision. To address this, we propose DGNav, a framework for Dynamic Topological Navigation, introducing a context-aware mechanism to modulate map density and connectivity on-the-fly. Our approach comprises two core innovations: (1) A Scene-Aware Adaptive Strategy that dynamically modulates graph construction thresholds based on the dispersion of predicted waypoints, enabling &#34;densification on demand&#34; in challenging environments; (2) A Dynamic Graph Transformer that reconstructs graph connectivity by fusing visual, linguistic, and geometric cues into dynamic edge weights, enabling the agent to filter out topological noise and enhancing instruction adherence. Extensive experiments on the R2R-CE and RxR-CE benchmarks demonstrate DGNav exhibits superior navigation performance and strong generalization capabilities. Furthermore, ablation studies confirm that our framework achieves an optimal trade-off between navigation efficiency and safe exploration. The code is available at this https URL."
    },
    {
      "title": "Disentangling perception and reasoning for improving data efficiency in learning cloth manipulation without demonstrations",
      "url": "https://arxiv.org/abs/2601.21713",
      "date": "2026-01-29",
      "authors_text": "Donatien Delehelle, Fei Chen, Darwin Caldwell",
      "is_highlight": false,
      "score": 87.0,
      "summary": "This paper presents a modular reinforcement learning approach for cloth manipulation that enhances data efficiency and reduces model size and training time, achieving improved performance in simulation and real-world transfer.",
      "teaser_image": "https://arxiv.org/html/2601.21713/x1.png",
      "debug_abstract": "Cloth manipulation is a ubiquitous task in everyday life, but it remains an open challenge for robotics. The difficulties in developing cloth manipulation policies are attributed to the high-dimensional state space, complex dynamics, and high propensity to self-occlusion exhibited by fabrics. As analytical methods have not been able to provide robust and general manipulation policies, reinforcement learning (RL) is considered a promising approach to these problems. However, to address the large state space and complex dynamics, data-based methods usually rely on large models and long training times. The resulting computational cost significantly hampers the development and adoption of these methods. Additionally, due to the challenge of robust state estimation, garment manipulation policies often adopt an end-to-end learning approach with workspace images as input. While this approach enables a conceptually straightforward sim-to-real transfer via real-world fine-tuning, it also incurs a significant computational cost by training agents on a highly lossy representation of the environment state. This paper questions this common design choice by exploring an efficient and modular approach to RL for cloth manipulation. We show that, through careful design choices, model size and training time can be significantly reduced when learning in simulation. Furthermore, we demonstrate how the resulting simulation-trained model can be transferred to the real world. We evaluate our approach on the SoftGym benchmark and achieve significant performance improvements over available baselines on our task, while using a substantially smaller model."
    },
    {
      "title": "Heterogeneous Vertiport Selection Optimization for On-Demand Air Taxi Services: A Deep Reinforcement Learning Approach",
      "url": "https://arxiv.org/abs/2601.21316",
      "date": "2026-01-29",
      "authors_text": "Aoyu Pang, Maonan Wang, Zifan Sha, Wenwei Yue, Changle Li",
      "is_highlight": false,
      "score": 80.0,
      "summary": "The paper presents a deep reinforcement learning framework for optimizing vertiport selection and air taxi routing, achieving significant travel time reductions in urban air mobility systems.",
      "teaser_image": "https://arxiv.org/html/2601.21316/x2.png",
      "debug_abstract": "Urban Air Mobility (UAM) has emerged as a transformative solution to alleviate urban congestion by utilizing low-altitude airspace, thereby reducing pressure on ground transportation networks. To enable truly efficient and seamless door-to-door travel experiences, UAM requires close integration with existing ground transportation infrastructure. However, current research on optimal integrated routing strategies for passengers in air-ground mobility systems remains limited, with a lack of systematic this http URL address this gap, we first propose a unified optimization model that integrates strategy selection for both air and ground transportation. This model captures the dynamic characteristics of multimodal transport networks and incorporates real-time traffic conditions alongside passenger decision-making behavior. Building on this model, we propose a Unified Air-Ground Mobility Coordination (UAGMC) framework, which leverages deep reinforcement learning (RL) and Vehicle-to-Everything (V2X) communication to optimize vertiport selection and dynamically plan air taxi routes. Experimental results demonstrate that UAGMC achieves a 34\\% reduction in average travel time compared to conventional proportional allocation methods, enhancing overall travel efficiency and providing novel insights into the integration and optimization of multimodal transportation systems. This work lays a solid foundation for advancing intelligent urban mobility solutions through the coordination of air and ground transportation modes. The related code can be found at this https URL."
    },
    {
      "title": "Listen, Look, Drive: Coupling Audio Instructions for User-aware VLA-based Autonomous Driving",
      "url": "https://arxiv.org/abs/2601.12142",
      "date": "2026-01-29",
      "authors_text": "Ziang Guo, Feng Yang, Xuefeng Zhang, Jiaqi Guo, Kun Zhao",
      "is_highlight": false,
      "score": 88.0,
      "summary": "EchoVLA enhances autonomous driving by integrating audio instructions with visual inputs, allowing for user-specific, emotionally adaptive driving behaviors, significantly improving performance metrics.",
      "teaser_image": "https://arxiv.org/html/2601.12142/Figs/overview.png",
      "debug_abstract": "Vision Language Action (VLA) models promise an open-vocabulary interface that can translate perceptual ambiguity into semantically grounded driving decisions, yet they still treat language as a static prior fixed at inference time. As a result, the model must infer continuously shifting objectives from pixels alone, yielding delayed or overly conservative maneuvers. We argue that effective VLAs for autonomous driving need an online channel in which users can influence driving with specific intentions. To this end, we present EchoVLA, a user-aware VLA that couples camera streams with in situ audio instructions. We augment the nuScenes dataset with temporally aligned, intent-specific speech commands generated by converting ego-motion descriptions into synthetic audios. Further, we compose emotional speech-trajectory pairs into a multimodal Chain-of-Thought (CoT) for fine-tuning a Multimodal Large Model (MLM) based on Qwen2.5-Omni. Specifically, we synthesize the audio-augmented dataset with different emotion types paired with corresponding driving behaviors, leveraging the emotional cues embedded in tone, pitch, and speech tempo to reflect varying user states, such as urgent or hesitant intentions, thus enabling our EchoVLA to interpret not only the semantic content but also the emotional context of audio commands for more nuanced and emotionally adaptive driving behavior. In open-loop benchmarks, our approach reduces the average L2 error by $59.4\\%$ and the collision rate by $74.4\\%$ compared to the baseline of vision-only perception. More experiments on nuScenes dataset validate that EchoVLA not only steers the trajectory through audio instructions, but also modulates driving behavior in response to the emotions detected in the user&#39;s speech."
    }
  ],
  "具身视觉与机器人实验室 (EVAR Lab)": [
    {
      "title": "InspecSafe-V1: A Multimodal Benchmark for Safety Assessment in Industrial Inspection Scenarios",
      "url": "https://arxiv.org/abs/2601.21173",
      "date": "2026-01-29",
      "authors_text": "Zeyi Liu, Shuang Liu, Jihai Min, Zhaoheng Zhang, Jun Cen",
      "is_highlight": false,
      "score": 82.0,
      "summary": "InspecSafe-V1 is a multimodal benchmark dataset for safety assessment in industrial inspections, featuring real-world data from diverse scenarios and multiple sensing modalities.",
      "teaser_image": "https://arxiv.org/html/2601.21173/x1.png",
      "debug_abstract": "With the rapid development of industrial intelligence and unmanned inspection, reliable perception and safety assessment for AI systems in complex and dynamic industrial sites has become a key bottleneck for deploying predictive maintenance and autonomous inspection. Most public datasets remain limited by simulated data sources, single-modality sensing, or the absence of fine-grained object-level annotations, which prevents robust scene understanding and multimodal safety reasoning for industrial foundation models. To address these limitations, InspecSafe-V1 is released as the first multimodal benchmark dataset for industrial inspection safety assessment that is collected from routine operations of real inspection robots in real-world environments. InspecSafe-V1 covers five representative industrial scenarios, including tunnels, power facilities, sintering equipment, oil and gas petrochemical plants, and coal conveyor trestles. The dataset is constructed from 41 wheeled and rail-mounted inspection robots operating at 2,239 valid inspection sites, yielding 5,013 inspection instances. For each instance, pixel-level segmentation annotations are provided for key objects in visible-spectrum images. In addition, a semantic scene description and a corresponding safety level label are provided according to practical inspection tasks. Seven synchronized sensing modalities are further included, including infrared video, audio, depth point clouds, radar point clouds, gas measurements, temperature, and humidity, to support multimodal anomaly recognition, cross-modal fusion, and comprehensive safety assessment in industrial environments."
    },
    {
      "title": "Listen, Look, Drive: Coupling Audio Instructions for User-aware VLA-based Autonomous Driving",
      "url": "https://arxiv.org/abs/2601.12142",
      "date": "2026-01-29",
      "authors_text": "Ziang Guo, Feng Yang, Xuefeng Zhang, Jiaqi Guo, Kun Zhao",
      "is_highlight": false,
      "score": 88.0,
      "summary": "EchoVLA enhances autonomous driving by integrating audio instructions with visual inputs, allowing for user-specific, emotionally adaptive driving behaviors, significantly improving performance metrics.",
      "teaser_image": "https://arxiv.org/html/2601.12142/Figs/overview.png",
      "debug_abstract": "Vision Language Action (VLA) models promise an open-vocabulary interface that can translate perceptual ambiguity into semantically grounded driving decisions, yet they still treat language as a static prior fixed at inference time. As a result, the model must infer continuously shifting objectives from pixels alone, yielding delayed or overly conservative maneuvers. We argue that effective VLAs for autonomous driving need an online channel in which users can influence driving with specific intentions. To this end, we present EchoVLA, a user-aware VLA that couples camera streams with in situ audio instructions. We augment the nuScenes dataset with temporally aligned, intent-specific speech commands generated by converting ego-motion descriptions into synthetic audios. Further, we compose emotional speech-trajectory pairs into a multimodal Chain-of-Thought (CoT) for fine-tuning a Multimodal Large Model (MLM) based on Qwen2.5-Omni. Specifically, we synthesize the audio-augmented dataset with different emotion types paired with corresponding driving behaviors, leveraging the emotional cues embedded in tone, pitch, and speech tempo to reflect varying user states, such as urgent or hesitant intentions, thus enabling our EchoVLA to interpret not only the semantic content but also the emotional context of audio commands for more nuanced and emotionally adaptive driving behavior. In open-loop benchmarks, our approach reduces the average L2 error by $59.4\\%$ and the collision rate by $74.4\\%$ compared to the baseline of vision-only perception. More experiments on nuScenes dataset validate that EchoVLA not only steers the trajectory through audio instructions, but also modulates driving behavior in response to the emotions detected in the user&#39;s speech."
    }
  ],
  "人机物智能融合实验室 (HCP)": [
    {
      "title": "MARE: Multimodal Alignment and Reinforcement for Explainable Deepfake Detection via Vision-Language Models",
      "url": "https://arxiv.org/abs/2601.20433",
      "date": "2026-01-29",
      "authors_text": "Wenbo Xu, Wei Lu, Xiangyang Luo, Jiantao Zhou",
      "is_highlight": false,
      "score": 75.0,
      "summary": "MARE enhances Deepfake detection using vision-language models through multimodal alignment, reinforcement learning from human feedback, and a forgery disentanglement module for improved accuracy and explainability.",
      "teaser_image": "https://arxiv.org/html/2601.20433/x2.png",
      "debug_abstract": "Deepfake detection is a widely researched topic that is crucial for combating the spread of malicious content, with existing methods mainly modeling the problem as classification or spatial localization. The rapid advancements in generative models impose new demands on Deepfake detection. In this paper, we propose multimodal alignment and reinforcement for explainable Deepfake detection via vision-language models, termed MARE, which aims to enhance the accuracy and reliability of Vision-Language Models (VLMs) in Deepfake detection and reasoning. Specifically, MARE designs comprehensive reward functions, incorporating reinforcement learning from human feedback (RLHF), to incentivize the generation of text-spatially aligned reasoning content that adheres to human preferences. Besides, MARE introduces a forgery disentanglement module to capture intrinsic forgery traces from high-level facial semantics, thereby improving its authenticity detection capability. We conduct thorough evaluations on the reasoning content generated by MARE. Both quantitative and qualitative experimental results demonstrate that MARE achieves state-of-the-art performance in terms of accuracy and reliability."
    }
  ],
  "卡内基-机器人研究所": [
    {
      "title": "Llama-3.1-FoundationAI-SecurityLLM-Reasoning-8B Technical Report",
      "url": "https://arxiv.org/abs/2601.21051",
      "date": "2026-01-28",
      "authors_text": "Zhuoran Yang, Ed Li, Jianliang He, Aman Priyanshu, Baturay Saglam",
      "is_highlight": false,
      "score": 70.0,
      "summary": "Foundation-Sec-8B-Reasoning is an open-source cybersecurity reasoning model that excels in specialized tasks while retaining strong general capabilities through innovative training methods.",
      "teaser_image": "https://arxiv.org/html/2601.21051/x2.png",
      "debug_abstract": "We present Foundation-Sec-8B-Reasoning, the first open-source native reasoning model for cybersecurity. Built upon our previously released Foundation-Sec-8B base model (derived from Llama-3.1-8B-Base), the model is trained through a two-stage process combining supervised fine-tuning (SFT) and reinforcement learning from verifiable rewards (RLVR). Our training leverages proprietary reasoning data spanning cybersecurity analysis, instruction-following, and mathematical reasoning. Evaluation across 10 cybersecurity benchmarks and 10 general-purpose benchmarks demonstrates performance competitive with significantly larger models on cybersecurity tasks while maintaining strong general capabilities. The model shows effective generalization on multi-hop reasoning tasks and strong safety performance when deployed with appropriate system prompts and guardrails. This work demonstrates that domain-specialized reasoning models can achieve strong performance on specialized tasks while maintaining broad general capabilities. We release the model publicly at this https URL."
    }
  ],
  "通用机器人、自动化、传感和感知实验室 (GRASP)": [
    {
      "title": "Llama-3.1-FoundationAI-SecurityLLM-Reasoning-8B Technical Report",
      "url": "https://arxiv.org/abs/2601.21051",
      "date": "2026-01-28",
      "authors_text": "Zhuoran Yang, Ed Li, Jianliang He, Aman Priyanshu, Baturay Saglam",
      "is_highlight": false,
      "score": 70.0,
      "summary": "Foundation-Sec-8B-Reasoning is an open-source cybersecurity reasoning model that excels in specialized tasks while retaining strong general capabilities through innovative training methods.",
      "teaser_image": "https://arxiv.org/html/2601.21051/x2.png",
      "debug_abstract": "We present Foundation-Sec-8B-Reasoning, the first open-source native reasoning model for cybersecurity. Built upon our previously released Foundation-Sec-8B base model (derived from Llama-3.1-8B-Base), the model is trained through a two-stage process combining supervised fine-tuning (SFT) and reinforcement learning from verifiable rewards (RLVR). Our training leverages proprietary reasoning data spanning cybersecurity analysis, instruction-following, and mathematical reasoning. Evaluation across 10 cybersecurity benchmarks and 10 general-purpose benchmarks demonstrates performance competitive with significantly larger models on cybersecurity tasks while maintaining strong general capabilities. The model shows effective generalization on multi-hop reasoning tasks and strong safety performance when deployed with appropriate system prompts and guardrails. This work demonstrates that domain-specialized reasoning models can achieve strong performance on specialized tasks while maintaining broad general capabilities. We release the model publicly at this https URL."
    }
  ],
  "SU Lab": [
    {
      "title": "Llama-3.1-FoundationAI-SecurityLLM-Reasoning-8B Technical Report",
      "url": "https://arxiv.org/abs/2601.21051",
      "date": "2026-01-28",
      "authors_text": "Zhuoran Yang, Ed Li, Jianliang He, Aman Priyanshu, Baturay Saglam",
      "is_highlight": false,
      "score": 70.0,
      "summary": "Foundation-Sec-8B-Reasoning is an open-source cybersecurity reasoning model that excels in specialized tasks while retaining strong general capabilities through innovative training methods.",
      "teaser_image": "https://arxiv.org/html/2601.21051/x2.png",
      "debug_abstract": "We present Foundation-Sec-8B-Reasoning, the first open-source native reasoning model for cybersecurity. Built upon our previously released Foundation-Sec-8B base model (derived from Llama-3.1-8B-Base), the model is trained through a two-stage process combining supervised fine-tuning (SFT) and reinforcement learning from verifiable rewards (RLVR). Our training leverages proprietary reasoning data spanning cybersecurity analysis, instruction-following, and mathematical reasoning. Evaluation across 10 cybersecurity benchmarks and 10 general-purpose benchmarks demonstrates performance competitive with significantly larger models on cybersecurity tasks while maintaining strong general capabilities. The model shows effective generalization on multi-hop reasoning tasks and strong safety performance when deployed with appropriate system prompts and guardrails. This work demonstrates that domain-specialized reasoning models can achieve strong performance on specialized tasks while maintaining broad general capabilities. We release the model publicly at this https URL."
    }
  ],
  "Affective Intelligence and Robotics Laboratory (AFAR)": [
    {
      "title": "Towards Mitigating Modality Bias in Vision-Language Models for Temporal Action Localization",
      "url": "https://arxiv.org/abs/2601.21078",
      "date": "2026-01-28",
      "authors_text": "Jiaqi Li, Guangming Wang, Shuntian Zheng, Minzhe Ni, Xiaoman Lu",
      "is_highlight": false,
      "score": 78.0,
      "summary": "The paper presents ActionVLM, a framework that reduces modality bias in Temporal Action Localization by prioritizing visual signals while adaptively incorporating language for improved performance.",
      "teaser_image": "https://arxiv.org/html/2601.21078/img/overview.png",
      "debug_abstract": "Temporal Action Localization (TAL) requires identifying both the boundaries and categories of actions in untrimmed videos. While vision-language models (VLMs) offer rich semantics to complement visual evidence, existing approaches tend to overemphasize linguistic priors at the expense of visual performance, leading to a pronounced modality bias. We propose ActionVLM, a vision-language aggregation framework that systematically mitigates modality bias in TAL. Our key insight is to preserve vision as the dominant signal while adaptively exploiting language only when beneficial. To this end, we introduce (i) a debiasing reweighting module that estimates the language advantage-the incremental benefit of language over vision-only predictions-and dynamically reweights language modality accordingly, and (ii) a residual aggregation strategy that treats language as a complementary refinement rather than the primary driver. This combination alleviates modality bias, reduces overconfidence from linguistic priors, and strengthens temporal reasoning. Experiments on THUMOS14 show that our model outperforms state-of-the-art by up to 3.2% mAP."
    }
  ],
  "Bio-Inspired Robotics Laboratory (BIRL)": [
    {
      "title": "Towards Mitigating Modality Bias in Vision-Language Models for Temporal Action Localization",
      "url": "https://arxiv.org/abs/2601.21078",
      "date": "2026-01-28",
      "authors_text": "Jiaqi Li, Guangming Wang, Shuntian Zheng, Minzhe Ni, Xiaoman Lu",
      "is_highlight": false,
      "score": 78.0,
      "summary": "The paper presents ActionVLM, a framework that reduces modality bias in Temporal Action Localization by prioritizing visual signals while adaptively incorporating language for improved performance.",
      "teaser_image": "https://arxiv.org/html/2601.21078/img/overview.png",
      "debug_abstract": "Temporal Action Localization (TAL) requires identifying both the boundaries and categories of actions in untrimmed videos. While vision-language models (VLMs) offer rich semantics to complement visual evidence, existing approaches tend to overemphasize linguistic priors at the expense of visual performance, leading to a pronounced modality bias. We propose ActionVLM, a vision-language aggregation framework that systematically mitigates modality bias in TAL. Our key insight is to preserve vision as the dominant signal while adaptively exploiting language only when beneficial. To this end, we introduce (i) a debiasing reweighting module that estimates the language advantage-the incremental benefit of language over vision-only predictions-and dynamically reweights language modality accordingly, and (ii) a residual aggregation strategy that treats language as a complementary refinement rather than the primary driver. This combination alleviates modality bias, reduces overconfidence from linguistic priors, and strengthens temporal reasoning. Experiments on THUMOS14 show that our model outperforms state-of-the-art by up to 3.2% mAP."
    }
  ],
  "北京航空航天大学": [
    {
      "title": "Latent Chain-of-Thought as Planning: Decoupling Reasoning from Verbalization",
      "url": "https://arxiv.org/abs/2601.21358",
      "date": "2026-01-29",
      "authors_text": "Jiecong Wang, Hao Peng, Chunyang Liu",
      "is_highlight": false,
      "score": 74.0,
      "summary": "PLaT decouples reasoning from verbalization in LLMs by modeling latent reasoning as planning, enhancing scalability and diversity despite lower initial accuracy.",
      "teaser_image": "https://arxiv.org/html/2601.21358/x2.png",
      "debug_abstract": "Chain-of-Thought (CoT) empowers Large Language Models (LLMs) to tackle complex problems, but remains constrained by the computational cost and reasoning path collapse when grounded in discrete token spaces. Recent latent reasoning approaches attempt to optimize efficiency by performing reasoning within continuous hidden states. However, these methods typically operate as opaque end-to-end mappings from explicit reasoning steps to latent states, and often require a pre-defined number of latent steps during inference. In this work, we introduce PLaT (Planning with Latent Thoughts), a framework that reformulates latent reasoning as planning by fundamentally decouple reasoning from verbalization. We model reasoning as a deterministic trajectory of latent planning states, while a separate Decoder grounds these thoughts into text when necessary. This decoupling allows the model to dynamically determine when to terminate reasoning rather than relying on fixed hyperparameters. Empirical results on mathematical benchmarks reveal a distinct trade-off: while PLaT achieves lower greedy accuracy than baselines, it demonstrates superior scalability in terms of reasoning diversity. This indicates that PLaT learns a robust, broader solution space, offering a transparent and scalable foundation for inference-time search."
    },
    {
      "title": "Concise Geometric Description as a Bridge: Unleashing the Potential of LLM for Plane Geometry Problem Solving",
      "url": "https://arxiv.org/abs/2601.21164",
      "date": "2026-01-29",
      "authors_text": "Jingyun Wang, Dian Li, Xiaohan Wang, Gang Liu, Jiahong Yan",
      "is_highlight": false,
      "score": 65.0,
      "summary": "This paper proposes a method using a MLLM Interpreter to convert visual geometric information into concise textual descriptions, enhancing LLMs' reasoning capabilities for plane geometry problem solving.",
      "teaser_image": "https://arxiv.org/html/2601.21164/x2.png",
      "debug_abstract": "Plane Geometry Problem Solving (PGPS) is a multimodal reasoning task that aims to solve a plane geometric problem based on a geometric diagram and problem textual descriptions. Although Large Language Models (LLMs) possess strong reasoning skills, their direct application to PGPS is hindered by their inability to process visual diagrams. Existing works typically fine-tune Multimodal LLMs (MLLMs) end-to-end on large-scale PGPS data to enhance visual understanding and reasoning simultaneously. However, such joint optimization may compromise base LLMs&#39; inherent reasoning capability. In this work, we observe that LLM itself is potentially a powerful PGPS solver when appropriately formulating visual information as textual descriptions. We propose to train a MLLM Interpreter to generate geometric descriptions for the visual diagram, and an off-the-shelf LLM is utilized to perform reasoning. Specifically, we choose Conditional Declaration Language (CDL) as the geometric description as its conciseness eases the MLLM Interpreter training. The MLLM Interpreter is fine-tuned via CoT (Chain-of-Thought)-augmented SFT followed by GRPO to generate CDL. Instead of using a conventional solution-based reward that compares the reasoning result with the ground-truth answer, we design CDL matching rewards to facilitate more effective GRPO training, which provides more direct and denser guidance for CDL generation. To support training, we construct a new dataset, Formalgeo7k-Rec-CoT, by manually reviewing Formalgeo7k v2 and incorporating CoT annotations. Extensive experiments on Formalgeo7k-Rec-CoT, Unigeo, and MathVista show our method (finetuned on only 5.5k data) performs favorably against leading open-source and closed-source MLLMs."
    }
  ],
  "中国人民大学": [
    {
      "title": "ProRAG: Process-Supervised Reinforcement Learning for Retrieval-Augmented Generation",
      "url": "https://arxiv.org/abs/2601.21912",
      "date": "2026-01-29",
      "authors_text": "Zhao Wang, Ziliang Zhao, Zhicheng Dou",
      "is_highlight": false,
      "score": 69.0,
      "summary": "ProRAG introduces a process-supervised reinforcement learning framework that enhances Retrieval-Augmented Generation by integrating step-level supervision to improve reasoning accuracy and performance.",
      "teaser_image": "https://arxiv.org/html/2601.21912/x3.png",
      "debug_abstract": "Reinforcement learning (RL) has become a promising paradigm for optimizing Retrieval-Augmented Generation (RAG) in complex reasoning tasks. However, traditional outcome-based RL approaches often suffer from reward sparsity and inefficient credit assignment, as coarse-grained scalar rewards fail to identify specific erroneous steps within long-horizon trajectories. This ambiguity frequently leads to &#34;process hallucinations&#34;, where models reach correct answers through flawed logic or redundant retrieval steps. Although recent process-aware approaches attempt to mitigate this via static preference learning or heuristic reward shaping, they often lack the on-policy exploration capabilities required to decouple step-level credit from global outcomes. To address these challenges, we propose ProRAG, a process-supervised reinforcement learning framework designed to integrate learned step-level supervision into the online optimization loop. Our framework consists of four stages: (1) Supervised Policy Warmup to initialize the model with a structured reasoning format; (2) construction of an MCTS-based Process Reward Model (PRM) to quantify intermediate reasoning quality; (3) PRM-Guided Reasoning Refinement to align the policy with fine-grained process preferences; and (4) Process-Supervised Reinforcement Learning with a dual-granularity advantage mechanism. By aggregating step-level process rewards with global outcome signals, ProRAG provides precise feedback for every action. Extensive experiments on five multi-hop reasoning benchmarks demonstrate that ProRAG achieves superior overall performance compared to strong outcome-based and process-aware RL baselines, particularly on complex long-horizon tasks, validating the effectiveness of fine-grained process supervision. The code and model are available at this https URL."
    },
    {
      "title": "Less Noise, More Voice: Reinforcement Learning for Reasoning via Instruction Purification",
      "url": "https://arxiv.org/abs/2601.21244",
      "date": "2026-01-29",
      "authors_text": "Yiju Guo, Tianyi Hu, Zexu Sun, Yankai Lin",
      "is_highlight": false,
      "score": 72.0,
      "summary": "The Less Noise Sampling Framework (LENS) enhances reinforcement learning by purging interference tokens, improving rollout efficiency and performance in reasoning tasks.",
      "teaser_image": "https://arxiv.org/html/2601.21244/x3.png",
      "debug_abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) has advanced LLM reasoning, but remains constrained by inefficient exploration under limited rollout budgets, leading to low sampling success and unstable training in complex tasks. We find that many exploration failures arise not from problem difficulty, but from a small number of prompt tokens that introduce interference. Building on this insight, we propose the Less Noise Sampling Framework (LENS), which first prompts by identifying and removing interference tokens. then transfers successful rollouts from the purification process to supervise policy optimization on the original noisy prompts, enabling the model to learn to ignore interference in the real-world, noisy prompting settings. Experimental results show that LENS significantly outperforms GRPO, delivering higher performance and faster convergence, with a 3.88% average gain and over 1.6$\\times$ speedup. Our work highlights the critical role of pruning interference tokens in improving rollout efficiency, offering a new perspective for RLVR research."
    },
    {
      "title": "Intelli-Planner: Towards Customized Urban Planning via Large Language Model Empowered Reinforcement Learning",
      "url": "https://arxiv.org/abs/2601.21212",
      "date": "2026-01-29",
      "authors_text": "Xixian Yong, Peilin Sun, Zihe Wang, Xiao Zhou",
      "is_highlight": true,
      "score": 76.0,
      "summary": "Intelli-Planner combines Deep Reinforcement Learning and large language models to enhance urban planning by improving stakeholder involvement, satisfaction, and decision-making efficiency.",
      "teaser_image": "https://arxiv.org/html/2601.21212/x1.png",
      "debug_abstract": "Effective urban planning is crucial for enhancing residents&#39; quality of life and ensuring societal stability, playing a pivotal role in the sustainable development of cities. Current planning methods heavily rely on human experts, which are time-consuming and labor-intensive, or utilize deep learning algorithms, often limiting stakeholder involvement. To bridge these gaps, we propose Intelli-Planner, a novel framework integrating Deep Reinforcement Learning (DRL) with large language models (LLMs) to facilitate participatory and customized planning scheme generation. Intelli-Planner utilizes demographic, geographic data, and planning preferences to determine high-level planning requirements and demands for each functional type. During training, a knowledge enhancement module is employed to enhance the decision-making capability of the policy network. Additionally, we establish a multi-dimensional evaluation system and employ LLM-based stakeholders for satisfaction scoring. Experimental validation across diverse urban settings shows that Intelli-Planner surpasses traditional baselines and achieves comparable performance to state-of-the-art DRL-based methods in objective metrics, while enhancing stakeholder satisfaction and convergence speed. These findings underscore the effectiveness and superiority of our framework, highlighting the potential for integrating the latest advancements in LLMs with DRL approaches to revolutionize tasks related to functional areas planning."
    }
  ],
  "Synteraction Lab": [
    {
      "title": "TraceRouter: Robust Safety for Large Foundation Models via Path-Level Intervention",
      "url": "https://arxiv.org/abs/2601.21900",
      "date": "2026-01-29",
      "authors_text": "Chuancheng Shi, Shangze Li, Wenjun Lu, Wenhua Wu, Cong Wang",
      "is_highlight": false,
      "score": 73.0,
      "summary": "TraceRouter enhances the safety of large foundation models by disconnecting harmful semantic pathways through a novel path-level intervention framework, outperforming existing defenses.",
      "teaser_image": "https://arxiv.org/html/2601.21900/x8.png",
      "debug_abstract": "Despite their capabilities, large foundation models (LFMs) remain susceptible to adversarial manipulation. Current defenses predominantly rely on the &#34;locality hypothesis&#34;, suppressing isolated neurons or features. However, harmful semantics act as distributed, cross-layer circuits, rendering such localized interventions brittle and detrimental to utility. To bridge this gap, we propose \\textbf{TraceRouter}, a path-level framework that traces and disconnects the causal propagation circuits of illicit semantics. TraceRouter operates in three stages: (1) it pinpoints a sensitive onset layer by analyzing attention divergence; (2) it leverages sparse autoencoders (SAEs) and differential activation analysis to disentangle and isolate malicious features; and (3) it maps these features to downstream causal pathways via feature influence scores (FIS) derived from zero-out interventions. By selectively suppressing these causal chains, TraceRouter physically severs the flow of harmful information while leaving orthogonal computation routes intact. Extensive experiments demonstrate that TraceRouter significantly outperforms state-of-the-art baselines, achieving a superior trade-off between adversarial robustness and general utility. Our code will be publicly released. WARNING: This paper contains unsafe model responses."
    },
    {
      "title": "Abstracting Robot Manipulation Skills via Mixture-of-Experts Diffusion Policies",
      "url": "https://arxiv.org/abs/2601.21251",
      "date": "2026-01-29",
      "authors_text": "Ce Hao, Xuanran Zhai, Yaohua Liu, Harold Soh",
      "is_highlight": false,
      "score": 90.0,
      "summary": "The Skill Mixture-of-Experts Policy (SMP) enhances robot manipulation by efficiently utilizing a compact skill basis and adaptive expert activation for improved multi-task performance.",
      "teaser_image": "https://arxiv.org/html/2601.21251/x1.png",
      "debug_abstract": "Diffusion-based policies have recently shown strong results in robot manipulation, but their extension to multi-task scenarios is hindered by the high cost of scaling model size and demonstrations. We introduce Skill Mixture-of-Experts Policy (SMP), a diffusion-based mixture-of-experts policy that learns a compact orthogonal skill basis and uses sticky routing to compose actions from a small, task-relevant subset of experts at each step. A variational training objective supports this design, and adaptive expert activation at inference yields fast sampling without oversized backbones. We validate SMP in simulation and on a real dual-arm platform with multi-task learning and transfer learning tasks, where SMP achieves higher success rates and markedly lower inference cost than large diffusion baselines. These results indicate a practical path toward scalable, transferable multi-task manipulation: learn reusable skills once, activate only what is needed, and adapt quickly when tasks change."
    }
  ],
  "NUS AI LAB": [
    {
      "title": "TraceRouter: Robust Safety for Large Foundation Models via Path-Level Intervention",
      "url": "https://arxiv.org/abs/2601.21900",
      "date": "2026-01-29",
      "authors_text": "Chuancheng Shi, Shangze Li, Wenjun Lu, Wenhua Wu, Cong Wang",
      "is_highlight": false,
      "score": 73.0,
      "summary": "TraceRouter enhances the safety of large foundation models by disconnecting harmful semantic pathways through a novel path-level intervention framework, outperforming existing defenses.",
      "teaser_image": "https://arxiv.org/html/2601.21900/x8.png",
      "debug_abstract": "Despite their capabilities, large foundation models (LFMs) remain susceptible to adversarial manipulation. Current defenses predominantly rely on the &#34;locality hypothesis&#34;, suppressing isolated neurons or features. However, harmful semantics act as distributed, cross-layer circuits, rendering such localized interventions brittle and detrimental to utility. To bridge this gap, we propose \\textbf{TraceRouter}, a path-level framework that traces and disconnects the causal propagation circuits of illicit semantics. TraceRouter operates in three stages: (1) it pinpoints a sensitive onset layer by analyzing attention divergence; (2) it leverages sparse autoencoders (SAEs) and differential activation analysis to disentangle and isolate malicious features; and (3) it maps these features to downstream causal pathways via feature influence scores (FIS) derived from zero-out interventions. By selectively suppressing these causal chains, TraceRouter physically severs the flow of harmful information while leaving orthogonal computation routes intact. Extensive experiments demonstrate that TraceRouter significantly outperforms state-of-the-art baselines, achieving a superior trade-off between adversarial robustness and general utility. Our code will be publicly released. WARNING: This paper contains unsafe model responses."
    },
    {
      "title": "Abstracting Robot Manipulation Skills via Mixture-of-Experts Diffusion Policies",
      "url": "https://arxiv.org/abs/2601.21251",
      "date": "2026-01-29",
      "authors_text": "Ce Hao, Xuanran Zhai, Yaohua Liu, Harold Soh",
      "is_highlight": false,
      "score": 90.0,
      "summary": "The Skill Mixture-of-Experts Policy (SMP) enhances robot manipulation by efficiently utilizing a compact skill basis and adaptive expert activation for improved multi-task performance.",
      "teaser_image": "https://arxiv.org/html/2601.21251/x1.png",
      "debug_abstract": "Diffusion-based policies have recently shown strong results in robot manipulation, but their extension to multi-task scenarios is hindered by the high cost of scaling model size and demonstrations. We introduce Skill Mixture-of-Experts Policy (SMP), a diffusion-based mixture-of-experts policy that learns a compact orthogonal skill basis and uses sticky routing to compose actions from a small, task-relevant subset of experts at each step. A variational training objective supports this design, and adaptive expert activation at inference yields fast sampling without oversized backbones. We validate SMP in simulation and on a real dual-arm platform with multi-task learning and transfer learning tasks, where SMP achieves higher success rates and markedly lower inference cost than large diffusion baselines. These results indicate a practical path toward scalable, transferable multi-task manipulation: learn reusable skills once, activate only what is needed, and adapt quickly when tasks change."
    }
  ],
  "Microsystem Engineering and Robotics": [
    {
      "title": "TraceRouter: Robust Safety for Large Foundation Models via Path-Level Intervention",
      "url": "https://arxiv.org/abs/2601.21900",
      "date": "2026-01-29",
      "authors_text": "Chuancheng Shi, Shangze Li, Wenjun Lu, Wenhua Wu, Cong Wang",
      "is_highlight": false,
      "score": 73.0,
      "summary": "TraceRouter enhances the safety of large foundation models by disconnecting harmful semantic pathways through a novel path-level intervention framework, outperforming existing defenses.",
      "teaser_image": "https://arxiv.org/html/2601.21900/x8.png",
      "debug_abstract": "Despite their capabilities, large foundation models (LFMs) remain susceptible to adversarial manipulation. Current defenses predominantly rely on the &#34;locality hypothesis&#34;, suppressing isolated neurons or features. However, harmful semantics act as distributed, cross-layer circuits, rendering such localized interventions brittle and detrimental to utility. To bridge this gap, we propose \\textbf{TraceRouter}, a path-level framework that traces and disconnects the causal propagation circuits of illicit semantics. TraceRouter operates in three stages: (1) it pinpoints a sensitive onset layer by analyzing attention divergence; (2) it leverages sparse autoencoders (SAEs) and differential activation analysis to disentangle and isolate malicious features; and (3) it maps these features to downstream causal pathways via feature influence scores (FIS) derived from zero-out interventions. By selectively suppressing these causal chains, TraceRouter physically severs the flow of harmful information while leaving orthogonal computation routes intact. Extensive experiments demonstrate that TraceRouter significantly outperforms state-of-the-art baselines, achieving a superior trade-off between adversarial robustness and general utility. Our code will be publicly released. WARNING: This paper contains unsafe model responses."
    },
    {
      "title": "Abstracting Robot Manipulation Skills via Mixture-of-Experts Diffusion Policies",
      "url": "https://arxiv.org/abs/2601.21251",
      "date": "2026-01-29",
      "authors_text": "Ce Hao, Xuanran Zhai, Yaohua Liu, Harold Soh",
      "is_highlight": false,
      "score": 90.0,
      "summary": "The Skill Mixture-of-Experts Policy (SMP) enhances robot manipulation by efficiently utilizing a compact skill basis and adaptive expert activation for improved multi-task performance.",
      "teaser_image": "https://arxiv.org/html/2601.21251/x1.png",
      "debug_abstract": "Diffusion-based policies have recently shown strong results in robot manipulation, but their extension to multi-task scenarios is hindered by the high cost of scaling model size and demonstrations. We introduce Skill Mixture-of-Experts Policy (SMP), a diffusion-based mixture-of-experts policy that learns a compact orthogonal skill basis and uses sticky routing to compose actions from a small, task-relevant subset of experts at each step. A variational training objective supports this design, and adaptive expert activation at inference yields fast sampling without oversized backbones. We validate SMP in simulation and on a real dual-arm platform with multi-task learning and transfer learning tasks, where SMP achieves higher success rates and markedly lower inference cost than large diffusion baselines. These results indicate a practical path toward scalable, transferable multi-task manipulation: learn reusable skills once, activate only what is needed, and adapt quickly when tasks change."
    }
  ],
  "Advanced Robotics Centre": [
    {
      "title": "TraceRouter: Robust Safety for Large Foundation Models via Path-Level Intervention",
      "url": "https://arxiv.org/abs/2601.21900",
      "date": "2026-01-29",
      "authors_text": "Chuancheng Shi, Shangze Li, Wenjun Lu, Wenhua Wu, Cong Wang",
      "is_highlight": false,
      "score": 73.0,
      "summary": "TraceRouter enhances the safety of large foundation models by disconnecting harmful semantic pathways through a novel path-level intervention framework, outperforming existing defenses.",
      "teaser_image": "https://arxiv.org/html/2601.21900/x8.png",
      "debug_abstract": "Despite their capabilities, large foundation models (LFMs) remain susceptible to adversarial manipulation. Current defenses predominantly rely on the &#34;locality hypothesis&#34;, suppressing isolated neurons or features. However, harmful semantics act as distributed, cross-layer circuits, rendering such localized interventions brittle and detrimental to utility. To bridge this gap, we propose \\textbf{TraceRouter}, a path-level framework that traces and disconnects the causal propagation circuits of illicit semantics. TraceRouter operates in three stages: (1) it pinpoints a sensitive onset layer by analyzing attention divergence; (2) it leverages sparse autoencoders (SAEs) and differential activation analysis to disentangle and isolate malicious features; and (3) it maps these features to downstream causal pathways via feature influence scores (FIS) derived from zero-out interventions. By selectively suppressing these causal chains, TraceRouter physically severs the flow of harmful information while leaving orthogonal computation routes intact. Extensive experiments demonstrate that TraceRouter significantly outperforms state-of-the-art baselines, achieving a superior trade-off between adversarial robustness and general utility. Our code will be publicly released. WARNING: This paper contains unsafe model responses."
    },
    {
      "title": "Abstracting Robot Manipulation Skills via Mixture-of-Experts Diffusion Policies",
      "url": "https://arxiv.org/abs/2601.21251",
      "date": "2026-01-29",
      "authors_text": "Ce Hao, Xuanran Zhai, Yaohua Liu, Harold Soh",
      "is_highlight": false,
      "score": 90.0,
      "summary": "The Skill Mixture-of-Experts Policy (SMP) enhances robot manipulation by efficiently utilizing a compact skill basis and adaptive expert activation for improved multi-task performance.",
      "teaser_image": "https://arxiv.org/html/2601.21251/x1.png",
      "debug_abstract": "Diffusion-based policies have recently shown strong results in robot manipulation, but their extension to multi-task scenarios is hindered by the high cost of scaling model size and demonstrations. We introduce Skill Mixture-of-Experts Policy (SMP), a diffusion-based mixture-of-experts policy that learns a compact orthogonal skill basis and uses sticky routing to compose actions from a small, task-relevant subset of experts at each step. A variational training objective supports this design, and adaptive expert activation at inference yields fast sampling without oversized backbones. We validate SMP in simulation and on a real dual-arm platform with multi-task learning and transfer learning tasks, where SMP achieves higher success rates and markedly lower inference cost than large diffusion baselines. These results indicate a practical path toward scalable, transferable multi-task manipulation: learn reusable skills once, activate only what is needed, and adapt quickly when tasks change."
    }
  ],
  "范明明老师实验室": [
    {
      "title": "DynaWeb: Model-Based Reinforcement Learning of Web Agents",
      "url": "https://arxiv.org/abs/2601.22149",
      "date": "2026-01-29",
      "authors_text": "Hang Ding, Peidong Liu, Junqiao Wang, Ziwei Ji, Meng Cao",
      "is_highlight": false,
      "score": 77.0,
      "summary": "DynaWeb introduces a model-based reinforcement learning framework that enhances web agent training by simulating interactions with a web world model, improving efficiency and performance.",
      "teaser_image": "https://arxiv.org/html/2601.22149/figures/dynaweb.png",
      "debug_abstract": "The development of autonomous web agents, powered by Large Language Models (LLMs) and reinforcement learning (RL), represents a significant step towards general-purpose AI assistants. However, training these agents is severely hampered by the challenges of interacting with the live internet, which is inefficient, costly, and fraught with risks. Model-based reinforcement learning (MBRL) offers a promising solution by learning a world model of the environment to enable simulated interaction. This paper introduces DynaWeb, a novel MBRL framework that trains web agents through interacting with a web world model trained to predict naturalistic web page representations given agent actions. This model serves as a synthetic web environment where an agent policy can dream by generating vast quantities of rollout action trajectories for efficient online reinforcement learning. Beyond free policy rollouts, DynaWeb incorporates real expert trajectories from training data, which are randomly interleaved with on-policy rollouts during training to improve stability and sample efficiency. Experiments conducted on the challenging WebArena and WebVoyager benchmarks demonstrate that DynaWeb consistently and significantly improves the performance of state-of-the-art open-source web agent models. Our findings establish the viability of training web agents through imagination, offering a scalable and efficient way to scale up online agentic RL."
    },
    {
      "title": "4D-CAAL: 4D Radar-Camera Calibration and Auto-Labeling for Autonomous Driving",
      "url": "https://arxiv.org/abs/2601.21454",
      "date": "2026-01-29",
      "authors_text": "Shanliang Yao, Zhuoxiao Li, Runwei Guan, Kebin Cao, Meng Xia",
      "is_highlight": false,
      "score": 89.0,
      "summary": "The 4D-CAAL framework integrates 4D radar-camera calibration and auto-labeling using a dual-purpose target and robust correspondence matching, enhancing autonomous driving perception efficiency.",
      "teaser_image": "https://arxiv.org/html/2601.21454/images/4DRC-fixed.png",
      "debug_abstract": "4D radar has emerged as a critical sensor for autonomous driving, primarily due to its enhanced capabilities in elevation measurement and higher resolution compared to traditional 3D radar. Effective integration of 4D radar with cameras requires accurate extrinsic calibration, and the development of radar-based perception algorithms demands large-scale annotated datasets. However, existing calibration methods often employ separate targets optimized for either visual or radar modalities, complicating correspondence establishment. Furthermore, manually labeling sparse radar data is labor-intensive and unreliable. To address these challenges, we propose 4D-CAAL, a unified framework for 4D radar-camera calibration and auto-labeling. Our approach introduces a novel dual-purpose calibration target design, integrating a checkerboard pattern on the front surface for camera detection and a corner reflector at the center of the back surface for radar detection. We develop a robust correspondence matching algorithm that aligns the checkerboard center with the strongest radar reflection point, enabling accurate extrinsic calibration. Subsequently, we present an auto-labeling pipeline that leverages the calibrated sensor relationship to transfer annotations from camera-based segmentations to radar point clouds through geometric projection and multi-feature optimization. Extensive experiments demonstrate that our method achieves high calibration accuracy while significantly reducing manual annotation effort, thereby accelerating the development of robust multi-modal perception systems for autonomous driving."
    },
    {
      "title": "Gaussian Belief Propagation Network for Depth Completion",
      "url": "https://arxiv.org/abs/2601.21291",
      "date": "2026-01-29",
      "authors_text": "Jie Tang, Pingping Xie, Jian Li, Ping Tan",
      "is_highlight": false,
      "score": 68.0,
      "summary": "The Gaussian Belief Propagation Network (GBPN) integrates deep learning with probabilistic models for effective depth completion from sparse measurements, achieving state-of-the-art results across various benchmarks.",
      "teaser_image": "https://arxiv.org/html/2601.21291/x1.png",
      "debug_abstract": "Depth completion aims to predict a dense depth map from a color image with sparse depth measurements. Although deep learning methods have achieved state-of-the-art (SOTA), effectively handling the sparse and irregular nature of input depth data in deep networks remains a significant challenge, often limiting performance, especially under high sparsity. To overcome this limitation, we introduce the Gaussian Belief Propagation Network (GBPN), a novel hybrid framework synergistically integrating deep learning with probabilistic graphical models for end-to-end depth completion. Specifically, a scene-specific Markov Random Field (MRF) is dynamically constructed by the Graphical Model Construction Network (GMCN), and then inferred via Gaussian Belief Propagation (GBP) to yield the dense depth distribution. Crucially, the GMCN learns to construct not only the data-dependent potentials of MRF but also its structure by predicting adaptive non-local edges, enabling the capture of complex, long-range spatial dependencies. Furthermore, we enhance GBP with a serial \\&amp; parallel message passing scheme, designed for effective information propagation, particularly from sparse measurements. Extensive experiments demonstrate that GBPN achieves SOTA performance on the NYUv2 and KITTI benchmarks. Evaluations across varying sparsity levels, sparsity patterns, and datasets highlight GBPN&#39;s superior performance, notable robustness, and generalizable capability."
    }
  ],
  "Jun MA老师实验室": [
    {
      "title": "DynaWeb: Model-Based Reinforcement Learning of Web Agents",
      "url": "https://arxiv.org/abs/2601.22149",
      "date": "2026-01-29",
      "authors_text": "Hang Ding, Peidong Liu, Junqiao Wang, Ziwei Ji, Meng Cao",
      "is_highlight": false,
      "score": 77.0,
      "summary": "DynaWeb introduces a model-based reinforcement learning framework that enhances web agent training by simulating interactions with a web world model, improving efficiency and performance.",
      "teaser_image": "https://arxiv.org/html/2601.22149/figures/dynaweb.png",
      "debug_abstract": "The development of autonomous web agents, powered by Large Language Models (LLMs) and reinforcement learning (RL), represents a significant step towards general-purpose AI assistants. However, training these agents is severely hampered by the challenges of interacting with the live internet, which is inefficient, costly, and fraught with risks. Model-based reinforcement learning (MBRL) offers a promising solution by learning a world model of the environment to enable simulated interaction. This paper introduces DynaWeb, a novel MBRL framework that trains web agents through interacting with a web world model trained to predict naturalistic web page representations given agent actions. This model serves as a synthetic web environment where an agent policy can dream by generating vast quantities of rollout action trajectories for efficient online reinforcement learning. Beyond free policy rollouts, DynaWeb incorporates real expert trajectories from training data, which are randomly interleaved with on-policy rollouts during training to improve stability and sample efficiency. Experiments conducted on the challenging WebArena and WebVoyager benchmarks demonstrate that DynaWeb consistently and significantly improves the performance of state-of-the-art open-source web agent models. Our findings establish the viability of training web agents through imagination, offering a scalable and efficient way to scale up online agentic RL."
    },
    {
      "title": "4D-CAAL: 4D Radar-Camera Calibration and Auto-Labeling for Autonomous Driving",
      "url": "https://arxiv.org/abs/2601.21454",
      "date": "2026-01-29",
      "authors_text": "Shanliang Yao, Zhuoxiao Li, Runwei Guan, Kebin Cao, Meng Xia",
      "is_highlight": false,
      "score": 89.0,
      "summary": "The 4D-CAAL framework integrates 4D radar-camera calibration and auto-labeling using a dual-purpose target and robust correspondence matching, enhancing autonomous driving perception efficiency.",
      "teaser_image": "https://arxiv.org/html/2601.21454/images/4DRC-fixed.png",
      "debug_abstract": "4D radar has emerged as a critical sensor for autonomous driving, primarily due to its enhanced capabilities in elevation measurement and higher resolution compared to traditional 3D radar. Effective integration of 4D radar with cameras requires accurate extrinsic calibration, and the development of radar-based perception algorithms demands large-scale annotated datasets. However, existing calibration methods often employ separate targets optimized for either visual or radar modalities, complicating correspondence establishment. Furthermore, manually labeling sparse radar data is labor-intensive and unreliable. To address these challenges, we propose 4D-CAAL, a unified framework for 4D radar-camera calibration and auto-labeling. Our approach introduces a novel dual-purpose calibration target design, integrating a checkerboard pattern on the front surface for camera detection and a corner reflector at the center of the back surface for radar detection. We develop a robust correspondence matching algorithm that aligns the checkerboard center with the strongest radar reflection point, enabling accurate extrinsic calibration. Subsequently, we present an auto-labeling pipeline that leverages the calibrated sensor relationship to transfer annotations from camera-based segmentations to radar point clouds through geometric projection and multi-feature optimization. Extensive experiments demonstrate that our method achieves high calibration accuracy while significantly reducing manual annotation effort, thereby accelerating the development of robust multi-modal perception systems for autonomous driving."
    },
    {
      "title": "Gaussian Belief Propagation Network for Depth Completion",
      "url": "https://arxiv.org/abs/2601.21291",
      "date": "2026-01-29",
      "authors_text": "Jie Tang, Pingping Xie, Jian Li, Ping Tan",
      "is_highlight": false,
      "score": 68.0,
      "summary": "The Gaussian Belief Propagation Network (GBPN) integrates deep learning with probabilistic models for effective depth completion from sparse measurements, achieving state-of-the-art results across various benchmarks.",
      "teaser_image": "https://arxiv.org/html/2601.21291/x1.png",
      "debug_abstract": "Depth completion aims to predict a dense depth map from a color image with sparse depth measurements. Although deep learning methods have achieved state-of-the-art (SOTA), effectively handling the sparse and irregular nature of input depth data in deep networks remains a significant challenge, often limiting performance, especially under high sparsity. To overcome this limitation, we introduce the Gaussian Belief Propagation Network (GBPN), a novel hybrid framework synergistically integrating deep learning with probabilistic graphical models for end-to-end depth completion. Specifically, a scene-specific Markov Random Field (MRF) is dynamically constructed by the Graphical Model Construction Network (GMCN), and then inferred via Gaussian Belief Propagation (GBP) to yield the dense depth distribution. Crucially, the GMCN learns to construct not only the data-dependent potentials of MRF but also its structure by predicting adaptive non-local edges, enabling the capture of complex, long-range spatial dependencies. Furthermore, we enhance GBP with a serial \\&amp; parallel message passing scheme, designed for effective information propagation, particularly from sparse measurements. Extensive experiments demonstrate that GBPN achieves SOTA performance on the NYUv2 and KITTI benchmarks. Evaluations across varying sparsity levels, sparsity patterns, and datasets highlight GBPN&#39;s superior performance, notable robustness, and generalizable capability."
    }
  ],
  "机器人研究所 (CKSRI)": [
    {
      "title": "DynaWeb: Model-Based Reinforcement Learning of Web Agents",
      "url": "https://arxiv.org/abs/2601.22149",
      "date": "2026-01-29",
      "authors_text": "Hang Ding, Peidong Liu, Junqiao Wang, Ziwei Ji, Meng Cao",
      "is_highlight": false,
      "score": 77.0,
      "summary": "DynaWeb introduces a model-based reinforcement learning framework that enhances web agent training by simulating interactions with a web world model, improving efficiency and performance.",
      "teaser_image": "https://arxiv.org/html/2601.22149/figures/dynaweb.png",
      "debug_abstract": "The development of autonomous web agents, powered by Large Language Models (LLMs) and reinforcement learning (RL), represents a significant step towards general-purpose AI assistants. However, training these agents is severely hampered by the challenges of interacting with the live internet, which is inefficient, costly, and fraught with risks. Model-based reinforcement learning (MBRL) offers a promising solution by learning a world model of the environment to enable simulated interaction. This paper introduces DynaWeb, a novel MBRL framework that trains web agents through interacting with a web world model trained to predict naturalistic web page representations given agent actions. This model serves as a synthetic web environment where an agent policy can dream by generating vast quantities of rollout action trajectories for efficient online reinforcement learning. Beyond free policy rollouts, DynaWeb incorporates real expert trajectories from training data, which are randomly interleaved with on-policy rollouts during training to improve stability and sample efficiency. Experiments conducted on the challenging WebArena and WebVoyager benchmarks demonstrate that DynaWeb consistently and significantly improves the performance of state-of-the-art open-source web agent models. Our findings establish the viability of training web agents through imagination, offering a scalable and efficient way to scale up online agentic RL."
    },
    {
      "title": "4D-CAAL: 4D Radar-Camera Calibration and Auto-Labeling for Autonomous Driving",
      "url": "https://arxiv.org/abs/2601.21454",
      "date": "2026-01-29",
      "authors_text": "Shanliang Yao, Zhuoxiao Li, Runwei Guan, Kebin Cao, Meng Xia",
      "is_highlight": false,
      "score": 89.0,
      "summary": "The 4D-CAAL framework integrates 4D radar-camera calibration and auto-labeling using a dual-purpose target and robust correspondence matching, enhancing autonomous driving perception efficiency.",
      "teaser_image": "https://arxiv.org/html/2601.21454/images/4DRC-fixed.png",
      "debug_abstract": "4D radar has emerged as a critical sensor for autonomous driving, primarily due to its enhanced capabilities in elevation measurement and higher resolution compared to traditional 3D radar. Effective integration of 4D radar with cameras requires accurate extrinsic calibration, and the development of radar-based perception algorithms demands large-scale annotated datasets. However, existing calibration methods often employ separate targets optimized for either visual or radar modalities, complicating correspondence establishment. Furthermore, manually labeling sparse radar data is labor-intensive and unreliable. To address these challenges, we propose 4D-CAAL, a unified framework for 4D radar-camera calibration and auto-labeling. Our approach introduces a novel dual-purpose calibration target design, integrating a checkerboard pattern on the front surface for camera detection and a corner reflector at the center of the back surface for radar detection. We develop a robust correspondence matching algorithm that aligns the checkerboard center with the strongest radar reflection point, enabling accurate extrinsic calibration. Subsequently, we present an auto-labeling pipeline that leverages the calibrated sensor relationship to transfer annotations from camera-based segmentations to radar point clouds through geometric projection and multi-feature optimization. Extensive experiments demonstrate that our method achieves high calibration accuracy while significantly reducing manual annotation effort, thereby accelerating the development of robust multi-modal perception systems for autonomous driving."
    },
    {
      "title": "Gaussian Belief Propagation Network for Depth Completion",
      "url": "https://arxiv.org/abs/2601.21291",
      "date": "2026-01-29",
      "authors_text": "Jie Tang, Pingping Xie, Jian Li, Ping Tan",
      "is_highlight": false,
      "score": 68.0,
      "summary": "The Gaussian Belief Propagation Network (GBPN) integrates deep learning with probabilistic models for effective depth completion from sparse measurements, achieving state-of-the-art results across various benchmarks.",
      "teaser_image": "https://arxiv.org/html/2601.21291/x1.png",
      "debug_abstract": "Depth completion aims to predict a dense depth map from a color image with sparse depth measurements. Although deep learning methods have achieved state-of-the-art (SOTA), effectively handling the sparse and irregular nature of input depth data in deep networks remains a significant challenge, often limiting performance, especially under high sparsity. To overcome this limitation, we introduce the Gaussian Belief Propagation Network (GBPN), a novel hybrid framework synergistically integrating deep learning with probabilistic graphical models for end-to-end depth completion. Specifically, a scene-specific Markov Random Field (MRF) is dynamically constructed by the Graphical Model Construction Network (GMCN), and then inferred via Gaussian Belief Propagation (GBP) to yield the dense depth distribution. Crucially, the GMCN learns to construct not only the data-dependent potentials of MRF but also its structure by predicting adaptive non-local edges, enabling the capture of complex, long-range spatial dependencies. Furthermore, we enhance GBP with a serial \\&amp; parallel message passing scheme, designed for effective information propagation, particularly from sparse measurements. Extensive experiments demonstrate that GBPN achieves SOTA performance on the NYUv2 and KITTI benchmarks. Evaluations across varying sparsity levels, sparsity patterns, and datasets highlight GBPN&#39;s superior performance, notable robustness, and generalizable capability."
    }
  ],
  "Cheng Kar-Shun Robotics Institute (CKSRI)": [
    {
      "title": "DynaWeb: Model-Based Reinforcement Learning of Web Agents",
      "url": "https://arxiv.org/abs/2601.22149",
      "date": "2026-01-29",
      "authors_text": "Hang Ding, Peidong Liu, Junqiao Wang, Ziwei Ji, Meng Cao",
      "is_highlight": false,
      "score": 77.0,
      "summary": "DynaWeb introduces a model-based reinforcement learning framework that enhances web agent training by simulating interactions with a web world model, improving efficiency and performance.",
      "teaser_image": "https://arxiv.org/html/2601.22149/figures/dynaweb.png",
      "debug_abstract": "The development of autonomous web agents, powered by Large Language Models (LLMs) and reinforcement learning (RL), represents a significant step towards general-purpose AI assistants. However, training these agents is severely hampered by the challenges of interacting with the live internet, which is inefficient, costly, and fraught with risks. Model-based reinforcement learning (MBRL) offers a promising solution by learning a world model of the environment to enable simulated interaction. This paper introduces DynaWeb, a novel MBRL framework that trains web agents through interacting with a web world model trained to predict naturalistic web page representations given agent actions. This model serves as a synthetic web environment where an agent policy can dream by generating vast quantities of rollout action trajectories for efficient online reinforcement learning. Beyond free policy rollouts, DynaWeb incorporates real expert trajectories from training data, which are randomly interleaved with on-policy rollouts during training to improve stability and sample efficiency. Experiments conducted on the challenging WebArena and WebVoyager benchmarks demonstrate that DynaWeb consistently and significantly improves the performance of state-of-the-art open-source web agent models. Our findings establish the viability of training web agents through imagination, offering a scalable and efficient way to scale up online agentic RL."
    },
    {
      "title": "4D-CAAL: 4D Radar-Camera Calibration and Auto-Labeling for Autonomous Driving",
      "url": "https://arxiv.org/abs/2601.21454",
      "date": "2026-01-29",
      "authors_text": "Shanliang Yao, Zhuoxiao Li, Runwei Guan, Kebin Cao, Meng Xia",
      "is_highlight": false,
      "score": 89.0,
      "summary": "The 4D-CAAL framework integrates 4D radar-camera calibration and auto-labeling using a dual-purpose target and robust correspondence matching, enhancing autonomous driving perception efficiency.",
      "teaser_image": "https://arxiv.org/html/2601.21454/images/4DRC-fixed.png",
      "debug_abstract": "4D radar has emerged as a critical sensor for autonomous driving, primarily due to its enhanced capabilities in elevation measurement and higher resolution compared to traditional 3D radar. Effective integration of 4D radar with cameras requires accurate extrinsic calibration, and the development of radar-based perception algorithms demands large-scale annotated datasets. However, existing calibration methods often employ separate targets optimized for either visual or radar modalities, complicating correspondence establishment. Furthermore, manually labeling sparse radar data is labor-intensive and unreliable. To address these challenges, we propose 4D-CAAL, a unified framework for 4D radar-camera calibration and auto-labeling. Our approach introduces a novel dual-purpose calibration target design, integrating a checkerboard pattern on the front surface for camera detection and a corner reflector at the center of the back surface for radar detection. We develop a robust correspondence matching algorithm that aligns the checkerboard center with the strongest radar reflection point, enabling accurate extrinsic calibration. Subsequently, we present an auto-labeling pipeline that leverages the calibrated sensor relationship to transfer annotations from camera-based segmentations to radar point clouds through geometric projection and multi-feature optimization. Extensive experiments demonstrate that our method achieves high calibration accuracy while significantly reducing manual annotation effort, thereby accelerating the development of robust multi-modal perception systems for autonomous driving."
    },
    {
      "title": "Gaussian Belief Propagation Network for Depth Completion",
      "url": "https://arxiv.org/abs/2601.21291",
      "date": "2026-01-29",
      "authors_text": "Jie Tang, Pingping Xie, Jian Li, Ping Tan",
      "is_highlight": false,
      "score": 68.0,
      "summary": "The Gaussian Belief Propagation Network (GBPN) integrates deep learning with probabilistic models for effective depth completion from sparse measurements, achieving state-of-the-art results across various benchmarks.",
      "teaser_image": "https://arxiv.org/html/2601.21291/x1.png",
      "debug_abstract": "Depth completion aims to predict a dense depth map from a color image with sparse depth measurements. Although deep learning methods have achieved state-of-the-art (SOTA), effectively handling the sparse and irregular nature of input depth data in deep networks remains a significant challenge, often limiting performance, especially under high sparsity. To overcome this limitation, we introduce the Gaussian Belief Propagation Network (GBPN), a novel hybrid framework synergistically integrating deep learning with probabilistic graphical models for end-to-end depth completion. Specifically, a scene-specific Markov Random Field (MRF) is dynamically constructed by the Graphical Model Construction Network (GMCN), and then inferred via Gaussian Belief Propagation (GBP) to yield the dense depth distribution. Crucially, the GMCN learns to construct not only the data-dependent potentials of MRF but also its structure by predicting adaptive non-local edges, enabling the capture of complex, long-range spatial dependencies. Furthermore, we enhance GBP with a serial \\&amp; parallel message passing scheme, designed for effective information propagation, particularly from sparse measurements. Extensive experiments demonstrate that GBPN achieves SOTA performance on the NYUv2 and KITTI benchmarks. Evaluations across varying sparsity levels, sparsity patterns, and datasets highlight GBPN&#39;s superior performance, notable robustness, and generalizable capability."
    }
  ],
  "Precognition Lab": [
    {
      "title": "DynaWeb: Model-Based Reinforcement Learning of Web Agents",
      "url": "https://arxiv.org/abs/2601.22149",
      "date": "2026-01-29",
      "authors_text": "Hang Ding, Peidong Liu, Junqiao Wang, Ziwei Ji, Meng Cao",
      "is_highlight": false,
      "score": 77.0,
      "summary": "DynaWeb introduces a model-based reinforcement learning framework that enhances web agent training by simulating interactions with a web world model, improving efficiency and performance.",
      "teaser_image": "https://arxiv.org/html/2601.22149/figures/dynaweb.png",
      "debug_abstract": "The development of autonomous web agents, powered by Large Language Models (LLMs) and reinforcement learning (RL), represents a significant step towards general-purpose AI assistants. However, training these agents is severely hampered by the challenges of interacting with the live internet, which is inefficient, costly, and fraught with risks. Model-based reinforcement learning (MBRL) offers a promising solution by learning a world model of the environment to enable simulated interaction. This paper introduces DynaWeb, a novel MBRL framework that trains web agents through interacting with a web world model trained to predict naturalistic web page representations given agent actions. This model serves as a synthetic web environment where an agent policy can dream by generating vast quantities of rollout action trajectories for efficient online reinforcement learning. Beyond free policy rollouts, DynaWeb incorporates real expert trajectories from training data, which are randomly interleaved with on-policy rollouts during training to improve stability and sample efficiency. Experiments conducted on the challenging WebArena and WebVoyager benchmarks demonstrate that DynaWeb consistently and significantly improves the performance of state-of-the-art open-source web agent models. Our findings establish the viability of training web agents through imagination, offering a scalable and efficient way to scale up online agentic RL."
    },
    {
      "title": "4D-CAAL: 4D Radar-Camera Calibration and Auto-Labeling for Autonomous Driving",
      "url": "https://arxiv.org/abs/2601.21454",
      "date": "2026-01-29",
      "authors_text": "Shanliang Yao, Zhuoxiao Li, Runwei Guan, Kebin Cao, Meng Xia",
      "is_highlight": false,
      "score": 89.0,
      "summary": "The 4D-CAAL framework integrates 4D radar-camera calibration and auto-labeling using a dual-purpose target and robust correspondence matching, enhancing autonomous driving perception efficiency.",
      "teaser_image": "https://arxiv.org/html/2601.21454/images/4DRC-fixed.png",
      "debug_abstract": "4D radar has emerged as a critical sensor for autonomous driving, primarily due to its enhanced capabilities in elevation measurement and higher resolution compared to traditional 3D radar. Effective integration of 4D radar with cameras requires accurate extrinsic calibration, and the development of radar-based perception algorithms demands large-scale annotated datasets. However, existing calibration methods often employ separate targets optimized for either visual or radar modalities, complicating correspondence establishment. Furthermore, manually labeling sparse radar data is labor-intensive and unreliable. To address these challenges, we propose 4D-CAAL, a unified framework for 4D radar-camera calibration and auto-labeling. Our approach introduces a novel dual-purpose calibration target design, integrating a checkerboard pattern on the front surface for camera detection and a corner reflector at the center of the back surface for radar detection. We develop a robust correspondence matching algorithm that aligns the checkerboard center with the strongest radar reflection point, enabling accurate extrinsic calibration. Subsequently, we present an auto-labeling pipeline that leverages the calibrated sensor relationship to transfer annotations from camera-based segmentations to radar point clouds through geometric projection and multi-feature optimization. Extensive experiments demonstrate that our method achieves high calibration accuracy while significantly reducing manual annotation effort, thereby accelerating the development of robust multi-modal perception systems for autonomous driving."
    },
    {
      "title": "Gaussian Belief Propagation Network for Depth Completion",
      "url": "https://arxiv.org/abs/2601.21291",
      "date": "2026-01-29",
      "authors_text": "Jie Tang, Pingping Xie, Jian Li, Ping Tan",
      "is_highlight": false,
      "score": 68.0,
      "summary": "The Gaussian Belief Propagation Network (GBPN) integrates deep learning with probabilistic models for effective depth completion from sparse measurements, achieving state-of-the-art results across various benchmarks.",
      "teaser_image": "https://arxiv.org/html/2601.21291/x1.png",
      "debug_abstract": "Depth completion aims to predict a dense depth map from a color image with sparse depth measurements. Although deep learning methods have achieved state-of-the-art (SOTA), effectively handling the sparse and irregular nature of input depth data in deep networks remains a significant challenge, often limiting performance, especially under high sparsity. To overcome this limitation, we introduce the Gaussian Belief Propagation Network (GBPN), a novel hybrid framework synergistically integrating deep learning with probabilistic graphical models for end-to-end depth completion. Specifically, a scene-specific Markov Random Field (MRF) is dynamically constructed by the Graphical Model Construction Network (GMCN), and then inferred via Gaussian Belief Propagation (GBP) to yield the dense depth distribution. Crucially, the GMCN learns to construct not only the data-dependent potentials of MRF but also its structure by predicting adaptive non-local edges, enabling the capture of complex, long-range spatial dependencies. Furthermore, we enhance GBP with a serial \\&amp; parallel message passing scheme, designed for effective information propagation, particularly from sparse measurements. Extensive experiments demonstrate that GBPN achieves SOTA performance on the NYUv2 and KITTI benchmarks. Evaluations across varying sparsity levels, sparsity patterns, and datasets highlight GBPN&#39;s superior performance, notable robustness, and generalizable capability."
    }
  ],
  "机器人研究所": [
    {
      "title": "DynaWeb: Model-Based Reinforcement Learning of Web Agents",
      "url": "https://arxiv.org/abs/2601.22149",
      "date": "2026-01-29",
      "authors_text": "Hang Ding, Peidong Liu, Junqiao Wang, Ziwei Ji, Meng Cao",
      "is_highlight": false,
      "score": 77.0,
      "summary": "DynaWeb introduces a model-based reinforcement learning framework that enhances web agent training by simulating interactions with a web world model, improving efficiency and performance.",
      "teaser_image": "https://arxiv.org/html/2601.22149/figures/dynaweb.png",
      "debug_abstract": "The development of autonomous web agents, powered by Large Language Models (LLMs) and reinforcement learning (RL), represents a significant step towards general-purpose AI assistants. However, training these agents is severely hampered by the challenges of interacting with the live internet, which is inefficient, costly, and fraught with risks. Model-based reinforcement learning (MBRL) offers a promising solution by learning a world model of the environment to enable simulated interaction. This paper introduces DynaWeb, a novel MBRL framework that trains web agents through interacting with a web world model trained to predict naturalistic web page representations given agent actions. This model serves as a synthetic web environment where an agent policy can dream by generating vast quantities of rollout action trajectories for efficient online reinforcement learning. Beyond free policy rollouts, DynaWeb incorporates real expert trajectories from training data, which are randomly interleaved with on-policy rollouts during training to improve stability and sample efficiency. Experiments conducted on the challenging WebArena and WebVoyager benchmarks demonstrate that DynaWeb consistently and significantly improves the performance of state-of-the-art open-source web agent models. Our findings establish the viability of training web agents through imagination, offering a scalable and efficient way to scale up online agentic RL."
    },
    {
      "title": "4D-CAAL: 4D Radar-Camera Calibration and Auto-Labeling for Autonomous Driving",
      "url": "https://arxiv.org/abs/2601.21454",
      "date": "2026-01-29",
      "authors_text": "Shanliang Yao, Zhuoxiao Li, Runwei Guan, Kebin Cao, Meng Xia",
      "is_highlight": false,
      "score": 89.0,
      "summary": "The 4D-CAAL framework integrates 4D radar-camera calibration and auto-labeling using a dual-purpose target and robust correspondence matching, enhancing autonomous driving perception efficiency.",
      "teaser_image": "https://arxiv.org/html/2601.21454/images/4DRC-fixed.png",
      "debug_abstract": "4D radar has emerged as a critical sensor for autonomous driving, primarily due to its enhanced capabilities in elevation measurement and higher resolution compared to traditional 3D radar. Effective integration of 4D radar with cameras requires accurate extrinsic calibration, and the development of radar-based perception algorithms demands large-scale annotated datasets. However, existing calibration methods often employ separate targets optimized for either visual or radar modalities, complicating correspondence establishment. Furthermore, manually labeling sparse radar data is labor-intensive and unreliable. To address these challenges, we propose 4D-CAAL, a unified framework for 4D radar-camera calibration and auto-labeling. Our approach introduces a novel dual-purpose calibration target design, integrating a checkerboard pattern on the front surface for camera detection and a corner reflector at the center of the back surface for radar detection. We develop a robust correspondence matching algorithm that aligns the checkerboard center with the strongest radar reflection point, enabling accurate extrinsic calibration. Subsequently, we present an auto-labeling pipeline that leverages the calibrated sensor relationship to transfer annotations from camera-based segmentations to radar point clouds through geometric projection and multi-feature optimization. Extensive experiments demonstrate that our method achieves high calibration accuracy while significantly reducing manual annotation effort, thereby accelerating the development of robust multi-modal perception systems for autonomous driving."
    },
    {
      "title": "Gaussian Belief Propagation Network for Depth Completion",
      "url": "https://arxiv.org/abs/2601.21291",
      "date": "2026-01-29",
      "authors_text": "Jie Tang, Pingping Xie, Jian Li, Ping Tan",
      "is_highlight": false,
      "score": 68.0,
      "summary": "The Gaussian Belief Propagation Network (GBPN) integrates deep learning with probabilistic models for effective depth completion from sparse measurements, achieving state-of-the-art results across various benchmarks.",
      "teaser_image": "https://arxiv.org/html/2601.21291/x1.png",
      "debug_abstract": "Depth completion aims to predict a dense depth map from a color image with sparse depth measurements. Although deep learning methods have achieved state-of-the-art (SOTA), effectively handling the sparse and irregular nature of input depth data in deep networks remains a significant challenge, often limiting performance, especially under high sparsity. To overcome this limitation, we introduce the Gaussian Belief Propagation Network (GBPN), a novel hybrid framework synergistically integrating deep learning with probabilistic graphical models for end-to-end depth completion. Specifically, a scene-specific Markov Random Field (MRF) is dynamically constructed by the Graphical Model Construction Network (GMCN), and then inferred via Gaussian Belief Propagation (GBP) to yield the dense depth distribution. Crucially, the GMCN learns to construct not only the data-dependent potentials of MRF but also its structure by predicting adaptive non-local edges, enabling the capture of complex, long-range spatial dependencies. Furthermore, we enhance GBP with a serial \\&amp; parallel message passing scheme, designed for effective information propagation, particularly from sparse measurements. Extensive experiments demonstrate that GBPN achieves SOTA performance on the NYUv2 and KITTI benchmarks. Evaluations across varying sparsity levels, sparsity patterns, and datasets highlight GBPN&#39;s superior performance, notable robustness, and generalizable capability."
    }
  ],
  "西安电子科技大学": [
    {
      "title": "Towards Bridging the Gap between Large-Scale Pretraining and Efficient Finetuning for Humanoid Control",
      "url": "https://arxiv.org/abs/2601.21363",
      "date": "2026-01-29",
      "authors_text": "Weidong Huang, Zhehan Li, Hangxin Liu, Biao Hou, Yao Su",
      "is_highlight": false,
      "score": 85.0,
      "summary": "This paper presents a method combining off-policy Soft Actor-Critic for efficient pretraining and model-based techniques for safe finetuning of humanoid locomotion policies.",
      "teaser_image": "https://arxiv.org/html/2601.21363/x1.png",
      "debug_abstract": "Reinforcement learning (RL) is widely used for humanoid control, with on-policy methods such as Proximal Policy Optimization (PPO) enabling robust training via large-scale parallel simulation and, in some cases, zero-shot deployment to real robots. However, the low sample efficiency of on-policy algorithms limits safe adaptation to new environments. Although off-policy RL and model-based RL have shown improved sample efficiency, the gap between large-scale pretraining and efficient finetuning on humanoids still exists. In this paper, we find that off-policy Soft Actor-Critic (SAC), with large-batch update and a high Update-To-Data (UTD) ratio, reliably supports large-scale pretraining of humanoid locomotion policies, achieving zero-shot deployment on real robots. For adaptation, we demonstrate that these SAC-pretrained policies can be finetuned in new environments and out-of-distribution tasks using model-based methods. Data collection in the new environment executes a deterministic policy while stochastic exploration is instead confined to a physics-informed world model. This separation mitigates the risks of random exploration during adaptation while preserving exploratory coverage for improvement. Overall, the approach couples the wall-clock efficiency of large-scale simulation during pretraining with the sample efficiency of model-based learning during fine-tuning."
    },
    {
      "title": "Heterogeneous Vertiport Selection Optimization for On-Demand Air Taxi Services: A Deep Reinforcement Learning Approach",
      "url": "https://arxiv.org/abs/2601.21316",
      "date": "2026-01-29",
      "authors_text": "Aoyu Pang, Maonan Wang, Zifan Sha, Wenwei Yue, Changle Li",
      "is_highlight": false,
      "score": 80.0,
      "summary": "The paper presents a deep reinforcement learning framework for optimizing vertiport selection and air taxi routing, achieving significant travel time reductions in urban air mobility systems.",
      "teaser_image": "https://arxiv.org/html/2601.21316/x2.png",
      "debug_abstract": "Urban Air Mobility (UAM) has emerged as a transformative solution to alleviate urban congestion by utilizing low-altitude airspace, thereby reducing pressure on ground transportation networks. To enable truly efficient and seamless door-to-door travel experiences, UAM requires close integration with existing ground transportation infrastructure. However, current research on optimal integrated routing strategies for passengers in air-ground mobility systems remains limited, with a lack of systematic this http URL address this gap, we first propose a unified optimization model that integrates strategy selection for both air and ground transportation. This model captures the dynamic characteristics of multimodal transport networks and incorporates real-time traffic conditions alongside passenger decision-making behavior. Building on this model, we propose a Unified Air-Ground Mobility Coordination (UAGMC) framework, which leverages deep reinforcement learning (RL) and Vehicle-to-Everything (V2X) communication to optimize vertiport selection and dynamically plan air taxi routes. Experimental results demonstrate that UAGMC achieves a 34\\% reduction in average travel time compared to conventional proportional allocation methods, enhancing overall travel efficiency and providing novel insights into the integration and optimization of multimodal transportation systems. This work lays a solid foundation for advancing intelligent urban mobility solutions through the coordination of air and ground transportation modes. The related code can be found at this https URL."
    }
  ],
  "上海人工智能实验室": [
    {
      "title": "Heterogeneous Vertiport Selection Optimization for On-Demand Air Taxi Services: A Deep Reinforcement Learning Approach",
      "url": "https://arxiv.org/abs/2601.21316",
      "date": "2026-01-29",
      "authors_text": "Aoyu Pang, Maonan Wang, Zifan Sha, Wenwei Yue, Changle Li",
      "is_highlight": false,
      "score": 80.0,
      "summary": "The paper presents a deep reinforcement learning framework for optimizing vertiport selection and air taxi routing, achieving significant travel time reductions in urban air mobility systems.",
      "teaser_image": "https://arxiv.org/html/2601.21316/x2.png",
      "debug_abstract": "Urban Air Mobility (UAM) has emerged as a transformative solution to alleviate urban congestion by utilizing low-altitude airspace, thereby reducing pressure on ground transportation networks. To enable truly efficient and seamless door-to-door travel experiences, UAM requires close integration with existing ground transportation infrastructure. However, current research on optimal integrated routing strategies for passengers in air-ground mobility systems remains limited, with a lack of systematic this http URL address this gap, we first propose a unified optimization model that integrates strategy selection for both air and ground transportation. This model captures the dynamic characteristics of multimodal transport networks and incorporates real-time traffic conditions alongside passenger decision-making behavior. Building on this model, we propose a Unified Air-Ground Mobility Coordination (UAGMC) framework, which leverages deep reinforcement learning (RL) and Vehicle-to-Everything (V2X) communication to optimize vertiport selection and dynamically plan air taxi routes. Experimental results demonstrate that UAGMC achieves a 34\\% reduction in average travel time compared to conventional proportional allocation methods, enhancing overall travel efficiency and providing novel insights into the integration and optimization of multimodal transportation systems. This work lays a solid foundation for advancing intelligent urban mobility solutions through the coordination of air and ground transportation modes. The related code can be found at this https URL."
    }
  ],
  "机器人与人工智能实验室": [
    {
      "title": "PLANING: A Loosely Coupled Triangle-Gaussian Framework for Streaming 3D Reconstruction",
      "url": "https://arxiv.org/abs/2601.22046",
      "date": "2026-01-29",
      "authors_text": "Changjian Jiang, Kerui Ren, Xudong Li, Kaiwen Song, Linning Xu",
      "is_highlight": false,
      "score": 66.0,
      "summary": "PLANING is a novel streaming 3D reconstruction framework that efficiently combines geometric primitives and neural Gaussians, enhancing quality and speed while reducing redundancy.",
      "teaser_image": "https://arxiv.org/html/2601.22046/x3.png",
      "debug_abstract": "Streaming reconstruction from monocular image sequences remains challenging, as existing methods typically favor either high-quality rendering or accurate geometry, but rarely both. We present PLANING, an efficient on-the-fly reconstruction framework built on a hybrid representation that loosely couples explicit geometric primitives with neural Gaussians, enabling geometry and appearance to be modeled in a decoupled manner. This decoupling supports an online initialization and optimization strategy that separates geometry and appearance updates, yielding stable streaming reconstruction with substantially reduced structural redundancy. PLANING improves dense mesh Chamfer-L2 by 18.52% over PGSR, surpasses ARTDECO by 1.31 dB PSNR, and reconstructs ScanNetV2 scenes in under 100 seconds, over 5x faster than 2D Gaussian Splatting, while matching the quality of offline per-scene optimization. Beyond reconstruction quality, the structural clarity and computational efficiency of \\modelname~make it well suited for a broad range of downstream applications, such as enabling large-scale scene modeling and simulation-ready environments for embodied AI. Project page: this https URL ."
    },
    {
      "title": "Disentangling perception and reasoning for improving data efficiency in learning cloth manipulation without demonstrations",
      "url": "https://arxiv.org/abs/2601.21713",
      "date": "2026-01-29",
      "authors_text": "Donatien Delehelle, Fei Chen, Darwin Caldwell",
      "is_highlight": false,
      "score": 87.0,
      "summary": "This paper presents a modular reinforcement learning approach for cloth manipulation that enhances data efficiency and reduces model size and training time, achieving improved performance in simulation and real-world transfer.",
      "teaser_image": "https://arxiv.org/html/2601.21713/x1.png",
      "debug_abstract": "Cloth manipulation is a ubiquitous task in everyday life, but it remains an open challenge for robotics. The difficulties in developing cloth manipulation policies are attributed to the high-dimensional state space, complex dynamics, and high propensity to self-occlusion exhibited by fabrics. As analytical methods have not been able to provide robust and general manipulation policies, reinforcement learning (RL) is considered a promising approach to these problems. However, to address the large state space and complex dynamics, data-based methods usually rely on large models and long training times. The resulting computational cost significantly hampers the development and adoption of these methods. Additionally, due to the challenge of robust state estimation, garment manipulation policies often adopt an end-to-end learning approach with workspace images as input. While this approach enables a conceptually straightforward sim-to-real transfer via real-world fine-tuning, it also incurs a significant computational cost by training agents on a highly lossy representation of the environment state. This paper questions this common design choice by exploring an efficient and modular approach to RL for cloth manipulation. We show that, through careful design choices, model size and training time can be significantly reduced when learning in simulation. Furthermore, we demonstrate how the resulting simulation-trained model can be transferred to the real world. We evaluate our approach on the SoftGym benchmark and achieve significant performance improvements over available baselines on our task, while using a substantially smaller model."
    },
    {
      "title": "Heterogeneous Vertiport Selection Optimization for On-Demand Air Taxi Services: A Deep Reinforcement Learning Approach",
      "url": "https://arxiv.org/abs/2601.21316",
      "date": "2026-01-29",
      "authors_text": "Aoyu Pang, Maonan Wang, Zifan Sha, Wenwei Yue, Changle Li",
      "is_highlight": false,
      "score": 80.0,
      "summary": "The paper presents a deep reinforcement learning framework for optimizing vertiport selection and air taxi routing, achieving significant travel time reductions in urban air mobility systems.",
      "teaser_image": "https://arxiv.org/html/2601.21316/x2.png",
      "debug_abstract": "Urban Air Mobility (UAM) has emerged as a transformative solution to alleviate urban congestion by utilizing low-altitude airspace, thereby reducing pressure on ground transportation networks. To enable truly efficient and seamless door-to-door travel experiences, UAM requires close integration with existing ground transportation infrastructure. However, current research on optimal integrated routing strategies for passengers in air-ground mobility systems remains limited, with a lack of systematic this http URL address this gap, we first propose a unified optimization model that integrates strategy selection for both air and ground transportation. This model captures the dynamic characteristics of multimodal transport networks and incorporates real-time traffic conditions alongside passenger decision-making behavior. Building on this model, we propose a Unified Air-Ground Mobility Coordination (UAGMC) framework, which leverages deep reinforcement learning (RL) and Vehicle-to-Everything (V2X) communication to optimize vertiport selection and dynamically plan air taxi routes. Experimental results demonstrate that UAGMC achieves a 34\\% reduction in average travel time compared to conventional proportional allocation methods, enhancing overall travel efficiency and providing novel insights into the integration and optimization of multimodal transportation systems. This work lays a solid foundation for advancing intelligent urban mobility solutions through the coordination of air and ground transportation modes. The related code can be found at this https URL."
    }
  ],
  "机器人与自动化研究中心": [
    {
      "title": "PLANING: A Loosely Coupled Triangle-Gaussian Framework for Streaming 3D Reconstruction",
      "url": "https://arxiv.org/abs/2601.22046",
      "date": "2026-01-29",
      "authors_text": "Changjian Jiang, Kerui Ren, Xudong Li, Kaiwen Song, Linning Xu",
      "is_highlight": false,
      "score": 66.0,
      "summary": "PLANING is a novel streaming 3D reconstruction framework that efficiently combines geometric primitives and neural Gaussians, enhancing quality and speed while reducing redundancy.",
      "teaser_image": "https://arxiv.org/html/2601.22046/x3.png",
      "debug_abstract": "Streaming reconstruction from monocular image sequences remains challenging, as existing methods typically favor either high-quality rendering or accurate geometry, but rarely both. We present PLANING, an efficient on-the-fly reconstruction framework built on a hybrid representation that loosely couples explicit geometric primitives with neural Gaussians, enabling geometry and appearance to be modeled in a decoupled manner. This decoupling supports an online initialization and optimization strategy that separates geometry and appearance updates, yielding stable streaming reconstruction with substantially reduced structural redundancy. PLANING improves dense mesh Chamfer-L2 by 18.52% over PGSR, surpasses ARTDECO by 1.31 dB PSNR, and reconstructs ScanNetV2 scenes in under 100 seconds, over 5x faster than 2D Gaussian Splatting, while matching the quality of offline per-scene optimization. Beyond reconstruction quality, the structural clarity and computational efficiency of \\modelname~make it well suited for a broad range of downstream applications, such as enabling large-scale scene modeling and simulation-ready environments for embodied AI. Project page: this https URL ."
    },
    {
      "title": "Dynamic Topology Awareness: Breaking the Granularity Rigidity in Vision-Language Navigation",
      "url": "https://arxiv.org/abs/2601.21751",
      "date": "2026-01-29",
      "authors_text": "Jiankun Peng, Jianyuan Guo, Ying Xu, Yue Liu, Jiashuang Yan",
      "is_highlight": false,
      "score": 84.0,
      "summary": "The paper presents DGNav, a dynamic framework for Vision-Language Navigation that adapts topological map density and connectivity to enhance navigation efficiency and safety in complex environments.",
      "teaser_image": "https://arxiv.org/html/2601.21751/x1.png",
      "debug_abstract": "Vision-Language Navigation in Continuous Environments (VLN-CE) presents a core challenge: grounding high-level linguistic instructions into precise, safe, and long-horizon spatial actions. Explicit topological maps have proven to be a vital solution for providing robust spatial memory in such tasks. However, existing topological planning methods suffer from a &#34;Granularity Rigidity&#34; problem. Specifically, these methods typically rely on fixed geometric thresholds to sample nodes, which fails to adapt to varying environmental complexities. This rigidity leads to a critical mismatch: the model tends to over-sample in simple areas, causing computational redundancy, while under-sampling in high-uncertainty regions, increasing collision risks and compromising precision. To address this, we propose DGNav, a framework for Dynamic Topological Navigation, introducing a context-aware mechanism to modulate map density and connectivity on-the-fly. Our approach comprises two core innovations: (1) A Scene-Aware Adaptive Strategy that dynamically modulates graph construction thresholds based on the dispersion of predicted waypoints, enabling &#34;densification on demand&#34; in challenging environments; (2) A Dynamic Graph Transformer that reconstructs graph connectivity by fusing visual, linguistic, and geometric cues into dynamic edge weights, enabling the agent to filter out topological noise and enhancing instruction adherence. Extensive experiments on the R2R-CE and RxR-CE benchmarks demonstrate DGNav exhibits superior navigation performance and strong generalization capabilities. Furthermore, ablation studies confirm that our framework achieves an optimal trade-off between navigation efficiency and safe exploration. The code is available at this https URL."
    },
    {
      "title": "Disentangling perception and reasoning for improving data efficiency in learning cloth manipulation without demonstrations",
      "url": "https://arxiv.org/abs/2601.21713",
      "date": "2026-01-29",
      "authors_text": "Donatien Delehelle, Fei Chen, Darwin Caldwell",
      "is_highlight": false,
      "score": 87.0,
      "summary": "This paper presents a modular reinforcement learning approach for cloth manipulation that enhances data efficiency and reduces model size and training time, achieving improved performance in simulation and real-world transfer.",
      "teaser_image": "https://arxiv.org/html/2601.21713/x1.png",
      "debug_abstract": "Cloth manipulation is a ubiquitous task in everyday life, but it remains an open challenge for robotics. The difficulties in developing cloth manipulation policies are attributed to the high-dimensional state space, complex dynamics, and high propensity to self-occlusion exhibited by fabrics. As analytical methods have not been able to provide robust and general manipulation policies, reinforcement learning (RL) is considered a promising approach to these problems. However, to address the large state space and complex dynamics, data-based methods usually rely on large models and long training times. The resulting computational cost significantly hampers the development and adoption of these methods. Additionally, due to the challenge of robust state estimation, garment manipulation policies often adopt an end-to-end learning approach with workspace images as input. While this approach enables a conceptually straightforward sim-to-real transfer via real-world fine-tuning, it also incurs a significant computational cost by training agents on a highly lossy representation of the environment state. This paper questions this common design choice by exploring an efficient and modular approach to RL for cloth manipulation. We show that, through careful design choices, model size and training time can be significantly reduced when learning in simulation. Furthermore, we demonstrate how the resulting simulation-trained model can be transferred to the real world. We evaluate our approach on the SoftGym benchmark and achieve significant performance improvements over available baselines on our task, while using a substantially smaller model."
    },
    {
      "title": "Heterogeneous Vertiport Selection Optimization for On-Demand Air Taxi Services: A Deep Reinforcement Learning Approach",
      "url": "https://arxiv.org/abs/2601.21316",
      "date": "2026-01-29",
      "authors_text": "Aoyu Pang, Maonan Wang, Zifan Sha, Wenwei Yue, Changle Li",
      "is_highlight": false,
      "score": 80.0,
      "summary": "The paper presents a deep reinforcement learning framework for optimizing vertiport selection and air taxi routing, achieving significant travel time reductions in urban air mobility systems.",
      "teaser_image": "https://arxiv.org/html/2601.21316/x2.png",
      "debug_abstract": "Urban Air Mobility (UAM) has emerged as a transformative solution to alleviate urban congestion by utilizing low-altitude airspace, thereby reducing pressure on ground transportation networks. To enable truly efficient and seamless door-to-door travel experiences, UAM requires close integration with existing ground transportation infrastructure. However, current research on optimal integrated routing strategies for passengers in air-ground mobility systems remains limited, with a lack of systematic this http URL address this gap, we first propose a unified optimization model that integrates strategy selection for both air and ground transportation. This model captures the dynamic characteristics of multimodal transport networks and incorporates real-time traffic conditions alongside passenger decision-making behavior. Building on this model, we propose a Unified Air-Ground Mobility Coordination (UAGMC) framework, which leverages deep reinforcement learning (RL) and Vehicle-to-Everything (V2X) communication to optimize vertiport selection and dynamically plan air taxi routes. Experimental results demonstrate that UAGMC achieves a 34\\% reduction in average travel time compared to conventional proportional allocation methods, enhancing overall travel efficiency and providing novel insights into the integration and optimization of multimodal transportation systems. This work lays a solid foundation for advancing intelligent urban mobility solutions through the coordination of air and ground transportation modes. The related code can be found at this https URL."
    }
  ],
  "机器人与人工智能实验室 (RAIL)": [
    {
      "title": "PLANING: A Loosely Coupled Triangle-Gaussian Framework for Streaming 3D Reconstruction",
      "url": "https://arxiv.org/abs/2601.22046",
      "date": "2026-01-29",
      "authors_text": "Changjian Jiang, Kerui Ren, Xudong Li, Kaiwen Song, Linning Xu",
      "is_highlight": false,
      "score": 66.0,
      "summary": "PLANING is a novel streaming 3D reconstruction framework that efficiently combines geometric primitives and neural Gaussians, enhancing quality and speed while reducing redundancy.",
      "teaser_image": "https://arxiv.org/html/2601.22046/x3.png",
      "debug_abstract": "Streaming reconstruction from monocular image sequences remains challenging, as existing methods typically favor either high-quality rendering or accurate geometry, but rarely both. We present PLANING, an efficient on-the-fly reconstruction framework built on a hybrid representation that loosely couples explicit geometric primitives with neural Gaussians, enabling geometry and appearance to be modeled in a decoupled manner. This decoupling supports an online initialization and optimization strategy that separates geometry and appearance updates, yielding stable streaming reconstruction with substantially reduced structural redundancy. PLANING improves dense mesh Chamfer-L2 by 18.52% over PGSR, surpasses ARTDECO by 1.31 dB PSNR, and reconstructs ScanNetV2 scenes in under 100 seconds, over 5x faster than 2D Gaussian Splatting, while matching the quality of offline per-scene optimization. Beyond reconstruction quality, the structural clarity and computational efficiency of \\modelname~make it well suited for a broad range of downstream applications, such as enabling large-scale scene modeling and simulation-ready environments for embodied AI. Project page: this https URL ."
    },
    {
      "title": "Disentangling perception and reasoning for improving data efficiency in learning cloth manipulation without demonstrations",
      "url": "https://arxiv.org/abs/2601.21713",
      "date": "2026-01-29",
      "authors_text": "Donatien Delehelle, Fei Chen, Darwin Caldwell",
      "is_highlight": false,
      "score": 87.0,
      "summary": "This paper presents a modular reinforcement learning approach for cloth manipulation that enhances data efficiency and reduces model size and training time, achieving improved performance in simulation and real-world transfer.",
      "teaser_image": "https://arxiv.org/html/2601.21713/x1.png",
      "debug_abstract": "Cloth manipulation is a ubiquitous task in everyday life, but it remains an open challenge for robotics. The difficulties in developing cloth manipulation policies are attributed to the high-dimensional state space, complex dynamics, and high propensity to self-occlusion exhibited by fabrics. As analytical methods have not been able to provide robust and general manipulation policies, reinforcement learning (RL) is considered a promising approach to these problems. However, to address the large state space and complex dynamics, data-based methods usually rely on large models and long training times. The resulting computational cost significantly hampers the development and adoption of these methods. Additionally, due to the challenge of robust state estimation, garment manipulation policies often adopt an end-to-end learning approach with workspace images as input. While this approach enables a conceptually straightforward sim-to-real transfer via real-world fine-tuning, it also incurs a significant computational cost by training agents on a highly lossy representation of the environment state. This paper questions this common design choice by exploring an efficient and modular approach to RL for cloth manipulation. We show that, through careful design choices, model size and training time can be significantly reduced when learning in simulation. Furthermore, we demonstrate how the resulting simulation-trained model can be transferred to the real world. We evaluate our approach on the SoftGym benchmark and achieve significant performance improvements over available baselines on our task, while using a substantially smaller model."
    },
    {
      "title": "Heterogeneous Vertiport Selection Optimization for On-Demand Air Taxi Services: A Deep Reinforcement Learning Approach",
      "url": "https://arxiv.org/abs/2601.21316",
      "date": "2026-01-29",
      "authors_text": "Aoyu Pang, Maonan Wang, Zifan Sha, Wenwei Yue, Changle Li",
      "is_highlight": false,
      "score": 80.0,
      "summary": "The paper presents a deep reinforcement learning framework for optimizing vertiport selection and air taxi routing, achieving significant travel time reductions in urban air mobility systems.",
      "teaser_image": "https://arxiv.org/html/2601.21316/x2.png",
      "debug_abstract": "Urban Air Mobility (UAM) has emerged as a transformative solution to alleviate urban congestion by utilizing low-altitude airspace, thereby reducing pressure on ground transportation networks. To enable truly efficient and seamless door-to-door travel experiences, UAM requires close integration with existing ground transportation infrastructure. However, current research on optimal integrated routing strategies for passengers in air-ground mobility systems remains limited, with a lack of systematic this http URL address this gap, we first propose a unified optimization model that integrates strategy selection for both air and ground transportation. This model captures the dynamic characteristics of multimodal transport networks and incorporates real-time traffic conditions alongside passenger decision-making behavior. Building on this model, we propose a Unified Air-Ground Mobility Coordination (UAGMC) framework, which leverages deep reinforcement learning (RL) and Vehicle-to-Everything (V2X) communication to optimize vertiport selection and dynamically plan air taxi routes. Experimental results demonstrate that UAGMC achieves a 34\\% reduction in average travel time compared to conventional proportional allocation methods, enhancing overall travel efficiency and providing novel insights into the integration and optimization of multimodal transportation systems. This work lays a solid foundation for advancing intelligent urban mobility solutions through the coordination of air and ground transportation modes. The related code can be found at this https URL."
    }
  ],
  "Multimedia Lab (MMLab)": [
    {
      "title": "PLANING: A Loosely Coupled Triangle-Gaussian Framework for Streaming 3D Reconstruction",
      "url": "https://arxiv.org/abs/2601.22046",
      "date": "2026-01-29",
      "authors_text": "Changjian Jiang, Kerui Ren, Xudong Li, Kaiwen Song, Linning Xu",
      "is_highlight": false,
      "score": 66.0,
      "summary": "PLANING is a novel streaming 3D reconstruction framework that efficiently combines geometric primitives and neural Gaussians, enhancing quality and speed while reducing redundancy.",
      "teaser_image": "https://arxiv.org/html/2601.22046/x3.png",
      "debug_abstract": "Streaming reconstruction from monocular image sequences remains challenging, as existing methods typically favor either high-quality rendering or accurate geometry, but rarely both. We present PLANING, an efficient on-the-fly reconstruction framework built on a hybrid representation that loosely couples explicit geometric primitives with neural Gaussians, enabling geometry and appearance to be modeled in a decoupled manner. This decoupling supports an online initialization and optimization strategy that separates geometry and appearance updates, yielding stable streaming reconstruction with substantially reduced structural redundancy. PLANING improves dense mesh Chamfer-L2 by 18.52% over PGSR, surpasses ARTDECO by 1.31 dB PSNR, and reconstructs ScanNetV2 scenes in under 100 seconds, over 5x faster than 2D Gaussian Splatting, while matching the quality of offline per-scene optimization. Beyond reconstruction quality, the structural clarity and computational efficiency of \\modelname~make it well suited for a broad range of downstream applications, such as enabling large-scale scene modeling and simulation-ready environments for embodied AI. Project page: this https URL ."
    },
    {
      "title": "Disentangling perception and reasoning for improving data efficiency in learning cloth manipulation without demonstrations",
      "url": "https://arxiv.org/abs/2601.21713",
      "date": "2026-01-29",
      "authors_text": "Donatien Delehelle, Fei Chen, Darwin Caldwell",
      "is_highlight": false,
      "score": 87.0,
      "summary": "This paper presents a modular reinforcement learning approach for cloth manipulation that enhances data efficiency and reduces model size and training time, achieving improved performance in simulation and real-world transfer.",
      "teaser_image": "https://arxiv.org/html/2601.21713/x1.png",
      "debug_abstract": "Cloth manipulation is a ubiquitous task in everyday life, but it remains an open challenge for robotics. The difficulties in developing cloth manipulation policies are attributed to the high-dimensional state space, complex dynamics, and high propensity to self-occlusion exhibited by fabrics. As analytical methods have not been able to provide robust and general manipulation policies, reinforcement learning (RL) is considered a promising approach to these problems. However, to address the large state space and complex dynamics, data-based methods usually rely on large models and long training times. The resulting computational cost significantly hampers the development and adoption of these methods. Additionally, due to the challenge of robust state estimation, garment manipulation policies often adopt an end-to-end learning approach with workspace images as input. While this approach enables a conceptually straightforward sim-to-real transfer via real-world fine-tuning, it also incurs a significant computational cost by training agents on a highly lossy representation of the environment state. This paper questions this common design choice by exploring an efficient and modular approach to RL for cloth manipulation. We show that, through careful design choices, model size and training time can be significantly reduced when learning in simulation. Furthermore, we demonstrate how the resulting simulation-trained model can be transferred to the real world. We evaluate our approach on the SoftGym benchmark and achieve significant performance improvements over available baselines on our task, while using a substantially smaller model."
    },
    {
      "title": "Heterogeneous Vertiport Selection Optimization for On-Demand Air Taxi Services: A Deep Reinforcement Learning Approach",
      "url": "https://arxiv.org/abs/2601.21316",
      "date": "2026-01-29",
      "authors_text": "Aoyu Pang, Maonan Wang, Zifan Sha, Wenwei Yue, Changle Li",
      "is_highlight": false,
      "score": 80.0,
      "summary": "The paper presents a deep reinforcement learning framework for optimizing vertiport selection and air taxi routing, achieving significant travel time reductions in urban air mobility systems.",
      "teaser_image": "https://arxiv.org/html/2601.21316/x2.png",
      "debug_abstract": "Urban Air Mobility (UAM) has emerged as a transformative solution to alleviate urban congestion by utilizing low-altitude airspace, thereby reducing pressure on ground transportation networks. To enable truly efficient and seamless door-to-door travel experiences, UAM requires close integration with existing ground transportation infrastructure. However, current research on optimal integrated routing strategies for passengers in air-ground mobility systems remains limited, with a lack of systematic this http URL address this gap, we first propose a unified optimization model that integrates strategy selection for both air and ground transportation. This model captures the dynamic characteristics of multimodal transport networks and incorporates real-time traffic conditions alongside passenger decision-making behavior. Building on this model, we propose a Unified Air-Ground Mobility Coordination (UAGMC) framework, which leverages deep reinforcement learning (RL) and Vehicle-to-Everything (V2X) communication to optimize vertiport selection and dynamically plan air taxi routes. Experimental results demonstrate that UAGMC achieves a 34\\% reduction in average travel time compared to conventional proportional allocation methods, enhancing overall travel efficiency and providing novel insights into the integration and optimization of multimodal transportation systems. This work lays a solid foundation for advancing intelligent urban mobility solutions through the coordination of air and ground transportation modes. The related code can be found at this https URL."
    }
  ],
  "深圳市人工智能与机器人研究院 (AIRS)": [
    {
      "title": "PLANING: A Loosely Coupled Triangle-Gaussian Framework for Streaming 3D Reconstruction",
      "url": "https://arxiv.org/abs/2601.22046",
      "date": "2026-01-29",
      "authors_text": "Changjian Jiang, Kerui Ren, Xudong Li, Kaiwen Song, Linning Xu",
      "is_highlight": false,
      "score": 66.0,
      "summary": "PLANING is a novel streaming 3D reconstruction framework that efficiently combines geometric primitives and neural Gaussians, enhancing quality and speed while reducing redundancy.",
      "teaser_image": "https://arxiv.org/html/2601.22046/x3.png",
      "debug_abstract": "Streaming reconstruction from monocular image sequences remains challenging, as existing methods typically favor either high-quality rendering or accurate geometry, but rarely both. We present PLANING, an efficient on-the-fly reconstruction framework built on a hybrid representation that loosely couples explicit geometric primitives with neural Gaussians, enabling geometry and appearance to be modeled in a decoupled manner. This decoupling supports an online initialization and optimization strategy that separates geometry and appearance updates, yielding stable streaming reconstruction with substantially reduced structural redundancy. PLANING improves dense mesh Chamfer-L2 by 18.52% over PGSR, surpasses ARTDECO by 1.31 dB PSNR, and reconstructs ScanNetV2 scenes in under 100 seconds, over 5x faster than 2D Gaussian Splatting, while matching the quality of offline per-scene optimization. Beyond reconstruction quality, the structural clarity and computational efficiency of \\modelname~make it well suited for a broad range of downstream applications, such as enabling large-scale scene modeling and simulation-ready environments for embodied AI. Project page: this https URL ."
    },
    {
      "title": "Disentangling perception and reasoning for improving data efficiency in learning cloth manipulation without demonstrations",
      "url": "https://arxiv.org/abs/2601.21713",
      "date": "2026-01-29",
      "authors_text": "Donatien Delehelle, Fei Chen, Darwin Caldwell",
      "is_highlight": false,
      "score": 87.0,
      "summary": "This paper presents a modular reinforcement learning approach for cloth manipulation that enhances data efficiency and reduces model size and training time, achieving improved performance in simulation and real-world transfer.",
      "teaser_image": "https://arxiv.org/html/2601.21713/x1.png",
      "debug_abstract": "Cloth manipulation is a ubiquitous task in everyday life, but it remains an open challenge for robotics. The difficulties in developing cloth manipulation policies are attributed to the high-dimensional state space, complex dynamics, and high propensity to self-occlusion exhibited by fabrics. As analytical methods have not been able to provide robust and general manipulation policies, reinforcement learning (RL) is considered a promising approach to these problems. However, to address the large state space and complex dynamics, data-based methods usually rely on large models and long training times. The resulting computational cost significantly hampers the development and adoption of these methods. Additionally, due to the challenge of robust state estimation, garment manipulation policies often adopt an end-to-end learning approach with workspace images as input. While this approach enables a conceptually straightforward sim-to-real transfer via real-world fine-tuning, it also incurs a significant computational cost by training agents on a highly lossy representation of the environment state. This paper questions this common design choice by exploring an efficient and modular approach to RL for cloth manipulation. We show that, through careful design choices, model size and training time can be significantly reduced when learning in simulation. Furthermore, we demonstrate how the resulting simulation-trained model can be transferred to the real world. We evaluate our approach on the SoftGym benchmark and achieve significant performance improvements over available baselines on our task, while using a substantially smaller model."
    },
    {
      "title": "Heterogeneous Vertiport Selection Optimization for On-Demand Air Taxi Services: A Deep Reinforcement Learning Approach",
      "url": "https://arxiv.org/abs/2601.21316",
      "date": "2026-01-29",
      "authors_text": "Aoyu Pang, Maonan Wang, Zifan Sha, Wenwei Yue, Changle Li",
      "is_highlight": false,
      "score": 80.0,
      "summary": "The paper presents a deep reinforcement learning framework for optimizing vertiport selection and air taxi routing, achieving significant travel time reductions in urban air mobility systems.",
      "teaser_image": "https://arxiv.org/html/2601.21316/x2.png",
      "debug_abstract": "Urban Air Mobility (UAM) has emerged as a transformative solution to alleviate urban congestion by utilizing low-altitude airspace, thereby reducing pressure on ground transportation networks. To enable truly efficient and seamless door-to-door travel experiences, UAM requires close integration with existing ground transportation infrastructure. However, current research on optimal integrated routing strategies for passengers in air-ground mobility systems remains limited, with a lack of systematic this http URL address this gap, we first propose a unified optimization model that integrates strategy selection for both air and ground transportation. This model captures the dynamic characteristics of multimodal transport networks and incorporates real-time traffic conditions alongside passenger decision-making behavior. Building on this model, we propose a Unified Air-Ground Mobility Coordination (UAGMC) framework, which leverages deep reinforcement learning (RL) and Vehicle-to-Everything (V2X) communication to optimize vertiport selection and dynamically plan air taxi routes. Experimental results demonstrate that UAGMC achieves a 34\\% reduction in average travel time compared to conventional proportional allocation methods, enhancing overall travel efficiency and providing novel insights into the integration and optimization of multimodal transportation systems. This work lays a solid foundation for advancing intelligent urban mobility solutions through the coordination of air and ground transportation modes. The related code can be found at this https URL."
    }
  ],
  "BIGAI": [
    {
      "title": "Towards Bridging the Gap between Large-Scale Pretraining and Efficient Finetuning for Humanoid Control",
      "url": "https://arxiv.org/abs/2601.21363",
      "date": "2026-01-29",
      "authors_text": "Weidong Huang, Zhehan Li, Hangxin Liu, Biao Hou, Yao Su",
      "is_highlight": false,
      "score": 85.0,
      "summary": "This paper presents a method combining off-policy Soft Actor-Critic for efficient pretraining and model-based techniques for safe finetuning of humanoid locomotion policies.",
      "teaser_image": "https://arxiv.org/html/2601.21363/x1.png",
      "debug_abstract": "Reinforcement learning (RL) is widely used for humanoid control, with on-policy methods such as Proximal Policy Optimization (PPO) enabling robust training via large-scale parallel simulation and, in some cases, zero-shot deployment to real robots. However, the low sample efficiency of on-policy algorithms limits safe adaptation to new environments. Although off-policy RL and model-based RL have shown improved sample efficiency, the gap between large-scale pretraining and efficient finetuning on humanoids still exists. In this paper, we find that off-policy Soft Actor-Critic (SAC), with large-batch update and a high Update-To-Data (UTD) ratio, reliably supports large-scale pretraining of humanoid locomotion policies, achieving zero-shot deployment on real robots. For adaptation, we demonstrate that these SAC-pretrained policies can be finetuned in new environments and out-of-distribution tasks using model-based methods. Data collection in the new environment executes a deterministic policy while stochastic exploration is instead confined to a physics-informed world model. This separation mitigates the risks of random exploration during adaptation while preserving exploratory coverage for improvement. Overall, the approach couples the wall-clock efficiency of large-scale simulation during pretraining with the sample efficiency of model-based learning during fine-tuning."
    }
  ],
  "武汉大学": [
    {
      "title": "4D-CAAL: 4D Radar-Camera Calibration and Auto-Labeling for Autonomous Driving",
      "url": "https://arxiv.org/abs/2601.21454",
      "date": "2026-01-29",
      "authors_text": "Shanliang Yao, Zhuoxiao Li, Runwei Guan, Kebin Cao, Meng Xia",
      "is_highlight": false,
      "score": 89.0,
      "summary": "The 4D-CAAL framework integrates 4D radar-camera calibration and auto-labeling using a dual-purpose target and robust correspondence matching, enhancing autonomous driving perception efficiency.",
      "teaser_image": "https://arxiv.org/html/2601.21454/images/4DRC-fixed.png",
      "debug_abstract": "4D radar has emerged as a critical sensor for autonomous driving, primarily due to its enhanced capabilities in elevation measurement and higher resolution compared to traditional 3D radar. Effective integration of 4D radar with cameras requires accurate extrinsic calibration, and the development of radar-based perception algorithms demands large-scale annotated datasets. However, existing calibration methods often employ separate targets optimized for either visual or radar modalities, complicating correspondence establishment. Furthermore, manually labeling sparse radar data is labor-intensive and unreliable. To address these challenges, we propose 4D-CAAL, a unified framework for 4D radar-camera calibration and auto-labeling. Our approach introduces a novel dual-purpose calibration target design, integrating a checkerboard pattern on the front surface for camera detection and a corner reflector at the center of the back surface for radar detection. We develop a robust correspondence matching algorithm that aligns the checkerboard center with the strongest radar reflection point, enabling accurate extrinsic calibration. Subsequently, we present an auto-labeling pipeline that leverages the calibrated sensor relationship to transfer annotations from camera-based segmentations to radar point clouds through geometric projection and multi-feature optimization. Extensive experiments demonstrate that our method achieves high calibration accuracy while significantly reducing manual annotation effort, thereby accelerating the development of robust multi-modal perception systems for autonomous driving."
    }
  ],
  "Sensing, Interaction & Perception Lab (SIPLAB)": [
    {
      "title": "Training slow silicon neurons to control extremely fast robots with spiking reinforcement learning",
      "url": "https://arxiv.org/abs/2601.21548",
      "date": "2026-01-29",
      "authors_text": "Irene Ambrosini, Ingo Blakowski, Dmitrii Zendrikov, Cristiano Capone, Luna Gava",
      "is_highlight": false,
      "score": 91.0,
      "summary": "This paper presents a neuromorphic processor-based spiking neural network that achieves real-time learning for controlling fast robots in air hockey through efficient reinforcement learning.",
      "teaser_image": "https://arxiv.org/html/2601.21548/img/figure1.png",
      "debug_abstract": "Air hockey demands split-second decisions at high puck velocities, a challenge we address with a compact network of spiking neurons running on a mixed-signal analog/digital neuromorphic processor. By co-designing hardware and learning algorithms, we train the system to achieve successful puck interactions through reinforcement learning in a remarkably small number of trials. The network leverages fixed random connectivity to capture the task&#39;s temporal structure and adopts a local e-prop learning rule in the readout layer to exploit event-driven activity for fast and efficient learning. The result is real-time learning with a setup comprising a computer and the neuromorphic chip in-the-loop, enabling practical training of spiking neural networks for robotic autonomous systems. This work bridges neuroscience-inspired hardware with real-world robotic control, showing that brain-inspired approaches can tackle fast-paced interaction tasks while supporting always-on learning in intelligent machines."
    }
  ],
  "机器人系统实验室 (RSL)": [
    {
      "title": "Training slow silicon neurons to control extremely fast robots with spiking reinforcement learning",
      "url": "https://arxiv.org/abs/2601.21548",
      "date": "2026-01-29",
      "authors_text": "Irene Ambrosini, Ingo Blakowski, Dmitrii Zendrikov, Cristiano Capone, Luna Gava",
      "is_highlight": false,
      "score": 91.0,
      "summary": "This paper presents a neuromorphic processor-based spiking neural network that achieves real-time learning for controlling fast robots in air hockey through efficient reinforcement learning.",
      "teaser_image": "https://arxiv.org/html/2601.21548/img/figure1.png",
      "debug_abstract": "Air hockey demands split-second decisions at high puck velocities, a challenge we address with a compact network of spiking neurons running on a mixed-signal analog/digital neuromorphic processor. By co-designing hardware and learning algorithms, we train the system to achieve successful puck interactions through reinforcement learning in a remarkably small number of trials. The network leverages fixed random connectivity to capture the task&#39;s temporal structure and adopts a local e-prop learning rule in the readout layer to exploit event-driven activity for fast and efficient learning. The result is real-time learning with a setup comprising a computer and the neuromorphic chip in-the-loop, enabling practical training of spiking neural networks for robotic autonomous systems. This work bridges neuroscience-inspired hardware with real-world robotic control, showing that brain-inspired approaches can tackle fast-paced interaction tasks while supporting always-on learning in intelligent machines."
    }
  ],
  "南京大学": [
    {
      "title": "TraceRouter: Robust Safety for Large Foundation Models via Path-Level Intervention",
      "url": "https://arxiv.org/abs/2601.21900",
      "date": "2026-01-29",
      "authors_text": "Chuancheng Shi, Shangze Li, Wenjun Lu, Wenhua Wu, Cong Wang",
      "is_highlight": false,
      "score": 73.0,
      "summary": "TraceRouter enhances the safety of large foundation models by disconnecting harmful semantic pathways through a novel path-level intervention framework, outperforming existing defenses.",
      "teaser_image": "https://arxiv.org/html/2601.21900/x8.png",
      "debug_abstract": "Despite their capabilities, large foundation models (LFMs) remain susceptible to adversarial manipulation. Current defenses predominantly rely on the &#34;locality hypothesis&#34;, suppressing isolated neurons or features. However, harmful semantics act as distributed, cross-layer circuits, rendering such localized interventions brittle and detrimental to utility. To bridge this gap, we propose \\textbf{TraceRouter}, a path-level framework that traces and disconnects the causal propagation circuits of illicit semantics. TraceRouter operates in three stages: (1) it pinpoints a sensitive onset layer by analyzing attention divergence; (2) it leverages sparse autoencoders (SAEs) and differential activation analysis to disentangle and isolate malicious features; and (3) it maps these features to downstream causal pathways via feature influence scores (FIS) derived from zero-out interventions. By selectively suppressing these causal chains, TraceRouter physically severs the flow of harmful information while leaving orthogonal computation routes intact. Extensive experiments demonstrate that TraceRouter significantly outperforms state-of-the-art baselines, achieving a superior trade-off between adversarial robustness and general utility. Our code will be publicly released. WARNING: This paper contains unsafe model responses."
    }
  ],
  "浙江大学": [
    {
      "title": "PLANING: A Loosely Coupled Triangle-Gaussian Framework for Streaming 3D Reconstruction",
      "url": "https://arxiv.org/abs/2601.22046",
      "date": "2026-01-29",
      "authors_text": "Changjian Jiang, Kerui Ren, Xudong Li, Kaiwen Song, Linning Xu",
      "is_highlight": false,
      "score": 66.0,
      "summary": "PLANING is a novel streaming 3D reconstruction framework that efficiently combines geometric primitives and neural Gaussians, enhancing quality and speed while reducing redundancy.",
      "teaser_image": "https://arxiv.org/html/2601.22046/x3.png",
      "debug_abstract": "Streaming reconstruction from monocular image sequences remains challenging, as existing methods typically favor either high-quality rendering or accurate geometry, but rarely both. We present PLANING, an efficient on-the-fly reconstruction framework built on a hybrid representation that loosely couples explicit geometric primitives with neural Gaussians, enabling geometry and appearance to be modeled in a decoupled manner. This decoupling supports an online initialization and optimization strategy that separates geometry and appearance updates, yielding stable streaming reconstruction with substantially reduced structural redundancy. PLANING improves dense mesh Chamfer-L2 by 18.52% over PGSR, surpasses ARTDECO by 1.31 dB PSNR, and reconstructs ScanNetV2 scenes in under 100 seconds, over 5x faster than 2D Gaussian Splatting, while matching the quality of offline per-scene optimization. Beyond reconstruction quality, the structural clarity and computational efficiency of \\modelname~make it well suited for a broad range of downstream applications, such as enabling large-scale scene modeling and simulation-ready environments for embodied AI. Project page: this https URL ."
    }
  ],
  "赵波老师实验室": [
    {
      "title": "E-mem: Multi-agent based Episodic Context Reconstruction for LLM Agent Memory",
      "url": "https://arxiv.org/abs/2601.21714",
      "date": "2026-01-29",
      "authors_text": "Kaixiang Wang, Yidan Lin, Jiong Lou, Zhaojiacheng Zhou, Bunyod Suvonov",
      "is_highlight": false,
      "score": 71.0,
      "summary": "E-mem introduces a multi-agent framework for episodic context reconstruction in LLMs, enhancing reasoning accuracy and efficiency while preserving contextual integrity.",
      "teaser_image": "https://arxiv.org/html/2601.21714/x2.png",
      "debug_abstract": "The evolution of Large Language Model (LLM) agents towards System~2 reasoning, characterized by deliberative, high-precision problem-solving, requires maintaining rigorous logical integrity over extended horizons. However, prevalent memory preprocessing paradigms suffer from destructive de-contextualization. By compressing complex sequential dependencies into pre-defined structures (e.g., embeddings or graphs), these methods sever the contextual integrity essential for deep reasoning. To address this, we propose E-mem, a framework shifting from Memory Preprocessing to Episodic Context Reconstruction. Inspired by biological engrams, E-mem employs a heterogeneous hierarchical architecture where multiple assistant agents maintain uncompressed memory contexts, while a central master agent orchestrates global planning. Unlike passive retrieval, our mechanism empowers assistants to locally reason within activated segments, extracting context-aware evidence before aggregation. Evaluations on the LoCoMo benchmark demonstrate that E-mem achieves over 54\\% F1, surpassing the state-of-the-art GAM by 7.75\\%, while reducing token cost by over 70\\%."
    },
    {
      "title": "DynaWeb: Model-Based Reinforcement Learning of Web Agents",
      "url": "https://arxiv.org/abs/2601.22149",
      "date": "2026-01-29",
      "authors_text": "Hang Ding, Peidong Liu, Junqiao Wang, Ziwei Ji, Meng Cao",
      "is_highlight": false,
      "score": 77.0,
      "summary": "DynaWeb introduces a model-based reinforcement learning framework that enhances web agent training by simulating interactions with a web world model, improving efficiency and performance.",
      "teaser_image": "https://arxiv.org/html/2601.22149/figures/dynaweb.png",
      "debug_abstract": "The development of autonomous web agents, powered by Large Language Models (LLMs) and reinforcement learning (RL), represents a significant step towards general-purpose AI assistants. However, training these agents is severely hampered by the challenges of interacting with the live internet, which is inefficient, costly, and fraught with risks. Model-based reinforcement learning (MBRL) offers a promising solution by learning a world model of the environment to enable simulated interaction. This paper introduces DynaWeb, a novel MBRL framework that trains web agents through interacting with a web world model trained to predict naturalistic web page representations given agent actions. This model serves as a synthetic web environment where an agent policy can dream by generating vast quantities of rollout action trajectories for efficient online reinforcement learning. Beyond free policy rollouts, DynaWeb incorporates real expert trajectories from training data, which are randomly interleaved with on-policy rollouts during training to improve stability and sample efficiency. Experiments conducted on the challenging WebArena and WebVoyager benchmarks demonstrate that DynaWeb consistently and significantly improves the performance of state-of-the-art open-source web agent models. Our findings establish the viability of training web agents through imagination, offering a scalable and efficient way to scale up online agentic RL."
    },
    {
      "title": "PLANING: A Loosely Coupled Triangle-Gaussian Framework for Streaming 3D Reconstruction",
      "url": "https://arxiv.org/abs/2601.22046",
      "date": "2026-01-29",
      "authors_text": "Changjian Jiang, Kerui Ren, Xudong Li, Kaiwen Song, Linning Xu",
      "is_highlight": false,
      "score": 66.0,
      "summary": "PLANING is a novel streaming 3D reconstruction framework that efficiently combines geometric primitives and neural Gaussians, enhancing quality and speed while reducing redundancy.",
      "teaser_image": "https://arxiv.org/html/2601.22046/x3.png",
      "debug_abstract": "Streaming reconstruction from monocular image sequences remains challenging, as existing methods typically favor either high-quality rendering or accurate geometry, but rarely both. We present PLANING, an efficient on-the-fly reconstruction framework built on a hybrid representation that loosely couples explicit geometric primitives with neural Gaussians, enabling geometry and appearance to be modeled in a decoupled manner. This decoupling supports an online initialization and optimization strategy that separates geometry and appearance updates, yielding stable streaming reconstruction with substantially reduced structural redundancy. PLANING improves dense mesh Chamfer-L2 by 18.52% over PGSR, surpasses ARTDECO by 1.31 dB PSNR, and reconstructs ScanNetV2 scenes in under 100 seconds, over 5x faster than 2D Gaussian Splatting, while matching the quality of offline per-scene optimization. Beyond reconstruction quality, the structural clarity and computational efficiency of \\modelname~make it well suited for a broad range of downstream applications, such as enabling large-scale scene modeling and simulation-ready environments for embodied AI. Project page: this https URL ."
    }
  ],
  "机器视觉与智能学习实验室 (MVIG)": [
    {
      "title": "E-mem: Multi-agent based Episodic Context Reconstruction for LLM Agent Memory",
      "url": "https://arxiv.org/abs/2601.21714",
      "date": "2026-01-29",
      "authors_text": "Kaixiang Wang, Yidan Lin, Jiong Lou, Zhaojiacheng Zhou, Bunyod Suvonov",
      "is_highlight": false,
      "score": 71.0,
      "summary": "E-mem introduces a multi-agent framework for episodic context reconstruction in LLMs, enhancing reasoning accuracy and efficiency while preserving contextual integrity.",
      "teaser_image": "https://arxiv.org/html/2601.21714/x2.png",
      "debug_abstract": "The evolution of Large Language Model (LLM) agents towards System~2 reasoning, characterized by deliberative, high-precision problem-solving, requires maintaining rigorous logical integrity over extended horizons. However, prevalent memory preprocessing paradigms suffer from destructive de-contextualization. By compressing complex sequential dependencies into pre-defined structures (e.g., embeddings or graphs), these methods sever the contextual integrity essential for deep reasoning. To address this, we propose E-mem, a framework shifting from Memory Preprocessing to Episodic Context Reconstruction. Inspired by biological engrams, E-mem employs a heterogeneous hierarchical architecture where multiple assistant agents maintain uncompressed memory contexts, while a central master agent orchestrates global planning. Unlike passive retrieval, our mechanism empowers assistants to locally reason within activated segments, extracting context-aware evidence before aggregation. Evaluations on the LoCoMo benchmark demonstrate that E-mem achieves over 54\\% F1, surpassing the state-of-the-art GAM by 7.75\\%, while reducing token cost by over 70\\%."
    },
    {
      "title": "DynaWeb: Model-Based Reinforcement Learning of Web Agents",
      "url": "https://arxiv.org/abs/2601.22149",
      "date": "2026-01-29",
      "authors_text": "Hang Ding, Peidong Liu, Junqiao Wang, Ziwei Ji, Meng Cao",
      "is_highlight": false,
      "score": 77.0,
      "summary": "DynaWeb introduces a model-based reinforcement learning framework that enhances web agent training by simulating interactions with a web world model, improving efficiency and performance.",
      "teaser_image": "https://arxiv.org/html/2601.22149/figures/dynaweb.png",
      "debug_abstract": "The development of autonomous web agents, powered by Large Language Models (LLMs) and reinforcement learning (RL), represents a significant step towards general-purpose AI assistants. However, training these agents is severely hampered by the challenges of interacting with the live internet, which is inefficient, costly, and fraught with risks. Model-based reinforcement learning (MBRL) offers a promising solution by learning a world model of the environment to enable simulated interaction. This paper introduces DynaWeb, a novel MBRL framework that trains web agents through interacting with a web world model trained to predict naturalistic web page representations given agent actions. This model serves as a synthetic web environment where an agent policy can dream by generating vast quantities of rollout action trajectories for efficient online reinforcement learning. Beyond free policy rollouts, DynaWeb incorporates real expert trajectories from training data, which are randomly interleaved with on-policy rollouts during training to improve stability and sample efficiency. Experiments conducted on the challenging WebArena and WebVoyager benchmarks demonstrate that DynaWeb consistently and significantly improves the performance of state-of-the-art open-source web agent models. Our findings establish the viability of training web agents through imagination, offering a scalable and efficient way to scale up online agentic RL."
    },
    {
      "title": "PLANING: A Loosely Coupled Triangle-Gaussian Framework for Streaming 3D Reconstruction",
      "url": "https://arxiv.org/abs/2601.22046",
      "date": "2026-01-29",
      "authors_text": "Changjian Jiang, Kerui Ren, Xudong Li, Kaiwen Song, Linning Xu",
      "is_highlight": false,
      "score": 66.0,
      "summary": "PLANING is a novel streaming 3D reconstruction framework that efficiently combines geometric primitives and neural Gaussians, enhancing quality and speed while reducing redundancy.",
      "teaser_image": "https://arxiv.org/html/2601.22046/x3.png",
      "debug_abstract": "Streaming reconstruction from monocular image sequences remains challenging, as existing methods typically favor either high-quality rendering or accurate geometry, but rarely both. We present PLANING, an efficient on-the-fly reconstruction framework built on a hybrid representation that loosely couples explicit geometric primitives with neural Gaussians, enabling geometry and appearance to be modeled in a decoupled manner. This decoupling supports an online initialization and optimization strategy that separates geometry and appearance updates, yielding stable streaming reconstruction with substantially reduced structural redundancy. PLANING improves dense mesh Chamfer-L2 by 18.52% over PGSR, surpasses ARTDECO by 1.31 dB PSNR, and reconstructs ScanNetV2 scenes in under 100 seconds, over 5x faster than 2D Gaussian Splatting, while matching the quality of offline per-scene optimization. Beyond reconstruction quality, the structural clarity and computational efficiency of \\modelname~make it well suited for a broad range of downstream applications, such as enabling large-scale scene modeling and simulation-ready environments for embodied AI. Project page: this https URL ."
    }
  ],
  "ReThinkLab": [
    {
      "title": "E-mem: Multi-agent based Episodic Context Reconstruction for LLM Agent Memory",
      "url": "https://arxiv.org/abs/2601.21714",
      "date": "2026-01-29",
      "authors_text": "Kaixiang Wang, Yidan Lin, Jiong Lou, Zhaojiacheng Zhou, Bunyod Suvonov",
      "is_highlight": false,
      "score": 71.0,
      "summary": "E-mem introduces a multi-agent framework for episodic context reconstruction in LLMs, enhancing reasoning accuracy and efficiency while preserving contextual integrity.",
      "teaser_image": "https://arxiv.org/html/2601.21714/x2.png",
      "debug_abstract": "The evolution of Large Language Model (LLM) agents towards System~2 reasoning, characterized by deliberative, high-precision problem-solving, requires maintaining rigorous logical integrity over extended horizons. However, prevalent memory preprocessing paradigms suffer from destructive de-contextualization. By compressing complex sequential dependencies into pre-defined structures (e.g., embeddings or graphs), these methods sever the contextual integrity essential for deep reasoning. To address this, we propose E-mem, a framework shifting from Memory Preprocessing to Episodic Context Reconstruction. Inspired by biological engrams, E-mem employs a heterogeneous hierarchical architecture where multiple assistant agents maintain uncompressed memory contexts, while a central master agent orchestrates global planning. Unlike passive retrieval, our mechanism empowers assistants to locally reason within activated segments, extracting context-aware evidence before aggregation. Evaluations on the LoCoMo benchmark demonstrate that E-mem achieves over 54\\% F1, surpassing the state-of-the-art GAM by 7.75\\%, while reducing token cost by over 70\\%."
    },
    {
      "title": "DynaWeb: Model-Based Reinforcement Learning of Web Agents",
      "url": "https://arxiv.org/abs/2601.22149",
      "date": "2026-01-29",
      "authors_text": "Hang Ding, Peidong Liu, Junqiao Wang, Ziwei Ji, Meng Cao",
      "is_highlight": false,
      "score": 77.0,
      "summary": "DynaWeb introduces a model-based reinforcement learning framework that enhances web agent training by simulating interactions with a web world model, improving efficiency and performance.",
      "teaser_image": "https://arxiv.org/html/2601.22149/figures/dynaweb.png",
      "debug_abstract": "The development of autonomous web agents, powered by Large Language Models (LLMs) and reinforcement learning (RL), represents a significant step towards general-purpose AI assistants. However, training these agents is severely hampered by the challenges of interacting with the live internet, which is inefficient, costly, and fraught with risks. Model-based reinforcement learning (MBRL) offers a promising solution by learning a world model of the environment to enable simulated interaction. This paper introduces DynaWeb, a novel MBRL framework that trains web agents through interacting with a web world model trained to predict naturalistic web page representations given agent actions. This model serves as a synthetic web environment where an agent policy can dream by generating vast quantities of rollout action trajectories for efficient online reinforcement learning. Beyond free policy rollouts, DynaWeb incorporates real expert trajectories from training data, which are randomly interleaved with on-policy rollouts during training to improve stability and sample efficiency. Experiments conducted on the challenging WebArena and WebVoyager benchmarks demonstrate that DynaWeb consistently and significantly improves the performance of state-of-the-art open-source web agent models. Our findings establish the viability of training web agents through imagination, offering a scalable and efficient way to scale up online agentic RL."
    },
    {
      "title": "PLANING: A Loosely Coupled Triangle-Gaussian Framework for Streaming 3D Reconstruction",
      "url": "https://arxiv.org/abs/2601.22046",
      "date": "2026-01-29",
      "authors_text": "Changjian Jiang, Kerui Ren, Xudong Li, Kaiwen Song, Linning Xu",
      "is_highlight": false,
      "score": 66.0,
      "summary": "PLANING is a novel streaming 3D reconstruction framework that efficiently combines geometric primitives and neural Gaussians, enhancing quality and speed while reducing redundancy.",
      "teaser_image": "https://arxiv.org/html/2601.22046/x3.png",
      "debug_abstract": "Streaming reconstruction from monocular image sequences remains challenging, as existing methods typically favor either high-quality rendering or accurate geometry, but rarely both. We present PLANING, an efficient on-the-fly reconstruction framework built on a hybrid representation that loosely couples explicit geometric primitives with neural Gaussians, enabling geometry and appearance to be modeled in a decoupled manner. This decoupling supports an online initialization and optimization strategy that separates geometry and appearance updates, yielding stable streaming reconstruction with substantially reduced structural redundancy. PLANING improves dense mesh Chamfer-L2 by 18.52% over PGSR, surpasses ARTDECO by 1.31 dB PSNR, and reconstructs ScanNetV2 scenes in under 100 seconds, over 5x faster than 2D Gaussian Splatting, while matching the quality of offline per-scene optimization. Beyond reconstruction quality, the structural clarity and computational efficiency of \\modelname~make it well suited for a broad range of downstream applications, such as enabling large-scale scene modeling and simulation-ready environments for embodied AI. Project page: this https URL ."
    }
  ],
  "中国科学技术大学": [
    {
      "title": "PLANING: A Loosely Coupled Triangle-Gaussian Framework for Streaming 3D Reconstruction",
      "url": "https://arxiv.org/abs/2601.22046",
      "date": "2026-01-29",
      "authors_text": "Changjian Jiang, Kerui Ren, Xudong Li, Kaiwen Song, Linning Xu",
      "is_highlight": false,
      "score": 66.0,
      "summary": "PLANING is a novel streaming 3D reconstruction framework that efficiently combines geometric primitives and neural Gaussians, enhancing quality and speed while reducing redundancy.",
      "teaser_image": "https://arxiv.org/html/2601.22046/x3.png",
      "debug_abstract": "Streaming reconstruction from monocular image sequences remains challenging, as existing methods typically favor either high-quality rendering or accurate geometry, but rarely both. We present PLANING, an efficient on-the-fly reconstruction framework built on a hybrid representation that loosely couples explicit geometric primitives with neural Gaussians, enabling geometry and appearance to be modeled in a decoupled manner. This decoupling supports an online initialization and optimization strategy that separates geometry and appearance updates, yielding stable streaming reconstruction with substantially reduced structural redundancy. PLANING improves dense mesh Chamfer-L2 by 18.52% over PGSR, surpasses ARTDECO by 1.31 dB PSNR, and reconstructs ScanNetV2 scenes in under 100 seconds, over 5x faster than 2D Gaussian Splatting, while matching the quality of offline per-scene optimization. Beyond reconstruction quality, the structural clarity and computational efficiency of \\modelname~make it well suited for a broad range of downstream applications, such as enabling large-scale scene modeling and simulation-ready environments for embodied AI. Project page: this https URL ."
    }
  ],
  "智能机器人与机器视觉(IRMV)实验室": [
    {
      "title": "E-mem: Multi-agent based Episodic Context Reconstruction for LLM Agent Memory",
      "url": "https://arxiv.org/abs/2601.21714",
      "date": "2026-01-29",
      "authors_text": "Kaixiang Wang, Yidan Lin, Jiong Lou, Zhaojiacheng Zhou, Bunyod Suvonov",
      "is_highlight": false,
      "score": 71.0,
      "summary": "E-mem introduces a multi-agent framework for episodic context reconstruction in LLMs, enhancing reasoning accuracy and efficiency while preserving contextual integrity.",
      "teaser_image": "https://arxiv.org/html/2601.21714/x2.png",
      "debug_abstract": "The evolution of Large Language Model (LLM) agents towards System~2 reasoning, characterized by deliberative, high-precision problem-solving, requires maintaining rigorous logical integrity over extended horizons. However, prevalent memory preprocessing paradigms suffer from destructive de-contextualization. By compressing complex sequential dependencies into pre-defined structures (e.g., embeddings or graphs), these methods sever the contextual integrity essential for deep reasoning. To address this, we propose E-mem, a framework shifting from Memory Preprocessing to Episodic Context Reconstruction. Inspired by biological engrams, E-mem employs a heterogeneous hierarchical architecture where multiple assistant agents maintain uncompressed memory contexts, while a central master agent orchestrates global planning. Unlike passive retrieval, our mechanism empowers assistants to locally reason within activated segments, extracting context-aware evidence before aggregation. Evaluations on the LoCoMo benchmark demonstrate that E-mem achieves over 54\\% F1, surpassing the state-of-the-art GAM by 7.75\\%, while reducing token cost by over 70\\%."
    },
    {
      "title": "DynaWeb: Model-Based Reinforcement Learning of Web Agents",
      "url": "https://arxiv.org/abs/2601.22149",
      "date": "2026-01-29",
      "authors_text": "Hang Ding, Peidong Liu, Junqiao Wang, Ziwei Ji, Meng Cao",
      "is_highlight": false,
      "score": 77.0,
      "summary": "DynaWeb introduces a model-based reinforcement learning framework that enhances web agent training by simulating interactions with a web world model, improving efficiency and performance.",
      "teaser_image": "https://arxiv.org/html/2601.22149/figures/dynaweb.png",
      "debug_abstract": "The development of autonomous web agents, powered by Large Language Models (LLMs) and reinforcement learning (RL), represents a significant step towards general-purpose AI assistants. However, training these agents is severely hampered by the challenges of interacting with the live internet, which is inefficient, costly, and fraught with risks. Model-based reinforcement learning (MBRL) offers a promising solution by learning a world model of the environment to enable simulated interaction. This paper introduces DynaWeb, a novel MBRL framework that trains web agents through interacting with a web world model trained to predict naturalistic web page representations given agent actions. This model serves as a synthetic web environment where an agent policy can dream by generating vast quantities of rollout action trajectories for efficient online reinforcement learning. Beyond free policy rollouts, DynaWeb incorporates real expert trajectories from training data, which are randomly interleaved with on-policy rollouts during training to improve stability and sample efficiency. Experiments conducted on the challenging WebArena and WebVoyager benchmarks demonstrate that DynaWeb consistently and significantly improves the performance of state-of-the-art open-source web agent models. Our findings establish the viability of training web agents through imagination, offering a scalable and efficient way to scale up online agentic RL."
    },
    {
      "title": "PLANING: A Loosely Coupled Triangle-Gaussian Framework for Streaming 3D Reconstruction",
      "url": "https://arxiv.org/abs/2601.22046",
      "date": "2026-01-29",
      "authors_text": "Changjian Jiang, Kerui Ren, Xudong Li, Kaiwen Song, Linning Xu",
      "is_highlight": false,
      "score": 66.0,
      "summary": "PLANING is a novel streaming 3D reconstruction framework that efficiently combines geometric primitives and neural Gaussians, enhancing quality and speed while reducing redundancy.",
      "teaser_image": "https://arxiv.org/html/2601.22046/x3.png",
      "debug_abstract": "Streaming reconstruction from monocular image sequences remains challenging, as existing methods typically favor either high-quality rendering or accurate geometry, but rarely both. We present PLANING, an efficient on-the-fly reconstruction framework built on a hybrid representation that loosely couples explicit geometric primitives with neural Gaussians, enabling geometry and appearance to be modeled in a decoupled manner. This decoupling supports an online initialization and optimization strategy that separates geometry and appearance updates, yielding stable streaming reconstruction with substantially reduced structural redundancy. PLANING improves dense mesh Chamfer-L2 by 18.52% over PGSR, surpasses ARTDECO by 1.31 dB PSNR, and reconstructs ScanNetV2 scenes in under 100 seconds, over 5x faster than 2D Gaussian Splatting, while matching the quality of offline per-scene optimization. Beyond reconstruction quality, the structural clarity and computational efficiency of \\modelname~make it well suited for a broad range of downstream applications, such as enabling large-scale scene modeling and simulation-ready environments for embodied AI. Project page: this https URL ."
    }
  ],
  "IWIN-FINS实验室": [
    {
      "title": "E-mem: Multi-agent based Episodic Context Reconstruction for LLM Agent Memory",
      "url": "https://arxiv.org/abs/2601.21714",
      "date": "2026-01-29",
      "authors_text": "Kaixiang Wang, Yidan Lin, Jiong Lou, Zhaojiacheng Zhou, Bunyod Suvonov",
      "is_highlight": false,
      "score": 71.0,
      "summary": "E-mem introduces a multi-agent framework for episodic context reconstruction in LLMs, enhancing reasoning accuracy and efficiency while preserving contextual integrity.",
      "teaser_image": "https://arxiv.org/html/2601.21714/x2.png",
      "debug_abstract": "The evolution of Large Language Model (LLM) agents towards System~2 reasoning, characterized by deliberative, high-precision problem-solving, requires maintaining rigorous logical integrity over extended horizons. However, prevalent memory preprocessing paradigms suffer from destructive de-contextualization. By compressing complex sequential dependencies into pre-defined structures (e.g., embeddings or graphs), these methods sever the contextual integrity essential for deep reasoning. To address this, we propose E-mem, a framework shifting from Memory Preprocessing to Episodic Context Reconstruction. Inspired by biological engrams, E-mem employs a heterogeneous hierarchical architecture where multiple assistant agents maintain uncompressed memory contexts, while a central master agent orchestrates global planning. Unlike passive retrieval, our mechanism empowers assistants to locally reason within activated segments, extracting context-aware evidence before aggregation. Evaluations on the LoCoMo benchmark demonstrate that E-mem achieves over 54\\% F1, surpassing the state-of-the-art GAM by 7.75\\%, while reducing token cost by over 70\\%."
    },
    {
      "title": "DynaWeb: Model-Based Reinforcement Learning of Web Agents",
      "url": "https://arxiv.org/abs/2601.22149",
      "date": "2026-01-29",
      "authors_text": "Hang Ding, Peidong Liu, Junqiao Wang, Ziwei Ji, Meng Cao",
      "is_highlight": false,
      "score": 77.0,
      "summary": "DynaWeb introduces a model-based reinforcement learning framework that enhances web agent training by simulating interactions with a web world model, improving efficiency and performance.",
      "teaser_image": "https://arxiv.org/html/2601.22149/figures/dynaweb.png",
      "debug_abstract": "The development of autonomous web agents, powered by Large Language Models (LLMs) and reinforcement learning (RL), represents a significant step towards general-purpose AI assistants. However, training these agents is severely hampered by the challenges of interacting with the live internet, which is inefficient, costly, and fraught with risks. Model-based reinforcement learning (MBRL) offers a promising solution by learning a world model of the environment to enable simulated interaction. This paper introduces DynaWeb, a novel MBRL framework that trains web agents through interacting with a web world model trained to predict naturalistic web page representations given agent actions. This model serves as a synthetic web environment where an agent policy can dream by generating vast quantities of rollout action trajectories for efficient online reinforcement learning. Beyond free policy rollouts, DynaWeb incorporates real expert trajectories from training data, which are randomly interleaved with on-policy rollouts during training to improve stability and sample efficiency. Experiments conducted on the challenging WebArena and WebVoyager benchmarks demonstrate that DynaWeb consistently and significantly improves the performance of state-of-the-art open-source web agent models. Our findings establish the viability of training web agents through imagination, offering a scalable and efficient way to scale up online agentic RL."
    },
    {
      "title": "PLANING: A Loosely Coupled Triangle-Gaussian Framework for Streaming 3D Reconstruction",
      "url": "https://arxiv.org/abs/2601.22046",
      "date": "2026-01-29",
      "authors_text": "Changjian Jiang, Kerui Ren, Xudong Li, Kaiwen Song, Linning Xu",
      "is_highlight": false,
      "score": 66.0,
      "summary": "PLANING is a novel streaming 3D reconstruction framework that efficiently combines geometric primitives and neural Gaussians, enhancing quality and speed while reducing redundancy.",
      "teaser_image": "https://arxiv.org/html/2601.22046/x3.png",
      "debug_abstract": "Streaming reconstruction from monocular image sequences remains challenging, as existing methods typically favor either high-quality rendering or accurate geometry, but rarely both. We present PLANING, an efficient on-the-fly reconstruction framework built on a hybrid representation that loosely couples explicit geometric primitives with neural Gaussians, enabling geometry and appearance to be modeled in a decoupled manner. This decoupling supports an online initialization and optimization strategy that separates geometry and appearance updates, yielding stable streaming reconstruction with substantially reduced structural redundancy. PLANING improves dense mesh Chamfer-L2 by 18.52% over PGSR, surpasses ARTDECO by 1.31 dB PSNR, and reconstructs ScanNetV2 scenes in under 100 seconds, over 5x faster than 2D Gaussian Splatting, while matching the quality of offline per-scene optimization. Beyond reconstruction quality, the structural clarity and computational efficiency of \\modelname~make it well suited for a broad range of downstream applications, such as enabling large-scale scene modeling and simulation-ready environments for embodied AI. Project page: this https URL ."
    }
  ],
  "数字媒体与计算机视觉实验室": [
    {
      "title": "E-mem: Multi-agent based Episodic Context Reconstruction for LLM Agent Memory",
      "url": "https://arxiv.org/abs/2601.21714",
      "date": "2026-01-29",
      "authors_text": "Kaixiang Wang, Yidan Lin, Jiong Lou, Zhaojiacheng Zhou, Bunyod Suvonov",
      "is_highlight": false,
      "score": 71.0,
      "summary": "E-mem introduces a multi-agent framework for episodic context reconstruction in LLMs, enhancing reasoning accuracy and efficiency while preserving contextual integrity.",
      "teaser_image": "https://arxiv.org/html/2601.21714/x2.png",
      "debug_abstract": "The evolution of Large Language Model (LLM) agents towards System~2 reasoning, characterized by deliberative, high-precision problem-solving, requires maintaining rigorous logical integrity over extended horizons. However, prevalent memory preprocessing paradigms suffer from destructive de-contextualization. By compressing complex sequential dependencies into pre-defined structures (e.g., embeddings or graphs), these methods sever the contextual integrity essential for deep reasoning. To address this, we propose E-mem, a framework shifting from Memory Preprocessing to Episodic Context Reconstruction. Inspired by biological engrams, E-mem employs a heterogeneous hierarchical architecture where multiple assistant agents maintain uncompressed memory contexts, while a central master agent orchestrates global planning. Unlike passive retrieval, our mechanism empowers assistants to locally reason within activated segments, extracting context-aware evidence before aggregation. Evaluations on the LoCoMo benchmark demonstrate that E-mem achieves over 54\\% F1, surpassing the state-of-the-art GAM by 7.75\\%, while reducing token cost by over 70\\%."
    },
    {
      "title": "DynaWeb: Model-Based Reinforcement Learning of Web Agents",
      "url": "https://arxiv.org/abs/2601.22149",
      "date": "2026-01-29",
      "authors_text": "Hang Ding, Peidong Liu, Junqiao Wang, Ziwei Ji, Meng Cao",
      "is_highlight": false,
      "score": 77.0,
      "summary": "DynaWeb introduces a model-based reinforcement learning framework that enhances web agent training by simulating interactions with a web world model, improving efficiency and performance.",
      "teaser_image": "https://arxiv.org/html/2601.22149/figures/dynaweb.png",
      "debug_abstract": "The development of autonomous web agents, powered by Large Language Models (LLMs) and reinforcement learning (RL), represents a significant step towards general-purpose AI assistants. However, training these agents is severely hampered by the challenges of interacting with the live internet, which is inefficient, costly, and fraught with risks. Model-based reinforcement learning (MBRL) offers a promising solution by learning a world model of the environment to enable simulated interaction. This paper introduces DynaWeb, a novel MBRL framework that trains web agents through interacting with a web world model trained to predict naturalistic web page representations given agent actions. This model serves as a synthetic web environment where an agent policy can dream by generating vast quantities of rollout action trajectories for efficient online reinforcement learning. Beyond free policy rollouts, DynaWeb incorporates real expert trajectories from training data, which are randomly interleaved with on-policy rollouts during training to improve stability and sample efficiency. Experiments conducted on the challenging WebArena and WebVoyager benchmarks demonstrate that DynaWeb consistently and significantly improves the performance of state-of-the-art open-source web agent models. Our findings establish the viability of training web agents through imagination, offering a scalable and efficient way to scale up online agentic RL."
    },
    {
      "title": "PLANING: A Loosely Coupled Triangle-Gaussian Framework for Streaming 3D Reconstruction",
      "url": "https://arxiv.org/abs/2601.22046",
      "date": "2026-01-29",
      "authors_text": "Changjian Jiang, Kerui Ren, Xudong Li, Kaiwen Song, Linning Xu",
      "is_highlight": false,
      "score": 66.0,
      "summary": "PLANING is a novel streaming 3D reconstruction framework that efficiently combines geometric primitives and neural Gaussians, enhancing quality and speed while reducing redundancy.",
      "teaser_image": "https://arxiv.org/html/2601.22046/x3.png",
      "debug_abstract": "Streaming reconstruction from monocular image sequences remains challenging, as existing methods typically favor either high-quality rendering or accurate geometry, but rarely both. We present PLANING, an efficient on-the-fly reconstruction framework built on a hybrid representation that loosely couples explicit geometric primitives with neural Gaussians, enabling geometry and appearance to be modeled in a decoupled manner. This decoupling supports an online initialization and optimization strategy that separates geometry and appearance updates, yielding stable streaming reconstruction with substantially reduced structural redundancy. PLANING improves dense mesh Chamfer-L2 by 18.52% over PGSR, surpasses ARTDECO by 1.31 dB PSNR, and reconstructs ScanNetV2 scenes in under 100 seconds, over 5x faster than 2D Gaussian Splatting, while matching the quality of offline per-scene optimization. Beyond reconstruction quality, the structural clarity and computational efficiency of \\modelname~make it well suited for a broad range of downstream applications, such as enabling large-scale scene modeling and simulation-ready environments for embodied AI. Project page: this https URL ."
    }
  ],
  "Berkeley Artificial Intelligence Research Lab (BAIR)": [
    {
      "title": "mjlab: A Lightweight Framework for GPU-Accelerated Robot Learning",
      "url": "https://arxiv.org/abs/2601.22074",
      "date": "2026-01-29",
      "authors_text": "Kevin Zakka, Qiayuan Liao, Brent Yi, Louis Le Lay, Koushil Sreenath",
      "is_highlight": false,
      "score": 92.0,
      "summary": "mjlab is an open-source, GPU-accelerated robot learning framework that simplifies setup and enables modular environment composition for various robotic tasks.",
      "teaser_image": "https://arxiv.org/html/2601.22074/rough_terrain.png",
      "debug_abstract": "We present mjlab, a lightweight, open-source framework for robot learning that combines GPU-accelerated simulation with composable environments and minimal setup friction. mjlab adopts the manager-based API introduced by Isaac Lab, where users compose modular building blocks for observations, rewards, and events, and pairs it with MuJoCo Warp for GPU-accelerated physics. The result is a framework installable with a single command, requiring minimal dependencies, and providing direct access to native MuJoCo data structures. mjlab ships with reference implementations of velocity tracking, motion imitation, and manipulation tasks."
    }
  ],
  "四川大学": [
    {
      "title": "DynaWeb: Model-Based Reinforcement Learning of Web Agents",
      "url": "https://arxiv.org/abs/2601.22149",
      "date": "2026-01-29",
      "authors_text": "Hang Ding, Peidong Liu, Junqiao Wang, Ziwei Ji, Meng Cao",
      "is_highlight": false,
      "score": 77.0,
      "summary": "DynaWeb introduces a model-based reinforcement learning framework that enhances web agent training by simulating interactions with a web world model, improving efficiency and performance.",
      "teaser_image": "https://arxiv.org/html/2601.22149/figures/dynaweb.png",
      "debug_abstract": "The development of autonomous web agents, powered by Large Language Models (LLMs) and reinforcement learning (RL), represents a significant step towards general-purpose AI assistants. However, training these agents is severely hampered by the challenges of interacting with the live internet, which is inefficient, costly, and fraught with risks. Model-based reinforcement learning (MBRL) offers a promising solution by learning a world model of the environment to enable simulated interaction. This paper introduces DynaWeb, a novel MBRL framework that trains web agents through interacting with a web world model trained to predict naturalistic web page representations given agent actions. This model serves as a synthetic web environment where an agent policy can dream by generating vast quantities of rollout action trajectories for efficient online reinforcement learning. Beyond free policy rollouts, DynaWeb incorporates real expert trajectories from training data, which are randomly interleaved with on-policy rollouts during training to improve stability and sample efficiency. Experiments conducted on the challenging WebArena and WebVoyager benchmarks demonstrate that DynaWeb consistently and significantly improves the performance of state-of-the-art open-source web agent models. Our findings establish the viability of training web agents through imagination, offering a scalable and efficient way to scale up online agentic RL."
    }
  ],
  "PINE Lab": [
    {
      "title": "DynamicVLA: A Vision-Language-Action Model for Dynamic Object Manipulation",
      "url": "https://arxiv.org/abs/2601.22153",
      "date": "2026-01-29",
      "authors_text": "Haozhe Xie, Beichen Wen, Jiarui Zheng, Zhaoxi Chen, Fangzhou Hong",
      "is_highlight": false,
      "score": 93.0,
      "summary": "DynamicVLA is a novel Vision-Language-Action framework that enhances dynamic object manipulation through efficient encoding, continuous inference, and a new benchmark for data collection.",
      "teaser_image": "https://arxiv.org/html/2601.22153/x1.png",
      "debug_abstract": "Manipulating dynamic objects remains an open challenge for Vision-Language-Action (VLA) models, which, despite strong generalization in static manipulation, struggle in dynamic scenarios requiring rapid perception, temporal anticipation, and continuous control. We present DynamicVLA, a framework for dynamic object manipulation that integrates temporal reasoning and closed-loop adaptation through three key designs: 1) a compact 0.4B VLA using a convolutional vision encoder for spatially efficient, structurally faithful encoding, enabling fast multimodal inference; 2) Continuous Inference, enabling overlapping reasoning and execution for lower latency and timely adaptation to object motion; and 3) Latent-aware Action Streaming, which bridges the perception-execution gap by enforcing temporally aligned action execution. To fill the missing foundation of dynamic manipulation data, we introduce the Dynamic Object Manipulation (DOM) benchmark, built from scratch with an auto data collection pipeline that efficiently gathers 200K synthetic episodes across 2.8K scenes and 206 objects, and enables fast collection of 2K real-world episodes without teleoperation. Extensive evaluations demonstrate remarkable improvements in response speed, perception, and generalization, positioning DynamicVLA as a unified framework for general dynamic object manipulation across embodiments."
    }
  ],
  "MMLab@NTU": [
    {
      "title": "DynamicVLA: A Vision-Language-Action Model for Dynamic Object Manipulation",
      "url": "https://arxiv.org/abs/2601.22153",
      "date": "2026-01-29",
      "authors_text": "Haozhe Xie, Beichen Wen, Jiarui Zheng, Zhaoxi Chen, Fangzhou Hong",
      "is_highlight": false,
      "score": 93.0,
      "summary": "DynamicVLA is a novel Vision-Language-Action framework that enhances dynamic object manipulation through efficient encoding, continuous inference, and a new benchmark for data collection.",
      "teaser_image": "https://arxiv.org/html/2601.22153/x1.png",
      "debug_abstract": "Manipulating dynamic objects remains an open challenge for Vision-Language-Action (VLA) models, which, despite strong generalization in static manipulation, struggle in dynamic scenarios requiring rapid perception, temporal anticipation, and continuous control. We present DynamicVLA, a framework for dynamic object manipulation that integrates temporal reasoning and closed-loop adaptation through three key designs: 1) a compact 0.4B VLA using a convolutional vision encoder for spatially efficient, structurally faithful encoding, enabling fast multimodal inference; 2) Continuous Inference, enabling overlapping reasoning and execution for lower latency and timely adaptation to object motion; and 3) Latent-aware Action Streaming, which bridges the perception-execution gap by enforcing temporally aligned action execution. To fill the missing foundation of dynamic manipulation data, we introduce the Dynamic Object Manipulation (DOM) benchmark, built from scratch with an auto data collection pipeline that efficiently gathers 200K synthetic episodes across 2.8K scenes and 206 objects, and enables fast collection of 2K real-world episodes without teleoperation. Extensive evaluations demonstrate remarkable improvements in response speed, perception, and generalization, positioning DynamicVLA as a unified framework for general dynamic object manipulation across embodiments."
    }
  ],
  "S-Lab for Advanced Intelligence": [
    {
      "title": "DynamicVLA: A Vision-Language-Action Model for Dynamic Object Manipulation",
      "url": "https://arxiv.org/abs/2601.22153",
      "date": "2026-01-29",
      "authors_text": "Haozhe Xie, Beichen Wen, Jiarui Zheng, Zhaoxi Chen, Fangzhou Hong",
      "is_highlight": false,
      "score": 93.0,
      "summary": "DynamicVLA is a novel Vision-Language-Action framework that enhances dynamic object manipulation through efficient encoding, continuous inference, and a new benchmark for data collection.",
      "teaser_image": "https://arxiv.org/html/2601.22153/x1.png",
      "debug_abstract": "Manipulating dynamic objects remains an open challenge for Vision-Language-Action (VLA) models, which, despite strong generalization in static manipulation, struggle in dynamic scenarios requiring rapid perception, temporal anticipation, and continuous control. We present DynamicVLA, a framework for dynamic object manipulation that integrates temporal reasoning and closed-loop adaptation through three key designs: 1) a compact 0.4B VLA using a convolutional vision encoder for spatially efficient, structurally faithful encoding, enabling fast multimodal inference; 2) Continuous Inference, enabling overlapping reasoning and execution for lower latency and timely adaptation to object motion; and 3) Latent-aware Action Streaming, which bridges the perception-execution gap by enforcing temporally aligned action execution. To fill the missing foundation of dynamic manipulation data, we introduce the Dynamic Object Manipulation (DOM) benchmark, built from scratch with an auto data collection pipeline that efficiently gathers 200K synthetic episodes across 2.8K scenes and 206 objects, and enables fast collection of 2K real-world episodes without teleoperation. Extensive evaluations demonstrate remarkable improvements in response speed, perception, and generalization, positioning DynamicVLA as a unified framework for general dynamic object manipulation across embodiments."
    }
  ],
  "MReaLLab": [
    {
      "title": "DynamicVLA: A Vision-Language-Action Model for Dynamic Object Manipulation",
      "url": "https://arxiv.org/abs/2601.22153",
      "date": "2026-01-29",
      "authors_text": "Haozhe Xie, Beichen Wen, Jiarui Zheng, Zhaoxi Chen, Fangzhou Hong",
      "is_highlight": false,
      "score": 93.0,
      "summary": "DynamicVLA is a novel Vision-Language-Action framework that enhances dynamic object manipulation through efficient encoding, continuous inference, and a new benchmark for data collection.",
      "teaser_image": "https://arxiv.org/html/2601.22153/x1.png",
      "debug_abstract": "Manipulating dynamic objects remains an open challenge for Vision-Language-Action (VLA) models, which, despite strong generalization in static manipulation, struggle in dynamic scenarios requiring rapid perception, temporal anticipation, and continuous control. We present DynamicVLA, a framework for dynamic object manipulation that integrates temporal reasoning and closed-loop adaptation through three key designs: 1) a compact 0.4B VLA using a convolutional vision encoder for spatially efficient, structurally faithful encoding, enabling fast multimodal inference; 2) Continuous Inference, enabling overlapping reasoning and execution for lower latency and timely adaptation to object motion; and 3) Latent-aware Action Streaming, which bridges the perception-execution gap by enforcing temporally aligned action execution. To fill the missing foundation of dynamic manipulation data, we introduce the Dynamic Object Manipulation (DOM) benchmark, built from scratch with an auto data collection pipeline that efficiently gathers 200K synthetic episodes across 2.8K scenes and 206 objects, and enables fast collection of 2K real-world episodes without teleoperation. Extensive evaluations demonstrate remarkable improvements in response speed, perception, and generalization, positioning DynamicVLA as a unified framework for general dynamic object manipulation across embodiments."
    }
  ],
  "ROSE Lab": [
    {
      "title": "DynamicVLA: A Vision-Language-Action Model for Dynamic Object Manipulation",
      "url": "https://arxiv.org/abs/2601.22153",
      "date": "2026-01-29",
      "authors_text": "Haozhe Xie, Beichen Wen, Jiarui Zheng, Zhaoxi Chen, Fangzhou Hong",
      "is_highlight": false,
      "score": 93.0,
      "summary": "DynamicVLA is a novel Vision-Language-Action framework that enhances dynamic object manipulation through efficient encoding, continuous inference, and a new benchmark for data collection.",
      "teaser_image": "https://arxiv.org/html/2601.22153/x1.png",
      "debug_abstract": "Manipulating dynamic objects remains an open challenge for Vision-Language-Action (VLA) models, which, despite strong generalization in static manipulation, struggle in dynamic scenarios requiring rapid perception, temporal anticipation, and continuous control. We present DynamicVLA, a framework for dynamic object manipulation that integrates temporal reasoning and closed-loop adaptation through three key designs: 1) a compact 0.4B VLA using a convolutional vision encoder for spatially efficient, structurally faithful encoding, enabling fast multimodal inference; 2) Continuous Inference, enabling overlapping reasoning and execution for lower latency and timely adaptation to object motion; and 3) Latent-aware Action Streaming, which bridges the perception-execution gap by enforcing temporally aligned action execution. To fill the missing foundation of dynamic manipulation data, we introduce the Dynamic Object Manipulation (DOM) benchmark, built from scratch with an auto data collection pipeline that efficiently gathers 200K synthetic episodes across 2.8K scenes and 206 objects, and enables fast collection of 2K real-world episodes without teleoperation. Extensive evaluations demonstrate remarkable improvements in response speed, perception, and generalization, positioning DynamicVLA as a unified framework for general dynamic object manipulation across embodiments."
    }
  ],
  "MARS Lab": [
    {
      "title": "DynamicVLA: A Vision-Language-Action Model for Dynamic Object Manipulation",
      "url": "https://arxiv.org/abs/2601.22153",
      "date": "2026-01-29",
      "authors_text": "Haozhe Xie, Beichen Wen, Jiarui Zheng, Zhaoxi Chen, Fangzhou Hong",
      "is_highlight": false,
      "score": 93.0,
      "summary": "DynamicVLA is a novel Vision-Language-Action framework that enhances dynamic object manipulation through efficient encoding, continuous inference, and a new benchmark for data collection.",
      "teaser_image": "https://arxiv.org/html/2601.22153/x1.png",
      "debug_abstract": "Manipulating dynamic objects remains an open challenge for Vision-Language-Action (VLA) models, which, despite strong generalization in static manipulation, struggle in dynamic scenarios requiring rapid perception, temporal anticipation, and continuous control. We present DynamicVLA, a framework for dynamic object manipulation that integrates temporal reasoning and closed-loop adaptation through three key designs: 1) a compact 0.4B VLA using a convolutional vision encoder for spatially efficient, structurally faithful encoding, enabling fast multimodal inference; 2) Continuous Inference, enabling overlapping reasoning and execution for lower latency and timely adaptation to object motion; and 3) Latent-aware Action Streaming, which bridges the perception-execution gap by enforcing temporally aligned action execution. To fill the missing foundation of dynamic manipulation data, we introduce the Dynamic Object Manipulation (DOM) benchmark, built from scratch with an auto data collection pipeline that efficiently gathers 200K synthetic episodes across 2.8K scenes and 206 objects, and enables fast collection of 2K real-world episodes without teleoperation. Extensive evaluations demonstrate remarkable improvements in response speed, perception, and generalization, positioning DynamicVLA as a unified framework for general dynamic object manipulation across embodiments."
    }
  ],
  "具身感知与交互实验室 (EPIC Lab)": [
    {
      "title": "Disturbance-Aware Flight Control of Robotic Gliding Blimp via Moving Mass Actuation",
      "url": "https://arxiv.org/abs/2601.21188",
      "date": "2026-01-29",
      "authors_text": "Hao Cheng, Feitian Zhang",
      "is_highlight": false,
      "score": 86.0,
      "summary": "This paper presents a disturbance-aware flight control system for robotic blimps using moving mass actuation and a combined moving horizon estimator and model predictive controller for enhanced stability.",
      "teaser_image": "https://arxiv.org/html/2601.21188/x2.png",
      "debug_abstract": "Robotic blimps, as lighter-than-air (LTA) aerial systems, offer long endurance and inherently safe operation but remain highly susceptible to wind disturbances. Building on recent advances in moving mass actuation, this paper addresses the lack of disturbance-aware control frameworks for LTA platforms by explicitly modeling and compensating for wind-induced effects. A moving horizon estimator (MHE) infers real-time wind perturbations and provides these estimates to a model predictive controller (MPC), enabling robust trajectory and heading regulation under varying wind conditions. The proposed approach leverages a two-degree-of-freedom (2-DoF) moving-mass mechanism to generate both inertial and aerodynamic moments for attitude and heading control, thereby enhancing flight stability in disturbance-prone environments. Extensive flight experiments under headwind and crosswind conditions show that the integrated MHE-MPC framework significantly outperforms baseline PID control, demonstrating its effectiveness for disturbance-aware LTA flight."
    }
  ],
  "情感与认知智能机器人实验室 (ACIR)": [
    {
      "title": "Disturbance-Aware Flight Control of Robotic Gliding Blimp via Moving Mass Actuation",
      "url": "https://arxiv.org/abs/2601.21188",
      "date": "2026-01-29",
      "authors_text": "Hao Cheng, Feitian Zhang",
      "is_highlight": false,
      "score": 86.0,
      "summary": "This paper presents a disturbance-aware flight control system for robotic blimps using moving mass actuation and a combined moving horizon estimator and model predictive controller for enhanced stability.",
      "teaser_image": "https://arxiv.org/html/2601.21188/x2.png",
      "debug_abstract": "Robotic blimps, as lighter-than-air (LTA) aerial systems, offer long endurance and inherently safe operation but remain highly susceptible to wind disturbances. Building on recent advances in moving mass actuation, this paper addresses the lack of disturbance-aware control frameworks for LTA platforms by explicitly modeling and compensating for wind-induced effects. A moving horizon estimator (MHE) infers real-time wind perturbations and provides these estimates to a model predictive controller (MPC), enabling robust trajectory and heading regulation under varying wind conditions. The proposed approach leverages a two-degree-of-freedom (2-DoF) moving-mass mechanism to generate both inertial and aerodynamic moments for attitude and heading control, thereby enhancing flight stability in disturbance-prone environments. Extensive flight experiments under headwind and crosswind conditions show that the integrated MHE-MPC framework significantly outperforms baseline PID control, demonstrating its effectiveness for disturbance-aware LTA flight."
    }
  ],
  "智元机器人联合实验室 (PKU-Agibot)": [
    {
      "title": "Disturbance-Aware Flight Control of Robotic Gliding Blimp via Moving Mass Actuation",
      "url": "https://arxiv.org/abs/2601.21188",
      "date": "2026-01-29",
      "authors_text": "Hao Cheng, Feitian Zhang",
      "is_highlight": false,
      "score": 86.0,
      "summary": "This paper presents a disturbance-aware flight control system for robotic blimps using moving mass actuation and a combined moving horizon estimator and model predictive controller for enhanced stability.",
      "teaser_image": "https://arxiv.org/html/2601.21188/x2.png",
      "debug_abstract": "Robotic blimps, as lighter-than-air (LTA) aerial systems, offer long endurance and inherently safe operation but remain highly susceptible to wind disturbances. Building on recent advances in moving mass actuation, this paper addresses the lack of disturbance-aware control frameworks for LTA platforms by explicitly modeling and compensating for wind-induced effects. A moving horizon estimator (MHE) infers real-time wind perturbations and provides these estimates to a model predictive controller (MPC), enabling robust trajectory and heading regulation under varying wind conditions. The proposed approach leverages a two-degree-of-freedom (2-DoF) moving-mass mechanism to generate both inertial and aerodynamic moments for attitude and heading control, thereby enhancing flight stability in disturbance-prone environments. Extensive flight experiments under headwind and crosswind conditions show that the integrated MHE-MPC framework significantly outperforms baseline PID control, demonstrating its effectiveness for disturbance-aware LTA flight."
    }
  ],
  "HMI Lab": [
    {
      "title": "Disturbance-Aware Flight Control of Robotic Gliding Blimp via Moving Mass Actuation",
      "url": "https://arxiv.org/abs/2601.21188",
      "date": "2026-01-29",
      "authors_text": "Hao Cheng, Feitian Zhang",
      "is_highlight": false,
      "score": 86.0,
      "summary": "This paper presents a disturbance-aware flight control system for robotic blimps using moving mass actuation and a combined moving horizon estimator and model predictive controller for enhanced stability.",
      "teaser_image": "https://arxiv.org/html/2601.21188/x2.png",
      "debug_abstract": "Robotic blimps, as lighter-than-air (LTA) aerial systems, offer long endurance and inherently safe operation but remain highly susceptible to wind disturbances. Building on recent advances in moving mass actuation, this paper addresses the lack of disturbance-aware control frameworks for LTA platforms by explicitly modeling and compensating for wind-induced effects. A moving horizon estimator (MHE) infers real-time wind perturbations and provides these estimates to a model predictive controller (MPC), enabling robust trajectory and heading regulation under varying wind conditions. The proposed approach leverages a two-degree-of-freedom (2-DoF) moving-mass mechanism to generate both inertial and aerodynamic moments for attitude and heading control, thereby enhancing flight stability in disturbance-prone environments. Extensive flight experiments under headwind and crosswind conditions show that the integrated MHE-MPC framework significantly outperforms baseline PID control, demonstrating its effectiveness for disturbance-aware LTA flight."
    }
  ]
}