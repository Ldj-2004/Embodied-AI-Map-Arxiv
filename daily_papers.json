{
  "机器人研究所 (CKSRI)": [
    {
      "title": "TKG-Thinker: Towards Dynamic Reasoning over Temporal Knowledge Graphs via Agentic Reinforcement Learning",
      "url": "https://arxiv.org/abs/2602.05818",
      "date": "2026-02-05",
      "authors_text": "Zihao Jiang, Miao Peng, Zhenyan Shan, Wenjie Xu, Ben Liu",
      "is_highlight": false,
      "score": 83.0,
      "summary": "TKG-Thinker enhances temporal knowledge graph question answering by utilizing agentic reinforcement learning for dynamic reasoning, outperforming existing models through autonomous planning and adaptive retrieval.",
      "teaser_image": "https://arxiv.org/html/2602.05818/x2.png",
      "debug_abstract": "Temporal knowledge graph question answering (TKGQA) aims to answer time-sensitive questions by leveraging temporal knowledge bases. While Large Language Models (LLMs) demonstrate significant potential in TKGQA, current prompting strategies constrain their efficacy in two primary ways. First, they are prone to reasoning hallucinations under complex temporal constraints. Second, static prompting limits model autonomy and generalization, as it lack optimization through dynamic interaction with temporal knowledge graphs (TKGs) environments. To address these limitations, we propose \\textbf{TKG-Thinker}, a novel agent equipped with autonomous planning and adaptive retrieval capabilities for reasoning over TKGs. Specifically, TKG-Thinker performs in-depth temporal reasoning through dynamic multi-turn interactions with TKGs via a dual-training strategy. We first apply Supervised Fine-Tuning (SFT) with chain-of thought data to instill core planning capabilities, followed by a Reinforcement Learning (RL) stage that leverages multi-dimensional rewards to refine reasoning policies under intricate temporal constraints. Experimental results on benchmark datasets with three open-source LLMs show that TKG-Thinker achieves state-of-the-art performance and exhibits strong generalization across complex TKGQA settings."
    },
    {
      "title": "GAS: Enhancing Reward-Cost Balance of Generative Model-assisted Offline Safe RL",
      "url": "https://arxiv.org/abs/2602.05323",
      "date": "2026-02-05",
      "authors_text": "Zifan Liu, Xinran Li, Shibo Chen, Jun Zhang",
      "is_highlight": false,
      "score": 75.0,
      "summary": "The GAS algorithm enhances offline safe reinforcement learning by improving trajectory stitching and balancing reward-cost targets through dataset augmentation and novel goal functions.",
      "teaser_image": "https://arxiv.org/html/2602.05323/GAS_details.png",
      "debug_abstract": "Offline Safe Reinforcement Learning (OSRL) aims to learn a policy to achieve high performance in sequential decision-making while satisfying constraints, using only pre-collected datasets. Recent works, inspired by the strong capabilities of Generative Models (GMs), reformulate decision-making in OSRL as a conditional generative process, where GMs generate desirable actions conditioned on predefined reward and cost values. However, GM-assisted methods face two major challenges in OSRL: (1) lacking the ability to &#34;stitch&#34; optimal transitions from suboptimal trajectories within the dataset, and (2) struggling to balance reward targets with cost targets, particularly when they are conflict. To address these issues, we propose Goal-Assisted Stitching (GAS), a novel algorithm designed to enhance stitching capabilities while effectively balancing reward maximization and constraint satisfaction. To enhance the stitching ability, GAS first augments and relabels the dataset at the transition level, enabling the construction of high-quality trajectories from suboptimal ones. GAS also introduces novel goal functions, which estimate the optimal achievable reward and cost goals from the dataset. These goal functions, trained using expectile regression on the relabeled and augmented dataset, allow GAS to accommodate a broader range of reward-cost return pairs and achieve a better tradeoff between reward maximization and constraint satisfaction compared to human-specified values. The estimated goals then guide policy training, ensuring robust performance under constrained settings. Furthermore, to improve training stability and efficiency, we reshape the dataset to achieve a more uniform reward-cost return distribution. Empirical results validate the effectiveness of GAS, demonstrating superior performance in balancing reward maximization and constraint satisfaction compared to existing methods."
    },
    {
      "title": "Invariance on Manifolds: Understanding Robust Visual Representations for Place Recognition",
      "url": "https://arxiv.org/abs/2602.00841",
      "date": "2026-02-05",
      "authors_text": "Jintao Cheng, Weibin Li, Zhijian He, Jin Wu, Chi Man Vong",
      "is_highlight": false,
      "score": 65.0,
      "summary": "This paper presents a training-free Second-Order Geometric Statistics framework for robust visual place recognition, leveraging covariance descriptors on the SPD manifold for effective zero-shot generalization.",
      "teaser_image": "https://arxiv.org/html/2602.00841/x2.png",
      "debug_abstract": "Visual Place Recognition (VPR) demands representations robust to drastic environmental and viewpoint shifts. Current aggregation paradigms, however, either rely on data-hungry supervision or simplistic first-order statistics, often neglecting intrinsic structural correlations. In this work, we propose a Second-Order Geometric Statistics framework that inherently captures geometric stability without training. We conceptualize scenes as covariance descriptors on the Symmetric Positive Definite (SPD) manifold, where perturbations manifest as tractable congruence transformations. By leveraging geometry-aware Riemannian mappings, we project these descriptors into a linearized Euclidean embedding, effectively decoupling signal structure from noise. Our approach introduces a training-free framework built upon fixed, pre-trained backbones, achieving strong zero-shot generalization without parameter updates. Extensive experiments confirm that our method achieves highly competitive performance against state-of-the-art baselines, particularly excelling in challenging zero-shot scenarios."
    }
  ],
  "机器人研究所": [
    {
      "title": "TKG-Thinker: Towards Dynamic Reasoning over Temporal Knowledge Graphs via Agentic Reinforcement Learning",
      "url": "https://arxiv.org/abs/2602.05818",
      "date": "2026-02-05",
      "authors_text": "Zihao Jiang, Miao Peng, Zhenyan Shan, Wenjie Xu, Ben Liu",
      "is_highlight": false,
      "score": 83.0,
      "summary": "TKG-Thinker enhances temporal knowledge graph question answering by utilizing agentic reinforcement learning for dynamic reasoning, outperforming existing models through autonomous planning and adaptive retrieval.",
      "teaser_image": "https://arxiv.org/html/2602.05818/x2.png",
      "debug_abstract": "Temporal knowledge graph question answering (TKGQA) aims to answer time-sensitive questions by leveraging temporal knowledge bases. While Large Language Models (LLMs) demonstrate significant potential in TKGQA, current prompting strategies constrain their efficacy in two primary ways. First, they are prone to reasoning hallucinations under complex temporal constraints. Second, static prompting limits model autonomy and generalization, as it lack optimization through dynamic interaction with temporal knowledge graphs (TKGs) environments. To address these limitations, we propose \\textbf{TKG-Thinker}, a novel agent equipped with autonomous planning and adaptive retrieval capabilities for reasoning over TKGs. Specifically, TKG-Thinker performs in-depth temporal reasoning through dynamic multi-turn interactions with TKGs via a dual-training strategy. We first apply Supervised Fine-Tuning (SFT) with chain-of thought data to instill core planning capabilities, followed by a Reinforcement Learning (RL) stage that leverages multi-dimensional rewards to refine reasoning policies under intricate temporal constraints. Experimental results on benchmark datasets with three open-source LLMs show that TKG-Thinker achieves state-of-the-art performance and exhibits strong generalization across complex TKGQA settings."
    },
    {
      "title": "GAS: Enhancing Reward-Cost Balance of Generative Model-assisted Offline Safe RL",
      "url": "https://arxiv.org/abs/2602.05323",
      "date": "2026-02-05",
      "authors_text": "Zifan Liu, Xinran Li, Shibo Chen, Jun Zhang",
      "is_highlight": false,
      "score": 75.0,
      "summary": "The GAS algorithm enhances offline safe reinforcement learning by improving trajectory stitching and balancing reward-cost targets through dataset augmentation and novel goal functions.",
      "teaser_image": "https://arxiv.org/html/2602.05323/GAS_details.png",
      "debug_abstract": "Offline Safe Reinforcement Learning (OSRL) aims to learn a policy to achieve high performance in sequential decision-making while satisfying constraints, using only pre-collected datasets. Recent works, inspired by the strong capabilities of Generative Models (GMs), reformulate decision-making in OSRL as a conditional generative process, where GMs generate desirable actions conditioned on predefined reward and cost values. However, GM-assisted methods face two major challenges in OSRL: (1) lacking the ability to &#34;stitch&#34; optimal transitions from suboptimal trajectories within the dataset, and (2) struggling to balance reward targets with cost targets, particularly when they are conflict. To address these issues, we propose Goal-Assisted Stitching (GAS), a novel algorithm designed to enhance stitching capabilities while effectively balancing reward maximization and constraint satisfaction. To enhance the stitching ability, GAS first augments and relabels the dataset at the transition level, enabling the construction of high-quality trajectories from suboptimal ones. GAS also introduces novel goal functions, which estimate the optimal achievable reward and cost goals from the dataset. These goal functions, trained using expectile regression on the relabeled and augmented dataset, allow GAS to accommodate a broader range of reward-cost return pairs and achieve a better tradeoff between reward maximization and constraint satisfaction compared to human-specified values. The estimated goals then guide policy training, ensuring robust performance under constrained settings. Furthermore, to improve training stability and efficiency, we reshape the dataset to achieve a more uniform reward-cost return distribution. Empirical results validate the effectiveness of GAS, demonstrating superior performance in balancing reward maximization and constraint satisfaction compared to existing methods."
    },
    {
      "title": "Invariance on Manifolds: Understanding Robust Visual Representations for Place Recognition",
      "url": "https://arxiv.org/abs/2602.00841",
      "date": "2026-02-05",
      "authors_text": "Jintao Cheng, Weibin Li, Zhijian He, Jin Wu, Chi Man Vong",
      "is_highlight": false,
      "score": 65.0,
      "summary": "This paper presents a training-free Second-Order Geometric Statistics framework for robust visual place recognition, leveraging covariance descriptors on the SPD manifold for effective zero-shot generalization.",
      "teaser_image": "https://arxiv.org/html/2602.00841/x2.png",
      "debug_abstract": "Visual Place Recognition (VPR) demands representations robust to drastic environmental and viewpoint shifts. Current aggregation paradigms, however, either rely on data-hungry supervision or simplistic first-order statistics, often neglecting intrinsic structural correlations. In this work, we propose a Second-Order Geometric Statistics framework that inherently captures geometric stability without training. We conceptualize scenes as covariance descriptors on the Symmetric Positive Definite (SPD) manifold, where perturbations manifest as tractable congruence transformations. By leveraging geometry-aware Riemannian mappings, we project these descriptors into a linearized Euclidean embedding, effectively decoupling signal structure from noise. Our approach introduces a training-free framework built upon fixed, pre-trained backbones, achieving strong zero-shot generalization without parameter updates. Extensive experiments confirm that our method achieves highly competitive performance against state-of-the-art baselines, particularly excelling in challenging zero-shot scenarios."
    }
  ],
  "Jun MA老师实验室": [
    {
      "title": "TKG-Thinker: Towards Dynamic Reasoning over Temporal Knowledge Graphs via Agentic Reinforcement Learning",
      "url": "https://arxiv.org/abs/2602.05818",
      "date": "2026-02-05",
      "authors_text": "Zihao Jiang, Miao Peng, Zhenyan Shan, Wenjie Xu, Ben Liu",
      "is_highlight": false,
      "score": 83.0,
      "summary": "TKG-Thinker enhances temporal knowledge graph question answering by utilizing agentic reinforcement learning for dynamic reasoning, outperforming existing models through autonomous planning and adaptive retrieval.",
      "teaser_image": "https://arxiv.org/html/2602.05818/x2.png",
      "debug_abstract": "Temporal knowledge graph question answering (TKGQA) aims to answer time-sensitive questions by leveraging temporal knowledge bases. While Large Language Models (LLMs) demonstrate significant potential in TKGQA, current prompting strategies constrain their efficacy in two primary ways. First, they are prone to reasoning hallucinations under complex temporal constraints. Second, static prompting limits model autonomy and generalization, as it lack optimization through dynamic interaction with temporal knowledge graphs (TKGs) environments. To address these limitations, we propose \\textbf{TKG-Thinker}, a novel agent equipped with autonomous planning and adaptive retrieval capabilities for reasoning over TKGs. Specifically, TKG-Thinker performs in-depth temporal reasoning through dynamic multi-turn interactions with TKGs via a dual-training strategy. We first apply Supervised Fine-Tuning (SFT) with chain-of thought data to instill core planning capabilities, followed by a Reinforcement Learning (RL) stage that leverages multi-dimensional rewards to refine reasoning policies under intricate temporal constraints. Experimental results on benchmark datasets with three open-source LLMs show that TKG-Thinker achieves state-of-the-art performance and exhibits strong generalization across complex TKGQA settings."
    },
    {
      "title": "GAS: Enhancing Reward-Cost Balance of Generative Model-assisted Offline Safe RL",
      "url": "https://arxiv.org/abs/2602.05323",
      "date": "2026-02-05",
      "authors_text": "Zifan Liu, Xinran Li, Shibo Chen, Jun Zhang",
      "is_highlight": false,
      "score": 75.0,
      "summary": "The GAS algorithm enhances offline safe reinforcement learning by improving trajectory stitching and balancing reward-cost targets through dataset augmentation and novel goal functions.",
      "teaser_image": "https://arxiv.org/html/2602.05323/GAS_details.png",
      "debug_abstract": "Offline Safe Reinforcement Learning (OSRL) aims to learn a policy to achieve high performance in sequential decision-making while satisfying constraints, using only pre-collected datasets. Recent works, inspired by the strong capabilities of Generative Models (GMs), reformulate decision-making in OSRL as a conditional generative process, where GMs generate desirable actions conditioned on predefined reward and cost values. However, GM-assisted methods face two major challenges in OSRL: (1) lacking the ability to &#34;stitch&#34; optimal transitions from suboptimal trajectories within the dataset, and (2) struggling to balance reward targets with cost targets, particularly when they are conflict. To address these issues, we propose Goal-Assisted Stitching (GAS), a novel algorithm designed to enhance stitching capabilities while effectively balancing reward maximization and constraint satisfaction. To enhance the stitching ability, GAS first augments and relabels the dataset at the transition level, enabling the construction of high-quality trajectories from suboptimal ones. GAS also introduces novel goal functions, which estimate the optimal achievable reward and cost goals from the dataset. These goal functions, trained using expectile regression on the relabeled and augmented dataset, allow GAS to accommodate a broader range of reward-cost return pairs and achieve a better tradeoff between reward maximization and constraint satisfaction compared to human-specified values. The estimated goals then guide policy training, ensuring robust performance under constrained settings. Furthermore, to improve training stability and efficiency, we reshape the dataset to achieve a more uniform reward-cost return distribution. Empirical results validate the effectiveness of GAS, demonstrating superior performance in balancing reward maximization and constraint satisfaction compared to existing methods."
    },
    {
      "title": "Invariance on Manifolds: Understanding Robust Visual Representations for Place Recognition",
      "url": "https://arxiv.org/abs/2602.00841",
      "date": "2026-02-05",
      "authors_text": "Jintao Cheng, Weibin Li, Zhijian He, Jin Wu, Chi Man Vong",
      "is_highlight": false,
      "score": 65.0,
      "summary": "This paper presents a training-free Second-Order Geometric Statistics framework for robust visual place recognition, leveraging covariance descriptors on the SPD manifold for effective zero-shot generalization.",
      "teaser_image": "https://arxiv.org/html/2602.00841/x2.png",
      "debug_abstract": "Visual Place Recognition (VPR) demands representations robust to drastic environmental and viewpoint shifts. Current aggregation paradigms, however, either rely on data-hungry supervision or simplistic first-order statistics, often neglecting intrinsic structural correlations. In this work, we propose a Second-Order Geometric Statistics framework that inherently captures geometric stability without training. We conceptualize scenes as covariance descriptors on the Symmetric Positive Definite (SPD) manifold, where perturbations manifest as tractable congruence transformations. By leveraging geometry-aware Riemannian mappings, we project these descriptors into a linearized Euclidean embedding, effectively decoupling signal structure from noise. Our approach introduces a training-free framework built upon fixed, pre-trained backbones, achieving strong zero-shot generalization without parameter updates. Extensive experiments confirm that our method achieves highly competitive performance against state-of-the-art baselines, particularly excelling in challenging zero-shot scenarios."
    }
  ],
  "范明明老师实验室": [
    {
      "title": "TKG-Thinker: Towards Dynamic Reasoning over Temporal Knowledge Graphs via Agentic Reinforcement Learning",
      "url": "https://arxiv.org/abs/2602.05818",
      "date": "2026-02-05",
      "authors_text": "Zihao Jiang, Miao Peng, Zhenyan Shan, Wenjie Xu, Ben Liu",
      "is_highlight": false,
      "score": 83.0,
      "summary": "TKG-Thinker enhances temporal knowledge graph question answering by utilizing agentic reinforcement learning for dynamic reasoning, outperforming existing models through autonomous planning and adaptive retrieval.",
      "teaser_image": "https://arxiv.org/html/2602.05818/x2.png",
      "debug_abstract": "Temporal knowledge graph question answering (TKGQA) aims to answer time-sensitive questions by leveraging temporal knowledge bases. While Large Language Models (LLMs) demonstrate significant potential in TKGQA, current prompting strategies constrain their efficacy in two primary ways. First, they are prone to reasoning hallucinations under complex temporal constraints. Second, static prompting limits model autonomy and generalization, as it lack optimization through dynamic interaction with temporal knowledge graphs (TKGs) environments. To address these limitations, we propose \\textbf{TKG-Thinker}, a novel agent equipped with autonomous planning and adaptive retrieval capabilities for reasoning over TKGs. Specifically, TKG-Thinker performs in-depth temporal reasoning through dynamic multi-turn interactions with TKGs via a dual-training strategy. We first apply Supervised Fine-Tuning (SFT) with chain-of thought data to instill core planning capabilities, followed by a Reinforcement Learning (RL) stage that leverages multi-dimensional rewards to refine reasoning policies under intricate temporal constraints. Experimental results on benchmark datasets with three open-source LLMs show that TKG-Thinker achieves state-of-the-art performance and exhibits strong generalization across complex TKGQA settings."
    },
    {
      "title": "GAS: Enhancing Reward-Cost Balance of Generative Model-assisted Offline Safe RL",
      "url": "https://arxiv.org/abs/2602.05323",
      "date": "2026-02-05",
      "authors_text": "Zifan Liu, Xinran Li, Shibo Chen, Jun Zhang",
      "is_highlight": false,
      "score": 75.0,
      "summary": "The GAS algorithm enhances offline safe reinforcement learning by improving trajectory stitching and balancing reward-cost targets through dataset augmentation and novel goal functions.",
      "teaser_image": "https://arxiv.org/html/2602.05323/GAS_details.png",
      "debug_abstract": "Offline Safe Reinforcement Learning (OSRL) aims to learn a policy to achieve high performance in sequential decision-making while satisfying constraints, using only pre-collected datasets. Recent works, inspired by the strong capabilities of Generative Models (GMs), reformulate decision-making in OSRL as a conditional generative process, where GMs generate desirable actions conditioned on predefined reward and cost values. However, GM-assisted methods face two major challenges in OSRL: (1) lacking the ability to &#34;stitch&#34; optimal transitions from suboptimal trajectories within the dataset, and (2) struggling to balance reward targets with cost targets, particularly when they are conflict. To address these issues, we propose Goal-Assisted Stitching (GAS), a novel algorithm designed to enhance stitching capabilities while effectively balancing reward maximization and constraint satisfaction. To enhance the stitching ability, GAS first augments and relabels the dataset at the transition level, enabling the construction of high-quality trajectories from suboptimal ones. GAS also introduces novel goal functions, which estimate the optimal achievable reward and cost goals from the dataset. These goal functions, trained using expectile regression on the relabeled and augmented dataset, allow GAS to accommodate a broader range of reward-cost return pairs and achieve a better tradeoff between reward maximization and constraint satisfaction compared to human-specified values. The estimated goals then guide policy training, ensuring robust performance under constrained settings. Furthermore, to improve training stability and efficiency, we reshape the dataset to achieve a more uniform reward-cost return distribution. Empirical results validate the effectiveness of GAS, demonstrating superior performance in balancing reward maximization and constraint satisfaction compared to existing methods."
    },
    {
      "title": "Invariance on Manifolds: Understanding Robust Visual Representations for Place Recognition",
      "url": "https://arxiv.org/abs/2602.00841",
      "date": "2026-02-05",
      "authors_text": "Jintao Cheng, Weibin Li, Zhijian He, Jin Wu, Chi Man Vong",
      "is_highlight": false,
      "score": 65.0,
      "summary": "This paper presents a training-free Second-Order Geometric Statistics framework for robust visual place recognition, leveraging covariance descriptors on the SPD manifold for effective zero-shot generalization.",
      "teaser_image": "https://arxiv.org/html/2602.00841/x2.png",
      "debug_abstract": "Visual Place Recognition (VPR) demands representations robust to drastic environmental and viewpoint shifts. Current aggregation paradigms, however, either rely on data-hungry supervision or simplistic first-order statistics, often neglecting intrinsic structural correlations. In this work, we propose a Second-Order Geometric Statistics framework that inherently captures geometric stability without training. We conceptualize scenes as covariance descriptors on the Symmetric Positive Definite (SPD) manifold, where perturbations manifest as tractable congruence transformations. By leveraging geometry-aware Riemannian mappings, we project these descriptors into a linearized Euclidean embedding, effectively decoupling signal structure from noise. Our approach introduces a training-free framework built upon fixed, pre-trained backbones, achieving strong zero-shot generalization without parameter updates. Extensive experiments confirm that our method achieves highly competitive performance against state-of-the-art baselines, particularly excelling in challenging zero-shot scenarios."
    }
  ],
  "Precognition Lab": [
    {
      "title": "TKG-Thinker: Towards Dynamic Reasoning over Temporal Knowledge Graphs via Agentic Reinforcement Learning",
      "url": "https://arxiv.org/abs/2602.05818",
      "date": "2026-02-05",
      "authors_text": "Zihao Jiang, Miao Peng, Zhenyan Shan, Wenjie Xu, Ben Liu",
      "is_highlight": false,
      "score": 83.0,
      "summary": "TKG-Thinker enhances temporal knowledge graph question answering by utilizing agentic reinforcement learning for dynamic reasoning, outperforming existing models through autonomous planning and adaptive retrieval.",
      "teaser_image": "https://arxiv.org/html/2602.05818/x2.png",
      "debug_abstract": "Temporal knowledge graph question answering (TKGQA) aims to answer time-sensitive questions by leveraging temporal knowledge bases. While Large Language Models (LLMs) demonstrate significant potential in TKGQA, current prompting strategies constrain their efficacy in two primary ways. First, they are prone to reasoning hallucinations under complex temporal constraints. Second, static prompting limits model autonomy and generalization, as it lack optimization through dynamic interaction with temporal knowledge graphs (TKGs) environments. To address these limitations, we propose \\textbf{TKG-Thinker}, a novel agent equipped with autonomous planning and adaptive retrieval capabilities for reasoning over TKGs. Specifically, TKG-Thinker performs in-depth temporal reasoning through dynamic multi-turn interactions with TKGs via a dual-training strategy. We first apply Supervised Fine-Tuning (SFT) with chain-of thought data to instill core planning capabilities, followed by a Reinforcement Learning (RL) stage that leverages multi-dimensional rewards to refine reasoning policies under intricate temporal constraints. Experimental results on benchmark datasets with three open-source LLMs show that TKG-Thinker achieves state-of-the-art performance and exhibits strong generalization across complex TKGQA settings."
    },
    {
      "title": "GAS: Enhancing Reward-Cost Balance of Generative Model-assisted Offline Safe RL",
      "url": "https://arxiv.org/abs/2602.05323",
      "date": "2026-02-05",
      "authors_text": "Zifan Liu, Xinran Li, Shibo Chen, Jun Zhang",
      "is_highlight": false,
      "score": 75.0,
      "summary": "The GAS algorithm enhances offline safe reinforcement learning by improving trajectory stitching and balancing reward-cost targets through dataset augmentation and novel goal functions.",
      "teaser_image": "https://arxiv.org/html/2602.05323/GAS_details.png",
      "debug_abstract": "Offline Safe Reinforcement Learning (OSRL) aims to learn a policy to achieve high performance in sequential decision-making while satisfying constraints, using only pre-collected datasets. Recent works, inspired by the strong capabilities of Generative Models (GMs), reformulate decision-making in OSRL as a conditional generative process, where GMs generate desirable actions conditioned on predefined reward and cost values. However, GM-assisted methods face two major challenges in OSRL: (1) lacking the ability to &#34;stitch&#34; optimal transitions from suboptimal trajectories within the dataset, and (2) struggling to balance reward targets with cost targets, particularly when they are conflict. To address these issues, we propose Goal-Assisted Stitching (GAS), a novel algorithm designed to enhance stitching capabilities while effectively balancing reward maximization and constraint satisfaction. To enhance the stitching ability, GAS first augments and relabels the dataset at the transition level, enabling the construction of high-quality trajectories from suboptimal ones. GAS also introduces novel goal functions, which estimate the optimal achievable reward and cost goals from the dataset. These goal functions, trained using expectile regression on the relabeled and augmented dataset, allow GAS to accommodate a broader range of reward-cost return pairs and achieve a better tradeoff between reward maximization and constraint satisfaction compared to human-specified values. The estimated goals then guide policy training, ensuring robust performance under constrained settings. Furthermore, to improve training stability and efficiency, we reshape the dataset to achieve a more uniform reward-cost return distribution. Empirical results validate the effectiveness of GAS, demonstrating superior performance in balancing reward maximization and constraint satisfaction compared to existing methods."
    },
    {
      "title": "Invariance on Manifolds: Understanding Robust Visual Representations for Place Recognition",
      "url": "https://arxiv.org/abs/2602.00841",
      "date": "2026-02-05",
      "authors_text": "Jintao Cheng, Weibin Li, Zhijian He, Jin Wu, Chi Man Vong",
      "is_highlight": false,
      "score": 65.0,
      "summary": "This paper presents a training-free Second-Order Geometric Statistics framework for robust visual place recognition, leveraging covariance descriptors on the SPD manifold for effective zero-shot generalization.",
      "teaser_image": "https://arxiv.org/html/2602.00841/x2.png",
      "debug_abstract": "Visual Place Recognition (VPR) demands representations robust to drastic environmental and viewpoint shifts. Current aggregation paradigms, however, either rely on data-hungry supervision or simplistic first-order statistics, often neglecting intrinsic structural correlations. In this work, we propose a Second-Order Geometric Statistics framework that inherently captures geometric stability without training. We conceptualize scenes as covariance descriptors on the Symmetric Positive Definite (SPD) manifold, where perturbations manifest as tractable congruence transformations. By leveraging geometry-aware Riemannian mappings, we project these descriptors into a linearized Euclidean embedding, effectively decoupling signal structure from noise. Our approach introduces a training-free framework built upon fixed, pre-trained backbones, achieving strong zero-shot generalization without parameter updates. Extensive experiments confirm that our method achieves highly competitive performance against state-of-the-art baselines, particularly excelling in challenging zero-shot scenarios."
    }
  ],
  "Cheng Kar-Shun Robotics Institute (CKSRI)": [
    {
      "title": "TKG-Thinker: Towards Dynamic Reasoning over Temporal Knowledge Graphs via Agentic Reinforcement Learning",
      "url": "https://arxiv.org/abs/2602.05818",
      "date": "2026-02-05",
      "authors_text": "Zihao Jiang, Miao Peng, Zhenyan Shan, Wenjie Xu, Ben Liu",
      "is_highlight": false,
      "score": 83.0,
      "summary": "TKG-Thinker enhances temporal knowledge graph question answering by utilizing agentic reinforcement learning for dynamic reasoning, outperforming existing models through autonomous planning and adaptive retrieval.",
      "teaser_image": "https://arxiv.org/html/2602.05818/x2.png",
      "debug_abstract": "Temporal knowledge graph question answering (TKGQA) aims to answer time-sensitive questions by leveraging temporal knowledge bases. While Large Language Models (LLMs) demonstrate significant potential in TKGQA, current prompting strategies constrain their efficacy in two primary ways. First, they are prone to reasoning hallucinations under complex temporal constraints. Second, static prompting limits model autonomy and generalization, as it lack optimization through dynamic interaction with temporal knowledge graphs (TKGs) environments. To address these limitations, we propose \\textbf{TKG-Thinker}, a novel agent equipped with autonomous planning and adaptive retrieval capabilities for reasoning over TKGs. Specifically, TKG-Thinker performs in-depth temporal reasoning through dynamic multi-turn interactions with TKGs via a dual-training strategy. We first apply Supervised Fine-Tuning (SFT) with chain-of thought data to instill core planning capabilities, followed by a Reinforcement Learning (RL) stage that leverages multi-dimensional rewards to refine reasoning policies under intricate temporal constraints. Experimental results on benchmark datasets with three open-source LLMs show that TKG-Thinker achieves state-of-the-art performance and exhibits strong generalization across complex TKGQA settings."
    },
    {
      "title": "GAS: Enhancing Reward-Cost Balance of Generative Model-assisted Offline Safe RL",
      "url": "https://arxiv.org/abs/2602.05323",
      "date": "2026-02-05",
      "authors_text": "Zifan Liu, Xinran Li, Shibo Chen, Jun Zhang",
      "is_highlight": false,
      "score": 75.0,
      "summary": "The GAS algorithm enhances offline safe reinforcement learning by improving trajectory stitching and balancing reward-cost targets through dataset augmentation and novel goal functions.",
      "teaser_image": "https://arxiv.org/html/2602.05323/GAS_details.png",
      "debug_abstract": "Offline Safe Reinforcement Learning (OSRL) aims to learn a policy to achieve high performance in sequential decision-making while satisfying constraints, using only pre-collected datasets. Recent works, inspired by the strong capabilities of Generative Models (GMs), reformulate decision-making in OSRL as a conditional generative process, where GMs generate desirable actions conditioned on predefined reward and cost values. However, GM-assisted methods face two major challenges in OSRL: (1) lacking the ability to &#34;stitch&#34; optimal transitions from suboptimal trajectories within the dataset, and (2) struggling to balance reward targets with cost targets, particularly when they are conflict. To address these issues, we propose Goal-Assisted Stitching (GAS), a novel algorithm designed to enhance stitching capabilities while effectively balancing reward maximization and constraint satisfaction. To enhance the stitching ability, GAS first augments and relabels the dataset at the transition level, enabling the construction of high-quality trajectories from suboptimal ones. GAS also introduces novel goal functions, which estimate the optimal achievable reward and cost goals from the dataset. These goal functions, trained using expectile regression on the relabeled and augmented dataset, allow GAS to accommodate a broader range of reward-cost return pairs and achieve a better tradeoff between reward maximization and constraint satisfaction compared to human-specified values. The estimated goals then guide policy training, ensuring robust performance under constrained settings. Furthermore, to improve training stability and efficiency, we reshape the dataset to achieve a more uniform reward-cost return distribution. Empirical results validate the effectiveness of GAS, demonstrating superior performance in balancing reward maximization and constraint satisfaction compared to existing methods."
    },
    {
      "title": "Invariance on Manifolds: Understanding Robust Visual Representations for Place Recognition",
      "url": "https://arxiv.org/abs/2602.00841",
      "date": "2026-02-05",
      "authors_text": "Jintao Cheng, Weibin Li, Zhijian He, Jin Wu, Chi Man Vong",
      "is_highlight": false,
      "score": 65.0,
      "summary": "This paper presents a training-free Second-Order Geometric Statistics framework for robust visual place recognition, leveraging covariance descriptors on the SPD manifold for effective zero-shot generalization.",
      "teaser_image": "https://arxiv.org/html/2602.00841/x2.png",
      "debug_abstract": "Visual Place Recognition (VPR) demands representations robust to drastic environmental and viewpoint shifts. Current aggregation paradigms, however, either rely on data-hungry supervision or simplistic first-order statistics, often neglecting intrinsic structural correlations. In this work, we propose a Second-Order Geometric Statistics framework that inherently captures geometric stability without training. We conceptualize scenes as covariance descriptors on the Symmetric Positive Definite (SPD) manifold, where perturbations manifest as tractable congruence transformations. By leveraging geometry-aware Riemannian mappings, we project these descriptors into a linearized Euclidean embedding, effectively decoupling signal structure from noise. Our approach introduces a training-free framework built upon fixed, pre-trained backbones, achieving strong zero-shot generalization without parameter updates. Extensive experiments confirm that our method achieves highly competitive performance against state-of-the-art baselines, particularly excelling in challenging zero-shot scenarios."
    }
  ],
  "Affective Intelligence and Robotics Laboratory (AFAR)": [
    {
      "title": "From Vision to Decision: Neuromorphic Control for Autonomous Navigation and Tracking",
      "url": "https://arxiv.org/abs/2602.05683",
      "date": "2026-02-05",
      "authors_text": "Chuwei Wang, Eduardo Sebastián, Amanda Prorok, Anastasia Bizyaeva",
      "is_highlight": false,
      "score": 86.0,
      "summary": "The paper presents a neuromorphic control framework that enables efficient vision-guided navigation and tracking by resolving decision-making indecision through dynamic neuronal processing.",
      "teaser_image": "https://arxiv.org/html/2602.05683/pipelinefig.png",
      "debug_abstract": "Robotic navigation has historically struggled to reconcile reactive, sensor-based control with the decisive capabilities of model-based planners. This duality becomes critical when the absence of a predominant option among goals leads to indecision, challenging reactive systems to break symmetries without computationally-intense planners. We propose a parsimonious neuromorphic control framework that bridges this gap for vision-guided navigation and tracking. Image pixels from an onboard camera are encoded as inputs to dynamic neuronal populations that directly transform visual target excitation into egocentric motion commands. A dynamic bifurcation mechanism resolves indecision by delaying commitment until a critical point induced by the environmental geometry. Inspired by recently proposed mechanistic models of animal cognition and opinion dynamics, the neuromorphic controller provides real-time autonomy with a minimal computational burden, a small number of interpretable parameters, and can be seamlessly integrated with application-specific image processing pipelines. We validate our approach in simulation environments as well as on an experimental quadrotor platform."
    },
    {
      "title": "TangramSR: Can Vision-Language Models Reason in Continuous Geometric Space?",
      "url": "https://arxiv.org/abs/2602.05570",
      "date": "2026-02-05",
      "authors_text": "Yikun Zong, Cheston Tan",
      "is_highlight": false,
      "score": 90.0,
      "summary": "The paper presents TangramSR, a framework enhancing Vision-Language Models' geometric reasoning through iterative self-refinement and reward feedback, significantly improving performance on Tangram tasks.",
      "teaser_image": "https://arxiv.org/html/2602.05570/x1.png",
      "debug_abstract": "Humans excel at spatial reasoning tasks like Tangram puzzle assembly through cognitive processes involving mental rotation, iterative refinement, and visual feedback. Inspired by how humans solve Tangram puzzles through trial-and-error, observation, and correction, we design a framework that models these human cognitive mechanisms. However, comprehensive experiments across five representative Vision-Language Models (VLMs) reveal systematic failures in continuous geometric reasoning: average IoU of only 0.41 on single-piece tasks, dropping to 0.23 on two-piece composition, far below human performance where children can complete Tangram tasks successfully. This paper addresses a fundamental challenge in self-improving AI: can models iteratively refine their predictions at test time without parameter updates? We introduce a test-time self-refinement framework that combines in-context learning (ICL) with reward-guided feedback loops, inspired by human cognitive processes. Our training-free verifier-refiner agent applies recursive refinement loops that iteratively self-refine predictions based on geometric consistency feedback, achieving IoU improvements from 0.63 to 0.932 on medium-triangle cases without any model retraining. This demonstrates that incorporating human-inspired iterative refinement mechanisms through ICL and reward loops can substantially enhance geometric reasoning in VLMs, moving self-improving AI from promise to practice in continuous spatial domains. Our work is available at this anonymous link this https URL."
    },
    {
      "title": "Governance at the Edge of Architecture: Regulating NeuroAI and Neuromorphic Systems",
      "url": "https://arxiv.org/abs/2602.01503",
      "date": "2026-02-04",
      "authors_text": "Afifah Kashif, Abdul Muhsin Hameed, Asim Iqbal",
      "is_highlight": false,
      "score": 50.0,
      "summary": "The paper critiques existing AI governance frameworks for inadequately addressing the unique challenges posed by NeuroAI and neuromorphic systems, advocating for adaptive regulatory approaches.",
      "teaser_image": "https://arxiv.org/html/2602.01503/data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==",
      "debug_abstract": "Current AI governance frameworks, including regulatory benchmarks for accuracy, latency, and energy efficiency, are built for static, centrally trained artificial neural networks on von Neumann hardware. NeuroAI systems, embodied in neuromorphic hardware and implemented via spiking neural networks, break these assumptions. This paper examines the limitations of current AI governance frameworks for NeuroAI, arguing that assurance and audit methods must co-evolve with these architectures, aligning traditional regulatory metrics with the physics, learning dynamics, and embodied efficiency of brain-inspired computation to enable technically grounded assurance."
    }
  ],
  "Bio-Inspired Robotics Laboratory (BIRL)": [
    {
      "title": "From Vision to Decision: Neuromorphic Control for Autonomous Navigation and Tracking",
      "url": "https://arxiv.org/abs/2602.05683",
      "date": "2026-02-05",
      "authors_text": "Chuwei Wang, Eduardo Sebastián, Amanda Prorok, Anastasia Bizyaeva",
      "is_highlight": false,
      "score": 86.0,
      "summary": "The paper presents a neuromorphic control framework that enables efficient vision-guided navigation and tracking by resolving decision-making indecision through dynamic neuronal processing.",
      "teaser_image": "https://arxiv.org/html/2602.05683/pipelinefig.png",
      "debug_abstract": "Robotic navigation has historically struggled to reconcile reactive, sensor-based control with the decisive capabilities of model-based planners. This duality becomes critical when the absence of a predominant option among goals leads to indecision, challenging reactive systems to break symmetries without computationally-intense planners. We propose a parsimonious neuromorphic control framework that bridges this gap for vision-guided navigation and tracking. Image pixels from an onboard camera are encoded as inputs to dynamic neuronal populations that directly transform visual target excitation into egocentric motion commands. A dynamic bifurcation mechanism resolves indecision by delaying commitment until a critical point induced by the environmental geometry. Inspired by recently proposed mechanistic models of animal cognition and opinion dynamics, the neuromorphic controller provides real-time autonomy with a minimal computational burden, a small number of interpretable parameters, and can be seamlessly integrated with application-specific image processing pipelines. We validate our approach in simulation environments as well as on an experimental quadrotor platform."
    },
    {
      "title": "TangramSR: Can Vision-Language Models Reason in Continuous Geometric Space?",
      "url": "https://arxiv.org/abs/2602.05570",
      "date": "2026-02-05",
      "authors_text": "Yikun Zong, Cheston Tan",
      "is_highlight": false,
      "score": 90.0,
      "summary": "The paper presents TangramSR, a framework enhancing Vision-Language Models' geometric reasoning through iterative self-refinement and reward feedback, significantly improving performance on Tangram tasks.",
      "teaser_image": "https://arxiv.org/html/2602.05570/x1.png",
      "debug_abstract": "Humans excel at spatial reasoning tasks like Tangram puzzle assembly through cognitive processes involving mental rotation, iterative refinement, and visual feedback. Inspired by how humans solve Tangram puzzles through trial-and-error, observation, and correction, we design a framework that models these human cognitive mechanisms. However, comprehensive experiments across five representative Vision-Language Models (VLMs) reveal systematic failures in continuous geometric reasoning: average IoU of only 0.41 on single-piece tasks, dropping to 0.23 on two-piece composition, far below human performance where children can complete Tangram tasks successfully. This paper addresses a fundamental challenge in self-improving AI: can models iteratively refine their predictions at test time without parameter updates? We introduce a test-time self-refinement framework that combines in-context learning (ICL) with reward-guided feedback loops, inspired by human cognitive processes. Our training-free verifier-refiner agent applies recursive refinement loops that iteratively self-refine predictions based on geometric consistency feedback, achieving IoU improvements from 0.63 to 0.932 on medium-triangle cases without any model retraining. This demonstrates that incorporating human-inspired iterative refinement mechanisms through ICL and reward loops can substantially enhance geometric reasoning in VLMs, moving self-improving AI from promise to practice in continuous spatial domains. Our work is available at this anonymous link this https URL."
    },
    {
      "title": "Governance at the Edge of Architecture: Regulating NeuroAI and Neuromorphic Systems",
      "url": "https://arxiv.org/abs/2602.01503",
      "date": "2026-02-04",
      "authors_text": "Afifah Kashif, Abdul Muhsin Hameed, Asim Iqbal",
      "is_highlight": false,
      "score": 50.0,
      "summary": "The paper critiques existing AI governance frameworks for inadequately addressing the unique challenges posed by NeuroAI and neuromorphic systems, advocating for adaptive regulatory approaches.",
      "teaser_image": "https://arxiv.org/html/2602.01503/data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==",
      "debug_abstract": "Current AI governance frameworks, including regulatory benchmarks for accuracy, latency, and energy efficiency, are built for static, centrally trained artificial neural networks on von Neumann hardware. NeuroAI systems, embodied in neuromorphic hardware and implemented via spiking neural networks, break these assumptions. This paper examines the limitations of current AI governance frameworks for NeuroAI, arguing that assurance and audit methods must co-evolve with these architectures, aligning traditional regulatory metrics with the physics, learning dynamics, and embodied efficiency of brain-inspired computation to enable technically grounded assurance."
    }
  ],
  "具身智能实验室 (TEA Lab)": [
    {
      "title": "Affordance-Aware Interactive Decision-Making and Execution for Ambiguous Instructions",
      "url": "https://arxiv.org/abs/2602.05273",
      "date": "2026-02-05",
      "authors_text": "Hengxuan Xu, Fengbo Lan, Zhixin Zhao, Shengjie Wang, Mengqiao Liu",
      "is_highlight": false,
      "score": 89.0,
      "summary": "The AIDE framework enhances robot interaction and decision-making under ambiguous instructions by integrating interactive exploration with vision-language reasoning, achieving high task success and execution accuracy.",
      "teaser_image": "https://arxiv.org/html/2602.05273/x2.png",
      "debug_abstract": "Enabling robots to explore and act in unfamiliar environments under ambiguous human instructions by interactively identifying task-relevant objects (e.g., identifying cups or beverages for &#34;I&#39;m thirsty&#34;) remains challenging for existing vision-language model (VLM)-based methods. This challenge stems from inefficient reasoning and the lack of environmental interaction, which hinder real-time task planning and execution. To address this, We propose Affordance-Aware Interactive Decision-Making and Execution for Ambiguous Instructions (AIDE), a dual-stream framework that integrates interactive exploration with vision-language reasoning, where Multi-Stage Inference (MSI) serves as the decision-making stream and Accelerated Decision-Making (ADM) as the execution stream, enabling zero-shot affordance analysis and interpretation of ambiguous instructions. Extensive experiments in simulation and real-world environments show that AIDE achieves the task planning success rate of over 80\\% and more than 95\\% accuracy in closed-loop continuous execution at 10 Hz, outperforming existing VLM-based methods in diverse open-world scenarios."
    },
    {
      "title": "TKG-Thinker: Towards Dynamic Reasoning over Temporal Knowledge Graphs via Agentic Reinforcement Learning",
      "url": "https://arxiv.org/abs/2602.05818",
      "date": "2026-02-05",
      "authors_text": "Zihao Jiang, Miao Peng, Zhenyan Shan, Wenjie Xu, Ben Liu",
      "is_highlight": false,
      "score": 83.0,
      "summary": "TKG-Thinker enhances temporal knowledge graph question answering by utilizing agentic reinforcement learning for dynamic reasoning, outperforming existing models through autonomous planning and adaptive retrieval.",
      "teaser_image": "https://arxiv.org/html/2602.05818/x2.png",
      "debug_abstract": "Temporal knowledge graph question answering (TKGQA) aims to answer time-sensitive questions by leveraging temporal knowledge bases. While Large Language Models (LLMs) demonstrate significant potential in TKGQA, current prompting strategies constrain their efficacy in two primary ways. First, they are prone to reasoning hallucinations under complex temporal constraints. Second, static prompting limits model autonomy and generalization, as it lack optimization through dynamic interaction with temporal knowledge graphs (TKGs) environments. To address these limitations, we propose \\textbf{TKG-Thinker}, a novel agent equipped with autonomous planning and adaptive retrieval capabilities for reasoning over TKGs. Specifically, TKG-Thinker performs in-depth temporal reasoning through dynamic multi-turn interactions with TKGs via a dual-training strategy. We first apply Supervised Fine-Tuning (SFT) with chain-of thought data to instill core planning capabilities, followed by a Reinforcement Learning (RL) stage that leverages multi-dimensional rewards to refine reasoning policies under intricate temporal constraints. Experimental results on benchmark datasets with three open-source LLMs show that TKG-Thinker achieves state-of-the-art performance and exhibits strong generalization across complex TKGQA settings."
    },
    {
      "title": "RL-VLA$^3$: Reinforcement Learning VLA Accelerating via Full Asynchronism",
      "url": "https://arxiv.org/abs/2602.05765",
      "date": "2026-02-05",
      "authors_text": "Zhong Guan, Haoran Sun, Yongjian Guo, Shuai Di, Xiaodong Bai",
      "is_highlight": false,
      "score": 91.0,
      "summary": "The paper introduces RL-VLA$^3$, a fully-asynchronous reinforcement learning framework that significantly enhances training efficiency and throughput for Vision-Language-Action models.",
      "teaser_image": "https://arxiv.org/html/2602.05765/x1.png",
      "debug_abstract": "In recent years, Vision-Language-Action (VLA) models have emerged as a crucial pathway towards general embodied intelligence, yet their training efficiency has become a key bottleneck. Although existing reinforcement learning (RL)-based training frameworks like RLinf can enhance model generalization, they still rely on synchronous execution, leading to severe resource underutilization and throughput limitations during environment interaction, policy generation (rollout), and model update phases (actor). To overcome this challenge, this paper, for the first time, proposes and implements a fully-asynchronous policy training framework encompassing the entire pipeline from environment interaction, rollout generation, to actor policy updates. Systematically drawing inspiration from asynchronous optimization ideas in large model RL, our framework designs a multi-level decoupled architecture. This includes asynchronous parallelization of environment interaction and trajectory collection, streaming execution for policy generation, and decoupled scheduling for training updates. We validated the effectiveness of our method across diverse VLA models and environments. On the LIBERO benchmark, the framework achieves throughput improvements of up to 59.25\\% compared to existing synchronous strategies. When deeply optimizing separation strategies, throughput can be increased by as much as 126.67\\%. We verified the effectiveness of each asynchronous component via ablation studies. Scaling law validation across 8 to 256 GPUs demonstrates our method&#39;s excellent scalability under most conditions."
    },
    {
      "title": "Towards a Science of Collective AI: LLM-based Multi-Agent Systems Need a Transition from Blind Trial-and-Error to Rigorous Science",
      "url": "https://arxiv.org/abs/2602.05289",
      "date": "2026-02-05",
      "authors_text": "Jingru Fan, Dewen Liu, Yufan Dang, Huatao Li, Yuheng Wang",
      "is_highlight": false,
      "score": 70.0,
      "summary": "The paper advocates for a structured scientific framework in Multi-Agent Systems, proposing a collaboration gain metric and factor attribution paradigm to enhance systematic optimization.",
      "teaser_image": "https://arxiv.org/html/2602.05289/section3.jpg",
      "debug_abstract": "Recent advancements in Large Language Models (LLMs) have greatly extended the capabilities of Multi-Agent Systems (MAS), demonstrating significant effectiveness across a wide range of complex and open-ended domains. However, despite this rapid progress, the field still relies heavily on empirical trial-and-error. It lacks a unified and principled scientific framework necessary for systematic optimization and improvement. This bottleneck stems from the ambiguity of attribution: first, the absence of a structured taxonomy of factors leaves researchers restricted to unguided adjustments; second, the lack of a unified metric fails to distinguish genuine collaboration gain from mere resource accumulation. In this paper, we advocate for a transition to design science through an integrated framework. We advocate to establish the collaboration gain metric ($\\Gamma$) as the scientific standard to isolate intrinsic gains from increased budgets. Leveraging $\\Gamma$, we propose a factor attribution paradigm to systematically identify collaboration-driving factors. To support this, we construct a systematic MAS factor library, structuring the design space into control-level presets and information-level dynamics. Ultimately, this framework facilitates the transition from blind experimentation to rigorous science, paving the way towards a true science of Collective AI."
    },
    {
      "title": "MobileManiBench: Simplifying Model Verification for Mobile Manipulation",
      "url": "https://arxiv.org/abs/2602.05233",
      "date": "2026-02-05",
      "authors_text": "Wenbo Wang, Fangyun Wei, QiXiu Li, Xi Chen, Yaobo Liang",
      "is_highlight": false,
      "score": 85.0,
      "summary": "MobileManiBench is a simulation-first framework that enhances verification of vision-language-action models for mobile manipulation through diverse, annotated trajectory generation in realistic environments.",
      "teaser_image": "https://arxiv.org/html/2602.05233/x1.png",
      "debug_abstract": "Vision-language-action models have advanced robotic manipulation but remain constrained by reliance on the large, teleoperation-collected datasets dominated by the static, tabletop scenes. We propose a simulation-first framework to verify VLA architectures before real-world deployment and introduce MobileManiBench, a large-scale benchmark for mobile-based robotic manipulation. Built on NVIDIA Isaac Sim and powered by reinforcement learning, our pipeline autonomously generates diverse manipulation trajectories with rich annotations (language instructions, multi-view RGB-depth-segmentation images, synchronized object/robot states and actions). MobileManiBench features 2 mobile platforms (parallel-gripper and dexterous-hand robots), 2 synchronized cameras (head and right wrist), 630 objects in 20 categories, 5 skills (open, close, pull, push, pick) with over 100 tasks performed in 100 realistic scenes, yielding 300K trajectories. This design enables controlled, scalable studies of robot embodiments, sensing modalities, and policy architectures, accelerating research on data efficiency and generalization. We benchmark representative VLA models and report insights into perception, reasoning, and control in complex simulated environments."
    }
  ],
  "具身视觉与机器人实验室 (EVAR Lab)": [
    {
      "title": "Affordance-Aware Interactive Decision-Making and Execution for Ambiguous Instructions",
      "url": "https://arxiv.org/abs/2602.05273",
      "date": "2026-02-05",
      "authors_text": "Hengxuan Xu, Fengbo Lan, Zhixin Zhao, Shengjie Wang, Mengqiao Liu",
      "is_highlight": false,
      "score": 89.0,
      "summary": "The AIDE framework enhances robot interaction and decision-making under ambiguous instructions by integrating interactive exploration with vision-language reasoning, achieving high task success and execution accuracy.",
      "teaser_image": "https://arxiv.org/html/2602.05273/x2.png",
      "debug_abstract": "Enabling robots to explore and act in unfamiliar environments under ambiguous human instructions by interactively identifying task-relevant objects (e.g., identifying cups or beverages for &#34;I&#39;m thirsty&#34;) remains challenging for existing vision-language model (VLM)-based methods. This challenge stems from inefficient reasoning and the lack of environmental interaction, which hinder real-time task planning and execution. To address this, We propose Affordance-Aware Interactive Decision-Making and Execution for Ambiguous Instructions (AIDE), a dual-stream framework that integrates interactive exploration with vision-language reasoning, where Multi-Stage Inference (MSI) serves as the decision-making stream and Accelerated Decision-Making (ADM) as the execution stream, enabling zero-shot affordance analysis and interpretation of ambiguous instructions. Extensive experiments in simulation and real-world environments show that AIDE achieves the task planning success rate of over 80\\% and more than 95\\% accuracy in closed-loop continuous execution at 10 Hz, outperforming existing VLM-based methods in diverse open-world scenarios."
    },
    {
      "title": "TKG-Thinker: Towards Dynamic Reasoning over Temporal Knowledge Graphs via Agentic Reinforcement Learning",
      "url": "https://arxiv.org/abs/2602.05818",
      "date": "2026-02-05",
      "authors_text": "Zihao Jiang, Miao Peng, Zhenyan Shan, Wenjie Xu, Ben Liu",
      "is_highlight": false,
      "score": 83.0,
      "summary": "TKG-Thinker enhances temporal knowledge graph question answering by utilizing agentic reinforcement learning for dynamic reasoning, outperforming existing models through autonomous planning and adaptive retrieval.",
      "teaser_image": "https://arxiv.org/html/2602.05818/x2.png",
      "debug_abstract": "Temporal knowledge graph question answering (TKGQA) aims to answer time-sensitive questions by leveraging temporal knowledge bases. While Large Language Models (LLMs) demonstrate significant potential in TKGQA, current prompting strategies constrain their efficacy in two primary ways. First, they are prone to reasoning hallucinations under complex temporal constraints. Second, static prompting limits model autonomy and generalization, as it lack optimization through dynamic interaction with temporal knowledge graphs (TKGs) environments. To address these limitations, we propose \\textbf{TKG-Thinker}, a novel agent equipped with autonomous planning and adaptive retrieval capabilities for reasoning over TKGs. Specifically, TKG-Thinker performs in-depth temporal reasoning through dynamic multi-turn interactions with TKGs via a dual-training strategy. We first apply Supervised Fine-Tuning (SFT) with chain-of thought data to instill core planning capabilities, followed by a Reinforcement Learning (RL) stage that leverages multi-dimensional rewards to refine reasoning policies under intricate temporal constraints. Experimental results on benchmark datasets with three open-source LLMs show that TKG-Thinker achieves state-of-the-art performance and exhibits strong generalization across complex TKGQA settings."
    },
    {
      "title": "RL-VLA$^3$: Reinforcement Learning VLA Accelerating via Full Asynchronism",
      "url": "https://arxiv.org/abs/2602.05765",
      "date": "2026-02-05",
      "authors_text": "Zhong Guan, Haoran Sun, Yongjian Guo, Shuai Di, Xiaodong Bai",
      "is_highlight": false,
      "score": 91.0,
      "summary": "The paper introduces RL-VLA$^3$, a fully-asynchronous reinforcement learning framework that significantly enhances training efficiency and throughput for Vision-Language-Action models.",
      "teaser_image": "https://arxiv.org/html/2602.05765/x1.png",
      "debug_abstract": "In recent years, Vision-Language-Action (VLA) models have emerged as a crucial pathway towards general embodied intelligence, yet their training efficiency has become a key bottleneck. Although existing reinforcement learning (RL)-based training frameworks like RLinf can enhance model generalization, they still rely on synchronous execution, leading to severe resource underutilization and throughput limitations during environment interaction, policy generation (rollout), and model update phases (actor). To overcome this challenge, this paper, for the first time, proposes and implements a fully-asynchronous policy training framework encompassing the entire pipeline from environment interaction, rollout generation, to actor policy updates. Systematically drawing inspiration from asynchronous optimization ideas in large model RL, our framework designs a multi-level decoupled architecture. This includes asynchronous parallelization of environment interaction and trajectory collection, streaming execution for policy generation, and decoupled scheduling for training updates. We validated the effectiveness of our method across diverse VLA models and environments. On the LIBERO benchmark, the framework achieves throughput improvements of up to 59.25\\% compared to existing synchronous strategies. When deeply optimizing separation strategies, throughput can be increased by as much as 126.67\\%. We verified the effectiveness of each asynchronous component via ablation studies. Scaling law validation across 8 to 256 GPUs demonstrates our method&#39;s excellent scalability under most conditions."
    },
    {
      "title": "Towards a Science of Collective AI: LLM-based Multi-Agent Systems Need a Transition from Blind Trial-and-Error to Rigorous Science",
      "url": "https://arxiv.org/abs/2602.05289",
      "date": "2026-02-05",
      "authors_text": "Jingru Fan, Dewen Liu, Yufan Dang, Huatao Li, Yuheng Wang",
      "is_highlight": false,
      "score": 70.0,
      "summary": "The paper advocates for a structured scientific framework in Multi-Agent Systems, proposing a collaboration gain metric and factor attribution paradigm to enhance systematic optimization.",
      "teaser_image": "https://arxiv.org/html/2602.05289/section3.jpg",
      "debug_abstract": "Recent advancements in Large Language Models (LLMs) have greatly extended the capabilities of Multi-Agent Systems (MAS), demonstrating significant effectiveness across a wide range of complex and open-ended domains. However, despite this rapid progress, the field still relies heavily on empirical trial-and-error. It lacks a unified and principled scientific framework necessary for systematic optimization and improvement. This bottleneck stems from the ambiguity of attribution: first, the absence of a structured taxonomy of factors leaves researchers restricted to unguided adjustments; second, the lack of a unified metric fails to distinguish genuine collaboration gain from mere resource accumulation. In this paper, we advocate for a transition to design science through an integrated framework. We advocate to establish the collaboration gain metric ($\\Gamma$) as the scientific standard to isolate intrinsic gains from increased budgets. Leveraging $\\Gamma$, we propose a factor attribution paradigm to systematically identify collaboration-driving factors. To support this, we construct a systematic MAS factor library, structuring the design space into control-level presets and information-level dynamics. Ultimately, this framework facilitates the transition from blind experimentation to rigorous science, paving the way towards a true science of Collective AI."
    },
    {
      "title": "MobileManiBench: Simplifying Model Verification for Mobile Manipulation",
      "url": "https://arxiv.org/abs/2602.05233",
      "date": "2026-02-05",
      "authors_text": "Wenbo Wang, Fangyun Wei, QiXiu Li, Xi Chen, Yaobo Liang",
      "is_highlight": false,
      "score": 85.0,
      "summary": "MobileManiBench is a simulation-first framework that enhances verification of vision-language-action models for mobile manipulation through diverse, annotated trajectory generation in realistic environments.",
      "teaser_image": "https://arxiv.org/html/2602.05233/x1.png",
      "debug_abstract": "Vision-language-action models have advanced robotic manipulation but remain constrained by reliance on the large, teleoperation-collected datasets dominated by the static, tabletop scenes. We propose a simulation-first framework to verify VLA architectures before real-world deployment and introduce MobileManiBench, a large-scale benchmark for mobile-based robotic manipulation. Built on NVIDIA Isaac Sim and powered by reinforcement learning, our pipeline autonomously generates diverse manipulation trajectories with rich annotations (language instructions, multi-view RGB-depth-segmentation images, synchronized object/robot states and actions). MobileManiBench features 2 mobile platforms (parallel-gripper and dexterous-hand robots), 2 synchronized cameras (head and right wrist), 630 objects in 20 categories, 5 skills (open, close, pull, push, pick) with over 100 tasks performed in 100 realistic scenes, yielding 300K trajectories. This design enables controlled, scalable studies of robot embodiments, sensing modalities, and policy architectures, accelerating research on data efficiency and generalization. We benchmark representative VLA models and report insights into perception, reasoning, and control in complex simulated environments."
    }
  ],
  "机器人控制实验室": [
    {
      "title": "Affordance-Aware Interactive Decision-Making and Execution for Ambiguous Instructions",
      "url": "https://arxiv.org/abs/2602.05273",
      "date": "2026-02-05",
      "authors_text": "Hengxuan Xu, Fengbo Lan, Zhixin Zhao, Shengjie Wang, Mengqiao Liu",
      "is_highlight": false,
      "score": 89.0,
      "summary": "The AIDE framework enhances robot interaction and decision-making under ambiguous instructions by integrating interactive exploration with vision-language reasoning, achieving high task success and execution accuracy.",
      "teaser_image": "https://arxiv.org/html/2602.05273/x2.png",
      "debug_abstract": "Enabling robots to explore and act in unfamiliar environments under ambiguous human instructions by interactively identifying task-relevant objects (e.g., identifying cups or beverages for &#34;I&#39;m thirsty&#34;) remains challenging for existing vision-language model (VLM)-based methods. This challenge stems from inefficient reasoning and the lack of environmental interaction, which hinder real-time task planning and execution. To address this, We propose Affordance-Aware Interactive Decision-Making and Execution for Ambiguous Instructions (AIDE), a dual-stream framework that integrates interactive exploration with vision-language reasoning, where Multi-Stage Inference (MSI) serves as the decision-making stream and Accelerated Decision-Making (ADM) as the execution stream, enabling zero-shot affordance analysis and interpretation of ambiguous instructions. Extensive experiments in simulation and real-world environments show that AIDE achieves the task planning success rate of over 80\\% and more than 95\\% accuracy in closed-loop continuous execution at 10 Hz, outperforming existing VLM-based methods in diverse open-world scenarios."
    },
    {
      "title": "TKG-Thinker: Towards Dynamic Reasoning over Temporal Knowledge Graphs via Agentic Reinforcement Learning",
      "url": "https://arxiv.org/abs/2602.05818",
      "date": "2026-02-05",
      "authors_text": "Zihao Jiang, Miao Peng, Zhenyan Shan, Wenjie Xu, Ben Liu",
      "is_highlight": false,
      "score": 83.0,
      "summary": "TKG-Thinker enhances temporal knowledge graph question answering by utilizing agentic reinforcement learning for dynamic reasoning, outperforming existing models through autonomous planning and adaptive retrieval.",
      "teaser_image": "https://arxiv.org/html/2602.05818/x2.png",
      "debug_abstract": "Temporal knowledge graph question answering (TKGQA) aims to answer time-sensitive questions by leveraging temporal knowledge bases. While Large Language Models (LLMs) demonstrate significant potential in TKGQA, current prompting strategies constrain their efficacy in two primary ways. First, they are prone to reasoning hallucinations under complex temporal constraints. Second, static prompting limits model autonomy and generalization, as it lack optimization through dynamic interaction with temporal knowledge graphs (TKGs) environments. To address these limitations, we propose \\textbf{TKG-Thinker}, a novel agent equipped with autonomous planning and adaptive retrieval capabilities for reasoning over TKGs. Specifically, TKG-Thinker performs in-depth temporal reasoning through dynamic multi-turn interactions with TKGs via a dual-training strategy. We first apply Supervised Fine-Tuning (SFT) with chain-of thought data to instill core planning capabilities, followed by a Reinforcement Learning (RL) stage that leverages multi-dimensional rewards to refine reasoning policies under intricate temporal constraints. Experimental results on benchmark datasets with three open-source LLMs show that TKG-Thinker achieves state-of-the-art performance and exhibits strong generalization across complex TKGQA settings."
    },
    {
      "title": "RL-VLA$^3$: Reinforcement Learning VLA Accelerating via Full Asynchronism",
      "url": "https://arxiv.org/abs/2602.05765",
      "date": "2026-02-05",
      "authors_text": "Zhong Guan, Haoran Sun, Yongjian Guo, Shuai Di, Xiaodong Bai",
      "is_highlight": false,
      "score": 91.0,
      "summary": "The paper introduces RL-VLA$^3$, a fully-asynchronous reinforcement learning framework that significantly enhances training efficiency and throughput for Vision-Language-Action models.",
      "teaser_image": "https://arxiv.org/html/2602.05765/x1.png",
      "debug_abstract": "In recent years, Vision-Language-Action (VLA) models have emerged as a crucial pathway towards general embodied intelligence, yet their training efficiency has become a key bottleneck. Although existing reinforcement learning (RL)-based training frameworks like RLinf can enhance model generalization, they still rely on synchronous execution, leading to severe resource underutilization and throughput limitations during environment interaction, policy generation (rollout), and model update phases (actor). To overcome this challenge, this paper, for the first time, proposes and implements a fully-asynchronous policy training framework encompassing the entire pipeline from environment interaction, rollout generation, to actor policy updates. Systematically drawing inspiration from asynchronous optimization ideas in large model RL, our framework designs a multi-level decoupled architecture. This includes asynchronous parallelization of environment interaction and trajectory collection, streaming execution for policy generation, and decoupled scheduling for training updates. We validated the effectiveness of our method across diverse VLA models and environments. On the LIBERO benchmark, the framework achieves throughput improvements of up to 59.25\\% compared to existing synchronous strategies. When deeply optimizing separation strategies, throughput can be increased by as much as 126.67\\%. We verified the effectiveness of each asynchronous component via ablation studies. Scaling law validation across 8 to 256 GPUs demonstrates our method&#39;s excellent scalability under most conditions."
    },
    {
      "title": "Towards a Science of Collective AI: LLM-based Multi-Agent Systems Need a Transition from Blind Trial-and-Error to Rigorous Science",
      "url": "https://arxiv.org/abs/2602.05289",
      "date": "2026-02-05",
      "authors_text": "Jingru Fan, Dewen Liu, Yufan Dang, Huatao Li, Yuheng Wang",
      "is_highlight": false,
      "score": 70.0,
      "summary": "The paper advocates for a structured scientific framework in Multi-Agent Systems, proposing a collaboration gain metric and factor attribution paradigm to enhance systematic optimization.",
      "teaser_image": "https://arxiv.org/html/2602.05289/section3.jpg",
      "debug_abstract": "Recent advancements in Large Language Models (LLMs) have greatly extended the capabilities of Multi-Agent Systems (MAS), demonstrating significant effectiveness across a wide range of complex and open-ended domains. However, despite this rapid progress, the field still relies heavily on empirical trial-and-error. It lacks a unified and principled scientific framework necessary for systematic optimization and improvement. This bottleneck stems from the ambiguity of attribution: first, the absence of a structured taxonomy of factors leaves researchers restricted to unguided adjustments; second, the lack of a unified metric fails to distinguish genuine collaboration gain from mere resource accumulation. In this paper, we advocate for a transition to design science through an integrated framework. We advocate to establish the collaboration gain metric ($\\Gamma$) as the scientific standard to isolate intrinsic gains from increased budgets. Leveraging $\\Gamma$, we propose a factor attribution paradigm to systematically identify collaboration-driving factors. To support this, we construct a systematic MAS factor library, structuring the design space into control-level presets and information-level dynamics. Ultimately, this framework facilitates the transition from blind experimentation to rigorous science, paving the way towards a true science of Collective AI."
    },
    {
      "title": "MobileManiBench: Simplifying Model Verification for Mobile Manipulation",
      "url": "https://arxiv.org/abs/2602.05233",
      "date": "2026-02-05",
      "authors_text": "Wenbo Wang, Fangyun Wei, QiXiu Li, Xi Chen, Yaobo Liang",
      "is_highlight": false,
      "score": 85.0,
      "summary": "MobileManiBench is a simulation-first framework that enhances verification of vision-language-action models for mobile manipulation through diverse, annotated trajectory generation in realistic environments.",
      "teaser_image": "https://arxiv.org/html/2602.05233/x1.png",
      "debug_abstract": "Vision-language-action models have advanced robotic manipulation but remain constrained by reliance on the large, teleoperation-collected datasets dominated by the static, tabletop scenes. We propose a simulation-first framework to verify VLA architectures before real-world deployment and introduce MobileManiBench, a large-scale benchmark for mobile-based robotic manipulation. Built on NVIDIA Isaac Sim and powered by reinforcement learning, our pipeline autonomously generates diverse manipulation trajectories with rich annotations (language instructions, multi-view RGB-depth-segmentation images, synchronized object/robot states and actions). MobileManiBench features 2 mobile platforms (parallel-gripper and dexterous-hand robots), 2 synchronized cameras (head and right wrist), 630 objects in 20 categories, 5 skills (open, close, pull, push, pick) with over 100 tasks performed in 100 realistic scenes, yielding 300K trajectories. This design enables controlled, scalable studies of robot embodiments, sensing modalities, and policy architectures, accelerating research on data efficiency and generalization. We benchmark representative VLA models and report insights into perception, reasoning, and control in complex simulated environments."
    }
  ],
  "智能系统与机器人实验室 (ISR Lab)": [
    {
      "title": "Affordance-Aware Interactive Decision-Making and Execution for Ambiguous Instructions",
      "url": "https://arxiv.org/abs/2602.05273",
      "date": "2026-02-05",
      "authors_text": "Hengxuan Xu, Fengbo Lan, Zhixin Zhao, Shengjie Wang, Mengqiao Liu",
      "is_highlight": false,
      "score": 89.0,
      "summary": "The AIDE framework enhances robot interaction and decision-making under ambiguous instructions by integrating interactive exploration with vision-language reasoning, achieving high task success and execution accuracy.",
      "teaser_image": "https://arxiv.org/html/2602.05273/x2.png",
      "debug_abstract": "Enabling robots to explore and act in unfamiliar environments under ambiguous human instructions by interactively identifying task-relevant objects (e.g., identifying cups or beverages for &#34;I&#39;m thirsty&#34;) remains challenging for existing vision-language model (VLM)-based methods. This challenge stems from inefficient reasoning and the lack of environmental interaction, which hinder real-time task planning and execution. To address this, We propose Affordance-Aware Interactive Decision-Making and Execution for Ambiguous Instructions (AIDE), a dual-stream framework that integrates interactive exploration with vision-language reasoning, where Multi-Stage Inference (MSI) serves as the decision-making stream and Accelerated Decision-Making (ADM) as the execution stream, enabling zero-shot affordance analysis and interpretation of ambiguous instructions. Extensive experiments in simulation and real-world environments show that AIDE achieves the task planning success rate of over 80\\% and more than 95\\% accuracy in closed-loop continuous execution at 10 Hz, outperforming existing VLM-based methods in diverse open-world scenarios."
    },
    {
      "title": "TKG-Thinker: Towards Dynamic Reasoning over Temporal Knowledge Graphs via Agentic Reinforcement Learning",
      "url": "https://arxiv.org/abs/2602.05818",
      "date": "2026-02-05",
      "authors_text": "Zihao Jiang, Miao Peng, Zhenyan Shan, Wenjie Xu, Ben Liu",
      "is_highlight": false,
      "score": 83.0,
      "summary": "TKG-Thinker enhances temporal knowledge graph question answering by utilizing agentic reinforcement learning for dynamic reasoning, outperforming existing models through autonomous planning and adaptive retrieval.",
      "teaser_image": "https://arxiv.org/html/2602.05818/x2.png",
      "debug_abstract": "Temporal knowledge graph question answering (TKGQA) aims to answer time-sensitive questions by leveraging temporal knowledge bases. While Large Language Models (LLMs) demonstrate significant potential in TKGQA, current prompting strategies constrain their efficacy in two primary ways. First, they are prone to reasoning hallucinations under complex temporal constraints. Second, static prompting limits model autonomy and generalization, as it lack optimization through dynamic interaction with temporal knowledge graphs (TKGs) environments. To address these limitations, we propose \\textbf{TKG-Thinker}, a novel agent equipped with autonomous planning and adaptive retrieval capabilities for reasoning over TKGs. Specifically, TKG-Thinker performs in-depth temporal reasoning through dynamic multi-turn interactions with TKGs via a dual-training strategy. We first apply Supervised Fine-Tuning (SFT) with chain-of thought data to instill core planning capabilities, followed by a Reinforcement Learning (RL) stage that leverages multi-dimensional rewards to refine reasoning policies under intricate temporal constraints. Experimental results on benchmark datasets with three open-source LLMs show that TKG-Thinker achieves state-of-the-art performance and exhibits strong generalization across complex TKGQA settings."
    },
    {
      "title": "RL-VLA$^3$: Reinforcement Learning VLA Accelerating via Full Asynchronism",
      "url": "https://arxiv.org/abs/2602.05765",
      "date": "2026-02-05",
      "authors_text": "Zhong Guan, Haoran Sun, Yongjian Guo, Shuai Di, Xiaodong Bai",
      "is_highlight": false,
      "score": 91.0,
      "summary": "The paper introduces RL-VLA$^3$, a fully-asynchronous reinforcement learning framework that significantly enhances training efficiency and throughput for Vision-Language-Action models.",
      "teaser_image": "https://arxiv.org/html/2602.05765/x1.png",
      "debug_abstract": "In recent years, Vision-Language-Action (VLA) models have emerged as a crucial pathway towards general embodied intelligence, yet their training efficiency has become a key bottleneck. Although existing reinforcement learning (RL)-based training frameworks like RLinf can enhance model generalization, they still rely on synchronous execution, leading to severe resource underutilization and throughput limitations during environment interaction, policy generation (rollout), and model update phases (actor). To overcome this challenge, this paper, for the first time, proposes and implements a fully-asynchronous policy training framework encompassing the entire pipeline from environment interaction, rollout generation, to actor policy updates. Systematically drawing inspiration from asynchronous optimization ideas in large model RL, our framework designs a multi-level decoupled architecture. This includes asynchronous parallelization of environment interaction and trajectory collection, streaming execution for policy generation, and decoupled scheduling for training updates. We validated the effectiveness of our method across diverse VLA models and environments. On the LIBERO benchmark, the framework achieves throughput improvements of up to 59.25\\% compared to existing synchronous strategies. When deeply optimizing separation strategies, throughput can be increased by as much as 126.67\\%. We verified the effectiveness of each asynchronous component via ablation studies. Scaling law validation across 8 to 256 GPUs demonstrates our method&#39;s excellent scalability under most conditions."
    },
    {
      "title": "Towards a Science of Collective AI: LLM-based Multi-Agent Systems Need a Transition from Blind Trial-and-Error to Rigorous Science",
      "url": "https://arxiv.org/abs/2602.05289",
      "date": "2026-02-05",
      "authors_text": "Jingru Fan, Dewen Liu, Yufan Dang, Huatao Li, Yuheng Wang",
      "is_highlight": false,
      "score": 70.0,
      "summary": "The paper advocates for a structured scientific framework in Multi-Agent Systems, proposing a collaboration gain metric and factor attribution paradigm to enhance systematic optimization.",
      "teaser_image": "https://arxiv.org/html/2602.05289/section3.jpg",
      "debug_abstract": "Recent advancements in Large Language Models (LLMs) have greatly extended the capabilities of Multi-Agent Systems (MAS), demonstrating significant effectiveness across a wide range of complex and open-ended domains. However, despite this rapid progress, the field still relies heavily on empirical trial-and-error. It lacks a unified and principled scientific framework necessary for systematic optimization and improvement. This bottleneck stems from the ambiguity of attribution: first, the absence of a structured taxonomy of factors leaves researchers restricted to unguided adjustments; second, the lack of a unified metric fails to distinguish genuine collaboration gain from mere resource accumulation. In this paper, we advocate for a transition to design science through an integrated framework. We advocate to establish the collaboration gain metric ($\\Gamma$) as the scientific standard to isolate intrinsic gains from increased budgets. Leveraging $\\Gamma$, we propose a factor attribution paradigm to systematically identify collaboration-driving factors. To support this, we construct a systematic MAS factor library, structuring the design space into control-level presets and information-level dynamics. Ultimately, this framework facilitates the transition from blind experimentation to rigorous science, paving the way towards a true science of Collective AI."
    },
    {
      "title": "MobileManiBench: Simplifying Model Verification for Mobile Manipulation",
      "url": "https://arxiv.org/abs/2602.05233",
      "date": "2026-02-05",
      "authors_text": "Wenbo Wang, Fangyun Wei, QiXiu Li, Xi Chen, Yaobo Liang",
      "is_highlight": false,
      "score": 85.0,
      "summary": "MobileManiBench is a simulation-first framework that enhances verification of vision-language-action models for mobile manipulation through diverse, annotated trajectory generation in realistic environments.",
      "teaser_image": "https://arxiv.org/html/2602.05233/x1.png",
      "debug_abstract": "Vision-language-action models have advanced robotic manipulation but remain constrained by reliance on the large, teleoperation-collected datasets dominated by the static, tabletop scenes. We propose a simulation-first framework to verify VLA architectures before real-world deployment and introduce MobileManiBench, a large-scale benchmark for mobile-based robotic manipulation. Built on NVIDIA Isaac Sim and powered by reinforcement learning, our pipeline autonomously generates diverse manipulation trajectories with rich annotations (language instructions, multi-view RGB-depth-segmentation images, synchronized object/robot states and actions). MobileManiBench features 2 mobile platforms (parallel-gripper and dexterous-hand robots), 2 synchronized cameras (head and right wrist), 630 objects in 20 categories, 5 skills (open, close, pull, push, pick) with over 100 tasks performed in 100 realistic scenes, yielding 300K trajectories. This design enables controlled, scalable studies of robot embodiments, sensing modalities, and policy architectures, accelerating research on data efficiency and generalization. We benchmark representative VLA models and report insights into perception, reasoning, and control in complex simulated environments."
    }
  ],
  "MARS多模态学习实验室": [
    {
      "title": "Affordance-Aware Interactive Decision-Making and Execution for Ambiguous Instructions",
      "url": "https://arxiv.org/abs/2602.05273",
      "date": "2026-02-05",
      "authors_text": "Hengxuan Xu, Fengbo Lan, Zhixin Zhao, Shengjie Wang, Mengqiao Liu",
      "is_highlight": false,
      "score": 89.0,
      "summary": "The AIDE framework enhances robot interaction and decision-making under ambiguous instructions by integrating interactive exploration with vision-language reasoning, achieving high task success and execution accuracy.",
      "teaser_image": "https://arxiv.org/html/2602.05273/x2.png",
      "debug_abstract": "Enabling robots to explore and act in unfamiliar environments under ambiguous human instructions by interactively identifying task-relevant objects (e.g., identifying cups or beverages for &#34;I&#39;m thirsty&#34;) remains challenging for existing vision-language model (VLM)-based methods. This challenge stems from inefficient reasoning and the lack of environmental interaction, which hinder real-time task planning and execution. To address this, We propose Affordance-Aware Interactive Decision-Making and Execution for Ambiguous Instructions (AIDE), a dual-stream framework that integrates interactive exploration with vision-language reasoning, where Multi-Stage Inference (MSI) serves as the decision-making stream and Accelerated Decision-Making (ADM) as the execution stream, enabling zero-shot affordance analysis and interpretation of ambiguous instructions. Extensive experiments in simulation and real-world environments show that AIDE achieves the task planning success rate of over 80\\% and more than 95\\% accuracy in closed-loop continuous execution at 10 Hz, outperforming existing VLM-based methods in diverse open-world scenarios."
    },
    {
      "title": "TKG-Thinker: Towards Dynamic Reasoning over Temporal Knowledge Graphs via Agentic Reinforcement Learning",
      "url": "https://arxiv.org/abs/2602.05818",
      "date": "2026-02-05",
      "authors_text": "Zihao Jiang, Miao Peng, Zhenyan Shan, Wenjie Xu, Ben Liu",
      "is_highlight": false,
      "score": 83.0,
      "summary": "TKG-Thinker enhances temporal knowledge graph question answering by utilizing agentic reinforcement learning for dynamic reasoning, outperforming existing models through autonomous planning and adaptive retrieval.",
      "teaser_image": "https://arxiv.org/html/2602.05818/x2.png",
      "debug_abstract": "Temporal knowledge graph question answering (TKGQA) aims to answer time-sensitive questions by leveraging temporal knowledge bases. While Large Language Models (LLMs) demonstrate significant potential in TKGQA, current prompting strategies constrain their efficacy in two primary ways. First, they are prone to reasoning hallucinations under complex temporal constraints. Second, static prompting limits model autonomy and generalization, as it lack optimization through dynamic interaction with temporal knowledge graphs (TKGs) environments. To address these limitations, we propose \\textbf{TKG-Thinker}, a novel agent equipped with autonomous planning and adaptive retrieval capabilities for reasoning over TKGs. Specifically, TKG-Thinker performs in-depth temporal reasoning through dynamic multi-turn interactions with TKGs via a dual-training strategy. We first apply Supervised Fine-Tuning (SFT) with chain-of thought data to instill core planning capabilities, followed by a Reinforcement Learning (RL) stage that leverages multi-dimensional rewards to refine reasoning policies under intricate temporal constraints. Experimental results on benchmark datasets with three open-source LLMs show that TKG-Thinker achieves state-of-the-art performance and exhibits strong generalization across complex TKGQA settings."
    },
    {
      "title": "RL-VLA$^3$: Reinforcement Learning VLA Accelerating via Full Asynchronism",
      "url": "https://arxiv.org/abs/2602.05765",
      "date": "2026-02-05",
      "authors_text": "Zhong Guan, Haoran Sun, Yongjian Guo, Shuai Di, Xiaodong Bai",
      "is_highlight": false,
      "score": 91.0,
      "summary": "The paper introduces RL-VLA$^3$, a fully-asynchronous reinforcement learning framework that significantly enhances training efficiency and throughput for Vision-Language-Action models.",
      "teaser_image": "https://arxiv.org/html/2602.05765/x1.png",
      "debug_abstract": "In recent years, Vision-Language-Action (VLA) models have emerged as a crucial pathway towards general embodied intelligence, yet their training efficiency has become a key bottleneck. Although existing reinforcement learning (RL)-based training frameworks like RLinf can enhance model generalization, they still rely on synchronous execution, leading to severe resource underutilization and throughput limitations during environment interaction, policy generation (rollout), and model update phases (actor). To overcome this challenge, this paper, for the first time, proposes and implements a fully-asynchronous policy training framework encompassing the entire pipeline from environment interaction, rollout generation, to actor policy updates. Systematically drawing inspiration from asynchronous optimization ideas in large model RL, our framework designs a multi-level decoupled architecture. This includes asynchronous parallelization of environment interaction and trajectory collection, streaming execution for policy generation, and decoupled scheduling for training updates. We validated the effectiveness of our method across diverse VLA models and environments. On the LIBERO benchmark, the framework achieves throughput improvements of up to 59.25\\% compared to existing synchronous strategies. When deeply optimizing separation strategies, throughput can be increased by as much as 126.67\\%. We verified the effectiveness of each asynchronous component via ablation studies. Scaling law validation across 8 to 256 GPUs demonstrates our method&#39;s excellent scalability under most conditions."
    },
    {
      "title": "Towards a Science of Collective AI: LLM-based Multi-Agent Systems Need a Transition from Blind Trial-and-Error to Rigorous Science",
      "url": "https://arxiv.org/abs/2602.05289",
      "date": "2026-02-05",
      "authors_text": "Jingru Fan, Dewen Liu, Yufan Dang, Huatao Li, Yuheng Wang",
      "is_highlight": false,
      "score": 70.0,
      "summary": "The paper advocates for a structured scientific framework in Multi-Agent Systems, proposing a collaboration gain metric and factor attribution paradigm to enhance systematic optimization.",
      "teaser_image": "https://arxiv.org/html/2602.05289/section3.jpg",
      "debug_abstract": "Recent advancements in Large Language Models (LLMs) have greatly extended the capabilities of Multi-Agent Systems (MAS), demonstrating significant effectiveness across a wide range of complex and open-ended domains. However, despite this rapid progress, the field still relies heavily on empirical trial-and-error. It lacks a unified and principled scientific framework necessary for systematic optimization and improvement. This bottleneck stems from the ambiguity of attribution: first, the absence of a structured taxonomy of factors leaves researchers restricted to unguided adjustments; second, the lack of a unified metric fails to distinguish genuine collaboration gain from mere resource accumulation. In this paper, we advocate for a transition to design science through an integrated framework. We advocate to establish the collaboration gain metric ($\\Gamma$) as the scientific standard to isolate intrinsic gains from increased budgets. Leveraging $\\Gamma$, we propose a factor attribution paradigm to systematically identify collaboration-driving factors. To support this, we construct a systematic MAS factor library, structuring the design space into control-level presets and information-level dynamics. Ultimately, this framework facilitates the transition from blind experimentation to rigorous science, paving the way towards a true science of Collective AI."
    },
    {
      "title": "MobileManiBench: Simplifying Model Verification for Mobile Manipulation",
      "url": "https://arxiv.org/abs/2602.05233",
      "date": "2026-02-05",
      "authors_text": "Wenbo Wang, Fangyun Wei, QiXiu Li, Xi Chen, Yaobo Liang",
      "is_highlight": false,
      "score": 85.0,
      "summary": "MobileManiBench is a simulation-first framework that enhances verification of vision-language-action models for mobile manipulation through diverse, annotated trajectory generation in realistic environments.",
      "teaser_image": "https://arxiv.org/html/2602.05233/x1.png",
      "debug_abstract": "Vision-language-action models have advanced robotic manipulation but remain constrained by reliance on the large, teleoperation-collected datasets dominated by the static, tabletop scenes. We propose a simulation-first framework to verify VLA architectures before real-world deployment and introduce MobileManiBench, a large-scale benchmark for mobile-based robotic manipulation. Built on NVIDIA Isaac Sim and powered by reinforcement learning, our pipeline autonomously generates diverse manipulation trajectories with rich annotations (language instructions, multi-view RGB-depth-segmentation images, synchronized object/robot states and actions). MobileManiBench features 2 mobile platforms (parallel-gripper and dexterous-hand robots), 2 synchronized cameras (head and right wrist), 630 objects in 20 categories, 5 skills (open, close, pull, push, pick) with over 100 tasks performed in 100 realistic scenes, yielding 300K trajectories. This design enables controlled, scalable studies of robot embodiments, sensing modalities, and policy architectures, accelerating research on data efficiency and generalization. We benchmark representative VLA models and report insights into perception, reasoning, and control in complex simulated environments."
    }
  ],
  "智能产业研究院 (AIR)": [
    {
      "title": "Affordance-Aware Interactive Decision-Making and Execution for Ambiguous Instructions",
      "url": "https://arxiv.org/abs/2602.05273",
      "date": "2026-02-05",
      "authors_text": "Hengxuan Xu, Fengbo Lan, Zhixin Zhao, Shengjie Wang, Mengqiao Liu",
      "is_highlight": false,
      "score": 89.0,
      "summary": "The AIDE framework enhances robot interaction and decision-making under ambiguous instructions by integrating interactive exploration with vision-language reasoning, achieving high task success and execution accuracy.",
      "teaser_image": "https://arxiv.org/html/2602.05273/x2.png",
      "debug_abstract": "Enabling robots to explore and act in unfamiliar environments under ambiguous human instructions by interactively identifying task-relevant objects (e.g., identifying cups or beverages for &#34;I&#39;m thirsty&#34;) remains challenging for existing vision-language model (VLM)-based methods. This challenge stems from inefficient reasoning and the lack of environmental interaction, which hinder real-time task planning and execution. To address this, We propose Affordance-Aware Interactive Decision-Making and Execution for Ambiguous Instructions (AIDE), a dual-stream framework that integrates interactive exploration with vision-language reasoning, where Multi-Stage Inference (MSI) serves as the decision-making stream and Accelerated Decision-Making (ADM) as the execution stream, enabling zero-shot affordance analysis and interpretation of ambiguous instructions. Extensive experiments in simulation and real-world environments show that AIDE achieves the task planning success rate of over 80\\% and more than 95\\% accuracy in closed-loop continuous execution at 10 Hz, outperforming existing VLM-based methods in diverse open-world scenarios."
    },
    {
      "title": "TKG-Thinker: Towards Dynamic Reasoning over Temporal Knowledge Graphs via Agentic Reinforcement Learning",
      "url": "https://arxiv.org/abs/2602.05818",
      "date": "2026-02-05",
      "authors_text": "Zihao Jiang, Miao Peng, Zhenyan Shan, Wenjie Xu, Ben Liu",
      "is_highlight": false,
      "score": 83.0,
      "summary": "TKG-Thinker enhances temporal knowledge graph question answering by utilizing agentic reinforcement learning for dynamic reasoning, outperforming existing models through autonomous planning and adaptive retrieval.",
      "teaser_image": "https://arxiv.org/html/2602.05818/x2.png",
      "debug_abstract": "Temporal knowledge graph question answering (TKGQA) aims to answer time-sensitive questions by leveraging temporal knowledge bases. While Large Language Models (LLMs) demonstrate significant potential in TKGQA, current prompting strategies constrain their efficacy in two primary ways. First, they are prone to reasoning hallucinations under complex temporal constraints. Second, static prompting limits model autonomy and generalization, as it lack optimization through dynamic interaction with temporal knowledge graphs (TKGs) environments. To address these limitations, we propose \\textbf{TKG-Thinker}, a novel agent equipped with autonomous planning and adaptive retrieval capabilities for reasoning over TKGs. Specifically, TKG-Thinker performs in-depth temporal reasoning through dynamic multi-turn interactions with TKGs via a dual-training strategy. We first apply Supervised Fine-Tuning (SFT) with chain-of thought data to instill core planning capabilities, followed by a Reinforcement Learning (RL) stage that leverages multi-dimensional rewards to refine reasoning policies under intricate temporal constraints. Experimental results on benchmark datasets with three open-source LLMs show that TKG-Thinker achieves state-of-the-art performance and exhibits strong generalization across complex TKGQA settings."
    },
    {
      "title": "RL-VLA$^3$: Reinforcement Learning VLA Accelerating via Full Asynchronism",
      "url": "https://arxiv.org/abs/2602.05765",
      "date": "2026-02-05",
      "authors_text": "Zhong Guan, Haoran Sun, Yongjian Guo, Shuai Di, Xiaodong Bai",
      "is_highlight": false,
      "score": 91.0,
      "summary": "The paper introduces RL-VLA$^3$, a fully-asynchronous reinforcement learning framework that significantly enhances training efficiency and throughput for Vision-Language-Action models.",
      "teaser_image": "https://arxiv.org/html/2602.05765/x1.png",
      "debug_abstract": "In recent years, Vision-Language-Action (VLA) models have emerged as a crucial pathway towards general embodied intelligence, yet their training efficiency has become a key bottleneck. Although existing reinforcement learning (RL)-based training frameworks like RLinf can enhance model generalization, they still rely on synchronous execution, leading to severe resource underutilization and throughput limitations during environment interaction, policy generation (rollout), and model update phases (actor). To overcome this challenge, this paper, for the first time, proposes and implements a fully-asynchronous policy training framework encompassing the entire pipeline from environment interaction, rollout generation, to actor policy updates. Systematically drawing inspiration from asynchronous optimization ideas in large model RL, our framework designs a multi-level decoupled architecture. This includes asynchronous parallelization of environment interaction and trajectory collection, streaming execution for policy generation, and decoupled scheduling for training updates. We validated the effectiveness of our method across diverse VLA models and environments. On the LIBERO benchmark, the framework achieves throughput improvements of up to 59.25\\% compared to existing synchronous strategies. When deeply optimizing separation strategies, throughput can be increased by as much as 126.67\\%. We verified the effectiveness of each asynchronous component via ablation studies. Scaling law validation across 8 to 256 GPUs demonstrates our method&#39;s excellent scalability under most conditions."
    },
    {
      "title": "Towards a Science of Collective AI: LLM-based Multi-Agent Systems Need a Transition from Blind Trial-and-Error to Rigorous Science",
      "url": "https://arxiv.org/abs/2602.05289",
      "date": "2026-02-05",
      "authors_text": "Jingru Fan, Dewen Liu, Yufan Dang, Huatao Li, Yuheng Wang",
      "is_highlight": false,
      "score": 70.0,
      "summary": "The paper advocates for a structured scientific framework in Multi-Agent Systems, proposing a collaboration gain metric and factor attribution paradigm to enhance systematic optimization.",
      "teaser_image": "https://arxiv.org/html/2602.05289/section3.jpg",
      "debug_abstract": "Recent advancements in Large Language Models (LLMs) have greatly extended the capabilities of Multi-Agent Systems (MAS), demonstrating significant effectiveness across a wide range of complex and open-ended domains. However, despite this rapid progress, the field still relies heavily on empirical trial-and-error. It lacks a unified and principled scientific framework necessary for systematic optimization and improvement. This bottleneck stems from the ambiguity of attribution: first, the absence of a structured taxonomy of factors leaves researchers restricted to unguided adjustments; second, the lack of a unified metric fails to distinguish genuine collaboration gain from mere resource accumulation. In this paper, we advocate for a transition to design science through an integrated framework. We advocate to establish the collaboration gain metric ($\\Gamma$) as the scientific standard to isolate intrinsic gains from increased budgets. Leveraging $\\Gamma$, we propose a factor attribution paradigm to systematically identify collaboration-driving factors. To support this, we construct a systematic MAS factor library, structuring the design space into control-level presets and information-level dynamics. Ultimately, this framework facilitates the transition from blind experimentation to rigorous science, paving the way towards a true science of Collective AI."
    },
    {
      "title": "MobileManiBench: Simplifying Model Verification for Mobile Manipulation",
      "url": "https://arxiv.org/abs/2602.05233",
      "date": "2026-02-05",
      "authors_text": "Wenbo Wang, Fangyun Wei, QiXiu Li, Xi Chen, Yaobo Liang",
      "is_highlight": false,
      "score": 85.0,
      "summary": "MobileManiBench is a simulation-first framework that enhances verification of vision-language-action models for mobile manipulation through diverse, annotated trajectory generation in realistic environments.",
      "teaser_image": "https://arxiv.org/html/2602.05233/x1.png",
      "debug_abstract": "Vision-language-action models have advanced robotic manipulation but remain constrained by reliance on the large, teleoperation-collected datasets dominated by the static, tabletop scenes. We propose a simulation-first framework to verify VLA architectures before real-world deployment and introduce MobileManiBench, a large-scale benchmark for mobile-based robotic manipulation. Built on NVIDIA Isaac Sim and powered by reinforcement learning, our pipeline autonomously generates diverse manipulation trajectories with rich annotations (language instructions, multi-view RGB-depth-segmentation images, synchronized object/robot states and actions). MobileManiBench features 2 mobile platforms (parallel-gripper and dexterous-hand robots), 2 synchronized cameras (head and right wrist), 630 objects in 20 categories, 5 skills (open, close, pull, push, pick) with over 100 tasks performed in 100 realistic scenes, yielding 300K trajectories. This design enables controlled, scalable studies of robot embodiments, sensing modalities, and policy architectures, accelerating research on data efficiency and generalization. We benchmark representative VLA models and report insights into perception, reasoning, and control in complex simulated environments."
    }
  ],
  "智能技术与系统实验室": [
    {
      "title": "Affordance-Aware Interactive Decision-Making and Execution for Ambiguous Instructions",
      "url": "https://arxiv.org/abs/2602.05273",
      "date": "2026-02-05",
      "authors_text": "Hengxuan Xu, Fengbo Lan, Zhixin Zhao, Shengjie Wang, Mengqiao Liu",
      "is_highlight": false,
      "score": 89.0,
      "summary": "The AIDE framework enhances robot interaction and decision-making under ambiguous instructions by integrating interactive exploration with vision-language reasoning, achieving high task success and execution accuracy.",
      "teaser_image": "https://arxiv.org/html/2602.05273/x2.png",
      "debug_abstract": "Enabling robots to explore and act in unfamiliar environments under ambiguous human instructions by interactively identifying task-relevant objects (e.g., identifying cups or beverages for &#34;I&#39;m thirsty&#34;) remains challenging for existing vision-language model (VLM)-based methods. This challenge stems from inefficient reasoning and the lack of environmental interaction, which hinder real-time task planning and execution. To address this, We propose Affordance-Aware Interactive Decision-Making and Execution for Ambiguous Instructions (AIDE), a dual-stream framework that integrates interactive exploration with vision-language reasoning, where Multi-Stage Inference (MSI) serves as the decision-making stream and Accelerated Decision-Making (ADM) as the execution stream, enabling zero-shot affordance analysis and interpretation of ambiguous instructions. Extensive experiments in simulation and real-world environments show that AIDE achieves the task planning success rate of over 80\\% and more than 95\\% accuracy in closed-loop continuous execution at 10 Hz, outperforming existing VLM-based methods in diverse open-world scenarios."
    },
    {
      "title": "TKG-Thinker: Towards Dynamic Reasoning over Temporal Knowledge Graphs via Agentic Reinforcement Learning",
      "url": "https://arxiv.org/abs/2602.05818",
      "date": "2026-02-05",
      "authors_text": "Zihao Jiang, Miao Peng, Zhenyan Shan, Wenjie Xu, Ben Liu",
      "is_highlight": false,
      "score": 83.0,
      "summary": "TKG-Thinker enhances temporal knowledge graph question answering by utilizing agentic reinforcement learning for dynamic reasoning, outperforming existing models through autonomous planning and adaptive retrieval.",
      "teaser_image": "https://arxiv.org/html/2602.05818/x2.png",
      "debug_abstract": "Temporal knowledge graph question answering (TKGQA) aims to answer time-sensitive questions by leveraging temporal knowledge bases. While Large Language Models (LLMs) demonstrate significant potential in TKGQA, current prompting strategies constrain their efficacy in two primary ways. First, they are prone to reasoning hallucinations under complex temporal constraints. Second, static prompting limits model autonomy and generalization, as it lack optimization through dynamic interaction with temporal knowledge graphs (TKGs) environments. To address these limitations, we propose \\textbf{TKG-Thinker}, a novel agent equipped with autonomous planning and adaptive retrieval capabilities for reasoning over TKGs. Specifically, TKG-Thinker performs in-depth temporal reasoning through dynamic multi-turn interactions with TKGs via a dual-training strategy. We first apply Supervised Fine-Tuning (SFT) with chain-of thought data to instill core planning capabilities, followed by a Reinforcement Learning (RL) stage that leverages multi-dimensional rewards to refine reasoning policies under intricate temporal constraints. Experimental results on benchmark datasets with three open-source LLMs show that TKG-Thinker achieves state-of-the-art performance and exhibits strong generalization across complex TKGQA settings."
    },
    {
      "title": "RL-VLA$^3$: Reinforcement Learning VLA Accelerating via Full Asynchronism",
      "url": "https://arxiv.org/abs/2602.05765",
      "date": "2026-02-05",
      "authors_text": "Zhong Guan, Haoran Sun, Yongjian Guo, Shuai Di, Xiaodong Bai",
      "is_highlight": false,
      "score": 91.0,
      "summary": "The paper introduces RL-VLA$^3$, a fully-asynchronous reinforcement learning framework that significantly enhances training efficiency and throughput for Vision-Language-Action models.",
      "teaser_image": "https://arxiv.org/html/2602.05765/x1.png",
      "debug_abstract": "In recent years, Vision-Language-Action (VLA) models have emerged as a crucial pathway towards general embodied intelligence, yet their training efficiency has become a key bottleneck. Although existing reinforcement learning (RL)-based training frameworks like RLinf can enhance model generalization, they still rely on synchronous execution, leading to severe resource underutilization and throughput limitations during environment interaction, policy generation (rollout), and model update phases (actor). To overcome this challenge, this paper, for the first time, proposes and implements a fully-asynchronous policy training framework encompassing the entire pipeline from environment interaction, rollout generation, to actor policy updates. Systematically drawing inspiration from asynchronous optimization ideas in large model RL, our framework designs a multi-level decoupled architecture. This includes asynchronous parallelization of environment interaction and trajectory collection, streaming execution for policy generation, and decoupled scheduling for training updates. We validated the effectiveness of our method across diverse VLA models and environments. On the LIBERO benchmark, the framework achieves throughput improvements of up to 59.25\\% compared to existing synchronous strategies. When deeply optimizing separation strategies, throughput can be increased by as much as 126.67\\%. We verified the effectiveness of each asynchronous component via ablation studies. Scaling law validation across 8 to 256 GPUs demonstrates our method&#39;s excellent scalability under most conditions."
    },
    {
      "title": "Towards a Science of Collective AI: LLM-based Multi-Agent Systems Need a Transition from Blind Trial-and-Error to Rigorous Science",
      "url": "https://arxiv.org/abs/2602.05289",
      "date": "2026-02-05",
      "authors_text": "Jingru Fan, Dewen Liu, Yufan Dang, Huatao Li, Yuheng Wang",
      "is_highlight": false,
      "score": 70.0,
      "summary": "The paper advocates for a structured scientific framework in Multi-Agent Systems, proposing a collaboration gain metric and factor attribution paradigm to enhance systematic optimization.",
      "teaser_image": "https://arxiv.org/html/2602.05289/section3.jpg",
      "debug_abstract": "Recent advancements in Large Language Models (LLMs) have greatly extended the capabilities of Multi-Agent Systems (MAS), demonstrating significant effectiveness across a wide range of complex and open-ended domains. However, despite this rapid progress, the field still relies heavily on empirical trial-and-error. It lacks a unified and principled scientific framework necessary for systematic optimization and improvement. This bottleneck stems from the ambiguity of attribution: first, the absence of a structured taxonomy of factors leaves researchers restricted to unguided adjustments; second, the lack of a unified metric fails to distinguish genuine collaboration gain from mere resource accumulation. In this paper, we advocate for a transition to design science through an integrated framework. We advocate to establish the collaboration gain metric ($\\Gamma$) as the scientific standard to isolate intrinsic gains from increased budgets. Leveraging $\\Gamma$, we propose a factor attribution paradigm to systematically identify collaboration-driving factors. To support this, we construct a systematic MAS factor library, structuring the design space into control-level presets and information-level dynamics. Ultimately, this framework facilitates the transition from blind experimentation to rigorous science, paving the way towards a true science of Collective AI."
    },
    {
      "title": "MobileManiBench: Simplifying Model Verification for Mobile Manipulation",
      "url": "https://arxiv.org/abs/2602.05233",
      "date": "2026-02-05",
      "authors_text": "Wenbo Wang, Fangyun Wei, QiXiu Li, Xi Chen, Yaobo Liang",
      "is_highlight": false,
      "score": 85.0,
      "summary": "MobileManiBench is a simulation-first framework that enhances verification of vision-language-action models for mobile manipulation through diverse, annotated trajectory generation in realistic environments.",
      "teaser_image": "https://arxiv.org/html/2602.05233/x1.png",
      "debug_abstract": "Vision-language-action models have advanced robotic manipulation but remain constrained by reliance on the large, teleoperation-collected datasets dominated by the static, tabletop scenes. We propose a simulation-first framework to verify VLA architectures before real-world deployment and introduce MobileManiBench, a large-scale benchmark for mobile-based robotic manipulation. Built on NVIDIA Isaac Sim and powered by reinforcement learning, our pipeline autonomously generates diverse manipulation trajectories with rich annotations (language instructions, multi-view RGB-depth-segmentation images, synchronized object/robot states and actions). MobileManiBench features 2 mobile platforms (parallel-gripper and dexterous-hand robots), 2 synchronized cameras (head and right wrist), 630 objects in 20 categories, 5 skills (open, close, pull, push, pick) with over 100 tasks performed in 100 realistic scenes, yielding 300K trajectories. This design enables controlled, scalable studies of robot embodiments, sensing modalities, and policy architectures, accelerating research on data efficiency and generalization. We benchmark representative VLA models and report insights into perception, reasoning, and control in complex simulated environments."
    }
  ],
  "情感与认知智能机器人实验室 (ACIR)": [
    {
      "title": "RL-VLA$^3$: Reinforcement Learning VLA Accelerating via Full Asynchronism",
      "url": "https://arxiv.org/abs/2602.05765",
      "date": "2026-02-05",
      "authors_text": "Zhong Guan, Haoran Sun, Yongjian Guo, Shuai Di, Xiaodong Bai",
      "is_highlight": false,
      "score": 91.0,
      "summary": "The paper introduces RL-VLA$^3$, a fully-asynchronous reinforcement learning framework that significantly enhances training efficiency and throughput for Vision-Language-Action models.",
      "teaser_image": "https://arxiv.org/html/2602.05765/x1.png",
      "debug_abstract": "In recent years, Vision-Language-Action (VLA) models have emerged as a crucial pathway towards general embodied intelligence, yet their training efficiency has become a key bottleneck. Although existing reinforcement learning (RL)-based training frameworks like RLinf can enhance model generalization, they still rely on synchronous execution, leading to severe resource underutilization and throughput limitations during environment interaction, policy generation (rollout), and model update phases (actor). To overcome this challenge, this paper, for the first time, proposes and implements a fully-asynchronous policy training framework encompassing the entire pipeline from environment interaction, rollout generation, to actor policy updates. Systematically drawing inspiration from asynchronous optimization ideas in large model RL, our framework designs a multi-level decoupled architecture. This includes asynchronous parallelization of environment interaction and trajectory collection, streaming execution for policy generation, and decoupled scheduling for training updates. We validated the effectiveness of our method across diverse VLA models and environments. On the LIBERO benchmark, the framework achieves throughput improvements of up to 59.25\\% compared to existing synchronous strategies. When deeply optimizing separation strategies, throughput can be increased by as much as 126.67\\%. We verified the effectiveness of each asynchronous component via ablation studies. Scaling law validation across 8 to 256 GPUs demonstrates our method&#39;s excellent scalability under most conditions."
    },
    {
      "title": "Automatic Cognitive Task Generation for In-Situ Evaluation of Embodied Agents",
      "url": "https://arxiv.org/abs/2602.05249",
      "date": "2026-02-05",
      "authors_text": "Xinyi He, Ying Yang, Chuanjian Fu, Sihan Guo, Songchun Zhu",
      "is_highlight": true,
      "score": 80.0,
      "summary": "The paper presents a novel in-situ task generation method for evaluating embodied agents in unseen environments, addressing limitations of existing benchmarks through dynamic, cognition-inspired task creation.",
      "teaser_image": "https://arxiv.org/html/2602.05249/x1.png",
      "debug_abstract": "As general intelligent agents are poised for widespread deployment in diverse households, evaluation tailored to each unique unseen 3D environment has become a critical prerequisite. However, existing benchmarks suffer from severe data contamination and a lack of scene specificity, inadequate for assessing agent capabilities in unseen settings. To address this, we propose a dynamic in-situ task generation method for unseen environments inspired by human cognition. We define tasks through a structured graph representation and construct a two-stage interaction-evolution task generation system for embodied agents (TEA). In the interaction stage, the agent actively interacts with the environment, creating a loop between task execution and generation that allows for continuous task generation. In the evolution stage, task graph modeling allows us to recombine and reuse existing tasks to generate new ones without external data. Experiments across 10 unseen scenes demonstrate that TEA automatically generated 87,876 tasks in two cycles, which human verification confirmed to be physically reasonable and encompassing essential daily cognitive capabilities. Benchmarking SOTA models against humans on our in-situ tasks reveals that models, despite excelling on public benchmarks, perform surprisingly poorly on basic perception tasks, severely lack 3D interaction awareness and show high sensitivity to task types in reasoning. These sobering findings highlight the necessity of in-situ evaluation before deploying agents into real-world human environments."
    }
  ],
  "智元机器人联合实验室 (PKU-Agibot)": [
    {
      "title": "RL-VLA$^3$: Reinforcement Learning VLA Accelerating via Full Asynchronism",
      "url": "https://arxiv.org/abs/2602.05765",
      "date": "2026-02-05",
      "authors_text": "Zhong Guan, Haoran Sun, Yongjian Guo, Shuai Di, Xiaodong Bai",
      "is_highlight": false,
      "score": 91.0,
      "summary": "The paper introduces RL-VLA$^3$, a fully-asynchronous reinforcement learning framework that significantly enhances training efficiency and throughput for Vision-Language-Action models.",
      "teaser_image": "https://arxiv.org/html/2602.05765/x1.png",
      "debug_abstract": "In recent years, Vision-Language-Action (VLA) models have emerged as a crucial pathway towards general embodied intelligence, yet their training efficiency has become a key bottleneck. Although existing reinforcement learning (RL)-based training frameworks like RLinf can enhance model generalization, they still rely on synchronous execution, leading to severe resource underutilization and throughput limitations during environment interaction, policy generation (rollout), and model update phases (actor). To overcome this challenge, this paper, for the first time, proposes and implements a fully-asynchronous policy training framework encompassing the entire pipeline from environment interaction, rollout generation, to actor policy updates. Systematically drawing inspiration from asynchronous optimization ideas in large model RL, our framework designs a multi-level decoupled architecture. This includes asynchronous parallelization of environment interaction and trajectory collection, streaming execution for policy generation, and decoupled scheduling for training updates. We validated the effectiveness of our method across diverse VLA models and environments. On the LIBERO benchmark, the framework achieves throughput improvements of up to 59.25\\% compared to existing synchronous strategies. When deeply optimizing separation strategies, throughput can be increased by as much as 126.67\\%. We verified the effectiveness of each asynchronous component via ablation studies. Scaling law validation across 8 to 256 GPUs demonstrates our method&#39;s excellent scalability under most conditions."
    },
    {
      "title": "Automatic Cognitive Task Generation for In-Situ Evaluation of Embodied Agents",
      "url": "https://arxiv.org/abs/2602.05249",
      "date": "2026-02-05",
      "authors_text": "Xinyi He, Ying Yang, Chuanjian Fu, Sihan Guo, Songchun Zhu",
      "is_highlight": true,
      "score": 80.0,
      "summary": "The paper presents a novel in-situ task generation method for evaluating embodied agents in unseen environments, addressing limitations of existing benchmarks through dynamic, cognition-inspired task creation.",
      "teaser_image": "https://arxiv.org/html/2602.05249/x1.png",
      "debug_abstract": "As general intelligent agents are poised for widespread deployment in diverse households, evaluation tailored to each unique unseen 3D environment has become a critical prerequisite. However, existing benchmarks suffer from severe data contamination and a lack of scene specificity, inadequate for assessing agent capabilities in unseen settings. To address this, we propose a dynamic in-situ task generation method for unseen environments inspired by human cognition. We define tasks through a structured graph representation and construct a two-stage interaction-evolution task generation system for embodied agents (TEA). In the interaction stage, the agent actively interacts with the environment, creating a loop between task execution and generation that allows for continuous task generation. In the evolution stage, task graph modeling allows us to recombine and reuse existing tasks to generate new ones without external data. Experiments across 10 unseen scenes demonstrate that TEA automatically generated 87,876 tasks in two cycles, which human verification confirmed to be physically reasonable and encompassing essential daily cognitive capabilities. Benchmarking SOTA models against humans on our in-situ tasks reveals that models, despite excelling on public benchmarks, perform surprisingly poorly on basic perception tasks, severely lack 3D interaction awareness and show high sensitivity to task types in reasoning. These sobering findings highlight the necessity of in-situ evaluation before deploying agents into real-world human environments."
    }
  ],
  "具身感知与交互实验室 (EPIC Lab)": [
    {
      "title": "RL-VLA$^3$: Reinforcement Learning VLA Accelerating via Full Asynchronism",
      "url": "https://arxiv.org/abs/2602.05765",
      "date": "2026-02-05",
      "authors_text": "Zhong Guan, Haoran Sun, Yongjian Guo, Shuai Di, Xiaodong Bai",
      "is_highlight": false,
      "score": 91.0,
      "summary": "The paper introduces RL-VLA$^3$, a fully-asynchronous reinforcement learning framework that significantly enhances training efficiency and throughput for Vision-Language-Action models.",
      "teaser_image": "https://arxiv.org/html/2602.05765/x1.png",
      "debug_abstract": "In recent years, Vision-Language-Action (VLA) models have emerged as a crucial pathway towards general embodied intelligence, yet their training efficiency has become a key bottleneck. Although existing reinforcement learning (RL)-based training frameworks like RLinf can enhance model generalization, they still rely on synchronous execution, leading to severe resource underutilization and throughput limitations during environment interaction, policy generation (rollout), and model update phases (actor). To overcome this challenge, this paper, for the first time, proposes and implements a fully-asynchronous policy training framework encompassing the entire pipeline from environment interaction, rollout generation, to actor policy updates. Systematically drawing inspiration from asynchronous optimization ideas in large model RL, our framework designs a multi-level decoupled architecture. This includes asynchronous parallelization of environment interaction and trajectory collection, streaming execution for policy generation, and decoupled scheduling for training updates. We validated the effectiveness of our method across diverse VLA models and environments. On the LIBERO benchmark, the framework achieves throughput improvements of up to 59.25\\% compared to existing synchronous strategies. When deeply optimizing separation strategies, throughput can be increased by as much as 126.67\\%. We verified the effectiveness of each asynchronous component via ablation studies. Scaling law validation across 8 to 256 GPUs demonstrates our method&#39;s excellent scalability under most conditions."
    },
    {
      "title": "Automatic Cognitive Task Generation for In-Situ Evaluation of Embodied Agents",
      "url": "https://arxiv.org/abs/2602.05249",
      "date": "2026-02-05",
      "authors_text": "Xinyi He, Ying Yang, Chuanjian Fu, Sihan Guo, Songchun Zhu",
      "is_highlight": true,
      "score": 80.0,
      "summary": "The paper presents a novel in-situ task generation method for evaluating embodied agents in unseen environments, addressing limitations of existing benchmarks through dynamic, cognition-inspired task creation.",
      "teaser_image": "https://arxiv.org/html/2602.05249/x1.png",
      "debug_abstract": "As general intelligent agents are poised for widespread deployment in diverse households, evaluation tailored to each unique unseen 3D environment has become a critical prerequisite. However, existing benchmarks suffer from severe data contamination and a lack of scene specificity, inadequate for assessing agent capabilities in unseen settings. To address this, we propose a dynamic in-situ task generation method for unseen environments inspired by human cognition. We define tasks through a structured graph representation and construct a two-stage interaction-evolution task generation system for embodied agents (TEA). In the interaction stage, the agent actively interacts with the environment, creating a loop between task execution and generation that allows for continuous task generation. In the evolution stage, task graph modeling allows us to recombine and reuse existing tasks to generate new ones without external data. Experiments across 10 unseen scenes demonstrate that TEA automatically generated 87,876 tasks in two cycles, which human verification confirmed to be physically reasonable and encompassing essential daily cognitive capabilities. Benchmarking SOTA models against humans on our in-situ tasks reveals that models, despite excelling on public benchmarks, perform surprisingly poorly on basic perception tasks, severely lack 3D interaction awareness and show high sensitivity to task types in reasoning. These sobering findings highlight the necessity of in-situ evaluation before deploying agents into real-world human environments."
    }
  ],
  "HMI Lab": [
    {
      "title": "RL-VLA$^3$: Reinforcement Learning VLA Accelerating via Full Asynchronism",
      "url": "https://arxiv.org/abs/2602.05765",
      "date": "2026-02-05",
      "authors_text": "Zhong Guan, Haoran Sun, Yongjian Guo, Shuai Di, Xiaodong Bai",
      "is_highlight": false,
      "score": 91.0,
      "summary": "The paper introduces RL-VLA$^3$, a fully-asynchronous reinforcement learning framework that significantly enhances training efficiency and throughput for Vision-Language-Action models.",
      "teaser_image": "https://arxiv.org/html/2602.05765/x1.png",
      "debug_abstract": "In recent years, Vision-Language-Action (VLA) models have emerged as a crucial pathway towards general embodied intelligence, yet their training efficiency has become a key bottleneck. Although existing reinforcement learning (RL)-based training frameworks like RLinf can enhance model generalization, they still rely on synchronous execution, leading to severe resource underutilization and throughput limitations during environment interaction, policy generation (rollout), and model update phases (actor). To overcome this challenge, this paper, for the first time, proposes and implements a fully-asynchronous policy training framework encompassing the entire pipeline from environment interaction, rollout generation, to actor policy updates. Systematically drawing inspiration from asynchronous optimization ideas in large model RL, our framework designs a multi-level decoupled architecture. This includes asynchronous parallelization of environment interaction and trajectory collection, streaming execution for policy generation, and decoupled scheduling for training updates. We validated the effectiveness of our method across diverse VLA models and environments. On the LIBERO benchmark, the framework achieves throughput improvements of up to 59.25\\% compared to existing synchronous strategies. When deeply optimizing separation strategies, throughput can be increased by as much as 126.67\\%. We verified the effectiveness of each asynchronous component via ablation studies. Scaling law validation across 8 to 256 GPUs demonstrates our method&#39;s excellent scalability under most conditions."
    },
    {
      "title": "Automatic Cognitive Task Generation for In-Situ Evaluation of Embodied Agents",
      "url": "https://arxiv.org/abs/2602.05249",
      "date": "2026-02-05",
      "authors_text": "Xinyi He, Ying Yang, Chuanjian Fu, Sihan Guo, Songchun Zhu",
      "is_highlight": true,
      "score": 80.0,
      "summary": "The paper presents a novel in-situ task generation method for evaluating embodied agents in unseen environments, addressing limitations of existing benchmarks through dynamic, cognition-inspired task creation.",
      "teaser_image": "https://arxiv.org/html/2602.05249/x1.png",
      "debug_abstract": "As general intelligent agents are poised for widespread deployment in diverse households, evaluation tailored to each unique unseen 3D environment has become a critical prerequisite. However, existing benchmarks suffer from severe data contamination and a lack of scene specificity, inadequate for assessing agent capabilities in unseen settings. To address this, we propose a dynamic in-situ task generation method for unseen environments inspired by human cognition. We define tasks through a structured graph representation and construct a two-stage interaction-evolution task generation system for embodied agents (TEA). In the interaction stage, the agent actively interacts with the environment, creating a loop between task execution and generation that allows for continuous task generation. In the evolution stage, task graph modeling allows us to recombine and reuse existing tasks to generate new ones without external data. Experiments across 10 unseen scenes demonstrate that TEA automatically generated 87,876 tasks in two cycles, which human verification confirmed to be physically reasonable and encompassing essential daily cognitive capabilities. Benchmarking SOTA models against humans on our in-situ tasks reveals that models, despite excelling on public benchmarks, perform surprisingly poorly on basic perception tasks, severely lack 3D interaction awareness and show high sensitivity to task types in reasoning. These sobering findings highlight the necessity of in-situ evaluation before deploying agents into real-world human environments."
    }
  ],
  "BIGAI": [
    {
      "title": "Automatic Cognitive Task Generation for In-Situ Evaluation of Embodied Agents",
      "url": "https://arxiv.org/abs/2602.05249",
      "date": "2026-02-05",
      "authors_text": "Xinyi He, Ying Yang, Chuanjian Fu, Sihan Guo, Songchun Zhu",
      "is_highlight": true,
      "score": 80.0,
      "summary": "The paper presents a novel in-situ task generation method for evaluating embodied agents in unseen environments, addressing limitations of existing benchmarks through dynamic, cognition-inspired task creation.",
      "teaser_image": "https://arxiv.org/html/2602.05249/x1.png",
      "debug_abstract": "As general intelligent agents are poised for widespread deployment in diverse households, evaluation tailored to each unique unseen 3D environment has become a critical prerequisite. However, existing benchmarks suffer from severe data contamination and a lack of scene specificity, inadequate for assessing agent capabilities in unseen settings. To address this, we propose a dynamic in-situ task generation method for unseen environments inspired by human cognition. We define tasks through a structured graph representation and construct a two-stage interaction-evolution task generation system for embodied agents (TEA). In the interaction stage, the agent actively interacts with the environment, creating a loop between task execution and generation that allows for continuous task generation. In the evolution stage, task graph modeling allows us to recombine and reuse existing tasks to generate new ones without external data. Experiments across 10 unseen scenes demonstrate that TEA automatically generated 87,876 tasks in two cycles, which human verification confirmed to be physically reasonable and encompassing essential daily cognitive capabilities. Benchmarking SOTA models against humans on our in-situ tasks reveals that models, despite excelling on public benchmarks, perform surprisingly poorly on basic perception tasks, severely lack 3D interaction awareness and show high sensitivity to task types in reasoning. These sobering findings highlight the necessity of in-situ evaluation before deploying agents into real-world human environments."
    }
  ],
  "中国科学技术大学": [
    {
      "title": "ROMAN: Reward-Orchestrated Multi-Head Attention Network for Autonomous Driving System Testing",
      "url": "https://arxiv.org/abs/2602.05629",
      "date": "2026-02-05",
      "authors_text": "Jianlei Chi, Yuzhen Wu, Jiaxuan Hou, Xiaodong Zhang, Ming Fan",
      "is_highlight": false,
      "score": 84.0,
      "summary": "ROMAN is a novel scenario generation framework that enhances Autonomous Driving System testing by effectively creating high-risk traffic violation scenarios using multi-head attention and law weighting mechanisms.",
      "teaser_image": "https://arxiv.org/html/2602.05629/x2.png",
      "debug_abstract": "Automated Driving System (ADS) acts as the brain of autonomous vehicles, responsible for their safety and efficiency. Safe deployment requires thorough testing in diverse real-world scenarios and compliance with traffic laws like speed limits, signal obedience, and right-of-way rules. Violations like running red lights or speeding pose severe safety risks. However, current testing approaches face significant challenges: limited ability to generate complex and high-risk law-breaking scenarios, and failing to account for complex interactions involving multiple vehicles and critical situations. To address these challenges, we propose ROMAN, a novel scenario generation approach for ADS testing that combines a multi-head attention network with a traffic law weighting mechanism. ROMAN is designed to generate high-risk violation scenarios to enable more thorough and targeted ADS evaluation. The multi-head attention mechanism models interactions among vehicles, traffic signals, and other factors. The traffic law weighting mechanism implements a workflow that leverages an LLM-based risk weighting module to evaluate violations based on the two dimensions of severity and occurrence. We have evaluated ROMAN by testing the Baidu Apollo ADS within the CARLA simulation platform and conducting extensive experiments to measure its performance. Experimental results demonstrate that ROMAN surpassed state-of-the-art tools ABLE and LawBreaker by achieving 7.91% higher average violation count than ABLE and 55.96% higher than LawBreaker, while also maintaining greater scenario diversity. In addition, only ROMAN successfully generated violation scenarios for every clause of the input traffic laws, enabling it to identify more high-risk violations than existing approaches."
    },
    {
      "title": "RFM-Pose:Reinforcement-Guided Flow Matching for Fast Category-Level 6D Pose Estimation",
      "url": "https://arxiv.org/abs/2602.05257",
      "date": "2026-02-05",
      "authors_text": "Diya He, Qingchen Liu, Cong Zhang, Jiahu Qin",
      "is_highlight": false,
      "score": 88.0,
      "summary": "RFM-Pose introduces a reinforcement learning framework for efficient category-level 6D pose estimation by optimizing sampling through flow-matching and hypothesis evaluation, outperforming existing methods.",
      "teaser_image": "https://arxiv.org/html/2602.05257/method_structure.jpg",
      "debug_abstract": "Object pose estimation is a fundamental problem in computer vision and plays a critical role in virtual reality and embodied intelligence, where agents must understand and interact with objects in 3D space. Recently, score based generative models have to some extent solved the rotational symmetry ambiguity problem in category level pose estimation, but their efficiency remains limited by the high sampling cost of score-based diffusion. In this work, we propose a new framework, RFM-Pose, that accelerates category-level 6D object pose generation while actively evaluating sampled hypotheses. To improve sampling efficiency, we adopt a flow-matching generative model and generate pose candidates along an optimal transport path from a simple prior to the pose distribution. To further refine these candidates, we cast the flow-matching sampling process as a Markov decision process and apply proximal policy optimization to fine-tune the sampling policy. In particular, we interpret the flow field as a learnable policy and map an estimator to a value network, enabling joint optimization of pose generation and hypothesis scoring within a reinforcement learning framework. Experiments on the REAL275 benchmark demonstrate that RFM-Pose achieves favorable performance while significantly reducing computational cost. Moreover, similar to prior work, our approach can be readily adapted to object pose tracking and attains competitive results in this setting."
    }
  ],
  "多智能体与具身智能研究所": [
    {
      "title": "Towards a Science of Collective AI: LLM-based Multi-Agent Systems Need a Transition from Blind Trial-and-Error to Rigorous Science",
      "url": "https://arxiv.org/abs/2602.05289",
      "date": "2026-02-05",
      "authors_text": "Jingru Fan, Dewen Liu, Yufan Dang, Huatao Li, Yuheng Wang",
      "is_highlight": false,
      "score": 70.0,
      "summary": "The paper advocates for a structured scientific framework in Multi-Agent Systems, proposing a collaboration gain metric and factor attribution paradigm to enhance systematic optimization.",
      "teaser_image": "https://arxiv.org/html/2602.05289/section3.jpg",
      "debug_abstract": "Recent advancements in Large Language Models (LLMs) have greatly extended the capabilities of Multi-Agent Systems (MAS), demonstrating significant effectiveness across a wide range of complex and open-ended domains. However, despite this rapid progress, the field still relies heavily on empirical trial-and-error. It lacks a unified and principled scientific framework necessary for systematic optimization and improvement. This bottleneck stems from the ambiguity of attribution: first, the absence of a structured taxonomy of factors leaves researchers restricted to unguided adjustments; second, the lack of a unified metric fails to distinguish genuine collaboration gain from mere resource accumulation. In this paper, we advocate for a transition to design science through an integrated framework. We advocate to establish the collaboration gain metric ($\\Gamma$) as the scientific standard to isolate intrinsic gains from increased budgets. Leveraging $\\Gamma$, we propose a factor attribution paradigm to systematically identify collaboration-driving factors. To support this, we construct a systematic MAS factor library, structuring the design space into control-level presets and information-level dynamics. Ultimately, this framework facilitates the transition from blind experimentation to rigorous science, paving the way towards a true science of Collective AI."
    }
  ],
  "智能机器人与机器视觉(IRMV)实验室": [
    {
      "title": "RoboPaint: From Human Demonstration to Any Robot and Any View",
      "url": "https://arxiv.org/abs/2602.05325",
      "date": "2026-02-05",
      "authors_text": "Jiacheng Fan, Zhiyue Zhao, Yiqian Zhang, Chao Chen, Peide Wang",
      "is_highlight": false,
      "score": 92.0,
      "summary": "The RoboPaint pipeline transforms human demonstrations into robot-executable training data, achieving high success rates in dexterous manipulation without direct teleoperation.",
      "teaser_image": "https://arxiv.org/html/2602.05325/x1.png",
      "debug_abstract": "Acquiring large-scale, high-fidelity robot demonstration data remains a critical bottleneck for scaling Vision-Language-Action (VLA) models in dexterous manipulation. We propose a Real-Sim-Real data collection and data editing pipeline that transforms human demonstrations into robot-executable, environment-specific training data without direct robot teleoperation. Standardized data collection rooms are built to capture multimodal human demonstrations (synchronized 3 RGB-D videos, 11 RGB videos, 29-DoF glove joint angles, and 14-channel tactile signals). Based on these human demonstrations, we introduce a tactile-aware retargeting method that maps human hand states to robot dex-hand states via geometry and force-guided optimization. Then the retargeted robot trajectories are rendered in a photorealistic Isaac Sim environment to build robot training data. Real world experiments have demonstrated: (1) The retargeted dex-hand trajectories achieve an 84\\% success rate across 10 diverse object manipulation tasks. (2) VLA policies (Pi0.5) trained exclusively on our generated data achieve 80\\% average success rate on three representative tasks, i.e., pick-and-place, pushing and pouring. To conclude, robot training data can be efficiently &#34;painted&#34; from human demonstrations using our real-sim-real data pipeline. We offer a scalable, cost-effective alternative to teleoperation with minimal performance loss for complex dexterous manipulation."
    },
    {
      "title": "Towards a Science of Collective AI: LLM-based Multi-Agent Systems Need a Transition from Blind Trial-and-Error to Rigorous Science",
      "url": "https://arxiv.org/abs/2602.05289",
      "date": "2026-02-05",
      "authors_text": "Jingru Fan, Dewen Liu, Yufan Dang, Huatao Li, Yuheng Wang",
      "is_highlight": false,
      "score": 70.0,
      "summary": "The paper advocates for a structured scientific framework in Multi-Agent Systems, proposing a collaboration gain metric and factor attribution paradigm to enhance systematic optimization.",
      "teaser_image": "https://arxiv.org/html/2602.05289/section3.jpg",
      "debug_abstract": "Recent advancements in Large Language Models (LLMs) have greatly extended the capabilities of Multi-Agent Systems (MAS), demonstrating significant effectiveness across a wide range of complex and open-ended domains. However, despite this rapid progress, the field still relies heavily on empirical trial-and-error. It lacks a unified and principled scientific framework necessary for systematic optimization and improvement. This bottleneck stems from the ambiguity of attribution: first, the absence of a structured taxonomy of factors leaves researchers restricted to unguided adjustments; second, the lack of a unified metric fails to distinguish genuine collaboration gain from mere resource accumulation. In this paper, we advocate for a transition to design science through an integrated framework. We advocate to establish the collaboration gain metric ($\\Gamma$) as the scientific standard to isolate intrinsic gains from increased budgets. Leveraging $\\Gamma$, we propose a factor attribution paradigm to systematically identify collaboration-driving factors. To support this, we construct a systematic MAS factor library, structuring the design space into control-level presets and information-level dynamics. Ultimately, this framework facilitates the transition from blind experimentation to rigorous science, paving the way towards a true science of Collective AI."
    }
  ],
  "赵波老师实验室": [
    {
      "title": "RoboPaint: From Human Demonstration to Any Robot and Any View",
      "url": "https://arxiv.org/abs/2602.05325",
      "date": "2026-02-05",
      "authors_text": "Jiacheng Fan, Zhiyue Zhao, Yiqian Zhang, Chao Chen, Peide Wang",
      "is_highlight": false,
      "score": 92.0,
      "summary": "The RoboPaint pipeline transforms human demonstrations into robot-executable training data, achieving high success rates in dexterous manipulation without direct teleoperation.",
      "teaser_image": "https://arxiv.org/html/2602.05325/x1.png",
      "debug_abstract": "Acquiring large-scale, high-fidelity robot demonstration data remains a critical bottleneck for scaling Vision-Language-Action (VLA) models in dexterous manipulation. We propose a Real-Sim-Real data collection and data editing pipeline that transforms human demonstrations into robot-executable, environment-specific training data without direct robot teleoperation. Standardized data collection rooms are built to capture multimodal human demonstrations (synchronized 3 RGB-D videos, 11 RGB videos, 29-DoF glove joint angles, and 14-channel tactile signals). Based on these human demonstrations, we introduce a tactile-aware retargeting method that maps human hand states to robot dex-hand states via geometry and force-guided optimization. Then the retargeted robot trajectories are rendered in a photorealistic Isaac Sim environment to build robot training data. Real world experiments have demonstrated: (1) The retargeted dex-hand trajectories achieve an 84\\% success rate across 10 diverse object manipulation tasks. (2) VLA policies (Pi0.5) trained exclusively on our generated data achieve 80\\% average success rate on three representative tasks, i.e., pick-and-place, pushing and pouring. To conclude, robot training data can be efficiently &#34;painted&#34; from human demonstrations using our real-sim-real data pipeline. We offer a scalable, cost-effective alternative to teleoperation with minimal performance loss for complex dexterous manipulation."
    },
    {
      "title": "Towards a Science of Collective AI: LLM-based Multi-Agent Systems Need a Transition from Blind Trial-and-Error to Rigorous Science",
      "url": "https://arxiv.org/abs/2602.05289",
      "date": "2026-02-05",
      "authors_text": "Jingru Fan, Dewen Liu, Yufan Dang, Huatao Li, Yuheng Wang",
      "is_highlight": false,
      "score": 70.0,
      "summary": "The paper advocates for a structured scientific framework in Multi-Agent Systems, proposing a collaboration gain metric and factor attribution paradigm to enhance systematic optimization.",
      "teaser_image": "https://arxiv.org/html/2602.05289/section3.jpg",
      "debug_abstract": "Recent advancements in Large Language Models (LLMs) have greatly extended the capabilities of Multi-Agent Systems (MAS), demonstrating significant effectiveness across a wide range of complex and open-ended domains. However, despite this rapid progress, the field still relies heavily on empirical trial-and-error. It lacks a unified and principled scientific framework necessary for systematic optimization and improvement. This bottleneck stems from the ambiguity of attribution: first, the absence of a structured taxonomy of factors leaves researchers restricted to unguided adjustments; second, the lack of a unified metric fails to distinguish genuine collaboration gain from mere resource accumulation. In this paper, we advocate for a transition to design science through an integrated framework. We advocate to establish the collaboration gain metric ($\\Gamma$) as the scientific standard to isolate intrinsic gains from increased budgets. Leveraging $\\Gamma$, we propose a factor attribution paradigm to systematically identify collaboration-driving factors. To support this, we construct a systematic MAS factor library, structuring the design space into control-level presets and information-level dynamics. Ultimately, this framework facilitates the transition from blind experimentation to rigorous science, paving the way towards a true science of Collective AI."
    }
  ],
  "智能感知与无人系统实验室": [
    {
      "title": "Towards a Science of Collective AI: LLM-based Multi-Agent Systems Need a Transition from Blind Trial-and-Error to Rigorous Science",
      "url": "https://arxiv.org/abs/2602.05289",
      "date": "2026-02-05",
      "authors_text": "Jingru Fan, Dewen Liu, Yufan Dang, Huatao Li, Yuheng Wang",
      "is_highlight": false,
      "score": 70.0,
      "summary": "The paper advocates for a structured scientific framework in Multi-Agent Systems, proposing a collaboration gain metric and factor attribution paradigm to enhance systematic optimization.",
      "teaser_image": "https://arxiv.org/html/2602.05289/section3.jpg",
      "debug_abstract": "Recent advancements in Large Language Models (LLMs) have greatly extended the capabilities of Multi-Agent Systems (MAS), demonstrating significant effectiveness across a wide range of complex and open-ended domains. However, despite this rapid progress, the field still relies heavily on empirical trial-and-error. It lacks a unified and principled scientific framework necessary for systematic optimization and improvement. This bottleneck stems from the ambiguity of attribution: first, the absence of a structured taxonomy of factors leaves researchers restricted to unguided adjustments; second, the lack of a unified metric fails to distinguish genuine collaboration gain from mere resource accumulation. In this paper, we advocate for a transition to design science through an integrated framework. We advocate to establish the collaboration gain metric ($\\Gamma$) as the scientific standard to isolate intrinsic gains from increased budgets. Leveraging $\\Gamma$, we propose a factor attribution paradigm to systematically identify collaboration-driving factors. To support this, we construct a systematic MAS factor library, structuring the design space into control-level presets and information-level dynamics. Ultimately, this framework facilitates the transition from blind experimentation to rigorous science, paving the way towards a true science of Collective AI."
    }
  ],
  "集群机器人系统实验室 (MagicLab)": [
    {
      "title": "Towards a Science of Collective AI: LLM-based Multi-Agent Systems Need a Transition from Blind Trial-and-Error to Rigorous Science",
      "url": "https://arxiv.org/abs/2602.05289",
      "date": "2026-02-05",
      "authors_text": "Jingru Fan, Dewen Liu, Yufan Dang, Huatao Li, Yuheng Wang",
      "is_highlight": false,
      "score": 70.0,
      "summary": "The paper advocates for a structured scientific framework in Multi-Agent Systems, proposing a collaboration gain metric and factor attribution paradigm to enhance systematic optimization.",
      "teaser_image": "https://arxiv.org/html/2602.05289/section3.jpg",
      "debug_abstract": "Recent advancements in Large Language Models (LLMs) have greatly extended the capabilities of Multi-Agent Systems (MAS), demonstrating significant effectiveness across a wide range of complex and open-ended domains. However, despite this rapid progress, the field still relies heavily on empirical trial-and-error. It lacks a unified and principled scientific framework necessary for systematic optimization and improvement. This bottleneck stems from the ambiguity of attribution: first, the absence of a structured taxonomy of factors leaves researchers restricted to unguided adjustments; second, the lack of a unified metric fails to distinguish genuine collaboration gain from mere resource accumulation. In this paper, we advocate for a transition to design science through an integrated framework. We advocate to establish the collaboration gain metric ($\\Gamma$) as the scientific standard to isolate intrinsic gains from increased budgets. Leveraging $\\Gamma$, we propose a factor attribution paradigm to systematically identify collaboration-driving factors. To support this, we construct a systematic MAS factor library, structuring the design space into control-level presets and information-level dynamics. Ultimately, this framework facilitates the transition from blind experimentation to rigorous science, paving the way towards a true science of Collective AI."
    }
  ],
  "机器视觉与智能学习实验室 (MVIG)": [
    {
      "title": "RoboPaint: From Human Demonstration to Any Robot and Any View",
      "url": "https://arxiv.org/abs/2602.05325",
      "date": "2026-02-05",
      "authors_text": "Jiacheng Fan, Zhiyue Zhao, Yiqian Zhang, Chao Chen, Peide Wang",
      "is_highlight": false,
      "score": 92.0,
      "summary": "The RoboPaint pipeline transforms human demonstrations into robot-executable training data, achieving high success rates in dexterous manipulation without direct teleoperation.",
      "teaser_image": "https://arxiv.org/html/2602.05325/x1.png",
      "debug_abstract": "Acquiring large-scale, high-fidelity robot demonstration data remains a critical bottleneck for scaling Vision-Language-Action (VLA) models in dexterous manipulation. We propose a Real-Sim-Real data collection and data editing pipeline that transforms human demonstrations into robot-executable, environment-specific training data without direct robot teleoperation. Standardized data collection rooms are built to capture multimodal human demonstrations (synchronized 3 RGB-D videos, 11 RGB videos, 29-DoF glove joint angles, and 14-channel tactile signals). Based on these human demonstrations, we introduce a tactile-aware retargeting method that maps human hand states to robot dex-hand states via geometry and force-guided optimization. Then the retargeted robot trajectories are rendered in a photorealistic Isaac Sim environment to build robot training data. Real world experiments have demonstrated: (1) The retargeted dex-hand trajectories achieve an 84\\% success rate across 10 diverse object manipulation tasks. (2) VLA policies (Pi0.5) trained exclusively on our generated data achieve 80\\% average success rate on three representative tasks, i.e., pick-and-place, pushing and pouring. To conclude, robot training data can be efficiently &#34;painted&#34; from human demonstrations using our real-sim-real data pipeline. We offer a scalable, cost-effective alternative to teleoperation with minimal performance loss for complex dexterous manipulation."
    },
    {
      "title": "Towards a Science of Collective AI: LLM-based Multi-Agent Systems Need a Transition from Blind Trial-and-Error to Rigorous Science",
      "url": "https://arxiv.org/abs/2602.05289",
      "date": "2026-02-05",
      "authors_text": "Jingru Fan, Dewen Liu, Yufan Dang, Huatao Li, Yuheng Wang",
      "is_highlight": false,
      "score": 70.0,
      "summary": "The paper advocates for a structured scientific framework in Multi-Agent Systems, proposing a collaboration gain metric and factor attribution paradigm to enhance systematic optimization.",
      "teaser_image": "https://arxiv.org/html/2602.05289/section3.jpg",
      "debug_abstract": "Recent advancements in Large Language Models (LLMs) have greatly extended the capabilities of Multi-Agent Systems (MAS), demonstrating significant effectiveness across a wide range of complex and open-ended domains. However, despite this rapid progress, the field still relies heavily on empirical trial-and-error. It lacks a unified and principled scientific framework necessary for systematic optimization and improvement. This bottleneck stems from the ambiguity of attribution: first, the absence of a structured taxonomy of factors leaves researchers restricted to unguided adjustments; second, the lack of a unified metric fails to distinguish genuine collaboration gain from mere resource accumulation. In this paper, we advocate for a transition to design science through an integrated framework. We advocate to establish the collaboration gain metric ($\\Gamma$) as the scientific standard to isolate intrinsic gains from increased budgets. Leveraging $\\Gamma$, we propose a factor attribution paradigm to systematically identify collaboration-driving factors. To support this, we construct a systematic MAS factor library, structuring the design space into control-level presets and information-level dynamics. Ultimately, this framework facilitates the transition from blind experimentation to rigorous science, paving the way towards a true science of Collective AI."
    }
  ],
  "智能人机交互实验室 (MemX)": [
    {
      "title": "Towards a Science of Collective AI: LLM-based Multi-Agent Systems Need a Transition from Blind Trial-and-Error to Rigorous Science",
      "url": "https://arxiv.org/abs/2602.05289",
      "date": "2026-02-05",
      "authors_text": "Jingru Fan, Dewen Liu, Yufan Dang, Huatao Li, Yuheng Wang",
      "is_highlight": false,
      "score": 70.0,
      "summary": "The paper advocates for a structured scientific framework in Multi-Agent Systems, proposing a collaboration gain metric and factor attribution paradigm to enhance systematic optimization.",
      "teaser_image": "https://arxiv.org/html/2602.05289/section3.jpg",
      "debug_abstract": "Recent advancements in Large Language Models (LLMs) have greatly extended the capabilities of Multi-Agent Systems (MAS), demonstrating significant effectiveness across a wide range of complex and open-ended domains. However, despite this rapid progress, the field still relies heavily on empirical trial-and-error. It lacks a unified and principled scientific framework necessary for systematic optimization and improvement. This bottleneck stems from the ambiguity of attribution: first, the absence of a structured taxonomy of factors leaves researchers restricted to unguided adjustments; second, the lack of a unified metric fails to distinguish genuine collaboration gain from mere resource accumulation. In this paper, we advocate for a transition to design science through an integrated framework. We advocate to establish the collaboration gain metric ($\\Gamma$) as the scientific standard to isolate intrinsic gains from increased budgets. Leveraging $\\Gamma$, we propose a factor attribution paradigm to systematically identify collaboration-driving factors. To support this, we construct a systematic MAS factor library, structuring the design space into control-level presets and information-level dynamics. Ultimately, this framework facilitates the transition from blind experimentation to rigorous science, paving the way towards a true science of Collective AI."
    }
  ],
  "IWIN-FINS实验室": [
    {
      "title": "RoboPaint: From Human Demonstration to Any Robot and Any View",
      "url": "https://arxiv.org/abs/2602.05325",
      "date": "2026-02-05",
      "authors_text": "Jiacheng Fan, Zhiyue Zhao, Yiqian Zhang, Chao Chen, Peide Wang",
      "is_highlight": false,
      "score": 92.0,
      "summary": "The RoboPaint pipeline transforms human demonstrations into robot-executable training data, achieving high success rates in dexterous manipulation without direct teleoperation.",
      "teaser_image": "https://arxiv.org/html/2602.05325/x1.png",
      "debug_abstract": "Acquiring large-scale, high-fidelity robot demonstration data remains a critical bottleneck for scaling Vision-Language-Action (VLA) models in dexterous manipulation. We propose a Real-Sim-Real data collection and data editing pipeline that transforms human demonstrations into robot-executable, environment-specific training data without direct robot teleoperation. Standardized data collection rooms are built to capture multimodal human demonstrations (synchronized 3 RGB-D videos, 11 RGB videos, 29-DoF glove joint angles, and 14-channel tactile signals). Based on these human demonstrations, we introduce a tactile-aware retargeting method that maps human hand states to robot dex-hand states via geometry and force-guided optimization. Then the retargeted robot trajectories are rendered in a photorealistic Isaac Sim environment to build robot training data. Real world experiments have demonstrated: (1) The retargeted dex-hand trajectories achieve an 84\\% success rate across 10 diverse object manipulation tasks. (2) VLA policies (Pi0.5) trained exclusively on our generated data achieve 80\\% average success rate on three representative tasks, i.e., pick-and-place, pushing and pouring. To conclude, robot training data can be efficiently &#34;painted&#34; from human demonstrations using our real-sim-real data pipeline. We offer a scalable, cost-effective alternative to teleoperation with minimal performance loss for complex dexterous manipulation."
    },
    {
      "title": "Towards a Science of Collective AI: LLM-based Multi-Agent Systems Need a Transition from Blind Trial-and-Error to Rigorous Science",
      "url": "https://arxiv.org/abs/2602.05289",
      "date": "2026-02-05",
      "authors_text": "Jingru Fan, Dewen Liu, Yufan Dang, Huatao Li, Yuheng Wang",
      "is_highlight": false,
      "score": 70.0,
      "summary": "The paper advocates for a structured scientific framework in Multi-Agent Systems, proposing a collaboration gain metric and factor attribution paradigm to enhance systematic optimization.",
      "teaser_image": "https://arxiv.org/html/2602.05289/section3.jpg",
      "debug_abstract": "Recent advancements in Large Language Models (LLMs) have greatly extended the capabilities of Multi-Agent Systems (MAS), demonstrating significant effectiveness across a wide range of complex and open-ended domains. However, despite this rapid progress, the field still relies heavily on empirical trial-and-error. It lacks a unified and principled scientific framework necessary for systematic optimization and improvement. This bottleneck stems from the ambiguity of attribution: first, the absence of a structured taxonomy of factors leaves researchers restricted to unguided adjustments; second, the lack of a unified metric fails to distinguish genuine collaboration gain from mere resource accumulation. In this paper, we advocate for a transition to design science through an integrated framework. We advocate to establish the collaboration gain metric ($\\Gamma$) as the scientific standard to isolate intrinsic gains from increased budgets. Leveraging $\\Gamma$, we propose a factor attribution paradigm to systematically identify collaboration-driving factors. To support this, we construct a systematic MAS factor library, structuring the design space into control-level presets and information-level dynamics. Ultimately, this framework facilitates the transition from blind experimentation to rigorous science, paving the way towards a true science of Collective AI."
    }
  ],
  "数字媒体与计算机视觉实验室": [
    {
      "title": "RoboPaint: From Human Demonstration to Any Robot and Any View",
      "url": "https://arxiv.org/abs/2602.05325",
      "date": "2026-02-05",
      "authors_text": "Jiacheng Fan, Zhiyue Zhao, Yiqian Zhang, Chao Chen, Peide Wang",
      "is_highlight": false,
      "score": 92.0,
      "summary": "The RoboPaint pipeline transforms human demonstrations into robot-executable training data, achieving high success rates in dexterous manipulation without direct teleoperation.",
      "teaser_image": "https://arxiv.org/html/2602.05325/x1.png",
      "debug_abstract": "Acquiring large-scale, high-fidelity robot demonstration data remains a critical bottleneck for scaling Vision-Language-Action (VLA) models in dexterous manipulation. We propose a Real-Sim-Real data collection and data editing pipeline that transforms human demonstrations into robot-executable, environment-specific training data without direct robot teleoperation. Standardized data collection rooms are built to capture multimodal human demonstrations (synchronized 3 RGB-D videos, 11 RGB videos, 29-DoF glove joint angles, and 14-channel tactile signals). Based on these human demonstrations, we introduce a tactile-aware retargeting method that maps human hand states to robot dex-hand states via geometry and force-guided optimization. Then the retargeted robot trajectories are rendered in a photorealistic Isaac Sim environment to build robot training data. Real world experiments have demonstrated: (1) The retargeted dex-hand trajectories achieve an 84\\% success rate across 10 diverse object manipulation tasks. (2) VLA policies (Pi0.5) trained exclusively on our generated data achieve 80\\% average success rate on three representative tasks, i.e., pick-and-place, pushing and pouring. To conclude, robot training data can be efficiently &#34;painted&#34; from human demonstrations using our real-sim-real data pipeline. We offer a scalable, cost-effective alternative to teleoperation with minimal performance loss for complex dexterous manipulation."
    },
    {
      "title": "Towards a Science of Collective AI: LLM-based Multi-Agent Systems Need a Transition from Blind Trial-and-Error to Rigorous Science",
      "url": "https://arxiv.org/abs/2602.05289",
      "date": "2026-02-05",
      "authors_text": "Jingru Fan, Dewen Liu, Yufan Dang, Huatao Li, Yuheng Wang",
      "is_highlight": false,
      "score": 70.0,
      "summary": "The paper advocates for a structured scientific framework in Multi-Agent Systems, proposing a collaboration gain metric and factor attribution paradigm to enhance systematic optimization.",
      "teaser_image": "https://arxiv.org/html/2602.05289/section3.jpg",
      "debug_abstract": "Recent advancements in Large Language Models (LLMs) have greatly extended the capabilities of Multi-Agent Systems (MAS), demonstrating significant effectiveness across a wide range of complex and open-ended domains. However, despite this rapid progress, the field still relies heavily on empirical trial-and-error. It lacks a unified and principled scientific framework necessary for systematic optimization and improvement. This bottleneck stems from the ambiguity of attribution: first, the absence of a structured taxonomy of factors leaves researchers restricted to unguided adjustments; second, the lack of a unified metric fails to distinguish genuine collaboration gain from mere resource accumulation. In this paper, we advocate for a transition to design science through an integrated framework. We advocate to establish the collaboration gain metric ($\\Gamma$) as the scientific standard to isolate intrinsic gains from increased budgets. Leveraging $\\Gamma$, we propose a factor attribution paradigm to systematically identify collaboration-driving factors. To support this, we construct a systematic MAS factor library, structuring the design space into control-level presets and information-level dynamics. Ultimately, this framework facilitates the transition from blind experimentation to rigorous science, paving the way towards a true science of Collective AI."
    }
  ],
  "ReThinkLab": [
    {
      "title": "RoboPaint: From Human Demonstration to Any Robot and Any View",
      "url": "https://arxiv.org/abs/2602.05325",
      "date": "2026-02-05",
      "authors_text": "Jiacheng Fan, Zhiyue Zhao, Yiqian Zhang, Chao Chen, Peide Wang",
      "is_highlight": false,
      "score": 92.0,
      "summary": "The RoboPaint pipeline transforms human demonstrations into robot-executable training data, achieving high success rates in dexterous manipulation without direct teleoperation.",
      "teaser_image": "https://arxiv.org/html/2602.05325/x1.png",
      "debug_abstract": "Acquiring large-scale, high-fidelity robot demonstration data remains a critical bottleneck for scaling Vision-Language-Action (VLA) models in dexterous manipulation. We propose a Real-Sim-Real data collection and data editing pipeline that transforms human demonstrations into robot-executable, environment-specific training data without direct robot teleoperation. Standardized data collection rooms are built to capture multimodal human demonstrations (synchronized 3 RGB-D videos, 11 RGB videos, 29-DoF glove joint angles, and 14-channel tactile signals). Based on these human demonstrations, we introduce a tactile-aware retargeting method that maps human hand states to robot dex-hand states via geometry and force-guided optimization. Then the retargeted robot trajectories are rendered in a photorealistic Isaac Sim environment to build robot training data. Real world experiments have demonstrated: (1) The retargeted dex-hand trajectories achieve an 84\\% success rate across 10 diverse object manipulation tasks. (2) VLA policies (Pi0.5) trained exclusively on our generated data achieve 80\\% average success rate on three representative tasks, i.e., pick-and-place, pushing and pouring. To conclude, robot training data can be efficiently &#34;painted&#34; from human demonstrations using our real-sim-real data pipeline. We offer a scalable, cost-effective alternative to teleoperation with minimal performance loss for complex dexterous manipulation."
    },
    {
      "title": "Towards a Science of Collective AI: LLM-based Multi-Agent Systems Need a Transition from Blind Trial-and-Error to Rigorous Science",
      "url": "https://arxiv.org/abs/2602.05289",
      "date": "2026-02-05",
      "authors_text": "Jingru Fan, Dewen Liu, Yufan Dang, Huatao Li, Yuheng Wang",
      "is_highlight": false,
      "score": 70.0,
      "summary": "The paper advocates for a structured scientific framework in Multi-Agent Systems, proposing a collaboration gain metric and factor attribution paradigm to enhance systematic optimization.",
      "teaser_image": "https://arxiv.org/html/2602.05289/section3.jpg",
      "debug_abstract": "Recent advancements in Large Language Models (LLMs) have greatly extended the capabilities of Multi-Agent Systems (MAS), demonstrating significant effectiveness across a wide range of complex and open-ended domains. However, despite this rapid progress, the field still relies heavily on empirical trial-and-error. It lacks a unified and principled scientific framework necessary for systematic optimization and improvement. This bottleneck stems from the ambiguity of attribution: first, the absence of a structured taxonomy of factors leaves researchers restricted to unguided adjustments; second, the lack of a unified metric fails to distinguish genuine collaboration gain from mere resource accumulation. In this paper, we advocate for a transition to design science through an integrated framework. We advocate to establish the collaboration gain metric ($\\Gamma$) as the scientific standard to isolate intrinsic gains from increased budgets. Leveraging $\\Gamma$, we propose a factor attribution paradigm to systematically identify collaboration-driving factors. To support this, we construct a systematic MAS factor library, structuring the design space into control-level presets and information-level dynamics. Ultimately, this framework facilitates the transition from blind experimentation to rigorous science, paving the way towards a true science of Collective AI."
    }
  ],
  "浙江大学": [
    {
      "title": "Focus-Scan-Refine: From Human Visual Perception to Efficient Visual Token Pruning",
      "url": "https://arxiv.org/abs/2602.05809",
      "date": "2026-02-05",
      "authors_text": "Enwei Tong, Yuanchao Bai, Yao Zhu, Junjun Jiang, Xianming Liu",
      "is_highlight": false,
      "score": 74.0,
      "summary": "The Focus-Scan-Refine framework enhances visual token pruning in vision-language models by mimicking human perception to improve accuracy and efficiency without increasing token usage.",
      "teaser_image": "https://arxiv.org/html/2602.05809/x4.png",
      "debug_abstract": "Vision-language models (VLMs) often generate massive visual tokens that greatly increase inference latency and memory footprint; while training-free token pruning offers a practical remedy, existing methods still struggle to balance local evidence and global context under aggressive compression. We propose Focus-Scan-Refine (FSR), a human-inspired, plug-and-play pruning framework that mimics how humans answer visual questions: focus on key evidence, then scan globally if needed, and refine the scanned context by aggregating relevant details. FSR first focuses on key evidence by combining visual importance with instruction relevance, avoiding the bias toward visually salient but query-irrelevant regions. It then scans for complementary context conditioned on the focused set, selecting tokens that are most different from the focused evidence. Finally, FSR refines the scanned context by aggregating nearby informative tokens into the scan anchors via similarity-based assignment and score-weighted merging, without increasing the token budget. Extensive experiments across multiple VLM backbones and vision-language benchmarks show that FSR consistently improves the accuracy-efficiency trade-off over existing state-of-the-art pruning methods. The source codes can be found at this https URL"
    },
    {
      "title": "RoboPaint: From Human Demonstration to Any Robot and Any View",
      "url": "https://arxiv.org/abs/2602.05325",
      "date": "2026-02-05",
      "authors_text": "Jiacheng Fan, Zhiyue Zhao, Yiqian Zhang, Chao Chen, Peide Wang",
      "is_highlight": false,
      "score": 92.0,
      "summary": "The RoboPaint pipeline transforms human demonstrations into robot-executable training data, achieving high success rates in dexterous manipulation without direct teleoperation.",
      "teaser_image": "https://arxiv.org/html/2602.05325/x1.png",
      "debug_abstract": "Acquiring large-scale, high-fidelity robot demonstration data remains a critical bottleneck for scaling Vision-Language-Action (VLA) models in dexterous manipulation. We propose a Real-Sim-Real data collection and data editing pipeline that transforms human demonstrations into robot-executable, environment-specific training data without direct robot teleoperation. Standardized data collection rooms are built to capture multimodal human demonstrations (synchronized 3 RGB-D videos, 11 RGB videos, 29-DoF glove joint angles, and 14-channel tactile signals). Based on these human demonstrations, we introduce a tactile-aware retargeting method that maps human hand states to robot dex-hand states via geometry and force-guided optimization. Then the retargeted robot trajectories are rendered in a photorealistic Isaac Sim environment to build robot training data. Real world experiments have demonstrated: (1) The retargeted dex-hand trajectories achieve an 84\\% success rate across 10 diverse object manipulation tasks. (2) VLA policies (Pi0.5) trained exclusively on our generated data achieve 80\\% average success rate on three representative tasks, i.e., pick-and-place, pushing and pouring. To conclude, robot training data can be efficiently &#34;painted&#34; from human demonstrations using our real-sim-real data pipeline. We offer a scalable, cost-effective alternative to teleoperation with minimal performance loss for complex dexterous manipulation."
    }
  ],
  "加州理工-机器人实验室": [
    {
      "title": "Ontology-Driven Robotic Specification Synthesis",
      "url": "https://arxiv.org/abs/2602.05456",
      "date": "2026-02-05",
      "authors_text": "Maksym Figat, Ryan M. Mackey, Michel D. Ingham",
      "is_highlight": false,
      "score": 78.0,
      "summary": "The paper presents RSTM2, an ontology-driven methodology for synthesizing executable robotic specifications, enhancing safety and performance analysis in complex, autonomous multi-robot systems.",
      "teaser_image": "https://arxiv.org/html/2602.05456/ontology_all_perspectives_new.png",
      "debug_abstract": "This paper addresses robotic system engineering for safety- and mission-critical applications by bridging the gap between high-level objectives and formal, executable specifications. The proposed method, Robotic System Task to Model Transformation Methodology (RSTM2) is an ontology-driven, hierarchical approach using stochastic timed Petri nets with resources, enabling Monte Carlo simulations at mission, system, and subsystem levels. A hypothetical case study demonstrates how the RSTM2 method supports architectural trades, resource allocation, and performance analysis under uncertainty. Ontological concepts further enable explainable AI-based assistants, facilitating fully autonomous specification synthesis. The methodology offers particular benefits to complex multi-robot systems, such as the NASA CADRE mission, representing decentralized, resource-aware, and adaptive autonomous systems of the future."
    }
  ],
  "北京航空航天大学": [
    {
      "title": "Virtual-Tube-Based Cooperative Transport Control for Multi-UAV Systems in Constrained Environments",
      "url": "https://arxiv.org/abs/2602.05516",
      "date": "2026-02-05",
      "authors_text": "Runxiao Liu, Pengda Mao, Xiangli Le, Shuang Gu, Yapeng Chen",
      "is_highlight": false,
      "score": 82.0,
      "summary": "This paper presents a virtual tube-based control framework for multi-UAV cooperative transport of cable-suspended loads in constrained environments, ensuring stability and efficiency.",
      "teaser_image": "https://arxiv.org/html/2602.05516/x2.png",
      "debug_abstract": "This paper proposes a novel control framework for cooperative transportation of cable-suspended loads by multiple unmanned aerial vehicles (UAVs) operating in constrained environments. Leveraging virtual tube theory and principles from dissipative systems theory, the framework facilitates efficient multi-UAV collaboration for navigating obstacle-rich areas. The proposed framework offers several key advantages. (1) It achieves tension distribution and coordinated transportation within the UAV-cable-load system with low computational overhead, dynamically adapting UAV configurations based on obstacle layouts to facilitate efficient navigation. (2) By integrating dissipative systems theory, the framework ensures high stability and robustness, essential for complex multi-UAV operations. The effectiveness of the proposed approach is validated through extensive simulations, demonstrating its scalability for large-scale multi-UAV systems. Furthermore, the method is experimentally validated in outdoor scenarios, showcasing its practical feasibility and robustness under real-world conditions."
    }
  ],
  "斯坦福-机器人与具身智能实验室 (REAL)": [
    {
      "title": "Conditional Diffusion Guidance under Hard Constraint: A Stochastic Analysis Approach",
      "url": "https://arxiv.org/abs/2602.05533",
      "date": "2026-02-05",
      "authors_text": "Zhengyi Guo, Wenpin Tang, Renyuan Xu",
      "is_highlight": false,
      "score": 60.0,
      "summary": "This paper presents a conditional diffusion guidance framework that ensures generated samples meet hard constraints using stochastic analysis, enhancing safety in critical applications.",
      "teaser_image": "https://arxiv.org/html/2602.05533/Synthetic_Data/1d_MCL.png",
      "debug_abstract": "We study conditional generation in diffusion models under hard constraints, where generated samples must satisfy prescribed events with probability one. Such constraints arise naturally in safety-critical applications and in rare-event simulation, where soft or reward-based guidance methods offer no guarantee of constraint satisfaction. Building on a probabilistic interpretation of diffusion models, we develop a principled conditional diffusion guidance framework based on Doob&#39;s h-transform, martingale representation and quadratic variation process. Specifically, the resulting guided dynamics augment a pretrained diffusion with an explicit drift correction involving the logarithmic gradient of a conditioning function, without modifying the pretrained score network. Leveraging martingale and quadratic-variation identities, we propose two novel off-policy learning algorithms based on a martingale loss and a martingale-covariation loss to estimate h and its gradient using only trajectories from the pretrained model. We provide non-asymptotic guarantees for the resulting conditional sampler in both total variation and Wasserstein distances, explicitly characterizing the impact of score approximation and guidance estimation errors. Numerical experiments demonstrate the effectiveness of the proposed methods in enforcing hard constraints and generating rare-event samples."
    }
  ],
  "斯坦福-视觉与学习实验室 (SVL)": [
    {
      "title": "Conditional Diffusion Guidance under Hard Constraint: A Stochastic Analysis Approach",
      "url": "https://arxiv.org/abs/2602.05533",
      "date": "2026-02-05",
      "authors_text": "Zhengyi Guo, Wenpin Tang, Renyuan Xu",
      "is_highlight": false,
      "score": 60.0,
      "summary": "This paper presents a conditional diffusion guidance framework that ensures generated samples meet hard constraints using stochastic analysis, enhancing safety in critical applications.",
      "teaser_image": "https://arxiv.org/html/2602.05533/Synthetic_Data/1d_MCL.png",
      "debug_abstract": "We study conditional generation in diffusion models under hard constraints, where generated samples must satisfy prescribed events with probability one. Such constraints arise naturally in safety-critical applications and in rare-event simulation, where soft or reward-based guidance methods offer no guarantee of constraint satisfaction. Building on a probabilistic interpretation of diffusion models, we develop a principled conditional diffusion guidance framework based on Doob&#39;s h-transform, martingale representation and quadratic variation process. Specifically, the resulting guided dynamics augment a pretrained diffusion with an explicit drift correction involving the logarithmic gradient of a conditioning function, without modifying the pretrained score network. Leveraging martingale and quadratic-variation identities, we propose two novel off-policy learning algorithms based on a martingale loss and a martingale-covariation loss to estimate h and its gradient using only trajectories from the pretrained model. We provide non-asymptotic guarantees for the resulting conditional sampler in both total variation and Wasserstein distances, explicitly characterizing the impact of score approximation and guidance estimation errors. Numerical experiments demonstrate the effectiveness of the proposed methods in enforcing hard constraints and generating rare-event samples."
    }
  ],
  "斯坦福-人工智能实验室 (SAIL)": [
    {
      "title": "Conditional Diffusion Guidance under Hard Constraint: A Stochastic Analysis Approach",
      "url": "https://arxiv.org/abs/2602.05533",
      "date": "2026-02-05",
      "authors_text": "Zhengyi Guo, Wenpin Tang, Renyuan Xu",
      "is_highlight": false,
      "score": 60.0,
      "summary": "This paper presents a conditional diffusion guidance framework that ensures generated samples meet hard constraints using stochastic analysis, enhancing safety in critical applications.",
      "teaser_image": "https://arxiv.org/html/2602.05533/Synthetic_Data/1d_MCL.png",
      "debug_abstract": "We study conditional generation in diffusion models under hard constraints, where generated samples must satisfy prescribed events with probability one. Such constraints arise naturally in safety-critical applications and in rare-event simulation, where soft or reward-based guidance methods offer no guarantee of constraint satisfaction. Building on a probabilistic interpretation of diffusion models, we develop a principled conditional diffusion guidance framework based on Doob&#39;s h-transform, martingale representation and quadratic variation process. Specifically, the resulting guided dynamics augment a pretrained diffusion with an explicit drift correction involving the logarithmic gradient of a conditioning function, without modifying the pretrained score network. Leveraging martingale and quadratic-variation identities, we propose two novel off-policy learning algorithms based on a martingale loss and a martingale-covariation loss to estimate h and its gradient using only trajectories from the pretrained model. We provide non-asymptotic guarantees for the resulting conditional sampler in both total variation and Wasserstein distances, explicitly characterizing the impact of score approximation and guidance estimation errors. Numerical experiments demonstrate the effectiveness of the proposed methods in enforcing hard constraints and generating rare-event samples."
    }
  ],
  "Robotics and Embodied Artificial Intelligence Lab (REAL)": [
    {
      "title": "Conditional Diffusion Guidance under Hard Constraint: A Stochastic Analysis Approach",
      "url": "https://arxiv.org/abs/2602.05533",
      "date": "2026-02-05",
      "authors_text": "Zhengyi Guo, Wenpin Tang, Renyuan Xu",
      "is_highlight": false,
      "score": 60.0,
      "summary": "This paper presents a conditional diffusion guidance framework that ensures generated samples meet hard constraints using stochastic analysis, enhancing safety in critical applications.",
      "teaser_image": "https://arxiv.org/html/2602.05533/Synthetic_Data/1d_MCL.png",
      "debug_abstract": "We study conditional generation in diffusion models under hard constraints, where generated samples must satisfy prescribed events with probability one. Such constraints arise naturally in safety-critical applications and in rare-event simulation, where soft or reward-based guidance methods offer no guarantee of constraint satisfaction. Building on a probabilistic interpretation of diffusion models, we develop a principled conditional diffusion guidance framework based on Doob&#39;s h-transform, martingale representation and quadratic variation process. Specifically, the resulting guided dynamics augment a pretrained diffusion with an explicit drift correction involving the logarithmic gradient of a conditioning function, without modifying the pretrained score network. Leveraging martingale and quadratic-variation identities, we propose two novel off-policy learning algorithms based on a martingale loss and a martingale-covariation loss to estimate h and its gradient using only trajectories from the pretrained model. We provide non-asymptotic guarantees for the resulting conditional sampler in both total variation and Wasserstein distances, explicitly characterizing the impact of score approximation and guidance estimation errors. Numerical experiments demonstrate the effectiveness of the proposed methods in enforcing hard constraints and generating rare-event samples."
    }
  ],
  "Stanford AI Lab (SAIL)": [
    {
      "title": "Conditional Diffusion Guidance under Hard Constraint: A Stochastic Analysis Approach",
      "url": "https://arxiv.org/abs/2602.05533",
      "date": "2026-02-05",
      "authors_text": "Zhengyi Guo, Wenpin Tang, Renyuan Xu",
      "is_highlight": false,
      "score": 60.0,
      "summary": "This paper presents a conditional diffusion guidance framework that ensures generated samples meet hard constraints using stochastic analysis, enhancing safety in critical applications.",
      "teaser_image": "https://arxiv.org/html/2602.05533/Synthetic_Data/1d_MCL.png",
      "debug_abstract": "We study conditional generation in diffusion models under hard constraints, where generated samples must satisfy prescribed events with probability one. Such constraints arise naturally in safety-critical applications and in rare-event simulation, where soft or reward-based guidance methods offer no guarantee of constraint satisfaction. Building on a probabilistic interpretation of diffusion models, we develop a principled conditional diffusion guidance framework based on Doob&#39;s h-transform, martingale representation and quadratic variation process. Specifically, the resulting guided dynamics augment a pretrained diffusion with an explicit drift correction involving the logarithmic gradient of a conditioning function, without modifying the pretrained score network. Leveraging martingale and quadratic-variation identities, we propose two novel off-policy learning algorithms based on a martingale loss and a martingale-covariation loss to estimate h and its gradient using only trajectories from the pretrained model. We provide non-asymptotic guarantees for the resulting conditional sampler in both total variation and Wasserstein distances, explicitly characterizing the impact of score approximation and guidance estimation errors. Numerical experiments demonstrate the effectiveness of the proposed methods in enforcing hard constraints and generating rare-event samples."
    }
  ],
  "NUS AI LAB": [
    {
      "title": "EgoPoseVR: Spatiotemporal Multi-Modal Reasoning for Egocentric Full-Body Pose in Virtual Reality",
      "url": "https://arxiv.org/abs/2602.05590",
      "date": "2026-02-05",
      "authors_text": "Haojie Cheng, Shaun Jing Heng Ong, Shaoyu Cai, Aiden Tat Yang Koh, Fuxi Ouyang",
      "is_highlight": false,
      "score": 68.0,
      "summary": "EgoPoseVR is an advanced framework for accurate egocentric full-body pose estimation in virtual reality, integrating headset motion cues and RGB-D data for improved stability and performance.",
      "teaser_image": "https://arxiv.org/html/2602.05590/Figures/Pipeline/Pipeline.jpg",
      "debug_abstract": "Immersive virtual reality (VR) applications demand accurate, temporally coherent full-body pose tracking. Recent head-mounted camera-based approaches show promise in egocentric pose estimation, but encounter challenges when applied to VR head-mounted displays (HMDs), including temporal instability, inaccurate lower-body estimation, and the lack of real-time performance. To address these limitations, we present EgoPoseVR, an end-to-end framework for accurate egocentric full-body pose estimation in VR that integrates headset motion cues with egocentric RGB-D observations through a dual-modality fusion pipeline. A spatiotemporal encoder extracts frame- and joint-level representations, which are fused via cross-attention to fully exploit complementary motion cues across modalities. A kinematic optimization module then imposes constraints from HMD signals, enhancing the accuracy and stability of pose estimation. To facilitate training and evaluation, we introduce a large-scale synthetic dataset of over 1.8 million temporally aligned HMD and RGB-D frames across diverse VR scenarios. Experimental results show that EgoPoseVR outperforms state-of-the-art egocentric pose estimation models. A user study in real-world scenes further shows that EgoPoseVR achieved significantly higher subjective ratings in accuracy, stability, embodiment, and intention for future use compared to baseline methods. These results show that EgoPoseVR enables robust full-body pose tracking, offering a practical solution for accurate VR embodiment without requiring additional body-worn sensors or room-scale tracking systems."
    },
    {
      "title": "A Mixed Reality System for Robust Manikin Localization in Childbirth Training",
      "url": "https://arxiv.org/abs/2602.05588",
      "date": "2026-02-05",
      "authors_text": "Haojie Cheng, Chang Liu, Abhiram Kanneganti, Mahesh Arjandas Choolani, Arundhati Tushar Gosavi",
      "is_highlight": false,
      "score": 77.0,
      "summary": "This paper presents a mixed reality system that enhances childbirth training through robust manikin localization, improving learning outcomes compared to traditional virtual reality methods.",
      "teaser_image": "https://arxiv.org/html/2602.05588/Figures/eps/teaser/0.jpg",
      "debug_abstract": "Opportunities for medical students to gain practical experience in vaginal births are increasingly constrained by shortened clinical rotations, patient reluctance, and the unpredictable nature of labour. To alleviate clinicians&#39; instructional burden and enhance trainees&#39; learning efficiency, we introduce a mixed reality (MR) system for childbirth training that combines virtual guidance with tactile manikin interaction, thereby preserving authentic haptic feedback while enabling independent practice without continuous on-site expert supervision. The system extends the passthrough capability of commercial head-mounted displays (HMDs) by spatially calibrating an external RGB-D camera, allowing real-time visual integration of physical training objects. Building on this capability, we implement a coarse-to-fine localization pipeline that first aligns the maternal manikin with fiducial markers to define a delivery region and then registers the pre-scanned neonatal head within this area. This process enables spatially accurate overlay of virtual guiding hands near the manikin, allowing trainees to follow expert trajectories reinforced by haptic interaction. Experimental evaluations demonstrate that the system achieves accurate and stable manikin localization on a standalone headset, ensuring practical deployment without external computing resources. A large-scale user study involving 83 fourth-year medical students was subsequently conducted to compare MR-based and virtual reality (VR)-based childbirth training. Four senior obstetricians independently assessed performance using standardized criteria. Results showed that MR training achieved significantly higher scores in delivery, post-delivery, and overall task performance, and was consistently preferred by trainees over VR training."
    }
  ],
  "Synteraction Lab": [
    {
      "title": "EgoPoseVR: Spatiotemporal Multi-Modal Reasoning for Egocentric Full-Body Pose in Virtual Reality",
      "url": "https://arxiv.org/abs/2602.05590",
      "date": "2026-02-05",
      "authors_text": "Haojie Cheng, Shaun Jing Heng Ong, Shaoyu Cai, Aiden Tat Yang Koh, Fuxi Ouyang",
      "is_highlight": false,
      "score": 68.0,
      "summary": "EgoPoseVR is an advanced framework for accurate egocentric full-body pose estimation in virtual reality, integrating headset motion cues and RGB-D data for improved stability and performance.",
      "teaser_image": "https://arxiv.org/html/2602.05590/Figures/Pipeline/Pipeline.jpg",
      "debug_abstract": "Immersive virtual reality (VR) applications demand accurate, temporally coherent full-body pose tracking. Recent head-mounted camera-based approaches show promise in egocentric pose estimation, but encounter challenges when applied to VR head-mounted displays (HMDs), including temporal instability, inaccurate lower-body estimation, and the lack of real-time performance. To address these limitations, we present EgoPoseVR, an end-to-end framework for accurate egocentric full-body pose estimation in VR that integrates headset motion cues with egocentric RGB-D observations through a dual-modality fusion pipeline. A spatiotemporal encoder extracts frame- and joint-level representations, which are fused via cross-attention to fully exploit complementary motion cues across modalities. A kinematic optimization module then imposes constraints from HMD signals, enhancing the accuracy and stability of pose estimation. To facilitate training and evaluation, we introduce a large-scale synthetic dataset of over 1.8 million temporally aligned HMD and RGB-D frames across diverse VR scenarios. Experimental results show that EgoPoseVR outperforms state-of-the-art egocentric pose estimation models. A user study in real-world scenes further shows that EgoPoseVR achieved significantly higher subjective ratings in accuracy, stability, embodiment, and intention for future use compared to baseline methods. These results show that EgoPoseVR enables robust full-body pose tracking, offering a practical solution for accurate VR embodiment without requiring additional body-worn sensors or room-scale tracking systems."
    },
    {
      "title": "A Mixed Reality System for Robust Manikin Localization in Childbirth Training",
      "url": "https://arxiv.org/abs/2602.05588",
      "date": "2026-02-05",
      "authors_text": "Haojie Cheng, Chang Liu, Abhiram Kanneganti, Mahesh Arjandas Choolani, Arundhati Tushar Gosavi",
      "is_highlight": false,
      "score": 77.0,
      "summary": "This paper presents a mixed reality system that enhances childbirth training through robust manikin localization, improving learning outcomes compared to traditional virtual reality methods.",
      "teaser_image": "https://arxiv.org/html/2602.05588/Figures/eps/teaser/0.jpg",
      "debug_abstract": "Opportunities for medical students to gain practical experience in vaginal births are increasingly constrained by shortened clinical rotations, patient reluctance, and the unpredictable nature of labour. To alleviate clinicians&#39; instructional burden and enhance trainees&#39; learning efficiency, we introduce a mixed reality (MR) system for childbirth training that combines virtual guidance with tactile manikin interaction, thereby preserving authentic haptic feedback while enabling independent practice without continuous on-site expert supervision. The system extends the passthrough capability of commercial head-mounted displays (HMDs) by spatially calibrating an external RGB-D camera, allowing real-time visual integration of physical training objects. Building on this capability, we implement a coarse-to-fine localization pipeline that first aligns the maternal manikin with fiducial markers to define a delivery region and then registers the pre-scanned neonatal head within this area. This process enables spatially accurate overlay of virtual guiding hands near the manikin, allowing trainees to follow expert trajectories reinforced by haptic interaction. Experimental evaluations demonstrate that the system achieves accurate and stable manikin localization on a standalone headset, ensuring practical deployment without external computing resources. A large-scale user study involving 83 fourth-year medical students was subsequently conducted to compare MR-based and virtual reality (VR)-based childbirth training. Four senior obstetricians independently assessed performance using standardized criteria. Results showed that MR training achieved significantly higher scores in delivery, post-delivery, and overall task performance, and was consistently preferred by trainees over VR training."
    }
  ],
  "Advanced Robotics Centre": [
    {
      "title": "EgoPoseVR: Spatiotemporal Multi-Modal Reasoning for Egocentric Full-Body Pose in Virtual Reality",
      "url": "https://arxiv.org/abs/2602.05590",
      "date": "2026-02-05",
      "authors_text": "Haojie Cheng, Shaun Jing Heng Ong, Shaoyu Cai, Aiden Tat Yang Koh, Fuxi Ouyang",
      "is_highlight": false,
      "score": 68.0,
      "summary": "EgoPoseVR is an advanced framework for accurate egocentric full-body pose estimation in virtual reality, integrating headset motion cues and RGB-D data for improved stability and performance.",
      "teaser_image": "https://arxiv.org/html/2602.05590/Figures/Pipeline/Pipeline.jpg",
      "debug_abstract": "Immersive virtual reality (VR) applications demand accurate, temporally coherent full-body pose tracking. Recent head-mounted camera-based approaches show promise in egocentric pose estimation, but encounter challenges when applied to VR head-mounted displays (HMDs), including temporal instability, inaccurate lower-body estimation, and the lack of real-time performance. To address these limitations, we present EgoPoseVR, an end-to-end framework for accurate egocentric full-body pose estimation in VR that integrates headset motion cues with egocentric RGB-D observations through a dual-modality fusion pipeline. A spatiotemporal encoder extracts frame- and joint-level representations, which are fused via cross-attention to fully exploit complementary motion cues across modalities. A kinematic optimization module then imposes constraints from HMD signals, enhancing the accuracy and stability of pose estimation. To facilitate training and evaluation, we introduce a large-scale synthetic dataset of over 1.8 million temporally aligned HMD and RGB-D frames across diverse VR scenarios. Experimental results show that EgoPoseVR outperforms state-of-the-art egocentric pose estimation models. A user study in real-world scenes further shows that EgoPoseVR achieved significantly higher subjective ratings in accuracy, stability, embodiment, and intention for future use compared to baseline methods. These results show that EgoPoseVR enables robust full-body pose tracking, offering a practical solution for accurate VR embodiment without requiring additional body-worn sensors or room-scale tracking systems."
    },
    {
      "title": "A Mixed Reality System for Robust Manikin Localization in Childbirth Training",
      "url": "https://arxiv.org/abs/2602.05588",
      "date": "2026-02-05",
      "authors_text": "Haojie Cheng, Chang Liu, Abhiram Kanneganti, Mahesh Arjandas Choolani, Arundhati Tushar Gosavi",
      "is_highlight": false,
      "score": 77.0,
      "summary": "This paper presents a mixed reality system that enhances childbirth training through robust manikin localization, improving learning outcomes compared to traditional virtual reality methods.",
      "teaser_image": "https://arxiv.org/html/2602.05588/Figures/eps/teaser/0.jpg",
      "debug_abstract": "Opportunities for medical students to gain practical experience in vaginal births are increasingly constrained by shortened clinical rotations, patient reluctance, and the unpredictable nature of labour. To alleviate clinicians&#39; instructional burden and enhance trainees&#39; learning efficiency, we introduce a mixed reality (MR) system for childbirth training that combines virtual guidance with tactile manikin interaction, thereby preserving authentic haptic feedback while enabling independent practice without continuous on-site expert supervision. The system extends the passthrough capability of commercial head-mounted displays (HMDs) by spatially calibrating an external RGB-D camera, allowing real-time visual integration of physical training objects. Building on this capability, we implement a coarse-to-fine localization pipeline that first aligns the maternal manikin with fiducial markers to define a delivery region and then registers the pre-scanned neonatal head within this area. This process enables spatially accurate overlay of virtual guiding hands near the manikin, allowing trainees to follow expert trajectories reinforced by haptic interaction. Experimental evaluations demonstrate that the system achieves accurate and stable manikin localization on a standalone headset, ensuring practical deployment without external computing resources. A large-scale user study involving 83 fourth-year medical students was subsequently conducted to compare MR-based and virtual reality (VR)-based childbirth training. Four senior obstetricians independently assessed performance using standardized criteria. Results showed that MR training achieved significantly higher scores in delivery, post-delivery, and overall task performance, and was consistently preferred by trainees over VR training."
    }
  ],
  "Microsystem Engineering and Robotics": [
    {
      "title": "EgoPoseVR: Spatiotemporal Multi-Modal Reasoning for Egocentric Full-Body Pose in Virtual Reality",
      "url": "https://arxiv.org/abs/2602.05590",
      "date": "2026-02-05",
      "authors_text": "Haojie Cheng, Shaun Jing Heng Ong, Shaoyu Cai, Aiden Tat Yang Koh, Fuxi Ouyang",
      "is_highlight": false,
      "score": 68.0,
      "summary": "EgoPoseVR is an advanced framework for accurate egocentric full-body pose estimation in virtual reality, integrating headset motion cues and RGB-D data for improved stability and performance.",
      "teaser_image": "https://arxiv.org/html/2602.05590/Figures/Pipeline/Pipeline.jpg",
      "debug_abstract": "Immersive virtual reality (VR) applications demand accurate, temporally coherent full-body pose tracking. Recent head-mounted camera-based approaches show promise in egocentric pose estimation, but encounter challenges when applied to VR head-mounted displays (HMDs), including temporal instability, inaccurate lower-body estimation, and the lack of real-time performance. To address these limitations, we present EgoPoseVR, an end-to-end framework for accurate egocentric full-body pose estimation in VR that integrates headset motion cues with egocentric RGB-D observations through a dual-modality fusion pipeline. A spatiotemporal encoder extracts frame- and joint-level representations, which are fused via cross-attention to fully exploit complementary motion cues across modalities. A kinematic optimization module then imposes constraints from HMD signals, enhancing the accuracy and stability of pose estimation. To facilitate training and evaluation, we introduce a large-scale synthetic dataset of over 1.8 million temporally aligned HMD and RGB-D frames across diverse VR scenarios. Experimental results show that EgoPoseVR outperforms state-of-the-art egocentric pose estimation models. A user study in real-world scenes further shows that EgoPoseVR achieved significantly higher subjective ratings in accuracy, stability, embodiment, and intention for future use compared to baseline methods. These results show that EgoPoseVR enables robust full-body pose tracking, offering a practical solution for accurate VR embodiment without requiring additional body-worn sensors or room-scale tracking systems."
    },
    {
      "title": "A Mixed Reality System for Robust Manikin Localization in Childbirth Training",
      "url": "https://arxiv.org/abs/2602.05588",
      "date": "2026-02-05",
      "authors_text": "Haojie Cheng, Chang Liu, Abhiram Kanneganti, Mahesh Arjandas Choolani, Arundhati Tushar Gosavi",
      "is_highlight": false,
      "score": 77.0,
      "summary": "This paper presents a mixed reality system that enhances childbirth training through robust manikin localization, improving learning outcomes compared to traditional virtual reality methods.",
      "teaser_image": "https://arxiv.org/html/2602.05588/Figures/eps/teaser/0.jpg",
      "debug_abstract": "Opportunities for medical students to gain practical experience in vaginal births are increasingly constrained by shortened clinical rotations, patient reluctance, and the unpredictable nature of labour. To alleviate clinicians&#39; instructional burden and enhance trainees&#39; learning efficiency, we introduce a mixed reality (MR) system for childbirth training that combines virtual guidance with tactile manikin interaction, thereby preserving authentic haptic feedback while enabling independent practice without continuous on-site expert supervision. The system extends the passthrough capability of commercial head-mounted displays (HMDs) by spatially calibrating an external RGB-D camera, allowing real-time visual integration of physical training objects. Building on this capability, we implement a coarse-to-fine localization pipeline that first aligns the maternal manikin with fiducial markers to define a delivery region and then registers the pre-scanned neonatal head within this area. This process enables spatially accurate overlay of virtual guiding hands near the manikin, allowing trainees to follow expert trajectories reinforced by haptic interaction. Experimental evaluations demonstrate that the system achieves accurate and stable manikin localization on a standalone headset, ensuring practical deployment without external computing resources. A large-scale user study involving 83 fourth-year medical students was subsequently conducted to compare MR-based and virtual reality (VR)-based childbirth training. Four senior obstetricians independently assessed performance using standardized criteria. Results showed that MR training achieved significantly higher scores in delivery, post-delivery, and overall task performance, and was consistently preferred by trainees over VR training."
    }
  ],
  "高性能人形技术实验室 (H²T)": [
    {
      "title": "Depth as Prior Knowledge for Object Detection",
      "url": "https://arxiv.org/abs/2602.05730",
      "date": "2026-02-05",
      "authors_text": "Moussa Kassem Sbeyti, Nadja Klein",
      "is_highlight": false,
      "score": 72.0,
      "summary": "The DepthPrior framework enhances object detection of small, distant objects by utilizing depth as prior knowledge, improving performance without altering detector architectures or requiring extra sensors.",
      "teaser_image": "https://arxiv.org/html/2602.05730/x1.png",
      "debug_abstract": "Detecting small and distant objects remains challenging for object detectors due to scale variation, low resolution, and background clutter. Safety-critical applications require reliable detection of these objects for safe planning. Depth information can improve detection, but existing approaches require complex, model-specific architectural modifications. We provide a theoretical analysis followed by an empirical investigation of the depth-detection relationship. Together, they explain how depth causes systematic performance degradation and why depth-informed supervision mitigates it. We introduce DepthPrior, a framework that uses depth as prior knowledge rather than as a fused feature, providing comparable benefits without modifying detector architectures. DepthPrior consists of Depth-Based Loss Weighting (DLW) and Depth-Based Loss Stratification (DLS) during training, and Depth-Aware Confidence Thresholding (DCT) during inference. The only overhead is the initial cost of depth estimation. Experiments across four benchmarks (KITTI, MS COCO, VisDrone, SUN RGB-D) and two detectors (YOLOv11, EfficientDet) demonstrate the effectiveness of DepthPrior, achieving up to +9% mAP$_S$ and +7% mAR$_S$ for small objects, with inference recovery rates as high as 95:1 (true vs. false detections). DepthPrior offers these benefits without additional sensors, architectural changes, or performance costs. Code is available at this https URL."
    }
  ],
  "天津大学": [
    {
      "title": "RL-VLA$^3$: Reinforcement Learning VLA Accelerating via Full Asynchronism",
      "url": "https://arxiv.org/abs/2602.05765",
      "date": "2026-02-05",
      "authors_text": "Zhong Guan, Haoran Sun, Yongjian Guo, Shuai Di, Xiaodong Bai",
      "is_highlight": false,
      "score": 91.0,
      "summary": "The paper introduces RL-VLA$^3$, a fully-asynchronous reinforcement learning framework that significantly enhances training efficiency and throughput for Vision-Language-Action models.",
      "teaser_image": "https://arxiv.org/html/2602.05765/x1.png",
      "debug_abstract": "In recent years, Vision-Language-Action (VLA) models have emerged as a crucial pathway towards general embodied intelligence, yet their training efficiency has become a key bottleneck. Although existing reinforcement learning (RL)-based training frameworks like RLinf can enhance model generalization, they still rely on synchronous execution, leading to severe resource underutilization and throughput limitations during environment interaction, policy generation (rollout), and model update phases (actor). To overcome this challenge, this paper, for the first time, proposes and implements a fully-asynchronous policy training framework encompassing the entire pipeline from environment interaction, rollout generation, to actor policy updates. Systematically drawing inspiration from asynchronous optimization ideas in large model RL, our framework designs a multi-level decoupled architecture. This includes asynchronous parallelization of environment interaction and trajectory collection, streaming execution for policy generation, and decoupled scheduling for training updates. We validated the effectiveness of our method across diverse VLA models and environments. On the LIBERO benchmark, the framework achieves throughput improvements of up to 59.25\\% compared to existing synchronous strategies. When deeply optimizing separation strategies, throughput can be increased by as much as 126.67\\%. We verified the effectiveness of each asynchronous component via ablation studies. Scaling law validation across 8 to 256 GPUs demonstrates our method&#39;s excellent scalability under most conditions."
    }
  ],
  "机器人技术与系统国家重点实验室": [
    {
      "title": "Focus-Scan-Refine: From Human Visual Perception to Efficient Visual Token Pruning",
      "url": "https://arxiv.org/abs/2602.05809",
      "date": "2026-02-05",
      "authors_text": "Enwei Tong, Yuanchao Bai, Yao Zhu, Junjun Jiang, Xianming Liu",
      "is_highlight": false,
      "score": 74.0,
      "summary": "The Focus-Scan-Refine framework enhances visual token pruning in vision-language models by mimicking human perception to improve accuracy and efficiency without increasing token usage.",
      "teaser_image": "https://arxiv.org/html/2602.05809/x4.png",
      "debug_abstract": "Vision-language models (VLMs) often generate massive visual tokens that greatly increase inference latency and memory footprint; while training-free token pruning offers a practical remedy, existing methods still struggle to balance local evidence and global context under aggressive compression. We propose Focus-Scan-Refine (FSR), a human-inspired, plug-and-play pruning framework that mimics how humans answer visual questions: focus on key evidence, then scan globally if needed, and refine the scanned context by aggregating relevant details. FSR first focuses on key evidence by combining visual importance with instruction relevance, avoiding the bias toward visually salient but query-irrelevant regions. It then scans for complementary context conditioned on the focused set, selecting tokens that are most different from the focused evidence. Finally, FSR refines the scanned context by aggregating nearby informative tokens into the scan anchors via similarity-based assignment and score-weighted merging, without increasing the token budget. Extensive experiments across multiple VLM backbones and vision-language benchmarks show that FSR consistently improves the accuracy-efficiency trade-off over existing state-of-the-art pruning methods. The source codes can be found at this https URL"
    }
  ],
  "武汉大学": [
    {
      "title": "TKG-Thinker: Towards Dynamic Reasoning over Temporal Knowledge Graphs via Agentic Reinforcement Learning",
      "url": "https://arxiv.org/abs/2602.05818",
      "date": "2026-02-05",
      "authors_text": "Zihao Jiang, Miao Peng, Zhenyan Shan, Wenjie Xu, Ben Liu",
      "is_highlight": false,
      "score": 83.0,
      "summary": "TKG-Thinker enhances temporal knowledge graph question answering by utilizing agentic reinforcement learning for dynamic reasoning, outperforming existing models through autonomous planning and adaptive retrieval.",
      "teaser_image": "https://arxiv.org/html/2602.05818/x2.png",
      "debug_abstract": "Temporal knowledge graph question answering (TKGQA) aims to answer time-sensitive questions by leveraging temporal knowledge bases. While Large Language Models (LLMs) demonstrate significant potential in TKGQA, current prompting strategies constrain their efficacy in two primary ways. First, they are prone to reasoning hallucinations under complex temporal constraints. Second, static prompting limits model autonomy and generalization, as it lack optimization through dynamic interaction with temporal knowledge graphs (TKGs) environments. To address these limitations, we propose \\textbf{TKG-Thinker}, a novel agent equipped with autonomous planning and adaptive retrieval capabilities for reasoning over TKGs. Specifically, TKG-Thinker performs in-depth temporal reasoning through dynamic multi-turn interactions with TKGs via a dual-training strategy. We first apply Supervised Fine-Tuning (SFT) with chain-of thought data to instill core planning capabilities, followed by a Reinforcement Learning (RL) stage that leverages multi-dimensional rewards to refine reasoning policies under intricate temporal constraints. Experimental results on benchmark datasets with three open-source LLMs show that TKG-Thinker achieves state-of-the-art performance and exhibits strong generalization across complex TKGQA settings."
    }
  ],
  "Sensing, Interaction & Perception Lab (SIPLAB)": [
    {
      "title": "LSA: Localized Semantic Alignment for Enhancing Temporal Consistency in Traffic Video Generation",
      "url": "https://arxiv.org/abs/2602.05966",
      "date": "2026-02-05",
      "authors_text": "Mirlan Karimov, Teodora Spasojevic, Markus Braun, Julian Wiederer, Vasileios Belagiannis",
      "is_highlight": false,
      "score": 69.0,
      "summary": "The paper introduces Localized Semantic Alignment (LSA), a framework that improves temporal consistency in traffic video generation by aligning semantic features without requiring control signals during inference.",
      "teaser_image": "https://arxiv.org/html/2602.05966/figs/pipeline.png",
      "debug_abstract": "Controllable video generation has emerged as a versatile tool for autonomous driving, enabling realistic synthesis of traffic scenarios. However, existing methods depend on control signals at inference time to guide the generative model towards temporally consistent generation of dynamic objects, limiting their utility as scalable and generalizable data engines. In this work, we propose Localized Semantic Alignment (LSA), a simple yet effective framework for fine-tuning pre-trained video generation models. LSA enhances temporal consistency by aligning semantic features between ground-truth and generated video clips. Specifically, we compare the output of an off-the-shelf feature extraction model between the ground-truth and generated video clips localized around dynamic objects inducing a semantic feature consistency loss. We fine-tune the base model by combining this loss with the standard diffusion loss. The model fine-tuned for a single epoch with our novel loss outperforms the baselines in common video generation evaluation metrics. To further test the temporal consistency in generated videos we adapt two additional metrics from object detection task, namely mAP and mIoU. Extensive experiments on nuScenes and KITTI datasets show the effectiveness of our approach in enhancing temporal consistency in video generation without the need for external control signals during inference and any computational overheads."
    }
  ],
  "机器人系统实验室 (RSL)": [
    {
      "title": "LSA: Localized Semantic Alignment for Enhancing Temporal Consistency in Traffic Video Generation",
      "url": "https://arxiv.org/abs/2602.05966",
      "date": "2026-02-05",
      "authors_text": "Mirlan Karimov, Teodora Spasojevic, Markus Braun, Julian Wiederer, Vasileios Belagiannis",
      "is_highlight": false,
      "score": 69.0,
      "summary": "The paper introduces Localized Semantic Alignment (LSA), a framework that improves temporal consistency in traffic video generation by aligning semantic features without requiring control signals during inference.",
      "teaser_image": "https://arxiv.org/html/2602.05966/figs/pipeline.png",
      "debug_abstract": "Controllable video generation has emerged as a versatile tool for autonomous driving, enabling realistic synthesis of traffic scenarios. However, existing methods depend on control signals at inference time to guide the generative model towards temporally consistent generation of dynamic objects, limiting their utility as scalable and generalizable data engines. In this work, we propose Localized Semantic Alignment (LSA), a simple yet effective framework for fine-tuning pre-trained video generation models. LSA enhances temporal consistency by aligning semantic features between ground-truth and generated video clips. Specifically, we compare the output of an off-the-shelf feature extraction model between the ground-truth and generated video clips localized around dynamic objects inducing a semantic feature consistency loss. We fine-tune the base model by combining this loss with the standard diffusion loss. The model fine-tuned for a single epoch with our novel loss outperforms the baselines in common video generation evaluation metrics. To further test the temporal consistency in generated videos we adapt two additional metrics from object detection task, namely mAP and mIoU. Extensive experiments on nuScenes and KITTI datasets show the effectiveness of our approach in enhancing temporal consistency in video generation without the need for external control signals during inference and any computational overheads."
    }
  ],
  "Language and Vision (LaVi) Lab": [
    {
      "title": "UniSurg: A Video-Native Foundation Model for Universal Understanding of Surgical Videos",
      "url": "https://arxiv.org/abs/2602.05638",
      "date": "2026-02-05",
      "authors_text": "Jinlin Wu, Felix Holm, Chuxi Chen, An Wang, Yaxin Hu",
      "is_highlight": false,
      "score": 76.0,
      "summary": "UniSurg is a novel video-native foundation model that enhances surgical video understanding by focusing on latent motion prediction and semantic structures, outperforming existing methods across multiple benchmarks.",
      "teaser_image": "https://arxiv.org/html/2602.05638/x1.png",
      "debug_abstract": "While foundation models have advanced surgical video analysis, current approaches rely predominantly on pixel-level reconstruction objectives that waste model capacity on low-level visual details - such as smoke, specular reflections, and fluid motion - rather than semantic structures essential for surgical understanding. We present UniSurg, a video-native foundation model that shifts the learning paradigm from pixel-level reconstruction to latent motion prediction. Built on the Video Joint Embedding Predictive Architecture (V-JEPA), UniSurg introduces three key technical innovations tailored to surgical videos: 1) motion-guided latent prediction to prioritize semantically meaningful regions, 2) spatiotemporal affinity self-distillation to enforce relational consistency, and 3) feature diversity regularization to prevent representation collapse in texture-sparse surgical scenes. To enable large-scale pretraining, we curate UniSurg-15M, the largest surgical video dataset to date, comprising 3,658 hours of video from 50 sources across 13 anatomical regions. Extensive experiments across 17 benchmarks demonstrate that UniSurg significantly outperforms state-of-the-art methods on surgical workflow recognition (+14.6% F1 on EgoSurgery, +10.3% on PitVis), action triplet recognition (39.54% mAP-IVT on CholecT50), skill assessment, polyp segmentation, and depth estimation. These results establish UniSurg as a new standard for universal, motion-oriented surgical video understanding."
    },
    {
      "title": "Affordance-Aware Interactive Decision-Making and Execution for Ambiguous Instructions",
      "url": "https://arxiv.org/abs/2602.05273",
      "date": "2026-02-05",
      "authors_text": "Hengxuan Xu, Fengbo Lan, Zhixin Zhao, Shengjie Wang, Mengqiao Liu",
      "is_highlight": false,
      "score": 89.0,
      "summary": "The AIDE framework enhances robot interaction and decision-making under ambiguous instructions by integrating interactive exploration with vision-language reasoning, achieving high task success and execution accuracy.",
      "teaser_image": "https://arxiv.org/html/2602.05273/x2.png",
      "debug_abstract": "Enabling robots to explore and act in unfamiliar environments under ambiguous human instructions by interactively identifying task-relevant objects (e.g., identifying cups or beverages for &#34;I&#39;m thirsty&#34;) remains challenging for existing vision-language model (VLM)-based methods. This challenge stems from inefficient reasoning and the lack of environmental interaction, which hinder real-time task planning and execution. To address this, We propose Affordance-Aware Interactive Decision-Making and Execution for Ambiguous Instructions (AIDE), a dual-stream framework that integrates interactive exploration with vision-language reasoning, where Multi-Stage Inference (MSI) serves as the decision-making stream and Accelerated Decision-Making (ADM) as the execution stream, enabling zero-shot affordance analysis and interpretation of ambiguous instructions. Extensive experiments in simulation and real-world environments show that AIDE achieves the task planning success rate of over 80\\% and more than 95\\% accuracy in closed-loop continuous execution at 10 Hz, outperforming existing VLM-based methods in diverse open-world scenarios."
    }
  ],
  "机器人与人工智能实验室 (RAIL)": [
    {
      "title": "UniSurg: A Video-Native Foundation Model for Universal Understanding of Surgical Videos",
      "url": "https://arxiv.org/abs/2602.05638",
      "date": "2026-02-05",
      "authors_text": "Jinlin Wu, Felix Holm, Chuxi Chen, An Wang, Yaxin Hu",
      "is_highlight": false,
      "score": 76.0,
      "summary": "UniSurg is a novel video-native foundation model that enhances surgical video understanding by focusing on latent motion prediction and semantic structures, outperforming existing methods across multiple benchmarks.",
      "teaser_image": "https://arxiv.org/html/2602.05638/x1.png",
      "debug_abstract": "While foundation models have advanced surgical video analysis, current approaches rely predominantly on pixel-level reconstruction objectives that waste model capacity on low-level visual details - such as smoke, specular reflections, and fluid motion - rather than semantic structures essential for surgical understanding. We present UniSurg, a video-native foundation model that shifts the learning paradigm from pixel-level reconstruction to latent motion prediction. Built on the Video Joint Embedding Predictive Architecture (V-JEPA), UniSurg introduces three key technical innovations tailored to surgical videos: 1) motion-guided latent prediction to prioritize semantically meaningful regions, 2) spatiotemporal affinity self-distillation to enforce relational consistency, and 3) feature diversity regularization to prevent representation collapse in texture-sparse surgical scenes. To enable large-scale pretraining, we curate UniSurg-15M, the largest surgical video dataset to date, comprising 3,658 hours of video from 50 sources across 13 anatomical regions. Extensive experiments across 17 benchmarks demonstrate that UniSurg significantly outperforms state-of-the-art methods on surgical workflow recognition (+14.6% F1 on EgoSurgery, +10.3% on PitVis), action triplet recognition (39.54% mAP-IVT on CholecT50), skill assessment, polyp segmentation, and depth estimation. These results establish UniSurg as a new standard for universal, motion-oriented surgical video understanding."
    },
    {
      "title": "Affordance-Aware Interactive Decision-Making and Execution for Ambiguous Instructions",
      "url": "https://arxiv.org/abs/2602.05273",
      "date": "2026-02-05",
      "authors_text": "Hengxuan Xu, Fengbo Lan, Zhixin Zhao, Shengjie Wang, Mengqiao Liu",
      "is_highlight": false,
      "score": 89.0,
      "summary": "The AIDE framework enhances robot interaction and decision-making under ambiguous instructions by integrating interactive exploration with vision-language reasoning, achieving high task success and execution accuracy.",
      "teaser_image": "https://arxiv.org/html/2602.05273/x2.png",
      "debug_abstract": "Enabling robots to explore and act in unfamiliar environments under ambiguous human instructions by interactively identifying task-relevant objects (e.g., identifying cups or beverages for &#34;I&#39;m thirsty&#34;) remains challenging for existing vision-language model (VLM)-based methods. This challenge stems from inefficient reasoning and the lack of environmental interaction, which hinder real-time task planning and execution. To address this, We propose Affordance-Aware Interactive Decision-Making and Execution for Ambiguous Instructions (AIDE), a dual-stream framework that integrates interactive exploration with vision-language reasoning, where Multi-Stage Inference (MSI) serves as the decision-making stream and Accelerated Decision-Making (ADM) as the execution stream, enabling zero-shot affordance analysis and interpretation of ambiguous instructions. Extensive experiments in simulation and real-world environments show that AIDE achieves the task planning success rate of over 80\\% and more than 95\\% accuracy in closed-loop continuous execution at 10 Hz, outperforming existing VLM-based methods in diverse open-world scenarios."
    }
  ],
  "深圳市人工智能与机器人研究院 (AIRS)": [
    {
      "title": "UniSurg: A Video-Native Foundation Model for Universal Understanding of Surgical Videos",
      "url": "https://arxiv.org/abs/2602.05638",
      "date": "2026-02-05",
      "authors_text": "Jinlin Wu, Felix Holm, Chuxi Chen, An Wang, Yaxin Hu",
      "is_highlight": false,
      "score": 76.0,
      "summary": "UniSurg is a novel video-native foundation model that enhances surgical video understanding by focusing on latent motion prediction and semantic structures, outperforming existing methods across multiple benchmarks.",
      "teaser_image": "https://arxiv.org/html/2602.05638/x1.png",
      "debug_abstract": "While foundation models have advanced surgical video analysis, current approaches rely predominantly on pixel-level reconstruction objectives that waste model capacity on low-level visual details - such as smoke, specular reflections, and fluid motion - rather than semantic structures essential for surgical understanding. We present UniSurg, a video-native foundation model that shifts the learning paradigm from pixel-level reconstruction to latent motion prediction. Built on the Video Joint Embedding Predictive Architecture (V-JEPA), UniSurg introduces three key technical innovations tailored to surgical videos: 1) motion-guided latent prediction to prioritize semantically meaningful regions, 2) spatiotemporal affinity self-distillation to enforce relational consistency, and 3) feature diversity regularization to prevent representation collapse in texture-sparse surgical scenes. To enable large-scale pretraining, we curate UniSurg-15M, the largest surgical video dataset to date, comprising 3,658 hours of video from 50 sources across 13 anatomical regions. Extensive experiments across 17 benchmarks demonstrate that UniSurg significantly outperforms state-of-the-art methods on surgical workflow recognition (+14.6% F1 on EgoSurgery, +10.3% on PitVis), action triplet recognition (39.54% mAP-IVT on CholecT50), skill assessment, polyp segmentation, and depth estimation. These results establish UniSurg as a new standard for universal, motion-oriented surgical video understanding."
    },
    {
      "title": "Affordance-Aware Interactive Decision-Making and Execution for Ambiguous Instructions",
      "url": "https://arxiv.org/abs/2602.05273",
      "date": "2026-02-05",
      "authors_text": "Hengxuan Xu, Fengbo Lan, Zhixin Zhao, Shengjie Wang, Mengqiao Liu",
      "is_highlight": false,
      "score": 89.0,
      "summary": "The AIDE framework enhances robot interaction and decision-making under ambiguous instructions by integrating interactive exploration with vision-language reasoning, achieving high task success and execution accuracy.",
      "teaser_image": "https://arxiv.org/html/2602.05273/x2.png",
      "debug_abstract": "Enabling robots to explore and act in unfamiliar environments under ambiguous human instructions by interactively identifying task-relevant objects (e.g., identifying cups or beverages for &#34;I&#39;m thirsty&#34;) remains challenging for existing vision-language model (VLM)-based methods. This challenge stems from inefficient reasoning and the lack of environmental interaction, which hinder real-time task planning and execution. To address this, We propose Affordance-Aware Interactive Decision-Making and Execution for Ambiguous Instructions (AIDE), a dual-stream framework that integrates interactive exploration with vision-language reasoning, where Multi-Stage Inference (MSI) serves as the decision-making stream and Accelerated Decision-Making (ADM) as the execution stream, enabling zero-shot affordance analysis and interpretation of ambiguous instructions. Extensive experiments in simulation and real-world environments show that AIDE achieves the task planning success rate of over 80\\% and more than 95\\% accuracy in closed-loop continuous execution at 10 Hz, outperforming existing VLM-based methods in diverse open-world scenarios."
    }
  ],
  "香港大学机械工程系机器人实验室": [
    {
      "title": "UniSurg: A Video-Native Foundation Model for Universal Understanding of Surgical Videos",
      "url": "https://arxiv.org/abs/2602.05638",
      "date": "2026-02-05",
      "authors_text": "Jinlin Wu, Felix Holm, Chuxi Chen, An Wang, Yaxin Hu",
      "is_highlight": false,
      "score": 76.0,
      "summary": "UniSurg is a novel video-native foundation model that enhances surgical video understanding by focusing on latent motion prediction and semantic structures, outperforming existing methods across multiple benchmarks.",
      "teaser_image": "https://arxiv.org/html/2602.05638/x1.png",
      "debug_abstract": "While foundation models have advanced surgical video analysis, current approaches rely predominantly on pixel-level reconstruction objectives that waste model capacity on low-level visual details - such as smoke, specular reflections, and fluid motion - rather than semantic structures essential for surgical understanding. We present UniSurg, a video-native foundation model that shifts the learning paradigm from pixel-level reconstruction to latent motion prediction. Built on the Video Joint Embedding Predictive Architecture (V-JEPA), UniSurg introduces three key technical innovations tailored to surgical videos: 1) motion-guided latent prediction to prioritize semantically meaningful regions, 2) spatiotemporal affinity self-distillation to enforce relational consistency, and 3) feature diversity regularization to prevent representation collapse in texture-sparse surgical scenes. To enable large-scale pretraining, we curate UniSurg-15M, the largest surgical video dataset to date, comprising 3,658 hours of video from 50 sources across 13 anatomical regions. Extensive experiments across 17 benchmarks demonstrate that UniSurg significantly outperforms state-of-the-art methods on surgical workflow recognition (+14.6% F1 on EgoSurgery, +10.3% on PitVis), action triplet recognition (39.54% mAP-IVT on CholecT50), skill assessment, polyp segmentation, and depth estimation. These results establish UniSurg as a new standard for universal, motion-oriented surgical video understanding."
    },
    {
      "title": "Affordance-Aware Interactive Decision-Making and Execution for Ambiguous Instructions",
      "url": "https://arxiv.org/abs/2602.05273",
      "date": "2026-02-05",
      "authors_text": "Hengxuan Xu, Fengbo Lan, Zhixin Zhao, Shengjie Wang, Mengqiao Liu",
      "is_highlight": false,
      "score": 89.0,
      "summary": "The AIDE framework enhances robot interaction and decision-making under ambiguous instructions by integrating interactive exploration with vision-language reasoning, achieving high task success and execution accuracy.",
      "teaser_image": "https://arxiv.org/html/2602.05273/x2.png",
      "debug_abstract": "Enabling robots to explore and act in unfamiliar environments under ambiguous human instructions by interactively identifying task-relevant objects (e.g., identifying cups or beverages for &#34;I&#39;m thirsty&#34;) remains challenging for existing vision-language model (VLM)-based methods. This challenge stems from inefficient reasoning and the lack of environmental interaction, which hinder real-time task planning and execution. To address this, We propose Affordance-Aware Interactive Decision-Making and Execution for Ambiguous Instructions (AIDE), a dual-stream framework that integrates interactive exploration with vision-language reasoning, where Multi-Stage Inference (MSI) serves as the decision-making stream and Accelerated Decision-Making (ADM) as the execution stream, enabling zero-shot affordance analysis and interpretation of ambiguous instructions. Extensive experiments in simulation and real-world environments show that AIDE achieves the task planning success rate of over 80\\% and more than 95\\% accuracy in closed-loop continuous execution at 10 Hz, outperforming existing VLM-based methods in diverse open-world scenarios."
    }
  ],
  "Multimedia Lab (MMLab)": [
    {
      "title": "UniSurg: A Video-Native Foundation Model for Universal Understanding of Surgical Videos",
      "url": "https://arxiv.org/abs/2602.05638",
      "date": "2026-02-05",
      "authors_text": "Jinlin Wu, Felix Holm, Chuxi Chen, An Wang, Yaxin Hu",
      "is_highlight": false,
      "score": 76.0,
      "summary": "UniSurg is a novel video-native foundation model that enhances surgical video understanding by focusing on latent motion prediction and semantic structures, outperforming existing methods across multiple benchmarks.",
      "teaser_image": "https://arxiv.org/html/2602.05638/x1.png",
      "debug_abstract": "While foundation models have advanced surgical video analysis, current approaches rely predominantly on pixel-level reconstruction objectives that waste model capacity on low-level visual details - such as smoke, specular reflections, and fluid motion - rather than semantic structures essential for surgical understanding. We present UniSurg, a video-native foundation model that shifts the learning paradigm from pixel-level reconstruction to latent motion prediction. Built on the Video Joint Embedding Predictive Architecture (V-JEPA), UniSurg introduces three key technical innovations tailored to surgical videos: 1) motion-guided latent prediction to prioritize semantically meaningful regions, 2) spatiotemporal affinity self-distillation to enforce relational consistency, and 3) feature diversity regularization to prevent representation collapse in texture-sparse surgical scenes. To enable large-scale pretraining, we curate UniSurg-15M, the largest surgical video dataset to date, comprising 3,658 hours of video from 50 sources across 13 anatomical regions. Extensive experiments across 17 benchmarks demonstrate that UniSurg significantly outperforms state-of-the-art methods on surgical workflow recognition (+14.6% F1 on EgoSurgery, +10.3% on PitVis), action triplet recognition (39.54% mAP-IVT on CholecT50), skill assessment, polyp segmentation, and depth estimation. These results establish UniSurg as a new standard for universal, motion-oriented surgical video understanding."
    },
    {
      "title": "Affordance-Aware Interactive Decision-Making and Execution for Ambiguous Instructions",
      "url": "https://arxiv.org/abs/2602.05273",
      "date": "2026-02-05",
      "authors_text": "Hengxuan Xu, Fengbo Lan, Zhixin Zhao, Shengjie Wang, Mengqiao Liu",
      "is_highlight": false,
      "score": 89.0,
      "summary": "The AIDE framework enhances robot interaction and decision-making under ambiguous instructions by integrating interactive exploration with vision-language reasoning, achieving high task success and execution accuracy.",
      "teaser_image": "https://arxiv.org/html/2602.05273/x2.png",
      "debug_abstract": "Enabling robots to explore and act in unfamiliar environments under ambiguous human instructions by interactively identifying task-relevant objects (e.g., identifying cups or beverages for &#34;I&#39;m thirsty&#34;) remains challenging for existing vision-language model (VLM)-based methods. This challenge stems from inefficient reasoning and the lack of environmental interaction, which hinder real-time task planning and execution. To address this, We propose Affordance-Aware Interactive Decision-Making and Execution for Ambiguous Instructions (AIDE), a dual-stream framework that integrates interactive exploration with vision-language reasoning, where Multi-Stage Inference (MSI) serves as the decision-making stream and Accelerated Decision-Making (ADM) as the execution stream, enabling zero-shot affordance analysis and interpretation of ambiguous instructions. Extensive experiments in simulation and real-world environments show that AIDE achieves the task planning success rate of over 80\\% and more than 95\\% accuracy in closed-loop continuous execution at 10 Hz, outperforming existing VLM-based methods in diverse open-world scenarios."
    }
  ],
  "潘佳老师实验室": [
    {
      "title": "UniSurg: A Video-Native Foundation Model for Universal Understanding of Surgical Videos",
      "url": "https://arxiv.org/abs/2602.05638",
      "date": "2026-02-05",
      "authors_text": "Jinlin Wu, Felix Holm, Chuxi Chen, An Wang, Yaxin Hu",
      "is_highlight": false,
      "score": 76.0,
      "summary": "UniSurg is a novel video-native foundation model that enhances surgical video understanding by focusing on latent motion prediction and semantic structures, outperforming existing methods across multiple benchmarks.",
      "teaser_image": "https://arxiv.org/html/2602.05638/x1.png",
      "debug_abstract": "While foundation models have advanced surgical video analysis, current approaches rely predominantly on pixel-level reconstruction objectives that waste model capacity on low-level visual details - such as smoke, specular reflections, and fluid motion - rather than semantic structures essential for surgical understanding. We present UniSurg, a video-native foundation model that shifts the learning paradigm from pixel-level reconstruction to latent motion prediction. Built on the Video Joint Embedding Predictive Architecture (V-JEPA), UniSurg introduces three key technical innovations tailored to surgical videos: 1) motion-guided latent prediction to prioritize semantically meaningful regions, 2) spatiotemporal affinity self-distillation to enforce relational consistency, and 3) feature diversity regularization to prevent representation collapse in texture-sparse surgical scenes. To enable large-scale pretraining, we curate UniSurg-15M, the largest surgical video dataset to date, comprising 3,658 hours of video from 50 sources across 13 anatomical regions. Extensive experiments across 17 benchmarks demonstrate that UniSurg significantly outperforms state-of-the-art methods on surgical workflow recognition (+14.6% F1 on EgoSurgery, +10.3% on PitVis), action triplet recognition (39.54% mAP-IVT on CholecT50), skill assessment, polyp segmentation, and depth estimation. These results establish UniSurg as a new standard for universal, motion-oriented surgical video understanding."
    },
    {
      "title": "Affordance-Aware Interactive Decision-Making and Execution for Ambiguous Instructions",
      "url": "https://arxiv.org/abs/2602.05273",
      "date": "2026-02-05",
      "authors_text": "Hengxuan Xu, Fengbo Lan, Zhixin Zhao, Shengjie Wang, Mengqiao Liu",
      "is_highlight": false,
      "score": 89.0,
      "summary": "The AIDE framework enhances robot interaction and decision-making under ambiguous instructions by integrating interactive exploration with vision-language reasoning, achieving high task success and execution accuracy.",
      "teaser_image": "https://arxiv.org/html/2602.05273/x2.png",
      "debug_abstract": "Enabling robots to explore and act in unfamiliar environments under ambiguous human instructions by interactively identifying task-relevant objects (e.g., identifying cups or beverages for &#34;I&#39;m thirsty&#34;) remains challenging for existing vision-language model (VLM)-based methods. This challenge stems from inefficient reasoning and the lack of environmental interaction, which hinder real-time task planning and execution. To address this, We propose Affordance-Aware Interactive Decision-Making and Execution for Ambiguous Instructions (AIDE), a dual-stream framework that integrates interactive exploration with vision-language reasoning, where Multi-Stage Inference (MSI) serves as the decision-making stream and Accelerated Decision-Making (ADM) as the execution stream, enabling zero-shot affordance analysis and interpretation of ambiguous instructions. Extensive experiments in simulation and real-world environments show that AIDE achieves the task planning success rate of over 80\\% and more than 95\\% accuracy in closed-loop continuous execution at 10 Hz, outperforming existing VLM-based methods in diverse open-world scenarios."
    }
  ],
  "机器人与自动化研究中心": [
    {
      "title": "UniSurg: A Video-Native Foundation Model for Universal Understanding of Surgical Videos",
      "url": "https://arxiv.org/abs/2602.05638",
      "date": "2026-02-05",
      "authors_text": "Jinlin Wu, Felix Holm, Chuxi Chen, An Wang, Yaxin Hu",
      "is_highlight": false,
      "score": 76.0,
      "summary": "UniSurg is a novel video-native foundation model that enhances surgical video understanding by focusing on latent motion prediction and semantic structures, outperforming existing methods across multiple benchmarks.",
      "teaser_image": "https://arxiv.org/html/2602.05638/x1.png",
      "debug_abstract": "While foundation models have advanced surgical video analysis, current approaches rely predominantly on pixel-level reconstruction objectives that waste model capacity on low-level visual details - such as smoke, specular reflections, and fluid motion - rather than semantic structures essential for surgical understanding. We present UniSurg, a video-native foundation model that shifts the learning paradigm from pixel-level reconstruction to latent motion prediction. Built on the Video Joint Embedding Predictive Architecture (V-JEPA), UniSurg introduces three key technical innovations tailored to surgical videos: 1) motion-guided latent prediction to prioritize semantically meaningful regions, 2) spatiotemporal affinity self-distillation to enforce relational consistency, and 3) feature diversity regularization to prevent representation collapse in texture-sparse surgical scenes. To enable large-scale pretraining, we curate UniSurg-15M, the largest surgical video dataset to date, comprising 3,658 hours of video from 50 sources across 13 anatomical regions. Extensive experiments across 17 benchmarks demonstrate that UniSurg significantly outperforms state-of-the-art methods on surgical workflow recognition (+14.6% F1 on EgoSurgery, +10.3% on PitVis), action triplet recognition (39.54% mAP-IVT on CholecT50), skill assessment, polyp segmentation, and depth estimation. These results establish UniSurg as a new standard for universal, motion-oriented surgical video understanding."
    },
    {
      "title": "Affordance-Aware Interactive Decision-Making and Execution for Ambiguous Instructions",
      "url": "https://arxiv.org/abs/2602.05273",
      "date": "2026-02-05",
      "authors_text": "Hengxuan Xu, Fengbo Lan, Zhixin Zhao, Shengjie Wang, Mengqiao Liu",
      "is_highlight": false,
      "score": 89.0,
      "summary": "The AIDE framework enhances robot interaction and decision-making under ambiguous instructions by integrating interactive exploration with vision-language reasoning, achieving high task success and execution accuracy.",
      "teaser_image": "https://arxiv.org/html/2602.05273/x2.png",
      "debug_abstract": "Enabling robots to explore and act in unfamiliar environments under ambiguous human instructions by interactively identifying task-relevant objects (e.g., identifying cups or beverages for &#34;I&#39;m thirsty&#34;) remains challenging for existing vision-language model (VLM)-based methods. This challenge stems from inefficient reasoning and the lack of environmental interaction, which hinder real-time task planning and execution. To address this, We propose Affordance-Aware Interactive Decision-Making and Execution for Ambiguous Instructions (AIDE), a dual-stream framework that integrates interactive exploration with vision-language reasoning, where Multi-Stage Inference (MSI) serves as the decision-making stream and Accelerated Decision-Making (ADM) as the execution stream, enabling zero-shot affordance analysis and interpretation of ambiguous instructions. Extensive experiments in simulation and real-world environments show that AIDE achieves the task planning success rate of over 80\\% and more than 95\\% accuracy in closed-loop continuous execution at 10 Hz, outperforming existing VLM-based methods in diverse open-world scenarios."
    }
  ],
  "OpenDriveLab": [
    {
      "title": "UniSurg: A Video-Native Foundation Model for Universal Understanding of Surgical Videos",
      "url": "https://arxiv.org/abs/2602.05638",
      "date": "2026-02-05",
      "authors_text": "Jinlin Wu, Felix Holm, Chuxi Chen, An Wang, Yaxin Hu",
      "is_highlight": false,
      "score": 76.0,
      "summary": "UniSurg is a novel video-native foundation model that enhances surgical video understanding by focusing on latent motion prediction and semantic structures, outperforming existing methods across multiple benchmarks.",
      "teaser_image": "https://arxiv.org/html/2602.05638/x1.png",
      "debug_abstract": "While foundation models have advanced surgical video analysis, current approaches rely predominantly on pixel-level reconstruction objectives that waste model capacity on low-level visual details - such as smoke, specular reflections, and fluid motion - rather than semantic structures essential for surgical understanding. We present UniSurg, a video-native foundation model that shifts the learning paradigm from pixel-level reconstruction to latent motion prediction. Built on the Video Joint Embedding Predictive Architecture (V-JEPA), UniSurg introduces three key technical innovations tailored to surgical videos: 1) motion-guided latent prediction to prioritize semantically meaningful regions, 2) spatiotemporal affinity self-distillation to enforce relational consistency, and 3) feature diversity regularization to prevent representation collapse in texture-sparse surgical scenes. To enable large-scale pretraining, we curate UniSurg-15M, the largest surgical video dataset to date, comprising 3,658 hours of video from 50 sources across 13 anatomical regions. Extensive experiments across 17 benchmarks demonstrate that UniSurg significantly outperforms state-of-the-art methods on surgical workflow recognition (+14.6% F1 on EgoSurgery, +10.3% on PitVis), action triplet recognition (39.54% mAP-IVT on CholecT50), skill assessment, polyp segmentation, and depth estimation. These results establish UniSurg as a new standard for universal, motion-oriented surgical video understanding."
    },
    {
      "title": "Affordance-Aware Interactive Decision-Making and Execution for Ambiguous Instructions",
      "url": "https://arxiv.org/abs/2602.05273",
      "date": "2026-02-05",
      "authors_text": "Hengxuan Xu, Fengbo Lan, Zhixin Zhao, Shengjie Wang, Mengqiao Liu",
      "is_highlight": false,
      "score": 89.0,
      "summary": "The AIDE framework enhances robot interaction and decision-making under ambiguous instructions by integrating interactive exploration with vision-language reasoning, achieving high task success and execution accuracy.",
      "teaser_image": "https://arxiv.org/html/2602.05273/x2.png",
      "debug_abstract": "Enabling robots to explore and act in unfamiliar environments under ambiguous human instructions by interactively identifying task-relevant objects (e.g., identifying cups or beverages for &#34;I&#39;m thirsty&#34;) remains challenging for existing vision-language model (VLM)-based methods. This challenge stems from inefficient reasoning and the lack of environmental interaction, which hinder real-time task planning and execution. To address this, We propose Affordance-Aware Interactive Decision-Making and Execution for Ambiguous Instructions (AIDE), a dual-stream framework that integrates interactive exploration with vision-language reasoning, where Multi-Stage Inference (MSI) serves as the decision-making stream and Accelerated Decision-Making (ADM) as the execution stream, enabling zero-shot affordance analysis and interpretation of ambiguous instructions. Extensive experiments in simulation and real-world environments show that AIDE achieves the task planning success rate of over 80\\% and more than 95\\% accuracy in closed-loop continuous execution at 10 Hz, outperforming existing VLM-based methods in diverse open-world scenarios."
    }
  ],
  "机器人与人工智能实验室": [
    {
      "title": "UniSurg: A Video-Native Foundation Model for Universal Understanding of Surgical Videos",
      "url": "https://arxiv.org/abs/2602.05638",
      "date": "2026-02-05",
      "authors_text": "Jinlin Wu, Felix Holm, Chuxi Chen, An Wang, Yaxin Hu",
      "is_highlight": false,
      "score": 76.0,
      "summary": "UniSurg is a novel video-native foundation model that enhances surgical video understanding by focusing on latent motion prediction and semantic structures, outperforming existing methods across multiple benchmarks.",
      "teaser_image": "https://arxiv.org/html/2602.05638/x1.png",
      "debug_abstract": "While foundation models have advanced surgical video analysis, current approaches rely predominantly on pixel-level reconstruction objectives that waste model capacity on low-level visual details - such as smoke, specular reflections, and fluid motion - rather than semantic structures essential for surgical understanding. We present UniSurg, a video-native foundation model that shifts the learning paradigm from pixel-level reconstruction to latent motion prediction. Built on the Video Joint Embedding Predictive Architecture (V-JEPA), UniSurg introduces three key technical innovations tailored to surgical videos: 1) motion-guided latent prediction to prioritize semantically meaningful regions, 2) spatiotemporal affinity self-distillation to enforce relational consistency, and 3) feature diversity regularization to prevent representation collapse in texture-sparse surgical scenes. To enable large-scale pretraining, we curate UniSurg-15M, the largest surgical video dataset to date, comprising 3,658 hours of video from 50 sources across 13 anatomical regions. Extensive experiments across 17 benchmarks demonstrate that UniSurg significantly outperforms state-of-the-art methods on surgical workflow recognition (+14.6% F1 on EgoSurgery, +10.3% on PitVis), action triplet recognition (39.54% mAP-IVT on CholecT50), skill assessment, polyp segmentation, and depth estimation. These results establish UniSurg as a new standard for universal, motion-oriented surgical video understanding."
    },
    {
      "title": "Affordance-Aware Interactive Decision-Making and Execution for Ambiguous Instructions",
      "url": "https://arxiv.org/abs/2602.05273",
      "date": "2026-02-05",
      "authors_text": "Hengxuan Xu, Fengbo Lan, Zhixin Zhao, Shengjie Wang, Mengqiao Liu",
      "is_highlight": false,
      "score": 89.0,
      "summary": "The AIDE framework enhances robot interaction and decision-making under ambiguous instructions by integrating interactive exploration with vision-language reasoning, achieving high task success and execution accuracy.",
      "teaser_image": "https://arxiv.org/html/2602.05273/x2.png",
      "debug_abstract": "Enabling robots to explore and act in unfamiliar environments under ambiguous human instructions by interactively identifying task-relevant objects (e.g., identifying cups or beverages for &#34;I&#39;m thirsty&#34;) remains challenging for existing vision-language model (VLM)-based methods. This challenge stems from inefficient reasoning and the lack of environmental interaction, which hinder real-time task planning and execution. To address this, We propose Affordance-Aware Interactive Decision-Making and Execution for Ambiguous Instructions (AIDE), a dual-stream framework that integrates interactive exploration with vision-language reasoning, where Multi-Stage Inference (MSI) serves as the decision-making stream and Accelerated Decision-Making (ADM) as the execution stream, enabling zero-shot affordance analysis and interpretation of ambiguous instructions. Extensive experiments in simulation and real-world environments show that AIDE achieves the task planning success rate of over 80\\% and more than 95\\% accuracy in closed-loop continuous execution at 10 Hz, outperforming existing VLM-based methods in diverse open-world scenarios."
    }
  ],
  "Hengshuang Zhao老师实验室": [
    {
      "title": "UniSurg: A Video-Native Foundation Model for Universal Understanding of Surgical Videos",
      "url": "https://arxiv.org/abs/2602.05638",
      "date": "2026-02-05",
      "authors_text": "Jinlin Wu, Felix Holm, Chuxi Chen, An Wang, Yaxin Hu",
      "is_highlight": false,
      "score": 76.0,
      "summary": "UniSurg is a novel video-native foundation model that enhances surgical video understanding by focusing on latent motion prediction and semantic structures, outperforming existing methods across multiple benchmarks.",
      "teaser_image": "https://arxiv.org/html/2602.05638/x1.png",
      "debug_abstract": "While foundation models have advanced surgical video analysis, current approaches rely predominantly on pixel-level reconstruction objectives that waste model capacity on low-level visual details - such as smoke, specular reflections, and fluid motion - rather than semantic structures essential for surgical understanding. We present UniSurg, a video-native foundation model that shifts the learning paradigm from pixel-level reconstruction to latent motion prediction. Built on the Video Joint Embedding Predictive Architecture (V-JEPA), UniSurg introduces three key technical innovations tailored to surgical videos: 1) motion-guided latent prediction to prioritize semantically meaningful regions, 2) spatiotemporal affinity self-distillation to enforce relational consistency, and 3) feature diversity regularization to prevent representation collapse in texture-sparse surgical scenes. To enable large-scale pretraining, we curate UniSurg-15M, the largest surgical video dataset to date, comprising 3,658 hours of video from 50 sources across 13 anatomical regions. Extensive experiments across 17 benchmarks demonstrate that UniSurg significantly outperforms state-of-the-art methods on surgical workflow recognition (+14.6% F1 on EgoSurgery, +10.3% on PitVis), action triplet recognition (39.54% mAP-IVT on CholecT50), skill assessment, polyp segmentation, and depth estimation. These results establish UniSurg as a new standard for universal, motion-oriented surgical video understanding."
    },
    {
      "title": "Affordance-Aware Interactive Decision-Making and Execution for Ambiguous Instructions",
      "url": "https://arxiv.org/abs/2602.05273",
      "date": "2026-02-05",
      "authors_text": "Hengxuan Xu, Fengbo Lan, Zhixin Zhao, Shengjie Wang, Mengqiao Liu",
      "is_highlight": false,
      "score": 89.0,
      "summary": "The AIDE framework enhances robot interaction and decision-making under ambiguous instructions by integrating interactive exploration with vision-language reasoning, achieving high task success and execution accuracy.",
      "teaser_image": "https://arxiv.org/html/2602.05273/x2.png",
      "debug_abstract": "Enabling robots to explore and act in unfamiliar environments under ambiguous human instructions by interactively identifying task-relevant objects (e.g., identifying cups or beverages for &#34;I&#39;m thirsty&#34;) remains challenging for existing vision-language model (VLM)-based methods. This challenge stems from inefficient reasoning and the lack of environmental interaction, which hinder real-time task planning and execution. To address this, We propose Affordance-Aware Interactive Decision-Making and Execution for Ambiguous Instructions (AIDE), a dual-stream framework that integrates interactive exploration with vision-language reasoning, where Multi-Stage Inference (MSI) serves as the decision-making stream and Accelerated Decision-Making (ADM) as the execution stream, enabling zero-shot affordance analysis and interpretation of ambiguous instructions. Extensive experiments in simulation and real-world environments show that AIDE achieves the task planning success rate of over 80\\% and more than 95\\% accuracy in closed-loop continuous execution at 10 Hz, outperforming existing VLM-based methods in diverse open-world scenarios."
    }
  ],
  "西安电子科技大学": [
    {
      "title": "ROMAN: Reward-Orchestrated Multi-Head Attention Network for Autonomous Driving System Testing",
      "url": "https://arxiv.org/abs/2602.05629",
      "date": "2026-02-05",
      "authors_text": "Jianlei Chi, Yuzhen Wu, Jiaxuan Hou, Xiaodong Zhang, Ming Fan",
      "is_highlight": false,
      "score": 84.0,
      "summary": "ROMAN is a novel scenario generation framework that enhances Autonomous Driving System testing by effectively creating high-risk traffic violation scenarios using multi-head attention and law weighting mechanisms.",
      "teaser_image": "https://arxiv.org/html/2602.05629/x2.png",
      "debug_abstract": "Automated Driving System (ADS) acts as the brain of autonomous vehicles, responsible for their safety and efficiency. Safe deployment requires thorough testing in diverse real-world scenarios and compliance with traffic laws like speed limits, signal obedience, and right-of-way rules. Violations like running red lights or speeding pose severe safety risks. However, current testing approaches face significant challenges: limited ability to generate complex and high-risk law-breaking scenarios, and failing to account for complex interactions involving multiple vehicles and critical situations. To address these challenges, we propose ROMAN, a novel scenario generation approach for ADS testing that combines a multi-head attention network with a traffic law weighting mechanism. ROMAN is designed to generate high-risk violation scenarios to enable more thorough and targeted ADS evaluation. The multi-head attention mechanism models interactions among vehicles, traffic signals, and other factors. The traffic law weighting mechanism implements a workflow that leverages an LLM-based risk weighting module to evaluate violations based on the two dimensions of severity and occurrence. We have evaluated ROMAN by testing the Baidu Apollo ADS within the CARLA simulation platform and conducting extensive experiments to measure its performance. Experimental results demonstrate that ROMAN surpassed state-of-the-art tools ABLE and LawBreaker by achieving 7.91% higher average violation count than ABLE and 55.96% higher than LawBreaker, while also maintaining greater scenario diversity. In addition, only ROMAN successfully generated violation scenarios for every clause of the input traffic laws, enabling it to identify more high-risk violations than existing approaches."
    }
  ],
  "人机物智能融合实验室 (HCP)": [
    {
      "title": "UniSurg: A Video-Native Foundation Model for Universal Understanding of Surgical Videos",
      "url": "https://arxiv.org/abs/2602.05638",
      "date": "2026-02-05",
      "authors_text": "Jinlin Wu, Felix Holm, Chuxi Chen, An Wang, Yaxin Hu",
      "is_highlight": false,
      "score": 76.0,
      "summary": "UniSurg is a novel video-native foundation model that enhances surgical video understanding by focusing on latent motion prediction and semantic structures, outperforming existing methods across multiple benchmarks.",
      "teaser_image": "https://arxiv.org/html/2602.05638/x1.png",
      "debug_abstract": "While foundation models have advanced surgical video analysis, current approaches rely predominantly on pixel-level reconstruction objectives that waste model capacity on low-level visual details - such as smoke, specular reflections, and fluid motion - rather than semantic structures essential for surgical understanding. We present UniSurg, a video-native foundation model that shifts the learning paradigm from pixel-level reconstruction to latent motion prediction. Built on the Video Joint Embedding Predictive Architecture (V-JEPA), UniSurg introduces three key technical innovations tailored to surgical videos: 1) motion-guided latent prediction to prioritize semantically meaningful regions, 2) spatiotemporal affinity self-distillation to enforce relational consistency, and 3) feature diversity regularization to prevent representation collapse in texture-sparse surgical scenes. To enable large-scale pretraining, we curate UniSurg-15M, the largest surgical video dataset to date, comprising 3,658 hours of video from 50 sources across 13 anatomical regions. Extensive experiments across 17 benchmarks demonstrate that UniSurg significantly outperforms state-of-the-art methods on surgical workflow recognition (+14.6% F1 on EgoSurgery, +10.3% on PitVis), action triplet recognition (39.54% mAP-IVT on CholecT50), skill assessment, polyp segmentation, and depth estimation. These results establish UniSurg as a new standard for universal, motion-oriented surgical video understanding."
    }
  ],
  "机器人与机械智能实验室 (ROMI Lab)": [
    {
      "title": "Graph-based Agent Memory: Taxonomy, Techniques, and Applications",
      "url": "https://arxiv.org/abs/2602.05665",
      "date": "2026-02-05",
      "authors_text": "Chang Yang, Chuang Zhou, Yilin Xiao, Su Dong, Luyao Zhuang",
      "is_highlight": false,
      "score": 73.0,
      "summary": "This paper reviews graph-based agent memory, presenting a taxonomy, key techniques, applications, and future challenges to enhance LLM-based agents' efficiency and reliability.",
      "teaser_image": "https://arxiv.org/html/2602.05665/x3.png",
      "debug_abstract": "Memory emerges as the core module in the Large Language Model (LLM)-based agents for long-horizon complex tasks (e.g., multi-turn dialogue, game playing, scientific discovery), where memory can enable knowledge accumulation, iterative reasoning and self-evolution. Among diverse paradigms, graph stands out as a powerful structure for agent memory due to the intrinsic capabilities to model relational dependencies, organize hierarchical information, and support efficient retrieval. This survey presents a comprehensive review of agent memory from the graph-based perspective. First, we introduce a taxonomy of agent memory, including short-term vs. long-term memory, knowledge vs. experience memory, non-structural vs. structural memory, with an implementation view of graph-based memory. Second, according to the life cycle of agent memory, we systematically analyze the key techniques in graph-based agent memory, covering memory extraction for transforming the data into the contents, storage for organizing the data efficiently, retrieval for retrieving the relevant contents from memory to support reasoning, and evolution for updating the contents in the memory. Third, we summarize the open-sourced libraries and benchmarks that support the development and evaluation of self-evolving agent memory. We also explore diverse application scenarios. Finally, we identify critical challenges and future research directions. This survey aims to offer actionable insights to advance the development of more efficient and reliable graph-based agent memory systems. All the related resources, including research papers, open-source data, and projects, are collected for the community in this https URL."
    }
  ]
}