{
  "卡内基-机器人研究所": [
    {
      "title": "How Does Delegation in Social Interaction Evolve Over Time? Navigation with a Robot for Blind People",
      "url": "https://arxiv.org/abs/2601.19851",
      "date": "2026-01-27",
      "authors_text": "Rayna Hata, Masaki Kuribayashi, Allan Wang, Hironobu Takagi, Chieko Asakawa",
      "is_highlight": false,
      "score": 87.0,
      "summary": "This study reveals that blind users refine their navigation strategies and preferences for robot assistance through repeated interactions, highlighting the importance of adaptive design in assistive robotics.",
      "teaser_image": "https://arxiv.org/html/2601.19851/images/Robot.png",
      "debug_abstract": "Autonomy and independent navigation are vital to daily life but remain challenging for individuals with blindness. Robotic systems can enhance mobility and confidence by providing intelligent navigation assistance. However, fully autonomous systems may reduce users&#39; sense of control, even when they wish to remain actively involved. Although collaboration between user and robot has been recognized as important, little is known about how perceptions of this relationship change with repeated use. We present a repeated exposure study with six blind participants who interacted with a navigation-assistive robot in a real-world museum. Participants completed tasks such as navigating crowds, approaching lines, and encountering obstacles. Findings show that participants refined their strategies over time, developing clearer preferences about when to rely on the robot versus act independently. This work provides insights into how strategies and preferences evolve with repeated interaction and offers design implications for robots that adapt to user needs over time."
    },
    {
      "title": "Constraint-Aware Discrete-Time PID Gain Optimization for Robotic Joint Control Under Actuator Saturation",
      "url": "https://arxiv.org/abs/2601.18639",
      "date": "2026-01-27",
      "authors_text": "Ojasva Mishra, Xiaolong Wu, Min Xu",
      "is_highlight": false,
      "score": 85.0,
      "summary": "This paper presents a robust optimization method for discrete-time PID control of robotic joints under actuator saturation, enhancing stability and performance while minimizing overshoot and sample inefficiency.",
      "teaser_image": "https://arxiv.org/html/2601.18639/figures/ki_bound_vs_dt.png",
      "debug_abstract": "The precise regulation of rotary actuation is fundamental in autonomous robotics, yet practical PID loops deviate from continuous-time theory due to discrete-time execution, actuator saturation, and small delays and measurement imperfections. We present an implementation-aware analysis and tuning workflow for saturated discrete-time joint control. We (i) derive PI stability regions under Euler and exact zero-order-hold (ZOH) discretizations using the Jury criterion, (ii) evaluate a discrete back-calculation anti-windup realization under saturation-dominant regimes, and (iii) propose a hybrid-certified Bayesian optimization workflow that screens analytically unstable candidates and behaviorally unsafe transients while optimizing a robust IAE objective with soft penalties on overshoot and saturation duty. Baseline sweeps ($\\tau=1.0$~s, $\\Delta t=0.01$~s, $u\\in[-10,10]$) quantify rise/settle trends for P/PI/PID. Under a randomized model family emulating uncertainty, delay, noise, quantization, and tighter saturation, robustness-oriented tuning improves median IAE from $0.843$ to $0.430$ while keeping median overshoot below $2\\%$. In simulation-only tuning, the certification screen rejects $11.6\\%$ of randomly sampled gains within bounds before full robust evaluation, improving sample efficiency without hardware experiments."
    },
    {
      "title": "Large-Scale Continual Scheduling and Execution for Dynamic Distributed Satellite Constellation Observation Allocation",
      "url": "https://arxiv.org/abs/2601.06188",
      "date": "2026-01-26",
      "authors_text": "Itai Zilberstein, Steve Chien",
      "is_highlight": false,
      "score": 65.0,
      "summary": "This paper introduces the Dynamic Incremental Neighborhood Stochastic Search algorithm for efficient scheduling in large-scale satellite constellations, enhancing autonomy and performance in Earth observation.",
      "teaser_image": "https://arxiv.org/html/2601.06188/constellation.png",
      "debug_abstract": "The size and capabilities of Earth-observing satellite constellations are rapidly increasing. Leveraging distributed onboard control, we can enable novel time-sensitive measurements and responses. However, deploying autonomy to large multiagent satellite systems necessitates algorithms with efficient computation and communication. We tackle this challenge and propose new, online algorithms for large-scale dynamic distributed constraint optimization problems (DDCOP). We present the Dynamic Multi-Satellite Constellation Observation Scheduling Problem (DCOSP), a new formulation of DDCOPs that models integrated scheduling and execution. We construct an omniscient offline algorithm to compute the novel optimality condition of DCOSP and present the Dynamic Incremental Neighborhood Stochastic Search (D-NSS) algorithm, an incomplete online decomposition-based DDCOP approach. We show through simulation that D-NSS converges to near-optimal solutions and outperforms DDCOP baselines in terms of solution quality, computation time, and message volume. Our work forms the foundation of the largest in-space demonstration of distributed multiagent AI to date: the NASA FAME mission."
    }
  ],
  "加州理工-机器人实验室": [
    {
      "title": "Large-Scale Continual Scheduling and Execution for Dynamic Distributed Satellite Constellation Observation Allocation",
      "url": "https://arxiv.org/abs/2601.06188",
      "date": "2026-01-26",
      "authors_text": "Itai Zilberstein, Steve Chien",
      "is_highlight": false,
      "score": 65.0,
      "summary": "This paper introduces the Dynamic Incremental Neighborhood Stochastic Search algorithm for efficient scheduling in large-scale satellite constellations, enhancing autonomy and performance in Earth observation.",
      "teaser_image": "https://arxiv.org/html/2601.06188/constellation.png",
      "debug_abstract": "The size and capabilities of Earth-observing satellite constellations are rapidly increasing. Leveraging distributed onboard control, we can enable novel time-sensitive measurements and responses. However, deploying autonomy to large multiagent satellite systems necessitates algorithms with efficient computation and communication. We tackle this challenge and propose new, online algorithms for large-scale dynamic distributed constraint optimization problems (DDCOP). We present the Dynamic Multi-Satellite Constellation Observation Scheduling Problem (DCOSP), a new formulation of DDCOPs that models integrated scheduling and execution. We construct an omniscient offline algorithm to compute the novel optimality condition of DCOSP and present the Dynamic Incremental Neighborhood Stochastic Search (D-NSS) algorithm, an incomplete online decomposition-based DDCOP approach. We show through simulation that D-NSS converges to near-optimal solutions and outperforms DDCOP baselines in terms of solution quality, computation time, and message volume. Our work forms the foundation of the largest in-space demonstration of distributed multiagent AI to date: the NASA FAME mission."
    }
  ],
  "OpenDriveLab": [
    {
      "title": "EgoHandICL: Egocentric 3D Hand Reconstruction with In-Context Learning",
      "url": "https://arxiv.org/abs/2601.19850",
      "date": "2026-01-27",
      "authors_text": "Binzhu Xie, Shi Qiu, Sicheng Zhang, Yinqiao Wang, Hao Xu",
      "is_highlight": false,
      "score": 76.0,
      "summary": "EgoHandICL introduces an innovative in-context learning framework for robust 3D hand reconstruction in egocentric vision, enhancing performance through multimodal context and exemplar retrieval.",
      "teaser_image": "https://arxiv.org/html/2601.19850/x2.png",
      "debug_abstract": "Robust 3D hand reconstruction in egocentric vision is challenging due to depth ambiguity, self-occlusion, and complex hand-object interactions. Prior methods mitigate these issues by scaling training data or adding auxiliary cues, but they often struggle in unseen contexts. We present EgoHandICL, the first in-context learning (ICL) framework for 3D hand reconstruction that improves semantic alignment, visual consistency, and robustness under challenging egocentric conditions. EgoHandICL introduces complementary exemplar retrieval guided by vision-language models (VLMs), an ICL-tailored tokenizer for multimodal context, and a masked autoencoder (MAE)-based architecture trained with hand-guided geometric and perceptual objectives. Experiments on ARCTIC and EgoExo4D show consistent gains over state-of-the-art methods. We also demonstrate real-world generalization and improve EgoVLM hand-object interaction reasoning by using reconstructed hands as visual prompts. Code and data: this https URL"
    },
    {
      "title": "Component-Level Lesioning of Language Models Reveals Clinically Aligned Aphasia Phenotypes",
      "url": "https://arxiv.org/abs/2601.19723",
      "date": "2026-01-27",
      "authors_text": "Yifan Wang, Jichen Zheng, Jingyuan Sun, Yunhao Zhang, Chunyu Ye",
      "is_highlight": false,
      "score": 61.0,
      "summary": "This paper presents a framework for simulating aphasia in language models by selectively perturbing components, enabling insights into language production impairments and rehabilitation strategies.",
      "teaser_image": "https://arxiv.org/html/2601.19723/x1.png",
      "debug_abstract": "Large language models (LLMs) increasingly exhibit human-like linguistic behaviors and internal representations that they could serve as computational simulators of language cognition. We ask whether LLMs can be systematically manipulated to reproduce language-production impairments characteristic of aphasia following focal brain lesions. Such models could provide scalable proxies for testing rehabilitation hypotheses, and offer a controlled framework for probing the functional organization of language. We introduce a clinically grounded, component-level framework that simulates aphasia by selectively perturbing functional components in LLMs, and apply it to both modular Mixture-of-Experts models and dense Transformers using a unified intervention interface. Our pipeline (i) identifies subtype-linked components for Broca&#39;s and Wernicke&#39;s aphasia, (ii) interprets these components with linguistic probing tasks, and (iii) induces graded impairments by progressively perturbing the top-k subtype-linked components, evaluating outcomes with Western Aphasia Battery (WAB) subtests summarized by Aphasia Quotient (AQ). Across architectures and lesioning strategies, subtype-targeted perturbations yield more systematic, aphasia-like regressions than size-matched random perturbations, and MoE modularity supports more localized and interpretable phenotype-to-component mappings. These findings suggest that modular LLMs, combined with clinically informed component perturbations, provide a promising platform for simulating aphasic language production and studying how distinct language functions degrade under targeted disruptions."
    },
    {
      "title": "Rhombot: Rhombus-shaped Modular Robots for Stable, Medium-Independent Reconfiguration Motion",
      "url": "https://arxiv.org/abs/2601.19529",
      "date": "2026-01-27",
      "authors_text": "Jie Gu, Yirui Sun, Zhihao Xia, Tin Lun Lam, Chunxu Tian",
      "is_highlight": false,
      "score": 91.0,
      "summary": "Rhombot is a novel rhombus-shaped modular robot enabling stable, medium-independent reconfiguration through morphpivoting, validated by experiments demonstrating its morphing and docking capabilities.",
      "teaser_image": "https://arxiv.org/html/2601.19529/x2.png",
      "debug_abstract": "In this paper, we present Rhombot, a novel deformable planar lattice modular self-reconfigurable robot (MSRR) with a rhombus shaped module. Each module consists of a parallelogram skeleton with a single centrally mounted actuator that enables folding and unfolding along its diagonal. The core design philosophy is to achieve essential MSRR functionalities such as morphing, docking, and locomotion with minimal control complexity. This enables a continuous and stable reconfiguration process that is independent of the surrounding medium, allowing the system to reliably form various configurations in diverse environments. To leverage the unique kinematics of Rhombot, we introduce morphpivoting, a novel motion primitive for reconfiguration that differs from advanced MSRR systems, and propose a strategy for its continuous execution. Finally, a series of physical experiments validate the module&#39;s stable reconfiguration ability, as well as its positional and docking accuracy."
    },
    {
      "title": "Fast and Safe Trajectory Optimization for Mobile Manipulators With Neural Configuration Space Distance Field",
      "url": "https://arxiv.org/abs/2601.18548",
      "date": "2026-01-27",
      "authors_text": "Yulin Li, Zhiyuan Song, Yiming Li, Zhicheng Song, Kai Chen",
      "is_highlight": false,
      "score": 88.0,
      "summary": "The paper presents Generalized Configuration Space Distance Fields (GCDF) for efficient trajectory optimization in mobile manipulators, enhancing collision reasoning and enabling rapid replanning in complex environments.",
      "teaser_image": "https://arxiv.org/html/2601.18548/x2.png",
      "debug_abstract": "Mobile manipulators promise agile, long-horizon behavior by coordinating base and arm motion, yet whole-body trajectory optimization in cluttered, confined spaces remains difficult due to high-dimensional nonconvexity and the need for fast, accurate collision reasoning. Configuration Space Distance Fields (CDF) enable fixed-base manipulators to model collisions directly in configuration space via smooth, implicit distances. This representation holds strong potential to bypass the nonlinear configuration-to-workspace mapping while preserving accurate whole-body geometry and providing optimization-friendly collision costs. Yet, extending this capability to mobile manipulators is hindered by unbounded workspaces and tighter base-arm coupling. We lift this promise to mobile manipulation with Generalized Configuration Space Distance Fields (GCDF), extending CDF to robots with both translational and rotational joints in unbounded workspaces with tighter base-arm coupling. We prove that GCDF preserves Euclidean-like local distance structure and accurately encodes whole-body geometry in configuration space, and develop a data generation and training pipeline that yields continuous neural GCDFs with accurate values and gradients, supporting efficient GPU-batched queries. Building on this representation, we develop a high-performance sequential convex optimization framework centered on GCDF-based collision reasoning. The solver scales to large numbers of implicit constraints through (i) online specification of neural constraints, (ii) sparsity-aware active-set detection with parallel batched evaluation across thousands of constraints, and (iii) incremental constraint management for rapid replanning under scene changes."
    }
  ],
  "Language and Vision (LaVi) Lab": [
    {
      "title": "EgoHandICL: Egocentric 3D Hand Reconstruction with In-Context Learning",
      "url": "https://arxiv.org/abs/2601.19850",
      "date": "2026-01-27",
      "authors_text": "Binzhu Xie, Shi Qiu, Sicheng Zhang, Yinqiao Wang, Hao Xu",
      "is_highlight": false,
      "score": 76.0,
      "summary": "EgoHandICL introduces an innovative in-context learning framework for robust 3D hand reconstruction in egocentric vision, enhancing performance through multimodal context and exemplar retrieval.",
      "teaser_image": "https://arxiv.org/html/2601.19850/x2.png",
      "debug_abstract": "Robust 3D hand reconstruction in egocentric vision is challenging due to depth ambiguity, self-occlusion, and complex hand-object interactions. Prior methods mitigate these issues by scaling training data or adding auxiliary cues, but they often struggle in unseen contexts. We present EgoHandICL, the first in-context learning (ICL) framework for 3D hand reconstruction that improves semantic alignment, visual consistency, and robustness under challenging egocentric conditions. EgoHandICL introduces complementary exemplar retrieval guided by vision-language models (VLMs), an ICL-tailored tokenizer for multimodal context, and a masked autoencoder (MAE)-based architecture trained with hand-guided geometric and perceptual objectives. Experiments on ARCTIC and EgoExo4D show consistent gains over state-of-the-art methods. We also demonstrate real-world generalization and improve EgoVLM hand-object interaction reasoning by using reconstructed hands as visual prompts. Code and data: this https URL"
    },
    {
      "title": "Component-Level Lesioning of Language Models Reveals Clinically Aligned Aphasia Phenotypes",
      "url": "https://arxiv.org/abs/2601.19723",
      "date": "2026-01-27",
      "authors_text": "Yifan Wang, Jichen Zheng, Jingyuan Sun, Yunhao Zhang, Chunyu Ye",
      "is_highlight": false,
      "score": 61.0,
      "summary": "This paper presents a framework for simulating aphasia in language models by selectively perturbing components, enabling insights into language production impairments and rehabilitation strategies.",
      "teaser_image": "https://arxiv.org/html/2601.19723/x1.png",
      "debug_abstract": "Large language models (LLMs) increasingly exhibit human-like linguistic behaviors and internal representations that they could serve as computational simulators of language cognition. We ask whether LLMs can be systematically manipulated to reproduce language-production impairments characteristic of aphasia following focal brain lesions. Such models could provide scalable proxies for testing rehabilitation hypotheses, and offer a controlled framework for probing the functional organization of language. We introduce a clinically grounded, component-level framework that simulates aphasia by selectively perturbing functional components in LLMs, and apply it to both modular Mixture-of-Experts models and dense Transformers using a unified intervention interface. Our pipeline (i) identifies subtype-linked components for Broca&#39;s and Wernicke&#39;s aphasia, (ii) interprets these components with linguistic probing tasks, and (iii) induces graded impairments by progressively perturbing the top-k subtype-linked components, evaluating outcomes with Western Aphasia Battery (WAB) subtests summarized by Aphasia Quotient (AQ). Across architectures and lesioning strategies, subtype-targeted perturbations yield more systematic, aphasia-like regressions than size-matched random perturbations, and MoE modularity supports more localized and interpretable phenotype-to-component mappings. These findings suggest that modular LLMs, combined with clinically informed component perturbations, provide a promising platform for simulating aphasic language production and studying how distinct language functions degrade under targeted disruptions."
    },
    {
      "title": "Rhombot: Rhombus-shaped Modular Robots for Stable, Medium-Independent Reconfiguration Motion",
      "url": "https://arxiv.org/abs/2601.19529",
      "date": "2026-01-27",
      "authors_text": "Jie Gu, Yirui Sun, Zhihao Xia, Tin Lun Lam, Chunxu Tian",
      "is_highlight": false,
      "score": 91.0,
      "summary": "Rhombot is a novel rhombus-shaped modular robot enabling stable, medium-independent reconfiguration through morphpivoting, validated by experiments demonstrating its morphing and docking capabilities.",
      "teaser_image": "https://arxiv.org/html/2601.19529/x2.png",
      "debug_abstract": "In this paper, we present Rhombot, a novel deformable planar lattice modular self-reconfigurable robot (MSRR) with a rhombus shaped module. Each module consists of a parallelogram skeleton with a single centrally mounted actuator that enables folding and unfolding along its diagonal. The core design philosophy is to achieve essential MSRR functionalities such as morphing, docking, and locomotion with minimal control complexity. This enables a continuous and stable reconfiguration process that is independent of the surrounding medium, allowing the system to reliably form various configurations in diverse environments. To leverage the unique kinematics of Rhombot, we introduce morphpivoting, a novel motion primitive for reconfiguration that differs from advanced MSRR systems, and propose a strategy for its continuous execution. Finally, a series of physical experiments validate the module&#39;s stable reconfiguration ability, as well as its positional and docking accuracy."
    },
    {
      "title": "Fast and Safe Trajectory Optimization for Mobile Manipulators With Neural Configuration Space Distance Field",
      "url": "https://arxiv.org/abs/2601.18548",
      "date": "2026-01-27",
      "authors_text": "Yulin Li, Zhiyuan Song, Yiming Li, Zhicheng Song, Kai Chen",
      "is_highlight": false,
      "score": 88.0,
      "summary": "The paper presents Generalized Configuration Space Distance Fields (GCDF) for efficient trajectory optimization in mobile manipulators, enhancing collision reasoning and enabling rapid replanning in complex environments.",
      "teaser_image": "https://arxiv.org/html/2601.18548/x2.png",
      "debug_abstract": "Mobile manipulators promise agile, long-horizon behavior by coordinating base and arm motion, yet whole-body trajectory optimization in cluttered, confined spaces remains difficult due to high-dimensional nonconvexity and the need for fast, accurate collision reasoning. Configuration Space Distance Fields (CDF) enable fixed-base manipulators to model collisions directly in configuration space via smooth, implicit distances. This representation holds strong potential to bypass the nonlinear configuration-to-workspace mapping while preserving accurate whole-body geometry and providing optimization-friendly collision costs. Yet, extending this capability to mobile manipulators is hindered by unbounded workspaces and tighter base-arm coupling. We lift this promise to mobile manipulation with Generalized Configuration Space Distance Fields (GCDF), extending CDF to robots with both translational and rotational joints in unbounded workspaces with tighter base-arm coupling. We prove that GCDF preserves Euclidean-like local distance structure and accurately encodes whole-body geometry in configuration space, and develop a data generation and training pipeline that yields continuous neural GCDFs with accurate values and gradients, supporting efficient GPU-batched queries. Building on this representation, we develop a high-performance sequential convex optimization framework centered on GCDF-based collision reasoning. The solver scales to large numbers of implicit constraints through (i) online specification of neural constraints, (ii) sparsity-aware active-set detection with parallel batched evaluation across thousands of constraints, and (iii) incremental constraint management for rapid replanning under scene changes."
    }
  ],
  "Precognition Lab": [
    {
      "title": "Geometry-Grounded Gaussian Splatting",
      "url": "https://arxiv.org/abs/2601.17835",
      "date": "2026-01-27",
      "authors_text": "Baowen Zhang, Chenxing Jiang, Heng Li, Shaojie Shen, Ping Tan",
      "is_highlight": true,
      "score": 73.0,
      "summary": "This paper introduces Geometry-Grounded Gaussian Splatting, enhancing shape extraction from Gaussian primitives through a theoretical framework that improves multi-view consistency and depth map quality.",
      "teaser_image": "https://arxiv.org/html/2601.17835/x2.png",
      "debug_abstract": "Gaussian Splatting (GS) has demonstrated impressive quality and efficiency in novel view synthesis. However, shape extraction from Gaussian primitives remains an open problem. Due to inadequate geometry parameterization and approximation, existing shape reconstruction methods suffer from poor multi-view consistency and are sensitive to floaters. In this paper, we present a rigorous theoretical derivation that establishes Gaussian primitives as a specific type of stochastic solids. This theoretical framework provides a principled foundation for Geometry-Grounded Gaussian Splatting by enabling the direct treatment of Gaussian primitives as explicit geometric representations. Using the volumetric nature of stochastic solids, our method efficiently renders high-quality depth maps for fine-grained geometry extraction. Experiments show that our method achieves the best shape reconstruction results among all Gaussian Splatting-based methods on public datasets."
    },
    {
      "title": "Enhancing Inverse Perspective Mapping for Automatic Vectorized Road Map Generation",
      "url": "https://arxiv.org/abs/2601.19536",
      "date": "2026-01-27",
      "authors_text": "Hongji Liu, Linwei Zheng, Yongjian Li, Mingkai Tang, Xiaoyang Yan",
      "is_highlight": false,
      "score": 66.0,
      "summary": "This study introduces a cost-effective framework using enhanced inverse perspective mapping to generate high-precision vectorized road maps with improved accuracy and reduced mapping errors.",
      "teaser_image": "https://arxiv.org/html/2601.19536/x2.png",
      "debug_abstract": "In this study, we present a low-cost and unified framework for vectorized road mapping leveraging enhanced inverse perspective mapping (IPM). In this framework, Catmull-Rom splines are utilized to characterize lane lines, and all the other ground markings are depicted using polygons uniformly. The results from instance segmentation serve as references to refine the three-dimensional position of spline control points and polygon corner points. In conjunction with this process, the homography matrix of IPM and vehicle poses are optimized simultaneously. Our proposed framework significantly reduces the mapping errors associated with IPM. It also improves the accuracy of the initial IPM homography matrix and the predicted vehicle poses. Furthermore, it addresses the limitations imposed by the coplanarity assumption in IPM. These enhancements enable IPM to be effectively applied to vectorized road mapping, which serves a cost-effective solution with enhanced accuracy. In addition, our framework generalizes road map elements to include all common ground markings and lane lines. The proposed framework is evaluated in two different practical scenarios, and the test results show that our method can automatically generate high-precision maps with near-centimeter-level accuracy. Importantly, the optimized IPM matrix achieves an accuracy comparable to that of manual calibration, while the accuracy of vehicle poses is also significantly improved."
    },
    {
      "title": "Glance and Focus Reinforcement for Pan-cancer Screening",
      "url": "https://arxiv.org/abs/2601.19103",
      "date": "2026-01-27",
      "authors_text": "Linshan Wu, Jiaxin Zhuang, Hao Chen",
      "is_highlight": false,
      "score": 60.0,
      "summary": "GF-Screen introduces a novel reinforcement learning framework that enhances pan-cancer screening by effectively localizing and segmenting lesions in CT scans, outperforming existing methods.",
      "teaser_image": "https://arxiv.org/html/2601.19103/x3.png",
      "debug_abstract": "Pan-cancer screening in large-scale CT scans remains challenging for existing AI methods, primarily due to the difficulty of localizing diverse types of tiny lesions in large CT volumes. The extreme foreground-background imbalance significantly hinders models from focusing on diseased regions, while redundant focus on healthy regions not only decreases the efficiency but also increases false positives. Inspired by radiologists&#39; glance and focus diagnostic strategy, we introduce GF-Screen, a Glance and Focus reinforcement learning framework for pan-cancer screening. GF-Screen employs a Glance model to localize the diseased regions and a Focus model to precisely segment the lesions, where segmentation results of the Focus model are leveraged to reward the Glance model via Reinforcement Learning (RL). Specifically, the Glance model crops a group of sub-volumes from the entire CT volume and learns to select the sub-volumes with lesions for the Focus model to segment. Given that the selecting operation is non-differentiable for segmentation training, we propose to employ the segmentation results to reward the Glance model. To optimize the Glance model, we introduce a novel group relative learning paradigm, which employs group relative comparison to prioritize high-advantage predictions and discard low-advantage predictions within sub-volume groups, not only improving efficiency but also reducing false positives. In this way, for the first time, we effectively extend cutting-edge RL techniques to tackle the specific challenges in pan-cancer screening. Extensive experiments on 16 internal and 7 external datasets across 9 lesion types demonstrated the effectiveness of GF-Screen. Notably, GF-Screen leads the public validation leaderboard of MICCAI FLARE25 pan-cancer challenge, surpassing the FLARE24 champion solution by a large margin (+25.6% DSC and +28.2% NSD)."
    },
    {
      "title": "Fast and Safe Trajectory Optimization for Mobile Manipulators With Neural Configuration Space Distance Field",
      "url": "https://arxiv.org/abs/2601.18548",
      "date": "2026-01-27",
      "authors_text": "Yulin Li, Zhiyuan Song, Yiming Li, Zhicheng Song, Kai Chen",
      "is_highlight": false,
      "score": 88.0,
      "summary": "The paper presents Generalized Configuration Space Distance Fields (GCDF) for efficient trajectory optimization in mobile manipulators, enhancing collision reasoning and enabling rapid replanning in complex environments.",
      "teaser_image": "https://arxiv.org/html/2601.18548/x2.png",
      "debug_abstract": "Mobile manipulators promise agile, long-horizon behavior by coordinating base and arm motion, yet whole-body trajectory optimization in cluttered, confined spaces remains difficult due to high-dimensional nonconvexity and the need for fast, accurate collision reasoning. Configuration Space Distance Fields (CDF) enable fixed-base manipulators to model collisions directly in configuration space via smooth, implicit distances. This representation holds strong potential to bypass the nonlinear configuration-to-workspace mapping while preserving accurate whole-body geometry and providing optimization-friendly collision costs. Yet, extending this capability to mobile manipulators is hindered by unbounded workspaces and tighter base-arm coupling. We lift this promise to mobile manipulation with Generalized Configuration Space Distance Fields (GCDF), extending CDF to robots with both translational and rotational joints in unbounded workspaces with tighter base-arm coupling. We prove that GCDF preserves Euclidean-like local distance structure and accurately encodes whole-body geometry in configuration space, and develop a data generation and training pipeline that yields continuous neural GCDFs with accurate values and gradients, supporting efficient GPU-batched queries. Building on this representation, we develop a high-performance sequential convex optimization framework centered on GCDF-based collision reasoning. The solver scales to large numbers of implicit constraints through (i) online specification of neural constraints, (ii) sparsity-aware active-set detection with parallel batched evaluation across thousands of constraints, and (iii) incremental constraint management for rapid replanning under scene changes."
    }
  ],
  "Microsystem Engineering and Robotics": [
    {
      "title": "Video-KTR: Reinforcing Video Reasoning via Key Token Attribution",
      "url": "https://arxiv.org/abs/2601.19686",
      "date": "2026-01-27",
      "authors_text": "Ziyue Wang, Sheng Jin, Zhongrong Zuo, Jiawei Wu, Han Qiu",
      "is_highlight": false,
      "score": 67.0,
      "summary": "Video-KTR enhances video reasoning in multimodal models by selectively reinforcing key tokens through a novel attribution framework, achieving state-of-the-art performance across multiple benchmarks.",
      "teaser_image": "https://arxiv.org/html/2601.19686/x1.png",
      "debug_abstract": "Reinforcement learning (RL) has shown strong potential for enhancing reasoning in multimodal large language models, yet existing video reasoning methods often rely on coarse sequence-level rewards or single-factor token selection, neglecting fine-grained links among visual inputs, temporal dynamics, and linguistic outputs, limiting both accuracy and interpretability. We propose Video-KTR, a modality-aware policy shaping framework that performs selective, token-level RL by combining three attribution signals: (1) visual-aware tokens identified via counterfactual masking to reveal perceptual dependence; (2) temporal-aware tokens detected through frame shuffling to expose temporal sensitivity; and (3) high-entropy tokens signaling predictive uncertainty. By reinforcing only these key tokens, Video-KTR focuses learning on semantically informative, modality-sensitive content while filtering out low-value tokens. Across five challenging benchmarks, Video-KTR achieves state-of-the-art or highly competitive results, achieving 42.7\\% on Video-Holmes (surpassing GPT-4o) with consistent gains on both reasoning and general video understanding tasks. Ablation studies verify the complementary roles of the attribution signals and the robustness of targeted token-level updates. Overall, Video-KTR improves accuracy and interpretability, offering a simple, drop-in extension to RL for complex video reasoning. Our code and models are available at this https URL."
    },
    {
      "title": "Curiosity Driven Knowledge Retrieval for Mobile Agents",
      "url": "https://arxiv.org/abs/2601.19306",
      "date": "2026-01-27",
      "authors_text": "Sijia Li, Xiaoyu Tan, Shahir Ali, Niels Schmidt, Gengchen Ma",
      "is_highlight": false,
      "score": 80.0,
      "summary": "The paper presents a curiosity-driven knowledge retrieval framework for mobile agents that enhances performance in complex applications by integrating structured AppCards during execution.",
      "teaser_image": "https://arxiv.org/html/2601.19306/main_framework.png",
      "debug_abstract": "Mobile agents have made progress toward reliable smartphone automation, yet performance in complex applications remains limited by incomplete knowledge and weak generalization to unseen environments. We introduce a curiosity driven knowledge retrieval framework that formalizes uncertainty during execution as a curiosity score. When this score exceeds a threshold, the system retrieves external information from documentation, code repositories, and historical trajectories. Retrieved content is organized into structured AppCards, which encode functional semantics, parameter conventions, interface mappings, and interaction patterns. During execution, an enhanced agent selectively integrates relevant AppCards into its reasoning process, thereby compensating for knowledge blind spots and improving planning reliability. Evaluation on the AndroidWorld benchmark shows consistent improvements across backbones, with an average gain of six percentage points and a new state of the art success rate of 88.8\\% when combined with GPT-5. Analysis indicates that AppCards are particularly effective for multi step and cross application tasks, while improvements depend on the backbone model. Case studies further confirm that AppCards reduce ambiguity, shorten exploration, and support stable execution trajectories. Task trajectories are publicly available at this https URL."
    },
    {
      "title": "Fast and Safe Trajectory Optimization for Mobile Manipulators With Neural Configuration Space Distance Field",
      "url": "https://arxiv.org/abs/2601.18548",
      "date": "2026-01-27",
      "authors_text": "Yulin Li, Zhiyuan Song, Yiming Li, Zhicheng Song, Kai Chen",
      "is_highlight": false,
      "score": 88.0,
      "summary": "The paper presents Generalized Configuration Space Distance Fields (GCDF) for efficient trajectory optimization in mobile manipulators, enhancing collision reasoning and enabling rapid replanning in complex environments.",
      "teaser_image": "https://arxiv.org/html/2601.18548/x2.png",
      "debug_abstract": "Mobile manipulators promise agile, long-horizon behavior by coordinating base and arm motion, yet whole-body trajectory optimization in cluttered, confined spaces remains difficult due to high-dimensional nonconvexity and the need for fast, accurate collision reasoning. Configuration Space Distance Fields (CDF) enable fixed-base manipulators to model collisions directly in configuration space via smooth, implicit distances. This representation holds strong potential to bypass the nonlinear configuration-to-workspace mapping while preserving accurate whole-body geometry and providing optimization-friendly collision costs. Yet, extending this capability to mobile manipulators is hindered by unbounded workspaces and tighter base-arm coupling. We lift this promise to mobile manipulation with Generalized Configuration Space Distance Fields (GCDF), extending CDF to robots with both translational and rotational joints in unbounded workspaces with tighter base-arm coupling. We prove that GCDF preserves Euclidean-like local distance structure and accurately encodes whole-body geometry in configuration space, and develop a data generation and training pipeline that yields continuous neural GCDFs with accurate values and gradients, supporting efficient GPU-batched queries. Building on this representation, we develop a high-performance sequential convex optimization framework centered on GCDF-based collision reasoning. The solver scales to large numbers of implicit constraints through (i) online specification of neural constraints, (ii) sparsity-aware active-set detection with parallel batched evaluation across thousands of constraints, and (iii) incremental constraint management for rapid replanning under scene changes."
    }
  ],
  "机器人研究所": [
    {
      "title": "Geometry-Grounded Gaussian Splatting",
      "url": "https://arxiv.org/abs/2601.17835",
      "date": "2026-01-27",
      "authors_text": "Baowen Zhang, Chenxing Jiang, Heng Li, Shaojie Shen, Ping Tan",
      "is_highlight": true,
      "score": 73.0,
      "summary": "This paper introduces Geometry-Grounded Gaussian Splatting, enhancing shape extraction from Gaussian primitives through a theoretical framework that improves multi-view consistency and depth map quality.",
      "teaser_image": "https://arxiv.org/html/2601.17835/x2.png",
      "debug_abstract": "Gaussian Splatting (GS) has demonstrated impressive quality and efficiency in novel view synthesis. However, shape extraction from Gaussian primitives remains an open problem. Due to inadequate geometry parameterization and approximation, existing shape reconstruction methods suffer from poor multi-view consistency and are sensitive to floaters. In this paper, we present a rigorous theoretical derivation that establishes Gaussian primitives as a specific type of stochastic solids. This theoretical framework provides a principled foundation for Geometry-Grounded Gaussian Splatting by enabling the direct treatment of Gaussian primitives as explicit geometric representations. Using the volumetric nature of stochastic solids, our method efficiently renders high-quality depth maps for fine-grained geometry extraction. Experiments show that our method achieves the best shape reconstruction results among all Gaussian Splatting-based methods on public datasets."
    },
    {
      "title": "Enhancing Inverse Perspective Mapping for Automatic Vectorized Road Map Generation",
      "url": "https://arxiv.org/abs/2601.19536",
      "date": "2026-01-27",
      "authors_text": "Hongji Liu, Linwei Zheng, Yongjian Li, Mingkai Tang, Xiaoyang Yan",
      "is_highlight": false,
      "score": 66.0,
      "summary": "This study introduces a cost-effective framework using enhanced inverse perspective mapping to generate high-precision vectorized road maps with improved accuracy and reduced mapping errors.",
      "teaser_image": "https://arxiv.org/html/2601.19536/x2.png",
      "debug_abstract": "In this study, we present a low-cost and unified framework for vectorized road mapping leveraging enhanced inverse perspective mapping (IPM). In this framework, Catmull-Rom splines are utilized to characterize lane lines, and all the other ground markings are depicted using polygons uniformly. The results from instance segmentation serve as references to refine the three-dimensional position of spline control points and polygon corner points. In conjunction with this process, the homography matrix of IPM and vehicle poses are optimized simultaneously. Our proposed framework significantly reduces the mapping errors associated with IPM. It also improves the accuracy of the initial IPM homography matrix and the predicted vehicle poses. Furthermore, it addresses the limitations imposed by the coplanarity assumption in IPM. These enhancements enable IPM to be effectively applied to vectorized road mapping, which serves a cost-effective solution with enhanced accuracy. In addition, our framework generalizes road map elements to include all common ground markings and lane lines. The proposed framework is evaluated in two different practical scenarios, and the test results show that our method can automatically generate high-precision maps with near-centimeter-level accuracy. Importantly, the optimized IPM matrix achieves an accuracy comparable to that of manual calibration, while the accuracy of vehicle poses is also significantly improved."
    },
    {
      "title": "Glance and Focus Reinforcement for Pan-cancer Screening",
      "url": "https://arxiv.org/abs/2601.19103",
      "date": "2026-01-27",
      "authors_text": "Linshan Wu, Jiaxin Zhuang, Hao Chen",
      "is_highlight": false,
      "score": 60.0,
      "summary": "GF-Screen introduces a novel reinforcement learning framework that enhances pan-cancer screening by effectively localizing and segmenting lesions in CT scans, outperforming existing methods.",
      "teaser_image": "https://arxiv.org/html/2601.19103/x3.png",
      "debug_abstract": "Pan-cancer screening in large-scale CT scans remains challenging for existing AI methods, primarily due to the difficulty of localizing diverse types of tiny lesions in large CT volumes. The extreme foreground-background imbalance significantly hinders models from focusing on diseased regions, while redundant focus on healthy regions not only decreases the efficiency but also increases false positives. Inspired by radiologists&#39; glance and focus diagnostic strategy, we introduce GF-Screen, a Glance and Focus reinforcement learning framework for pan-cancer screening. GF-Screen employs a Glance model to localize the diseased regions and a Focus model to precisely segment the lesions, where segmentation results of the Focus model are leveraged to reward the Glance model via Reinforcement Learning (RL). Specifically, the Glance model crops a group of sub-volumes from the entire CT volume and learns to select the sub-volumes with lesions for the Focus model to segment. Given that the selecting operation is non-differentiable for segmentation training, we propose to employ the segmentation results to reward the Glance model. To optimize the Glance model, we introduce a novel group relative learning paradigm, which employs group relative comparison to prioritize high-advantage predictions and discard low-advantage predictions within sub-volume groups, not only improving efficiency but also reducing false positives. In this way, for the first time, we effectively extend cutting-edge RL techniques to tackle the specific challenges in pan-cancer screening. Extensive experiments on 16 internal and 7 external datasets across 9 lesion types demonstrated the effectiveness of GF-Screen. Notably, GF-Screen leads the public validation leaderboard of MICCAI FLARE25 pan-cancer challenge, surpassing the FLARE24 champion solution by a large margin (+25.6% DSC and +28.2% NSD)."
    },
    {
      "title": "Fast and Safe Trajectory Optimization for Mobile Manipulators With Neural Configuration Space Distance Field",
      "url": "https://arxiv.org/abs/2601.18548",
      "date": "2026-01-27",
      "authors_text": "Yulin Li, Zhiyuan Song, Yiming Li, Zhicheng Song, Kai Chen",
      "is_highlight": false,
      "score": 88.0,
      "summary": "The paper presents Generalized Configuration Space Distance Fields (GCDF) for efficient trajectory optimization in mobile manipulators, enhancing collision reasoning and enabling rapid replanning in complex environments.",
      "teaser_image": "https://arxiv.org/html/2601.18548/x2.png",
      "debug_abstract": "Mobile manipulators promise agile, long-horizon behavior by coordinating base and arm motion, yet whole-body trajectory optimization in cluttered, confined spaces remains difficult due to high-dimensional nonconvexity and the need for fast, accurate collision reasoning. Configuration Space Distance Fields (CDF) enable fixed-base manipulators to model collisions directly in configuration space via smooth, implicit distances. This representation holds strong potential to bypass the nonlinear configuration-to-workspace mapping while preserving accurate whole-body geometry and providing optimization-friendly collision costs. Yet, extending this capability to mobile manipulators is hindered by unbounded workspaces and tighter base-arm coupling. We lift this promise to mobile manipulation with Generalized Configuration Space Distance Fields (GCDF), extending CDF to robots with both translational and rotational joints in unbounded workspaces with tighter base-arm coupling. We prove that GCDF preserves Euclidean-like local distance structure and accurately encodes whole-body geometry in configuration space, and develop a data generation and training pipeline that yields continuous neural GCDFs with accurate values and gradients, supporting efficient GPU-batched queries. Building on this representation, we develop a high-performance sequential convex optimization framework centered on GCDF-based collision reasoning. The solver scales to large numbers of implicit constraints through (i) online specification of neural constraints, (ii) sparsity-aware active-set detection with parallel batched evaluation across thousands of constraints, and (iii) incremental constraint management for rapid replanning under scene changes."
    }
  ],
  "Hengshuang Zhao老师实验室": [
    {
      "title": "EgoHandICL: Egocentric 3D Hand Reconstruction with In-Context Learning",
      "url": "https://arxiv.org/abs/2601.19850",
      "date": "2026-01-27",
      "authors_text": "Binzhu Xie, Shi Qiu, Sicheng Zhang, Yinqiao Wang, Hao Xu",
      "is_highlight": false,
      "score": 76.0,
      "summary": "EgoHandICL introduces an innovative in-context learning framework for robust 3D hand reconstruction in egocentric vision, enhancing performance through multimodal context and exemplar retrieval.",
      "teaser_image": "https://arxiv.org/html/2601.19850/x2.png",
      "debug_abstract": "Robust 3D hand reconstruction in egocentric vision is challenging due to depth ambiguity, self-occlusion, and complex hand-object interactions. Prior methods mitigate these issues by scaling training data or adding auxiliary cues, but they often struggle in unseen contexts. We present EgoHandICL, the first in-context learning (ICL) framework for 3D hand reconstruction that improves semantic alignment, visual consistency, and robustness under challenging egocentric conditions. EgoHandICL introduces complementary exemplar retrieval guided by vision-language models (VLMs), an ICL-tailored tokenizer for multimodal context, and a masked autoencoder (MAE)-based architecture trained with hand-guided geometric and perceptual objectives. Experiments on ARCTIC and EgoExo4D show consistent gains over state-of-the-art methods. We also demonstrate real-world generalization and improve EgoVLM hand-object interaction reasoning by using reconstructed hands as visual prompts. Code and data: this https URL"
    },
    {
      "title": "Component-Level Lesioning of Language Models Reveals Clinically Aligned Aphasia Phenotypes",
      "url": "https://arxiv.org/abs/2601.19723",
      "date": "2026-01-27",
      "authors_text": "Yifan Wang, Jichen Zheng, Jingyuan Sun, Yunhao Zhang, Chunyu Ye",
      "is_highlight": false,
      "score": 61.0,
      "summary": "This paper presents a framework for simulating aphasia in language models by selectively perturbing components, enabling insights into language production impairments and rehabilitation strategies.",
      "teaser_image": "https://arxiv.org/html/2601.19723/x1.png",
      "debug_abstract": "Large language models (LLMs) increasingly exhibit human-like linguistic behaviors and internal representations that they could serve as computational simulators of language cognition. We ask whether LLMs can be systematically manipulated to reproduce language-production impairments characteristic of aphasia following focal brain lesions. Such models could provide scalable proxies for testing rehabilitation hypotheses, and offer a controlled framework for probing the functional organization of language. We introduce a clinically grounded, component-level framework that simulates aphasia by selectively perturbing functional components in LLMs, and apply it to both modular Mixture-of-Experts models and dense Transformers using a unified intervention interface. Our pipeline (i) identifies subtype-linked components for Broca&#39;s and Wernicke&#39;s aphasia, (ii) interprets these components with linguistic probing tasks, and (iii) induces graded impairments by progressively perturbing the top-k subtype-linked components, evaluating outcomes with Western Aphasia Battery (WAB) subtests summarized by Aphasia Quotient (AQ). Across architectures and lesioning strategies, subtype-targeted perturbations yield more systematic, aphasia-like regressions than size-matched random perturbations, and MoE modularity supports more localized and interpretable phenotype-to-component mappings. These findings suggest that modular LLMs, combined with clinically informed component perturbations, provide a promising platform for simulating aphasic language production and studying how distinct language functions degrade under targeted disruptions."
    },
    {
      "title": "Rhombot: Rhombus-shaped Modular Robots for Stable, Medium-Independent Reconfiguration Motion",
      "url": "https://arxiv.org/abs/2601.19529",
      "date": "2026-01-27",
      "authors_text": "Jie Gu, Yirui Sun, Zhihao Xia, Tin Lun Lam, Chunxu Tian",
      "is_highlight": false,
      "score": 91.0,
      "summary": "Rhombot is a novel rhombus-shaped modular robot enabling stable, medium-independent reconfiguration through morphpivoting, validated by experiments demonstrating its morphing and docking capabilities.",
      "teaser_image": "https://arxiv.org/html/2601.19529/x2.png",
      "debug_abstract": "In this paper, we present Rhombot, a novel deformable planar lattice modular self-reconfigurable robot (MSRR) with a rhombus shaped module. Each module consists of a parallelogram skeleton with a single centrally mounted actuator that enables folding and unfolding along its diagonal. The core design philosophy is to achieve essential MSRR functionalities such as morphing, docking, and locomotion with minimal control complexity. This enables a continuous and stable reconfiguration process that is independent of the surrounding medium, allowing the system to reliably form various configurations in diverse environments. To leverage the unique kinematics of Rhombot, we introduce morphpivoting, a novel motion primitive for reconfiguration that differs from advanced MSRR systems, and propose a strategy for its continuous execution. Finally, a series of physical experiments validate the module&#39;s stable reconfiguration ability, as well as its positional and docking accuracy."
    },
    {
      "title": "Fast and Safe Trajectory Optimization for Mobile Manipulators With Neural Configuration Space Distance Field",
      "url": "https://arxiv.org/abs/2601.18548",
      "date": "2026-01-27",
      "authors_text": "Yulin Li, Zhiyuan Song, Yiming Li, Zhicheng Song, Kai Chen",
      "is_highlight": false,
      "score": 88.0,
      "summary": "The paper presents Generalized Configuration Space Distance Fields (GCDF) for efficient trajectory optimization in mobile manipulators, enhancing collision reasoning and enabling rapid replanning in complex environments.",
      "teaser_image": "https://arxiv.org/html/2601.18548/x2.png",
      "debug_abstract": "Mobile manipulators promise agile, long-horizon behavior by coordinating base and arm motion, yet whole-body trajectory optimization in cluttered, confined spaces remains difficult due to high-dimensional nonconvexity and the need for fast, accurate collision reasoning. Configuration Space Distance Fields (CDF) enable fixed-base manipulators to model collisions directly in configuration space via smooth, implicit distances. This representation holds strong potential to bypass the nonlinear configuration-to-workspace mapping while preserving accurate whole-body geometry and providing optimization-friendly collision costs. Yet, extending this capability to mobile manipulators is hindered by unbounded workspaces and tighter base-arm coupling. We lift this promise to mobile manipulation with Generalized Configuration Space Distance Fields (GCDF), extending CDF to robots with both translational and rotational joints in unbounded workspaces with tighter base-arm coupling. We prove that GCDF preserves Euclidean-like local distance structure and accurately encodes whole-body geometry in configuration space, and develop a data generation and training pipeline that yields continuous neural GCDFs with accurate values and gradients, supporting efficient GPU-batched queries. Building on this representation, we develop a high-performance sequential convex optimization framework centered on GCDF-based collision reasoning. The solver scales to large numbers of implicit constraints through (i) online specification of neural constraints, (ii) sparsity-aware active-set detection with parallel batched evaluation across thousands of constraints, and (iii) incremental constraint management for rapid replanning under scene changes."
    }
  ],
  "Cheng Kar-Shun Robotics Institute (CKSRI)": [
    {
      "title": "Geometry-Grounded Gaussian Splatting",
      "url": "https://arxiv.org/abs/2601.17835",
      "date": "2026-01-27",
      "authors_text": "Baowen Zhang, Chenxing Jiang, Heng Li, Shaojie Shen, Ping Tan",
      "is_highlight": true,
      "score": 73.0,
      "summary": "This paper introduces Geometry-Grounded Gaussian Splatting, enhancing shape extraction from Gaussian primitives through a theoretical framework that improves multi-view consistency and depth map quality.",
      "teaser_image": "https://arxiv.org/html/2601.17835/x2.png",
      "debug_abstract": "Gaussian Splatting (GS) has demonstrated impressive quality and efficiency in novel view synthesis. However, shape extraction from Gaussian primitives remains an open problem. Due to inadequate geometry parameterization and approximation, existing shape reconstruction methods suffer from poor multi-view consistency and are sensitive to floaters. In this paper, we present a rigorous theoretical derivation that establishes Gaussian primitives as a specific type of stochastic solids. This theoretical framework provides a principled foundation for Geometry-Grounded Gaussian Splatting by enabling the direct treatment of Gaussian primitives as explicit geometric representations. Using the volumetric nature of stochastic solids, our method efficiently renders high-quality depth maps for fine-grained geometry extraction. Experiments show that our method achieves the best shape reconstruction results among all Gaussian Splatting-based methods on public datasets."
    },
    {
      "title": "Enhancing Inverse Perspective Mapping for Automatic Vectorized Road Map Generation",
      "url": "https://arxiv.org/abs/2601.19536",
      "date": "2026-01-27",
      "authors_text": "Hongji Liu, Linwei Zheng, Yongjian Li, Mingkai Tang, Xiaoyang Yan",
      "is_highlight": false,
      "score": 66.0,
      "summary": "This study introduces a cost-effective framework using enhanced inverse perspective mapping to generate high-precision vectorized road maps with improved accuracy and reduced mapping errors.",
      "teaser_image": "https://arxiv.org/html/2601.19536/x2.png",
      "debug_abstract": "In this study, we present a low-cost and unified framework for vectorized road mapping leveraging enhanced inverse perspective mapping (IPM). In this framework, Catmull-Rom splines are utilized to characterize lane lines, and all the other ground markings are depicted using polygons uniformly. The results from instance segmentation serve as references to refine the three-dimensional position of spline control points and polygon corner points. In conjunction with this process, the homography matrix of IPM and vehicle poses are optimized simultaneously. Our proposed framework significantly reduces the mapping errors associated with IPM. It also improves the accuracy of the initial IPM homography matrix and the predicted vehicle poses. Furthermore, it addresses the limitations imposed by the coplanarity assumption in IPM. These enhancements enable IPM to be effectively applied to vectorized road mapping, which serves a cost-effective solution with enhanced accuracy. In addition, our framework generalizes road map elements to include all common ground markings and lane lines. The proposed framework is evaluated in two different practical scenarios, and the test results show that our method can automatically generate high-precision maps with near-centimeter-level accuracy. Importantly, the optimized IPM matrix achieves an accuracy comparable to that of manual calibration, while the accuracy of vehicle poses is also significantly improved."
    },
    {
      "title": "Glance and Focus Reinforcement for Pan-cancer Screening",
      "url": "https://arxiv.org/abs/2601.19103",
      "date": "2026-01-27",
      "authors_text": "Linshan Wu, Jiaxin Zhuang, Hao Chen",
      "is_highlight": false,
      "score": 60.0,
      "summary": "GF-Screen introduces a novel reinforcement learning framework that enhances pan-cancer screening by effectively localizing and segmenting lesions in CT scans, outperforming existing methods.",
      "teaser_image": "https://arxiv.org/html/2601.19103/x3.png",
      "debug_abstract": "Pan-cancer screening in large-scale CT scans remains challenging for existing AI methods, primarily due to the difficulty of localizing diverse types of tiny lesions in large CT volumes. The extreme foreground-background imbalance significantly hinders models from focusing on diseased regions, while redundant focus on healthy regions not only decreases the efficiency but also increases false positives. Inspired by radiologists&#39; glance and focus diagnostic strategy, we introduce GF-Screen, a Glance and Focus reinforcement learning framework for pan-cancer screening. GF-Screen employs a Glance model to localize the diseased regions and a Focus model to precisely segment the lesions, where segmentation results of the Focus model are leveraged to reward the Glance model via Reinforcement Learning (RL). Specifically, the Glance model crops a group of sub-volumes from the entire CT volume and learns to select the sub-volumes with lesions for the Focus model to segment. Given that the selecting operation is non-differentiable for segmentation training, we propose to employ the segmentation results to reward the Glance model. To optimize the Glance model, we introduce a novel group relative learning paradigm, which employs group relative comparison to prioritize high-advantage predictions and discard low-advantage predictions within sub-volume groups, not only improving efficiency but also reducing false positives. In this way, for the first time, we effectively extend cutting-edge RL techniques to tackle the specific challenges in pan-cancer screening. Extensive experiments on 16 internal and 7 external datasets across 9 lesion types demonstrated the effectiveness of GF-Screen. Notably, GF-Screen leads the public validation leaderboard of MICCAI FLARE25 pan-cancer challenge, surpassing the FLARE24 champion solution by a large margin (+25.6% DSC and +28.2% NSD)."
    },
    {
      "title": "Fast and Safe Trajectory Optimization for Mobile Manipulators With Neural Configuration Space Distance Field",
      "url": "https://arxiv.org/abs/2601.18548",
      "date": "2026-01-27",
      "authors_text": "Yulin Li, Zhiyuan Song, Yiming Li, Zhicheng Song, Kai Chen",
      "is_highlight": false,
      "score": 88.0,
      "summary": "The paper presents Generalized Configuration Space Distance Fields (GCDF) for efficient trajectory optimization in mobile manipulators, enhancing collision reasoning and enabling rapid replanning in complex environments.",
      "teaser_image": "https://arxiv.org/html/2601.18548/x2.png",
      "debug_abstract": "Mobile manipulators promise agile, long-horizon behavior by coordinating base and arm motion, yet whole-body trajectory optimization in cluttered, confined spaces remains difficult due to high-dimensional nonconvexity and the need for fast, accurate collision reasoning. Configuration Space Distance Fields (CDF) enable fixed-base manipulators to model collisions directly in configuration space via smooth, implicit distances. This representation holds strong potential to bypass the nonlinear configuration-to-workspace mapping while preserving accurate whole-body geometry and providing optimization-friendly collision costs. Yet, extending this capability to mobile manipulators is hindered by unbounded workspaces and tighter base-arm coupling. We lift this promise to mobile manipulation with Generalized Configuration Space Distance Fields (GCDF), extending CDF to robots with both translational and rotational joints in unbounded workspaces with tighter base-arm coupling. We prove that GCDF preserves Euclidean-like local distance structure and accurately encodes whole-body geometry in configuration space, and develop a data generation and training pipeline that yields continuous neural GCDFs with accurate values and gradients, supporting efficient GPU-batched queries. Building on this representation, we develop a high-performance sequential convex optimization framework centered on GCDF-based collision reasoning. The solver scales to large numbers of implicit constraints through (i) online specification of neural constraints, (ii) sparsity-aware active-set detection with parallel batched evaluation across thousands of constraints, and (iii) incremental constraint management for rapid replanning under scene changes."
    }
  ],
  "Synteraction Lab": [
    {
      "title": "Video-KTR: Reinforcing Video Reasoning via Key Token Attribution",
      "url": "https://arxiv.org/abs/2601.19686",
      "date": "2026-01-27",
      "authors_text": "Ziyue Wang, Sheng Jin, Zhongrong Zuo, Jiawei Wu, Han Qiu",
      "is_highlight": false,
      "score": 67.0,
      "summary": "Video-KTR enhances video reasoning in multimodal models by selectively reinforcing key tokens through a novel attribution framework, achieving state-of-the-art performance across multiple benchmarks.",
      "teaser_image": "https://arxiv.org/html/2601.19686/x1.png",
      "debug_abstract": "Reinforcement learning (RL) has shown strong potential for enhancing reasoning in multimodal large language models, yet existing video reasoning methods often rely on coarse sequence-level rewards or single-factor token selection, neglecting fine-grained links among visual inputs, temporal dynamics, and linguistic outputs, limiting both accuracy and interpretability. We propose Video-KTR, a modality-aware policy shaping framework that performs selective, token-level RL by combining three attribution signals: (1) visual-aware tokens identified via counterfactual masking to reveal perceptual dependence; (2) temporal-aware tokens detected through frame shuffling to expose temporal sensitivity; and (3) high-entropy tokens signaling predictive uncertainty. By reinforcing only these key tokens, Video-KTR focuses learning on semantically informative, modality-sensitive content while filtering out low-value tokens. Across five challenging benchmarks, Video-KTR achieves state-of-the-art or highly competitive results, achieving 42.7\\% on Video-Holmes (surpassing GPT-4o) with consistent gains on both reasoning and general video understanding tasks. Ablation studies verify the complementary roles of the attribution signals and the robustness of targeted token-level updates. Overall, Video-KTR improves accuracy and interpretability, offering a simple, drop-in extension to RL for complex video reasoning. Our code and models are available at this https URL."
    },
    {
      "title": "Curiosity Driven Knowledge Retrieval for Mobile Agents",
      "url": "https://arxiv.org/abs/2601.19306",
      "date": "2026-01-27",
      "authors_text": "Sijia Li, Xiaoyu Tan, Shahir Ali, Niels Schmidt, Gengchen Ma",
      "is_highlight": false,
      "score": 80.0,
      "summary": "The paper presents a curiosity-driven knowledge retrieval framework for mobile agents that enhances performance in complex applications by integrating structured AppCards during execution.",
      "teaser_image": "https://arxiv.org/html/2601.19306/main_framework.png",
      "debug_abstract": "Mobile agents have made progress toward reliable smartphone automation, yet performance in complex applications remains limited by incomplete knowledge and weak generalization to unseen environments. We introduce a curiosity driven knowledge retrieval framework that formalizes uncertainty during execution as a curiosity score. When this score exceeds a threshold, the system retrieves external information from documentation, code repositories, and historical trajectories. Retrieved content is organized into structured AppCards, which encode functional semantics, parameter conventions, interface mappings, and interaction patterns. During execution, an enhanced agent selectively integrates relevant AppCards into its reasoning process, thereby compensating for knowledge blind spots and improving planning reliability. Evaluation on the AndroidWorld benchmark shows consistent improvements across backbones, with an average gain of six percentage points and a new state of the art success rate of 88.8\\% when combined with GPT-5. Analysis indicates that AppCards are particularly effective for multi step and cross application tasks, while improvements depend on the backbone model. Case studies further confirm that AppCards reduce ambiguity, shorten exploration, and support stable execution trajectories. Task trajectories are publicly available at this https URL."
    },
    {
      "title": "Fast and Safe Trajectory Optimization for Mobile Manipulators With Neural Configuration Space Distance Field",
      "url": "https://arxiv.org/abs/2601.18548",
      "date": "2026-01-27",
      "authors_text": "Yulin Li, Zhiyuan Song, Yiming Li, Zhicheng Song, Kai Chen",
      "is_highlight": false,
      "score": 88.0,
      "summary": "The paper presents Generalized Configuration Space Distance Fields (GCDF) for efficient trajectory optimization in mobile manipulators, enhancing collision reasoning and enabling rapid replanning in complex environments.",
      "teaser_image": "https://arxiv.org/html/2601.18548/x2.png",
      "debug_abstract": "Mobile manipulators promise agile, long-horizon behavior by coordinating base and arm motion, yet whole-body trajectory optimization in cluttered, confined spaces remains difficult due to high-dimensional nonconvexity and the need for fast, accurate collision reasoning. Configuration Space Distance Fields (CDF) enable fixed-base manipulators to model collisions directly in configuration space via smooth, implicit distances. This representation holds strong potential to bypass the nonlinear configuration-to-workspace mapping while preserving accurate whole-body geometry and providing optimization-friendly collision costs. Yet, extending this capability to mobile manipulators is hindered by unbounded workspaces and tighter base-arm coupling. We lift this promise to mobile manipulation with Generalized Configuration Space Distance Fields (GCDF), extending CDF to robots with both translational and rotational joints in unbounded workspaces with tighter base-arm coupling. We prove that GCDF preserves Euclidean-like local distance structure and accurately encodes whole-body geometry in configuration space, and develop a data generation and training pipeline that yields continuous neural GCDFs with accurate values and gradients, supporting efficient GPU-batched queries. Building on this representation, we develop a high-performance sequential convex optimization framework centered on GCDF-based collision reasoning. The solver scales to large numbers of implicit constraints through (i) online specification of neural constraints, (ii) sparsity-aware active-set detection with parallel batched evaluation across thousands of constraints, and (iii) incremental constraint management for rapid replanning under scene changes."
    }
  ],
  "NUS AI LAB": [
    {
      "title": "Video-KTR: Reinforcing Video Reasoning via Key Token Attribution",
      "url": "https://arxiv.org/abs/2601.19686",
      "date": "2026-01-27",
      "authors_text": "Ziyue Wang, Sheng Jin, Zhongrong Zuo, Jiawei Wu, Han Qiu",
      "is_highlight": false,
      "score": 67.0,
      "summary": "Video-KTR enhances video reasoning in multimodal models by selectively reinforcing key tokens through a novel attribution framework, achieving state-of-the-art performance across multiple benchmarks.",
      "teaser_image": "https://arxiv.org/html/2601.19686/x1.png",
      "debug_abstract": "Reinforcement learning (RL) has shown strong potential for enhancing reasoning in multimodal large language models, yet existing video reasoning methods often rely on coarse sequence-level rewards or single-factor token selection, neglecting fine-grained links among visual inputs, temporal dynamics, and linguistic outputs, limiting both accuracy and interpretability. We propose Video-KTR, a modality-aware policy shaping framework that performs selective, token-level RL by combining three attribution signals: (1) visual-aware tokens identified via counterfactual masking to reveal perceptual dependence; (2) temporal-aware tokens detected through frame shuffling to expose temporal sensitivity; and (3) high-entropy tokens signaling predictive uncertainty. By reinforcing only these key tokens, Video-KTR focuses learning on semantically informative, modality-sensitive content while filtering out low-value tokens. Across five challenging benchmarks, Video-KTR achieves state-of-the-art or highly competitive results, achieving 42.7\\% on Video-Holmes (surpassing GPT-4o) with consistent gains on both reasoning and general video understanding tasks. Ablation studies verify the complementary roles of the attribution signals and the robustness of targeted token-level updates. Overall, Video-KTR improves accuracy and interpretability, offering a simple, drop-in extension to RL for complex video reasoning. Our code and models are available at this https URL."
    },
    {
      "title": "Curiosity Driven Knowledge Retrieval for Mobile Agents",
      "url": "https://arxiv.org/abs/2601.19306",
      "date": "2026-01-27",
      "authors_text": "Sijia Li, Xiaoyu Tan, Shahir Ali, Niels Schmidt, Gengchen Ma",
      "is_highlight": false,
      "score": 80.0,
      "summary": "The paper presents a curiosity-driven knowledge retrieval framework for mobile agents that enhances performance in complex applications by integrating structured AppCards during execution.",
      "teaser_image": "https://arxiv.org/html/2601.19306/main_framework.png",
      "debug_abstract": "Mobile agents have made progress toward reliable smartphone automation, yet performance in complex applications remains limited by incomplete knowledge and weak generalization to unseen environments. We introduce a curiosity driven knowledge retrieval framework that formalizes uncertainty during execution as a curiosity score. When this score exceeds a threshold, the system retrieves external information from documentation, code repositories, and historical trajectories. Retrieved content is organized into structured AppCards, which encode functional semantics, parameter conventions, interface mappings, and interaction patterns. During execution, an enhanced agent selectively integrates relevant AppCards into its reasoning process, thereby compensating for knowledge blind spots and improving planning reliability. Evaluation on the AndroidWorld benchmark shows consistent improvements across backbones, with an average gain of six percentage points and a new state of the art success rate of 88.8\\% when combined with GPT-5. Analysis indicates that AppCards are particularly effective for multi step and cross application tasks, while improvements depend on the backbone model. Case studies further confirm that AppCards reduce ambiguity, shorten exploration, and support stable execution trajectories. Task trajectories are publicly available at this https URL."
    },
    {
      "title": "Fast and Safe Trajectory Optimization for Mobile Manipulators With Neural Configuration Space Distance Field",
      "url": "https://arxiv.org/abs/2601.18548",
      "date": "2026-01-27",
      "authors_text": "Yulin Li, Zhiyuan Song, Yiming Li, Zhicheng Song, Kai Chen",
      "is_highlight": false,
      "score": 88.0,
      "summary": "The paper presents Generalized Configuration Space Distance Fields (GCDF) for efficient trajectory optimization in mobile manipulators, enhancing collision reasoning and enabling rapid replanning in complex environments.",
      "teaser_image": "https://arxiv.org/html/2601.18548/x2.png",
      "debug_abstract": "Mobile manipulators promise agile, long-horizon behavior by coordinating base and arm motion, yet whole-body trajectory optimization in cluttered, confined spaces remains difficult due to high-dimensional nonconvexity and the need for fast, accurate collision reasoning. Configuration Space Distance Fields (CDF) enable fixed-base manipulators to model collisions directly in configuration space via smooth, implicit distances. This representation holds strong potential to bypass the nonlinear configuration-to-workspace mapping while preserving accurate whole-body geometry and providing optimization-friendly collision costs. Yet, extending this capability to mobile manipulators is hindered by unbounded workspaces and tighter base-arm coupling. We lift this promise to mobile manipulation with Generalized Configuration Space Distance Fields (GCDF), extending CDF to robots with both translational and rotational joints in unbounded workspaces with tighter base-arm coupling. We prove that GCDF preserves Euclidean-like local distance structure and accurately encodes whole-body geometry in configuration space, and develop a data generation and training pipeline that yields continuous neural GCDFs with accurate values and gradients, supporting efficient GPU-batched queries. Building on this representation, we develop a high-performance sequential convex optimization framework centered on GCDF-based collision reasoning. The solver scales to large numbers of implicit constraints through (i) online specification of neural constraints, (ii) sparsity-aware active-set detection with parallel batched evaluation across thousands of constraints, and (iii) incremental constraint management for rapid replanning under scene changes."
    }
  ],
  "Advanced Robotics Centre": [
    {
      "title": "Video-KTR: Reinforcing Video Reasoning via Key Token Attribution",
      "url": "https://arxiv.org/abs/2601.19686",
      "date": "2026-01-27",
      "authors_text": "Ziyue Wang, Sheng Jin, Zhongrong Zuo, Jiawei Wu, Han Qiu",
      "is_highlight": false,
      "score": 67.0,
      "summary": "Video-KTR enhances video reasoning in multimodal models by selectively reinforcing key tokens through a novel attribution framework, achieving state-of-the-art performance across multiple benchmarks.",
      "teaser_image": "https://arxiv.org/html/2601.19686/x1.png",
      "debug_abstract": "Reinforcement learning (RL) has shown strong potential for enhancing reasoning in multimodal large language models, yet existing video reasoning methods often rely on coarse sequence-level rewards or single-factor token selection, neglecting fine-grained links among visual inputs, temporal dynamics, and linguistic outputs, limiting both accuracy and interpretability. We propose Video-KTR, a modality-aware policy shaping framework that performs selective, token-level RL by combining three attribution signals: (1) visual-aware tokens identified via counterfactual masking to reveal perceptual dependence; (2) temporal-aware tokens detected through frame shuffling to expose temporal sensitivity; and (3) high-entropy tokens signaling predictive uncertainty. By reinforcing only these key tokens, Video-KTR focuses learning on semantically informative, modality-sensitive content while filtering out low-value tokens. Across five challenging benchmarks, Video-KTR achieves state-of-the-art or highly competitive results, achieving 42.7\\% on Video-Holmes (surpassing GPT-4o) with consistent gains on both reasoning and general video understanding tasks. Ablation studies verify the complementary roles of the attribution signals and the robustness of targeted token-level updates. Overall, Video-KTR improves accuracy and interpretability, offering a simple, drop-in extension to RL for complex video reasoning. Our code and models are available at this https URL."
    },
    {
      "title": "Curiosity Driven Knowledge Retrieval for Mobile Agents",
      "url": "https://arxiv.org/abs/2601.19306",
      "date": "2026-01-27",
      "authors_text": "Sijia Li, Xiaoyu Tan, Shahir Ali, Niels Schmidt, Gengchen Ma",
      "is_highlight": false,
      "score": 80.0,
      "summary": "The paper presents a curiosity-driven knowledge retrieval framework for mobile agents that enhances performance in complex applications by integrating structured AppCards during execution.",
      "teaser_image": "https://arxiv.org/html/2601.19306/main_framework.png",
      "debug_abstract": "Mobile agents have made progress toward reliable smartphone automation, yet performance in complex applications remains limited by incomplete knowledge and weak generalization to unseen environments. We introduce a curiosity driven knowledge retrieval framework that formalizes uncertainty during execution as a curiosity score. When this score exceeds a threshold, the system retrieves external information from documentation, code repositories, and historical trajectories. Retrieved content is organized into structured AppCards, which encode functional semantics, parameter conventions, interface mappings, and interaction patterns. During execution, an enhanced agent selectively integrates relevant AppCards into its reasoning process, thereby compensating for knowledge blind spots and improving planning reliability. Evaluation on the AndroidWorld benchmark shows consistent improvements across backbones, with an average gain of six percentage points and a new state of the art success rate of 88.8\\% when combined with GPT-5. Analysis indicates that AppCards are particularly effective for multi step and cross application tasks, while improvements depend on the backbone model. Case studies further confirm that AppCards reduce ambiguity, shorten exploration, and support stable execution trajectories. Task trajectories are publicly available at this https URL."
    },
    {
      "title": "Fast and Safe Trajectory Optimization for Mobile Manipulators With Neural Configuration Space Distance Field",
      "url": "https://arxiv.org/abs/2601.18548",
      "date": "2026-01-27",
      "authors_text": "Yulin Li, Zhiyuan Song, Yiming Li, Zhicheng Song, Kai Chen",
      "is_highlight": false,
      "score": 88.0,
      "summary": "The paper presents Generalized Configuration Space Distance Fields (GCDF) for efficient trajectory optimization in mobile manipulators, enhancing collision reasoning and enabling rapid replanning in complex environments.",
      "teaser_image": "https://arxiv.org/html/2601.18548/x2.png",
      "debug_abstract": "Mobile manipulators promise agile, long-horizon behavior by coordinating base and arm motion, yet whole-body trajectory optimization in cluttered, confined spaces remains difficult due to high-dimensional nonconvexity and the need for fast, accurate collision reasoning. Configuration Space Distance Fields (CDF) enable fixed-base manipulators to model collisions directly in configuration space via smooth, implicit distances. This representation holds strong potential to bypass the nonlinear configuration-to-workspace mapping while preserving accurate whole-body geometry and providing optimization-friendly collision costs. Yet, extending this capability to mobile manipulators is hindered by unbounded workspaces and tighter base-arm coupling. We lift this promise to mobile manipulation with Generalized Configuration Space Distance Fields (GCDF), extending CDF to robots with both translational and rotational joints in unbounded workspaces with tighter base-arm coupling. We prove that GCDF preserves Euclidean-like local distance structure and accurately encodes whole-body geometry in configuration space, and develop a data generation and training pipeline that yields continuous neural GCDFs with accurate values and gradients, supporting efficient GPU-batched queries. Building on this representation, we develop a high-performance sequential convex optimization framework centered on GCDF-based collision reasoning. The solver scales to large numbers of implicit constraints through (i) online specification of neural constraints, (ii) sparsity-aware active-set detection with parallel batched evaluation across thousands of constraints, and (iii) incremental constraint management for rapid replanning under scene changes."
    }
  ],
  "香港大学机械工程系机器人实验室": [
    {
      "title": "EgoHandICL: Egocentric 3D Hand Reconstruction with In-Context Learning",
      "url": "https://arxiv.org/abs/2601.19850",
      "date": "2026-01-27",
      "authors_text": "Binzhu Xie, Shi Qiu, Sicheng Zhang, Yinqiao Wang, Hao Xu",
      "is_highlight": false,
      "score": 76.0,
      "summary": "EgoHandICL introduces an innovative in-context learning framework for robust 3D hand reconstruction in egocentric vision, enhancing performance through multimodal context and exemplar retrieval.",
      "teaser_image": "https://arxiv.org/html/2601.19850/x2.png",
      "debug_abstract": "Robust 3D hand reconstruction in egocentric vision is challenging due to depth ambiguity, self-occlusion, and complex hand-object interactions. Prior methods mitigate these issues by scaling training data or adding auxiliary cues, but they often struggle in unseen contexts. We present EgoHandICL, the first in-context learning (ICL) framework for 3D hand reconstruction that improves semantic alignment, visual consistency, and robustness under challenging egocentric conditions. EgoHandICL introduces complementary exemplar retrieval guided by vision-language models (VLMs), an ICL-tailored tokenizer for multimodal context, and a masked autoencoder (MAE)-based architecture trained with hand-guided geometric and perceptual objectives. Experiments on ARCTIC and EgoExo4D show consistent gains over state-of-the-art methods. We also demonstrate real-world generalization and improve EgoVLM hand-object interaction reasoning by using reconstructed hands as visual prompts. Code and data: this https URL"
    },
    {
      "title": "Component-Level Lesioning of Language Models Reveals Clinically Aligned Aphasia Phenotypes",
      "url": "https://arxiv.org/abs/2601.19723",
      "date": "2026-01-27",
      "authors_text": "Yifan Wang, Jichen Zheng, Jingyuan Sun, Yunhao Zhang, Chunyu Ye",
      "is_highlight": false,
      "score": 61.0,
      "summary": "This paper presents a framework for simulating aphasia in language models by selectively perturbing components, enabling insights into language production impairments and rehabilitation strategies.",
      "teaser_image": "https://arxiv.org/html/2601.19723/x1.png",
      "debug_abstract": "Large language models (LLMs) increasingly exhibit human-like linguistic behaviors and internal representations that they could serve as computational simulators of language cognition. We ask whether LLMs can be systematically manipulated to reproduce language-production impairments characteristic of aphasia following focal brain lesions. Such models could provide scalable proxies for testing rehabilitation hypotheses, and offer a controlled framework for probing the functional organization of language. We introduce a clinically grounded, component-level framework that simulates aphasia by selectively perturbing functional components in LLMs, and apply it to both modular Mixture-of-Experts models and dense Transformers using a unified intervention interface. Our pipeline (i) identifies subtype-linked components for Broca&#39;s and Wernicke&#39;s aphasia, (ii) interprets these components with linguistic probing tasks, and (iii) induces graded impairments by progressively perturbing the top-k subtype-linked components, evaluating outcomes with Western Aphasia Battery (WAB) subtests summarized by Aphasia Quotient (AQ). Across architectures and lesioning strategies, subtype-targeted perturbations yield more systematic, aphasia-like regressions than size-matched random perturbations, and MoE modularity supports more localized and interpretable phenotype-to-component mappings. These findings suggest that modular LLMs, combined with clinically informed component perturbations, provide a promising platform for simulating aphasic language production and studying how distinct language functions degrade under targeted disruptions."
    },
    {
      "title": "Rhombot: Rhombus-shaped Modular Robots for Stable, Medium-Independent Reconfiguration Motion",
      "url": "https://arxiv.org/abs/2601.19529",
      "date": "2026-01-27",
      "authors_text": "Jie Gu, Yirui Sun, Zhihao Xia, Tin Lun Lam, Chunxu Tian",
      "is_highlight": false,
      "score": 91.0,
      "summary": "Rhombot is a novel rhombus-shaped modular robot enabling stable, medium-independent reconfiguration through morphpivoting, validated by experiments demonstrating its morphing and docking capabilities.",
      "teaser_image": "https://arxiv.org/html/2601.19529/x2.png",
      "debug_abstract": "In this paper, we present Rhombot, a novel deformable planar lattice modular self-reconfigurable robot (MSRR) with a rhombus shaped module. Each module consists of a parallelogram skeleton with a single centrally mounted actuator that enables folding and unfolding along its diagonal. The core design philosophy is to achieve essential MSRR functionalities such as morphing, docking, and locomotion with minimal control complexity. This enables a continuous and stable reconfiguration process that is independent of the surrounding medium, allowing the system to reliably form various configurations in diverse environments. To leverage the unique kinematics of Rhombot, we introduce morphpivoting, a novel motion primitive for reconfiguration that differs from advanced MSRR systems, and propose a strategy for its continuous execution. Finally, a series of physical experiments validate the module&#39;s stable reconfiguration ability, as well as its positional and docking accuracy."
    },
    {
      "title": "Fast and Safe Trajectory Optimization for Mobile Manipulators With Neural Configuration Space Distance Field",
      "url": "https://arxiv.org/abs/2601.18548",
      "date": "2026-01-27",
      "authors_text": "Yulin Li, Zhiyuan Song, Yiming Li, Zhicheng Song, Kai Chen",
      "is_highlight": false,
      "score": 88.0,
      "summary": "The paper presents Generalized Configuration Space Distance Fields (GCDF) for efficient trajectory optimization in mobile manipulators, enhancing collision reasoning and enabling rapid replanning in complex environments.",
      "teaser_image": "https://arxiv.org/html/2601.18548/x2.png",
      "debug_abstract": "Mobile manipulators promise agile, long-horizon behavior by coordinating base and arm motion, yet whole-body trajectory optimization in cluttered, confined spaces remains difficult due to high-dimensional nonconvexity and the need for fast, accurate collision reasoning. Configuration Space Distance Fields (CDF) enable fixed-base manipulators to model collisions directly in configuration space via smooth, implicit distances. This representation holds strong potential to bypass the nonlinear configuration-to-workspace mapping while preserving accurate whole-body geometry and providing optimization-friendly collision costs. Yet, extending this capability to mobile manipulators is hindered by unbounded workspaces and tighter base-arm coupling. We lift this promise to mobile manipulation with Generalized Configuration Space Distance Fields (GCDF), extending CDF to robots with both translational and rotational joints in unbounded workspaces with tighter base-arm coupling. We prove that GCDF preserves Euclidean-like local distance structure and accurately encodes whole-body geometry in configuration space, and develop a data generation and training pipeline that yields continuous neural GCDFs with accurate values and gradients, supporting efficient GPU-batched queries. Building on this representation, we develop a high-performance sequential convex optimization framework centered on GCDF-based collision reasoning. The solver scales to large numbers of implicit constraints through (i) online specification of neural constraints, (ii) sparsity-aware active-set detection with parallel batched evaluation across thousands of constraints, and (iii) incremental constraint management for rapid replanning under scene changes."
    }
  ],
  "机器人研究所 (CKSRI)": [
    {
      "title": "Geometry-Grounded Gaussian Splatting",
      "url": "https://arxiv.org/abs/2601.17835",
      "date": "2026-01-27",
      "authors_text": "Baowen Zhang, Chenxing Jiang, Heng Li, Shaojie Shen, Ping Tan",
      "is_highlight": true,
      "score": 73.0,
      "summary": "This paper introduces Geometry-Grounded Gaussian Splatting, enhancing shape extraction from Gaussian primitives through a theoretical framework that improves multi-view consistency and depth map quality.",
      "teaser_image": "https://arxiv.org/html/2601.17835/x2.png",
      "debug_abstract": "Gaussian Splatting (GS) has demonstrated impressive quality and efficiency in novel view synthesis. However, shape extraction from Gaussian primitives remains an open problem. Due to inadequate geometry parameterization and approximation, existing shape reconstruction methods suffer from poor multi-view consistency and are sensitive to floaters. In this paper, we present a rigorous theoretical derivation that establishes Gaussian primitives as a specific type of stochastic solids. This theoretical framework provides a principled foundation for Geometry-Grounded Gaussian Splatting by enabling the direct treatment of Gaussian primitives as explicit geometric representations. Using the volumetric nature of stochastic solids, our method efficiently renders high-quality depth maps for fine-grained geometry extraction. Experiments show that our method achieves the best shape reconstruction results among all Gaussian Splatting-based methods on public datasets."
    },
    {
      "title": "Enhancing Inverse Perspective Mapping for Automatic Vectorized Road Map Generation",
      "url": "https://arxiv.org/abs/2601.19536",
      "date": "2026-01-27",
      "authors_text": "Hongji Liu, Linwei Zheng, Yongjian Li, Mingkai Tang, Xiaoyang Yan",
      "is_highlight": false,
      "score": 66.0,
      "summary": "This study introduces a cost-effective framework using enhanced inverse perspective mapping to generate high-precision vectorized road maps with improved accuracy and reduced mapping errors.",
      "teaser_image": "https://arxiv.org/html/2601.19536/x2.png",
      "debug_abstract": "In this study, we present a low-cost and unified framework for vectorized road mapping leveraging enhanced inverse perspective mapping (IPM). In this framework, Catmull-Rom splines are utilized to characterize lane lines, and all the other ground markings are depicted using polygons uniformly. The results from instance segmentation serve as references to refine the three-dimensional position of spline control points and polygon corner points. In conjunction with this process, the homography matrix of IPM and vehicle poses are optimized simultaneously. Our proposed framework significantly reduces the mapping errors associated with IPM. It also improves the accuracy of the initial IPM homography matrix and the predicted vehicle poses. Furthermore, it addresses the limitations imposed by the coplanarity assumption in IPM. These enhancements enable IPM to be effectively applied to vectorized road mapping, which serves a cost-effective solution with enhanced accuracy. In addition, our framework generalizes road map elements to include all common ground markings and lane lines. The proposed framework is evaluated in two different practical scenarios, and the test results show that our method can automatically generate high-precision maps with near-centimeter-level accuracy. Importantly, the optimized IPM matrix achieves an accuracy comparable to that of manual calibration, while the accuracy of vehicle poses is also significantly improved."
    },
    {
      "title": "Glance and Focus Reinforcement for Pan-cancer Screening",
      "url": "https://arxiv.org/abs/2601.19103",
      "date": "2026-01-27",
      "authors_text": "Linshan Wu, Jiaxin Zhuang, Hao Chen",
      "is_highlight": false,
      "score": 60.0,
      "summary": "GF-Screen introduces a novel reinforcement learning framework that enhances pan-cancer screening by effectively localizing and segmenting lesions in CT scans, outperforming existing methods.",
      "teaser_image": "https://arxiv.org/html/2601.19103/x3.png",
      "debug_abstract": "Pan-cancer screening in large-scale CT scans remains challenging for existing AI methods, primarily due to the difficulty of localizing diverse types of tiny lesions in large CT volumes. The extreme foreground-background imbalance significantly hinders models from focusing on diseased regions, while redundant focus on healthy regions not only decreases the efficiency but also increases false positives. Inspired by radiologists&#39; glance and focus diagnostic strategy, we introduce GF-Screen, a Glance and Focus reinforcement learning framework for pan-cancer screening. GF-Screen employs a Glance model to localize the diseased regions and a Focus model to precisely segment the lesions, where segmentation results of the Focus model are leveraged to reward the Glance model via Reinforcement Learning (RL). Specifically, the Glance model crops a group of sub-volumes from the entire CT volume and learns to select the sub-volumes with lesions for the Focus model to segment. Given that the selecting operation is non-differentiable for segmentation training, we propose to employ the segmentation results to reward the Glance model. To optimize the Glance model, we introduce a novel group relative learning paradigm, which employs group relative comparison to prioritize high-advantage predictions and discard low-advantage predictions within sub-volume groups, not only improving efficiency but also reducing false positives. In this way, for the first time, we effectively extend cutting-edge RL techniques to tackle the specific challenges in pan-cancer screening. Extensive experiments on 16 internal and 7 external datasets across 9 lesion types demonstrated the effectiveness of GF-Screen. Notably, GF-Screen leads the public validation leaderboard of MICCAI FLARE25 pan-cancer challenge, surpassing the FLARE24 champion solution by a large margin (+25.6% DSC and +28.2% NSD)."
    },
    {
      "title": "Fast and Safe Trajectory Optimization for Mobile Manipulators With Neural Configuration Space Distance Field",
      "url": "https://arxiv.org/abs/2601.18548",
      "date": "2026-01-27",
      "authors_text": "Yulin Li, Zhiyuan Song, Yiming Li, Zhicheng Song, Kai Chen",
      "is_highlight": false,
      "score": 88.0,
      "summary": "The paper presents Generalized Configuration Space Distance Fields (GCDF) for efficient trajectory optimization in mobile manipulators, enhancing collision reasoning and enabling rapid replanning in complex environments.",
      "teaser_image": "https://arxiv.org/html/2601.18548/x2.png",
      "debug_abstract": "Mobile manipulators promise agile, long-horizon behavior by coordinating base and arm motion, yet whole-body trajectory optimization in cluttered, confined spaces remains difficult due to high-dimensional nonconvexity and the need for fast, accurate collision reasoning. Configuration Space Distance Fields (CDF) enable fixed-base manipulators to model collisions directly in configuration space via smooth, implicit distances. This representation holds strong potential to bypass the nonlinear configuration-to-workspace mapping while preserving accurate whole-body geometry and providing optimization-friendly collision costs. Yet, extending this capability to mobile manipulators is hindered by unbounded workspaces and tighter base-arm coupling. We lift this promise to mobile manipulation with Generalized Configuration Space Distance Fields (GCDF), extending CDF to robots with both translational and rotational joints in unbounded workspaces with tighter base-arm coupling. We prove that GCDF preserves Euclidean-like local distance structure and accurately encodes whole-body geometry in configuration space, and develop a data generation and training pipeline that yields continuous neural GCDFs with accurate values and gradients, supporting efficient GPU-batched queries. Building on this representation, we develop a high-performance sequential convex optimization framework centered on GCDF-based collision reasoning. The solver scales to large numbers of implicit constraints through (i) online specification of neural constraints, (ii) sparsity-aware active-set detection with parallel batched evaluation across thousands of constraints, and (iii) incremental constraint management for rapid replanning under scene changes."
    }
  ],
  "范明明老师实验室": [
    {
      "title": "Geometry-Grounded Gaussian Splatting",
      "url": "https://arxiv.org/abs/2601.17835",
      "date": "2026-01-27",
      "authors_text": "Baowen Zhang, Chenxing Jiang, Heng Li, Shaojie Shen, Ping Tan",
      "is_highlight": true,
      "score": 73.0,
      "summary": "This paper introduces Geometry-Grounded Gaussian Splatting, enhancing shape extraction from Gaussian primitives through a theoretical framework that improves multi-view consistency and depth map quality.",
      "teaser_image": "https://arxiv.org/html/2601.17835/x2.png",
      "debug_abstract": "Gaussian Splatting (GS) has demonstrated impressive quality and efficiency in novel view synthesis. However, shape extraction from Gaussian primitives remains an open problem. Due to inadequate geometry parameterization and approximation, existing shape reconstruction methods suffer from poor multi-view consistency and are sensitive to floaters. In this paper, we present a rigorous theoretical derivation that establishes Gaussian primitives as a specific type of stochastic solids. This theoretical framework provides a principled foundation for Geometry-Grounded Gaussian Splatting by enabling the direct treatment of Gaussian primitives as explicit geometric representations. Using the volumetric nature of stochastic solids, our method efficiently renders high-quality depth maps for fine-grained geometry extraction. Experiments show that our method achieves the best shape reconstruction results among all Gaussian Splatting-based methods on public datasets."
    },
    {
      "title": "Enhancing Inverse Perspective Mapping for Automatic Vectorized Road Map Generation",
      "url": "https://arxiv.org/abs/2601.19536",
      "date": "2026-01-27",
      "authors_text": "Hongji Liu, Linwei Zheng, Yongjian Li, Mingkai Tang, Xiaoyang Yan",
      "is_highlight": false,
      "score": 66.0,
      "summary": "This study introduces a cost-effective framework using enhanced inverse perspective mapping to generate high-precision vectorized road maps with improved accuracy and reduced mapping errors.",
      "teaser_image": "https://arxiv.org/html/2601.19536/x2.png",
      "debug_abstract": "In this study, we present a low-cost and unified framework for vectorized road mapping leveraging enhanced inverse perspective mapping (IPM). In this framework, Catmull-Rom splines are utilized to characterize lane lines, and all the other ground markings are depicted using polygons uniformly. The results from instance segmentation serve as references to refine the three-dimensional position of spline control points and polygon corner points. In conjunction with this process, the homography matrix of IPM and vehicle poses are optimized simultaneously. Our proposed framework significantly reduces the mapping errors associated with IPM. It also improves the accuracy of the initial IPM homography matrix and the predicted vehicle poses. Furthermore, it addresses the limitations imposed by the coplanarity assumption in IPM. These enhancements enable IPM to be effectively applied to vectorized road mapping, which serves a cost-effective solution with enhanced accuracy. In addition, our framework generalizes road map elements to include all common ground markings and lane lines. The proposed framework is evaluated in two different practical scenarios, and the test results show that our method can automatically generate high-precision maps with near-centimeter-level accuracy. Importantly, the optimized IPM matrix achieves an accuracy comparable to that of manual calibration, while the accuracy of vehicle poses is also significantly improved."
    },
    {
      "title": "Glance and Focus Reinforcement for Pan-cancer Screening",
      "url": "https://arxiv.org/abs/2601.19103",
      "date": "2026-01-27",
      "authors_text": "Linshan Wu, Jiaxin Zhuang, Hao Chen",
      "is_highlight": false,
      "score": 60.0,
      "summary": "GF-Screen introduces a novel reinforcement learning framework that enhances pan-cancer screening by effectively localizing and segmenting lesions in CT scans, outperforming existing methods.",
      "teaser_image": "https://arxiv.org/html/2601.19103/x3.png",
      "debug_abstract": "Pan-cancer screening in large-scale CT scans remains challenging for existing AI methods, primarily due to the difficulty of localizing diverse types of tiny lesions in large CT volumes. The extreme foreground-background imbalance significantly hinders models from focusing on diseased regions, while redundant focus on healthy regions not only decreases the efficiency but also increases false positives. Inspired by radiologists&#39; glance and focus diagnostic strategy, we introduce GF-Screen, a Glance and Focus reinforcement learning framework for pan-cancer screening. GF-Screen employs a Glance model to localize the diseased regions and a Focus model to precisely segment the lesions, where segmentation results of the Focus model are leveraged to reward the Glance model via Reinforcement Learning (RL). Specifically, the Glance model crops a group of sub-volumes from the entire CT volume and learns to select the sub-volumes with lesions for the Focus model to segment. Given that the selecting operation is non-differentiable for segmentation training, we propose to employ the segmentation results to reward the Glance model. To optimize the Glance model, we introduce a novel group relative learning paradigm, which employs group relative comparison to prioritize high-advantage predictions and discard low-advantage predictions within sub-volume groups, not only improving efficiency but also reducing false positives. In this way, for the first time, we effectively extend cutting-edge RL techniques to tackle the specific challenges in pan-cancer screening. Extensive experiments on 16 internal and 7 external datasets across 9 lesion types demonstrated the effectiveness of GF-Screen. Notably, GF-Screen leads the public validation leaderboard of MICCAI FLARE25 pan-cancer challenge, surpassing the FLARE24 champion solution by a large margin (+25.6% DSC and +28.2% NSD)."
    },
    {
      "title": "Fast and Safe Trajectory Optimization for Mobile Manipulators With Neural Configuration Space Distance Field",
      "url": "https://arxiv.org/abs/2601.18548",
      "date": "2026-01-27",
      "authors_text": "Yulin Li, Zhiyuan Song, Yiming Li, Zhicheng Song, Kai Chen",
      "is_highlight": false,
      "score": 88.0,
      "summary": "The paper presents Generalized Configuration Space Distance Fields (GCDF) for efficient trajectory optimization in mobile manipulators, enhancing collision reasoning and enabling rapid replanning in complex environments.",
      "teaser_image": "https://arxiv.org/html/2601.18548/x2.png",
      "debug_abstract": "Mobile manipulators promise agile, long-horizon behavior by coordinating base and arm motion, yet whole-body trajectory optimization in cluttered, confined spaces remains difficult due to high-dimensional nonconvexity and the need for fast, accurate collision reasoning. Configuration Space Distance Fields (CDF) enable fixed-base manipulators to model collisions directly in configuration space via smooth, implicit distances. This representation holds strong potential to bypass the nonlinear configuration-to-workspace mapping while preserving accurate whole-body geometry and providing optimization-friendly collision costs. Yet, extending this capability to mobile manipulators is hindered by unbounded workspaces and tighter base-arm coupling. We lift this promise to mobile manipulation with Generalized Configuration Space Distance Fields (GCDF), extending CDF to robots with both translational and rotational joints in unbounded workspaces with tighter base-arm coupling. We prove that GCDF preserves Euclidean-like local distance structure and accurately encodes whole-body geometry in configuration space, and develop a data generation and training pipeline that yields continuous neural GCDFs with accurate values and gradients, supporting efficient GPU-batched queries. Building on this representation, we develop a high-performance sequential convex optimization framework centered on GCDF-based collision reasoning. The solver scales to large numbers of implicit constraints through (i) online specification of neural constraints, (ii) sparsity-aware active-set detection with parallel batched evaluation across thousands of constraints, and (iii) incremental constraint management for rapid replanning under scene changes."
    }
  ],
  "潘佳老师实验室": [
    {
      "title": "EgoHandICL: Egocentric 3D Hand Reconstruction with In-Context Learning",
      "url": "https://arxiv.org/abs/2601.19850",
      "date": "2026-01-27",
      "authors_text": "Binzhu Xie, Shi Qiu, Sicheng Zhang, Yinqiao Wang, Hao Xu",
      "is_highlight": false,
      "score": 76.0,
      "summary": "EgoHandICL introduces an innovative in-context learning framework for robust 3D hand reconstruction in egocentric vision, enhancing performance through multimodal context and exemplar retrieval.",
      "teaser_image": "https://arxiv.org/html/2601.19850/x2.png",
      "debug_abstract": "Robust 3D hand reconstruction in egocentric vision is challenging due to depth ambiguity, self-occlusion, and complex hand-object interactions. Prior methods mitigate these issues by scaling training data or adding auxiliary cues, but they often struggle in unseen contexts. We present EgoHandICL, the first in-context learning (ICL) framework for 3D hand reconstruction that improves semantic alignment, visual consistency, and robustness under challenging egocentric conditions. EgoHandICL introduces complementary exemplar retrieval guided by vision-language models (VLMs), an ICL-tailored tokenizer for multimodal context, and a masked autoencoder (MAE)-based architecture trained with hand-guided geometric and perceptual objectives. Experiments on ARCTIC and EgoExo4D show consistent gains over state-of-the-art methods. We also demonstrate real-world generalization and improve EgoVLM hand-object interaction reasoning by using reconstructed hands as visual prompts. Code and data: this https URL"
    },
    {
      "title": "Component-Level Lesioning of Language Models Reveals Clinically Aligned Aphasia Phenotypes",
      "url": "https://arxiv.org/abs/2601.19723",
      "date": "2026-01-27",
      "authors_text": "Yifan Wang, Jichen Zheng, Jingyuan Sun, Yunhao Zhang, Chunyu Ye",
      "is_highlight": false,
      "score": 61.0,
      "summary": "This paper presents a framework for simulating aphasia in language models by selectively perturbing components, enabling insights into language production impairments and rehabilitation strategies.",
      "teaser_image": "https://arxiv.org/html/2601.19723/x1.png",
      "debug_abstract": "Large language models (LLMs) increasingly exhibit human-like linguistic behaviors and internal representations that they could serve as computational simulators of language cognition. We ask whether LLMs can be systematically manipulated to reproduce language-production impairments characteristic of aphasia following focal brain lesions. Such models could provide scalable proxies for testing rehabilitation hypotheses, and offer a controlled framework for probing the functional organization of language. We introduce a clinically grounded, component-level framework that simulates aphasia by selectively perturbing functional components in LLMs, and apply it to both modular Mixture-of-Experts models and dense Transformers using a unified intervention interface. Our pipeline (i) identifies subtype-linked components for Broca&#39;s and Wernicke&#39;s aphasia, (ii) interprets these components with linguistic probing tasks, and (iii) induces graded impairments by progressively perturbing the top-k subtype-linked components, evaluating outcomes with Western Aphasia Battery (WAB) subtests summarized by Aphasia Quotient (AQ). Across architectures and lesioning strategies, subtype-targeted perturbations yield more systematic, aphasia-like regressions than size-matched random perturbations, and MoE modularity supports more localized and interpretable phenotype-to-component mappings. These findings suggest that modular LLMs, combined with clinically informed component perturbations, provide a promising platform for simulating aphasic language production and studying how distinct language functions degrade under targeted disruptions."
    },
    {
      "title": "Rhombot: Rhombus-shaped Modular Robots for Stable, Medium-Independent Reconfiguration Motion",
      "url": "https://arxiv.org/abs/2601.19529",
      "date": "2026-01-27",
      "authors_text": "Jie Gu, Yirui Sun, Zhihao Xia, Tin Lun Lam, Chunxu Tian",
      "is_highlight": false,
      "score": 91.0,
      "summary": "Rhombot is a novel rhombus-shaped modular robot enabling stable, medium-independent reconfiguration through morphpivoting, validated by experiments demonstrating its morphing and docking capabilities.",
      "teaser_image": "https://arxiv.org/html/2601.19529/x2.png",
      "debug_abstract": "In this paper, we present Rhombot, a novel deformable planar lattice modular self-reconfigurable robot (MSRR) with a rhombus shaped module. Each module consists of a parallelogram skeleton with a single centrally mounted actuator that enables folding and unfolding along its diagonal. The core design philosophy is to achieve essential MSRR functionalities such as morphing, docking, and locomotion with minimal control complexity. This enables a continuous and stable reconfiguration process that is independent of the surrounding medium, allowing the system to reliably form various configurations in diverse environments. To leverage the unique kinematics of Rhombot, we introduce morphpivoting, a novel motion primitive for reconfiguration that differs from advanced MSRR systems, and propose a strategy for its continuous execution. Finally, a series of physical experiments validate the module&#39;s stable reconfiguration ability, as well as its positional and docking accuracy."
    },
    {
      "title": "Fast and Safe Trajectory Optimization for Mobile Manipulators With Neural Configuration Space Distance Field",
      "url": "https://arxiv.org/abs/2601.18548",
      "date": "2026-01-27",
      "authors_text": "Yulin Li, Zhiyuan Song, Yiming Li, Zhicheng Song, Kai Chen",
      "is_highlight": false,
      "score": 88.0,
      "summary": "The paper presents Generalized Configuration Space Distance Fields (GCDF) for efficient trajectory optimization in mobile manipulators, enhancing collision reasoning and enabling rapid replanning in complex environments.",
      "teaser_image": "https://arxiv.org/html/2601.18548/x2.png",
      "debug_abstract": "Mobile manipulators promise agile, long-horizon behavior by coordinating base and arm motion, yet whole-body trajectory optimization in cluttered, confined spaces remains difficult due to high-dimensional nonconvexity and the need for fast, accurate collision reasoning. Configuration Space Distance Fields (CDF) enable fixed-base manipulators to model collisions directly in configuration space via smooth, implicit distances. This representation holds strong potential to bypass the nonlinear configuration-to-workspace mapping while preserving accurate whole-body geometry and providing optimization-friendly collision costs. Yet, extending this capability to mobile manipulators is hindered by unbounded workspaces and tighter base-arm coupling. We lift this promise to mobile manipulation with Generalized Configuration Space Distance Fields (GCDF), extending CDF to robots with both translational and rotational joints in unbounded workspaces with tighter base-arm coupling. We prove that GCDF preserves Euclidean-like local distance structure and accurately encodes whole-body geometry in configuration space, and develop a data generation and training pipeline that yields continuous neural GCDFs with accurate values and gradients, supporting efficient GPU-batched queries. Building on this representation, we develop a high-performance sequential convex optimization framework centered on GCDF-based collision reasoning. The solver scales to large numbers of implicit constraints through (i) online specification of neural constraints, (ii) sparsity-aware active-set detection with parallel batched evaluation across thousands of constraints, and (iii) incremental constraint management for rapid replanning under scene changes."
    }
  ],
  "Jun MA老师实验室": [
    {
      "title": "Geometry-Grounded Gaussian Splatting",
      "url": "https://arxiv.org/abs/2601.17835",
      "date": "2026-01-27",
      "authors_text": "Baowen Zhang, Chenxing Jiang, Heng Li, Shaojie Shen, Ping Tan",
      "is_highlight": true,
      "score": 73.0,
      "summary": "This paper introduces Geometry-Grounded Gaussian Splatting, enhancing shape extraction from Gaussian primitives through a theoretical framework that improves multi-view consistency and depth map quality.",
      "teaser_image": "https://arxiv.org/html/2601.17835/x2.png",
      "debug_abstract": "Gaussian Splatting (GS) has demonstrated impressive quality and efficiency in novel view synthesis. However, shape extraction from Gaussian primitives remains an open problem. Due to inadequate geometry parameterization and approximation, existing shape reconstruction methods suffer from poor multi-view consistency and are sensitive to floaters. In this paper, we present a rigorous theoretical derivation that establishes Gaussian primitives as a specific type of stochastic solids. This theoretical framework provides a principled foundation for Geometry-Grounded Gaussian Splatting by enabling the direct treatment of Gaussian primitives as explicit geometric representations. Using the volumetric nature of stochastic solids, our method efficiently renders high-quality depth maps for fine-grained geometry extraction. Experiments show that our method achieves the best shape reconstruction results among all Gaussian Splatting-based methods on public datasets."
    },
    {
      "title": "Enhancing Inverse Perspective Mapping for Automatic Vectorized Road Map Generation",
      "url": "https://arxiv.org/abs/2601.19536",
      "date": "2026-01-27",
      "authors_text": "Hongji Liu, Linwei Zheng, Yongjian Li, Mingkai Tang, Xiaoyang Yan",
      "is_highlight": false,
      "score": 66.0,
      "summary": "This study introduces a cost-effective framework using enhanced inverse perspective mapping to generate high-precision vectorized road maps with improved accuracy and reduced mapping errors.",
      "teaser_image": "https://arxiv.org/html/2601.19536/x2.png",
      "debug_abstract": "In this study, we present a low-cost and unified framework for vectorized road mapping leveraging enhanced inverse perspective mapping (IPM). In this framework, Catmull-Rom splines are utilized to characterize lane lines, and all the other ground markings are depicted using polygons uniformly. The results from instance segmentation serve as references to refine the three-dimensional position of spline control points and polygon corner points. In conjunction with this process, the homography matrix of IPM and vehicle poses are optimized simultaneously. Our proposed framework significantly reduces the mapping errors associated with IPM. It also improves the accuracy of the initial IPM homography matrix and the predicted vehicle poses. Furthermore, it addresses the limitations imposed by the coplanarity assumption in IPM. These enhancements enable IPM to be effectively applied to vectorized road mapping, which serves a cost-effective solution with enhanced accuracy. In addition, our framework generalizes road map elements to include all common ground markings and lane lines. The proposed framework is evaluated in two different practical scenarios, and the test results show that our method can automatically generate high-precision maps with near-centimeter-level accuracy. Importantly, the optimized IPM matrix achieves an accuracy comparable to that of manual calibration, while the accuracy of vehicle poses is also significantly improved."
    },
    {
      "title": "Glance and Focus Reinforcement for Pan-cancer Screening",
      "url": "https://arxiv.org/abs/2601.19103",
      "date": "2026-01-27",
      "authors_text": "Linshan Wu, Jiaxin Zhuang, Hao Chen",
      "is_highlight": false,
      "score": 60.0,
      "summary": "GF-Screen introduces a novel reinforcement learning framework that enhances pan-cancer screening by effectively localizing and segmenting lesions in CT scans, outperforming existing methods.",
      "teaser_image": "https://arxiv.org/html/2601.19103/x3.png",
      "debug_abstract": "Pan-cancer screening in large-scale CT scans remains challenging for existing AI methods, primarily due to the difficulty of localizing diverse types of tiny lesions in large CT volumes. The extreme foreground-background imbalance significantly hinders models from focusing on diseased regions, while redundant focus on healthy regions not only decreases the efficiency but also increases false positives. Inspired by radiologists&#39; glance and focus diagnostic strategy, we introduce GF-Screen, a Glance and Focus reinforcement learning framework for pan-cancer screening. GF-Screen employs a Glance model to localize the diseased regions and a Focus model to precisely segment the lesions, where segmentation results of the Focus model are leveraged to reward the Glance model via Reinforcement Learning (RL). Specifically, the Glance model crops a group of sub-volumes from the entire CT volume and learns to select the sub-volumes with lesions for the Focus model to segment. Given that the selecting operation is non-differentiable for segmentation training, we propose to employ the segmentation results to reward the Glance model. To optimize the Glance model, we introduce a novel group relative learning paradigm, which employs group relative comparison to prioritize high-advantage predictions and discard low-advantage predictions within sub-volume groups, not only improving efficiency but also reducing false positives. In this way, for the first time, we effectively extend cutting-edge RL techniques to tackle the specific challenges in pan-cancer screening. Extensive experiments on 16 internal and 7 external datasets across 9 lesion types demonstrated the effectiveness of GF-Screen. Notably, GF-Screen leads the public validation leaderboard of MICCAI FLARE25 pan-cancer challenge, surpassing the FLARE24 champion solution by a large margin (+25.6% DSC and +28.2% NSD)."
    },
    {
      "title": "Fast and Safe Trajectory Optimization for Mobile Manipulators With Neural Configuration Space Distance Field",
      "url": "https://arxiv.org/abs/2601.18548",
      "date": "2026-01-27",
      "authors_text": "Yulin Li, Zhiyuan Song, Yiming Li, Zhicheng Song, Kai Chen",
      "is_highlight": false,
      "score": 88.0,
      "summary": "The paper presents Generalized Configuration Space Distance Fields (GCDF) for efficient trajectory optimization in mobile manipulators, enhancing collision reasoning and enabling rapid replanning in complex environments.",
      "teaser_image": "https://arxiv.org/html/2601.18548/x2.png",
      "debug_abstract": "Mobile manipulators promise agile, long-horizon behavior by coordinating base and arm motion, yet whole-body trajectory optimization in cluttered, confined spaces remains difficult due to high-dimensional nonconvexity and the need for fast, accurate collision reasoning. Configuration Space Distance Fields (CDF) enable fixed-base manipulators to model collisions directly in configuration space via smooth, implicit distances. This representation holds strong potential to bypass the nonlinear configuration-to-workspace mapping while preserving accurate whole-body geometry and providing optimization-friendly collision costs. Yet, extending this capability to mobile manipulators is hindered by unbounded workspaces and tighter base-arm coupling. We lift this promise to mobile manipulation with Generalized Configuration Space Distance Fields (GCDF), extending CDF to robots with both translational and rotational joints in unbounded workspaces with tighter base-arm coupling. We prove that GCDF preserves Euclidean-like local distance structure and accurately encodes whole-body geometry in configuration space, and develop a data generation and training pipeline that yields continuous neural GCDFs with accurate values and gradients, supporting efficient GPU-batched queries. Building on this representation, we develop a high-performance sequential convex optimization framework centered on GCDF-based collision reasoning. The solver scales to large numbers of implicit constraints through (i) online specification of neural constraints, (ii) sparsity-aware active-set detection with parallel batched evaluation across thousands of constraints, and (iii) incremental constraint management for rapid replanning under scene changes."
    }
  ],
  "通用机器人、自动化、传感和感知实验室 (GRASP)": [
    {
      "title": "FreeOrbit4D: Training-Free Arbitrary Camera Redirection for Monocular Videos via Geometry-Complete 4D Reconstruction",
      "url": "https://arxiv.org/abs/2601.18993",
      "date": "2026-01-26",
      "authors_text": "Wei Cao, Hao Zhang, Fengrui Tian, Yulun Wu, Yingying Li",
      "is_highlight": false,
      "score": 70.0,
      "summary": "FreeOrbit4D introduces a training-free method for effective camera redirection in monocular videos by reconstructing a geometry-complete 4D proxy to resolve geometric ambiguities.",
      "teaser_image": "https://arxiv.org/html/2601.18993/x3.png",
      "debug_abstract": "Camera redirection aims to replay a dynamic scene from a single monocular video under a user-specified camera trajectory. However, large-angle redirection is inherently ill-posed: a monocular video captures only a narrow spatio-temporal view of a dynamic 3D scene, providing highly partial observations of the underlying 4D world. The key challenge is therefore to recover a complete and coherent representation from this limited input, with consistent geometry and motion. While recent diffusion-based methods achieve impressive results, they often break down under large-angle viewpoint changes far from the original trajectory, where missing visual grounding leads to severe geometric ambiguity and temporal inconsistency. To address this, we present FreeOrbit4D, an effective training-free framework that tackles this geometric ambiguity by recovering a geometry-complete 4D proxy as structural grounding for video generation. We obtain this proxy by decoupling foreground and background reconstructions: we unproject the monocular video into a static background and geometry-incomplete foreground point clouds in a unified global space, then leverage an object-centric multi-view diffusion model to synthesize multi-view images and reconstruct geometry-complete foreground point clouds in canonical object space. By aligning the canonical foreground point cloud to the global scene space via dense pixel-synchronized 3D--3D correspondences and projecting the geometry-complete 4D proxy onto target camera viewpoints, we provide geometric scaffolds that guide a conditional video diffusion model. Extensive experiments show that FreeOrbit4D produces more faithful redirected videos under challenging large-angle trajectories, and our geometry-complete 4D proxy further opens a potential avenue for practical applications such as edit propagation and 4D data generation. Project page and code will be released soon."
    }
  ],
  "南方科技大学": [
    {
      "title": "UniPCB: A Unified Vision-Language Benchmark for Open-Ended PCB Quality Inspection",
      "url": "https://arxiv.org/abs/2601.19222",
      "date": "2026-01-27",
      "authors_text": "Fuxiang Sun, Xi Jiang, Jiansheng Wu, Haigang Zhang, Feng Zheng",
      "is_highlight": false,
      "score": 82.0,
      "summary": "The paper introduces UniPCB, a unified vision-language benchmark for PCB quality inspection, and PCB-GPT, an MLLM that significantly improves defect localization performance.",
      "teaser_image": "https://arxiv.org/html/2601.19222/x2.png",
      "debug_abstract": "Multimodal Large Language Models (MLLMs) show promise for general industrial quality inspection, but fall short in complex scenarios, such as Printed Circuit Board (PCB) inspection. PCB inspection poses unique challenges due to densely packed components, complex wiring structures, and subtle defect patterns that require specialized domain expertise. However, a high-quality, unified vision-language benchmark for quantitatively evaluating MLLMs across PCB inspection tasks remains absent, stemming not only from limited data availability but also from fragmented datasets and inconsistent standardization. To fill this gap, we propose UniPCB, the first unified vision-language benchmark for open-ended PCB quality inspection. UniPCB is built via a systematic pipeline that curates and standardizes data from disparate sources across three annotated scenarios. Furthermore, we introduce PCB-GPT, an MLLM trained on a new instruction dataset generated by this pipeline, utilizing a novel progressive curriculum that mimics the learning process of human experts. Evaluations on the UniPCB benchmark show that while existing MLLMs falter on domain-specific tasks, PCB-GPT establishes a new baseline. Notably, it more than doubles the performance on fine-grained defect localization compared to the strongest competitors, with significant advantages in localization and analysis. We will release the instruction data, benchmark, and model to facilitate future research."
    }
  ],
  "具身智能多模态大模型中心": [
    {
      "title": "TIGaussian: Disentangle Gaussians for Spatial-Awared Text-Image-3D Alignment",
      "url": "https://arxiv.org/abs/2601.19247",
      "date": "2026-01-27",
      "authors_text": "Jiarun Liu, Qifeng Chen, Yiru Zhao, Minghua Liu, Baorui Ma",
      "is_highlight": false,
      "score": 75.0,
      "summary": "TIGaussian enhances cross-modal alignment by utilizing 3D Gaussian Splatting for improved feature extraction and integration across text, image, and 3D data modalities.",
      "teaser_image": "https://arxiv.org/html/2601.19247/x1.png",
      "debug_abstract": "While visual-language models have profoundly linked features between texts and images, the incorporation of 3D modality data, such as point clouds and 3D Gaussians, further enables pretraining for 3D-related tasks, e.g., cross-modal retrieval, zero-shot classification, and scene recognition. As challenges remain in extracting 3D modal features and bridging the gap between different modalities, we propose TIGaussian, a framework that harnesses 3D Gaussian Splatting (3DGS) characteristics to strengthen cross-modality alignment through multi-branch 3DGS tokenizer and modality-specific 3D feature alignment strategies. Specifically, our multi-branch 3DGS tokenizer decouples the intrinsic properties of 3DGS structures into compact latent representations, enabling more generalizable feature extraction. To further bridge the modality gap, we develop a bidirectional cross-modal alignment strategies: a multi-view feature fusion mechanism that leverages diffusion priors to resolve perspective ambiguity in image-3D alignment, while a text-3D projection module adaptively maps 3D features to text embedding space for better text-3D alignment. Extensive experiments on various datasets demonstrate the state-of-the-art performance of TIGaussian in multiple tasks."
    }
  ],
  "具身智能研究中心": [
    {
      "title": "TIGaussian: Disentangle Gaussians for Spatial-Awared Text-Image-3D Alignment",
      "url": "https://arxiv.org/abs/2601.19247",
      "date": "2026-01-27",
      "authors_text": "Jiarun Liu, Qifeng Chen, Yiru Zhao, Minghua Liu, Baorui Ma",
      "is_highlight": false,
      "score": 75.0,
      "summary": "TIGaussian enhances cross-modal alignment by utilizing 3D Gaussian Splatting for improved feature extraction and integration across text, image, and 3D data modalities.",
      "teaser_image": "https://arxiv.org/html/2601.19247/x1.png",
      "debug_abstract": "While visual-language models have profoundly linked features between texts and images, the incorporation of 3D modality data, such as point clouds and 3D Gaussians, further enables pretraining for 3D-related tasks, e.g., cross-modal retrieval, zero-shot classification, and scene recognition. As challenges remain in extracting 3D modal features and bridging the gap between different modalities, we propose TIGaussian, a framework that harnesses 3D Gaussian Splatting (3DGS) characteristics to strengthen cross-modality alignment through multi-branch 3DGS tokenizer and modality-specific 3D feature alignment strategies. Specifically, our multi-branch 3DGS tokenizer decouples the intrinsic properties of 3DGS structures into compact latent representations, enabling more generalizable feature extraction. To further bridge the modality gap, we develop a bidirectional cross-modal alignment strategies: a multi-view feature fusion mechanism that leverages diffusion priors to resolve perspective ambiguity in image-3D alignment, while a text-3D projection module adaptively maps 3D features to text embedding space for better text-3D alignment. Extensive experiments on various datasets demonstrate the state-of-the-art performance of TIGaussian in multiple tasks."
    }
  ],
  "智能机器人与机器视觉(IRMV)实验室": [
    {
      "title": "Innovator-VL: A Multimodal Large Language Model for Scientific Discovery",
      "url": "https://arxiv.org/abs/2601.19325",
      "date": "2026-01-27",
      "authors_text": "Zichen Wen, Boxue Yang, Shuang Chen, Yaojie Zhang, Yuhang Han",
      "is_highlight": false,
      "score": 78.0,
      "summary": "Innovator-VL is a multimodal large language model that enhances scientific discovery through efficient training, transparency, and strong generalization without relying on extensive pretraining.",
      "teaser_image": "https://arxiv.org/html/2601.19325/x8.png",
      "debug_abstract": "We present Innovator-VL, a scientific multimodal large language model designed to advance understanding and reasoning across diverse scientific domains while maintaining excellent performance on general vision tasks. Contrary to the trend of relying on massive domain-specific pretraining and opaque pipelines, our work demonstrates that principled training design and transparent methodology can yield strong scientific intelligence with substantially reduced data requirements. (i) First, we provide a fully transparent, end-to-end reproducible training pipeline, covering data collection, cleaning, preprocessing, supervised fine-tuning, reinforcement learning, and evaluation, along with detailed optimization recipes. This facilitates systematic extension by the community. (ii) Second, Innovator-VL exhibits remarkable data efficiency, achieving competitive performance on various scientific tasks using fewer than five million curated samples without large-scale pretraining. These results highlight that effective reasoning can be achieved through principled data selection rather than indiscriminate scaling. (iii) Third, Innovator-VL demonstrates strong generalization, achieving competitive performance on general vision, multimodal reasoning, and scientific benchmarks. This indicates that scientific alignment can be integrated into a unified model without compromising general-purpose capabilities. Our practices suggest that efficient, reproducible, and high-performing scientific multimodal models can be built even without large-scale data, providing a practical foundation for future research."
    }
  ],
  "ReThinkLab": [
    {
      "title": "Innovator-VL: A Multimodal Large Language Model for Scientific Discovery",
      "url": "https://arxiv.org/abs/2601.19325",
      "date": "2026-01-27",
      "authors_text": "Zichen Wen, Boxue Yang, Shuang Chen, Yaojie Zhang, Yuhang Han",
      "is_highlight": false,
      "score": 78.0,
      "summary": "Innovator-VL is a multimodal large language model that enhances scientific discovery through efficient training, transparency, and strong generalization without relying on extensive pretraining.",
      "teaser_image": "https://arxiv.org/html/2601.19325/x8.png",
      "debug_abstract": "We present Innovator-VL, a scientific multimodal large language model designed to advance understanding and reasoning across diverse scientific domains while maintaining excellent performance on general vision tasks. Contrary to the trend of relying on massive domain-specific pretraining and opaque pipelines, our work demonstrates that principled training design and transparent methodology can yield strong scientific intelligence with substantially reduced data requirements. (i) First, we provide a fully transparent, end-to-end reproducible training pipeline, covering data collection, cleaning, preprocessing, supervised fine-tuning, reinforcement learning, and evaluation, along with detailed optimization recipes. This facilitates systematic extension by the community. (ii) Second, Innovator-VL exhibits remarkable data efficiency, achieving competitive performance on various scientific tasks using fewer than five million curated samples without large-scale pretraining. These results highlight that effective reasoning can be achieved through principled data selection rather than indiscriminate scaling. (iii) Third, Innovator-VL demonstrates strong generalization, achieving competitive performance on general vision, multimodal reasoning, and scientific benchmarks. This indicates that scientific alignment can be integrated into a unified model without compromising general-purpose capabilities. Our practices suggest that efficient, reproducible, and high-performing scientific multimodal models can be built even without large-scale data, providing a practical foundation for future research."
    }
  ],
  "数字媒体与计算机视觉实验室": [
    {
      "title": "Innovator-VL: A Multimodal Large Language Model for Scientific Discovery",
      "url": "https://arxiv.org/abs/2601.19325",
      "date": "2026-01-27",
      "authors_text": "Zichen Wen, Boxue Yang, Shuang Chen, Yaojie Zhang, Yuhang Han",
      "is_highlight": false,
      "score": 78.0,
      "summary": "Innovator-VL is a multimodal large language model that enhances scientific discovery through efficient training, transparency, and strong generalization without relying on extensive pretraining.",
      "teaser_image": "https://arxiv.org/html/2601.19325/x8.png",
      "debug_abstract": "We present Innovator-VL, a scientific multimodal large language model designed to advance understanding and reasoning across diverse scientific domains while maintaining excellent performance on general vision tasks. Contrary to the trend of relying on massive domain-specific pretraining and opaque pipelines, our work demonstrates that principled training design and transparent methodology can yield strong scientific intelligence with substantially reduced data requirements. (i) First, we provide a fully transparent, end-to-end reproducible training pipeline, covering data collection, cleaning, preprocessing, supervised fine-tuning, reinforcement learning, and evaluation, along with detailed optimization recipes. This facilitates systematic extension by the community. (ii) Second, Innovator-VL exhibits remarkable data efficiency, achieving competitive performance on various scientific tasks using fewer than five million curated samples without large-scale pretraining. These results highlight that effective reasoning can be achieved through principled data selection rather than indiscriminate scaling. (iii) Third, Innovator-VL demonstrates strong generalization, achieving competitive performance on general vision, multimodal reasoning, and scientific benchmarks. This indicates that scientific alignment can be integrated into a unified model without compromising general-purpose capabilities. Our practices suggest that efficient, reproducible, and high-performing scientific multimodal models can be built even without large-scale data, providing a practical foundation for future research."
    }
  ],
  "赵波老师实验室": [
    {
      "title": "Innovator-VL: A Multimodal Large Language Model for Scientific Discovery",
      "url": "https://arxiv.org/abs/2601.19325",
      "date": "2026-01-27",
      "authors_text": "Zichen Wen, Boxue Yang, Shuang Chen, Yaojie Zhang, Yuhang Han",
      "is_highlight": false,
      "score": 78.0,
      "summary": "Innovator-VL is a multimodal large language model that enhances scientific discovery through efficient training, transparency, and strong generalization without relying on extensive pretraining.",
      "teaser_image": "https://arxiv.org/html/2601.19325/x8.png",
      "debug_abstract": "We present Innovator-VL, a scientific multimodal large language model designed to advance understanding and reasoning across diverse scientific domains while maintaining excellent performance on general vision tasks. Contrary to the trend of relying on massive domain-specific pretraining and opaque pipelines, our work demonstrates that principled training design and transparent methodology can yield strong scientific intelligence with substantially reduced data requirements. (i) First, we provide a fully transparent, end-to-end reproducible training pipeline, covering data collection, cleaning, preprocessing, supervised fine-tuning, reinforcement learning, and evaluation, along with detailed optimization recipes. This facilitates systematic extension by the community. (ii) Second, Innovator-VL exhibits remarkable data efficiency, achieving competitive performance on various scientific tasks using fewer than five million curated samples without large-scale pretraining. These results highlight that effective reasoning can be achieved through principled data selection rather than indiscriminate scaling. (iii) Third, Innovator-VL demonstrates strong generalization, achieving competitive performance on general vision, multimodal reasoning, and scientific benchmarks. This indicates that scientific alignment can be integrated into a unified model without compromising general-purpose capabilities. Our practices suggest that efficient, reproducible, and high-performing scientific multimodal models can be built even without large-scale data, providing a practical foundation for future research."
    }
  ],
  "机器视觉与智能学习实验室 (MVIG)": [
    {
      "title": "Innovator-VL: A Multimodal Large Language Model for Scientific Discovery",
      "url": "https://arxiv.org/abs/2601.19325",
      "date": "2026-01-27",
      "authors_text": "Zichen Wen, Boxue Yang, Shuang Chen, Yaojie Zhang, Yuhang Han",
      "is_highlight": false,
      "score": 78.0,
      "summary": "Innovator-VL is a multimodal large language model that enhances scientific discovery through efficient training, transparency, and strong generalization without relying on extensive pretraining.",
      "teaser_image": "https://arxiv.org/html/2601.19325/x8.png",
      "debug_abstract": "We present Innovator-VL, a scientific multimodal large language model designed to advance understanding and reasoning across diverse scientific domains while maintaining excellent performance on general vision tasks. Contrary to the trend of relying on massive domain-specific pretraining and opaque pipelines, our work demonstrates that principled training design and transparent methodology can yield strong scientific intelligence with substantially reduced data requirements. (i) First, we provide a fully transparent, end-to-end reproducible training pipeline, covering data collection, cleaning, preprocessing, supervised fine-tuning, reinforcement learning, and evaluation, along with detailed optimization recipes. This facilitates systematic extension by the community. (ii) Second, Innovator-VL exhibits remarkable data efficiency, achieving competitive performance on various scientific tasks using fewer than five million curated samples without large-scale pretraining. These results highlight that effective reasoning can be achieved through principled data selection rather than indiscriminate scaling. (iii) Third, Innovator-VL demonstrates strong generalization, achieving competitive performance on general vision, multimodal reasoning, and scientific benchmarks. This indicates that scientific alignment can be integrated into a unified model without compromising general-purpose capabilities. Our practices suggest that efficient, reproducible, and high-performing scientific multimodal models can be built even without large-scale data, providing a practical foundation for future research."
    }
  ],
  "IWIN-FINS实验室": [
    {
      "title": "Innovator-VL: A Multimodal Large Language Model for Scientific Discovery",
      "url": "https://arxiv.org/abs/2601.19325",
      "date": "2026-01-27",
      "authors_text": "Zichen Wen, Boxue Yang, Shuang Chen, Yaojie Zhang, Yuhang Han",
      "is_highlight": false,
      "score": 78.0,
      "summary": "Innovator-VL is a multimodal large language model that enhances scientific discovery through efficient training, transparency, and strong generalization without relying on extensive pretraining.",
      "teaser_image": "https://arxiv.org/html/2601.19325/x8.png",
      "debug_abstract": "We present Innovator-VL, a scientific multimodal large language model designed to advance understanding and reasoning across diverse scientific domains while maintaining excellent performance on general vision tasks. Contrary to the trend of relying on massive domain-specific pretraining and opaque pipelines, our work demonstrates that principled training design and transparent methodology can yield strong scientific intelligence with substantially reduced data requirements. (i) First, we provide a fully transparent, end-to-end reproducible training pipeline, covering data collection, cleaning, preprocessing, supervised fine-tuning, reinforcement learning, and evaluation, along with detailed optimization recipes. This facilitates systematic extension by the community. (ii) Second, Innovator-VL exhibits remarkable data efficiency, achieving competitive performance on various scientific tasks using fewer than five million curated samples without large-scale pretraining. These results highlight that effective reasoning can be achieved through principled data selection rather than indiscriminate scaling. (iii) Third, Innovator-VL demonstrates strong generalization, achieving competitive performance on general vision, multimodal reasoning, and scientific benchmarks. This indicates that scientific alignment can be integrated into a unified model without compromising general-purpose capabilities. Our practices suggest that efficient, reproducible, and high-performing scientific multimodal models can be built even without large-scale data, providing a practical foundation for future research."
    }
  ],
  "智能技术与系统实验室": [
    {
      "title": "Scalable Exploration for High-Dimensional Continuous Control via Value-Guided Flow",
      "url": "https://arxiv.org/abs/2601.19707",
      "date": "2026-01-27",
      "authors_text": "Yunyue Wei, Chenhui Zuo, Yanan Sui",
      "is_highlight": false,
      "score": 88.0,
      "summary": "Qflex introduces a scalable exploration strategy for high-dimensional continuous control in reinforcement learning, leveraging value-guided flows to enhance efficiency and performance.",
      "teaser_image": "https://arxiv.org/html/2601.19707/figs/syn_res3_small.png",
      "debug_abstract": "Controlling high-dimensional systems in biological and robotic applications is challenging due to expansive state-action spaces, where effective exploration is critical. Commonly used exploration strategies in reinforcement learning are largely undirected with sharp degradation as action dimensionality grows. Many existing methods resort to dimensionality reduction, which constrains policy expressiveness and forfeits system flexibility. We introduce Q-guided Flow Exploration (Qflex), a scalable reinforcement learning method that conducts exploration directly in the native high-dimensional action space. During training, Qflex traverses actions from a learnable source distribution along a probability flow induced by the learned value function, aligning exploration with task-relevant gradients rather than isotropic noise. Our proposed method substantially outperforms representative online reinforcement learning baselines across diverse high-dimensional continuous-control benchmarks. Qflex also successfully controls a full-body human musculoskeletal model to perform agile, complex movements, demonstrating superior scalability and sample efficiency in very high-dimensional settings. Our results indicate that value-guided flows offer a principled and practical route to exploration at scale."
    },
    {
      "title": "Visual Generation Unlocks Human-Like Reasoning through Multimodal World Models",
      "url": "https://arxiv.org/abs/2601.19834",
      "date": "2026-01-27",
      "authors_text": "Jialong Wu, Xiaoying Zhang, Hongyi Yuan, Xiangcheng Zhang, Tianhao Huang",
      "is_highlight": false,
      "score": 83.0,
      "summary": "This paper demonstrates that visual generation enhances reasoning in AI, particularly for physical tasks, by integrating multimodal world models with chain-of-thought reasoning.",
      "teaser_image": "https://arxiv.org/html/2601.19834/x1.png",
      "debug_abstract": "Humans construct internal world models and reason by manipulating the concepts within these models. Recent advances in AI, particularly chain-of-thought (CoT) reasoning, approximate such human cognitive abilities, where world models are believed to be embedded within large language models. Expert-level performance in formal and abstract domains such as mathematics and programming has been achieved in current systems by relying predominantly on verbal reasoning. However, they still lag far behind humans in domains like physical and spatial intelligence, which require richer representations and prior knowledge. The emergence of unified multimodal models (UMMs) capable of both verbal and visual generation has therefore sparked interest in more human-like reasoning grounded in complementary multimodal pathways, though their benefits remain unclear. From a world-model perspective, this paper presents the first principled study of when and how visual generation benefits reasoning. Our key position is the visual superiority hypothesis: for certain tasks--particularly those grounded in the physical world--visual generation more naturally serves as world models, whereas purely verbal world models encounter bottlenecks arising from representational limitations or insufficient prior knowledge. Theoretically, we formalize internal world modeling as a core component of CoT reasoning and analyze distinctions among different forms of world models. Empirically, we identify tasks that necessitate interleaved visual-verbal CoT reasoning, constructing a new evaluation suite, VisWorld-Eval. Controlled experiments on a state-of-the-art UMM show that interleaved CoT significantly outperforms purely verbal CoT on tasks that favor visual world modeling, but offers no clear advantage otherwise. Together, this work clarifies the potential of multimodal world modeling for more powerful, human-like multimodal AI."
    },
    {
      "title": "AtomMem : Learnable Dynamic Agentic Memory with Atomic Memory Operation",
      "url": "https://arxiv.org/abs/2601.08323",
      "date": "2026-01-27",
      "authors_text": "Yupeng Huo, Yaxi Lu, Zhong Zhang, Haotian Chen, Yankai Lin",
      "is_highlight": false,
      "score": 84.0,
      "summary": "AtomMem introduces a learnable dynamic memory framework that optimizes memory management through atomic CRUD operations, outperforming static methods in long-context tasks.",
      "teaser_image": "https://arxiv.org/html/2601.08323/x2.png",
      "debug_abstract": "Equipping agents with memory is essential for solving real-world long-horizon problems. However, most existing agent memory mechanisms rely on static and hand-crafted workflows. This limits the performance and generalization ability of these memory designs, which highlights the need for a more flexible, learning-based memory framework. In this paper, we propose AtomMem, which reframes memory management as a dynamic decision-making problem. We deconstruct high-level memory processes into fundamental atomic CRUD (Create, Read, Update, Delete) operations, transforming the memory workflow into a learnable decision process. By combining supervised fine-tuning with reinforcement learning, AtomMem learns an autonomous, task-aligned policy to orchestrate memory behaviors tailored to specific task demands. Experimental results across 3 long-context benchmarks demonstrate that the trained AtomMem-8B consistently outperforms prior static-workflow memory methods. Further analysis of training dynamics shows that our learning-based formulation enables the agent to discover structured, task-aligned memory management strategies, highlighting a key advantage over predefined routines."
    },
    {
      "title": "Self-Supervised Path Planning in Unstructured Environments via Global-Guided Differentiable Hard Constraint Projection",
      "url": "https://arxiv.org/abs/2601.19354",
      "date": "2026-01-27",
      "authors_text": "Ziqian Wang, Chenxi Fang, Zhen Zhang",
      "is_highlight": false,
      "score": 89.0,
      "summary": "The paper presents a self-supervised path planning framework using a differentiable hard constraint projection for safe navigation in unstructured environments, achieving high success rates and real-time performance on embedded hardware.",
      "teaser_image": "https://arxiv.org/html/2601.19354/x1.png",
      "debug_abstract": "Deploying deep learning agents for autonomous navigation in unstructured environments faces critical challenges regarding safety, data scarcity, and limited computational resources. Traditional solvers often suffer from high latency, while emerging learning-based approaches struggle to ensure deterministic feasibility. To bridge the gap from embodied to embedded intelligence, we propose a self-supervised framework incorporating a differentiable hard constraint projection layer for runtime assurance. To mitigate data scarcity, we construct a Global-Guided Artificial Potential Field (G-APF), which provides dense supervision signals without manual labeling. To enforce actuator limitations and geometric constraints efficiently, we employ an adaptive neural projection layer, which iteratively rectifies the coarse network output onto the feasible manifold. Extensive benchmarks on a test set of 20,000 scenarios demonstrate an 88.75\\% success rate, substantiating the enhanced operational safety. Closed-loop experiments in CARLA further validate the physical realizability of the planned paths under dynamic constraints. Furthermore, deployment verification on an NVIDIA Jetson Orin NX confirms an inference latency of 94 ms, showing real-time feasibility on resource-constrained embedded hardware. This framework offers a generalized paradigm for embedding physical laws into neural architectures, providing a viable direction for solving constrained optimization in mechatronics. Source code is available at: this https URL."
    }
  ],
  "智能产业研究院 (AIR)": [
    {
      "title": "Scalable Exploration for High-Dimensional Continuous Control via Value-Guided Flow",
      "url": "https://arxiv.org/abs/2601.19707",
      "date": "2026-01-27",
      "authors_text": "Yunyue Wei, Chenhui Zuo, Yanan Sui",
      "is_highlight": false,
      "score": 88.0,
      "summary": "Qflex introduces a scalable exploration strategy for high-dimensional continuous control in reinforcement learning, leveraging value-guided flows to enhance efficiency and performance.",
      "teaser_image": "https://arxiv.org/html/2601.19707/figs/syn_res3_small.png",
      "debug_abstract": "Controlling high-dimensional systems in biological and robotic applications is challenging due to expansive state-action spaces, where effective exploration is critical. Commonly used exploration strategies in reinforcement learning are largely undirected with sharp degradation as action dimensionality grows. Many existing methods resort to dimensionality reduction, which constrains policy expressiveness and forfeits system flexibility. We introduce Q-guided Flow Exploration (Qflex), a scalable reinforcement learning method that conducts exploration directly in the native high-dimensional action space. During training, Qflex traverses actions from a learnable source distribution along a probability flow induced by the learned value function, aligning exploration with task-relevant gradients rather than isotropic noise. Our proposed method substantially outperforms representative online reinforcement learning baselines across diverse high-dimensional continuous-control benchmarks. Qflex also successfully controls a full-body human musculoskeletal model to perform agile, complex movements, demonstrating superior scalability and sample efficiency in very high-dimensional settings. Our results indicate that value-guided flows offer a principled and practical route to exploration at scale."
    },
    {
      "title": "Visual Generation Unlocks Human-Like Reasoning through Multimodal World Models",
      "url": "https://arxiv.org/abs/2601.19834",
      "date": "2026-01-27",
      "authors_text": "Jialong Wu, Xiaoying Zhang, Hongyi Yuan, Xiangcheng Zhang, Tianhao Huang",
      "is_highlight": false,
      "score": 83.0,
      "summary": "This paper demonstrates that visual generation enhances reasoning in AI, particularly for physical tasks, by integrating multimodal world models with chain-of-thought reasoning.",
      "teaser_image": "https://arxiv.org/html/2601.19834/x1.png",
      "debug_abstract": "Humans construct internal world models and reason by manipulating the concepts within these models. Recent advances in AI, particularly chain-of-thought (CoT) reasoning, approximate such human cognitive abilities, where world models are believed to be embedded within large language models. Expert-level performance in formal and abstract domains such as mathematics and programming has been achieved in current systems by relying predominantly on verbal reasoning. However, they still lag far behind humans in domains like physical and spatial intelligence, which require richer representations and prior knowledge. The emergence of unified multimodal models (UMMs) capable of both verbal and visual generation has therefore sparked interest in more human-like reasoning grounded in complementary multimodal pathways, though their benefits remain unclear. From a world-model perspective, this paper presents the first principled study of when and how visual generation benefits reasoning. Our key position is the visual superiority hypothesis: for certain tasks--particularly those grounded in the physical world--visual generation more naturally serves as world models, whereas purely verbal world models encounter bottlenecks arising from representational limitations or insufficient prior knowledge. Theoretically, we formalize internal world modeling as a core component of CoT reasoning and analyze distinctions among different forms of world models. Empirically, we identify tasks that necessitate interleaved visual-verbal CoT reasoning, constructing a new evaluation suite, VisWorld-Eval. Controlled experiments on a state-of-the-art UMM show that interleaved CoT significantly outperforms purely verbal CoT on tasks that favor visual world modeling, but offers no clear advantage otherwise. Together, this work clarifies the potential of multimodal world modeling for more powerful, human-like multimodal AI."
    },
    {
      "title": "AtomMem : Learnable Dynamic Agentic Memory with Atomic Memory Operation",
      "url": "https://arxiv.org/abs/2601.08323",
      "date": "2026-01-27",
      "authors_text": "Yupeng Huo, Yaxi Lu, Zhong Zhang, Haotian Chen, Yankai Lin",
      "is_highlight": false,
      "score": 84.0,
      "summary": "AtomMem introduces a learnable dynamic memory framework that optimizes memory management through atomic CRUD operations, outperforming static methods in long-context tasks.",
      "teaser_image": "https://arxiv.org/html/2601.08323/x2.png",
      "debug_abstract": "Equipping agents with memory is essential for solving real-world long-horizon problems. However, most existing agent memory mechanisms rely on static and hand-crafted workflows. This limits the performance and generalization ability of these memory designs, which highlights the need for a more flexible, learning-based memory framework. In this paper, we propose AtomMem, which reframes memory management as a dynamic decision-making problem. We deconstruct high-level memory processes into fundamental atomic CRUD (Create, Read, Update, Delete) operations, transforming the memory workflow into a learnable decision process. By combining supervised fine-tuning with reinforcement learning, AtomMem learns an autonomous, task-aligned policy to orchestrate memory behaviors tailored to specific task demands. Experimental results across 3 long-context benchmarks demonstrate that the trained AtomMem-8B consistently outperforms prior static-workflow memory methods. Further analysis of training dynamics shows that our learning-based formulation enables the agent to discover structured, task-aligned memory management strategies, highlighting a key advantage over predefined routines."
    },
    {
      "title": "Self-Supervised Path Planning in Unstructured Environments via Global-Guided Differentiable Hard Constraint Projection",
      "url": "https://arxiv.org/abs/2601.19354",
      "date": "2026-01-27",
      "authors_text": "Ziqian Wang, Chenxi Fang, Zhen Zhang",
      "is_highlight": false,
      "score": 89.0,
      "summary": "The paper presents a self-supervised path planning framework using a differentiable hard constraint projection for safe navigation in unstructured environments, achieving high success rates and real-time performance on embedded hardware.",
      "teaser_image": "https://arxiv.org/html/2601.19354/x1.png",
      "debug_abstract": "Deploying deep learning agents for autonomous navigation in unstructured environments faces critical challenges regarding safety, data scarcity, and limited computational resources. Traditional solvers often suffer from high latency, while emerging learning-based approaches struggle to ensure deterministic feasibility. To bridge the gap from embodied to embedded intelligence, we propose a self-supervised framework incorporating a differentiable hard constraint projection layer for runtime assurance. To mitigate data scarcity, we construct a Global-Guided Artificial Potential Field (G-APF), which provides dense supervision signals without manual labeling. To enforce actuator limitations and geometric constraints efficiently, we employ an adaptive neural projection layer, which iteratively rectifies the coarse network output onto the feasible manifold. Extensive benchmarks on a test set of 20,000 scenarios demonstrate an 88.75\\% success rate, substantiating the enhanced operational safety. Closed-loop experiments in CARLA further validate the physical realizability of the planned paths under dynamic constraints. Furthermore, deployment verification on an NVIDIA Jetson Orin NX confirms an inference latency of 94 ms, showing real-time feasibility on resource-constrained embedded hardware. This framework offers a generalized paradigm for embedding physical laws into neural architectures, providing a viable direction for solving constrained optimization in mechatronics. Source code is available at: this https URL."
    }
  ],
  "机器人控制实验室": [
    {
      "title": "Scalable Exploration for High-Dimensional Continuous Control via Value-Guided Flow",
      "url": "https://arxiv.org/abs/2601.19707",
      "date": "2026-01-27",
      "authors_text": "Yunyue Wei, Chenhui Zuo, Yanan Sui",
      "is_highlight": false,
      "score": 88.0,
      "summary": "Qflex introduces a scalable exploration strategy for high-dimensional continuous control in reinforcement learning, leveraging value-guided flows to enhance efficiency and performance.",
      "teaser_image": "https://arxiv.org/html/2601.19707/figs/syn_res3_small.png",
      "debug_abstract": "Controlling high-dimensional systems in biological and robotic applications is challenging due to expansive state-action spaces, where effective exploration is critical. Commonly used exploration strategies in reinforcement learning are largely undirected with sharp degradation as action dimensionality grows. Many existing methods resort to dimensionality reduction, which constrains policy expressiveness and forfeits system flexibility. We introduce Q-guided Flow Exploration (Qflex), a scalable reinforcement learning method that conducts exploration directly in the native high-dimensional action space. During training, Qflex traverses actions from a learnable source distribution along a probability flow induced by the learned value function, aligning exploration with task-relevant gradients rather than isotropic noise. Our proposed method substantially outperforms representative online reinforcement learning baselines across diverse high-dimensional continuous-control benchmarks. Qflex also successfully controls a full-body human musculoskeletal model to perform agile, complex movements, demonstrating superior scalability and sample efficiency in very high-dimensional settings. Our results indicate that value-guided flows offer a principled and practical route to exploration at scale."
    },
    {
      "title": "Visual Generation Unlocks Human-Like Reasoning through Multimodal World Models",
      "url": "https://arxiv.org/abs/2601.19834",
      "date": "2026-01-27",
      "authors_text": "Jialong Wu, Xiaoying Zhang, Hongyi Yuan, Xiangcheng Zhang, Tianhao Huang",
      "is_highlight": false,
      "score": 83.0,
      "summary": "This paper demonstrates that visual generation enhances reasoning in AI, particularly for physical tasks, by integrating multimodal world models with chain-of-thought reasoning.",
      "teaser_image": "https://arxiv.org/html/2601.19834/x1.png",
      "debug_abstract": "Humans construct internal world models and reason by manipulating the concepts within these models. Recent advances in AI, particularly chain-of-thought (CoT) reasoning, approximate such human cognitive abilities, where world models are believed to be embedded within large language models. Expert-level performance in formal and abstract domains such as mathematics and programming has been achieved in current systems by relying predominantly on verbal reasoning. However, they still lag far behind humans in domains like physical and spatial intelligence, which require richer representations and prior knowledge. The emergence of unified multimodal models (UMMs) capable of both verbal and visual generation has therefore sparked interest in more human-like reasoning grounded in complementary multimodal pathways, though their benefits remain unclear. From a world-model perspective, this paper presents the first principled study of when and how visual generation benefits reasoning. Our key position is the visual superiority hypothesis: for certain tasks--particularly those grounded in the physical world--visual generation more naturally serves as world models, whereas purely verbal world models encounter bottlenecks arising from representational limitations or insufficient prior knowledge. Theoretically, we formalize internal world modeling as a core component of CoT reasoning and analyze distinctions among different forms of world models. Empirically, we identify tasks that necessitate interleaved visual-verbal CoT reasoning, constructing a new evaluation suite, VisWorld-Eval. Controlled experiments on a state-of-the-art UMM show that interleaved CoT significantly outperforms purely verbal CoT on tasks that favor visual world modeling, but offers no clear advantage otherwise. Together, this work clarifies the potential of multimodal world modeling for more powerful, human-like multimodal AI."
    },
    {
      "title": "AtomMem : Learnable Dynamic Agentic Memory with Atomic Memory Operation",
      "url": "https://arxiv.org/abs/2601.08323",
      "date": "2026-01-27",
      "authors_text": "Yupeng Huo, Yaxi Lu, Zhong Zhang, Haotian Chen, Yankai Lin",
      "is_highlight": false,
      "score": 84.0,
      "summary": "AtomMem introduces a learnable dynamic memory framework that optimizes memory management through atomic CRUD operations, outperforming static methods in long-context tasks.",
      "teaser_image": "https://arxiv.org/html/2601.08323/x2.png",
      "debug_abstract": "Equipping agents with memory is essential for solving real-world long-horizon problems. However, most existing agent memory mechanisms rely on static and hand-crafted workflows. This limits the performance and generalization ability of these memory designs, which highlights the need for a more flexible, learning-based memory framework. In this paper, we propose AtomMem, which reframes memory management as a dynamic decision-making problem. We deconstruct high-level memory processes into fundamental atomic CRUD (Create, Read, Update, Delete) operations, transforming the memory workflow into a learnable decision process. By combining supervised fine-tuning with reinforcement learning, AtomMem learns an autonomous, task-aligned policy to orchestrate memory behaviors tailored to specific task demands. Experimental results across 3 long-context benchmarks demonstrate that the trained AtomMem-8B consistently outperforms prior static-workflow memory methods. Further analysis of training dynamics shows that our learning-based formulation enables the agent to discover structured, task-aligned memory management strategies, highlighting a key advantage over predefined routines."
    },
    {
      "title": "Self-Supervised Path Planning in Unstructured Environments via Global-Guided Differentiable Hard Constraint Projection",
      "url": "https://arxiv.org/abs/2601.19354",
      "date": "2026-01-27",
      "authors_text": "Ziqian Wang, Chenxi Fang, Zhen Zhang",
      "is_highlight": false,
      "score": 89.0,
      "summary": "The paper presents a self-supervised path planning framework using a differentiable hard constraint projection for safe navigation in unstructured environments, achieving high success rates and real-time performance on embedded hardware.",
      "teaser_image": "https://arxiv.org/html/2601.19354/x1.png",
      "debug_abstract": "Deploying deep learning agents for autonomous navigation in unstructured environments faces critical challenges regarding safety, data scarcity, and limited computational resources. Traditional solvers often suffer from high latency, while emerging learning-based approaches struggle to ensure deterministic feasibility. To bridge the gap from embodied to embedded intelligence, we propose a self-supervised framework incorporating a differentiable hard constraint projection layer for runtime assurance. To mitigate data scarcity, we construct a Global-Guided Artificial Potential Field (G-APF), which provides dense supervision signals without manual labeling. To enforce actuator limitations and geometric constraints efficiently, we employ an adaptive neural projection layer, which iteratively rectifies the coarse network output onto the feasible manifold. Extensive benchmarks on a test set of 20,000 scenarios demonstrate an 88.75\\% success rate, substantiating the enhanced operational safety. Closed-loop experiments in CARLA further validate the physical realizability of the planned paths under dynamic constraints. Furthermore, deployment verification on an NVIDIA Jetson Orin NX confirms an inference latency of 94 ms, showing real-time feasibility on resource-constrained embedded hardware. This framework offers a generalized paradigm for embedding physical laws into neural architectures, providing a viable direction for solving constrained optimization in mechatronics. Source code is available at: this https URL."
    }
  ],
  "具身视觉与机器人实验室 (EVAR Lab)": [
    {
      "title": "Scalable Exploration for High-Dimensional Continuous Control via Value-Guided Flow",
      "url": "https://arxiv.org/abs/2601.19707",
      "date": "2026-01-27",
      "authors_text": "Yunyue Wei, Chenhui Zuo, Yanan Sui",
      "is_highlight": false,
      "score": 88.0,
      "summary": "Qflex introduces a scalable exploration strategy for high-dimensional continuous control in reinforcement learning, leveraging value-guided flows to enhance efficiency and performance.",
      "teaser_image": "https://arxiv.org/html/2601.19707/figs/syn_res3_small.png",
      "debug_abstract": "Controlling high-dimensional systems in biological and robotic applications is challenging due to expansive state-action spaces, where effective exploration is critical. Commonly used exploration strategies in reinforcement learning are largely undirected with sharp degradation as action dimensionality grows. Many existing methods resort to dimensionality reduction, which constrains policy expressiveness and forfeits system flexibility. We introduce Q-guided Flow Exploration (Qflex), a scalable reinforcement learning method that conducts exploration directly in the native high-dimensional action space. During training, Qflex traverses actions from a learnable source distribution along a probability flow induced by the learned value function, aligning exploration with task-relevant gradients rather than isotropic noise. Our proposed method substantially outperforms representative online reinforcement learning baselines across diverse high-dimensional continuous-control benchmarks. Qflex also successfully controls a full-body human musculoskeletal model to perform agile, complex movements, demonstrating superior scalability and sample efficiency in very high-dimensional settings. Our results indicate that value-guided flows offer a principled and practical route to exploration at scale."
    },
    {
      "title": "Visual Generation Unlocks Human-Like Reasoning through Multimodal World Models",
      "url": "https://arxiv.org/abs/2601.19834",
      "date": "2026-01-27",
      "authors_text": "Jialong Wu, Xiaoying Zhang, Hongyi Yuan, Xiangcheng Zhang, Tianhao Huang",
      "is_highlight": false,
      "score": 83.0,
      "summary": "This paper demonstrates that visual generation enhances reasoning in AI, particularly for physical tasks, by integrating multimodal world models with chain-of-thought reasoning.",
      "teaser_image": "https://arxiv.org/html/2601.19834/x1.png",
      "debug_abstract": "Humans construct internal world models and reason by manipulating the concepts within these models. Recent advances in AI, particularly chain-of-thought (CoT) reasoning, approximate such human cognitive abilities, where world models are believed to be embedded within large language models. Expert-level performance in formal and abstract domains such as mathematics and programming has been achieved in current systems by relying predominantly on verbal reasoning. However, they still lag far behind humans in domains like physical and spatial intelligence, which require richer representations and prior knowledge. The emergence of unified multimodal models (UMMs) capable of both verbal and visual generation has therefore sparked interest in more human-like reasoning grounded in complementary multimodal pathways, though their benefits remain unclear. From a world-model perspective, this paper presents the first principled study of when and how visual generation benefits reasoning. Our key position is the visual superiority hypothesis: for certain tasks--particularly those grounded in the physical world--visual generation more naturally serves as world models, whereas purely verbal world models encounter bottlenecks arising from representational limitations or insufficient prior knowledge. Theoretically, we formalize internal world modeling as a core component of CoT reasoning and analyze distinctions among different forms of world models. Empirically, we identify tasks that necessitate interleaved visual-verbal CoT reasoning, constructing a new evaluation suite, VisWorld-Eval. Controlled experiments on a state-of-the-art UMM show that interleaved CoT significantly outperforms purely verbal CoT on tasks that favor visual world modeling, but offers no clear advantage otherwise. Together, this work clarifies the potential of multimodal world modeling for more powerful, human-like multimodal AI."
    },
    {
      "title": "AtomMem : Learnable Dynamic Agentic Memory with Atomic Memory Operation",
      "url": "https://arxiv.org/abs/2601.08323",
      "date": "2026-01-27",
      "authors_text": "Yupeng Huo, Yaxi Lu, Zhong Zhang, Haotian Chen, Yankai Lin",
      "is_highlight": false,
      "score": 84.0,
      "summary": "AtomMem introduces a learnable dynamic memory framework that optimizes memory management through atomic CRUD operations, outperforming static methods in long-context tasks.",
      "teaser_image": "https://arxiv.org/html/2601.08323/x2.png",
      "debug_abstract": "Equipping agents with memory is essential for solving real-world long-horizon problems. However, most existing agent memory mechanisms rely on static and hand-crafted workflows. This limits the performance and generalization ability of these memory designs, which highlights the need for a more flexible, learning-based memory framework. In this paper, we propose AtomMem, which reframes memory management as a dynamic decision-making problem. We deconstruct high-level memory processes into fundamental atomic CRUD (Create, Read, Update, Delete) operations, transforming the memory workflow into a learnable decision process. By combining supervised fine-tuning with reinforcement learning, AtomMem learns an autonomous, task-aligned policy to orchestrate memory behaviors tailored to specific task demands. Experimental results across 3 long-context benchmarks demonstrate that the trained AtomMem-8B consistently outperforms prior static-workflow memory methods. Further analysis of training dynamics shows that our learning-based formulation enables the agent to discover structured, task-aligned memory management strategies, highlighting a key advantage over predefined routines."
    },
    {
      "title": "Self-Supervised Path Planning in Unstructured Environments via Global-Guided Differentiable Hard Constraint Projection",
      "url": "https://arxiv.org/abs/2601.19354",
      "date": "2026-01-27",
      "authors_text": "Ziqian Wang, Chenxi Fang, Zhen Zhang",
      "is_highlight": false,
      "score": 89.0,
      "summary": "The paper presents a self-supervised path planning framework using a differentiable hard constraint projection for safe navigation in unstructured environments, achieving high success rates and real-time performance on embedded hardware.",
      "teaser_image": "https://arxiv.org/html/2601.19354/x1.png",
      "debug_abstract": "Deploying deep learning agents for autonomous navigation in unstructured environments faces critical challenges regarding safety, data scarcity, and limited computational resources. Traditional solvers often suffer from high latency, while emerging learning-based approaches struggle to ensure deterministic feasibility. To bridge the gap from embodied to embedded intelligence, we propose a self-supervised framework incorporating a differentiable hard constraint projection layer for runtime assurance. To mitigate data scarcity, we construct a Global-Guided Artificial Potential Field (G-APF), which provides dense supervision signals without manual labeling. To enforce actuator limitations and geometric constraints efficiently, we employ an adaptive neural projection layer, which iteratively rectifies the coarse network output onto the feasible manifold. Extensive benchmarks on a test set of 20,000 scenarios demonstrate an 88.75\\% success rate, substantiating the enhanced operational safety. Closed-loop experiments in CARLA further validate the physical realizability of the planned paths under dynamic constraints. Furthermore, deployment verification on an NVIDIA Jetson Orin NX confirms an inference latency of 94 ms, showing real-time feasibility on resource-constrained embedded hardware. This framework offers a generalized paradigm for embedding physical laws into neural architectures, providing a viable direction for solving constrained optimization in mechatronics. Source code is available at: this https URL."
    }
  ],
  "智能系统与机器人实验室 (ISR Lab)": [
    {
      "title": "Scalable Exploration for High-Dimensional Continuous Control via Value-Guided Flow",
      "url": "https://arxiv.org/abs/2601.19707",
      "date": "2026-01-27",
      "authors_text": "Yunyue Wei, Chenhui Zuo, Yanan Sui",
      "is_highlight": false,
      "score": 88.0,
      "summary": "Qflex introduces a scalable exploration strategy for high-dimensional continuous control in reinforcement learning, leveraging value-guided flows to enhance efficiency and performance.",
      "teaser_image": "https://arxiv.org/html/2601.19707/figs/syn_res3_small.png",
      "debug_abstract": "Controlling high-dimensional systems in biological and robotic applications is challenging due to expansive state-action spaces, where effective exploration is critical. Commonly used exploration strategies in reinforcement learning are largely undirected with sharp degradation as action dimensionality grows. Many existing methods resort to dimensionality reduction, which constrains policy expressiveness and forfeits system flexibility. We introduce Q-guided Flow Exploration (Qflex), a scalable reinforcement learning method that conducts exploration directly in the native high-dimensional action space. During training, Qflex traverses actions from a learnable source distribution along a probability flow induced by the learned value function, aligning exploration with task-relevant gradients rather than isotropic noise. Our proposed method substantially outperforms representative online reinforcement learning baselines across diverse high-dimensional continuous-control benchmarks. Qflex also successfully controls a full-body human musculoskeletal model to perform agile, complex movements, demonstrating superior scalability and sample efficiency in very high-dimensional settings. Our results indicate that value-guided flows offer a principled and practical route to exploration at scale."
    },
    {
      "title": "Visual Generation Unlocks Human-Like Reasoning through Multimodal World Models",
      "url": "https://arxiv.org/abs/2601.19834",
      "date": "2026-01-27",
      "authors_text": "Jialong Wu, Xiaoying Zhang, Hongyi Yuan, Xiangcheng Zhang, Tianhao Huang",
      "is_highlight": false,
      "score": 83.0,
      "summary": "This paper demonstrates that visual generation enhances reasoning in AI, particularly for physical tasks, by integrating multimodal world models with chain-of-thought reasoning.",
      "teaser_image": "https://arxiv.org/html/2601.19834/x1.png",
      "debug_abstract": "Humans construct internal world models and reason by manipulating the concepts within these models. Recent advances in AI, particularly chain-of-thought (CoT) reasoning, approximate such human cognitive abilities, where world models are believed to be embedded within large language models. Expert-level performance in formal and abstract domains such as mathematics and programming has been achieved in current systems by relying predominantly on verbal reasoning. However, they still lag far behind humans in domains like physical and spatial intelligence, which require richer representations and prior knowledge. The emergence of unified multimodal models (UMMs) capable of both verbal and visual generation has therefore sparked interest in more human-like reasoning grounded in complementary multimodal pathways, though their benefits remain unclear. From a world-model perspective, this paper presents the first principled study of when and how visual generation benefits reasoning. Our key position is the visual superiority hypothesis: for certain tasks--particularly those grounded in the physical world--visual generation more naturally serves as world models, whereas purely verbal world models encounter bottlenecks arising from representational limitations or insufficient prior knowledge. Theoretically, we formalize internal world modeling as a core component of CoT reasoning and analyze distinctions among different forms of world models. Empirically, we identify tasks that necessitate interleaved visual-verbal CoT reasoning, constructing a new evaluation suite, VisWorld-Eval. Controlled experiments on a state-of-the-art UMM show that interleaved CoT significantly outperforms purely verbal CoT on tasks that favor visual world modeling, but offers no clear advantage otherwise. Together, this work clarifies the potential of multimodal world modeling for more powerful, human-like multimodal AI."
    },
    {
      "title": "AtomMem : Learnable Dynamic Agentic Memory with Atomic Memory Operation",
      "url": "https://arxiv.org/abs/2601.08323",
      "date": "2026-01-27",
      "authors_text": "Yupeng Huo, Yaxi Lu, Zhong Zhang, Haotian Chen, Yankai Lin",
      "is_highlight": false,
      "score": 84.0,
      "summary": "AtomMem introduces a learnable dynamic memory framework that optimizes memory management through atomic CRUD operations, outperforming static methods in long-context tasks.",
      "teaser_image": "https://arxiv.org/html/2601.08323/x2.png",
      "debug_abstract": "Equipping agents with memory is essential for solving real-world long-horizon problems. However, most existing agent memory mechanisms rely on static and hand-crafted workflows. This limits the performance and generalization ability of these memory designs, which highlights the need for a more flexible, learning-based memory framework. In this paper, we propose AtomMem, which reframes memory management as a dynamic decision-making problem. We deconstruct high-level memory processes into fundamental atomic CRUD (Create, Read, Update, Delete) operations, transforming the memory workflow into a learnable decision process. By combining supervised fine-tuning with reinforcement learning, AtomMem learns an autonomous, task-aligned policy to orchestrate memory behaviors tailored to specific task demands. Experimental results across 3 long-context benchmarks demonstrate that the trained AtomMem-8B consistently outperforms prior static-workflow memory methods. Further analysis of training dynamics shows that our learning-based formulation enables the agent to discover structured, task-aligned memory management strategies, highlighting a key advantage over predefined routines."
    },
    {
      "title": "Self-Supervised Path Planning in Unstructured Environments via Global-Guided Differentiable Hard Constraint Projection",
      "url": "https://arxiv.org/abs/2601.19354",
      "date": "2026-01-27",
      "authors_text": "Ziqian Wang, Chenxi Fang, Zhen Zhang",
      "is_highlight": false,
      "score": 89.0,
      "summary": "The paper presents a self-supervised path planning framework using a differentiable hard constraint projection for safe navigation in unstructured environments, achieving high success rates and real-time performance on embedded hardware.",
      "teaser_image": "https://arxiv.org/html/2601.19354/x1.png",
      "debug_abstract": "Deploying deep learning agents for autonomous navigation in unstructured environments faces critical challenges regarding safety, data scarcity, and limited computational resources. Traditional solvers often suffer from high latency, while emerging learning-based approaches struggle to ensure deterministic feasibility. To bridge the gap from embodied to embedded intelligence, we propose a self-supervised framework incorporating a differentiable hard constraint projection layer for runtime assurance. To mitigate data scarcity, we construct a Global-Guided Artificial Potential Field (G-APF), which provides dense supervision signals without manual labeling. To enforce actuator limitations and geometric constraints efficiently, we employ an adaptive neural projection layer, which iteratively rectifies the coarse network output onto the feasible manifold. Extensive benchmarks on a test set of 20,000 scenarios demonstrate an 88.75\\% success rate, substantiating the enhanced operational safety. Closed-loop experiments in CARLA further validate the physical realizability of the planned paths under dynamic constraints. Furthermore, deployment verification on an NVIDIA Jetson Orin NX confirms an inference latency of 94 ms, showing real-time feasibility on resource-constrained embedded hardware. This framework offers a generalized paradigm for embedding physical laws into neural architectures, providing a viable direction for solving constrained optimization in mechatronics. Source code is available at: this https URL."
    }
  ],
  "具身智能实验室 (TEA Lab)": [
    {
      "title": "Scalable Exploration for High-Dimensional Continuous Control via Value-Guided Flow",
      "url": "https://arxiv.org/abs/2601.19707",
      "date": "2026-01-27",
      "authors_text": "Yunyue Wei, Chenhui Zuo, Yanan Sui",
      "is_highlight": false,
      "score": 88.0,
      "summary": "Qflex introduces a scalable exploration strategy for high-dimensional continuous control in reinforcement learning, leveraging value-guided flows to enhance efficiency and performance.",
      "teaser_image": "https://arxiv.org/html/2601.19707/figs/syn_res3_small.png",
      "debug_abstract": "Controlling high-dimensional systems in biological and robotic applications is challenging due to expansive state-action spaces, where effective exploration is critical. Commonly used exploration strategies in reinforcement learning are largely undirected with sharp degradation as action dimensionality grows. Many existing methods resort to dimensionality reduction, which constrains policy expressiveness and forfeits system flexibility. We introduce Q-guided Flow Exploration (Qflex), a scalable reinforcement learning method that conducts exploration directly in the native high-dimensional action space. During training, Qflex traverses actions from a learnable source distribution along a probability flow induced by the learned value function, aligning exploration with task-relevant gradients rather than isotropic noise. Our proposed method substantially outperforms representative online reinforcement learning baselines across diverse high-dimensional continuous-control benchmarks. Qflex also successfully controls a full-body human musculoskeletal model to perform agile, complex movements, demonstrating superior scalability and sample efficiency in very high-dimensional settings. Our results indicate that value-guided flows offer a principled and practical route to exploration at scale."
    },
    {
      "title": "Visual Generation Unlocks Human-Like Reasoning through Multimodal World Models",
      "url": "https://arxiv.org/abs/2601.19834",
      "date": "2026-01-27",
      "authors_text": "Jialong Wu, Xiaoying Zhang, Hongyi Yuan, Xiangcheng Zhang, Tianhao Huang",
      "is_highlight": false,
      "score": 83.0,
      "summary": "This paper demonstrates that visual generation enhances reasoning in AI, particularly for physical tasks, by integrating multimodal world models with chain-of-thought reasoning.",
      "teaser_image": "https://arxiv.org/html/2601.19834/x1.png",
      "debug_abstract": "Humans construct internal world models and reason by manipulating the concepts within these models. Recent advances in AI, particularly chain-of-thought (CoT) reasoning, approximate such human cognitive abilities, where world models are believed to be embedded within large language models. Expert-level performance in formal and abstract domains such as mathematics and programming has been achieved in current systems by relying predominantly on verbal reasoning. However, they still lag far behind humans in domains like physical and spatial intelligence, which require richer representations and prior knowledge. The emergence of unified multimodal models (UMMs) capable of both verbal and visual generation has therefore sparked interest in more human-like reasoning grounded in complementary multimodal pathways, though their benefits remain unclear. From a world-model perspective, this paper presents the first principled study of when and how visual generation benefits reasoning. Our key position is the visual superiority hypothesis: for certain tasks--particularly those grounded in the physical world--visual generation more naturally serves as world models, whereas purely verbal world models encounter bottlenecks arising from representational limitations or insufficient prior knowledge. Theoretically, we formalize internal world modeling as a core component of CoT reasoning and analyze distinctions among different forms of world models. Empirically, we identify tasks that necessitate interleaved visual-verbal CoT reasoning, constructing a new evaluation suite, VisWorld-Eval. Controlled experiments on a state-of-the-art UMM show that interleaved CoT significantly outperforms purely verbal CoT on tasks that favor visual world modeling, but offers no clear advantage otherwise. Together, this work clarifies the potential of multimodal world modeling for more powerful, human-like multimodal AI."
    },
    {
      "title": "AtomMem : Learnable Dynamic Agentic Memory with Atomic Memory Operation",
      "url": "https://arxiv.org/abs/2601.08323",
      "date": "2026-01-27",
      "authors_text": "Yupeng Huo, Yaxi Lu, Zhong Zhang, Haotian Chen, Yankai Lin",
      "is_highlight": false,
      "score": 84.0,
      "summary": "AtomMem introduces a learnable dynamic memory framework that optimizes memory management through atomic CRUD operations, outperforming static methods in long-context tasks.",
      "teaser_image": "https://arxiv.org/html/2601.08323/x2.png",
      "debug_abstract": "Equipping agents with memory is essential for solving real-world long-horizon problems. However, most existing agent memory mechanisms rely on static and hand-crafted workflows. This limits the performance and generalization ability of these memory designs, which highlights the need for a more flexible, learning-based memory framework. In this paper, we propose AtomMem, which reframes memory management as a dynamic decision-making problem. We deconstruct high-level memory processes into fundamental atomic CRUD (Create, Read, Update, Delete) operations, transforming the memory workflow into a learnable decision process. By combining supervised fine-tuning with reinforcement learning, AtomMem learns an autonomous, task-aligned policy to orchestrate memory behaviors tailored to specific task demands. Experimental results across 3 long-context benchmarks demonstrate that the trained AtomMem-8B consistently outperforms prior static-workflow memory methods. Further analysis of training dynamics shows that our learning-based formulation enables the agent to discover structured, task-aligned memory management strategies, highlighting a key advantage over predefined routines."
    },
    {
      "title": "Self-Supervised Path Planning in Unstructured Environments via Global-Guided Differentiable Hard Constraint Projection",
      "url": "https://arxiv.org/abs/2601.19354",
      "date": "2026-01-27",
      "authors_text": "Ziqian Wang, Chenxi Fang, Zhen Zhang",
      "is_highlight": false,
      "score": 89.0,
      "summary": "The paper presents a self-supervised path planning framework using a differentiable hard constraint projection for safe navigation in unstructured environments, achieving high success rates and real-time performance on embedded hardware.",
      "teaser_image": "https://arxiv.org/html/2601.19354/x1.png",
      "debug_abstract": "Deploying deep learning agents for autonomous navigation in unstructured environments faces critical challenges regarding safety, data scarcity, and limited computational resources. Traditional solvers often suffer from high latency, while emerging learning-based approaches struggle to ensure deterministic feasibility. To bridge the gap from embodied to embedded intelligence, we propose a self-supervised framework incorporating a differentiable hard constraint projection layer for runtime assurance. To mitigate data scarcity, we construct a Global-Guided Artificial Potential Field (G-APF), which provides dense supervision signals without manual labeling. To enforce actuator limitations and geometric constraints efficiently, we employ an adaptive neural projection layer, which iteratively rectifies the coarse network output onto the feasible manifold. Extensive benchmarks on a test set of 20,000 scenarios demonstrate an 88.75\\% success rate, substantiating the enhanced operational safety. Closed-loop experiments in CARLA further validate the physical realizability of the planned paths under dynamic constraints. Furthermore, deployment verification on an NVIDIA Jetson Orin NX confirms an inference latency of 94 ms, showing real-time feasibility on resource-constrained embedded hardware. This framework offers a generalized paradigm for embedding physical laws into neural architectures, providing a viable direction for solving constrained optimization in mechatronics. Source code is available at: this https URL."
    }
  ],
  "MARS多模态学习实验室": [
    {
      "title": "Scalable Exploration for High-Dimensional Continuous Control via Value-Guided Flow",
      "url": "https://arxiv.org/abs/2601.19707",
      "date": "2026-01-27",
      "authors_text": "Yunyue Wei, Chenhui Zuo, Yanan Sui",
      "is_highlight": false,
      "score": 88.0,
      "summary": "Qflex introduces a scalable exploration strategy for high-dimensional continuous control in reinforcement learning, leveraging value-guided flows to enhance efficiency and performance.",
      "teaser_image": "https://arxiv.org/html/2601.19707/figs/syn_res3_small.png",
      "debug_abstract": "Controlling high-dimensional systems in biological and robotic applications is challenging due to expansive state-action spaces, where effective exploration is critical. Commonly used exploration strategies in reinforcement learning are largely undirected with sharp degradation as action dimensionality grows. Many existing methods resort to dimensionality reduction, which constrains policy expressiveness and forfeits system flexibility. We introduce Q-guided Flow Exploration (Qflex), a scalable reinforcement learning method that conducts exploration directly in the native high-dimensional action space. During training, Qflex traverses actions from a learnable source distribution along a probability flow induced by the learned value function, aligning exploration with task-relevant gradients rather than isotropic noise. Our proposed method substantially outperforms representative online reinforcement learning baselines across diverse high-dimensional continuous-control benchmarks. Qflex also successfully controls a full-body human musculoskeletal model to perform agile, complex movements, demonstrating superior scalability and sample efficiency in very high-dimensional settings. Our results indicate that value-guided flows offer a principled and practical route to exploration at scale."
    },
    {
      "title": "Visual Generation Unlocks Human-Like Reasoning through Multimodal World Models",
      "url": "https://arxiv.org/abs/2601.19834",
      "date": "2026-01-27",
      "authors_text": "Jialong Wu, Xiaoying Zhang, Hongyi Yuan, Xiangcheng Zhang, Tianhao Huang",
      "is_highlight": false,
      "score": 83.0,
      "summary": "This paper demonstrates that visual generation enhances reasoning in AI, particularly for physical tasks, by integrating multimodal world models with chain-of-thought reasoning.",
      "teaser_image": "https://arxiv.org/html/2601.19834/x1.png",
      "debug_abstract": "Humans construct internal world models and reason by manipulating the concepts within these models. Recent advances in AI, particularly chain-of-thought (CoT) reasoning, approximate such human cognitive abilities, where world models are believed to be embedded within large language models. Expert-level performance in formal and abstract domains such as mathematics and programming has been achieved in current systems by relying predominantly on verbal reasoning. However, they still lag far behind humans in domains like physical and spatial intelligence, which require richer representations and prior knowledge. The emergence of unified multimodal models (UMMs) capable of both verbal and visual generation has therefore sparked interest in more human-like reasoning grounded in complementary multimodal pathways, though their benefits remain unclear. From a world-model perspective, this paper presents the first principled study of when and how visual generation benefits reasoning. Our key position is the visual superiority hypothesis: for certain tasks--particularly those grounded in the physical world--visual generation more naturally serves as world models, whereas purely verbal world models encounter bottlenecks arising from representational limitations or insufficient prior knowledge. Theoretically, we formalize internal world modeling as a core component of CoT reasoning and analyze distinctions among different forms of world models. Empirically, we identify tasks that necessitate interleaved visual-verbal CoT reasoning, constructing a new evaluation suite, VisWorld-Eval. Controlled experiments on a state-of-the-art UMM show that interleaved CoT significantly outperforms purely verbal CoT on tasks that favor visual world modeling, but offers no clear advantage otherwise. Together, this work clarifies the potential of multimodal world modeling for more powerful, human-like multimodal AI."
    },
    {
      "title": "AtomMem : Learnable Dynamic Agentic Memory with Atomic Memory Operation",
      "url": "https://arxiv.org/abs/2601.08323",
      "date": "2026-01-27",
      "authors_text": "Yupeng Huo, Yaxi Lu, Zhong Zhang, Haotian Chen, Yankai Lin",
      "is_highlight": false,
      "score": 84.0,
      "summary": "AtomMem introduces a learnable dynamic memory framework that optimizes memory management through atomic CRUD operations, outperforming static methods in long-context tasks.",
      "teaser_image": "https://arxiv.org/html/2601.08323/x2.png",
      "debug_abstract": "Equipping agents with memory is essential for solving real-world long-horizon problems. However, most existing agent memory mechanisms rely on static and hand-crafted workflows. This limits the performance and generalization ability of these memory designs, which highlights the need for a more flexible, learning-based memory framework. In this paper, we propose AtomMem, which reframes memory management as a dynamic decision-making problem. We deconstruct high-level memory processes into fundamental atomic CRUD (Create, Read, Update, Delete) operations, transforming the memory workflow into a learnable decision process. By combining supervised fine-tuning with reinforcement learning, AtomMem learns an autonomous, task-aligned policy to orchestrate memory behaviors tailored to specific task demands. Experimental results across 3 long-context benchmarks demonstrate that the trained AtomMem-8B consistently outperforms prior static-workflow memory methods. Further analysis of training dynamics shows that our learning-based formulation enables the agent to discover structured, task-aligned memory management strategies, highlighting a key advantage over predefined routines."
    },
    {
      "title": "Self-Supervised Path Planning in Unstructured Environments via Global-Guided Differentiable Hard Constraint Projection",
      "url": "https://arxiv.org/abs/2601.19354",
      "date": "2026-01-27",
      "authors_text": "Ziqian Wang, Chenxi Fang, Zhen Zhang",
      "is_highlight": false,
      "score": 89.0,
      "summary": "The paper presents a self-supervised path planning framework using a differentiable hard constraint projection for safe navigation in unstructured environments, achieving high success rates and real-time performance on embedded hardware.",
      "teaser_image": "https://arxiv.org/html/2601.19354/x1.png",
      "debug_abstract": "Deploying deep learning agents for autonomous navigation in unstructured environments faces critical challenges regarding safety, data scarcity, and limited computational resources. Traditional solvers often suffer from high latency, while emerging learning-based approaches struggle to ensure deterministic feasibility. To bridge the gap from embodied to embedded intelligence, we propose a self-supervised framework incorporating a differentiable hard constraint projection layer for runtime assurance. To mitigate data scarcity, we construct a Global-Guided Artificial Potential Field (G-APF), which provides dense supervision signals without manual labeling. To enforce actuator limitations and geometric constraints efficiently, we employ an adaptive neural projection layer, which iteratively rectifies the coarse network output onto the feasible manifold. Extensive benchmarks on a test set of 20,000 scenarios demonstrate an 88.75\\% success rate, substantiating the enhanced operational safety. Closed-loop experiments in CARLA further validate the physical realizability of the planned paths under dynamic constraints. Furthermore, deployment verification on an NVIDIA Jetson Orin NX confirms an inference latency of 94 ms, showing real-time feasibility on resource-constrained embedded hardware. This framework offers a generalized paradigm for embedding physical laws into neural architectures, providing a viable direction for solving constrained optimization in mechatronics. Source code is available at: this https URL."
    }
  ],
  "北京航空航天大学": [
    {
      "title": "Dynamic Worlds, Dynamic Humans: Generating Virtual Human-Scene Interaction Motion in Dynamic Scenes",
      "url": "https://arxiv.org/abs/2601.19484",
      "date": "2026-01-27",
      "authors_text": "Yin Wang, Zhiying Leng, Haitian Liu, Frederick W. B. Li, Mu Li",
      "is_highlight": false,
      "score": 77.0,
      "summary": "Dyn-HSI introduces a cognitive architecture for virtual humans that adapts to dynamic scenes through perception, memory, and control, outperforming existing interaction generation methods.",
      "teaser_image": "https://arxiv.org/html/2601.19484/x1.png",
      "debug_abstract": "Scenes are continuously undergoing dynamic changes in the real world. However, existing human-scene interaction generation methods typically treat the scene as static, which deviates from reality. Inspired by world models, we introduce Dyn-HSI, the first cognitive architecture for dynamic human-scene interaction, which endows virtual humans with three humanoid components. (1)Vision (human eyes): we equip the virtual human with a Dynamic Scene-Aware Navigation, which continuously perceives changes in the surrounding environment and adaptively predicts the next waypoint. (2)Memory (human brain): we equip the virtual human with a Hierarchical Experience Memory, which stores and updates experiential data accumulated during training. This allows the model to leverage prior knowledge during inference for context-aware motion priming, thereby enhancing both motion quality and generalization. (3) Control (human body): we equip the virtual human with Human-Scene Interaction Diffusion Model, which generates high-fidelity interaction motions conditioned on multimodal inputs. To evaluate performance in dynamic scenes, we extend the existing static human-scene interaction datasets to construct a dynamic benchmark, Dyn-Scenes. We conduct extensive qualitative and quantitative experiments to validate Dyn-HSI, showing that our method consistently outperforms existing approaches and generates high-quality human-scene interaction motions in both static and dynamic settings."
    }
  ],
  "武汉大学": [
    {
      "title": "Fast Converging 3D Gaussian Splatting for 1-Minute Reconstruction",
      "url": "https://arxiv.org/abs/2601.19489",
      "date": "2026-01-27",
      "authors_text": "Ziyu Zhang, Tianle Liu, Diantao Tu, Shuhan Shen",
      "is_highlight": false,
      "score": 72.0,
      "summary": "The paper presents a rapid 3D Gaussian Splatting reconstruction pipeline achieving high-fidelity results within one minute, winning the SIGGRAPH Asia competition with innovative optimization techniques.",
      "teaser_image": "https://arxiv.org/html/2601.19489/assets/load_balance_demo.png",
      "debug_abstract": "We present a fast 3DGS reconstruction pipeline designed to converge within one minute, developed for the SIGGRAPH Asia 3DGS Fast Reconstruction Challenge. The challenge consists of an initial round using SLAM-generated camera poses (with noisy trajectories) and a final round using COLMAP poses (highly accurate). To robustly handle these heterogeneous settings, we develop a two-stage solution. In the first round, we use reverse per-Gaussian parallel optimization and compact forward splatting based on Taming-GS and Speedy-splat, load-balanced tiling, an anchor-based Neural-Gaussian representation enabling rapid convergence with fewer learnable parameters, initialization from monocular depth and partially from feed-forward 3DGS models, and a global pose refinement module for noisy SLAM trajectories. In the final round, the accurate COLMAP poses change the optimization landscape; we disable pose refinement, revert from Neural-Gaussians back to standard 3DGS to eliminate MLP inference overhead, introduce multi-view consistency-guided Gaussian splitting inspired by Fast-GS, and introduce a depth estimator to supervise the rendered depth. Together, these techniques enable high-fidelity reconstruction under a strict one-minute budget. Our method achieved the top performance with a PSNR of 28.43 and ranked first in the competition."
    }
  ],
  "机器人与机械智能实验室 (ROMI Lab)": [
    {
      "title": "Component-Level Lesioning of Language Models Reveals Clinically Aligned Aphasia Phenotypes",
      "url": "https://arxiv.org/abs/2601.19723",
      "date": "2026-01-27",
      "authors_text": "Yifan Wang, Jichen Zheng, Jingyuan Sun, Yunhao Zhang, Chunyu Ye",
      "is_highlight": false,
      "score": 61.0,
      "summary": "This paper presents a framework for simulating aphasia in language models by selectively perturbing components, enabling insights into language production impairments and rehabilitation strategies.",
      "teaser_image": "https://arxiv.org/html/2601.19723/x1.png",
      "debug_abstract": "Large language models (LLMs) increasingly exhibit human-like linguistic behaviors and internal representations that they could serve as computational simulators of language cognition. We ask whether LLMs can be systematically manipulated to reproduce language-production impairments characteristic of aphasia following focal brain lesions. Such models could provide scalable proxies for testing rehabilitation hypotheses, and offer a controlled framework for probing the functional organization of language. We introduce a clinically grounded, component-level framework that simulates aphasia by selectively perturbing functional components in LLMs, and apply it to both modular Mixture-of-Experts models and dense Transformers using a unified intervention interface. Our pipeline (i) identifies subtype-linked components for Broca&#39;s and Wernicke&#39;s aphasia, (ii) interprets these components with linguistic probing tasks, and (iii) induces graded impairments by progressively perturbing the top-k subtype-linked components, evaluating outcomes with Western Aphasia Battery (WAB) subtests summarized by Aphasia Quotient (AQ). Across architectures and lesioning strategies, subtype-targeted perturbations yield more systematic, aphasia-like regressions than size-matched random perturbations, and MoE modularity supports more localized and interpretable phenotype-to-component mappings. These findings suggest that modular LLMs, combined with clinically informed component perturbations, provide a promising platform for simulating aphasic language production and studying how distinct language functions degrade under targeted disruptions."
    },
    {
      "title": "Rhombot: Rhombus-shaped Modular Robots for Stable, Medium-Independent Reconfiguration Motion",
      "url": "https://arxiv.org/abs/2601.19529",
      "date": "2026-01-27",
      "authors_text": "Jie Gu, Yirui Sun, Zhihao Xia, Tin Lun Lam, Chunxu Tian",
      "is_highlight": false,
      "score": 91.0,
      "summary": "Rhombot is a novel rhombus-shaped modular robot enabling stable, medium-independent reconfiguration through morphpivoting, validated by experiments demonstrating its morphing and docking capabilities.",
      "teaser_image": "https://arxiv.org/html/2601.19529/x2.png",
      "debug_abstract": "In this paper, we present Rhombot, a novel deformable planar lattice modular self-reconfigurable robot (MSRR) with a rhombus shaped module. Each module consists of a parallelogram skeleton with a single centrally mounted actuator that enables folding and unfolding along its diagonal. The core design philosophy is to achieve essential MSRR functionalities such as morphing, docking, and locomotion with minimal control complexity. This enables a continuous and stable reconfiguration process that is independent of the surrounding medium, allowing the system to reliably form various configurations in diverse environments. To leverage the unique kinematics of Rhombot, we introduce morphpivoting, a novel motion primitive for reconfiguration that differs from advanced MSRR systems, and propose a strategy for its continuous execution. Finally, a series of physical experiments validate the module&#39;s stable reconfiguration ability, as well as its positional and docking accuracy."
    },
    {
      "title": "Self-Reconfiguration Planning for Deformable Quadrilateral Modular Robots",
      "url": "https://arxiv.org/abs/2601.19496",
      "date": "2026-01-27",
      "authors_text": "Jie Gu, Hongrun Gao, Zhihao Xia, Yirun Sun, Chunxu Tian",
      "is_highlight": false,
      "score": 90.0,
      "summary": "This paper introduces a self-reconfiguration planning algorithm for deformable quadrilateral modular robots that ensures stable connections and efficient execution sequences during reconfiguration.",
      "teaser_image": "https://arxiv.org/html/2601.19496/x2.png",
      "debug_abstract": "For lattice modular self-reconfigurable robots (MSRRs), maintaining stable connections during reconfiguration is crucial for physical feasibility and deployability. This letter presents a novel self-reconfiguration planning algorithm for deformable quadrilateral MSRRs that guarantees stable connection. The method first constructs feasible connect/disconnect actions using a virtual graph representation, and then organizes these actions into a valid execution sequence through a Dependence-based Reverse Tree (DRTree) that resolves interdependencies. We also prove that reconfiguration sequences satisfying motion characteristics exist for any pair of configurations with seven or more modules (excluding linear topologies). Finally, comparisons with a modified BiRRT algorithm highlight the superior efficiency and stability of our approach, while deployment on a physical robotic platform confirms its practical feasibility."
    }
  ],
  "智能人机交互实验室 (MemX)": [
    {
      "title": "Rhombot: Rhombus-shaped Modular Robots for Stable, Medium-Independent Reconfiguration Motion",
      "url": "https://arxiv.org/abs/2601.19529",
      "date": "2026-01-27",
      "authors_text": "Jie Gu, Yirui Sun, Zhihao Xia, Tin Lun Lam, Chunxu Tian",
      "is_highlight": false,
      "score": 91.0,
      "summary": "Rhombot is a novel rhombus-shaped modular robot enabling stable, medium-independent reconfiguration through morphpivoting, validated by experiments demonstrating its morphing and docking capabilities.",
      "teaser_image": "https://arxiv.org/html/2601.19529/x2.png",
      "debug_abstract": "In this paper, we present Rhombot, a novel deformable planar lattice modular self-reconfigurable robot (MSRR) with a rhombus shaped module. Each module consists of a parallelogram skeleton with a single centrally mounted actuator that enables folding and unfolding along its diagonal. The core design philosophy is to achieve essential MSRR functionalities such as morphing, docking, and locomotion with minimal control complexity. This enables a continuous and stable reconfiguration process that is independent of the surrounding medium, allowing the system to reliably form various configurations in diverse environments. To leverage the unique kinematics of Rhombot, we introduce morphpivoting, a novel motion primitive for reconfiguration that differs from advanced MSRR systems, and propose a strategy for its continuous execution. Finally, a series of physical experiments validate the module&#39;s stable reconfiguration ability, as well as its positional and docking accuracy."
    },
    {
      "title": "Self-Reconfiguration Planning for Deformable Quadrilateral Modular Robots",
      "url": "https://arxiv.org/abs/2601.19496",
      "date": "2026-01-27",
      "authors_text": "Jie Gu, Hongrun Gao, Zhihao Xia, Yirun Sun, Chunxu Tian",
      "is_highlight": false,
      "score": 90.0,
      "summary": "This paper introduces a self-reconfiguration planning algorithm for deformable quadrilateral modular robots that ensures stable connections and efficient execution sequences during reconfiguration.",
      "teaser_image": "https://arxiv.org/html/2601.19496/x2.png",
      "debug_abstract": "For lattice modular self-reconfigurable robots (MSRRs), maintaining stable connections during reconfiguration is crucial for physical feasibility and deployability. This letter presents a novel self-reconfiguration planning algorithm for deformable quadrilateral MSRRs that guarantees stable connection. The method first constructs feasible connect/disconnect actions using a virtual graph representation, and then organizes these actions into a valid execution sequence through a Dependence-based Reverse Tree (DRTree) that resolves interdependencies. We also prove that reconfiguration sequences satisfying motion characteristics exist for any pair of configurations with seven or more modules (excluding linear topologies). Finally, comparisons with a modified BiRRT algorithm highlight the superior efficiency and stability of our approach, while deployment on a physical robotic platform confirms its practical feasibility."
    }
  ],
  "智能感知与无人系统实验室": [
    {
      "title": "Rhombot: Rhombus-shaped Modular Robots for Stable, Medium-Independent Reconfiguration Motion",
      "url": "https://arxiv.org/abs/2601.19529",
      "date": "2026-01-27",
      "authors_text": "Jie Gu, Yirui Sun, Zhihao Xia, Tin Lun Lam, Chunxu Tian",
      "is_highlight": false,
      "score": 91.0,
      "summary": "Rhombot is a novel rhombus-shaped modular robot enabling stable, medium-independent reconfiguration through morphpivoting, validated by experiments demonstrating its morphing and docking capabilities.",
      "teaser_image": "https://arxiv.org/html/2601.19529/x2.png",
      "debug_abstract": "In this paper, we present Rhombot, a novel deformable planar lattice modular self-reconfigurable robot (MSRR) with a rhombus shaped module. Each module consists of a parallelogram skeleton with a single centrally mounted actuator that enables folding and unfolding along its diagonal. The core design philosophy is to achieve essential MSRR functionalities such as morphing, docking, and locomotion with minimal control complexity. This enables a continuous and stable reconfiguration process that is independent of the surrounding medium, allowing the system to reliably form various configurations in diverse environments. To leverage the unique kinematics of Rhombot, we introduce morphpivoting, a novel motion primitive for reconfiguration that differs from advanced MSRR systems, and propose a strategy for its continuous execution. Finally, a series of physical experiments validate the module&#39;s stable reconfiguration ability, as well as its positional and docking accuracy."
    },
    {
      "title": "Self-Reconfiguration Planning for Deformable Quadrilateral Modular Robots",
      "url": "https://arxiv.org/abs/2601.19496",
      "date": "2026-01-27",
      "authors_text": "Jie Gu, Hongrun Gao, Zhihao Xia, Yirun Sun, Chunxu Tian",
      "is_highlight": false,
      "score": 90.0,
      "summary": "This paper introduces a self-reconfiguration planning algorithm for deformable quadrilateral modular robots that ensures stable connections and efficient execution sequences during reconfiguration.",
      "teaser_image": "https://arxiv.org/html/2601.19496/x2.png",
      "debug_abstract": "For lattice modular self-reconfigurable robots (MSRRs), maintaining stable connections during reconfiguration is crucial for physical feasibility and deployability. This letter presents a novel self-reconfiguration planning algorithm for deformable quadrilateral MSRRs that guarantees stable connection. The method first constructs feasible connect/disconnect actions using a virtual graph representation, and then organizes these actions into a valid execution sequence through a Dependence-based Reverse Tree (DRTree) that resolves interdependencies. We also prove that reconfiguration sequences satisfying motion characteristics exist for any pair of configurations with seven or more modules (excluding linear topologies). Finally, comparisons with a modified BiRRT algorithm highlight the superior efficiency and stability of our approach, while deployment on a physical robotic platform confirms its practical feasibility."
    }
  ],
  "集群机器人系统实验室 (MagicLab)": [
    {
      "title": "Rhombot: Rhombus-shaped Modular Robots for Stable, Medium-Independent Reconfiguration Motion",
      "url": "https://arxiv.org/abs/2601.19529",
      "date": "2026-01-27",
      "authors_text": "Jie Gu, Yirui Sun, Zhihao Xia, Tin Lun Lam, Chunxu Tian",
      "is_highlight": false,
      "score": 91.0,
      "summary": "Rhombot is a novel rhombus-shaped modular robot enabling stable, medium-independent reconfiguration through morphpivoting, validated by experiments demonstrating its morphing and docking capabilities.",
      "teaser_image": "https://arxiv.org/html/2601.19529/x2.png",
      "debug_abstract": "In this paper, we present Rhombot, a novel deformable planar lattice modular self-reconfigurable robot (MSRR) with a rhombus shaped module. Each module consists of a parallelogram skeleton with a single centrally mounted actuator that enables folding and unfolding along its diagonal. The core design philosophy is to achieve essential MSRR functionalities such as morphing, docking, and locomotion with minimal control complexity. This enables a continuous and stable reconfiguration process that is independent of the surrounding medium, allowing the system to reliably form various configurations in diverse environments. To leverage the unique kinematics of Rhombot, we introduce morphpivoting, a novel motion primitive for reconfiguration that differs from advanced MSRR systems, and propose a strategy for its continuous execution. Finally, a series of physical experiments validate the module&#39;s stable reconfiguration ability, as well as its positional and docking accuracy."
    },
    {
      "title": "Self-Reconfiguration Planning for Deformable Quadrilateral Modular Robots",
      "url": "https://arxiv.org/abs/2601.19496",
      "date": "2026-01-27",
      "authors_text": "Jie Gu, Hongrun Gao, Zhihao Xia, Yirun Sun, Chunxu Tian",
      "is_highlight": false,
      "score": 90.0,
      "summary": "This paper introduces a self-reconfiguration planning algorithm for deformable quadrilateral modular robots that ensures stable connections and efficient execution sequences during reconfiguration.",
      "teaser_image": "https://arxiv.org/html/2601.19496/x2.png",
      "debug_abstract": "For lattice modular self-reconfigurable robots (MSRRs), maintaining stable connections during reconfiguration is crucial for physical feasibility and deployability. This letter presents a novel self-reconfiguration planning algorithm for deformable quadrilateral MSRRs that guarantees stable connection. The method first constructs feasible connect/disconnect actions using a virtual graph representation, and then organizes these actions into a valid execution sequence through a Dependence-based Reverse Tree (DRTree) that resolves interdependencies. We also prove that reconfiguration sequences satisfying motion characteristics exist for any pair of configurations with seven or more modules (excluding linear topologies). Finally, comparisons with a modified BiRRT algorithm highlight the superior efficiency and stability of our approach, while deployment on a physical robotic platform confirms its practical feasibility."
    }
  ],
  "浙江大学": [
    {
      "title": "KeepLoRA: Continual Learning with Residual Gradient Adaptation",
      "url": "https://arxiv.org/abs/2601.19659",
      "date": "2026-01-27",
      "authors_text": "Mao-Lin Luo, Zi-Hao Zhou, Yi-Lin Zhang, Yuanyu Wan, Tong Wei",
      "is_highlight": false,
      "score": 64.0,
      "summary": "KeepLoRA is a novel approach for continual learning in vision-language models that effectively balances knowledge retention and task plasticity by restricting updates to a residual subspace.",
      "teaser_image": "https://arxiv.org/html/2601.19659/x2.png",
      "debug_abstract": "Continual learning for pre-trained vision-language models requires balancing three competing objectives: retaining pre-trained knowledge, preserving knowledge from a sequence of learned tasks, and maintaining the plasticity to acquire new knowledge. This paper presents a simple but effective approach called KeepLoRA to effectively balance these objectives. We first analyze the knowledge retention mechanism within the model parameter space and find that general knowledge is mainly encoded in the principal subspace, while task-specific knowledge is encoded in the residual subspace. Motivated by this finding, KeepLoRA learns new tasks by restricting LoRA parameter updates in the residual subspace to prevent interfering with previously learned capabilities. Specifically, we infuse knowledge for a new task by projecting its gradient onto a subspace orthogonal to both the principal subspace of pre-trained model and the dominant directions of previous task features. Our theoretical and empirical analyses confirm that KeepLoRA balances the three objectives and achieves state-of-the-art performance. The implementation code is available at this https URL."
    },
    {
      "title": "A DVL Aided Loosely Coupled Inertial Navigation Strategy for AUVs with Attitude Error Modeling and Variance Propagation",
      "url": "https://arxiv.org/abs/2601.19509",
      "date": "2026-01-27",
      "authors_text": "Jin Huang, Zichen Liu, Haoda Li, Zhikun Wang, Ying Chen",
      "is_highlight": false,
      "score": 68.0,
      "summary": "This paper presents a novel navigation strategy for AUVs that incorporates attitude error modeling and variance propagation to significantly enhance long-term SINS/DVL navigation accuracy.",
      "teaser_image": "https://arxiv.org/html/2601.19509/x2.png",
      "debug_abstract": "In underwater navigation systems, strap-down inertial navigation system/Doppler velocity log (SINS/DVL)-based loosely coupled architectures are widely adopted. Conventional approaches project DVL velocities from the body coordinate system to the navigation coordinate system using SINS-derived attitude; however, accumulated attitude estimation errors introduce biases into velocity projection and degrade navigation performance during long-term operation. To address this issue, two complementary improvements are introduced. First, a vehicle attitude error-aware DVL velocity transformation model is formulated by incorporating attitude error terms into the observation equation to reduce projection-induced velocity bias. Second, a covariance matrix-based variance propagation method is developed to transform DVL measurement uncertainty across coordinate systems, introducing an expectation-based attitude error compensation term to achieve statistically consistent noise modeling. Simulation and field experiment results demonstrate that both improvements individually enhance navigation accuracy and confirm that accumulated attitude errors affect both projected velocity measurements and their associated uncertainty. When jointly applied, long-term error divergence is effectively suppressed. Field experimental results show that the proposed approach achieves a 78.3% improvement in 3D position RMSE and a 71.8% reduction in the maximum component-wise position error compared with the baseline IMU+DVL method, providing a robust solution for improving long-term SINS/DVL navigation performance."
    }
  ],
  "Multimedia Lab (MMLab)": [
    {
      "title": "EgoHandICL: Egocentric 3D Hand Reconstruction with In-Context Learning",
      "url": "https://arxiv.org/abs/2601.19850",
      "date": "2026-01-27",
      "authors_text": "Binzhu Xie, Shi Qiu, Sicheng Zhang, Yinqiao Wang, Hao Xu",
      "is_highlight": false,
      "score": 76.0,
      "summary": "EgoHandICL introduces an innovative in-context learning framework for robust 3D hand reconstruction in egocentric vision, enhancing performance through multimodal context and exemplar retrieval.",
      "teaser_image": "https://arxiv.org/html/2601.19850/x2.png",
      "debug_abstract": "Robust 3D hand reconstruction in egocentric vision is challenging due to depth ambiguity, self-occlusion, and complex hand-object interactions. Prior methods mitigate these issues by scaling training data or adding auxiliary cues, but they often struggle in unseen contexts. We present EgoHandICL, the first in-context learning (ICL) framework for 3D hand reconstruction that improves semantic alignment, visual consistency, and robustness under challenging egocentric conditions. EgoHandICL introduces complementary exemplar retrieval guided by vision-language models (VLMs), an ICL-tailored tokenizer for multimodal context, and a masked autoencoder (MAE)-based architecture trained with hand-guided geometric and perceptual objectives. Experiments on ARCTIC and EgoExo4D show consistent gains over state-of-the-art methods. We also demonstrate real-world generalization and improve EgoVLM hand-object interaction reasoning by using reconstructed hands as visual prompts. Code and data: this https URL"
    },
    {
      "title": "Rhombot: Rhombus-shaped Modular Robots for Stable, Medium-Independent Reconfiguration Motion",
      "url": "https://arxiv.org/abs/2601.19529",
      "date": "2026-01-27",
      "authors_text": "Jie Gu, Yirui Sun, Zhihao Xia, Tin Lun Lam, Chunxu Tian",
      "is_highlight": false,
      "score": 91.0,
      "summary": "Rhombot is a novel rhombus-shaped modular robot enabling stable, medium-independent reconfiguration through morphpivoting, validated by experiments demonstrating its morphing and docking capabilities.",
      "teaser_image": "https://arxiv.org/html/2601.19529/x2.png",
      "debug_abstract": "In this paper, we present Rhombot, a novel deformable planar lattice modular self-reconfigurable robot (MSRR) with a rhombus shaped module. Each module consists of a parallelogram skeleton with a single centrally mounted actuator that enables folding and unfolding along its diagonal. The core design philosophy is to achieve essential MSRR functionalities such as morphing, docking, and locomotion with minimal control complexity. This enables a continuous and stable reconfiguration process that is independent of the surrounding medium, allowing the system to reliably form various configurations in diverse environments. To leverage the unique kinematics of Rhombot, we introduce morphpivoting, a novel motion primitive for reconfiguration that differs from advanced MSRR systems, and propose a strategy for its continuous execution. Finally, a series of physical experiments validate the module&#39;s stable reconfiguration ability, as well as its positional and docking accuracy."
    }
  ],
  "机器人与人工智能实验室 (RAIL)": [
    {
      "title": "EgoHandICL: Egocentric 3D Hand Reconstruction with In-Context Learning",
      "url": "https://arxiv.org/abs/2601.19850",
      "date": "2026-01-27",
      "authors_text": "Binzhu Xie, Shi Qiu, Sicheng Zhang, Yinqiao Wang, Hao Xu",
      "is_highlight": false,
      "score": 76.0,
      "summary": "EgoHandICL introduces an innovative in-context learning framework for robust 3D hand reconstruction in egocentric vision, enhancing performance through multimodal context and exemplar retrieval.",
      "teaser_image": "https://arxiv.org/html/2601.19850/x2.png",
      "debug_abstract": "Robust 3D hand reconstruction in egocentric vision is challenging due to depth ambiguity, self-occlusion, and complex hand-object interactions. Prior methods mitigate these issues by scaling training data or adding auxiliary cues, but they often struggle in unseen contexts. We present EgoHandICL, the first in-context learning (ICL) framework for 3D hand reconstruction that improves semantic alignment, visual consistency, and robustness under challenging egocentric conditions. EgoHandICL introduces complementary exemplar retrieval guided by vision-language models (VLMs), an ICL-tailored tokenizer for multimodal context, and a masked autoencoder (MAE)-based architecture trained with hand-guided geometric and perceptual objectives. Experiments on ARCTIC and EgoExo4D show consistent gains over state-of-the-art methods. We also demonstrate real-world generalization and improve EgoVLM hand-object interaction reasoning by using reconstructed hands as visual prompts. Code and data: this https URL"
    },
    {
      "title": "AC^2-VLA: Action-Context-Aware Adaptive Computation in Vision-Language-Action Models for Efficient Robotic Manipulation",
      "url": "https://arxiv.org/abs/2601.19634",
      "date": "2026-01-27",
      "authors_text": "Wenda Yu, Tianshi Wang, Fengling Li, Jingjing Li, Lei Zhu",
      "is_highlight": false,
      "score": 92.0,
      "summary": "AC^2-VLA enhances robotic manipulation efficiency by adaptively optimizing computation based on action context, achieving significant speedup and reduced resource usage without sacrificing performance.",
      "teaser_image": "https://arxiv.org/html/2601.19634/x2.png",
      "debug_abstract": "Vision-Language-Action (VLA) models have demonstrated strong performance in robotic manipulation, yet their closed-loop deployment is hindered by the high latency and compute cost of repeatedly running large vision-language backbones at every timestep. We observe that VLA inference exhibits structured redundancies across temporal, spatial, and depth dimensions, and that most existing efficiency methods ignore action context, despite its central role in embodied tasks. To address this gap, we propose Action-Context-aware Adaptive Computation for VLA models (AC^2-VLA), a unified framework that conditions computation on current visual observations, language instructions, and previous action states. Based on this action-centric context, AC^2-VLA adaptively performs cognition reuse across timesteps, token pruning, and selective execution of model components within a unified mechanism. To train the adaptive policy, we introduce an action-guided self-distillation scheme that preserves the behavior of the dense VLA policy while enabling structured sparsification that transfers across tasks and settings. Extensive experiments on robotic manipulation benchmarks show that AC^2-VLA achieves up to a 1.79\\times speedup while reducing FLOPs to 29.4% of the dense baseline, with comparable task success."
    },
    {
      "title": "Rhombot: Rhombus-shaped Modular Robots for Stable, Medium-Independent Reconfiguration Motion",
      "url": "https://arxiv.org/abs/2601.19529",
      "date": "2026-01-27",
      "authors_text": "Jie Gu, Yirui Sun, Zhihao Xia, Tin Lun Lam, Chunxu Tian",
      "is_highlight": false,
      "score": 91.0,
      "summary": "Rhombot is a novel rhombus-shaped modular robot enabling stable, medium-independent reconfiguration through morphpivoting, validated by experiments demonstrating its morphing and docking capabilities.",
      "teaser_image": "https://arxiv.org/html/2601.19529/x2.png",
      "debug_abstract": "In this paper, we present Rhombot, a novel deformable planar lattice modular self-reconfigurable robot (MSRR) with a rhombus shaped module. Each module consists of a parallelogram skeleton with a single centrally mounted actuator that enables folding and unfolding along its diagonal. The core design philosophy is to achieve essential MSRR functionalities such as morphing, docking, and locomotion with minimal control complexity. This enables a continuous and stable reconfiguration process that is independent of the surrounding medium, allowing the system to reliably form various configurations in diverse environments. To leverage the unique kinematics of Rhombot, we introduce morphpivoting, a novel motion primitive for reconfiguration that differs from advanced MSRR systems, and propose a strategy for its continuous execution. Finally, a series of physical experiments validate the module&#39;s stable reconfiguration ability, as well as its positional and docking accuracy."
    }
  ],
  "深圳市人工智能与机器人研究院 (AIRS)": [
    {
      "title": "EgoHandICL: Egocentric 3D Hand Reconstruction with In-Context Learning",
      "url": "https://arxiv.org/abs/2601.19850",
      "date": "2026-01-27",
      "authors_text": "Binzhu Xie, Shi Qiu, Sicheng Zhang, Yinqiao Wang, Hao Xu",
      "is_highlight": false,
      "score": 76.0,
      "summary": "EgoHandICL introduces an innovative in-context learning framework for robust 3D hand reconstruction in egocentric vision, enhancing performance through multimodal context and exemplar retrieval.",
      "teaser_image": "https://arxiv.org/html/2601.19850/x2.png",
      "debug_abstract": "Robust 3D hand reconstruction in egocentric vision is challenging due to depth ambiguity, self-occlusion, and complex hand-object interactions. Prior methods mitigate these issues by scaling training data or adding auxiliary cues, but they often struggle in unseen contexts. We present EgoHandICL, the first in-context learning (ICL) framework for 3D hand reconstruction that improves semantic alignment, visual consistency, and robustness under challenging egocentric conditions. EgoHandICL introduces complementary exemplar retrieval guided by vision-language models (VLMs), an ICL-tailored tokenizer for multimodal context, and a masked autoencoder (MAE)-based architecture trained with hand-guided geometric and perceptual objectives. Experiments on ARCTIC and EgoExo4D show consistent gains over state-of-the-art methods. We also demonstrate real-world generalization and improve EgoVLM hand-object interaction reasoning by using reconstructed hands as visual prompts. Code and data: this https URL"
    },
    {
      "title": "Rhombot: Rhombus-shaped Modular Robots for Stable, Medium-Independent Reconfiguration Motion",
      "url": "https://arxiv.org/abs/2601.19529",
      "date": "2026-01-27",
      "authors_text": "Jie Gu, Yirui Sun, Zhihao Xia, Tin Lun Lam, Chunxu Tian",
      "is_highlight": false,
      "score": 91.0,
      "summary": "Rhombot is a novel rhombus-shaped modular robot enabling stable, medium-independent reconfiguration through morphpivoting, validated by experiments demonstrating its morphing and docking capabilities.",
      "teaser_image": "https://arxiv.org/html/2601.19529/x2.png",
      "debug_abstract": "In this paper, we present Rhombot, a novel deformable planar lattice modular self-reconfigurable robot (MSRR) with a rhombus shaped module. Each module consists of a parallelogram skeleton with a single centrally mounted actuator that enables folding and unfolding along its diagonal. The core design philosophy is to achieve essential MSRR functionalities such as morphing, docking, and locomotion with minimal control complexity. This enables a continuous and stable reconfiguration process that is independent of the surrounding medium, allowing the system to reliably form various configurations in diverse environments. To leverage the unique kinematics of Rhombot, we introduce morphpivoting, a novel motion primitive for reconfiguration that differs from advanced MSRR systems, and propose a strategy for its continuous execution. Finally, a series of physical experiments validate the module&#39;s stable reconfiguration ability, as well as its positional and docking accuracy."
    }
  ],
  "机器人与人工智能实验室": [
    {
      "title": "EgoHandICL: Egocentric 3D Hand Reconstruction with In-Context Learning",
      "url": "https://arxiv.org/abs/2601.19850",
      "date": "2026-01-27",
      "authors_text": "Binzhu Xie, Shi Qiu, Sicheng Zhang, Yinqiao Wang, Hao Xu",
      "is_highlight": false,
      "score": 76.0,
      "summary": "EgoHandICL introduces an innovative in-context learning framework for robust 3D hand reconstruction in egocentric vision, enhancing performance through multimodal context and exemplar retrieval.",
      "teaser_image": "https://arxiv.org/html/2601.19850/x2.png",
      "debug_abstract": "Robust 3D hand reconstruction in egocentric vision is challenging due to depth ambiguity, self-occlusion, and complex hand-object interactions. Prior methods mitigate these issues by scaling training data or adding auxiliary cues, but they often struggle in unseen contexts. We present EgoHandICL, the first in-context learning (ICL) framework for 3D hand reconstruction that improves semantic alignment, visual consistency, and robustness under challenging egocentric conditions. EgoHandICL introduces complementary exemplar retrieval guided by vision-language models (VLMs), an ICL-tailored tokenizer for multimodal context, and a masked autoencoder (MAE)-based architecture trained with hand-guided geometric and perceptual objectives. Experiments on ARCTIC and EgoExo4D show consistent gains over state-of-the-art methods. We also demonstrate real-world generalization and improve EgoVLM hand-object interaction reasoning by using reconstructed hands as visual prompts. Code and data: this https URL"
    },
    {
      "title": "Rhombot: Rhombus-shaped Modular Robots for Stable, Medium-Independent Reconfiguration Motion",
      "url": "https://arxiv.org/abs/2601.19529",
      "date": "2026-01-27",
      "authors_text": "Jie Gu, Yirui Sun, Zhihao Xia, Tin Lun Lam, Chunxu Tian",
      "is_highlight": false,
      "score": 91.0,
      "summary": "Rhombot is a novel rhombus-shaped modular robot enabling stable, medium-independent reconfiguration through morphpivoting, validated by experiments demonstrating its morphing and docking capabilities.",
      "teaser_image": "https://arxiv.org/html/2601.19529/x2.png",
      "debug_abstract": "In this paper, we present Rhombot, a novel deformable planar lattice modular self-reconfigurable robot (MSRR) with a rhombus shaped module. Each module consists of a parallelogram skeleton with a single centrally mounted actuator that enables folding and unfolding along its diagonal. The core design philosophy is to achieve essential MSRR functionalities such as morphing, docking, and locomotion with minimal control complexity. This enables a continuous and stable reconfiguration process that is independent of the surrounding medium, allowing the system to reliably form various configurations in diverse environments. To leverage the unique kinematics of Rhombot, we introduce morphpivoting, a novel motion primitive for reconfiguration that differs from advanced MSRR systems, and propose a strategy for its continuous execution. Finally, a series of physical experiments validate the module&#39;s stable reconfiguration ability, as well as its positional and docking accuracy."
    }
  ],
  "机器人与自动化研究中心": [
    {
      "title": "EgoHandICL: Egocentric 3D Hand Reconstruction with In-Context Learning",
      "url": "https://arxiv.org/abs/2601.19850",
      "date": "2026-01-27",
      "authors_text": "Binzhu Xie, Shi Qiu, Sicheng Zhang, Yinqiao Wang, Hao Xu",
      "is_highlight": false,
      "score": 76.0,
      "summary": "EgoHandICL introduces an innovative in-context learning framework for robust 3D hand reconstruction in egocentric vision, enhancing performance through multimodal context and exemplar retrieval.",
      "teaser_image": "https://arxiv.org/html/2601.19850/x2.png",
      "debug_abstract": "Robust 3D hand reconstruction in egocentric vision is challenging due to depth ambiguity, self-occlusion, and complex hand-object interactions. Prior methods mitigate these issues by scaling training data or adding auxiliary cues, but they often struggle in unseen contexts. We present EgoHandICL, the first in-context learning (ICL) framework for 3D hand reconstruction that improves semantic alignment, visual consistency, and robustness under challenging egocentric conditions. EgoHandICL introduces complementary exemplar retrieval guided by vision-language models (VLMs), an ICL-tailored tokenizer for multimodal context, and a masked autoencoder (MAE)-based architecture trained with hand-guided geometric and perceptual objectives. Experiments on ARCTIC and EgoExo4D show consistent gains over state-of-the-art methods. We also demonstrate real-world generalization and improve EgoVLM hand-object interaction reasoning by using reconstructed hands as visual prompts. Code and data: this https URL"
    },
    {
      "title": "Component-Level Lesioning of Language Models Reveals Clinically Aligned Aphasia Phenotypes",
      "url": "https://arxiv.org/abs/2601.19723",
      "date": "2026-01-27",
      "authors_text": "Yifan Wang, Jichen Zheng, Jingyuan Sun, Yunhao Zhang, Chunyu Ye",
      "is_highlight": false,
      "score": 61.0,
      "summary": "This paper presents a framework for simulating aphasia in language models by selectively perturbing components, enabling insights into language production impairments and rehabilitation strategies.",
      "teaser_image": "https://arxiv.org/html/2601.19723/x1.png",
      "debug_abstract": "Large language models (LLMs) increasingly exhibit human-like linguistic behaviors and internal representations that they could serve as computational simulators of language cognition. We ask whether LLMs can be systematically manipulated to reproduce language-production impairments characteristic of aphasia following focal brain lesions. Such models could provide scalable proxies for testing rehabilitation hypotheses, and offer a controlled framework for probing the functional organization of language. We introduce a clinically grounded, component-level framework that simulates aphasia by selectively perturbing functional components in LLMs, and apply it to both modular Mixture-of-Experts models and dense Transformers using a unified intervention interface. Our pipeline (i) identifies subtype-linked components for Broca&#39;s and Wernicke&#39;s aphasia, (ii) interprets these components with linguistic probing tasks, and (iii) induces graded impairments by progressively perturbing the top-k subtype-linked components, evaluating outcomes with Western Aphasia Battery (WAB) subtests summarized by Aphasia Quotient (AQ). Across architectures and lesioning strategies, subtype-targeted perturbations yield more systematic, aphasia-like regressions than size-matched random perturbations, and MoE modularity supports more localized and interpretable phenotype-to-component mappings. These findings suggest that modular LLMs, combined with clinically informed component perturbations, provide a promising platform for simulating aphasic language production and studying how distinct language functions degrade under targeted disruptions."
    },
    {
      "title": "Rhombot: Rhombus-shaped Modular Robots for Stable, Medium-Independent Reconfiguration Motion",
      "url": "https://arxiv.org/abs/2601.19529",
      "date": "2026-01-27",
      "authors_text": "Jie Gu, Yirui Sun, Zhihao Xia, Tin Lun Lam, Chunxu Tian",
      "is_highlight": false,
      "score": 91.0,
      "summary": "Rhombot is a novel rhombus-shaped modular robot enabling stable, medium-independent reconfiguration through morphpivoting, validated by experiments demonstrating its morphing and docking capabilities.",
      "teaser_image": "https://arxiv.org/html/2601.19529/x2.png",
      "debug_abstract": "In this paper, we present Rhombot, a novel deformable planar lattice modular self-reconfigurable robot (MSRR) with a rhombus shaped module. Each module consists of a parallelogram skeleton with a single centrally mounted actuator that enables folding and unfolding along its diagonal. The core design philosophy is to achieve essential MSRR functionalities such as morphing, docking, and locomotion with minimal control complexity. This enables a continuous and stable reconfiguration process that is independent of the surrounding medium, allowing the system to reliably form various configurations in diverse environments. To leverage the unique kinematics of Rhombot, we introduce morphpivoting, a novel motion primitive for reconfiguration that differs from advanced MSRR systems, and propose a strategy for its continuous execution. Finally, a series of physical experiments validate the module&#39;s stable reconfiguration ability, as well as its positional and docking accuracy."
    }
  ],
  "中国人民大学": [
    {
      "title": "AtomMem : Learnable Dynamic Agentic Memory with Atomic Memory Operation",
      "url": "https://arxiv.org/abs/2601.08323",
      "date": "2026-01-27",
      "authors_text": "Yupeng Huo, Yaxi Lu, Zhong Zhang, Haotian Chen, Yankai Lin",
      "is_highlight": false,
      "score": 84.0,
      "summary": "AtomMem introduces a learnable dynamic memory framework that optimizes memory management through atomic CRUD operations, outperforming static methods in long-context tasks.",
      "teaser_image": "https://arxiv.org/html/2601.08323/x2.png",
      "debug_abstract": "Equipping agents with memory is essential for solving real-world long-horizon problems. However, most existing agent memory mechanisms rely on static and hand-crafted workflows. This limits the performance and generalization ability of these memory designs, which highlights the need for a more flexible, learning-based memory framework. In this paper, we propose AtomMem, which reframes memory management as a dynamic decision-making problem. We deconstruct high-level memory processes into fundamental atomic CRUD (Create, Read, Update, Delete) operations, transforming the memory workflow into a learnable decision process. By combining supervised fine-tuning with reinforcement learning, AtomMem learns an autonomous, task-aligned policy to orchestrate memory behaviors tailored to specific task demands. Experimental results across 3 long-context benchmarks demonstrate that the trained AtomMem-8B consistently outperforms prior static-workflow memory methods. Further analysis of training dynamics shows that our learning-based formulation enables the agent to discover structured, task-aligned memory management strategies, highlighting a key advantage over predefined routines."
    }
  ],
  "Sensing, Interaction & Perception Lab (SIPLAB)": [
    {
      "title": "DeFM: Learning Foundation Representations from Depth for Robotics",
      "url": "https://arxiv.org/abs/2601.18923",
      "date": "2026-01-26",
      "authors_text": "Manthan Patel, Jonas Frey, Mayank Mittal, Fan Yang, Alexander Hansson",
      "is_highlight": true,
      "score": 94.0,
      "summary": "DeFM is a self-supervised foundation model that learns robust geometric and semantic representations from depth images, achieving state-of-the-art performance in various robotic tasks and environments.",
      "teaser_image": "https://arxiv.org/html/2601.18923/x1.png",
      "debug_abstract": "Depth sensors are widely deployed across robotic platforms, and advances in fast, high-fidelity depth simulation have enabled robotic policies trained on depth observations to achieve robust sim-to-real transfer for a wide range of tasks. Despite this, representation learning for depth modality remains underexplored compared to RGB, where large-scale foundation models now define the state of the art. To address this gap, we present DeFM, a self-supervised foundation model trained entirely on depth images for robotic applications. Using a DINO-style self-distillation objective on a curated dataset of 60M depth images, DeFM learns geometric and semantic representations that generalize to diverse environments, tasks, and sensors. To retain metric awareness across multiple scales, we introduce a novel input normalization strategy. We further distill DeFM into compact models suitable for resource-constrained robotic systems. When evaluated on depth-based classification, segmentation, navigation, locomotion, and manipulation benchmarks, DeFM achieves state-of-the-art performance and demonstrates strong generalization from simulation to real-world environments. We release all our pretrained models, which can be adopted off-the-shelf for depth-based robotic learning without task-specific fine-tuning. Webpage: this https URL"
    },
    {
      "title": "Safe Exploration via Policy Priors",
      "url": "https://arxiv.org/abs/2601.19612",
      "date": "2026-01-27",
      "authors_text": "Manuel Wendl, Yarden As, Manish Prajapat, Anton Pollak, Stelian Coros",
      "is_highlight": false,
      "score": 86.0,
      "summary": "The paper presents SOOPER, a safe exploration method for reinforcement learning that utilizes conservative policy priors, ensuring safety and optimal policy convergence through probabilistic dynamics models.",
      "teaser_image": "https://arxiv.org/html/2601.19612/x2.png",
      "debug_abstract": "Safe exploration is a key requirement for reinforcement learning (RL) agents to learn and adapt online, beyond controlled (e.g. simulated) environments. In this work, we tackle this challenge by utilizing suboptimal yet conservative policies (e.g., obtained from offline data or simulators) as priors. Our approach, SOOPER, uses probabilistic dynamics models to optimistically explore, yet pessimistically fall back to the conservative policy prior if needed. We prove that SOOPER guarantees safety throughout learning, and establish convergence to an optimal policy by bounding its cumulative regret. Extensive experiments on key safe RL benchmarks and real-world hardware demonstrate that SOOPER is scalable, outperforms the state-of-the-art and validate our theoretical guarantees in practice."
    }
  ],
  "机器人系统实验室 (RSL)": [
    {
      "title": "DeFM: Learning Foundation Representations from Depth for Robotics",
      "url": "https://arxiv.org/abs/2601.18923",
      "date": "2026-01-26",
      "authors_text": "Manthan Patel, Jonas Frey, Mayank Mittal, Fan Yang, Alexander Hansson",
      "is_highlight": true,
      "score": 94.0,
      "summary": "DeFM is a self-supervised foundation model that learns robust geometric and semantic representations from depth images, achieving state-of-the-art performance in various robotic tasks and environments.",
      "teaser_image": "https://arxiv.org/html/2601.18923/x1.png",
      "debug_abstract": "Depth sensors are widely deployed across robotic platforms, and advances in fast, high-fidelity depth simulation have enabled robotic policies trained on depth observations to achieve robust sim-to-real transfer for a wide range of tasks. Despite this, representation learning for depth modality remains underexplored compared to RGB, where large-scale foundation models now define the state of the art. To address this gap, we present DeFM, a self-supervised foundation model trained entirely on depth images for robotic applications. Using a DINO-style self-distillation objective on a curated dataset of 60M depth images, DeFM learns geometric and semantic representations that generalize to diverse environments, tasks, and sensors. To retain metric awareness across multiple scales, we introduce a novel input normalization strategy. We further distill DeFM into compact models suitable for resource-constrained robotic systems. When evaluated on depth-based classification, segmentation, navigation, locomotion, and manipulation benchmarks, DeFM achieves state-of-the-art performance and demonstrates strong generalization from simulation to real-world environments. We release all our pretrained models, which can be adopted off-the-shelf for depth-based robotic learning without task-specific fine-tuning. Webpage: this https URL"
    },
    {
      "title": "Safe Exploration via Policy Priors",
      "url": "https://arxiv.org/abs/2601.19612",
      "date": "2026-01-27",
      "authors_text": "Manuel Wendl, Yarden As, Manish Prajapat, Anton Pollak, Stelian Coros",
      "is_highlight": false,
      "score": 86.0,
      "summary": "The paper presents SOOPER, a safe exploration method for reinforcement learning that utilizes conservative policy priors, ensuring safety and optimal policy convergence through probabilistic dynamics models.",
      "teaser_image": "https://arxiv.org/html/2601.19612/x2.png",
      "debug_abstract": "Safe exploration is a key requirement for reinforcement learning (RL) agents to learn and adapt online, beyond controlled (e.g. simulated) environments. In this work, we tackle this challenge by utilizing suboptimal yet conservative policies (e.g., obtained from offline data or simulators) as priors. Our approach, SOOPER, uses probabilistic dynamics models to optimistically explore, yet pessimistically fall back to the conservative policy prior if needed. We prove that SOOPER guarantees safety throughout learning, and establish convergence to an optimal policy by bounding its cumulative regret. Extensive experiments on key safe RL benchmarks and real-world hardware demonstrate that SOOPER is scalable, outperforms the state-of-the-art and validate our theoretical guarantees in practice."
    }
  ],
  "电子科技大学": [
    {
      "title": "AC^2-VLA: Action-Context-Aware Adaptive Computation in Vision-Language-Action Models for Efficient Robotic Manipulation",
      "url": "https://arxiv.org/abs/2601.19634",
      "date": "2026-01-27",
      "authors_text": "Wenda Yu, Tianshi Wang, Fengling Li, Jingjing Li, Lei Zhu",
      "is_highlight": false,
      "score": 92.0,
      "summary": "AC^2-VLA enhances robotic manipulation efficiency by adaptively optimizing computation based on action context, achieving significant speedup and reduced resource usage without sacrificing performance.",
      "teaser_image": "https://arxiv.org/html/2601.19634/x2.png",
      "debug_abstract": "Vision-Language-Action (VLA) models have demonstrated strong performance in robotic manipulation, yet their closed-loop deployment is hindered by the high latency and compute cost of repeatedly running large vision-language backbones at every timestep. We observe that VLA inference exhibits structured redundancies across temporal, spatial, and depth dimensions, and that most existing efficiency methods ignore action context, despite its central role in embodied tasks. To address this gap, we propose Action-Context-aware Adaptive Computation for VLA models (AC^2-VLA), a unified framework that conditions computation on current visual observations, language instructions, and previous action states. Based on this action-centric context, AC^2-VLA adaptively performs cognition reuse across timesteps, token pruning, and selective execution of model components within a unified mechanism. To train the adaptive policy, we introduce an action-guided self-distillation scheme that preserves the behavior of the dense VLA policy while enabling structured sparsification that transfers across tasks and settings. Extensive experiments on robotic manipulation benchmarks show that AC^2-VLA achieves up to a 1.79\\times speedup while reducing FLOPs to 29.4% of the dense baseline, with comparable task success."
    },
    {
      "title": "R^3: Replay, Reflection, and Ranking Rewards for LLM Reinforcement Learning",
      "url": "https://arxiv.org/abs/2601.19620",
      "date": "2026-01-27",
      "authors_text": "Zhizheng Jiang, Kang Zhao, Weikai Xu, Xinkui Lin, Wei Liu",
      "is_highlight": false,
      "score": 62.0,
      "summary": "The R^3 framework enhances reinforcement learning for large reasoning models by integrating cross-context replay, in-context self-reflection, and structural entropy ranking rewards, achieving state-of-the-art performance in math tasks.",
      "teaser_image": "https://arxiv.org/html/2601.19620/x1.png",
      "debug_abstract": "Large reasoning models (LRMs) aim to solve diverse and complex problems through structured reasoning. Recent advances in group-based policy optimization methods have shown promise in enabling stable advantage estimation without reliance on process-level annotations. However, these methods rely on advantage gaps induced by high-quality samples within the same batch, which makes the training process fragile and inefficient when intra-group advantages collapse under challenging tasks. To address these problems, we propose a reinforcement learning mechanism named \\emph{\\textbf{R^3}} that along three directions: (1) a \\emph{cross-context \\underline{\\textbf{R}}eplay} strategy that maintains the intra-group advantage by recalling valuable examples from historical trajectories of the same query, (2) an \\emph{in-context self-\\underline{\\textbf{R}}eflection} mechanism enabling models to refine outputs by leveraging past failures, and (3) a \\emph{structural entropy \\underline{\\textbf{R}}anking reward}, which assigns relative rewards to truncated or failed samples by ranking responses based on token-level entropy patterns, capturing both local exploration and global stability. We implement our method on Deepseek-R1-Distill-Qwen-1.5B and train it on the DeepscaleR-40k in the math domain. Experiments demonstrate our method achieves SoTA performance on several math benchmarks, representing significant improvements and fewer reasoning tokens over the base models. Code and model will be released."
    }
  ],
  "MReaLLab": [
    {
      "title": "RoamScene3D: Immersive Text-to-3D Scene Generation via Adaptive Object-aware Roaming",
      "url": "https://arxiv.org/abs/2601.19433",
      "date": "2026-01-27",
      "authors_text": "Jisheng Chu, Wenrui Li, Rui Zhao, Wangmeng Zuo, Shifeng Chen",
      "is_highlight": false,
      "score": 63.0,
      "summary": "RoamScene3D introduces a framework for generating immersive 3D scenes from text by leveraging semantic object relations and adaptive camera trajectories, outperforming existing methods.",
      "teaser_image": "https://arxiv.org/html/2601.19433/x2.png",
      "debug_abstract": "Generating immersive 3D scenes from texts is a core task in computer vision, crucial for applications in virtual reality and game development. Despite the promise of leveraging 2D diffusion priors, existing methods suffer from spatial blindness and rely on predefined trajectories that fail to exploit the inner relationships among salient objects. Consequently, these approaches are unable to comprehend the semantic layout, preventing them from exploring the scene adaptively to infer occluded content. Moreover, current inpainting models operate in 2D image space, struggling to plausibly fill holes caused by camera motion. To address these limitations, we propose RoamScene3D, a novel framework that bridges the gap between semantic guidance and spatial generation. Our method reasons about the semantic relations among objects and produces consistent and photorealistic scenes. Specifically, we employ a vision-language model (VLM) to construct a scene graph that encodes object relations, guiding the camera to perceive salient object boundaries and plan an adaptive roaming trajectory. Furthermore, to mitigate the limitations of static 2D priors, we introduce a Motion-Injected Inpainting model that is fine-tuned on a synthetic panoramic dataset integrating authentic camera trajectories, making it adaptive to camera motion. Extensive experiments demonstrate that with semantic reasoning and geometric constraints, our method significantly outperforms state-of-the-art approaches in producing consistent and photorealistic scenes. Our code is available at this https URL."
    },
    {
      "title": "Video-KTR: Reinforcing Video Reasoning via Key Token Attribution",
      "url": "https://arxiv.org/abs/2601.19686",
      "date": "2026-01-27",
      "authors_text": "Ziyue Wang, Sheng Jin, Zhongrong Zuo, Jiawei Wu, Han Qiu",
      "is_highlight": false,
      "score": 67.0,
      "summary": "Video-KTR enhances video reasoning in multimodal models by selectively reinforcing key tokens through a novel attribution framework, achieving state-of-the-art performance across multiple benchmarks.",
      "teaser_image": "https://arxiv.org/html/2601.19686/x1.png",
      "debug_abstract": "Reinforcement learning (RL) has shown strong potential for enhancing reasoning in multimodal large language models, yet existing video reasoning methods often rely on coarse sequence-level rewards or single-factor token selection, neglecting fine-grained links among visual inputs, temporal dynamics, and linguistic outputs, limiting both accuracy and interpretability. We propose Video-KTR, a modality-aware policy shaping framework that performs selective, token-level RL by combining three attribution signals: (1) visual-aware tokens identified via counterfactual masking to reveal perceptual dependence; (2) temporal-aware tokens detected through frame shuffling to expose temporal sensitivity; and (3) high-entropy tokens signaling predictive uncertainty. By reinforcing only these key tokens, Video-KTR focuses learning on semantically informative, modality-sensitive content while filtering out low-value tokens. Across five challenging benchmarks, Video-KTR achieves state-of-the-art or highly competitive results, achieving 42.7\\% on Video-Holmes (surpassing GPT-4o) with consistent gains on both reasoning and general video understanding tasks. Ablation studies verify the complementary roles of the attribution signals and the robustness of targeted token-level updates. Overall, Video-KTR improves accuracy and interpretability, offering a simple, drop-in extension to RL for complex video reasoning. Our code and models are available at this https URL."
    }
  ],
  "PINE Lab": [
    {
      "title": "RoamScene3D: Immersive Text-to-3D Scene Generation via Adaptive Object-aware Roaming",
      "url": "https://arxiv.org/abs/2601.19433",
      "date": "2026-01-27",
      "authors_text": "Jisheng Chu, Wenrui Li, Rui Zhao, Wangmeng Zuo, Shifeng Chen",
      "is_highlight": false,
      "score": 63.0,
      "summary": "RoamScene3D introduces a framework for generating immersive 3D scenes from text by leveraging semantic object relations and adaptive camera trajectories, outperforming existing methods.",
      "teaser_image": "https://arxiv.org/html/2601.19433/x2.png",
      "debug_abstract": "Generating immersive 3D scenes from texts is a core task in computer vision, crucial for applications in virtual reality and game development. Despite the promise of leveraging 2D diffusion priors, existing methods suffer from spatial blindness and rely on predefined trajectories that fail to exploit the inner relationships among salient objects. Consequently, these approaches are unable to comprehend the semantic layout, preventing them from exploring the scene adaptively to infer occluded content. Moreover, current inpainting models operate in 2D image space, struggling to plausibly fill holes caused by camera motion. To address these limitations, we propose RoamScene3D, a novel framework that bridges the gap between semantic guidance and spatial generation. Our method reasons about the semantic relations among objects and produces consistent and photorealistic scenes. Specifically, we employ a vision-language model (VLM) to construct a scene graph that encodes object relations, guiding the camera to perceive salient object boundaries and plan an adaptive roaming trajectory. Furthermore, to mitigate the limitations of static 2D priors, we introduce a Motion-Injected Inpainting model that is fine-tuned on a synthetic panoramic dataset integrating authentic camera trajectories, making it adaptive to camera motion. Extensive experiments demonstrate that with semantic reasoning and geometric constraints, our method significantly outperforms state-of-the-art approaches in producing consistent and photorealistic scenes. Our code is available at this https URL."
    },
    {
      "title": "Video-KTR: Reinforcing Video Reasoning via Key Token Attribution",
      "url": "https://arxiv.org/abs/2601.19686",
      "date": "2026-01-27",
      "authors_text": "Ziyue Wang, Sheng Jin, Zhongrong Zuo, Jiawei Wu, Han Qiu",
      "is_highlight": false,
      "score": 67.0,
      "summary": "Video-KTR enhances video reasoning in multimodal models by selectively reinforcing key tokens through a novel attribution framework, achieving state-of-the-art performance across multiple benchmarks.",
      "teaser_image": "https://arxiv.org/html/2601.19686/x1.png",
      "debug_abstract": "Reinforcement learning (RL) has shown strong potential for enhancing reasoning in multimodal large language models, yet existing video reasoning methods often rely on coarse sequence-level rewards or single-factor token selection, neglecting fine-grained links among visual inputs, temporal dynamics, and linguistic outputs, limiting both accuracy and interpretability. We propose Video-KTR, a modality-aware policy shaping framework that performs selective, token-level RL by combining three attribution signals: (1) visual-aware tokens identified via counterfactual masking to reveal perceptual dependence; (2) temporal-aware tokens detected through frame shuffling to expose temporal sensitivity; and (3) high-entropy tokens signaling predictive uncertainty. By reinforcing only these key tokens, Video-KTR focuses learning on semantically informative, modality-sensitive content while filtering out low-value tokens. Across five challenging benchmarks, Video-KTR achieves state-of-the-art or highly competitive results, achieving 42.7\\% on Video-Holmes (surpassing GPT-4o) with consistent gains on both reasoning and general video understanding tasks. Ablation studies verify the complementary roles of the attribution signals and the robustness of targeted token-level updates. Overall, Video-KTR improves accuracy and interpretability, offering a simple, drop-in extension to RL for complex video reasoning. Our code and models are available at this https URL."
    }
  ],
  "ROSE Lab": [
    {
      "title": "RoamScene3D: Immersive Text-to-3D Scene Generation via Adaptive Object-aware Roaming",
      "url": "https://arxiv.org/abs/2601.19433",
      "date": "2026-01-27",
      "authors_text": "Jisheng Chu, Wenrui Li, Rui Zhao, Wangmeng Zuo, Shifeng Chen",
      "is_highlight": false,
      "score": 63.0,
      "summary": "RoamScene3D introduces a framework for generating immersive 3D scenes from text by leveraging semantic object relations and adaptive camera trajectories, outperforming existing methods.",
      "teaser_image": "https://arxiv.org/html/2601.19433/x2.png",
      "debug_abstract": "Generating immersive 3D scenes from texts is a core task in computer vision, crucial for applications in virtual reality and game development. Despite the promise of leveraging 2D diffusion priors, existing methods suffer from spatial blindness and rely on predefined trajectories that fail to exploit the inner relationships among salient objects. Consequently, these approaches are unable to comprehend the semantic layout, preventing them from exploring the scene adaptively to infer occluded content. Moreover, current inpainting models operate in 2D image space, struggling to plausibly fill holes caused by camera motion. To address these limitations, we propose RoamScene3D, a novel framework that bridges the gap between semantic guidance and spatial generation. Our method reasons about the semantic relations among objects and produces consistent and photorealistic scenes. Specifically, we employ a vision-language model (VLM) to construct a scene graph that encodes object relations, guiding the camera to perceive salient object boundaries and plan an adaptive roaming trajectory. Furthermore, to mitigate the limitations of static 2D priors, we introduce a Motion-Injected Inpainting model that is fine-tuned on a synthetic panoramic dataset integrating authentic camera trajectories, making it adaptive to camera motion. Extensive experiments demonstrate that with semantic reasoning and geometric constraints, our method significantly outperforms state-of-the-art approaches in producing consistent and photorealistic scenes. Our code is available at this https URL."
    },
    {
      "title": "Video-KTR: Reinforcing Video Reasoning via Key Token Attribution",
      "url": "https://arxiv.org/abs/2601.19686",
      "date": "2026-01-27",
      "authors_text": "Ziyue Wang, Sheng Jin, Zhongrong Zuo, Jiawei Wu, Han Qiu",
      "is_highlight": false,
      "score": 67.0,
      "summary": "Video-KTR enhances video reasoning in multimodal models by selectively reinforcing key tokens through a novel attribution framework, achieving state-of-the-art performance across multiple benchmarks.",
      "teaser_image": "https://arxiv.org/html/2601.19686/x1.png",
      "debug_abstract": "Reinforcement learning (RL) has shown strong potential for enhancing reasoning in multimodal large language models, yet existing video reasoning methods often rely on coarse sequence-level rewards or single-factor token selection, neglecting fine-grained links among visual inputs, temporal dynamics, and linguistic outputs, limiting both accuracy and interpretability. We propose Video-KTR, a modality-aware policy shaping framework that performs selective, token-level RL by combining three attribution signals: (1) visual-aware tokens identified via counterfactual masking to reveal perceptual dependence; (2) temporal-aware tokens detected through frame shuffling to expose temporal sensitivity; and (3) high-entropy tokens signaling predictive uncertainty. By reinforcing only these key tokens, Video-KTR focuses learning on semantically informative, modality-sensitive content while filtering out low-value tokens. Across five challenging benchmarks, Video-KTR achieves state-of-the-art or highly competitive results, achieving 42.7\\% on Video-Holmes (surpassing GPT-4o) with consistent gains on both reasoning and general video understanding tasks. Ablation studies verify the complementary roles of the attribution signals and the robustness of targeted token-level updates. Overall, Video-KTR improves accuracy and interpretability, offering a simple, drop-in extension to RL for complex video reasoning. Our code and models are available at this https URL."
    }
  ],
  "S-Lab for Advanced Intelligence": [
    {
      "title": "RoamScene3D: Immersive Text-to-3D Scene Generation via Adaptive Object-aware Roaming",
      "url": "https://arxiv.org/abs/2601.19433",
      "date": "2026-01-27",
      "authors_text": "Jisheng Chu, Wenrui Li, Rui Zhao, Wangmeng Zuo, Shifeng Chen",
      "is_highlight": false,
      "score": 63.0,
      "summary": "RoamScene3D introduces a framework for generating immersive 3D scenes from text by leveraging semantic object relations and adaptive camera trajectories, outperforming existing methods.",
      "teaser_image": "https://arxiv.org/html/2601.19433/x2.png",
      "debug_abstract": "Generating immersive 3D scenes from texts is a core task in computer vision, crucial for applications in virtual reality and game development. Despite the promise of leveraging 2D diffusion priors, existing methods suffer from spatial blindness and rely on predefined trajectories that fail to exploit the inner relationships among salient objects. Consequently, these approaches are unable to comprehend the semantic layout, preventing them from exploring the scene adaptively to infer occluded content. Moreover, current inpainting models operate in 2D image space, struggling to plausibly fill holes caused by camera motion. To address these limitations, we propose RoamScene3D, a novel framework that bridges the gap between semantic guidance and spatial generation. Our method reasons about the semantic relations among objects and produces consistent and photorealistic scenes. Specifically, we employ a vision-language model (VLM) to construct a scene graph that encodes object relations, guiding the camera to perceive salient object boundaries and plan an adaptive roaming trajectory. Furthermore, to mitigate the limitations of static 2D priors, we introduce a Motion-Injected Inpainting model that is fine-tuned on a synthetic panoramic dataset integrating authentic camera trajectories, making it adaptive to camera motion. Extensive experiments demonstrate that with semantic reasoning and geometric constraints, our method significantly outperforms state-of-the-art approaches in producing consistent and photorealistic scenes. Our code is available at this https URL."
    },
    {
      "title": "Video-KTR: Reinforcing Video Reasoning via Key Token Attribution",
      "url": "https://arxiv.org/abs/2601.19686",
      "date": "2026-01-27",
      "authors_text": "Ziyue Wang, Sheng Jin, Zhongrong Zuo, Jiawei Wu, Han Qiu",
      "is_highlight": false,
      "score": 67.0,
      "summary": "Video-KTR enhances video reasoning in multimodal models by selectively reinforcing key tokens through a novel attribution framework, achieving state-of-the-art performance across multiple benchmarks.",
      "teaser_image": "https://arxiv.org/html/2601.19686/x1.png",
      "debug_abstract": "Reinforcement learning (RL) has shown strong potential for enhancing reasoning in multimodal large language models, yet existing video reasoning methods often rely on coarse sequence-level rewards or single-factor token selection, neglecting fine-grained links among visual inputs, temporal dynamics, and linguistic outputs, limiting both accuracy and interpretability. We propose Video-KTR, a modality-aware policy shaping framework that performs selective, token-level RL by combining three attribution signals: (1) visual-aware tokens identified via counterfactual masking to reveal perceptual dependence; (2) temporal-aware tokens detected through frame shuffling to expose temporal sensitivity; and (3) high-entropy tokens signaling predictive uncertainty. By reinforcing only these key tokens, Video-KTR focuses learning on semantically informative, modality-sensitive content while filtering out low-value tokens. Across five challenging benchmarks, Video-KTR achieves state-of-the-art or highly competitive results, achieving 42.7\\% on Video-Holmes (surpassing GPT-4o) with consistent gains on both reasoning and general video understanding tasks. Ablation studies verify the complementary roles of the attribution signals and the robustness of targeted token-level updates. Overall, Video-KTR improves accuracy and interpretability, offering a simple, drop-in extension to RL for complex video reasoning. Our code and models are available at this https URL."
    }
  ],
  "MMLab@NTU": [
    {
      "title": "RoamScene3D: Immersive Text-to-3D Scene Generation via Adaptive Object-aware Roaming",
      "url": "https://arxiv.org/abs/2601.19433",
      "date": "2026-01-27",
      "authors_text": "Jisheng Chu, Wenrui Li, Rui Zhao, Wangmeng Zuo, Shifeng Chen",
      "is_highlight": false,
      "score": 63.0,
      "summary": "RoamScene3D introduces a framework for generating immersive 3D scenes from text by leveraging semantic object relations and adaptive camera trajectories, outperforming existing methods.",
      "teaser_image": "https://arxiv.org/html/2601.19433/x2.png",
      "debug_abstract": "Generating immersive 3D scenes from texts is a core task in computer vision, crucial for applications in virtual reality and game development. Despite the promise of leveraging 2D diffusion priors, existing methods suffer from spatial blindness and rely on predefined trajectories that fail to exploit the inner relationships among salient objects. Consequently, these approaches are unable to comprehend the semantic layout, preventing them from exploring the scene adaptively to infer occluded content. Moreover, current inpainting models operate in 2D image space, struggling to plausibly fill holes caused by camera motion. To address these limitations, we propose RoamScene3D, a novel framework that bridges the gap between semantic guidance and spatial generation. Our method reasons about the semantic relations among objects and produces consistent and photorealistic scenes. Specifically, we employ a vision-language model (VLM) to construct a scene graph that encodes object relations, guiding the camera to perceive salient object boundaries and plan an adaptive roaming trajectory. Furthermore, to mitigate the limitations of static 2D priors, we introduce a Motion-Injected Inpainting model that is fine-tuned on a synthetic panoramic dataset integrating authentic camera trajectories, making it adaptive to camera motion. Extensive experiments demonstrate that with semantic reasoning and geometric constraints, our method significantly outperforms state-of-the-art approaches in producing consistent and photorealistic scenes. Our code is available at this https URL."
    },
    {
      "title": "Video-KTR: Reinforcing Video Reasoning via Key Token Attribution",
      "url": "https://arxiv.org/abs/2601.19686",
      "date": "2026-01-27",
      "authors_text": "Ziyue Wang, Sheng Jin, Zhongrong Zuo, Jiawei Wu, Han Qiu",
      "is_highlight": false,
      "score": 67.0,
      "summary": "Video-KTR enhances video reasoning in multimodal models by selectively reinforcing key tokens through a novel attribution framework, achieving state-of-the-art performance across multiple benchmarks.",
      "teaser_image": "https://arxiv.org/html/2601.19686/x1.png",
      "debug_abstract": "Reinforcement learning (RL) has shown strong potential for enhancing reasoning in multimodal large language models, yet existing video reasoning methods often rely on coarse sequence-level rewards or single-factor token selection, neglecting fine-grained links among visual inputs, temporal dynamics, and linguistic outputs, limiting both accuracy and interpretability. We propose Video-KTR, a modality-aware policy shaping framework that performs selective, token-level RL by combining three attribution signals: (1) visual-aware tokens identified via counterfactual masking to reveal perceptual dependence; (2) temporal-aware tokens detected through frame shuffling to expose temporal sensitivity; and (3) high-entropy tokens signaling predictive uncertainty. By reinforcing only these key tokens, Video-KTR focuses learning on semantically informative, modality-sensitive content while filtering out low-value tokens. Across five challenging benchmarks, Video-KTR achieves state-of-the-art or highly competitive results, achieving 42.7\\% on Video-Holmes (surpassing GPT-4o) with consistent gains on both reasoning and general video understanding tasks. Ablation studies verify the complementary roles of the attribution signals and the robustness of targeted token-level updates. Overall, Video-KTR improves accuracy and interpretability, offering a simple, drop-in extension to RL for complex video reasoning. Our code and models are available at this https URL."
    }
  ],
  "MARS Lab": [
    {
      "title": "RoamScene3D: Immersive Text-to-3D Scene Generation via Adaptive Object-aware Roaming",
      "url": "https://arxiv.org/abs/2601.19433",
      "date": "2026-01-27",
      "authors_text": "Jisheng Chu, Wenrui Li, Rui Zhao, Wangmeng Zuo, Shifeng Chen",
      "is_highlight": false,
      "score": 63.0,
      "summary": "RoamScene3D introduces a framework for generating immersive 3D scenes from text by leveraging semantic object relations and adaptive camera trajectories, outperforming existing methods.",
      "teaser_image": "https://arxiv.org/html/2601.19433/x2.png",
      "debug_abstract": "Generating immersive 3D scenes from texts is a core task in computer vision, crucial for applications in virtual reality and game development. Despite the promise of leveraging 2D diffusion priors, existing methods suffer from spatial blindness and rely on predefined trajectories that fail to exploit the inner relationships among salient objects. Consequently, these approaches are unable to comprehend the semantic layout, preventing them from exploring the scene adaptively to infer occluded content. Moreover, current inpainting models operate in 2D image space, struggling to plausibly fill holes caused by camera motion. To address these limitations, we propose RoamScene3D, a novel framework that bridges the gap between semantic guidance and spatial generation. Our method reasons about the semantic relations among objects and produces consistent and photorealistic scenes. Specifically, we employ a vision-language model (VLM) to construct a scene graph that encodes object relations, guiding the camera to perceive salient object boundaries and plan an adaptive roaming trajectory. Furthermore, to mitigate the limitations of static 2D priors, we introduce a Motion-Injected Inpainting model that is fine-tuned on a synthetic panoramic dataset integrating authentic camera trajectories, making it adaptive to camera motion. Extensive experiments demonstrate that with semantic reasoning and geometric constraints, our method significantly outperforms state-of-the-art approaches in producing consistent and photorealistic scenes. Our code is available at this https URL."
    },
    {
      "title": "Video-KTR: Reinforcing Video Reasoning via Key Token Attribution",
      "url": "https://arxiv.org/abs/2601.19686",
      "date": "2026-01-27",
      "authors_text": "Ziyue Wang, Sheng Jin, Zhongrong Zuo, Jiawei Wu, Han Qiu",
      "is_highlight": false,
      "score": 67.0,
      "summary": "Video-KTR enhances video reasoning in multimodal models by selectively reinforcing key tokens through a novel attribution framework, achieving state-of-the-art performance across multiple benchmarks.",
      "teaser_image": "https://arxiv.org/html/2601.19686/x1.png",
      "debug_abstract": "Reinforcement learning (RL) has shown strong potential for enhancing reasoning in multimodal large language models, yet existing video reasoning methods often rely on coarse sequence-level rewards or single-factor token selection, neglecting fine-grained links among visual inputs, temporal dynamics, and linguistic outputs, limiting both accuracy and interpretability. We propose Video-KTR, a modality-aware policy shaping framework that performs selective, token-level RL by combining three attribution signals: (1) visual-aware tokens identified via counterfactual masking to reveal perceptual dependence; (2) temporal-aware tokens detected through frame shuffling to expose temporal sensitivity; and (3) high-entropy tokens signaling predictive uncertainty. By reinforcing only these key tokens, Video-KTR focuses learning on semantically informative, modality-sensitive content while filtering out low-value tokens. Across five challenging benchmarks, Video-KTR achieves state-of-the-art or highly competitive results, achieving 42.7\\% on Video-Holmes (surpassing GPT-4o) with consistent gains on both reasoning and general video understanding tasks. Ablation studies verify the complementary roles of the attribution signals and the robustness of targeted token-level updates. Overall, Video-KTR improves accuracy and interpretability, offering a simple, drop-in extension to RL for complex video reasoning. Our code and models are available at this https URL."
    }
  ],
  "Affective Intelligence and Robotics Laboratory (AFAR)": [
    {
      "title": "Reimagining Social Robots as Recommender Systems: Foundations, Framework, and Applications",
      "url": "https://arxiv.org/abs/2601.19761",
      "date": "2026-01-27",
      "authors_text": "Jin Huang, Fethiye Irmak Doğan, Hatice Gunes",
      "is_highlight": true,
      "score": 74.0,
      "summary": "The paper proposes integrating recommender systems into social robots to enhance personalization by effectively modeling user preferences and ensuring ethical interactions.",
      "teaser_image": "https://arxiv.org/html/2601.19761/data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==",
      "debug_abstract": "Personalization in social robots refers to the ability of the robot to meet the needs and/or preferences of an individual user. Existing approaches typically rely on large language models (LLMs) to generate context-aware responses based on user metadata and historical interactions or on adaptive methods such as reinforcement learning (RL) to learn from users&#39; immediate reactions in real time. However, these approaches fall short of comprehensively capturing user preferences-including long-term, short-term, and fine-grained aspects-, and of using them to rank and select actions, proactively personalize interactions, and ensure ethically responsible adaptations. To address the limitations, we propose drawing on recommender systems (RSs), which specialize in modeling user preferences and providing personalized recommendations. To ensure the integration of RS techniques is well-grounded and seamless throughout the social robot pipeline, we (i) align the paradigms underlying social robots and RSs, (ii) identify key techniques that can enhance personalization in social robots, and (iii) design them as modular, plug-and-play components. This work not only establishes a framework for integrating RS techniques into social robots but also opens a pathway for deep collaboration between the RS and HRI communities, accelerating innovation in both fields."
    }
  ],
  "Bio-Inspired Robotics Laboratory (BIRL)": [
    {
      "title": "Reimagining Social Robots as Recommender Systems: Foundations, Framework, and Applications",
      "url": "https://arxiv.org/abs/2601.19761",
      "date": "2026-01-27",
      "authors_text": "Jin Huang, Fethiye Irmak Doğan, Hatice Gunes",
      "is_highlight": true,
      "score": 74.0,
      "summary": "The paper proposes integrating recommender systems into social robots to enhance personalization by effectively modeling user preferences and ensuring ethical interactions.",
      "teaser_image": "https://arxiv.org/html/2601.19761/data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==",
      "debug_abstract": "Personalization in social robots refers to the ability of the robot to meet the needs and/or preferences of an individual user. Existing approaches typically rely on large language models (LLMs) to generate context-aware responses based on user metadata and historical interactions or on adaptive methods such as reinforcement learning (RL) to learn from users&#39; immediate reactions in real time. However, these approaches fall short of comprehensively capturing user preferences-including long-term, short-term, and fine-grained aspects-, and of using them to rank and select actions, proactively personalize interactions, and ensure ethically responsible adaptations. To address the limitations, we propose drawing on recommender systems (RSs), which specialize in modeling user preferences and providing personalized recommendations. To ensure the integration of RS techniques is well-grounded and seamless throughout the social robot pipeline, we (i) align the paradigms underlying social robots and RSs, (ii) identify key techniques that can enhance personalization in social robots, and (iii) design them as modular, plug-and-play components. This work not only establishes a framework for integrating RS techniques into social robots but also opens a pathway for deep collaboration between the RS and HRI communities, accelerating innovation in both fields."
    }
  ],
  "南京大学": [
    {
      "title": "GeoDiff3D: Self-Supervised 3D Scene Generation with Geometry-Constrained 2D Diffusion Guidance",
      "url": "https://arxiv.org/abs/2601.19785",
      "date": "2026-01-27",
      "authors_text": "Haozhi Zhu, Miaomiao Zhao, Dingyao Liu, Runze Tian, Yan Zhang",
      "is_highlight": false,
      "score": 69.0,
      "summary": "GeoDiff3D introduces a self-supervised framework for efficient, high-quality 3D scene generation using geometry-constrained 2D diffusion, minimizing reliance on labeled data and enhancing structural coherence.",
      "teaser_image": "https://arxiv.org/html/2601.19785/x1.png",
      "debug_abstract": "3D scene generation is a core technology for gaming, film/VFX, and VR/AR. Growing demand for rapid iteration, high-fidelity detail, and accessible content creation has further increased interest in this area. Existing methods broadly follow two paradigms - indirect 2D-to-3D reconstruction and direct 3D generation - but both are limited by weak structural modeling and heavy reliance on large-scale ground-truth supervision, often producing structural artifacts, geometric inconsistencies, and degraded high-frequency details in complex scenes. We propose GeoDiff3D, an efficient self-supervised framework that uses coarse geometry as a structural anchor and a geometry-constrained 2D diffusion model to provide texture-rich reference images. Importantly, GeoDiff3D does not require strict multi-view consistency of the diffusion-generated references and remains robust to the resulting noisy, inconsistent guidance. We further introduce voxel-aligned 3D feature aggregation and dual self-supervision to maintain scene coherence and fine details while substantially reducing dependence on labeled data. GeoDiff3D also trains with low computational cost and enables fast, high-quality 3D scene generation. Extensive experiments on challenging scenes show improved generalization and generation quality over existing baselines, offering a practical solution for accessible and efficient 3D scene construction."
    }
  ],
  "Robot Perception and Learning Lab": [
    {
      "title": "Unsupervised Learning of Efficient Exploration: Pre-training Adaptive Policies via Self-Imposed Goals",
      "url": "https://arxiv.org/abs/2601.19810",
      "date": "2026-01-27",
      "authors_text": "Octavio Pappalardo",
      "is_highlight": false,
      "score": 81.0,
      "summary": "The paper introduces ULEE, an unsupervised meta-learning method that enhances exploration and adaptation in reinforcement learning agents through self-imposed goals, outperforming existing pre-training strategies.",
      "teaser_image": "https://arxiv.org/html/2601.19810/figs/trivial4rooms_example.png",
      "debug_abstract": "Unsupervised pre-training can equip reinforcement learning agents with prior knowledge and accelerate learning in downstream tasks. A promising direction, grounded in human development, investigates agents that learn by setting and pursuing their own goals. The core challenge lies in how to effectively generate, select, and learn from such goals. Our focus is on broad distributions of downstream tasks where solving every task zero-shot is infeasible. Such settings naturally arise when the target tasks lie outside of the pre-training distribution or when their identities are unknown to the agent. In this work, we (i) optimize for efficient multi-episode exploration and adaptation within a meta-learning framework, and (ii) guide the training curriculum with evolving estimates of the agent&#39;s post-adaptation performance. We present ULEE, an unsupervised meta-learning method that combines an in-context learner with an adversarial goal-generation strategy that maintains training at the frontier of the agent&#39;s capabilities. On XLand-MiniGrid benchmarks, ULEE pre-training yields improved exploration and adaptation abilities that generalize to novel objectives, environment dynamics, and map structures. The resulting policy attains improved zero-shot and few-shot performance, and provides a strong initialization for longer fine-tuning processes. It outperforms learning from scratch, DIAYN pre-training, and alternative curricula."
    }
  ],
  "斯坦福-人工智能实验室 (SAIL)": [
    {
      "title": "DeFM: Learning Foundation Representations from Depth for Robotics",
      "url": "https://arxiv.org/abs/2601.18923",
      "date": "2026-01-26",
      "authors_text": "Manthan Patel, Jonas Frey, Mayank Mittal, Fan Yang, Alexander Hansson",
      "is_highlight": true,
      "score": 94.0,
      "summary": "DeFM is a self-supervised foundation model that learns robust geometric and semantic representations from depth images, achieving state-of-the-art performance in various robotic tasks and environments.",
      "teaser_image": "https://arxiv.org/html/2601.18923/x1.png",
      "debug_abstract": "Depth sensors are widely deployed across robotic platforms, and advances in fast, high-fidelity depth simulation have enabled robotic policies trained on depth observations to achieve robust sim-to-real transfer for a wide range of tasks. Despite this, representation learning for depth modality remains underexplored compared to RGB, where large-scale foundation models now define the state of the art. To address this gap, we present DeFM, a self-supervised foundation model trained entirely on depth images for robotic applications. Using a DINO-style self-distillation objective on a curated dataset of 60M depth images, DeFM learns geometric and semantic representations that generalize to diverse environments, tasks, and sensors. To retain metric awareness across multiple scales, we introduce a novel input normalization strategy. We further distill DeFM into compact models suitable for resource-constrained robotic systems. When evaluated on depth-based classification, segmentation, navigation, locomotion, and manipulation benchmarks, DeFM achieves state-of-the-art performance and demonstrates strong generalization from simulation to real-world environments. We release all our pretrained models, which can be adopted off-the-shelf for depth-based robotic learning without task-specific fine-tuning. Webpage: this https URL"
    }
  ],
  "Berkeley Artificial Intelligence Research Lab (BAIR)": [
    {
      "title": "DeFM: Learning Foundation Representations from Depth for Robotics",
      "url": "https://arxiv.org/abs/2601.18923",
      "date": "2026-01-26",
      "authors_text": "Manthan Patel, Jonas Frey, Mayank Mittal, Fan Yang, Alexander Hansson",
      "is_highlight": true,
      "score": 94.0,
      "summary": "DeFM is a self-supervised foundation model that learns robust geometric and semantic representations from depth images, achieving state-of-the-art performance in various robotic tasks and environments.",
      "teaser_image": "https://arxiv.org/html/2601.18923/x1.png",
      "debug_abstract": "Depth sensors are widely deployed across robotic platforms, and advances in fast, high-fidelity depth simulation have enabled robotic policies trained on depth observations to achieve robust sim-to-real transfer for a wide range of tasks. Despite this, representation learning for depth modality remains underexplored compared to RGB, where large-scale foundation models now define the state of the art. To address this gap, we present DeFM, a self-supervised foundation model trained entirely on depth images for robotic applications. Using a DINO-style self-distillation objective on a curated dataset of 60M depth images, DeFM learns geometric and semantic representations that generalize to diverse environments, tasks, and sensors. To retain metric awareness across multiple scales, we introduce a novel input normalization strategy. We further distill DeFM into compact models suitable for resource-constrained robotic systems. When evaluated on depth-based classification, segmentation, navigation, locomotion, and manipulation benchmarks, DeFM achieves state-of-the-art performance and demonstrates strong generalization from simulation to real-world environments. We release all our pretrained models, which can be adopted off-the-shelf for depth-based robotic learning without task-specific fine-tuning. Webpage: this https URL"
    }
  ],
  "Robotics and Embodied Artificial Intelligence Lab (REAL)": [
    {
      "title": "DeFM: Learning Foundation Representations from Depth for Robotics",
      "url": "https://arxiv.org/abs/2601.18923",
      "date": "2026-01-26",
      "authors_text": "Manthan Patel, Jonas Frey, Mayank Mittal, Fan Yang, Alexander Hansson",
      "is_highlight": true,
      "score": 94.0,
      "summary": "DeFM is a self-supervised foundation model that learns robust geometric and semantic representations from depth images, achieving state-of-the-art performance in various robotic tasks and environments.",
      "teaser_image": "https://arxiv.org/html/2601.18923/x1.png",
      "debug_abstract": "Depth sensors are widely deployed across robotic platforms, and advances in fast, high-fidelity depth simulation have enabled robotic policies trained on depth observations to achieve robust sim-to-real transfer for a wide range of tasks. Despite this, representation learning for depth modality remains underexplored compared to RGB, where large-scale foundation models now define the state of the art. To address this gap, we present DeFM, a self-supervised foundation model trained entirely on depth images for robotic applications. Using a DINO-style self-distillation objective on a curated dataset of 60M depth images, DeFM learns geometric and semantic representations that generalize to diverse environments, tasks, and sensors. To retain metric awareness across multiple scales, we introduce a novel input normalization strategy. We further distill DeFM into compact models suitable for resource-constrained robotic systems. When evaluated on depth-based classification, segmentation, navigation, locomotion, and manipulation benchmarks, DeFM achieves state-of-the-art performance and demonstrates strong generalization from simulation to real-world environments. We release all our pretrained models, which can be adopted off-the-shelf for depth-based robotic learning without task-specific fine-tuning. Webpage: this https URL"
    }
  ],
  "Stanford AI Lab (SAIL)": [
    {
      "title": "DeFM: Learning Foundation Representations from Depth for Robotics",
      "url": "https://arxiv.org/abs/2601.18923",
      "date": "2026-01-26",
      "authors_text": "Manthan Patel, Jonas Frey, Mayank Mittal, Fan Yang, Alexander Hansson",
      "is_highlight": true,
      "score": 94.0,
      "summary": "DeFM is a self-supervised foundation model that learns robust geometric and semantic representations from depth images, achieving state-of-the-art performance in various robotic tasks and environments.",
      "teaser_image": "https://arxiv.org/html/2601.18923/x1.png",
      "debug_abstract": "Depth sensors are widely deployed across robotic platforms, and advances in fast, high-fidelity depth simulation have enabled robotic policies trained on depth observations to achieve robust sim-to-real transfer for a wide range of tasks. Despite this, representation learning for depth modality remains underexplored compared to RGB, where large-scale foundation models now define the state of the art. To address this gap, we present DeFM, a self-supervised foundation model trained entirely on depth images for robotic applications. Using a DINO-style self-distillation objective on a curated dataset of 60M depth images, DeFM learns geometric and semantic representations that generalize to diverse environments, tasks, and sensors. To retain metric awareness across multiple scales, we introduce a novel input normalization strategy. We further distill DeFM into compact models suitable for resource-constrained robotic systems. When evaluated on depth-based classification, segmentation, navigation, locomotion, and manipulation benchmarks, DeFM achieves state-of-the-art performance and demonstrates strong generalization from simulation to real-world environments. We release all our pretrained models, which can be adopted off-the-shelf for depth-based robotic learning without task-specific fine-tuning. Webpage: this https URL"
    }
  ],
  "斯坦福-机器人与具身智能实验室 (REAL)": [
    {
      "title": "DeFM: Learning Foundation Representations from Depth for Robotics",
      "url": "https://arxiv.org/abs/2601.18923",
      "date": "2026-01-26",
      "authors_text": "Manthan Patel, Jonas Frey, Mayank Mittal, Fan Yang, Alexander Hansson",
      "is_highlight": true,
      "score": 94.0,
      "summary": "DeFM is a self-supervised foundation model that learns robust geometric and semantic representations from depth images, achieving state-of-the-art performance in various robotic tasks and environments.",
      "teaser_image": "https://arxiv.org/html/2601.18923/x1.png",
      "debug_abstract": "Depth sensors are widely deployed across robotic platforms, and advances in fast, high-fidelity depth simulation have enabled robotic policies trained on depth observations to achieve robust sim-to-real transfer for a wide range of tasks. Despite this, representation learning for depth modality remains underexplored compared to RGB, where large-scale foundation models now define the state of the art. To address this gap, we present DeFM, a self-supervised foundation model trained entirely on depth images for robotic applications. Using a DINO-style self-distillation objective on a curated dataset of 60M depth images, DeFM learns geometric and semantic representations that generalize to diverse environments, tasks, and sensors. To retain metric awareness across multiple scales, we introduce a novel input normalization strategy. We further distill DeFM into compact models suitable for resource-constrained robotic systems. When evaluated on depth-based classification, segmentation, navigation, locomotion, and manipulation benchmarks, DeFM achieves state-of-the-art performance and demonstrates strong generalization from simulation to real-world environments. We release all our pretrained models, which can be adopted off-the-shelf for depth-based robotic learning without task-specific fine-tuning. Webpage: this https URL"
    }
  ],
  "斯坦福-视觉与学习实验室 (SVL)": [
    {
      "title": "DeFM: Learning Foundation Representations from Depth for Robotics",
      "url": "https://arxiv.org/abs/2601.18923",
      "date": "2026-01-26",
      "authors_text": "Manthan Patel, Jonas Frey, Mayank Mittal, Fan Yang, Alexander Hansson",
      "is_highlight": true,
      "score": 94.0,
      "summary": "DeFM is a self-supervised foundation model that learns robust geometric and semantic representations from depth images, achieving state-of-the-art performance in various robotic tasks and environments.",
      "teaser_image": "https://arxiv.org/html/2601.18923/x1.png",
      "debug_abstract": "Depth sensors are widely deployed across robotic platforms, and advances in fast, high-fidelity depth simulation have enabled robotic policies trained on depth observations to achieve robust sim-to-real transfer for a wide range of tasks. Despite this, representation learning for depth modality remains underexplored compared to RGB, where large-scale foundation models now define the state of the art. To address this gap, we present DeFM, a self-supervised foundation model trained entirely on depth images for robotic applications. Using a DINO-style self-distillation objective on a curated dataset of 60M depth images, DeFM learns geometric and semantic representations that generalize to diverse environments, tasks, and sensors. To retain metric awareness across multiple scales, we introduce a novel input normalization strategy. We further distill DeFM into compact models suitable for resource-constrained robotic systems. When evaluated on depth-based classification, segmentation, navigation, locomotion, and manipulation benchmarks, DeFM achieves state-of-the-art performance and demonstrates strong generalization from simulation to real-world environments. We release all our pretrained models, which can be adopted off-the-shelf for depth-based robotic learning without task-specific fine-tuning. Webpage: this https URL"
    }
  ],
  "机器人技术与系统国家重点实验室": [
    {
      "title": "RoamScene3D: Immersive Text-to-3D Scene Generation via Adaptive Object-aware Roaming",
      "url": "https://arxiv.org/abs/2601.19433",
      "date": "2026-01-27",
      "authors_text": "Jisheng Chu, Wenrui Li, Rui Zhao, Wangmeng Zuo, Shifeng Chen",
      "is_highlight": false,
      "score": 63.0,
      "summary": "RoamScene3D introduces a framework for generating immersive 3D scenes from text by leveraging semantic object relations and adaptive camera trajectories, outperforming existing methods.",
      "teaser_image": "https://arxiv.org/html/2601.19433/x2.png",
      "debug_abstract": "Generating immersive 3D scenes from texts is a core task in computer vision, crucial for applications in virtual reality and game development. Despite the promise of leveraging 2D diffusion priors, existing methods suffer from spatial blindness and rely on predefined trajectories that fail to exploit the inner relationships among salient objects. Consequently, these approaches are unable to comprehend the semantic layout, preventing them from exploring the scene adaptively to infer occluded content. Moreover, current inpainting models operate in 2D image space, struggling to plausibly fill holes caused by camera motion. To address these limitations, we propose RoamScene3D, a novel framework that bridges the gap between semantic guidance and spatial generation. Our method reasons about the semantic relations among objects and produces consistent and photorealistic scenes. Specifically, we employ a vision-language model (VLM) to construct a scene graph that encodes object relations, guiding the camera to perceive salient object boundaries and plan an adaptive roaming trajectory. Furthermore, to mitigate the limitations of static 2D priors, we introduce a Motion-Injected Inpainting model that is fine-tuned on a synthetic panoramic dataset integrating authentic camera trajectories, making it adaptive to camera motion. Extensive experiments demonstrate that with semantic reasoning and geometric constraints, our method significantly outperforms state-of-the-art approaches in producing consistent and photorealistic scenes. Our code is available at this https URL."
    }
  ],
  "Quest for Intelligence": [
    {
      "title": "VGGT-SLAM 2.0: Real time Dense Feed-forward Scene Reconstruction",
      "url": "https://arxiv.org/abs/2601.19887",
      "date": "2026-01-27",
      "authors_text": "Dominic Maggio, Luca Carlone",
      "is_highlight": false,
      "score": 95.0,
      "summary": "VGGT-SLAM 2.0 enhances real-time RGB SLAM by improving submap alignment, reducing drift, and enabling effective image retrieval, achieving superior accuracy in diverse environments.",
      "teaser_image": "https://arxiv.org/html/2601.19887/fig/pipeline_semantic.png",
      "debug_abstract": "We present VGGT-SLAM 2.0, a real time RGB feed-forward SLAM system which substantially improves upon VGGT-SLAM for incrementally aligning submaps created from VGGT. Firstly, we remove high-dimensional 15-degree-of-freedom drift and planar degeneracy from VGGT-SLAM by creating a new factor graph design while still addressing the reconstruction ambiguity of VGGT given unknown camera intrinsics. Secondly, by studying the attention layers of VGGT, we show that one of the layers is well suited to assist in image retrieval verification for free without additional training, which enables both rejecting false positive matches and allows for completing more loop closures. Finally, we conduct a suite of experiments which includes showing VGGT-SLAM 2.0 can easily be adapted for open-set object detection and demonstrating real time performance while running online onboard a ground robot using a Jetson Thor. We also test in environments ranging from cluttered indoor apartments and office scenes to a 4,200 square foot barn, and we also demonstrate VGGT-SLAM 2.0 achieves the highest accuracy on the TUM dataset with about 23 percent less pose error than VGGT-SLAM. Code will be released upon publication."
    }
  ],
  "CSAIL Embodied Intelligence Labs": [
    {
      "title": "VGGT-SLAM 2.0: Real time Dense Feed-forward Scene Reconstruction",
      "url": "https://arxiv.org/abs/2601.19887",
      "date": "2026-01-27",
      "authors_text": "Dominic Maggio, Luca Carlone",
      "is_highlight": false,
      "score": 95.0,
      "summary": "VGGT-SLAM 2.0 enhances real-time RGB SLAM by improving submap alignment, reducing drift, and enabling effective image retrieval, achieving superior accuracy in diverse environments.",
      "teaser_image": "https://arxiv.org/html/2601.19887/fig/pipeline_semantic.png",
      "debug_abstract": "We present VGGT-SLAM 2.0, a real time RGB feed-forward SLAM system which substantially improves upon VGGT-SLAM for incrementally aligning submaps created from VGGT. Firstly, we remove high-dimensional 15-degree-of-freedom drift and planar degeneracy from VGGT-SLAM by creating a new factor graph design while still addressing the reconstruction ambiguity of VGGT given unknown camera intrinsics. Secondly, by studying the attention layers of VGGT, we show that one of the layers is well suited to assist in image retrieval verification for free without additional training, which enables both rejecting false positive matches and allows for completing more loop closures. Finally, we conduct a suite of experiments which includes showing VGGT-SLAM 2.0 can easily be adapted for open-set object detection and demonstrating real time performance while running online onboard a ground robot using a Jetson Thor. We also test in environments ranging from cluttered indoor apartments and office scenes to a 4,200 square foot barn, and we also demonstrate VGGT-SLAM 2.0 achieves the highest accuracy on the TUM dataset with about 23 percent less pose error than VGGT-SLAM. Code will be released upon publication."
    }
  ]
}