{
  "加州理工-机器人实验室": [
    {
      "title": "Large-Scale Continual Scheduling and Execution for Dynamic Distributed Satellite Constellation Observation Allocation",
      "url": "https://arxiv.org/abs/2601.06188",
      "date": "2026-01-26",
      "authors_text": "Itai Zilberstein, Steve Chien",
      "is_highlight": false,
      "score": 70.0,
      "summary": "This paper introduces the Dynamic Incremental Neighborhood Stochastic Search algorithm for efficient scheduling in large-scale satellite constellations, enhancing autonomy and performance in dynamic environments.",
      "debug_abstract": "The size and capabilities of Earth-observing satellite constellations are rapidly increasing. Leveraging distributed onboard control, we can enable novel time-sensitive measurements and responses. However, deploying autonomy to large multiagent satellite systems necessitates algorithms with efficient computation and communication. We tackle this challenge and propose new, online algorithms for large-scale dynamic distributed constraint optimization problems (DDCOP). We present the Dynamic Multi-Satellite Constellation Observation Scheduling Problem (DCOSP), a new formulation of DDCOPs that models integrated scheduling and execution. We construct an omniscient offline algorithm to compute the novel optimality condition of DCOSP and present the Dynamic Incremental Neighborhood Stochastic Search (D-NSS) algorithm, an incomplete online decomposition-based DDCOP approach. We show through simulation that D-NSS converges to near-optimal solutions and outperforms DDCOP baselines in terms of solution quality, computation time, and message volume. Our work forms the foundation of the largest in-space demonstration of distributed multiagent AI to date: the NASA FAME mission."
    }
  ],
  "卡内基-机器人研究所": [
    {
      "title": "How Does Delegation in Social Interaction Evolve Over Time? Navigation with a Robot for Blind People",
      "url": "https://arxiv.org/abs/2601.19851",
      "date": "2026-01-27",
      "authors_text": "Rayna Hata, Masaki Kuribayashi, Allan Wang, Hironobu Takagi, Chieko Asakawa",
      "is_highlight": false,
      "score": 83.0,
      "summary": "The study reveals that blind participants refined their navigation strategies and preferences for robot assistance through repeated interactions, highlighting the importance of adaptive design in assistive robotics.",
      "debug_abstract": "Autonomy and independent navigation are vital to daily life but remain challenging for individuals with blindness. Robotic systems can enhance mobility and confidence by providing intelligent navigation assistance. However, fully autonomous systems may reduce users&#39; sense of control, even when they wish to remain actively involved. Although collaboration between user and robot has been recognized as important, little is known about how perceptions of this relationship change with repeated use. We present a repeated exposure study with six blind participants who interacted with a navigation-assistive robot in a real-world museum. Participants completed tasks such as navigating crowds, approaching lines, and encountering obstacles. Findings show that participants refined their strategies over time, developing clearer preferences about when to rely on the robot versus act independently. This work provides insights into how strategies and preferences evolve with repeated interaction and offers design implications for robots that adapt to user needs over time."
    },
    {
      "title": "Constraint-Aware Discrete-Time PID Gain Optimization for Robotic Joint Control Under Actuator Saturation",
      "url": "https://arxiv.org/abs/2601.18639",
      "date": "2026-01-27",
      "authors_text": "Ojasva Mishra, Xiaolong Wu, Min Xu",
      "is_highlight": false,
      "score": 85.0,
      "summary": "This paper presents a workflow for optimizing PID gains in robotic joint control under actuator saturation, improving stability and performance through discrete-time analysis and Bayesian optimization.",
      "debug_abstract": "The precise regulation of rotary actuation is fundamental in autonomous robotics, yet practical PID loops deviate from continuous-time theory due to discrete-time execution, actuator saturation, and small delays and measurement imperfections. We present an implementation-aware analysis and tuning workflow for saturated discrete-time joint control. We (i) derive PI stability regions under Euler and exact zero-order-hold (ZOH) discretizations using the Jury criterion, (ii) evaluate a discrete back-calculation anti-windup realization under saturation-dominant regimes, and (iii) propose a hybrid-certified Bayesian optimization workflow that screens analytically unstable candidates and behaviorally unsafe transients while optimizing a robust IAE objective with soft penalties on overshoot and saturation duty. Baseline sweeps ($\\tau=1.0$~s, $\\Delta t=0.01$~s, $u\\in[-10,10]$) quantify rise/settle trends for P/PI/PID. Under a randomized model family emulating uncertainty, delay, noise, quantization, and tighter saturation, robustness-oriented tuning improves median IAE from $0.843$ to $0.430$ while keeping median overshoot below $2\\%$. In simulation-only tuning, the certification screen rejects $11.6\\%$ of randomly sampled gains within bounds before full robust evaluation, improving sample efficiency without hardware experiments."
    },
    {
      "title": "Large-Scale Continual Scheduling and Execution for Dynamic Distributed Satellite Constellation Observation Allocation",
      "url": "https://arxiv.org/abs/2601.06188",
      "date": "2026-01-26",
      "authors_text": "Itai Zilberstein, Steve Chien",
      "is_highlight": false,
      "score": 70.0,
      "summary": "This paper introduces the Dynamic Incremental Neighborhood Stochastic Search algorithm for efficient scheduling in large-scale satellite constellations, enhancing autonomy and performance in dynamic environments.",
      "debug_abstract": "The size and capabilities of Earth-observing satellite constellations are rapidly increasing. Leveraging distributed onboard control, we can enable novel time-sensitive measurements and responses. However, deploying autonomy to large multiagent satellite systems necessitates algorithms with efficient computation and communication. We tackle this challenge and propose new, online algorithms for large-scale dynamic distributed constraint optimization problems (DDCOP). We present the Dynamic Multi-Satellite Constellation Observation Scheduling Problem (DCOSP), a new formulation of DDCOPs that models integrated scheduling and execution. We construct an omniscient offline algorithm to compute the novel optimality condition of DCOSP and present the Dynamic Incremental Neighborhood Stochastic Search (D-NSS) algorithm, an incomplete online decomposition-based DDCOP approach. We show through simulation that D-NSS converges to near-optimal solutions and outperforms DDCOP baselines in terms of solution quality, computation time, and message volume. Our work forms the foundation of the largest in-space demonstration of distributed multiagent AI to date: the NASA FAME mission."
    }
  ],
  "范明明老师实验室": [
    {
      "title": "Enhancing Inverse Perspective Mapping for Automatic Vectorized Road Map Generation",
      "url": "https://arxiv.org/abs/2601.19536",
      "date": "2026-01-27",
      "authors_text": "Hongji Liu, Linwei Zheng, Yongjian Li, Mingkai Tang, Xiaoyang Yan",
      "is_highlight": false,
      "score": 70.0,
      "summary": "This study introduces a cost-effective framework using enhanced inverse perspective mapping and Catmull-Rom splines for high-precision, automatic vectorized road map generation.",
      "debug_abstract": "In this study, we present a low-cost and unified framework for vectorized road mapping leveraging enhanced inverse perspective mapping (IPM). In this framework, Catmull-Rom splines are utilized to characterize lane lines, and all the other ground markings are depicted using polygons uniformly. The results from instance segmentation serve as references to refine the three-dimensional position of spline control points and polygon corner points. In conjunction with this process, the homography matrix of IPM and vehicle poses are optimized simultaneously. Our proposed framework significantly reduces the mapping errors associated with IPM. It also improves the accuracy of the initial IPM homography matrix and the predicted vehicle poses. Furthermore, it addresses the limitations imposed by the coplanarity assumption in IPM. These enhancements enable IPM to be effectively applied to vectorized road mapping, which serves a cost-effective solution with enhanced accuracy. In addition, our framework generalizes road map elements to include all common ground markings and lane lines. The proposed framework is evaluated in two different practical scenarios, and the test results show that our method can automatically generate high-precision maps with near-centimeter-level accuracy. Importantly, the optimized IPM matrix achieves an accuracy comparable to that of manual calibration, while the accuracy of vehicle poses is also significantly improved."
    },
    {
      "title": "Glance and Focus Reinforcement for Pan-cancer Screening",
      "url": "https://arxiv.org/abs/2601.19103",
      "date": "2026-01-27",
      "authors_text": "Linshan Wu, Jiaxin Zhuang, Hao Chen",
      "is_highlight": false,
      "score": 65.0,
      "summary": "GF-Screen introduces a novel reinforcement learning framework that enhances pan-cancer screening by effectively localizing and segmenting lesions in CT scans, outperforming existing methods.",
      "debug_abstract": "Pan-cancer screening in large-scale CT scans remains challenging for existing AI methods, primarily due to the difficulty of localizing diverse types of tiny lesions in large CT volumes. The extreme foreground-background imbalance significantly hinders models from focusing on diseased regions, while redundant focus on healthy regions not only decreases the efficiency but also increases false positives. Inspired by radiologists&#39; glance and focus diagnostic strategy, we introduce GF-Screen, a Glance and Focus reinforcement learning framework for pan-cancer screening. GF-Screen employs a Glance model to localize the diseased regions and a Focus model to precisely segment the lesions, where segmentation results of the Focus model are leveraged to reward the Glance model via Reinforcement Learning (RL). Specifically, the Glance model crops a group of sub-volumes from the entire CT volume and learns to select the sub-volumes with lesions for the Focus model to segment. Given that the selecting operation is non-differentiable for segmentation training, we propose to employ the segmentation results to reward the Glance model. To optimize the Glance model, we introduce a novel group relative learning paradigm, which employs group relative comparison to prioritize high-advantage predictions and discard low-advantage predictions within sub-volume groups, not only improving efficiency but also reducing false positives. In this way, for the first time, we effectively extend cutting-edge RL techniques to tackle the specific challenges in pan-cancer screening. Extensive experiments on 16 internal and 7 external datasets across 9 lesion types demonstrated the effectiveness of GF-Screen. Notably, GF-Screen leads the public validation leaderboard of MICCAI FLARE25 pan-cancer challenge, surpassing the FLARE24 champion solution by a large margin (+25.6% DSC and +28.2% NSD)."
    },
    {
      "title": "Fast and Safe Trajectory Optimization for Mobile Manipulators With Neural Configuration Space Distance Field",
      "url": "https://arxiv.org/abs/2601.18548",
      "date": "2026-01-27",
      "authors_text": "Yulin Li, Zhiyuan Song, Yiming Li, Zhicheng Song, Kai Chen",
      "is_highlight": false,
      "score": 92.0,
      "summary": "This paper presents Generalized Configuration Space Distance Fields (GCDF) for efficient trajectory optimization in mobile manipulators, enhancing collision reasoning and enabling agile motion in complex environments.",
      "debug_abstract": "Mobile manipulators promise agile, long-horizon behavior by coordinating base and arm motion, yet whole-body trajectory optimization in cluttered, confined spaces remains difficult due to high-dimensional nonconvexity and the need for fast, accurate collision reasoning. Configuration Space Distance Fields (CDF) enable fixed-base manipulators to model collisions directly in configuration space via smooth, implicit distances. This representation holds strong potential to bypass the nonlinear configuration-to-workspace mapping while preserving accurate whole-body geometry and providing optimization-friendly collision costs. Yet, extending this capability to mobile manipulators is hindered by unbounded workspaces and tighter base-arm coupling. We lift this promise to mobile manipulation with Generalized Configuration Space Distance Fields (GCDF), extending CDF to robots with both translational and rotational joints in unbounded workspaces with tighter base-arm coupling. We prove that GCDF preserves Euclidean-like local distance structure and accurately encodes whole-body geometry in configuration space, and develop a data generation and training pipeline that yields continuous neural GCDFs with accurate values and gradients, supporting efficient GPU-batched queries. Building on this representation, we develop a high-performance sequential convex optimization framework centered on GCDF-based collision reasoning. The solver scales to large numbers of implicit constraints through (i) online specification of neural constraints, (ii) sparsity-aware active-set detection with parallel batched evaluation across thousands of constraints, and (iii) incremental constraint management for rapid replanning under scene changes."
    }
  ],
  "NUS AI LAB": [
    {
      "title": "Video-KTR: Reinforcing Video Reasoning via Key Token Attribution",
      "url": "https://arxiv.org/abs/2601.19686",
      "date": "2026-01-27",
      "authors_text": "Ziyue Wang, Sheng Jin, Zhongrong Zuo, Jiawei Wu, Han Qiu",
      "is_highlight": false,
      "score": 66.0,
      "summary": "Video-KTR enhances video reasoning in multimodal models by selectively reinforcing key tokens through a novel attribution framework, achieving state-of-the-art performance across multiple benchmarks.",
      "debug_abstract": "Reinforcement learning (RL) has shown strong potential for enhancing reasoning in multimodal large language models, yet existing video reasoning methods often rely on coarse sequence-level rewards or single-factor token selection, neglecting fine-grained links among visual inputs, temporal dynamics, and linguistic outputs, limiting both accuracy and interpretability. We propose Video-KTR, a modality-aware policy shaping framework that performs selective, token-level RL by combining three attribution signals: (1) visual-aware tokens identified via counterfactual masking to reveal perceptual dependence; (2) temporal-aware tokens detected through frame shuffling to expose temporal sensitivity; and (3) high-entropy tokens signaling predictive uncertainty. By reinforcing only these key tokens, Video-KTR focuses learning on semantically informative, modality-sensitive content while filtering out low-value tokens. Across five challenging benchmarks, Video-KTR achieves state-of-the-art or highly competitive results, achieving 42.7\\% on Video-Holmes (surpassing GPT-4o) with consistent gains on both reasoning and general video understanding tasks. Ablation studies verify the complementary roles of the attribution signals and the robustness of targeted token-level updates. Overall, Video-KTR improves accuracy and interpretability, offering a simple, drop-in extension to RL for complex video reasoning. Our code and models are available at this https URL."
    },
    {
      "title": "Curiosity Driven Knowledge Retrieval for Mobile Agents",
      "url": "https://arxiv.org/abs/2601.19306",
      "date": "2026-01-27",
      "authors_text": "Sijia Li, Xiaoyu Tan, Shahir Ali, Niels Schmidt, Gengchen Ma",
      "is_highlight": false,
      "score": 80.0,
      "summary": "The paper presents a curiosity-driven framework for mobile agents that enhances performance by retrieving and integrating structured knowledge (AppCards) when faced with uncertainty, achieving improved planning reliability.",
      "debug_abstract": "Mobile agents have made progress toward reliable smartphone automation, yet performance in complex applications remains limited by incomplete knowledge and weak generalization to unseen environments. We introduce a curiosity driven knowledge retrieval framework that formalizes uncertainty during execution as a curiosity score. When this score exceeds a threshold, the system retrieves external information from documentation, code repositories, and historical trajectories. Retrieved content is organized into structured AppCards, which encode functional semantics, parameter conventions, interface mappings, and interaction patterns. During execution, an enhanced agent selectively integrates relevant AppCards into its reasoning process, thereby compensating for knowledge blind spots and improving planning reliability. Evaluation on the AndroidWorld benchmark shows consistent improvements across backbones, with an average gain of six percentage points and a new state of the art success rate of 88.8\\% when combined with GPT-5. Analysis indicates that AppCards are particularly effective for multi step and cross application tasks, while improvements depend on the backbone model. Case studies further confirm that AppCards reduce ambiguity, shorten exploration, and support stable execution trajectories. Task trajectories are publicly available at this https URL."
    },
    {
      "title": "Fast and Safe Trajectory Optimization for Mobile Manipulators With Neural Configuration Space Distance Field",
      "url": "https://arxiv.org/abs/2601.18548",
      "date": "2026-01-27",
      "authors_text": "Yulin Li, Zhiyuan Song, Yiming Li, Zhicheng Song, Kai Chen",
      "is_highlight": false,
      "score": 92.0,
      "summary": "This paper presents Generalized Configuration Space Distance Fields (GCDF) for efficient trajectory optimization in mobile manipulators, enhancing collision reasoning and enabling agile motion in complex environments.",
      "debug_abstract": "Mobile manipulators promise agile, long-horizon behavior by coordinating base and arm motion, yet whole-body trajectory optimization in cluttered, confined spaces remains difficult due to high-dimensional nonconvexity and the need for fast, accurate collision reasoning. Configuration Space Distance Fields (CDF) enable fixed-base manipulators to model collisions directly in configuration space via smooth, implicit distances. This representation holds strong potential to bypass the nonlinear configuration-to-workspace mapping while preserving accurate whole-body geometry and providing optimization-friendly collision costs. Yet, extending this capability to mobile manipulators is hindered by unbounded workspaces and tighter base-arm coupling. We lift this promise to mobile manipulation with Generalized Configuration Space Distance Fields (GCDF), extending CDF to robots with both translational and rotational joints in unbounded workspaces with tighter base-arm coupling. We prove that GCDF preserves Euclidean-like local distance structure and accurately encodes whole-body geometry in configuration space, and develop a data generation and training pipeline that yields continuous neural GCDFs with accurate values and gradients, supporting efficient GPU-batched queries. Building on this representation, we develop a high-performance sequential convex optimization framework centered on GCDF-based collision reasoning. The solver scales to large numbers of implicit constraints through (i) online specification of neural constraints, (ii) sparsity-aware active-set detection with parallel batched evaluation across thousands of constraints, and (iii) incremental constraint management for rapid replanning under scene changes."
    }
  ],
  "机器人研究所": [
    {
      "title": "Enhancing Inverse Perspective Mapping for Automatic Vectorized Road Map Generation",
      "url": "https://arxiv.org/abs/2601.19536",
      "date": "2026-01-27",
      "authors_text": "Hongji Liu, Linwei Zheng, Yongjian Li, Mingkai Tang, Xiaoyang Yan",
      "is_highlight": false,
      "score": 70.0,
      "summary": "This study introduces a cost-effective framework using enhanced inverse perspective mapping and Catmull-Rom splines for high-precision, automatic vectorized road map generation.",
      "debug_abstract": "In this study, we present a low-cost and unified framework for vectorized road mapping leveraging enhanced inverse perspective mapping (IPM). In this framework, Catmull-Rom splines are utilized to characterize lane lines, and all the other ground markings are depicted using polygons uniformly. The results from instance segmentation serve as references to refine the three-dimensional position of spline control points and polygon corner points. In conjunction with this process, the homography matrix of IPM and vehicle poses are optimized simultaneously. Our proposed framework significantly reduces the mapping errors associated with IPM. It also improves the accuracy of the initial IPM homography matrix and the predicted vehicle poses. Furthermore, it addresses the limitations imposed by the coplanarity assumption in IPM. These enhancements enable IPM to be effectively applied to vectorized road mapping, which serves a cost-effective solution with enhanced accuracy. In addition, our framework generalizes road map elements to include all common ground markings and lane lines. The proposed framework is evaluated in two different practical scenarios, and the test results show that our method can automatically generate high-precision maps with near-centimeter-level accuracy. Importantly, the optimized IPM matrix achieves an accuracy comparable to that of manual calibration, while the accuracy of vehicle poses is also significantly improved."
    },
    {
      "title": "Glance and Focus Reinforcement for Pan-cancer Screening",
      "url": "https://arxiv.org/abs/2601.19103",
      "date": "2026-01-27",
      "authors_text": "Linshan Wu, Jiaxin Zhuang, Hao Chen",
      "is_highlight": false,
      "score": 65.0,
      "summary": "GF-Screen introduces a novel reinforcement learning framework that enhances pan-cancer screening by effectively localizing and segmenting lesions in CT scans, outperforming existing methods.",
      "debug_abstract": "Pan-cancer screening in large-scale CT scans remains challenging for existing AI methods, primarily due to the difficulty of localizing diverse types of tiny lesions in large CT volumes. The extreme foreground-background imbalance significantly hinders models from focusing on diseased regions, while redundant focus on healthy regions not only decreases the efficiency but also increases false positives. Inspired by radiologists&#39; glance and focus diagnostic strategy, we introduce GF-Screen, a Glance and Focus reinforcement learning framework for pan-cancer screening. GF-Screen employs a Glance model to localize the diseased regions and a Focus model to precisely segment the lesions, where segmentation results of the Focus model are leveraged to reward the Glance model via Reinforcement Learning (RL). Specifically, the Glance model crops a group of sub-volumes from the entire CT volume and learns to select the sub-volumes with lesions for the Focus model to segment. Given that the selecting operation is non-differentiable for segmentation training, we propose to employ the segmentation results to reward the Glance model. To optimize the Glance model, we introduce a novel group relative learning paradigm, which employs group relative comparison to prioritize high-advantage predictions and discard low-advantage predictions within sub-volume groups, not only improving efficiency but also reducing false positives. In this way, for the first time, we effectively extend cutting-edge RL techniques to tackle the specific challenges in pan-cancer screening. Extensive experiments on 16 internal and 7 external datasets across 9 lesion types demonstrated the effectiveness of GF-Screen. Notably, GF-Screen leads the public validation leaderboard of MICCAI FLARE25 pan-cancer challenge, surpassing the FLARE24 champion solution by a large margin (+25.6% DSC and +28.2% NSD)."
    },
    {
      "title": "Fast and Safe Trajectory Optimization for Mobile Manipulators With Neural Configuration Space Distance Field",
      "url": "https://arxiv.org/abs/2601.18548",
      "date": "2026-01-27",
      "authors_text": "Yulin Li, Zhiyuan Song, Yiming Li, Zhicheng Song, Kai Chen",
      "is_highlight": false,
      "score": 92.0,
      "summary": "This paper presents Generalized Configuration Space Distance Fields (GCDF) for efficient trajectory optimization in mobile manipulators, enhancing collision reasoning and enabling agile motion in complex environments.",
      "debug_abstract": "Mobile manipulators promise agile, long-horizon behavior by coordinating base and arm motion, yet whole-body trajectory optimization in cluttered, confined spaces remains difficult due to high-dimensional nonconvexity and the need for fast, accurate collision reasoning. Configuration Space Distance Fields (CDF) enable fixed-base manipulators to model collisions directly in configuration space via smooth, implicit distances. This representation holds strong potential to bypass the nonlinear configuration-to-workspace mapping while preserving accurate whole-body geometry and providing optimization-friendly collision costs. Yet, extending this capability to mobile manipulators is hindered by unbounded workspaces and tighter base-arm coupling. We lift this promise to mobile manipulation with Generalized Configuration Space Distance Fields (GCDF), extending CDF to robots with both translational and rotational joints in unbounded workspaces with tighter base-arm coupling. We prove that GCDF preserves Euclidean-like local distance structure and accurately encodes whole-body geometry in configuration space, and develop a data generation and training pipeline that yields continuous neural GCDFs with accurate values and gradients, supporting efficient GPU-batched queries. Building on this representation, we develop a high-performance sequential convex optimization framework centered on GCDF-based collision reasoning. The solver scales to large numbers of implicit constraints through (i) online specification of neural constraints, (ii) sparsity-aware active-set detection with parallel batched evaluation across thousands of constraints, and (iii) incremental constraint management for rapid replanning under scene changes."
    }
  ],
  "Advanced Robotics Centre": [
    {
      "title": "Video-KTR: Reinforcing Video Reasoning via Key Token Attribution",
      "url": "https://arxiv.org/abs/2601.19686",
      "date": "2026-01-27",
      "authors_text": "Ziyue Wang, Sheng Jin, Zhongrong Zuo, Jiawei Wu, Han Qiu",
      "is_highlight": false,
      "score": 66.0,
      "summary": "Video-KTR enhances video reasoning in multimodal models by selectively reinforcing key tokens through a novel attribution framework, achieving state-of-the-art performance across multiple benchmarks.",
      "debug_abstract": "Reinforcement learning (RL) has shown strong potential for enhancing reasoning in multimodal large language models, yet existing video reasoning methods often rely on coarse sequence-level rewards or single-factor token selection, neglecting fine-grained links among visual inputs, temporal dynamics, and linguistic outputs, limiting both accuracy and interpretability. We propose Video-KTR, a modality-aware policy shaping framework that performs selective, token-level RL by combining three attribution signals: (1) visual-aware tokens identified via counterfactual masking to reveal perceptual dependence; (2) temporal-aware tokens detected through frame shuffling to expose temporal sensitivity; and (3) high-entropy tokens signaling predictive uncertainty. By reinforcing only these key tokens, Video-KTR focuses learning on semantically informative, modality-sensitive content while filtering out low-value tokens. Across five challenging benchmarks, Video-KTR achieves state-of-the-art or highly competitive results, achieving 42.7\\% on Video-Holmes (surpassing GPT-4o) with consistent gains on both reasoning and general video understanding tasks. Ablation studies verify the complementary roles of the attribution signals and the robustness of targeted token-level updates. Overall, Video-KTR improves accuracy and interpretability, offering a simple, drop-in extension to RL for complex video reasoning. Our code and models are available at this https URL."
    },
    {
      "title": "Curiosity Driven Knowledge Retrieval for Mobile Agents",
      "url": "https://arxiv.org/abs/2601.19306",
      "date": "2026-01-27",
      "authors_text": "Sijia Li, Xiaoyu Tan, Shahir Ali, Niels Schmidt, Gengchen Ma",
      "is_highlight": false,
      "score": 80.0,
      "summary": "The paper presents a curiosity-driven framework for mobile agents that enhances performance by retrieving and integrating structured knowledge (AppCards) when faced with uncertainty, achieving improved planning reliability.",
      "debug_abstract": "Mobile agents have made progress toward reliable smartphone automation, yet performance in complex applications remains limited by incomplete knowledge and weak generalization to unseen environments. We introduce a curiosity driven knowledge retrieval framework that formalizes uncertainty during execution as a curiosity score. When this score exceeds a threshold, the system retrieves external information from documentation, code repositories, and historical trajectories. Retrieved content is organized into structured AppCards, which encode functional semantics, parameter conventions, interface mappings, and interaction patterns. During execution, an enhanced agent selectively integrates relevant AppCards into its reasoning process, thereby compensating for knowledge blind spots and improving planning reliability. Evaluation on the AndroidWorld benchmark shows consistent improvements across backbones, with an average gain of six percentage points and a new state of the art success rate of 88.8\\% when combined with GPT-5. Analysis indicates that AppCards are particularly effective for multi step and cross application tasks, while improvements depend on the backbone model. Case studies further confirm that AppCards reduce ambiguity, shorten exploration, and support stable execution trajectories. Task trajectories are publicly available at this https URL."
    },
    {
      "title": "Fast and Safe Trajectory Optimization for Mobile Manipulators With Neural Configuration Space Distance Field",
      "url": "https://arxiv.org/abs/2601.18548",
      "date": "2026-01-27",
      "authors_text": "Yulin Li, Zhiyuan Song, Yiming Li, Zhicheng Song, Kai Chen",
      "is_highlight": false,
      "score": 92.0,
      "summary": "This paper presents Generalized Configuration Space Distance Fields (GCDF) for efficient trajectory optimization in mobile manipulators, enhancing collision reasoning and enabling agile motion in complex environments.",
      "debug_abstract": "Mobile manipulators promise agile, long-horizon behavior by coordinating base and arm motion, yet whole-body trajectory optimization in cluttered, confined spaces remains difficult due to high-dimensional nonconvexity and the need for fast, accurate collision reasoning. Configuration Space Distance Fields (CDF) enable fixed-base manipulators to model collisions directly in configuration space via smooth, implicit distances. This representation holds strong potential to bypass the nonlinear configuration-to-workspace mapping while preserving accurate whole-body geometry and providing optimization-friendly collision costs. Yet, extending this capability to mobile manipulators is hindered by unbounded workspaces and tighter base-arm coupling. We lift this promise to mobile manipulation with Generalized Configuration Space Distance Fields (GCDF), extending CDF to robots with both translational and rotational joints in unbounded workspaces with tighter base-arm coupling. We prove that GCDF preserves Euclidean-like local distance structure and accurately encodes whole-body geometry in configuration space, and develop a data generation and training pipeline that yields continuous neural GCDFs with accurate values and gradients, supporting efficient GPU-batched queries. Building on this representation, we develop a high-performance sequential convex optimization framework centered on GCDF-based collision reasoning. The solver scales to large numbers of implicit constraints through (i) online specification of neural constraints, (ii) sparsity-aware active-set detection with parallel batched evaluation across thousands of constraints, and (iii) incremental constraint management for rapid replanning under scene changes."
    }
  ],
  "Cheng Kar-Shun Robotics Institute (CKSRI)": [
    {
      "title": "Enhancing Inverse Perspective Mapping for Automatic Vectorized Road Map Generation",
      "url": "https://arxiv.org/abs/2601.19536",
      "date": "2026-01-27",
      "authors_text": "Hongji Liu, Linwei Zheng, Yongjian Li, Mingkai Tang, Xiaoyang Yan",
      "is_highlight": false,
      "score": 70.0,
      "summary": "This study introduces a cost-effective framework using enhanced inverse perspective mapping and Catmull-Rom splines for high-precision, automatic vectorized road map generation.",
      "debug_abstract": "In this study, we present a low-cost and unified framework for vectorized road mapping leveraging enhanced inverse perspective mapping (IPM). In this framework, Catmull-Rom splines are utilized to characterize lane lines, and all the other ground markings are depicted using polygons uniformly. The results from instance segmentation serve as references to refine the three-dimensional position of spline control points and polygon corner points. In conjunction with this process, the homography matrix of IPM and vehicle poses are optimized simultaneously. Our proposed framework significantly reduces the mapping errors associated with IPM. It also improves the accuracy of the initial IPM homography matrix and the predicted vehicle poses. Furthermore, it addresses the limitations imposed by the coplanarity assumption in IPM. These enhancements enable IPM to be effectively applied to vectorized road mapping, which serves a cost-effective solution with enhanced accuracy. In addition, our framework generalizes road map elements to include all common ground markings and lane lines. The proposed framework is evaluated in two different practical scenarios, and the test results show that our method can automatically generate high-precision maps with near-centimeter-level accuracy. Importantly, the optimized IPM matrix achieves an accuracy comparable to that of manual calibration, while the accuracy of vehicle poses is also significantly improved."
    },
    {
      "title": "Glance and Focus Reinforcement for Pan-cancer Screening",
      "url": "https://arxiv.org/abs/2601.19103",
      "date": "2026-01-27",
      "authors_text": "Linshan Wu, Jiaxin Zhuang, Hao Chen",
      "is_highlight": false,
      "score": 65.0,
      "summary": "GF-Screen introduces a novel reinforcement learning framework that enhances pan-cancer screening by effectively localizing and segmenting lesions in CT scans, outperforming existing methods.",
      "debug_abstract": "Pan-cancer screening in large-scale CT scans remains challenging for existing AI methods, primarily due to the difficulty of localizing diverse types of tiny lesions in large CT volumes. The extreme foreground-background imbalance significantly hinders models from focusing on diseased regions, while redundant focus on healthy regions not only decreases the efficiency but also increases false positives. Inspired by radiologists&#39; glance and focus diagnostic strategy, we introduce GF-Screen, a Glance and Focus reinforcement learning framework for pan-cancer screening. GF-Screen employs a Glance model to localize the diseased regions and a Focus model to precisely segment the lesions, where segmentation results of the Focus model are leveraged to reward the Glance model via Reinforcement Learning (RL). Specifically, the Glance model crops a group of sub-volumes from the entire CT volume and learns to select the sub-volumes with lesions for the Focus model to segment. Given that the selecting operation is non-differentiable for segmentation training, we propose to employ the segmentation results to reward the Glance model. To optimize the Glance model, we introduce a novel group relative learning paradigm, which employs group relative comparison to prioritize high-advantage predictions and discard low-advantage predictions within sub-volume groups, not only improving efficiency but also reducing false positives. In this way, for the first time, we effectively extend cutting-edge RL techniques to tackle the specific challenges in pan-cancer screening. Extensive experiments on 16 internal and 7 external datasets across 9 lesion types demonstrated the effectiveness of GF-Screen. Notably, GF-Screen leads the public validation leaderboard of MICCAI FLARE25 pan-cancer challenge, surpassing the FLARE24 champion solution by a large margin (+25.6% DSC and +28.2% NSD)."
    },
    {
      "title": "Fast and Safe Trajectory Optimization for Mobile Manipulators With Neural Configuration Space Distance Field",
      "url": "https://arxiv.org/abs/2601.18548",
      "date": "2026-01-27",
      "authors_text": "Yulin Li, Zhiyuan Song, Yiming Li, Zhicheng Song, Kai Chen",
      "is_highlight": false,
      "score": 92.0,
      "summary": "This paper presents Generalized Configuration Space Distance Fields (GCDF) for efficient trajectory optimization in mobile manipulators, enhancing collision reasoning and enabling agile motion in complex environments.",
      "debug_abstract": "Mobile manipulators promise agile, long-horizon behavior by coordinating base and arm motion, yet whole-body trajectory optimization in cluttered, confined spaces remains difficult due to high-dimensional nonconvexity and the need for fast, accurate collision reasoning. Configuration Space Distance Fields (CDF) enable fixed-base manipulators to model collisions directly in configuration space via smooth, implicit distances. This representation holds strong potential to bypass the nonlinear configuration-to-workspace mapping while preserving accurate whole-body geometry and providing optimization-friendly collision costs. Yet, extending this capability to mobile manipulators is hindered by unbounded workspaces and tighter base-arm coupling. We lift this promise to mobile manipulation with Generalized Configuration Space Distance Fields (GCDF), extending CDF to robots with both translational and rotational joints in unbounded workspaces with tighter base-arm coupling. We prove that GCDF preserves Euclidean-like local distance structure and accurately encodes whole-body geometry in configuration space, and develop a data generation and training pipeline that yields continuous neural GCDFs with accurate values and gradients, supporting efficient GPU-batched queries. Building on this representation, we develop a high-performance sequential convex optimization framework centered on GCDF-based collision reasoning. The solver scales to large numbers of implicit constraints through (i) online specification of neural constraints, (ii) sparsity-aware active-set detection with parallel batched evaluation across thousands of constraints, and (iii) incremental constraint management for rapid replanning under scene changes."
    }
  ],
  "Language and Vision (LaVi) Lab": [
    {
      "title": "EgoHandICL: Egocentric 3D Hand Reconstruction with In-Context Learning",
      "url": "https://arxiv.org/abs/2601.19850",
      "date": "2026-01-27",
      "authors_text": "Binzhu Xie, Shi Qiu, Sicheng Zhang, Yinqiao Wang, Hao Xu",
      "is_highlight": false,
      "score": 76.0,
      "summary": "EgoHandICL introduces an innovative in-context learning framework for robust 3D hand reconstruction in egocentric vision, enhancing performance through exemplar retrieval and multimodal context integration.",
      "debug_abstract": "Robust 3D hand reconstruction in egocentric vision is challenging due to depth ambiguity, self-occlusion, and complex hand-object interactions. Prior methods mitigate these issues by scaling training data or adding auxiliary cues, but they often struggle in unseen contexts. We present EgoHandICL, the first in-context learning (ICL) framework for 3D hand reconstruction that improves semantic alignment, visual consistency, and robustness under challenging egocentric conditions. EgoHandICL introduces complementary exemplar retrieval guided by vision-language models (VLMs), an ICL-tailored tokenizer for multimodal context, and a masked autoencoder (MAE)-based architecture trained with hand-guided geometric and perceptual objectives. Experiments on ARCTIC and EgoExo4D show consistent gains over state-of-the-art methods. We also demonstrate real-world generalization and improve EgoVLM hand-object interaction reasoning by using reconstructed hands as visual prompts. Code and data: this https URL"
    },
    {
      "title": "Component-Level Lesioning of Language Models Reveals Clinically Aligned Aphasia Phenotypes",
      "url": "https://arxiv.org/abs/2601.19723",
      "date": "2026-01-27",
      "authors_text": "Yifan Wang, Jichen Zheng, Jingyuan Sun, Yunhao Zhang, Chunyu Ye",
      "is_highlight": false,
      "score": 62.0,
      "summary": "This study presents a framework for simulating aphasia in language models by selectively perturbing components, revealing clinically relevant language impairments and enhancing understanding of language function degradation.",
      "debug_abstract": "Large language models (LLMs) increasingly exhibit human-like linguistic behaviors and internal representations that they could serve as computational simulators of language cognition. We ask whether LLMs can be systematically manipulated to reproduce language-production impairments characteristic of aphasia following focal brain lesions. Such models could provide scalable proxies for testing rehabilitation hypotheses, and offer a controlled framework for probing the functional organization of language. We introduce a clinically grounded, component-level framework that simulates aphasia by selectively perturbing functional components in LLMs, and apply it to both modular Mixture-of-Experts models and dense Transformers using a unified intervention interface. Our pipeline (i) identifies subtype-linked components for Broca&#39;s and Wernicke&#39;s aphasia, (ii) interprets these components with linguistic probing tasks, and (iii) induces graded impairments by progressively perturbing the top-k subtype-linked components, evaluating outcomes with Western Aphasia Battery (WAB) subtests summarized by Aphasia Quotient (AQ). Across architectures and lesioning strategies, subtype-targeted perturbations yield more systematic, aphasia-like regressions than size-matched random perturbations, and MoE modularity supports more localized and interpretable phenotype-to-component mappings. These findings suggest that modular LLMs, combined with clinically informed component perturbations, provide a promising platform for simulating aphasic language production and studying how distinct language functions degrade under targeted disruptions."
    },
    {
      "title": "Rhombot: Rhombus-shaped Modular Robots for Stable, Medium-Independent Reconfiguration Motion",
      "url": "https://arxiv.org/abs/2601.19529",
      "date": "2026-01-27",
      "authors_text": "Jie Gu, Yirui Sun, Zhihao Xia, Tin Lun Lam, Chunxu Tian",
      "is_highlight": false,
      "score": 86.0,
      "summary": "Rhombot is a modular self-reconfigurable robot featuring rhombus-shaped modules that enable stable, medium-independent morphing and locomotion through a novel motion primitive called morphpivoting.",
      "debug_abstract": "In this paper, we present Rhombot, a novel deformable planar lattice modular self-reconfigurable robot (MSRR) with a rhombus shaped module. Each module consists of a parallelogram skeleton with a single centrally mounted actuator that enables folding and unfolding along its diagonal. The core design philosophy is to achieve essential MSRR functionalities such as morphing, docking, and locomotion with minimal control complexity. This enables a continuous and stable reconfiguration process that is independent of the surrounding medium, allowing the system to reliably form various configurations in diverse environments. To leverage the unique kinematics of Rhombot, we introduce morphpivoting, a novel motion primitive for reconfiguration that differs from advanced MSRR systems, and propose a strategy for its continuous execution. Finally, a series of physical experiments validate the module&#39;s stable reconfiguration ability, as well as its positional and docking accuracy."
    },
    {
      "title": "Fast and Safe Trajectory Optimization for Mobile Manipulators With Neural Configuration Space Distance Field",
      "url": "https://arxiv.org/abs/2601.18548",
      "date": "2026-01-27",
      "authors_text": "Yulin Li, Zhiyuan Song, Yiming Li, Zhicheng Song, Kai Chen",
      "is_highlight": false,
      "score": 92.0,
      "summary": "This paper presents Generalized Configuration Space Distance Fields (GCDF) for efficient trajectory optimization in mobile manipulators, enhancing collision reasoning and enabling agile motion in complex environments.",
      "debug_abstract": "Mobile manipulators promise agile, long-horizon behavior by coordinating base and arm motion, yet whole-body trajectory optimization in cluttered, confined spaces remains difficult due to high-dimensional nonconvexity and the need for fast, accurate collision reasoning. Configuration Space Distance Fields (CDF) enable fixed-base manipulators to model collisions directly in configuration space via smooth, implicit distances. This representation holds strong potential to bypass the nonlinear configuration-to-workspace mapping while preserving accurate whole-body geometry and providing optimization-friendly collision costs. Yet, extending this capability to mobile manipulators is hindered by unbounded workspaces and tighter base-arm coupling. We lift this promise to mobile manipulation with Generalized Configuration Space Distance Fields (GCDF), extending CDF to robots with both translational and rotational joints in unbounded workspaces with tighter base-arm coupling. We prove that GCDF preserves Euclidean-like local distance structure and accurately encodes whole-body geometry in configuration space, and develop a data generation and training pipeline that yields continuous neural GCDFs with accurate values and gradients, supporting efficient GPU-batched queries. Building on this representation, we develop a high-performance sequential convex optimization framework centered on GCDF-based collision reasoning. The solver scales to large numbers of implicit constraints through (i) online specification of neural constraints, (ii) sparsity-aware active-set detection with parallel batched evaluation across thousands of constraints, and (iii) incremental constraint management for rapid replanning under scene changes."
    }
  ],
  "Hengshuang Zhao老师实验室": [
    {
      "title": "EgoHandICL: Egocentric 3D Hand Reconstruction with In-Context Learning",
      "url": "https://arxiv.org/abs/2601.19850",
      "date": "2026-01-27",
      "authors_text": "Binzhu Xie, Shi Qiu, Sicheng Zhang, Yinqiao Wang, Hao Xu",
      "is_highlight": false,
      "score": 76.0,
      "summary": "EgoHandICL introduces an innovative in-context learning framework for robust 3D hand reconstruction in egocentric vision, enhancing performance through exemplar retrieval and multimodal context integration.",
      "debug_abstract": "Robust 3D hand reconstruction in egocentric vision is challenging due to depth ambiguity, self-occlusion, and complex hand-object interactions. Prior methods mitigate these issues by scaling training data or adding auxiliary cues, but they often struggle in unseen contexts. We present EgoHandICL, the first in-context learning (ICL) framework for 3D hand reconstruction that improves semantic alignment, visual consistency, and robustness under challenging egocentric conditions. EgoHandICL introduces complementary exemplar retrieval guided by vision-language models (VLMs), an ICL-tailored tokenizer for multimodal context, and a masked autoencoder (MAE)-based architecture trained with hand-guided geometric and perceptual objectives. Experiments on ARCTIC and EgoExo4D show consistent gains over state-of-the-art methods. We also demonstrate real-world generalization and improve EgoVLM hand-object interaction reasoning by using reconstructed hands as visual prompts. Code and data: this https URL"
    },
    {
      "title": "Component-Level Lesioning of Language Models Reveals Clinically Aligned Aphasia Phenotypes",
      "url": "https://arxiv.org/abs/2601.19723",
      "date": "2026-01-27",
      "authors_text": "Yifan Wang, Jichen Zheng, Jingyuan Sun, Yunhao Zhang, Chunyu Ye",
      "is_highlight": false,
      "score": 62.0,
      "summary": "This study presents a framework for simulating aphasia in language models by selectively perturbing components, revealing clinically relevant language impairments and enhancing understanding of language function degradation.",
      "debug_abstract": "Large language models (LLMs) increasingly exhibit human-like linguistic behaviors and internal representations that they could serve as computational simulators of language cognition. We ask whether LLMs can be systematically manipulated to reproduce language-production impairments characteristic of aphasia following focal brain lesions. Such models could provide scalable proxies for testing rehabilitation hypotheses, and offer a controlled framework for probing the functional organization of language. We introduce a clinically grounded, component-level framework that simulates aphasia by selectively perturbing functional components in LLMs, and apply it to both modular Mixture-of-Experts models and dense Transformers using a unified intervention interface. Our pipeline (i) identifies subtype-linked components for Broca&#39;s and Wernicke&#39;s aphasia, (ii) interprets these components with linguistic probing tasks, and (iii) induces graded impairments by progressively perturbing the top-k subtype-linked components, evaluating outcomes with Western Aphasia Battery (WAB) subtests summarized by Aphasia Quotient (AQ). Across architectures and lesioning strategies, subtype-targeted perturbations yield more systematic, aphasia-like regressions than size-matched random perturbations, and MoE modularity supports more localized and interpretable phenotype-to-component mappings. These findings suggest that modular LLMs, combined with clinically informed component perturbations, provide a promising platform for simulating aphasic language production and studying how distinct language functions degrade under targeted disruptions."
    },
    {
      "title": "Rhombot: Rhombus-shaped Modular Robots for Stable, Medium-Independent Reconfiguration Motion",
      "url": "https://arxiv.org/abs/2601.19529",
      "date": "2026-01-27",
      "authors_text": "Jie Gu, Yirui Sun, Zhihao Xia, Tin Lun Lam, Chunxu Tian",
      "is_highlight": false,
      "score": 86.0,
      "summary": "Rhombot is a modular self-reconfigurable robot featuring rhombus-shaped modules that enable stable, medium-independent morphing and locomotion through a novel motion primitive called morphpivoting.",
      "debug_abstract": "In this paper, we present Rhombot, a novel deformable planar lattice modular self-reconfigurable robot (MSRR) with a rhombus shaped module. Each module consists of a parallelogram skeleton with a single centrally mounted actuator that enables folding and unfolding along its diagonal. The core design philosophy is to achieve essential MSRR functionalities such as morphing, docking, and locomotion with minimal control complexity. This enables a continuous and stable reconfiguration process that is independent of the surrounding medium, allowing the system to reliably form various configurations in diverse environments. To leverage the unique kinematics of Rhombot, we introduce morphpivoting, a novel motion primitive for reconfiguration that differs from advanced MSRR systems, and propose a strategy for its continuous execution. Finally, a series of physical experiments validate the module&#39;s stable reconfiguration ability, as well as its positional and docking accuracy."
    },
    {
      "title": "Fast and Safe Trajectory Optimization for Mobile Manipulators With Neural Configuration Space Distance Field",
      "url": "https://arxiv.org/abs/2601.18548",
      "date": "2026-01-27",
      "authors_text": "Yulin Li, Zhiyuan Song, Yiming Li, Zhicheng Song, Kai Chen",
      "is_highlight": false,
      "score": 92.0,
      "summary": "This paper presents Generalized Configuration Space Distance Fields (GCDF) for efficient trajectory optimization in mobile manipulators, enhancing collision reasoning and enabling agile motion in complex environments.",
      "debug_abstract": "Mobile manipulators promise agile, long-horizon behavior by coordinating base and arm motion, yet whole-body trajectory optimization in cluttered, confined spaces remains difficult due to high-dimensional nonconvexity and the need for fast, accurate collision reasoning. Configuration Space Distance Fields (CDF) enable fixed-base manipulators to model collisions directly in configuration space via smooth, implicit distances. This representation holds strong potential to bypass the nonlinear configuration-to-workspace mapping while preserving accurate whole-body geometry and providing optimization-friendly collision costs. Yet, extending this capability to mobile manipulators is hindered by unbounded workspaces and tighter base-arm coupling. We lift this promise to mobile manipulation with Generalized Configuration Space Distance Fields (GCDF), extending CDF to robots with both translational and rotational joints in unbounded workspaces with tighter base-arm coupling. We prove that GCDF preserves Euclidean-like local distance structure and accurately encodes whole-body geometry in configuration space, and develop a data generation and training pipeline that yields continuous neural GCDFs with accurate values and gradients, supporting efficient GPU-batched queries. Building on this representation, we develop a high-performance sequential convex optimization framework centered on GCDF-based collision reasoning. The solver scales to large numbers of implicit constraints through (i) online specification of neural constraints, (ii) sparsity-aware active-set detection with parallel batched evaluation across thousands of constraints, and (iii) incremental constraint management for rapid replanning under scene changes."
    }
  ],
  "Precognition Lab": [
    {
      "title": "Enhancing Inverse Perspective Mapping for Automatic Vectorized Road Map Generation",
      "url": "https://arxiv.org/abs/2601.19536",
      "date": "2026-01-27",
      "authors_text": "Hongji Liu, Linwei Zheng, Yongjian Li, Mingkai Tang, Xiaoyang Yan",
      "is_highlight": false,
      "score": 70.0,
      "summary": "This study introduces a cost-effective framework using enhanced inverse perspective mapping and Catmull-Rom splines for high-precision, automatic vectorized road map generation.",
      "debug_abstract": "In this study, we present a low-cost and unified framework for vectorized road mapping leveraging enhanced inverse perspective mapping (IPM). In this framework, Catmull-Rom splines are utilized to characterize lane lines, and all the other ground markings are depicted using polygons uniformly. The results from instance segmentation serve as references to refine the three-dimensional position of spline control points and polygon corner points. In conjunction with this process, the homography matrix of IPM and vehicle poses are optimized simultaneously. Our proposed framework significantly reduces the mapping errors associated with IPM. It also improves the accuracy of the initial IPM homography matrix and the predicted vehicle poses. Furthermore, it addresses the limitations imposed by the coplanarity assumption in IPM. These enhancements enable IPM to be effectively applied to vectorized road mapping, which serves a cost-effective solution with enhanced accuracy. In addition, our framework generalizes road map elements to include all common ground markings and lane lines. The proposed framework is evaluated in two different practical scenarios, and the test results show that our method can automatically generate high-precision maps with near-centimeter-level accuracy. Importantly, the optimized IPM matrix achieves an accuracy comparable to that of manual calibration, while the accuracy of vehicle poses is also significantly improved."
    },
    {
      "title": "Glance and Focus Reinforcement for Pan-cancer Screening",
      "url": "https://arxiv.org/abs/2601.19103",
      "date": "2026-01-27",
      "authors_text": "Linshan Wu, Jiaxin Zhuang, Hao Chen",
      "is_highlight": false,
      "score": 65.0,
      "summary": "GF-Screen introduces a novel reinforcement learning framework that enhances pan-cancer screening by effectively localizing and segmenting lesions in CT scans, outperforming existing methods.",
      "debug_abstract": "Pan-cancer screening in large-scale CT scans remains challenging for existing AI methods, primarily due to the difficulty of localizing diverse types of tiny lesions in large CT volumes. The extreme foreground-background imbalance significantly hinders models from focusing on diseased regions, while redundant focus on healthy regions not only decreases the efficiency but also increases false positives. Inspired by radiologists&#39; glance and focus diagnostic strategy, we introduce GF-Screen, a Glance and Focus reinforcement learning framework for pan-cancer screening. GF-Screen employs a Glance model to localize the diseased regions and a Focus model to precisely segment the lesions, where segmentation results of the Focus model are leveraged to reward the Glance model via Reinforcement Learning (RL). Specifically, the Glance model crops a group of sub-volumes from the entire CT volume and learns to select the sub-volumes with lesions for the Focus model to segment. Given that the selecting operation is non-differentiable for segmentation training, we propose to employ the segmentation results to reward the Glance model. To optimize the Glance model, we introduce a novel group relative learning paradigm, which employs group relative comparison to prioritize high-advantage predictions and discard low-advantage predictions within sub-volume groups, not only improving efficiency but also reducing false positives. In this way, for the first time, we effectively extend cutting-edge RL techniques to tackle the specific challenges in pan-cancer screening. Extensive experiments on 16 internal and 7 external datasets across 9 lesion types demonstrated the effectiveness of GF-Screen. Notably, GF-Screen leads the public validation leaderboard of MICCAI FLARE25 pan-cancer challenge, surpassing the FLARE24 champion solution by a large margin (+25.6% DSC and +28.2% NSD)."
    },
    {
      "title": "Fast and Safe Trajectory Optimization for Mobile Manipulators With Neural Configuration Space Distance Field",
      "url": "https://arxiv.org/abs/2601.18548",
      "date": "2026-01-27",
      "authors_text": "Yulin Li, Zhiyuan Song, Yiming Li, Zhicheng Song, Kai Chen",
      "is_highlight": false,
      "score": 92.0,
      "summary": "This paper presents Generalized Configuration Space Distance Fields (GCDF) for efficient trajectory optimization in mobile manipulators, enhancing collision reasoning and enabling agile motion in complex environments.",
      "debug_abstract": "Mobile manipulators promise agile, long-horizon behavior by coordinating base and arm motion, yet whole-body trajectory optimization in cluttered, confined spaces remains difficult due to high-dimensional nonconvexity and the need for fast, accurate collision reasoning. Configuration Space Distance Fields (CDF) enable fixed-base manipulators to model collisions directly in configuration space via smooth, implicit distances. This representation holds strong potential to bypass the nonlinear configuration-to-workspace mapping while preserving accurate whole-body geometry and providing optimization-friendly collision costs. Yet, extending this capability to mobile manipulators is hindered by unbounded workspaces and tighter base-arm coupling. We lift this promise to mobile manipulation with Generalized Configuration Space Distance Fields (GCDF), extending CDF to robots with both translational and rotational joints in unbounded workspaces with tighter base-arm coupling. We prove that GCDF preserves Euclidean-like local distance structure and accurately encodes whole-body geometry in configuration space, and develop a data generation and training pipeline that yields continuous neural GCDFs with accurate values and gradients, supporting efficient GPU-batched queries. Building on this representation, we develop a high-performance sequential convex optimization framework centered on GCDF-based collision reasoning. The solver scales to large numbers of implicit constraints through (i) online specification of neural constraints, (ii) sparsity-aware active-set detection with parallel batched evaluation across thousands of constraints, and (iii) incremental constraint management for rapid replanning under scene changes."
    }
  ],
  "Jun MA老师实验室": [
    {
      "title": "Enhancing Inverse Perspective Mapping for Automatic Vectorized Road Map Generation",
      "url": "https://arxiv.org/abs/2601.19536",
      "date": "2026-01-27",
      "authors_text": "Hongji Liu, Linwei Zheng, Yongjian Li, Mingkai Tang, Xiaoyang Yan",
      "is_highlight": false,
      "score": 70.0,
      "summary": "This study introduces a cost-effective framework using enhanced inverse perspective mapping and Catmull-Rom splines for high-precision, automatic vectorized road map generation.",
      "debug_abstract": "In this study, we present a low-cost and unified framework for vectorized road mapping leveraging enhanced inverse perspective mapping (IPM). In this framework, Catmull-Rom splines are utilized to characterize lane lines, and all the other ground markings are depicted using polygons uniformly. The results from instance segmentation serve as references to refine the three-dimensional position of spline control points and polygon corner points. In conjunction with this process, the homography matrix of IPM and vehicle poses are optimized simultaneously. Our proposed framework significantly reduces the mapping errors associated with IPM. It also improves the accuracy of the initial IPM homography matrix and the predicted vehicle poses. Furthermore, it addresses the limitations imposed by the coplanarity assumption in IPM. These enhancements enable IPM to be effectively applied to vectorized road mapping, which serves a cost-effective solution with enhanced accuracy. In addition, our framework generalizes road map elements to include all common ground markings and lane lines. The proposed framework is evaluated in two different practical scenarios, and the test results show that our method can automatically generate high-precision maps with near-centimeter-level accuracy. Importantly, the optimized IPM matrix achieves an accuracy comparable to that of manual calibration, while the accuracy of vehicle poses is also significantly improved."
    },
    {
      "title": "Glance and Focus Reinforcement for Pan-cancer Screening",
      "url": "https://arxiv.org/abs/2601.19103",
      "date": "2026-01-27",
      "authors_text": "Linshan Wu, Jiaxin Zhuang, Hao Chen",
      "is_highlight": false,
      "score": 65.0,
      "summary": "GF-Screen introduces a novel reinforcement learning framework that enhances pan-cancer screening by effectively localizing and segmenting lesions in CT scans, outperforming existing methods.",
      "debug_abstract": "Pan-cancer screening in large-scale CT scans remains challenging for existing AI methods, primarily due to the difficulty of localizing diverse types of tiny lesions in large CT volumes. The extreme foreground-background imbalance significantly hinders models from focusing on diseased regions, while redundant focus on healthy regions not only decreases the efficiency but also increases false positives. Inspired by radiologists&#39; glance and focus diagnostic strategy, we introduce GF-Screen, a Glance and Focus reinforcement learning framework for pan-cancer screening. GF-Screen employs a Glance model to localize the diseased regions and a Focus model to precisely segment the lesions, where segmentation results of the Focus model are leveraged to reward the Glance model via Reinforcement Learning (RL). Specifically, the Glance model crops a group of sub-volumes from the entire CT volume and learns to select the sub-volumes with lesions for the Focus model to segment. Given that the selecting operation is non-differentiable for segmentation training, we propose to employ the segmentation results to reward the Glance model. To optimize the Glance model, we introduce a novel group relative learning paradigm, which employs group relative comparison to prioritize high-advantage predictions and discard low-advantage predictions within sub-volume groups, not only improving efficiency but also reducing false positives. In this way, for the first time, we effectively extend cutting-edge RL techniques to tackle the specific challenges in pan-cancer screening. Extensive experiments on 16 internal and 7 external datasets across 9 lesion types demonstrated the effectiveness of GF-Screen. Notably, GF-Screen leads the public validation leaderboard of MICCAI FLARE25 pan-cancer challenge, surpassing the FLARE24 champion solution by a large margin (+25.6% DSC and +28.2% NSD)."
    },
    {
      "title": "Fast and Safe Trajectory Optimization for Mobile Manipulators With Neural Configuration Space Distance Field",
      "url": "https://arxiv.org/abs/2601.18548",
      "date": "2026-01-27",
      "authors_text": "Yulin Li, Zhiyuan Song, Yiming Li, Zhicheng Song, Kai Chen",
      "is_highlight": false,
      "score": 92.0,
      "summary": "This paper presents Generalized Configuration Space Distance Fields (GCDF) for efficient trajectory optimization in mobile manipulators, enhancing collision reasoning and enabling agile motion in complex environments.",
      "debug_abstract": "Mobile manipulators promise agile, long-horizon behavior by coordinating base and arm motion, yet whole-body trajectory optimization in cluttered, confined spaces remains difficult due to high-dimensional nonconvexity and the need for fast, accurate collision reasoning. Configuration Space Distance Fields (CDF) enable fixed-base manipulators to model collisions directly in configuration space via smooth, implicit distances. This representation holds strong potential to bypass the nonlinear configuration-to-workspace mapping while preserving accurate whole-body geometry and providing optimization-friendly collision costs. Yet, extending this capability to mobile manipulators is hindered by unbounded workspaces and tighter base-arm coupling. We lift this promise to mobile manipulation with Generalized Configuration Space Distance Fields (GCDF), extending CDF to robots with both translational and rotational joints in unbounded workspaces with tighter base-arm coupling. We prove that GCDF preserves Euclidean-like local distance structure and accurately encodes whole-body geometry in configuration space, and develop a data generation and training pipeline that yields continuous neural GCDFs with accurate values and gradients, supporting efficient GPU-batched queries. Building on this representation, we develop a high-performance sequential convex optimization framework centered on GCDF-based collision reasoning. The solver scales to large numbers of implicit constraints through (i) online specification of neural constraints, (ii) sparsity-aware active-set detection with parallel batched evaluation across thousands of constraints, and (iii) incremental constraint management for rapid replanning under scene changes."
    }
  ],
  "Synteraction Lab": [
    {
      "title": "Video-KTR: Reinforcing Video Reasoning via Key Token Attribution",
      "url": "https://arxiv.org/abs/2601.19686",
      "date": "2026-01-27",
      "authors_text": "Ziyue Wang, Sheng Jin, Zhongrong Zuo, Jiawei Wu, Han Qiu",
      "is_highlight": false,
      "score": 66.0,
      "summary": "Video-KTR enhances video reasoning in multimodal models by selectively reinforcing key tokens through a novel attribution framework, achieving state-of-the-art performance across multiple benchmarks.",
      "debug_abstract": "Reinforcement learning (RL) has shown strong potential for enhancing reasoning in multimodal large language models, yet existing video reasoning methods often rely on coarse sequence-level rewards or single-factor token selection, neglecting fine-grained links among visual inputs, temporal dynamics, and linguistic outputs, limiting both accuracy and interpretability. We propose Video-KTR, a modality-aware policy shaping framework that performs selective, token-level RL by combining three attribution signals: (1) visual-aware tokens identified via counterfactual masking to reveal perceptual dependence; (2) temporal-aware tokens detected through frame shuffling to expose temporal sensitivity; and (3) high-entropy tokens signaling predictive uncertainty. By reinforcing only these key tokens, Video-KTR focuses learning on semantically informative, modality-sensitive content while filtering out low-value tokens. Across five challenging benchmarks, Video-KTR achieves state-of-the-art or highly competitive results, achieving 42.7\\% on Video-Holmes (surpassing GPT-4o) with consistent gains on both reasoning and general video understanding tasks. Ablation studies verify the complementary roles of the attribution signals and the robustness of targeted token-level updates. Overall, Video-KTR improves accuracy and interpretability, offering a simple, drop-in extension to RL for complex video reasoning. Our code and models are available at this https URL."
    },
    {
      "title": "Curiosity Driven Knowledge Retrieval for Mobile Agents",
      "url": "https://arxiv.org/abs/2601.19306",
      "date": "2026-01-27",
      "authors_text": "Sijia Li, Xiaoyu Tan, Shahir Ali, Niels Schmidt, Gengchen Ma",
      "is_highlight": false,
      "score": 80.0,
      "summary": "The paper presents a curiosity-driven framework for mobile agents that enhances performance by retrieving and integrating structured knowledge (AppCards) when faced with uncertainty, achieving improved planning reliability.",
      "debug_abstract": "Mobile agents have made progress toward reliable smartphone automation, yet performance in complex applications remains limited by incomplete knowledge and weak generalization to unseen environments. We introduce a curiosity driven knowledge retrieval framework that formalizes uncertainty during execution as a curiosity score. When this score exceeds a threshold, the system retrieves external information from documentation, code repositories, and historical trajectories. Retrieved content is organized into structured AppCards, which encode functional semantics, parameter conventions, interface mappings, and interaction patterns. During execution, an enhanced agent selectively integrates relevant AppCards into its reasoning process, thereby compensating for knowledge blind spots and improving planning reliability. Evaluation on the AndroidWorld benchmark shows consistent improvements across backbones, with an average gain of six percentage points and a new state of the art success rate of 88.8\\% when combined with GPT-5. Analysis indicates that AppCards are particularly effective for multi step and cross application tasks, while improvements depend on the backbone model. Case studies further confirm that AppCards reduce ambiguity, shorten exploration, and support stable execution trajectories. Task trajectories are publicly available at this https URL."
    },
    {
      "title": "Fast and Safe Trajectory Optimization for Mobile Manipulators With Neural Configuration Space Distance Field",
      "url": "https://arxiv.org/abs/2601.18548",
      "date": "2026-01-27",
      "authors_text": "Yulin Li, Zhiyuan Song, Yiming Li, Zhicheng Song, Kai Chen",
      "is_highlight": false,
      "score": 92.0,
      "summary": "This paper presents Generalized Configuration Space Distance Fields (GCDF) for efficient trajectory optimization in mobile manipulators, enhancing collision reasoning and enabling agile motion in complex environments.",
      "debug_abstract": "Mobile manipulators promise agile, long-horizon behavior by coordinating base and arm motion, yet whole-body trajectory optimization in cluttered, confined spaces remains difficult due to high-dimensional nonconvexity and the need for fast, accurate collision reasoning. Configuration Space Distance Fields (CDF) enable fixed-base manipulators to model collisions directly in configuration space via smooth, implicit distances. This representation holds strong potential to bypass the nonlinear configuration-to-workspace mapping while preserving accurate whole-body geometry and providing optimization-friendly collision costs. Yet, extending this capability to mobile manipulators is hindered by unbounded workspaces and tighter base-arm coupling. We lift this promise to mobile manipulation with Generalized Configuration Space Distance Fields (GCDF), extending CDF to robots with both translational and rotational joints in unbounded workspaces with tighter base-arm coupling. We prove that GCDF preserves Euclidean-like local distance structure and accurately encodes whole-body geometry in configuration space, and develop a data generation and training pipeline that yields continuous neural GCDFs with accurate values and gradients, supporting efficient GPU-batched queries. Building on this representation, we develop a high-performance sequential convex optimization framework centered on GCDF-based collision reasoning. The solver scales to large numbers of implicit constraints through (i) online specification of neural constraints, (ii) sparsity-aware active-set detection with parallel batched evaluation across thousands of constraints, and (iii) incremental constraint management for rapid replanning under scene changes."
    }
  ],
  "OpenDriveLab": [
    {
      "title": "EgoHandICL: Egocentric 3D Hand Reconstruction with In-Context Learning",
      "url": "https://arxiv.org/abs/2601.19850",
      "date": "2026-01-27",
      "authors_text": "Binzhu Xie, Shi Qiu, Sicheng Zhang, Yinqiao Wang, Hao Xu",
      "is_highlight": false,
      "score": 76.0,
      "summary": "EgoHandICL introduces an innovative in-context learning framework for robust 3D hand reconstruction in egocentric vision, enhancing performance through exemplar retrieval and multimodal context integration.",
      "debug_abstract": "Robust 3D hand reconstruction in egocentric vision is challenging due to depth ambiguity, self-occlusion, and complex hand-object interactions. Prior methods mitigate these issues by scaling training data or adding auxiliary cues, but they often struggle in unseen contexts. We present EgoHandICL, the first in-context learning (ICL) framework for 3D hand reconstruction that improves semantic alignment, visual consistency, and robustness under challenging egocentric conditions. EgoHandICL introduces complementary exemplar retrieval guided by vision-language models (VLMs), an ICL-tailored tokenizer for multimodal context, and a masked autoencoder (MAE)-based architecture trained with hand-guided geometric and perceptual objectives. Experiments on ARCTIC and EgoExo4D show consistent gains over state-of-the-art methods. We also demonstrate real-world generalization and improve EgoVLM hand-object interaction reasoning by using reconstructed hands as visual prompts. Code and data: this https URL"
    },
    {
      "title": "Component-Level Lesioning of Language Models Reveals Clinically Aligned Aphasia Phenotypes",
      "url": "https://arxiv.org/abs/2601.19723",
      "date": "2026-01-27",
      "authors_text": "Yifan Wang, Jichen Zheng, Jingyuan Sun, Yunhao Zhang, Chunyu Ye",
      "is_highlight": false,
      "score": 62.0,
      "summary": "This study presents a framework for simulating aphasia in language models by selectively perturbing components, revealing clinically relevant language impairments and enhancing understanding of language function degradation.",
      "debug_abstract": "Large language models (LLMs) increasingly exhibit human-like linguistic behaviors and internal representations that they could serve as computational simulators of language cognition. We ask whether LLMs can be systematically manipulated to reproduce language-production impairments characteristic of aphasia following focal brain lesions. Such models could provide scalable proxies for testing rehabilitation hypotheses, and offer a controlled framework for probing the functional organization of language. We introduce a clinically grounded, component-level framework that simulates aphasia by selectively perturbing functional components in LLMs, and apply it to both modular Mixture-of-Experts models and dense Transformers using a unified intervention interface. Our pipeline (i) identifies subtype-linked components for Broca&#39;s and Wernicke&#39;s aphasia, (ii) interprets these components with linguistic probing tasks, and (iii) induces graded impairments by progressively perturbing the top-k subtype-linked components, evaluating outcomes with Western Aphasia Battery (WAB) subtests summarized by Aphasia Quotient (AQ). Across architectures and lesioning strategies, subtype-targeted perturbations yield more systematic, aphasia-like regressions than size-matched random perturbations, and MoE modularity supports more localized and interpretable phenotype-to-component mappings. These findings suggest that modular LLMs, combined with clinically informed component perturbations, provide a promising platform for simulating aphasic language production and studying how distinct language functions degrade under targeted disruptions."
    },
    {
      "title": "Rhombot: Rhombus-shaped Modular Robots for Stable, Medium-Independent Reconfiguration Motion",
      "url": "https://arxiv.org/abs/2601.19529",
      "date": "2026-01-27",
      "authors_text": "Jie Gu, Yirui Sun, Zhihao Xia, Tin Lun Lam, Chunxu Tian",
      "is_highlight": false,
      "score": 86.0,
      "summary": "Rhombot is a modular self-reconfigurable robot featuring rhombus-shaped modules that enable stable, medium-independent morphing and locomotion through a novel motion primitive called morphpivoting.",
      "debug_abstract": "In this paper, we present Rhombot, a novel deformable planar lattice modular self-reconfigurable robot (MSRR) with a rhombus shaped module. Each module consists of a parallelogram skeleton with a single centrally mounted actuator that enables folding and unfolding along its diagonal. The core design philosophy is to achieve essential MSRR functionalities such as morphing, docking, and locomotion with minimal control complexity. This enables a continuous and stable reconfiguration process that is independent of the surrounding medium, allowing the system to reliably form various configurations in diverse environments. To leverage the unique kinematics of Rhombot, we introduce morphpivoting, a novel motion primitive for reconfiguration that differs from advanced MSRR systems, and propose a strategy for its continuous execution. Finally, a series of physical experiments validate the module&#39;s stable reconfiguration ability, as well as its positional and docking accuracy."
    },
    {
      "title": "Fast and Safe Trajectory Optimization for Mobile Manipulators With Neural Configuration Space Distance Field",
      "url": "https://arxiv.org/abs/2601.18548",
      "date": "2026-01-27",
      "authors_text": "Yulin Li, Zhiyuan Song, Yiming Li, Zhicheng Song, Kai Chen",
      "is_highlight": false,
      "score": 92.0,
      "summary": "This paper presents Generalized Configuration Space Distance Fields (GCDF) for efficient trajectory optimization in mobile manipulators, enhancing collision reasoning and enabling agile motion in complex environments.",
      "debug_abstract": "Mobile manipulators promise agile, long-horizon behavior by coordinating base and arm motion, yet whole-body trajectory optimization in cluttered, confined spaces remains difficult due to high-dimensional nonconvexity and the need for fast, accurate collision reasoning. Configuration Space Distance Fields (CDF) enable fixed-base manipulators to model collisions directly in configuration space via smooth, implicit distances. This representation holds strong potential to bypass the nonlinear configuration-to-workspace mapping while preserving accurate whole-body geometry and providing optimization-friendly collision costs. Yet, extending this capability to mobile manipulators is hindered by unbounded workspaces and tighter base-arm coupling. We lift this promise to mobile manipulation with Generalized Configuration Space Distance Fields (GCDF), extending CDF to robots with both translational and rotational joints in unbounded workspaces with tighter base-arm coupling. We prove that GCDF preserves Euclidean-like local distance structure and accurately encodes whole-body geometry in configuration space, and develop a data generation and training pipeline that yields continuous neural GCDFs with accurate values and gradients, supporting efficient GPU-batched queries. Building on this representation, we develop a high-performance sequential convex optimization framework centered on GCDF-based collision reasoning. The solver scales to large numbers of implicit constraints through (i) online specification of neural constraints, (ii) sparsity-aware active-set detection with parallel batched evaluation across thousands of constraints, and (iii) incremental constraint management for rapid replanning under scene changes."
    }
  ],
  "潘佳老师实验室": [
    {
      "title": "EgoHandICL: Egocentric 3D Hand Reconstruction with In-Context Learning",
      "url": "https://arxiv.org/abs/2601.19850",
      "date": "2026-01-27",
      "authors_text": "Binzhu Xie, Shi Qiu, Sicheng Zhang, Yinqiao Wang, Hao Xu",
      "is_highlight": false,
      "score": 76.0,
      "summary": "EgoHandICL introduces an innovative in-context learning framework for robust 3D hand reconstruction in egocentric vision, enhancing performance through exemplar retrieval and multimodal context integration.",
      "debug_abstract": "Robust 3D hand reconstruction in egocentric vision is challenging due to depth ambiguity, self-occlusion, and complex hand-object interactions. Prior methods mitigate these issues by scaling training data or adding auxiliary cues, but they often struggle in unseen contexts. We present EgoHandICL, the first in-context learning (ICL) framework for 3D hand reconstruction that improves semantic alignment, visual consistency, and robustness under challenging egocentric conditions. EgoHandICL introduces complementary exemplar retrieval guided by vision-language models (VLMs), an ICL-tailored tokenizer for multimodal context, and a masked autoencoder (MAE)-based architecture trained with hand-guided geometric and perceptual objectives. Experiments on ARCTIC and EgoExo4D show consistent gains over state-of-the-art methods. We also demonstrate real-world generalization and improve EgoVLM hand-object interaction reasoning by using reconstructed hands as visual prompts. Code and data: this https URL"
    },
    {
      "title": "Component-Level Lesioning of Language Models Reveals Clinically Aligned Aphasia Phenotypes",
      "url": "https://arxiv.org/abs/2601.19723",
      "date": "2026-01-27",
      "authors_text": "Yifan Wang, Jichen Zheng, Jingyuan Sun, Yunhao Zhang, Chunyu Ye",
      "is_highlight": false,
      "score": 62.0,
      "summary": "This study presents a framework for simulating aphasia in language models by selectively perturbing components, revealing clinically relevant language impairments and enhancing understanding of language function degradation.",
      "debug_abstract": "Large language models (LLMs) increasingly exhibit human-like linguistic behaviors and internal representations that they could serve as computational simulators of language cognition. We ask whether LLMs can be systematically manipulated to reproduce language-production impairments characteristic of aphasia following focal brain lesions. Such models could provide scalable proxies for testing rehabilitation hypotheses, and offer a controlled framework for probing the functional organization of language. We introduce a clinically grounded, component-level framework that simulates aphasia by selectively perturbing functional components in LLMs, and apply it to both modular Mixture-of-Experts models and dense Transformers using a unified intervention interface. Our pipeline (i) identifies subtype-linked components for Broca&#39;s and Wernicke&#39;s aphasia, (ii) interprets these components with linguistic probing tasks, and (iii) induces graded impairments by progressively perturbing the top-k subtype-linked components, evaluating outcomes with Western Aphasia Battery (WAB) subtests summarized by Aphasia Quotient (AQ). Across architectures and lesioning strategies, subtype-targeted perturbations yield more systematic, aphasia-like regressions than size-matched random perturbations, and MoE modularity supports more localized and interpretable phenotype-to-component mappings. These findings suggest that modular LLMs, combined with clinically informed component perturbations, provide a promising platform for simulating aphasic language production and studying how distinct language functions degrade under targeted disruptions."
    },
    {
      "title": "Rhombot: Rhombus-shaped Modular Robots for Stable, Medium-Independent Reconfiguration Motion",
      "url": "https://arxiv.org/abs/2601.19529",
      "date": "2026-01-27",
      "authors_text": "Jie Gu, Yirui Sun, Zhihao Xia, Tin Lun Lam, Chunxu Tian",
      "is_highlight": false,
      "score": 86.0,
      "summary": "Rhombot is a modular self-reconfigurable robot featuring rhombus-shaped modules that enable stable, medium-independent morphing and locomotion through a novel motion primitive called morphpivoting.",
      "debug_abstract": "In this paper, we present Rhombot, a novel deformable planar lattice modular self-reconfigurable robot (MSRR) with a rhombus shaped module. Each module consists of a parallelogram skeleton with a single centrally mounted actuator that enables folding and unfolding along its diagonal. The core design philosophy is to achieve essential MSRR functionalities such as morphing, docking, and locomotion with minimal control complexity. This enables a continuous and stable reconfiguration process that is independent of the surrounding medium, allowing the system to reliably form various configurations in diverse environments. To leverage the unique kinematics of Rhombot, we introduce morphpivoting, a novel motion primitive for reconfiguration that differs from advanced MSRR systems, and propose a strategy for its continuous execution. Finally, a series of physical experiments validate the module&#39;s stable reconfiguration ability, as well as its positional and docking accuracy."
    },
    {
      "title": "Fast and Safe Trajectory Optimization for Mobile Manipulators With Neural Configuration Space Distance Field",
      "url": "https://arxiv.org/abs/2601.18548",
      "date": "2026-01-27",
      "authors_text": "Yulin Li, Zhiyuan Song, Yiming Li, Zhicheng Song, Kai Chen",
      "is_highlight": false,
      "score": 92.0,
      "summary": "This paper presents Generalized Configuration Space Distance Fields (GCDF) for efficient trajectory optimization in mobile manipulators, enhancing collision reasoning and enabling agile motion in complex environments.",
      "debug_abstract": "Mobile manipulators promise agile, long-horizon behavior by coordinating base and arm motion, yet whole-body trajectory optimization in cluttered, confined spaces remains difficult due to high-dimensional nonconvexity and the need for fast, accurate collision reasoning. Configuration Space Distance Fields (CDF) enable fixed-base manipulators to model collisions directly in configuration space via smooth, implicit distances. This representation holds strong potential to bypass the nonlinear configuration-to-workspace mapping while preserving accurate whole-body geometry and providing optimization-friendly collision costs. Yet, extending this capability to mobile manipulators is hindered by unbounded workspaces and tighter base-arm coupling. We lift this promise to mobile manipulation with Generalized Configuration Space Distance Fields (GCDF), extending CDF to robots with both translational and rotational joints in unbounded workspaces with tighter base-arm coupling. We prove that GCDF preserves Euclidean-like local distance structure and accurately encodes whole-body geometry in configuration space, and develop a data generation and training pipeline that yields continuous neural GCDFs with accurate values and gradients, supporting efficient GPU-batched queries. Building on this representation, we develop a high-performance sequential convex optimization framework centered on GCDF-based collision reasoning. The solver scales to large numbers of implicit constraints through (i) online specification of neural constraints, (ii) sparsity-aware active-set detection with parallel batched evaluation across thousands of constraints, and (iii) incremental constraint management for rapid replanning under scene changes."
    }
  ],
  "机器人研究所 (CKSRI)": [
    {
      "title": "Enhancing Inverse Perspective Mapping for Automatic Vectorized Road Map Generation",
      "url": "https://arxiv.org/abs/2601.19536",
      "date": "2026-01-27",
      "authors_text": "Hongji Liu, Linwei Zheng, Yongjian Li, Mingkai Tang, Xiaoyang Yan",
      "is_highlight": false,
      "score": 70.0,
      "summary": "This study introduces a cost-effective framework using enhanced inverse perspective mapping and Catmull-Rom splines for high-precision, automatic vectorized road map generation.",
      "debug_abstract": "In this study, we present a low-cost and unified framework for vectorized road mapping leveraging enhanced inverse perspective mapping (IPM). In this framework, Catmull-Rom splines are utilized to characterize lane lines, and all the other ground markings are depicted using polygons uniformly. The results from instance segmentation serve as references to refine the three-dimensional position of spline control points and polygon corner points. In conjunction with this process, the homography matrix of IPM and vehicle poses are optimized simultaneously. Our proposed framework significantly reduces the mapping errors associated with IPM. It also improves the accuracy of the initial IPM homography matrix and the predicted vehicle poses. Furthermore, it addresses the limitations imposed by the coplanarity assumption in IPM. These enhancements enable IPM to be effectively applied to vectorized road mapping, which serves a cost-effective solution with enhanced accuracy. In addition, our framework generalizes road map elements to include all common ground markings and lane lines. The proposed framework is evaluated in two different practical scenarios, and the test results show that our method can automatically generate high-precision maps with near-centimeter-level accuracy. Importantly, the optimized IPM matrix achieves an accuracy comparable to that of manual calibration, while the accuracy of vehicle poses is also significantly improved."
    },
    {
      "title": "Glance and Focus Reinforcement for Pan-cancer Screening",
      "url": "https://arxiv.org/abs/2601.19103",
      "date": "2026-01-27",
      "authors_text": "Linshan Wu, Jiaxin Zhuang, Hao Chen",
      "is_highlight": false,
      "score": 65.0,
      "summary": "GF-Screen introduces a novel reinforcement learning framework that enhances pan-cancer screening by effectively localizing and segmenting lesions in CT scans, outperforming existing methods.",
      "debug_abstract": "Pan-cancer screening in large-scale CT scans remains challenging for existing AI methods, primarily due to the difficulty of localizing diverse types of tiny lesions in large CT volumes. The extreme foreground-background imbalance significantly hinders models from focusing on diseased regions, while redundant focus on healthy regions not only decreases the efficiency but also increases false positives. Inspired by radiologists&#39; glance and focus diagnostic strategy, we introduce GF-Screen, a Glance and Focus reinforcement learning framework for pan-cancer screening. GF-Screen employs a Glance model to localize the diseased regions and a Focus model to precisely segment the lesions, where segmentation results of the Focus model are leveraged to reward the Glance model via Reinforcement Learning (RL). Specifically, the Glance model crops a group of sub-volumes from the entire CT volume and learns to select the sub-volumes with lesions for the Focus model to segment. Given that the selecting operation is non-differentiable for segmentation training, we propose to employ the segmentation results to reward the Glance model. To optimize the Glance model, we introduce a novel group relative learning paradigm, which employs group relative comparison to prioritize high-advantage predictions and discard low-advantage predictions within sub-volume groups, not only improving efficiency but also reducing false positives. In this way, for the first time, we effectively extend cutting-edge RL techniques to tackle the specific challenges in pan-cancer screening. Extensive experiments on 16 internal and 7 external datasets across 9 lesion types demonstrated the effectiveness of GF-Screen. Notably, GF-Screen leads the public validation leaderboard of MICCAI FLARE25 pan-cancer challenge, surpassing the FLARE24 champion solution by a large margin (+25.6% DSC and +28.2% NSD)."
    },
    {
      "title": "Fast and Safe Trajectory Optimization for Mobile Manipulators With Neural Configuration Space Distance Field",
      "url": "https://arxiv.org/abs/2601.18548",
      "date": "2026-01-27",
      "authors_text": "Yulin Li, Zhiyuan Song, Yiming Li, Zhicheng Song, Kai Chen",
      "is_highlight": false,
      "score": 92.0,
      "summary": "This paper presents Generalized Configuration Space Distance Fields (GCDF) for efficient trajectory optimization in mobile manipulators, enhancing collision reasoning and enabling agile motion in complex environments.",
      "debug_abstract": "Mobile manipulators promise agile, long-horizon behavior by coordinating base and arm motion, yet whole-body trajectory optimization in cluttered, confined spaces remains difficult due to high-dimensional nonconvexity and the need for fast, accurate collision reasoning. Configuration Space Distance Fields (CDF) enable fixed-base manipulators to model collisions directly in configuration space via smooth, implicit distances. This representation holds strong potential to bypass the nonlinear configuration-to-workspace mapping while preserving accurate whole-body geometry and providing optimization-friendly collision costs. Yet, extending this capability to mobile manipulators is hindered by unbounded workspaces and tighter base-arm coupling. We lift this promise to mobile manipulation with Generalized Configuration Space Distance Fields (GCDF), extending CDF to robots with both translational and rotational joints in unbounded workspaces with tighter base-arm coupling. We prove that GCDF preserves Euclidean-like local distance structure and accurately encodes whole-body geometry in configuration space, and develop a data generation and training pipeline that yields continuous neural GCDFs with accurate values and gradients, supporting efficient GPU-batched queries. Building on this representation, we develop a high-performance sequential convex optimization framework centered on GCDF-based collision reasoning. The solver scales to large numbers of implicit constraints through (i) online specification of neural constraints, (ii) sparsity-aware active-set detection with parallel batched evaluation across thousands of constraints, and (iii) incremental constraint management for rapid replanning under scene changes."
    }
  ],
  "Microsystem Engineering and Robotics": [
    {
      "title": "Video-KTR: Reinforcing Video Reasoning via Key Token Attribution",
      "url": "https://arxiv.org/abs/2601.19686",
      "date": "2026-01-27",
      "authors_text": "Ziyue Wang, Sheng Jin, Zhongrong Zuo, Jiawei Wu, Han Qiu",
      "is_highlight": false,
      "score": 66.0,
      "summary": "Video-KTR enhances video reasoning in multimodal models by selectively reinforcing key tokens through a novel attribution framework, achieving state-of-the-art performance across multiple benchmarks.",
      "debug_abstract": "Reinforcement learning (RL) has shown strong potential for enhancing reasoning in multimodal large language models, yet existing video reasoning methods often rely on coarse sequence-level rewards or single-factor token selection, neglecting fine-grained links among visual inputs, temporal dynamics, and linguistic outputs, limiting both accuracy and interpretability. We propose Video-KTR, a modality-aware policy shaping framework that performs selective, token-level RL by combining three attribution signals: (1) visual-aware tokens identified via counterfactual masking to reveal perceptual dependence; (2) temporal-aware tokens detected through frame shuffling to expose temporal sensitivity; and (3) high-entropy tokens signaling predictive uncertainty. By reinforcing only these key tokens, Video-KTR focuses learning on semantically informative, modality-sensitive content while filtering out low-value tokens. Across five challenging benchmarks, Video-KTR achieves state-of-the-art or highly competitive results, achieving 42.7\\% on Video-Holmes (surpassing GPT-4o) with consistent gains on both reasoning and general video understanding tasks. Ablation studies verify the complementary roles of the attribution signals and the robustness of targeted token-level updates. Overall, Video-KTR improves accuracy and interpretability, offering a simple, drop-in extension to RL for complex video reasoning. Our code and models are available at this https URL."
    },
    {
      "title": "Curiosity Driven Knowledge Retrieval for Mobile Agents",
      "url": "https://arxiv.org/abs/2601.19306",
      "date": "2026-01-27",
      "authors_text": "Sijia Li, Xiaoyu Tan, Shahir Ali, Niels Schmidt, Gengchen Ma",
      "is_highlight": false,
      "score": 80.0,
      "summary": "The paper presents a curiosity-driven framework for mobile agents that enhances performance by retrieving and integrating structured knowledge (AppCards) when faced with uncertainty, achieving improved planning reliability.",
      "debug_abstract": "Mobile agents have made progress toward reliable smartphone automation, yet performance in complex applications remains limited by incomplete knowledge and weak generalization to unseen environments. We introduce a curiosity driven knowledge retrieval framework that formalizes uncertainty during execution as a curiosity score. When this score exceeds a threshold, the system retrieves external information from documentation, code repositories, and historical trajectories. Retrieved content is organized into structured AppCards, which encode functional semantics, parameter conventions, interface mappings, and interaction patterns. During execution, an enhanced agent selectively integrates relevant AppCards into its reasoning process, thereby compensating for knowledge blind spots and improving planning reliability. Evaluation on the AndroidWorld benchmark shows consistent improvements across backbones, with an average gain of six percentage points and a new state of the art success rate of 88.8\\% when combined with GPT-5. Analysis indicates that AppCards are particularly effective for multi step and cross application tasks, while improvements depend on the backbone model. Case studies further confirm that AppCards reduce ambiguity, shorten exploration, and support stable execution trajectories. Task trajectories are publicly available at this https URL."
    },
    {
      "title": "Fast and Safe Trajectory Optimization for Mobile Manipulators With Neural Configuration Space Distance Field",
      "url": "https://arxiv.org/abs/2601.18548",
      "date": "2026-01-27",
      "authors_text": "Yulin Li, Zhiyuan Song, Yiming Li, Zhicheng Song, Kai Chen",
      "is_highlight": false,
      "score": 92.0,
      "summary": "This paper presents Generalized Configuration Space Distance Fields (GCDF) for efficient trajectory optimization in mobile manipulators, enhancing collision reasoning and enabling agile motion in complex environments.",
      "debug_abstract": "Mobile manipulators promise agile, long-horizon behavior by coordinating base and arm motion, yet whole-body trajectory optimization in cluttered, confined spaces remains difficult due to high-dimensional nonconvexity and the need for fast, accurate collision reasoning. Configuration Space Distance Fields (CDF) enable fixed-base manipulators to model collisions directly in configuration space via smooth, implicit distances. This representation holds strong potential to bypass the nonlinear configuration-to-workspace mapping while preserving accurate whole-body geometry and providing optimization-friendly collision costs. Yet, extending this capability to mobile manipulators is hindered by unbounded workspaces and tighter base-arm coupling. We lift this promise to mobile manipulation with Generalized Configuration Space Distance Fields (GCDF), extending CDF to robots with both translational and rotational joints in unbounded workspaces with tighter base-arm coupling. We prove that GCDF preserves Euclidean-like local distance structure and accurately encodes whole-body geometry in configuration space, and develop a data generation and training pipeline that yields continuous neural GCDFs with accurate values and gradients, supporting efficient GPU-batched queries. Building on this representation, we develop a high-performance sequential convex optimization framework centered on GCDF-based collision reasoning. The solver scales to large numbers of implicit constraints through (i) online specification of neural constraints, (ii) sparsity-aware active-set detection with parallel batched evaluation across thousands of constraints, and (iii) incremental constraint management for rapid replanning under scene changes."
    }
  ],
  "香港大学机械工程系机器人实验室": [
    {
      "title": "EgoHandICL: Egocentric 3D Hand Reconstruction with In-Context Learning",
      "url": "https://arxiv.org/abs/2601.19850",
      "date": "2026-01-27",
      "authors_text": "Binzhu Xie, Shi Qiu, Sicheng Zhang, Yinqiao Wang, Hao Xu",
      "is_highlight": false,
      "score": 76.0,
      "summary": "EgoHandICL introduces an innovative in-context learning framework for robust 3D hand reconstruction in egocentric vision, enhancing performance through exemplar retrieval and multimodal context integration.",
      "debug_abstract": "Robust 3D hand reconstruction in egocentric vision is challenging due to depth ambiguity, self-occlusion, and complex hand-object interactions. Prior methods mitigate these issues by scaling training data or adding auxiliary cues, but they often struggle in unseen contexts. We present EgoHandICL, the first in-context learning (ICL) framework for 3D hand reconstruction that improves semantic alignment, visual consistency, and robustness under challenging egocentric conditions. EgoHandICL introduces complementary exemplar retrieval guided by vision-language models (VLMs), an ICL-tailored tokenizer for multimodal context, and a masked autoencoder (MAE)-based architecture trained with hand-guided geometric and perceptual objectives. Experiments on ARCTIC and EgoExo4D show consistent gains over state-of-the-art methods. We also demonstrate real-world generalization and improve EgoVLM hand-object interaction reasoning by using reconstructed hands as visual prompts. Code and data: this https URL"
    },
    {
      "title": "Component-Level Lesioning of Language Models Reveals Clinically Aligned Aphasia Phenotypes",
      "url": "https://arxiv.org/abs/2601.19723",
      "date": "2026-01-27",
      "authors_text": "Yifan Wang, Jichen Zheng, Jingyuan Sun, Yunhao Zhang, Chunyu Ye",
      "is_highlight": false,
      "score": 62.0,
      "summary": "This study presents a framework for simulating aphasia in language models by selectively perturbing components, revealing clinically relevant language impairments and enhancing understanding of language function degradation.",
      "debug_abstract": "Large language models (LLMs) increasingly exhibit human-like linguistic behaviors and internal representations that they could serve as computational simulators of language cognition. We ask whether LLMs can be systematically manipulated to reproduce language-production impairments characteristic of aphasia following focal brain lesions. Such models could provide scalable proxies for testing rehabilitation hypotheses, and offer a controlled framework for probing the functional organization of language. We introduce a clinically grounded, component-level framework that simulates aphasia by selectively perturbing functional components in LLMs, and apply it to both modular Mixture-of-Experts models and dense Transformers using a unified intervention interface. Our pipeline (i) identifies subtype-linked components for Broca&#39;s and Wernicke&#39;s aphasia, (ii) interprets these components with linguistic probing tasks, and (iii) induces graded impairments by progressively perturbing the top-k subtype-linked components, evaluating outcomes with Western Aphasia Battery (WAB) subtests summarized by Aphasia Quotient (AQ). Across architectures and lesioning strategies, subtype-targeted perturbations yield more systematic, aphasia-like regressions than size-matched random perturbations, and MoE modularity supports more localized and interpretable phenotype-to-component mappings. These findings suggest that modular LLMs, combined with clinically informed component perturbations, provide a promising platform for simulating aphasic language production and studying how distinct language functions degrade under targeted disruptions."
    },
    {
      "title": "Rhombot: Rhombus-shaped Modular Robots for Stable, Medium-Independent Reconfiguration Motion",
      "url": "https://arxiv.org/abs/2601.19529",
      "date": "2026-01-27",
      "authors_text": "Jie Gu, Yirui Sun, Zhihao Xia, Tin Lun Lam, Chunxu Tian",
      "is_highlight": false,
      "score": 86.0,
      "summary": "Rhombot is a modular self-reconfigurable robot featuring rhombus-shaped modules that enable stable, medium-independent morphing and locomotion through a novel motion primitive called morphpivoting.",
      "debug_abstract": "In this paper, we present Rhombot, a novel deformable planar lattice modular self-reconfigurable robot (MSRR) with a rhombus shaped module. Each module consists of a parallelogram skeleton with a single centrally mounted actuator that enables folding and unfolding along its diagonal. The core design philosophy is to achieve essential MSRR functionalities such as morphing, docking, and locomotion with minimal control complexity. This enables a continuous and stable reconfiguration process that is independent of the surrounding medium, allowing the system to reliably form various configurations in diverse environments. To leverage the unique kinematics of Rhombot, we introduce morphpivoting, a novel motion primitive for reconfiguration that differs from advanced MSRR systems, and propose a strategy for its continuous execution. Finally, a series of physical experiments validate the module&#39;s stable reconfiguration ability, as well as its positional and docking accuracy."
    },
    {
      "title": "Fast and Safe Trajectory Optimization for Mobile Manipulators With Neural Configuration Space Distance Field",
      "url": "https://arxiv.org/abs/2601.18548",
      "date": "2026-01-27",
      "authors_text": "Yulin Li, Zhiyuan Song, Yiming Li, Zhicheng Song, Kai Chen",
      "is_highlight": false,
      "score": 92.0,
      "summary": "This paper presents Generalized Configuration Space Distance Fields (GCDF) for efficient trajectory optimization in mobile manipulators, enhancing collision reasoning and enabling agile motion in complex environments.",
      "debug_abstract": "Mobile manipulators promise agile, long-horizon behavior by coordinating base and arm motion, yet whole-body trajectory optimization in cluttered, confined spaces remains difficult due to high-dimensional nonconvexity and the need for fast, accurate collision reasoning. Configuration Space Distance Fields (CDF) enable fixed-base manipulators to model collisions directly in configuration space via smooth, implicit distances. This representation holds strong potential to bypass the nonlinear configuration-to-workspace mapping while preserving accurate whole-body geometry and providing optimization-friendly collision costs. Yet, extending this capability to mobile manipulators is hindered by unbounded workspaces and tighter base-arm coupling. We lift this promise to mobile manipulation with Generalized Configuration Space Distance Fields (GCDF), extending CDF to robots with both translational and rotational joints in unbounded workspaces with tighter base-arm coupling. We prove that GCDF preserves Euclidean-like local distance structure and accurately encodes whole-body geometry in configuration space, and develop a data generation and training pipeline that yields continuous neural GCDFs with accurate values and gradients, supporting efficient GPU-batched queries. Building on this representation, we develop a high-performance sequential convex optimization framework centered on GCDF-based collision reasoning. The solver scales to large numbers of implicit constraints through (i) online specification of neural constraints, (ii) sparsity-aware active-set detection with parallel batched evaluation across thousands of constraints, and (iii) incremental constraint management for rapid replanning under scene changes."
    }
  ],
  "集群机器人系统实验室 (MagicLab)": [
    {
      "title": "Rhombot: Rhombus-shaped Modular Robots for Stable, Medium-Independent Reconfiguration Motion",
      "url": "https://arxiv.org/abs/2601.19529",
      "date": "2026-01-27",
      "authors_text": "Jie Gu, Yirui Sun, Zhihao Xia, Tin Lun Lam, Chunxu Tian",
      "is_highlight": false,
      "score": 86.0,
      "summary": "Rhombot is a modular self-reconfigurable robot featuring rhombus-shaped modules that enable stable, medium-independent morphing and locomotion through a novel motion primitive called morphpivoting.",
      "debug_abstract": "In this paper, we present Rhombot, a novel deformable planar lattice modular self-reconfigurable robot (MSRR) with a rhombus shaped module. Each module consists of a parallelogram skeleton with a single centrally mounted actuator that enables folding and unfolding along its diagonal. The core design philosophy is to achieve essential MSRR functionalities such as morphing, docking, and locomotion with minimal control complexity. This enables a continuous and stable reconfiguration process that is independent of the surrounding medium, allowing the system to reliably form various configurations in diverse environments. To leverage the unique kinematics of Rhombot, we introduce morphpivoting, a novel motion primitive for reconfiguration that differs from advanced MSRR systems, and propose a strategy for its continuous execution. Finally, a series of physical experiments validate the module&#39;s stable reconfiguration ability, as well as its positional and docking accuracy."
    },
    {
      "title": "Self-Reconfiguration Planning for Deformable Quadrilateral Modular Robots",
      "url": "https://arxiv.org/abs/2601.19496",
      "date": "2026-01-27",
      "authors_text": "Jie Gu, Hongrun Gao, Zhihao Xia, Yirun Sun, Chunxu Tian",
      "is_highlight": false,
      "score": 91.0,
      "summary": "This paper introduces a self-reconfiguration planning algorithm for deformable quadrilateral modular robots that ensures stable connections and efficient execution sequences through a novel graph-based approach.",
      "debug_abstract": "For lattice modular self-reconfigurable robots (MSRRs), maintaining stable connections during reconfiguration is crucial for physical feasibility and deployability. This letter presents a novel self-reconfiguration planning algorithm for deformable quadrilateral MSRRs that guarantees stable connection. The method first constructs feasible connect/disconnect actions using a virtual graph representation, and then organizes these actions into a valid execution sequence through a Dependence-based Reverse Tree (DRTree) that resolves interdependencies. We also prove that reconfiguration sequences satisfying motion characteristics exist for any pair of configurations with seven or more modules (excluding linear topologies). Finally, comparisons with a modified BiRRT algorithm highlight the superior efficiency and stability of our approach, while deployment on a physical robotic platform confirms its practical feasibility."
    },
    {
      "title": "MAGNET: Towards Adaptive GUI Agents with Memory-Driven Knowledge Evolution",
      "url": "https://arxiv.org/abs/2601.19199",
      "date": "2026-01-27",
      "authors_text": "Libo Sun, Jiwen Zhang, Siyuan Wang, Zhongyu Wei",
      "is_highlight": false,
      "score": 75.0,
      "summary": "MAGNET introduces a memory-driven adaptive agent framework that enhances mobile GUI agents' performance by linking visual features to stable semantics and refining task intents amidst UI changes.",
      "debug_abstract": "Mobile GUI agents powered by large foundation models enable autonomous task execution, but frequent updates altering UI appearance and reorganizing workflows cause agents trained on historical data to fail. Despite surface changes, functional semantics and task intents remain fundamentally stable. Building on this insight, we introduce MAGNET, a memory-driven adaptive agent framework with dual-level memory: stationary memory linking diverse visual features to stable functional semantics for robust action grounding and procedural memory capturing stable task intents across varying workflows. We propose a dynamic memory evolution mechanism that continuously refines both memories by prioritizing frequently accessed knowledge. Online benchmark AndroidWorld evaluations show substantial improvements over baselines, while offline benchmarks confirm consistent gains under distribution shifts. These results validate that leveraging stable structures across interface changes improves agent performance and generalization in evolving software environments."
    }
  ],
  "智能人机交互实验室 (MemX)": [
    {
      "title": "Rhombot: Rhombus-shaped Modular Robots for Stable, Medium-Independent Reconfiguration Motion",
      "url": "https://arxiv.org/abs/2601.19529",
      "date": "2026-01-27",
      "authors_text": "Jie Gu, Yirui Sun, Zhihao Xia, Tin Lun Lam, Chunxu Tian",
      "is_highlight": false,
      "score": 86.0,
      "summary": "Rhombot is a modular self-reconfigurable robot featuring rhombus-shaped modules that enable stable, medium-independent morphing and locomotion through a novel motion primitive called morphpivoting.",
      "debug_abstract": "In this paper, we present Rhombot, a novel deformable planar lattice modular self-reconfigurable robot (MSRR) with a rhombus shaped module. Each module consists of a parallelogram skeleton with a single centrally mounted actuator that enables folding and unfolding along its diagonal. The core design philosophy is to achieve essential MSRR functionalities such as morphing, docking, and locomotion with minimal control complexity. This enables a continuous and stable reconfiguration process that is independent of the surrounding medium, allowing the system to reliably form various configurations in diverse environments. To leverage the unique kinematics of Rhombot, we introduce morphpivoting, a novel motion primitive for reconfiguration that differs from advanced MSRR systems, and propose a strategy for its continuous execution. Finally, a series of physical experiments validate the module&#39;s stable reconfiguration ability, as well as its positional and docking accuracy."
    },
    {
      "title": "Self-Reconfiguration Planning for Deformable Quadrilateral Modular Robots",
      "url": "https://arxiv.org/abs/2601.19496",
      "date": "2026-01-27",
      "authors_text": "Jie Gu, Hongrun Gao, Zhihao Xia, Yirun Sun, Chunxu Tian",
      "is_highlight": false,
      "score": 91.0,
      "summary": "This paper introduces a self-reconfiguration planning algorithm for deformable quadrilateral modular robots that ensures stable connections and efficient execution sequences through a novel graph-based approach.",
      "debug_abstract": "For lattice modular self-reconfigurable robots (MSRRs), maintaining stable connections during reconfiguration is crucial for physical feasibility and deployability. This letter presents a novel self-reconfiguration planning algorithm for deformable quadrilateral MSRRs that guarantees stable connection. The method first constructs feasible connect/disconnect actions using a virtual graph representation, and then organizes these actions into a valid execution sequence through a Dependence-based Reverse Tree (DRTree) that resolves interdependencies. We also prove that reconfiguration sequences satisfying motion characteristics exist for any pair of configurations with seven or more modules (excluding linear topologies). Finally, comparisons with a modified BiRRT algorithm highlight the superior efficiency and stability of our approach, while deployment on a physical robotic platform confirms its practical feasibility."
    },
    {
      "title": "MAGNET: Towards Adaptive GUI Agents with Memory-Driven Knowledge Evolution",
      "url": "https://arxiv.org/abs/2601.19199",
      "date": "2026-01-27",
      "authors_text": "Libo Sun, Jiwen Zhang, Siyuan Wang, Zhongyu Wei",
      "is_highlight": false,
      "score": 75.0,
      "summary": "MAGNET introduces a memory-driven adaptive agent framework that enhances mobile GUI agents' performance by linking visual features to stable semantics and refining task intents amidst UI changes.",
      "debug_abstract": "Mobile GUI agents powered by large foundation models enable autonomous task execution, but frequent updates altering UI appearance and reorganizing workflows cause agents trained on historical data to fail. Despite surface changes, functional semantics and task intents remain fundamentally stable. Building on this insight, we introduce MAGNET, a memory-driven adaptive agent framework with dual-level memory: stationary memory linking diverse visual features to stable functional semantics for robust action grounding and procedural memory capturing stable task intents across varying workflows. We propose a dynamic memory evolution mechanism that continuously refines both memories by prioritizing frequently accessed knowledge. Online benchmark AndroidWorld evaluations show substantial improvements over baselines, while offline benchmarks confirm consistent gains under distribution shifts. These results validate that leveraging stable structures across interface changes improves agent performance and generalization in evolving software environments."
    }
  ],
  "智能感知与无人系统实验室": [
    {
      "title": "Rhombot: Rhombus-shaped Modular Robots for Stable, Medium-Independent Reconfiguration Motion",
      "url": "https://arxiv.org/abs/2601.19529",
      "date": "2026-01-27",
      "authors_text": "Jie Gu, Yirui Sun, Zhihao Xia, Tin Lun Lam, Chunxu Tian",
      "is_highlight": false,
      "score": 86.0,
      "summary": "Rhombot is a modular self-reconfigurable robot featuring rhombus-shaped modules that enable stable, medium-independent morphing and locomotion through a novel motion primitive called morphpivoting.",
      "debug_abstract": "In this paper, we present Rhombot, a novel deformable planar lattice modular self-reconfigurable robot (MSRR) with a rhombus shaped module. Each module consists of a parallelogram skeleton with a single centrally mounted actuator that enables folding and unfolding along its diagonal. The core design philosophy is to achieve essential MSRR functionalities such as morphing, docking, and locomotion with minimal control complexity. This enables a continuous and stable reconfiguration process that is independent of the surrounding medium, allowing the system to reliably form various configurations in diverse environments. To leverage the unique kinematics of Rhombot, we introduce morphpivoting, a novel motion primitive for reconfiguration that differs from advanced MSRR systems, and propose a strategy for its continuous execution. Finally, a series of physical experiments validate the module&#39;s stable reconfiguration ability, as well as its positional and docking accuracy."
    },
    {
      "title": "Self-Reconfiguration Planning for Deformable Quadrilateral Modular Robots",
      "url": "https://arxiv.org/abs/2601.19496",
      "date": "2026-01-27",
      "authors_text": "Jie Gu, Hongrun Gao, Zhihao Xia, Yirun Sun, Chunxu Tian",
      "is_highlight": false,
      "score": 91.0,
      "summary": "This paper introduces a self-reconfiguration planning algorithm for deformable quadrilateral modular robots that ensures stable connections and efficient execution sequences through a novel graph-based approach.",
      "debug_abstract": "For lattice modular self-reconfigurable robots (MSRRs), maintaining stable connections during reconfiguration is crucial for physical feasibility and deployability. This letter presents a novel self-reconfiguration planning algorithm for deformable quadrilateral MSRRs that guarantees stable connection. The method first constructs feasible connect/disconnect actions using a virtual graph representation, and then organizes these actions into a valid execution sequence through a Dependence-based Reverse Tree (DRTree) that resolves interdependencies. We also prove that reconfiguration sequences satisfying motion characteristics exist for any pair of configurations with seven or more modules (excluding linear topologies). Finally, comparisons with a modified BiRRT algorithm highlight the superior efficiency and stability of our approach, while deployment on a physical robotic platform confirms its practical feasibility."
    },
    {
      "title": "MAGNET: Towards Adaptive GUI Agents with Memory-Driven Knowledge Evolution",
      "url": "https://arxiv.org/abs/2601.19199",
      "date": "2026-01-27",
      "authors_text": "Libo Sun, Jiwen Zhang, Siyuan Wang, Zhongyu Wei",
      "is_highlight": false,
      "score": 75.0,
      "summary": "MAGNET introduces a memory-driven adaptive agent framework that enhances mobile GUI agents' performance by linking visual features to stable semantics and refining task intents amidst UI changes.",
      "debug_abstract": "Mobile GUI agents powered by large foundation models enable autonomous task execution, but frequent updates altering UI appearance and reorganizing workflows cause agents trained on historical data to fail. Despite surface changes, functional semantics and task intents remain fundamentally stable. Building on this insight, we introduce MAGNET, a memory-driven adaptive agent framework with dual-level memory: stationary memory linking diverse visual features to stable functional semantics for robust action grounding and procedural memory capturing stable task intents across varying workflows. We propose a dynamic memory evolution mechanism that continuously refines both memories by prioritizing frequently accessed knowledge. Online benchmark AndroidWorld evaluations show substantial improvements over baselines, while offline benchmarks confirm consistent gains under distribution shifts. These results validate that leveraging stable structures across interface changes improves agent performance and generalization in evolving software environments."
    }
  ],
  "南方科技大学": [
    {
      "title": "UniPCB: A Unified Vision-Language Benchmark for Open-Ended PCB Quality Inspection",
      "url": "https://arxiv.org/abs/2601.19222",
      "date": "2026-01-27",
      "authors_text": "Fuxiang Sun, Xi Jiang, Jiansheng Wu, Haigang Zhang, Feng Zheng",
      "is_highlight": false,
      "score": 88.0,
      "summary": "The paper introduces UniPCB, a unified vision-language benchmark for PCB quality inspection, and PCB-GPT, an MLLM that significantly outperforms existing models in defect localization.",
      "debug_abstract": "Multimodal Large Language Models (MLLMs) show promise for general industrial quality inspection, but fall short in complex scenarios, such as Printed Circuit Board (PCB) inspection. PCB inspection poses unique challenges due to densely packed components, complex wiring structures, and subtle defect patterns that require specialized domain expertise. However, a high-quality, unified vision-language benchmark for quantitatively evaluating MLLMs across PCB inspection tasks remains absent, stemming not only from limited data availability but also from fragmented datasets and inconsistent standardization. To fill this gap, we propose UniPCB, the first unified vision-language benchmark for open-ended PCB quality inspection. UniPCB is built via a systematic pipeline that curates and standardizes data from disparate sources across three annotated scenarios. Furthermore, we introduce PCB-GPT, an MLLM trained on a new instruction dataset generated by this pipeline, utilizing a novel progressive curriculum that mimics the learning process of human experts. Evaluations on the UniPCB benchmark show that while existing MLLMs falter on domain-specific tasks, PCB-GPT establishes a new baseline. Notably, it more than doubles the performance on fine-grained defect localization compared to the strongest competitors, with significant advantages in localization and analysis. We will release the instruction data, benchmark, and model to facilitate future research."
    }
  ],
  "具身智能多模态大模型中心": [
    {
      "title": "TIGaussian: Disentangle Gaussians for Spatial-Awared Text-Image-3D Alignment",
      "url": "https://arxiv.org/abs/2601.19247",
      "date": "2026-01-27",
      "authors_text": "Jiarun Liu, Qifeng Chen, Yiru Zhao, Minghua Liu, Baorui Ma",
      "is_highlight": false,
      "score": 72.0,
      "summary": "TIGaussian enhances cross-modal alignment by utilizing 3D Gaussian Splatting for improved feature extraction and integration of text, image, and 3D data.",
      "debug_abstract": "While visual-language models have profoundly linked features between texts and images, the incorporation of 3D modality data, such as point clouds and 3D Gaussians, further enables pretraining for 3D-related tasks, e.g., cross-modal retrieval, zero-shot classification, and scene recognition. As challenges remain in extracting 3D modal features and bridging the gap between different modalities, we propose TIGaussian, a framework that harnesses 3D Gaussian Splatting (3DGS) characteristics to strengthen cross-modality alignment through multi-branch 3DGS tokenizer and modality-specific 3D feature alignment strategies. Specifically, our multi-branch 3DGS tokenizer decouples the intrinsic properties of 3DGS structures into compact latent representations, enabling more generalizable feature extraction. To further bridge the modality gap, we develop a bidirectional cross-modal alignment strategies: a multi-view feature fusion mechanism that leverages diffusion priors to resolve perspective ambiguity in image-3D alignment, while a text-3D projection module adaptively maps 3D features to text embedding space for better text-3D alignment. Extensive experiments on various datasets demonstrate the state-of-the-art performance of TIGaussian in multiple tasks."
    }
  ],
  "具身智能研究中心": [
    {
      "title": "TIGaussian: Disentangle Gaussians for Spatial-Awared Text-Image-3D Alignment",
      "url": "https://arxiv.org/abs/2601.19247",
      "date": "2026-01-27",
      "authors_text": "Jiarun Liu, Qifeng Chen, Yiru Zhao, Minghua Liu, Baorui Ma",
      "is_highlight": false,
      "score": 72.0,
      "summary": "TIGaussian enhances cross-modal alignment by utilizing 3D Gaussian Splatting for improved feature extraction and integration of text, image, and 3D data.",
      "debug_abstract": "While visual-language models have profoundly linked features between texts and images, the incorporation of 3D modality data, such as point clouds and 3D Gaussians, further enables pretraining for 3D-related tasks, e.g., cross-modal retrieval, zero-shot classification, and scene recognition. As challenges remain in extracting 3D modal features and bridging the gap between different modalities, we propose TIGaussian, a framework that harnesses 3D Gaussian Splatting (3DGS) characteristics to strengthen cross-modality alignment through multi-branch 3DGS tokenizer and modality-specific 3D feature alignment strategies. Specifically, our multi-branch 3DGS tokenizer decouples the intrinsic properties of 3DGS structures into compact latent representations, enabling more generalizable feature extraction. To further bridge the modality gap, we develop a bidirectional cross-modal alignment strategies: a multi-view feature fusion mechanism that leverages diffusion priors to resolve perspective ambiguity in image-3D alignment, while a text-3D projection module adaptively maps 3D features to text embedding space for better text-3D alignment. Extensive experiments on various datasets demonstrate the state-of-the-art performance of TIGaussian in multiple tasks."
    }
  ],
  "机器人控制实验室": [
    {
      "title": "Scalable Exploration for High-Dimensional Continuous Control via Value-Guided Flow",
      "url": "https://arxiv.org/abs/2601.19707",
      "date": "2026-01-27",
      "authors_text": "Yunyue Wei, Chenhui Zuo, Yanan Sui",
      "is_highlight": false,
      "score": 79.0,
      "summary": "Qflex introduces a scalable reinforcement learning method for high-dimensional continuous control that enhances exploration efficiency by aligning actions with learned value gradients.",
      "debug_abstract": "Controlling high-dimensional systems in biological and robotic applications is challenging due to expansive state-action spaces, where effective exploration is critical. Commonly used exploration strategies in reinforcement learning are largely undirected with sharp degradation as action dimensionality grows. Many existing methods resort to dimensionality reduction, which constrains policy expressiveness and forfeits system flexibility. We introduce Q-guided Flow Exploration (Qflex), a scalable reinforcement learning method that conducts exploration directly in the native high-dimensional action space. During training, Qflex traverses actions from a learnable source distribution along a probability flow induced by the learned value function, aligning exploration with task-relevant gradients rather than isotropic noise. Our proposed method substantially outperforms representative online reinforcement learning baselines across diverse high-dimensional continuous-control benchmarks. Qflex also successfully controls a full-body human musculoskeletal model to perform agile, complex movements, demonstrating superior scalability and sample efficiency in very high-dimensional settings. Our results indicate that value-guided flows offer a principled and practical route to exploration at scale."
    },
    {
      "title": "Visual Generation Unlocks Human-Like Reasoning through Multimodal World Models",
      "url": "https://arxiv.org/abs/2601.19834",
      "date": "2026-01-27",
      "authors_text": "Jialong Wu, Xiaoying Zhang, Hongyi Yuan, Xiangcheng Zhang, Tianhao Huang",
      "is_highlight": false,
      "score": 89.0,
      "summary": "This paper demonstrates that visual generation enhances reasoning in multimodal AI, particularly for tasks requiring physical world understanding, outperforming purely verbal approaches.",
      "debug_abstract": "Humans construct internal world models and reason by manipulating the concepts within these models. Recent advances in AI, particularly chain-of-thought (CoT) reasoning, approximate such human cognitive abilities, where world models are believed to be embedded within large language models. Expert-level performance in formal and abstract domains such as mathematics and programming has been achieved in current systems by relying predominantly on verbal reasoning. However, they still lag far behind humans in domains like physical and spatial intelligence, which require richer representations and prior knowledge. The emergence of unified multimodal models (UMMs) capable of both verbal and visual generation has therefore sparked interest in more human-like reasoning grounded in complementary multimodal pathways, though their benefits remain unclear. From a world-model perspective, this paper presents the first principled study of when and how visual generation benefits reasoning. Our key position is the visual superiority hypothesis: for certain tasks--particularly those grounded in the physical world--visual generation more naturally serves as world models, whereas purely verbal world models encounter bottlenecks arising from representational limitations or insufficient prior knowledge. Theoretically, we formalize internal world modeling as a core component of CoT reasoning and analyze distinctions among different forms of world models. Empirically, we identify tasks that necessitate interleaved visual-verbal CoT reasoning, constructing a new evaluation suite, VisWorld-Eval. Controlled experiments on a state-of-the-art UMM show that interleaved CoT significantly outperforms purely verbal CoT on tasks that favor visual world modeling, but offers no clear advantage otherwise. Together, this work clarifies the potential of multimodal world modeling for more powerful, human-like multimodal AI."
    },
    {
      "title": "AtomMem : Learnable Dynamic Agentic Memory with Atomic Memory Operation",
      "url": "https://arxiv.org/abs/2601.08323",
      "date": "2026-01-27",
      "authors_text": "Yupeng Huo, Yaxi Lu, Zhong Zhang, Haotian Chen, Yankai Lin",
      "is_highlight": false,
      "score": 82.0,
      "summary": "AtomMem introduces a learnable dynamic memory framework that optimizes memory management through atomic CRUD operations, significantly outperforming static methods in long-horizon tasks.",
      "debug_abstract": "Equipping agents with memory is essential for solving real-world long-horizon problems. However, most existing agent memory mechanisms rely on static and hand-crafted workflows. This limits the performance and generalization ability of these memory designs, which highlights the need for a more flexible, learning-based memory framework. In this paper, we propose AtomMem, which reframes memory management as a dynamic decision-making problem. We deconstruct high-level memory processes into fundamental atomic CRUD (Create, Read, Update, Delete) operations, transforming the memory workflow into a learnable decision process. By combining supervised fine-tuning with reinforcement learning, AtomMem learns an autonomous, task-aligned policy to orchestrate memory behaviors tailored to specific task demands. Experimental results across 3 long-context benchmarks demonstrate that the trained AtomMem-8B consistently outperforms prior static-workflow memory methods. Further analysis of training dynamics shows that our learning-based formulation enables the agent to discover structured, task-aligned memory management strategies, highlighting a key advantage over predefined routines."
    },
    {
      "title": "Self-Supervised Path Planning in Unstructured Environments via Global-Guided Differentiable Hard Constraint Projection",
      "url": "https://arxiv.org/abs/2601.19354",
      "date": "2026-01-27",
      "authors_text": "Ziqian Wang, Chenxi Fang, Zhen Zhang",
      "is_highlight": false,
      "score": 90.0,
      "summary": "This paper presents a self-supervised framework for autonomous navigation in unstructured environments, utilizing a differentiable projection layer to ensure safety and efficiency in path planning.",
      "debug_abstract": "Deploying deep learning agents for autonomous navigation in unstructured environments faces critical challenges regarding safety, data scarcity, and limited computational resources. Traditional solvers often suffer from high latency, while emerging learning-based approaches struggle to ensure deterministic feasibility. To bridge the gap from embodied to embedded intelligence, we propose a self-supervised framework incorporating a differentiable hard constraint projection layer for runtime assurance. To mitigate data scarcity, we construct a Global-Guided Artificial Potential Field (G-APF), which provides dense supervision signals without manual labeling. To enforce actuator limitations and geometric constraints efficiently, we employ an adaptive neural projection layer, which iteratively rectifies the coarse network output onto the feasible manifold. Extensive benchmarks on a test set of 20,000 scenarios demonstrate an 88.75\\% success rate, substantiating the enhanced operational safety. Closed-loop experiments in CARLA further validate the physical realizability of the planned paths under dynamic constraints. Furthermore, deployment verification on an NVIDIA Jetson Orin NX confirms an inference latency of 94 ms, showing real-time feasibility on resource-constrained embedded hardware. This framework offers a generalized paradigm for embedding physical laws into neural architectures, providing a viable direction for solving constrained optimization in mechatronics. Source code is available at: this https URL."
    }
  ],
  "智能技术与系统实验室": [
    {
      "title": "Scalable Exploration for High-Dimensional Continuous Control via Value-Guided Flow",
      "url": "https://arxiv.org/abs/2601.19707",
      "date": "2026-01-27",
      "authors_text": "Yunyue Wei, Chenhui Zuo, Yanan Sui",
      "is_highlight": false,
      "score": 79.0,
      "summary": "Qflex introduces a scalable reinforcement learning method for high-dimensional continuous control that enhances exploration efficiency by aligning actions with learned value gradients.",
      "debug_abstract": "Controlling high-dimensional systems in biological and robotic applications is challenging due to expansive state-action spaces, where effective exploration is critical. Commonly used exploration strategies in reinforcement learning are largely undirected with sharp degradation as action dimensionality grows. Many existing methods resort to dimensionality reduction, which constrains policy expressiveness and forfeits system flexibility. We introduce Q-guided Flow Exploration (Qflex), a scalable reinforcement learning method that conducts exploration directly in the native high-dimensional action space. During training, Qflex traverses actions from a learnable source distribution along a probability flow induced by the learned value function, aligning exploration with task-relevant gradients rather than isotropic noise. Our proposed method substantially outperforms representative online reinforcement learning baselines across diverse high-dimensional continuous-control benchmarks. Qflex also successfully controls a full-body human musculoskeletal model to perform agile, complex movements, demonstrating superior scalability and sample efficiency in very high-dimensional settings. Our results indicate that value-guided flows offer a principled and practical route to exploration at scale."
    },
    {
      "title": "Visual Generation Unlocks Human-Like Reasoning through Multimodal World Models",
      "url": "https://arxiv.org/abs/2601.19834",
      "date": "2026-01-27",
      "authors_text": "Jialong Wu, Xiaoying Zhang, Hongyi Yuan, Xiangcheng Zhang, Tianhao Huang",
      "is_highlight": false,
      "score": 89.0,
      "summary": "This paper demonstrates that visual generation enhances reasoning in multimodal AI, particularly for tasks requiring physical world understanding, outperforming purely verbal approaches.",
      "debug_abstract": "Humans construct internal world models and reason by manipulating the concepts within these models. Recent advances in AI, particularly chain-of-thought (CoT) reasoning, approximate such human cognitive abilities, where world models are believed to be embedded within large language models. Expert-level performance in formal and abstract domains such as mathematics and programming has been achieved in current systems by relying predominantly on verbal reasoning. However, they still lag far behind humans in domains like physical and spatial intelligence, which require richer representations and prior knowledge. The emergence of unified multimodal models (UMMs) capable of both verbal and visual generation has therefore sparked interest in more human-like reasoning grounded in complementary multimodal pathways, though their benefits remain unclear. From a world-model perspective, this paper presents the first principled study of when and how visual generation benefits reasoning. Our key position is the visual superiority hypothesis: for certain tasks--particularly those grounded in the physical world--visual generation more naturally serves as world models, whereas purely verbal world models encounter bottlenecks arising from representational limitations or insufficient prior knowledge. Theoretically, we formalize internal world modeling as a core component of CoT reasoning and analyze distinctions among different forms of world models. Empirically, we identify tasks that necessitate interleaved visual-verbal CoT reasoning, constructing a new evaluation suite, VisWorld-Eval. Controlled experiments on a state-of-the-art UMM show that interleaved CoT significantly outperforms purely verbal CoT on tasks that favor visual world modeling, but offers no clear advantage otherwise. Together, this work clarifies the potential of multimodal world modeling for more powerful, human-like multimodal AI."
    },
    {
      "title": "AtomMem : Learnable Dynamic Agentic Memory with Atomic Memory Operation",
      "url": "https://arxiv.org/abs/2601.08323",
      "date": "2026-01-27",
      "authors_text": "Yupeng Huo, Yaxi Lu, Zhong Zhang, Haotian Chen, Yankai Lin",
      "is_highlight": false,
      "score": 82.0,
      "summary": "AtomMem introduces a learnable dynamic memory framework that optimizes memory management through atomic CRUD operations, significantly outperforming static methods in long-horizon tasks.",
      "debug_abstract": "Equipping agents with memory is essential for solving real-world long-horizon problems. However, most existing agent memory mechanisms rely on static and hand-crafted workflows. This limits the performance and generalization ability of these memory designs, which highlights the need for a more flexible, learning-based memory framework. In this paper, we propose AtomMem, which reframes memory management as a dynamic decision-making problem. We deconstruct high-level memory processes into fundamental atomic CRUD (Create, Read, Update, Delete) operations, transforming the memory workflow into a learnable decision process. By combining supervised fine-tuning with reinforcement learning, AtomMem learns an autonomous, task-aligned policy to orchestrate memory behaviors tailored to specific task demands. Experimental results across 3 long-context benchmarks demonstrate that the trained AtomMem-8B consistently outperforms prior static-workflow memory methods. Further analysis of training dynamics shows that our learning-based formulation enables the agent to discover structured, task-aligned memory management strategies, highlighting a key advantage over predefined routines."
    },
    {
      "title": "Self-Supervised Path Planning in Unstructured Environments via Global-Guided Differentiable Hard Constraint Projection",
      "url": "https://arxiv.org/abs/2601.19354",
      "date": "2026-01-27",
      "authors_text": "Ziqian Wang, Chenxi Fang, Zhen Zhang",
      "is_highlight": false,
      "score": 90.0,
      "summary": "This paper presents a self-supervised framework for autonomous navigation in unstructured environments, utilizing a differentiable projection layer to ensure safety and efficiency in path planning.",
      "debug_abstract": "Deploying deep learning agents for autonomous navigation in unstructured environments faces critical challenges regarding safety, data scarcity, and limited computational resources. Traditional solvers often suffer from high latency, while emerging learning-based approaches struggle to ensure deterministic feasibility. To bridge the gap from embodied to embedded intelligence, we propose a self-supervised framework incorporating a differentiable hard constraint projection layer for runtime assurance. To mitigate data scarcity, we construct a Global-Guided Artificial Potential Field (G-APF), which provides dense supervision signals without manual labeling. To enforce actuator limitations and geometric constraints efficiently, we employ an adaptive neural projection layer, which iteratively rectifies the coarse network output onto the feasible manifold. Extensive benchmarks on a test set of 20,000 scenarios demonstrate an 88.75\\% success rate, substantiating the enhanced operational safety. Closed-loop experiments in CARLA further validate the physical realizability of the planned paths under dynamic constraints. Furthermore, deployment verification on an NVIDIA Jetson Orin NX confirms an inference latency of 94 ms, showing real-time feasibility on resource-constrained embedded hardware. This framework offers a generalized paradigm for embedding physical laws into neural architectures, providing a viable direction for solving constrained optimization in mechatronics. Source code is available at: this https URL."
    }
  ],
  "智能系统与机器人实验室 (ISR Lab)": [
    {
      "title": "Scalable Exploration for High-Dimensional Continuous Control via Value-Guided Flow",
      "url": "https://arxiv.org/abs/2601.19707",
      "date": "2026-01-27",
      "authors_text": "Yunyue Wei, Chenhui Zuo, Yanan Sui",
      "is_highlight": false,
      "score": 79.0,
      "summary": "Qflex introduces a scalable reinforcement learning method for high-dimensional continuous control that enhances exploration efficiency by aligning actions with learned value gradients.",
      "debug_abstract": "Controlling high-dimensional systems in biological and robotic applications is challenging due to expansive state-action spaces, where effective exploration is critical. Commonly used exploration strategies in reinforcement learning are largely undirected with sharp degradation as action dimensionality grows. Many existing methods resort to dimensionality reduction, which constrains policy expressiveness and forfeits system flexibility. We introduce Q-guided Flow Exploration (Qflex), a scalable reinforcement learning method that conducts exploration directly in the native high-dimensional action space. During training, Qflex traverses actions from a learnable source distribution along a probability flow induced by the learned value function, aligning exploration with task-relevant gradients rather than isotropic noise. Our proposed method substantially outperforms representative online reinforcement learning baselines across diverse high-dimensional continuous-control benchmarks. Qflex also successfully controls a full-body human musculoskeletal model to perform agile, complex movements, demonstrating superior scalability and sample efficiency in very high-dimensional settings. Our results indicate that value-guided flows offer a principled and practical route to exploration at scale."
    },
    {
      "title": "Visual Generation Unlocks Human-Like Reasoning through Multimodal World Models",
      "url": "https://arxiv.org/abs/2601.19834",
      "date": "2026-01-27",
      "authors_text": "Jialong Wu, Xiaoying Zhang, Hongyi Yuan, Xiangcheng Zhang, Tianhao Huang",
      "is_highlight": false,
      "score": 89.0,
      "summary": "This paper demonstrates that visual generation enhances reasoning in multimodal AI, particularly for tasks requiring physical world understanding, outperforming purely verbal approaches.",
      "debug_abstract": "Humans construct internal world models and reason by manipulating the concepts within these models. Recent advances in AI, particularly chain-of-thought (CoT) reasoning, approximate such human cognitive abilities, where world models are believed to be embedded within large language models. Expert-level performance in formal and abstract domains such as mathematics and programming has been achieved in current systems by relying predominantly on verbal reasoning. However, they still lag far behind humans in domains like physical and spatial intelligence, which require richer representations and prior knowledge. The emergence of unified multimodal models (UMMs) capable of both verbal and visual generation has therefore sparked interest in more human-like reasoning grounded in complementary multimodal pathways, though their benefits remain unclear. From a world-model perspective, this paper presents the first principled study of when and how visual generation benefits reasoning. Our key position is the visual superiority hypothesis: for certain tasks--particularly those grounded in the physical world--visual generation more naturally serves as world models, whereas purely verbal world models encounter bottlenecks arising from representational limitations or insufficient prior knowledge. Theoretically, we formalize internal world modeling as a core component of CoT reasoning and analyze distinctions among different forms of world models. Empirically, we identify tasks that necessitate interleaved visual-verbal CoT reasoning, constructing a new evaluation suite, VisWorld-Eval. Controlled experiments on a state-of-the-art UMM show that interleaved CoT significantly outperforms purely verbal CoT on tasks that favor visual world modeling, but offers no clear advantage otherwise. Together, this work clarifies the potential of multimodal world modeling for more powerful, human-like multimodal AI."
    },
    {
      "title": "AtomMem : Learnable Dynamic Agentic Memory with Atomic Memory Operation",
      "url": "https://arxiv.org/abs/2601.08323",
      "date": "2026-01-27",
      "authors_text": "Yupeng Huo, Yaxi Lu, Zhong Zhang, Haotian Chen, Yankai Lin",
      "is_highlight": false,
      "score": 82.0,
      "summary": "AtomMem introduces a learnable dynamic memory framework that optimizes memory management through atomic CRUD operations, significantly outperforming static methods in long-horizon tasks.",
      "debug_abstract": "Equipping agents with memory is essential for solving real-world long-horizon problems. However, most existing agent memory mechanisms rely on static and hand-crafted workflows. This limits the performance and generalization ability of these memory designs, which highlights the need for a more flexible, learning-based memory framework. In this paper, we propose AtomMem, which reframes memory management as a dynamic decision-making problem. We deconstruct high-level memory processes into fundamental atomic CRUD (Create, Read, Update, Delete) operations, transforming the memory workflow into a learnable decision process. By combining supervised fine-tuning with reinforcement learning, AtomMem learns an autonomous, task-aligned policy to orchestrate memory behaviors tailored to specific task demands. Experimental results across 3 long-context benchmarks demonstrate that the trained AtomMem-8B consistently outperforms prior static-workflow memory methods. Further analysis of training dynamics shows that our learning-based formulation enables the agent to discover structured, task-aligned memory management strategies, highlighting a key advantage over predefined routines."
    },
    {
      "title": "Self-Supervised Path Planning in Unstructured Environments via Global-Guided Differentiable Hard Constraint Projection",
      "url": "https://arxiv.org/abs/2601.19354",
      "date": "2026-01-27",
      "authors_text": "Ziqian Wang, Chenxi Fang, Zhen Zhang",
      "is_highlight": false,
      "score": 90.0,
      "summary": "This paper presents a self-supervised framework for autonomous navigation in unstructured environments, utilizing a differentiable projection layer to ensure safety and efficiency in path planning.",
      "debug_abstract": "Deploying deep learning agents for autonomous navigation in unstructured environments faces critical challenges regarding safety, data scarcity, and limited computational resources. Traditional solvers often suffer from high latency, while emerging learning-based approaches struggle to ensure deterministic feasibility. To bridge the gap from embodied to embedded intelligence, we propose a self-supervised framework incorporating a differentiable hard constraint projection layer for runtime assurance. To mitigate data scarcity, we construct a Global-Guided Artificial Potential Field (G-APF), which provides dense supervision signals without manual labeling. To enforce actuator limitations and geometric constraints efficiently, we employ an adaptive neural projection layer, which iteratively rectifies the coarse network output onto the feasible manifold. Extensive benchmarks on a test set of 20,000 scenarios demonstrate an 88.75\\% success rate, substantiating the enhanced operational safety. Closed-loop experiments in CARLA further validate the physical realizability of the planned paths under dynamic constraints. Furthermore, deployment verification on an NVIDIA Jetson Orin NX confirms an inference latency of 94 ms, showing real-time feasibility on resource-constrained embedded hardware. This framework offers a generalized paradigm for embedding physical laws into neural architectures, providing a viable direction for solving constrained optimization in mechatronics. Source code is available at: this https URL."
    }
  ],
  "MARS多模态学习实验室": [
    {
      "title": "Scalable Exploration for High-Dimensional Continuous Control via Value-Guided Flow",
      "url": "https://arxiv.org/abs/2601.19707",
      "date": "2026-01-27",
      "authors_text": "Yunyue Wei, Chenhui Zuo, Yanan Sui",
      "is_highlight": false,
      "score": 79.0,
      "summary": "Qflex introduces a scalable reinforcement learning method for high-dimensional continuous control that enhances exploration efficiency by aligning actions with learned value gradients.",
      "debug_abstract": "Controlling high-dimensional systems in biological and robotic applications is challenging due to expansive state-action spaces, where effective exploration is critical. Commonly used exploration strategies in reinforcement learning are largely undirected with sharp degradation as action dimensionality grows. Many existing methods resort to dimensionality reduction, which constrains policy expressiveness and forfeits system flexibility. We introduce Q-guided Flow Exploration (Qflex), a scalable reinforcement learning method that conducts exploration directly in the native high-dimensional action space. During training, Qflex traverses actions from a learnable source distribution along a probability flow induced by the learned value function, aligning exploration with task-relevant gradients rather than isotropic noise. Our proposed method substantially outperforms representative online reinforcement learning baselines across diverse high-dimensional continuous-control benchmarks. Qflex also successfully controls a full-body human musculoskeletal model to perform agile, complex movements, demonstrating superior scalability and sample efficiency in very high-dimensional settings. Our results indicate that value-guided flows offer a principled and practical route to exploration at scale."
    },
    {
      "title": "Visual Generation Unlocks Human-Like Reasoning through Multimodal World Models",
      "url": "https://arxiv.org/abs/2601.19834",
      "date": "2026-01-27",
      "authors_text": "Jialong Wu, Xiaoying Zhang, Hongyi Yuan, Xiangcheng Zhang, Tianhao Huang",
      "is_highlight": false,
      "score": 89.0,
      "summary": "This paper demonstrates that visual generation enhances reasoning in multimodal AI, particularly for tasks requiring physical world understanding, outperforming purely verbal approaches.",
      "debug_abstract": "Humans construct internal world models and reason by manipulating the concepts within these models. Recent advances in AI, particularly chain-of-thought (CoT) reasoning, approximate such human cognitive abilities, where world models are believed to be embedded within large language models. Expert-level performance in formal and abstract domains such as mathematics and programming has been achieved in current systems by relying predominantly on verbal reasoning. However, they still lag far behind humans in domains like physical and spatial intelligence, which require richer representations and prior knowledge. The emergence of unified multimodal models (UMMs) capable of both verbal and visual generation has therefore sparked interest in more human-like reasoning grounded in complementary multimodal pathways, though their benefits remain unclear. From a world-model perspective, this paper presents the first principled study of when and how visual generation benefits reasoning. Our key position is the visual superiority hypothesis: for certain tasks--particularly those grounded in the physical world--visual generation more naturally serves as world models, whereas purely verbal world models encounter bottlenecks arising from representational limitations or insufficient prior knowledge. Theoretically, we formalize internal world modeling as a core component of CoT reasoning and analyze distinctions among different forms of world models. Empirically, we identify tasks that necessitate interleaved visual-verbal CoT reasoning, constructing a new evaluation suite, VisWorld-Eval. Controlled experiments on a state-of-the-art UMM show that interleaved CoT significantly outperforms purely verbal CoT on tasks that favor visual world modeling, but offers no clear advantage otherwise. Together, this work clarifies the potential of multimodal world modeling for more powerful, human-like multimodal AI."
    },
    {
      "title": "AtomMem : Learnable Dynamic Agentic Memory with Atomic Memory Operation",
      "url": "https://arxiv.org/abs/2601.08323",
      "date": "2026-01-27",
      "authors_text": "Yupeng Huo, Yaxi Lu, Zhong Zhang, Haotian Chen, Yankai Lin",
      "is_highlight": false,
      "score": 82.0,
      "summary": "AtomMem introduces a learnable dynamic memory framework that optimizes memory management through atomic CRUD operations, significantly outperforming static methods in long-horizon tasks.",
      "debug_abstract": "Equipping agents with memory is essential for solving real-world long-horizon problems. However, most existing agent memory mechanisms rely on static and hand-crafted workflows. This limits the performance and generalization ability of these memory designs, which highlights the need for a more flexible, learning-based memory framework. In this paper, we propose AtomMem, which reframes memory management as a dynamic decision-making problem. We deconstruct high-level memory processes into fundamental atomic CRUD (Create, Read, Update, Delete) operations, transforming the memory workflow into a learnable decision process. By combining supervised fine-tuning with reinforcement learning, AtomMem learns an autonomous, task-aligned policy to orchestrate memory behaviors tailored to specific task demands. Experimental results across 3 long-context benchmarks demonstrate that the trained AtomMem-8B consistently outperforms prior static-workflow memory methods. Further analysis of training dynamics shows that our learning-based formulation enables the agent to discover structured, task-aligned memory management strategies, highlighting a key advantage over predefined routines."
    },
    {
      "title": "Self-Supervised Path Planning in Unstructured Environments via Global-Guided Differentiable Hard Constraint Projection",
      "url": "https://arxiv.org/abs/2601.19354",
      "date": "2026-01-27",
      "authors_text": "Ziqian Wang, Chenxi Fang, Zhen Zhang",
      "is_highlight": false,
      "score": 90.0,
      "summary": "This paper presents a self-supervised framework for autonomous navigation in unstructured environments, utilizing a differentiable projection layer to ensure safety and efficiency in path planning.",
      "debug_abstract": "Deploying deep learning agents for autonomous navigation in unstructured environments faces critical challenges regarding safety, data scarcity, and limited computational resources. Traditional solvers often suffer from high latency, while emerging learning-based approaches struggle to ensure deterministic feasibility. To bridge the gap from embodied to embedded intelligence, we propose a self-supervised framework incorporating a differentiable hard constraint projection layer for runtime assurance. To mitigate data scarcity, we construct a Global-Guided Artificial Potential Field (G-APF), which provides dense supervision signals without manual labeling. To enforce actuator limitations and geometric constraints efficiently, we employ an adaptive neural projection layer, which iteratively rectifies the coarse network output onto the feasible manifold. Extensive benchmarks on a test set of 20,000 scenarios demonstrate an 88.75\\% success rate, substantiating the enhanced operational safety. Closed-loop experiments in CARLA further validate the physical realizability of the planned paths under dynamic constraints. Furthermore, deployment verification on an NVIDIA Jetson Orin NX confirms an inference latency of 94 ms, showing real-time feasibility on resource-constrained embedded hardware. This framework offers a generalized paradigm for embedding physical laws into neural architectures, providing a viable direction for solving constrained optimization in mechatronics. Source code is available at: this https URL."
    }
  ],
  "具身智能实验室 (TEA Lab)": [
    {
      "title": "Scalable Exploration for High-Dimensional Continuous Control via Value-Guided Flow",
      "url": "https://arxiv.org/abs/2601.19707",
      "date": "2026-01-27",
      "authors_text": "Yunyue Wei, Chenhui Zuo, Yanan Sui",
      "is_highlight": false,
      "score": 79.0,
      "summary": "Qflex introduces a scalable reinforcement learning method for high-dimensional continuous control that enhances exploration efficiency by aligning actions with learned value gradients.",
      "debug_abstract": "Controlling high-dimensional systems in biological and robotic applications is challenging due to expansive state-action spaces, where effective exploration is critical. Commonly used exploration strategies in reinforcement learning are largely undirected with sharp degradation as action dimensionality grows. Many existing methods resort to dimensionality reduction, which constrains policy expressiveness and forfeits system flexibility. We introduce Q-guided Flow Exploration (Qflex), a scalable reinforcement learning method that conducts exploration directly in the native high-dimensional action space. During training, Qflex traverses actions from a learnable source distribution along a probability flow induced by the learned value function, aligning exploration with task-relevant gradients rather than isotropic noise. Our proposed method substantially outperforms representative online reinforcement learning baselines across diverse high-dimensional continuous-control benchmarks. Qflex also successfully controls a full-body human musculoskeletal model to perform agile, complex movements, demonstrating superior scalability and sample efficiency in very high-dimensional settings. Our results indicate that value-guided flows offer a principled and practical route to exploration at scale."
    },
    {
      "title": "Visual Generation Unlocks Human-Like Reasoning through Multimodal World Models",
      "url": "https://arxiv.org/abs/2601.19834",
      "date": "2026-01-27",
      "authors_text": "Jialong Wu, Xiaoying Zhang, Hongyi Yuan, Xiangcheng Zhang, Tianhao Huang",
      "is_highlight": false,
      "score": 89.0,
      "summary": "This paper demonstrates that visual generation enhances reasoning in multimodal AI, particularly for tasks requiring physical world understanding, outperforming purely verbal approaches.",
      "debug_abstract": "Humans construct internal world models and reason by manipulating the concepts within these models. Recent advances in AI, particularly chain-of-thought (CoT) reasoning, approximate such human cognitive abilities, where world models are believed to be embedded within large language models. Expert-level performance in formal and abstract domains such as mathematics and programming has been achieved in current systems by relying predominantly on verbal reasoning. However, they still lag far behind humans in domains like physical and spatial intelligence, which require richer representations and prior knowledge. The emergence of unified multimodal models (UMMs) capable of both verbal and visual generation has therefore sparked interest in more human-like reasoning grounded in complementary multimodal pathways, though their benefits remain unclear. From a world-model perspective, this paper presents the first principled study of when and how visual generation benefits reasoning. Our key position is the visual superiority hypothesis: for certain tasks--particularly those grounded in the physical world--visual generation more naturally serves as world models, whereas purely verbal world models encounter bottlenecks arising from representational limitations or insufficient prior knowledge. Theoretically, we formalize internal world modeling as a core component of CoT reasoning and analyze distinctions among different forms of world models. Empirically, we identify tasks that necessitate interleaved visual-verbal CoT reasoning, constructing a new evaluation suite, VisWorld-Eval. Controlled experiments on a state-of-the-art UMM show that interleaved CoT significantly outperforms purely verbal CoT on tasks that favor visual world modeling, but offers no clear advantage otherwise. Together, this work clarifies the potential of multimodal world modeling for more powerful, human-like multimodal AI."
    },
    {
      "title": "AtomMem : Learnable Dynamic Agentic Memory with Atomic Memory Operation",
      "url": "https://arxiv.org/abs/2601.08323",
      "date": "2026-01-27",
      "authors_text": "Yupeng Huo, Yaxi Lu, Zhong Zhang, Haotian Chen, Yankai Lin",
      "is_highlight": false,
      "score": 82.0,
      "summary": "AtomMem introduces a learnable dynamic memory framework that optimizes memory management through atomic CRUD operations, significantly outperforming static methods in long-horizon tasks.",
      "debug_abstract": "Equipping agents with memory is essential for solving real-world long-horizon problems. However, most existing agent memory mechanisms rely on static and hand-crafted workflows. This limits the performance and generalization ability of these memory designs, which highlights the need for a more flexible, learning-based memory framework. In this paper, we propose AtomMem, which reframes memory management as a dynamic decision-making problem. We deconstruct high-level memory processes into fundamental atomic CRUD (Create, Read, Update, Delete) operations, transforming the memory workflow into a learnable decision process. By combining supervised fine-tuning with reinforcement learning, AtomMem learns an autonomous, task-aligned policy to orchestrate memory behaviors tailored to specific task demands. Experimental results across 3 long-context benchmarks demonstrate that the trained AtomMem-8B consistently outperforms prior static-workflow memory methods. Further analysis of training dynamics shows that our learning-based formulation enables the agent to discover structured, task-aligned memory management strategies, highlighting a key advantage over predefined routines."
    },
    {
      "title": "Self-Supervised Path Planning in Unstructured Environments via Global-Guided Differentiable Hard Constraint Projection",
      "url": "https://arxiv.org/abs/2601.19354",
      "date": "2026-01-27",
      "authors_text": "Ziqian Wang, Chenxi Fang, Zhen Zhang",
      "is_highlight": false,
      "score": 90.0,
      "summary": "This paper presents a self-supervised framework for autonomous navigation in unstructured environments, utilizing a differentiable projection layer to ensure safety and efficiency in path planning.",
      "debug_abstract": "Deploying deep learning agents for autonomous navigation in unstructured environments faces critical challenges regarding safety, data scarcity, and limited computational resources. Traditional solvers often suffer from high latency, while emerging learning-based approaches struggle to ensure deterministic feasibility. To bridge the gap from embodied to embedded intelligence, we propose a self-supervised framework incorporating a differentiable hard constraint projection layer for runtime assurance. To mitigate data scarcity, we construct a Global-Guided Artificial Potential Field (G-APF), which provides dense supervision signals without manual labeling. To enforce actuator limitations and geometric constraints efficiently, we employ an adaptive neural projection layer, which iteratively rectifies the coarse network output onto the feasible manifold. Extensive benchmarks on a test set of 20,000 scenarios demonstrate an 88.75\\% success rate, substantiating the enhanced operational safety. Closed-loop experiments in CARLA further validate the physical realizability of the planned paths under dynamic constraints. Furthermore, deployment verification on an NVIDIA Jetson Orin NX confirms an inference latency of 94 ms, showing real-time feasibility on resource-constrained embedded hardware. This framework offers a generalized paradigm for embedding physical laws into neural architectures, providing a viable direction for solving constrained optimization in mechatronics. Source code is available at: this https URL."
    }
  ],
  "智能产业研究院 (AIR)": [
    {
      "title": "Scalable Exploration for High-Dimensional Continuous Control via Value-Guided Flow",
      "url": "https://arxiv.org/abs/2601.19707",
      "date": "2026-01-27",
      "authors_text": "Yunyue Wei, Chenhui Zuo, Yanan Sui",
      "is_highlight": false,
      "score": 79.0,
      "summary": "Qflex introduces a scalable reinforcement learning method for high-dimensional continuous control that enhances exploration efficiency by aligning actions with learned value gradients.",
      "debug_abstract": "Controlling high-dimensional systems in biological and robotic applications is challenging due to expansive state-action spaces, where effective exploration is critical. Commonly used exploration strategies in reinforcement learning are largely undirected with sharp degradation as action dimensionality grows. Many existing methods resort to dimensionality reduction, which constrains policy expressiveness and forfeits system flexibility. We introduce Q-guided Flow Exploration (Qflex), a scalable reinforcement learning method that conducts exploration directly in the native high-dimensional action space. During training, Qflex traverses actions from a learnable source distribution along a probability flow induced by the learned value function, aligning exploration with task-relevant gradients rather than isotropic noise. Our proposed method substantially outperforms representative online reinforcement learning baselines across diverse high-dimensional continuous-control benchmarks. Qflex also successfully controls a full-body human musculoskeletal model to perform agile, complex movements, demonstrating superior scalability and sample efficiency in very high-dimensional settings. Our results indicate that value-guided flows offer a principled and practical route to exploration at scale."
    },
    {
      "title": "Visual Generation Unlocks Human-Like Reasoning through Multimodal World Models",
      "url": "https://arxiv.org/abs/2601.19834",
      "date": "2026-01-27",
      "authors_text": "Jialong Wu, Xiaoying Zhang, Hongyi Yuan, Xiangcheng Zhang, Tianhao Huang",
      "is_highlight": false,
      "score": 89.0,
      "summary": "This paper demonstrates that visual generation enhances reasoning in multimodal AI, particularly for tasks requiring physical world understanding, outperforming purely verbal approaches.",
      "debug_abstract": "Humans construct internal world models and reason by manipulating the concepts within these models. Recent advances in AI, particularly chain-of-thought (CoT) reasoning, approximate such human cognitive abilities, where world models are believed to be embedded within large language models. Expert-level performance in formal and abstract domains such as mathematics and programming has been achieved in current systems by relying predominantly on verbal reasoning. However, they still lag far behind humans in domains like physical and spatial intelligence, which require richer representations and prior knowledge. The emergence of unified multimodal models (UMMs) capable of both verbal and visual generation has therefore sparked interest in more human-like reasoning grounded in complementary multimodal pathways, though their benefits remain unclear. From a world-model perspective, this paper presents the first principled study of when and how visual generation benefits reasoning. Our key position is the visual superiority hypothesis: for certain tasks--particularly those grounded in the physical world--visual generation more naturally serves as world models, whereas purely verbal world models encounter bottlenecks arising from representational limitations or insufficient prior knowledge. Theoretically, we formalize internal world modeling as a core component of CoT reasoning and analyze distinctions among different forms of world models. Empirically, we identify tasks that necessitate interleaved visual-verbal CoT reasoning, constructing a new evaluation suite, VisWorld-Eval. Controlled experiments on a state-of-the-art UMM show that interleaved CoT significantly outperforms purely verbal CoT on tasks that favor visual world modeling, but offers no clear advantage otherwise. Together, this work clarifies the potential of multimodal world modeling for more powerful, human-like multimodal AI."
    },
    {
      "title": "AtomMem : Learnable Dynamic Agentic Memory with Atomic Memory Operation",
      "url": "https://arxiv.org/abs/2601.08323",
      "date": "2026-01-27",
      "authors_text": "Yupeng Huo, Yaxi Lu, Zhong Zhang, Haotian Chen, Yankai Lin",
      "is_highlight": false,
      "score": 82.0,
      "summary": "AtomMem introduces a learnable dynamic memory framework that optimizes memory management through atomic CRUD operations, significantly outperforming static methods in long-horizon tasks.",
      "debug_abstract": "Equipping agents with memory is essential for solving real-world long-horizon problems. However, most existing agent memory mechanisms rely on static and hand-crafted workflows. This limits the performance and generalization ability of these memory designs, which highlights the need for a more flexible, learning-based memory framework. In this paper, we propose AtomMem, which reframes memory management as a dynamic decision-making problem. We deconstruct high-level memory processes into fundamental atomic CRUD (Create, Read, Update, Delete) operations, transforming the memory workflow into a learnable decision process. By combining supervised fine-tuning with reinforcement learning, AtomMem learns an autonomous, task-aligned policy to orchestrate memory behaviors tailored to specific task demands. Experimental results across 3 long-context benchmarks demonstrate that the trained AtomMem-8B consistently outperforms prior static-workflow memory methods. Further analysis of training dynamics shows that our learning-based formulation enables the agent to discover structured, task-aligned memory management strategies, highlighting a key advantage over predefined routines."
    },
    {
      "title": "Self-Supervised Path Planning in Unstructured Environments via Global-Guided Differentiable Hard Constraint Projection",
      "url": "https://arxiv.org/abs/2601.19354",
      "date": "2026-01-27",
      "authors_text": "Ziqian Wang, Chenxi Fang, Zhen Zhang",
      "is_highlight": false,
      "score": 90.0,
      "summary": "This paper presents a self-supervised framework for autonomous navigation in unstructured environments, utilizing a differentiable projection layer to ensure safety and efficiency in path planning.",
      "debug_abstract": "Deploying deep learning agents for autonomous navigation in unstructured environments faces critical challenges regarding safety, data scarcity, and limited computational resources. Traditional solvers often suffer from high latency, while emerging learning-based approaches struggle to ensure deterministic feasibility. To bridge the gap from embodied to embedded intelligence, we propose a self-supervised framework incorporating a differentiable hard constraint projection layer for runtime assurance. To mitigate data scarcity, we construct a Global-Guided Artificial Potential Field (G-APF), which provides dense supervision signals without manual labeling. To enforce actuator limitations and geometric constraints efficiently, we employ an adaptive neural projection layer, which iteratively rectifies the coarse network output onto the feasible manifold. Extensive benchmarks on a test set of 20,000 scenarios demonstrate an 88.75\\% success rate, substantiating the enhanced operational safety. Closed-loop experiments in CARLA further validate the physical realizability of the planned paths under dynamic constraints. Furthermore, deployment verification on an NVIDIA Jetson Orin NX confirms an inference latency of 94 ms, showing real-time feasibility on resource-constrained embedded hardware. This framework offers a generalized paradigm for embedding physical laws into neural architectures, providing a viable direction for solving constrained optimization in mechatronics. Source code is available at: this https URL."
    }
  ],
  "具身视觉与机器人实验室 (EVAR Lab)": [
    {
      "title": "Scalable Exploration for High-Dimensional Continuous Control via Value-Guided Flow",
      "url": "https://arxiv.org/abs/2601.19707",
      "date": "2026-01-27",
      "authors_text": "Yunyue Wei, Chenhui Zuo, Yanan Sui",
      "is_highlight": false,
      "score": 79.0,
      "summary": "Qflex introduces a scalable reinforcement learning method for high-dimensional continuous control that enhances exploration efficiency by aligning actions with learned value gradients.",
      "debug_abstract": "Controlling high-dimensional systems in biological and robotic applications is challenging due to expansive state-action spaces, where effective exploration is critical. Commonly used exploration strategies in reinforcement learning are largely undirected with sharp degradation as action dimensionality grows. Many existing methods resort to dimensionality reduction, which constrains policy expressiveness and forfeits system flexibility. We introduce Q-guided Flow Exploration (Qflex), a scalable reinforcement learning method that conducts exploration directly in the native high-dimensional action space. During training, Qflex traverses actions from a learnable source distribution along a probability flow induced by the learned value function, aligning exploration with task-relevant gradients rather than isotropic noise. Our proposed method substantially outperforms representative online reinforcement learning baselines across diverse high-dimensional continuous-control benchmarks. Qflex also successfully controls a full-body human musculoskeletal model to perform agile, complex movements, demonstrating superior scalability and sample efficiency in very high-dimensional settings. Our results indicate that value-guided flows offer a principled and practical route to exploration at scale."
    },
    {
      "title": "Visual Generation Unlocks Human-Like Reasoning through Multimodal World Models",
      "url": "https://arxiv.org/abs/2601.19834",
      "date": "2026-01-27",
      "authors_text": "Jialong Wu, Xiaoying Zhang, Hongyi Yuan, Xiangcheng Zhang, Tianhao Huang",
      "is_highlight": false,
      "score": 89.0,
      "summary": "This paper demonstrates that visual generation enhances reasoning in multimodal AI, particularly for tasks requiring physical world understanding, outperforming purely verbal approaches.",
      "debug_abstract": "Humans construct internal world models and reason by manipulating the concepts within these models. Recent advances in AI, particularly chain-of-thought (CoT) reasoning, approximate such human cognitive abilities, where world models are believed to be embedded within large language models. Expert-level performance in formal and abstract domains such as mathematics and programming has been achieved in current systems by relying predominantly on verbal reasoning. However, they still lag far behind humans in domains like physical and spatial intelligence, which require richer representations and prior knowledge. The emergence of unified multimodal models (UMMs) capable of both verbal and visual generation has therefore sparked interest in more human-like reasoning grounded in complementary multimodal pathways, though their benefits remain unclear. From a world-model perspective, this paper presents the first principled study of when and how visual generation benefits reasoning. Our key position is the visual superiority hypothesis: for certain tasks--particularly those grounded in the physical world--visual generation more naturally serves as world models, whereas purely verbal world models encounter bottlenecks arising from representational limitations or insufficient prior knowledge. Theoretically, we formalize internal world modeling as a core component of CoT reasoning and analyze distinctions among different forms of world models. Empirically, we identify tasks that necessitate interleaved visual-verbal CoT reasoning, constructing a new evaluation suite, VisWorld-Eval. Controlled experiments on a state-of-the-art UMM show that interleaved CoT significantly outperforms purely verbal CoT on tasks that favor visual world modeling, but offers no clear advantage otherwise. Together, this work clarifies the potential of multimodal world modeling for more powerful, human-like multimodal AI."
    },
    {
      "title": "AtomMem : Learnable Dynamic Agentic Memory with Atomic Memory Operation",
      "url": "https://arxiv.org/abs/2601.08323",
      "date": "2026-01-27",
      "authors_text": "Yupeng Huo, Yaxi Lu, Zhong Zhang, Haotian Chen, Yankai Lin",
      "is_highlight": false,
      "score": 82.0,
      "summary": "AtomMem introduces a learnable dynamic memory framework that optimizes memory management through atomic CRUD operations, significantly outperforming static methods in long-horizon tasks.",
      "debug_abstract": "Equipping agents with memory is essential for solving real-world long-horizon problems. However, most existing agent memory mechanisms rely on static and hand-crafted workflows. This limits the performance and generalization ability of these memory designs, which highlights the need for a more flexible, learning-based memory framework. In this paper, we propose AtomMem, which reframes memory management as a dynamic decision-making problem. We deconstruct high-level memory processes into fundamental atomic CRUD (Create, Read, Update, Delete) operations, transforming the memory workflow into a learnable decision process. By combining supervised fine-tuning with reinforcement learning, AtomMem learns an autonomous, task-aligned policy to orchestrate memory behaviors tailored to specific task demands. Experimental results across 3 long-context benchmarks demonstrate that the trained AtomMem-8B consistently outperforms prior static-workflow memory methods. Further analysis of training dynamics shows that our learning-based formulation enables the agent to discover structured, task-aligned memory management strategies, highlighting a key advantage over predefined routines."
    },
    {
      "title": "Self-Supervised Path Planning in Unstructured Environments via Global-Guided Differentiable Hard Constraint Projection",
      "url": "https://arxiv.org/abs/2601.19354",
      "date": "2026-01-27",
      "authors_text": "Ziqian Wang, Chenxi Fang, Zhen Zhang",
      "is_highlight": false,
      "score": 90.0,
      "summary": "This paper presents a self-supervised framework for autonomous navigation in unstructured environments, utilizing a differentiable projection layer to ensure safety and efficiency in path planning.",
      "debug_abstract": "Deploying deep learning agents for autonomous navigation in unstructured environments faces critical challenges regarding safety, data scarcity, and limited computational resources. Traditional solvers often suffer from high latency, while emerging learning-based approaches struggle to ensure deterministic feasibility. To bridge the gap from embodied to embedded intelligence, we propose a self-supervised framework incorporating a differentiable hard constraint projection layer for runtime assurance. To mitigate data scarcity, we construct a Global-Guided Artificial Potential Field (G-APF), which provides dense supervision signals without manual labeling. To enforce actuator limitations and geometric constraints efficiently, we employ an adaptive neural projection layer, which iteratively rectifies the coarse network output onto the feasible manifold. Extensive benchmarks on a test set of 20,000 scenarios demonstrate an 88.75\\% success rate, substantiating the enhanced operational safety. Closed-loop experiments in CARLA further validate the physical realizability of the planned paths under dynamic constraints. Furthermore, deployment verification on an NVIDIA Jetson Orin NX confirms an inference latency of 94 ms, showing real-time feasibility on resource-constrained embedded hardware. This framework offers a generalized paradigm for embedding physical laws into neural architectures, providing a viable direction for solving constrained optimization in mechatronics. Source code is available at: this https URL."
    }
  ],
  "北京航空航天大学": [
    {
      "title": "Dynamic Worlds, Dynamic Humans: Generating Virtual Human-Scene Interaction Motion in Dynamic Scenes",
      "url": "https://arxiv.org/abs/2601.19484",
      "date": "2026-01-27",
      "authors_text": "Yin Wang, Zhiying Leng, Haitian Liu, Frederick W. B. Li, Mu Li",
      "is_highlight": false,
      "score": 78.0,
      "summary": "Dyn-HSI introduces a cognitive architecture for generating adaptive human-scene interactions in dynamic environments, enhancing motion quality through perception, memory, and control mechanisms.",
      "debug_abstract": "Scenes are continuously undergoing dynamic changes in the real world. However, existing human-scene interaction generation methods typically treat the scene as static, which deviates from reality. Inspired by world models, we introduce Dyn-HSI, the first cognitive architecture for dynamic human-scene interaction, which endows virtual humans with three humanoid components. (1)Vision (human eyes): we equip the virtual human with a Dynamic Scene-Aware Navigation, which continuously perceives changes in the surrounding environment and adaptively predicts the next waypoint. (2)Memory (human brain): we equip the virtual human with a Hierarchical Experience Memory, which stores and updates experiential data accumulated during training. This allows the model to leverage prior knowledge during inference for context-aware motion priming, thereby enhancing both motion quality and generalization. (3) Control (human body): we equip the virtual human with Human-Scene Interaction Diffusion Model, which generates high-fidelity interaction motions conditioned on multimodal inputs. To evaluate performance in dynamic scenes, we extend the existing static human-scene interaction datasets to construct a dynamic benchmark, Dyn-Scenes. We conduct extensive qualitative and quantitative experiments to validate Dyn-HSI, showing that our method consistently outperforms existing approaches and generates high-quality human-scene interaction motions in both static and dynamic settings."
    }
  ],
  "武汉大学": [
    {
      "title": "Fast Converging 3D Gaussian Splatting for 1-Minute Reconstruction",
      "url": "https://arxiv.org/abs/2601.19489",
      "date": "2026-01-27",
      "authors_text": "Ziyu Zhang, Tianle Liu, Diantao Tu, Shuhan Shen",
      "is_highlight": false,
      "score": 60.0,
      "summary": "The paper presents a rapid 3D Gaussian Splatting reconstruction method that achieves high-fidelity results within one minute, winning the SIGGRAPH Asia 3DGS Fast Reconstruction Challenge.",
      "debug_abstract": "We present a fast 3DGS reconstruction pipeline designed to converge within one minute, developed for the SIGGRAPH Asia 3DGS Fast Reconstruction Challenge. The challenge consists of an initial round using SLAM-generated camera poses (with noisy trajectories) and a final round using COLMAP poses (highly accurate). To robustly handle these heterogeneous settings, we develop a two-stage solution. In the first round, we use reverse per-Gaussian parallel optimization and compact forward splatting based on Taming-GS and Speedy-splat, load-balanced tiling, an anchor-based Neural-Gaussian representation enabling rapid convergence with fewer learnable parameters, initialization from monocular depth and partially from feed-forward 3DGS models, and a global pose refinement module for noisy SLAM trajectories. In the final round, the accurate COLMAP poses change the optimization landscape; we disable pose refinement, revert from Neural-Gaussians back to standard 3DGS to eliminate MLP inference overhead, introduce multi-view consistency-guided Gaussian splitting inspired by Fast-GS, and introduce a depth estimator to supervise the rendered depth. Together, these techniques enable high-fidelity reconstruction under a strict one-minute budget. Our method achieved the top performance with a PSNR of 28.43 and ranked first in the competition."
    }
  ],
  "机器人与机械智能实验室 (ROMI Lab)": [
    {
      "title": "Component-Level Lesioning of Language Models Reveals Clinically Aligned Aphasia Phenotypes",
      "url": "https://arxiv.org/abs/2601.19723",
      "date": "2026-01-27",
      "authors_text": "Yifan Wang, Jichen Zheng, Jingyuan Sun, Yunhao Zhang, Chunyu Ye",
      "is_highlight": false,
      "score": 62.0,
      "summary": "This study presents a framework for simulating aphasia in language models by selectively perturbing components, revealing clinically relevant language impairments and enhancing understanding of language function degradation.",
      "debug_abstract": "Large language models (LLMs) increasingly exhibit human-like linguistic behaviors and internal representations that they could serve as computational simulators of language cognition. We ask whether LLMs can be systematically manipulated to reproduce language-production impairments characteristic of aphasia following focal brain lesions. Such models could provide scalable proxies for testing rehabilitation hypotheses, and offer a controlled framework for probing the functional organization of language. We introduce a clinically grounded, component-level framework that simulates aphasia by selectively perturbing functional components in LLMs, and apply it to both modular Mixture-of-Experts models and dense Transformers using a unified intervention interface. Our pipeline (i) identifies subtype-linked components for Broca&#39;s and Wernicke&#39;s aphasia, (ii) interprets these components with linguistic probing tasks, and (iii) induces graded impairments by progressively perturbing the top-k subtype-linked components, evaluating outcomes with Western Aphasia Battery (WAB) subtests summarized by Aphasia Quotient (AQ). Across architectures and lesioning strategies, subtype-targeted perturbations yield more systematic, aphasia-like regressions than size-matched random perturbations, and MoE modularity supports more localized and interpretable phenotype-to-component mappings. These findings suggest that modular LLMs, combined with clinically informed component perturbations, provide a promising platform for simulating aphasic language production and studying how distinct language functions degrade under targeted disruptions."
    },
    {
      "title": "Rhombot: Rhombus-shaped Modular Robots for Stable, Medium-Independent Reconfiguration Motion",
      "url": "https://arxiv.org/abs/2601.19529",
      "date": "2026-01-27",
      "authors_text": "Jie Gu, Yirui Sun, Zhihao Xia, Tin Lun Lam, Chunxu Tian",
      "is_highlight": false,
      "score": 86.0,
      "summary": "Rhombot is a modular self-reconfigurable robot featuring rhombus-shaped modules that enable stable, medium-independent morphing and locomotion through a novel motion primitive called morphpivoting.",
      "debug_abstract": "In this paper, we present Rhombot, a novel deformable planar lattice modular self-reconfigurable robot (MSRR) with a rhombus shaped module. Each module consists of a parallelogram skeleton with a single centrally mounted actuator that enables folding and unfolding along its diagonal. The core design philosophy is to achieve essential MSRR functionalities such as morphing, docking, and locomotion with minimal control complexity. This enables a continuous and stable reconfiguration process that is independent of the surrounding medium, allowing the system to reliably form various configurations in diverse environments. To leverage the unique kinematics of Rhombot, we introduce morphpivoting, a novel motion primitive for reconfiguration that differs from advanced MSRR systems, and propose a strategy for its continuous execution. Finally, a series of physical experiments validate the module&#39;s stable reconfiguration ability, as well as its positional and docking accuracy."
    },
    {
      "title": "Self-Reconfiguration Planning for Deformable Quadrilateral Modular Robots",
      "url": "https://arxiv.org/abs/2601.19496",
      "date": "2026-01-27",
      "authors_text": "Jie Gu, Hongrun Gao, Zhihao Xia, Yirun Sun, Chunxu Tian",
      "is_highlight": false,
      "score": 91.0,
      "summary": "This paper introduces a self-reconfiguration planning algorithm for deformable quadrilateral modular robots that ensures stable connections and efficient execution sequences through a novel graph-based approach.",
      "debug_abstract": "For lattice modular self-reconfigurable robots (MSRRs), maintaining stable connections during reconfiguration is crucial for physical feasibility and deployability. This letter presents a novel self-reconfiguration planning algorithm for deformable quadrilateral MSRRs that guarantees stable connection. The method first constructs feasible connect/disconnect actions using a virtual graph representation, and then organizes these actions into a valid execution sequence through a Dependence-based Reverse Tree (DRTree) that resolves interdependencies. We also prove that reconfiguration sequences satisfying motion characteristics exist for any pair of configurations with seven or more modules (excluding linear topologies). Finally, comparisons with a modified BiRRT algorithm highlight the superior efficiency and stability of our approach, while deployment on a physical robotic platform confirms its practical feasibility."
    }
  ],
  "浙江大学": [
    {
      "title": "A DVL Aided Loosely Coupled Inertial Navigation Strategy for AUVs with Attitude Error Modeling and Variance Propagation",
      "url": "https://arxiv.org/abs/2601.19509",
      "date": "2026-01-27",
      "authors_text": "Jin Huang, Zichen Liu, Haoda Li, Zhikun Wang, Ying Chen",
      "is_highlight": false,
      "score": 77.0,
      "summary": "This paper presents a novel navigation strategy for AUVs that integrates attitude error modeling and variance propagation to significantly enhance long-term SINS/DVL navigation accuracy.",
      "debug_abstract": "In underwater navigation systems, strap-down inertial navigation system/Doppler velocity log (SINS/DVL)-based loosely coupled architectures are widely adopted. Conventional approaches project DVL velocities from the body coordinate system to the navigation coordinate system using SINS-derived attitude; however, accumulated attitude estimation errors introduce biases into velocity projection and degrade navigation performance during long-term operation. To address this issue, two complementary improvements are introduced. First, a vehicle attitude error-aware DVL velocity transformation model is formulated by incorporating attitude error terms into the observation equation to reduce projection-induced velocity bias. Second, a covariance matrix-based variance propagation method is developed to transform DVL measurement uncertainty across coordinate systems, introducing an expectation-based attitude error compensation term to achieve statistically consistent noise modeling. Simulation and field experiment results demonstrate that both improvements individually enhance navigation accuracy and confirm that accumulated attitude errors affect both projected velocity measurements and their associated uncertainty. When jointly applied, long-term error divergence is effectively suppressed. Field experimental results show that the proposed approach achieves a 78.3% improvement in 3D position RMSE and a 71.8% reduction in the maximum component-wise position error compared with the baseline IMU+DVL method, providing a robust solution for improving long-term SINS/DVL navigation performance."
    }
  ],
  "深圳市人工智能与机器人研究院 (AIRS)": [
    {
      "title": "EgoHandICL: Egocentric 3D Hand Reconstruction with In-Context Learning",
      "url": "https://arxiv.org/abs/2601.19850",
      "date": "2026-01-27",
      "authors_text": "Binzhu Xie, Shi Qiu, Sicheng Zhang, Yinqiao Wang, Hao Xu",
      "is_highlight": false,
      "score": 76.0,
      "summary": "EgoHandICL introduces an innovative in-context learning framework for robust 3D hand reconstruction in egocentric vision, enhancing performance through exemplar retrieval and multimodal context integration.",
      "debug_abstract": "Robust 3D hand reconstruction in egocentric vision is challenging due to depth ambiguity, self-occlusion, and complex hand-object interactions. Prior methods mitigate these issues by scaling training data or adding auxiliary cues, but they often struggle in unseen contexts. We present EgoHandICL, the first in-context learning (ICL) framework for 3D hand reconstruction that improves semantic alignment, visual consistency, and robustness under challenging egocentric conditions. EgoHandICL introduces complementary exemplar retrieval guided by vision-language models (VLMs), an ICL-tailored tokenizer for multimodal context, and a masked autoencoder (MAE)-based architecture trained with hand-guided geometric and perceptual objectives. Experiments on ARCTIC and EgoExo4D show consistent gains over state-of-the-art methods. We also demonstrate real-world generalization and improve EgoVLM hand-object interaction reasoning by using reconstructed hands as visual prompts. Code and data: this https URL"
    },
    {
      "title": "Rhombot: Rhombus-shaped Modular Robots for Stable, Medium-Independent Reconfiguration Motion",
      "url": "https://arxiv.org/abs/2601.19529",
      "date": "2026-01-27",
      "authors_text": "Jie Gu, Yirui Sun, Zhihao Xia, Tin Lun Lam, Chunxu Tian",
      "is_highlight": false,
      "score": 86.0,
      "summary": "Rhombot is a modular self-reconfigurable robot featuring rhombus-shaped modules that enable stable, medium-independent morphing and locomotion through a novel motion primitive called morphpivoting.",
      "debug_abstract": "In this paper, we present Rhombot, a novel deformable planar lattice modular self-reconfigurable robot (MSRR) with a rhombus shaped module. Each module consists of a parallelogram skeleton with a single centrally mounted actuator that enables folding and unfolding along its diagonal. The core design philosophy is to achieve essential MSRR functionalities such as morphing, docking, and locomotion with minimal control complexity. This enables a continuous and stable reconfiguration process that is independent of the surrounding medium, allowing the system to reliably form various configurations in diverse environments. To leverage the unique kinematics of Rhombot, we introduce morphpivoting, a novel motion primitive for reconfiguration that differs from advanced MSRR systems, and propose a strategy for its continuous execution. Finally, a series of physical experiments validate the module&#39;s stable reconfiguration ability, as well as its positional and docking accuracy."
    }
  ],
  "机器人与人工智能实验室 (RAIL)": [
    {
      "title": "EgoHandICL: Egocentric 3D Hand Reconstruction with In-Context Learning",
      "url": "https://arxiv.org/abs/2601.19850",
      "date": "2026-01-27",
      "authors_text": "Binzhu Xie, Shi Qiu, Sicheng Zhang, Yinqiao Wang, Hao Xu",
      "is_highlight": false,
      "score": 76.0,
      "summary": "EgoHandICL introduces an innovative in-context learning framework for robust 3D hand reconstruction in egocentric vision, enhancing performance through exemplar retrieval and multimodal context integration.",
      "debug_abstract": "Robust 3D hand reconstruction in egocentric vision is challenging due to depth ambiguity, self-occlusion, and complex hand-object interactions. Prior methods mitigate these issues by scaling training data or adding auxiliary cues, but they often struggle in unseen contexts. We present EgoHandICL, the first in-context learning (ICL) framework for 3D hand reconstruction that improves semantic alignment, visual consistency, and robustness under challenging egocentric conditions. EgoHandICL introduces complementary exemplar retrieval guided by vision-language models (VLMs), an ICL-tailored tokenizer for multimodal context, and a masked autoencoder (MAE)-based architecture trained with hand-guided geometric and perceptual objectives. Experiments on ARCTIC and EgoExo4D show consistent gains over state-of-the-art methods. We also demonstrate real-world generalization and improve EgoVLM hand-object interaction reasoning by using reconstructed hands as visual prompts. Code and data: this https URL"
    },
    {
      "title": "AC^2-VLA: Action-Context-Aware Adaptive Computation in Vision-Language-Action Models for Efficient Robotic Manipulation",
      "url": "https://arxiv.org/abs/2601.19634",
      "date": "2026-01-27",
      "authors_text": "Wenda Yu, Tianshi Wang, Fengling Li, Jingjing Li, Lei Zhu",
      "is_highlight": false,
      "score": 94.0,
      "summary": "AC^2-VLA enhances robotic manipulation efficiency by adaptively optimizing computation based on action context, achieving significant speedup and reduced resource usage without sacrificing performance.",
      "debug_abstract": "Vision-Language-Action (VLA) models have demonstrated strong performance in robotic manipulation, yet their closed-loop deployment is hindered by the high latency and compute cost of repeatedly running large vision-language backbones at every timestep. We observe that VLA inference exhibits structured redundancies across temporal, spatial, and depth dimensions, and that most existing efficiency methods ignore action context, despite its central role in embodied tasks. To address this gap, we propose Action-Context-aware Adaptive Computation for VLA models (AC^2-VLA), a unified framework that conditions computation on current visual observations, language instructions, and previous action states. Based on this action-centric context, AC^2-VLA adaptively performs cognition reuse across timesteps, token pruning, and selective execution of model components within a unified mechanism. To train the adaptive policy, we introduce an action-guided self-distillation scheme that preserves the behavior of the dense VLA policy while enabling structured sparsification that transfers across tasks and settings. Extensive experiments on robotic manipulation benchmarks show that AC^2-VLA achieves up to a 1.79\\times speedup while reducing FLOPs to 29.4% of the dense baseline, with comparable task success."
    },
    {
      "title": "Rhombot: Rhombus-shaped Modular Robots for Stable, Medium-Independent Reconfiguration Motion",
      "url": "https://arxiv.org/abs/2601.19529",
      "date": "2026-01-27",
      "authors_text": "Jie Gu, Yirui Sun, Zhihao Xia, Tin Lun Lam, Chunxu Tian",
      "is_highlight": false,
      "score": 86.0,
      "summary": "Rhombot is a modular self-reconfigurable robot featuring rhombus-shaped modules that enable stable, medium-independent morphing and locomotion through a novel motion primitive called morphpivoting.",
      "debug_abstract": "In this paper, we present Rhombot, a novel deformable planar lattice modular self-reconfigurable robot (MSRR) with a rhombus shaped module. Each module consists of a parallelogram skeleton with a single centrally mounted actuator that enables folding and unfolding along its diagonal. The core design philosophy is to achieve essential MSRR functionalities such as morphing, docking, and locomotion with minimal control complexity. This enables a continuous and stable reconfiguration process that is independent of the surrounding medium, allowing the system to reliably form various configurations in diverse environments. To leverage the unique kinematics of Rhombot, we introduce morphpivoting, a novel motion primitive for reconfiguration that differs from advanced MSRR systems, and propose a strategy for its continuous execution. Finally, a series of physical experiments validate the module&#39;s stable reconfiguration ability, as well as its positional and docking accuracy."
    }
  ],
  "机器人与自动化研究中心": [
    {
      "title": "EgoHandICL: Egocentric 3D Hand Reconstruction with In-Context Learning",
      "url": "https://arxiv.org/abs/2601.19850",
      "date": "2026-01-27",
      "authors_text": "Binzhu Xie, Shi Qiu, Sicheng Zhang, Yinqiao Wang, Hao Xu",
      "is_highlight": false,
      "score": 76.0,
      "summary": "EgoHandICL introduces an innovative in-context learning framework for robust 3D hand reconstruction in egocentric vision, enhancing performance through exemplar retrieval and multimodal context integration.",
      "debug_abstract": "Robust 3D hand reconstruction in egocentric vision is challenging due to depth ambiguity, self-occlusion, and complex hand-object interactions. Prior methods mitigate these issues by scaling training data or adding auxiliary cues, but they often struggle in unseen contexts. We present EgoHandICL, the first in-context learning (ICL) framework for 3D hand reconstruction that improves semantic alignment, visual consistency, and robustness under challenging egocentric conditions. EgoHandICL introduces complementary exemplar retrieval guided by vision-language models (VLMs), an ICL-tailored tokenizer for multimodal context, and a masked autoencoder (MAE)-based architecture trained with hand-guided geometric and perceptual objectives. Experiments on ARCTIC and EgoExo4D show consistent gains over state-of-the-art methods. We also demonstrate real-world generalization and improve EgoVLM hand-object interaction reasoning by using reconstructed hands as visual prompts. Code and data: this https URL"
    },
    {
      "title": "Component-Level Lesioning of Language Models Reveals Clinically Aligned Aphasia Phenotypes",
      "url": "https://arxiv.org/abs/2601.19723",
      "date": "2026-01-27",
      "authors_text": "Yifan Wang, Jichen Zheng, Jingyuan Sun, Yunhao Zhang, Chunyu Ye",
      "is_highlight": false,
      "score": 62.0,
      "summary": "This study presents a framework for simulating aphasia in language models by selectively perturbing components, revealing clinically relevant language impairments and enhancing understanding of language function degradation.",
      "debug_abstract": "Large language models (LLMs) increasingly exhibit human-like linguistic behaviors and internal representations that they could serve as computational simulators of language cognition. We ask whether LLMs can be systematically manipulated to reproduce language-production impairments characteristic of aphasia following focal brain lesions. Such models could provide scalable proxies for testing rehabilitation hypotheses, and offer a controlled framework for probing the functional organization of language. We introduce a clinically grounded, component-level framework that simulates aphasia by selectively perturbing functional components in LLMs, and apply it to both modular Mixture-of-Experts models and dense Transformers using a unified intervention interface. Our pipeline (i) identifies subtype-linked components for Broca&#39;s and Wernicke&#39;s aphasia, (ii) interprets these components with linguistic probing tasks, and (iii) induces graded impairments by progressively perturbing the top-k subtype-linked components, evaluating outcomes with Western Aphasia Battery (WAB) subtests summarized by Aphasia Quotient (AQ). Across architectures and lesioning strategies, subtype-targeted perturbations yield more systematic, aphasia-like regressions than size-matched random perturbations, and MoE modularity supports more localized and interpretable phenotype-to-component mappings. These findings suggest that modular LLMs, combined with clinically informed component perturbations, provide a promising platform for simulating aphasic language production and studying how distinct language functions degrade under targeted disruptions."
    },
    {
      "title": "Rhombot: Rhombus-shaped Modular Robots for Stable, Medium-Independent Reconfiguration Motion",
      "url": "https://arxiv.org/abs/2601.19529",
      "date": "2026-01-27",
      "authors_text": "Jie Gu, Yirui Sun, Zhihao Xia, Tin Lun Lam, Chunxu Tian",
      "is_highlight": false,
      "score": 86.0,
      "summary": "Rhombot is a modular self-reconfigurable robot featuring rhombus-shaped modules that enable stable, medium-independent morphing and locomotion through a novel motion primitive called morphpivoting.",
      "debug_abstract": "In this paper, we present Rhombot, a novel deformable planar lattice modular self-reconfigurable robot (MSRR) with a rhombus shaped module. Each module consists of a parallelogram skeleton with a single centrally mounted actuator that enables folding and unfolding along its diagonal. The core design philosophy is to achieve essential MSRR functionalities such as morphing, docking, and locomotion with minimal control complexity. This enables a continuous and stable reconfiguration process that is independent of the surrounding medium, allowing the system to reliably form various configurations in diverse environments. To leverage the unique kinematics of Rhombot, we introduce morphpivoting, a novel motion primitive for reconfiguration that differs from advanced MSRR systems, and propose a strategy for its continuous execution. Finally, a series of physical experiments validate the module&#39;s stable reconfiguration ability, as well as its positional and docking accuracy."
    }
  ],
  "机器人与人工智能实验室": [
    {
      "title": "EgoHandICL: Egocentric 3D Hand Reconstruction with In-Context Learning",
      "url": "https://arxiv.org/abs/2601.19850",
      "date": "2026-01-27",
      "authors_text": "Binzhu Xie, Shi Qiu, Sicheng Zhang, Yinqiao Wang, Hao Xu",
      "is_highlight": false,
      "score": 76.0,
      "summary": "EgoHandICL introduces an innovative in-context learning framework for robust 3D hand reconstruction in egocentric vision, enhancing performance through exemplar retrieval and multimodal context integration.",
      "debug_abstract": "Robust 3D hand reconstruction in egocentric vision is challenging due to depth ambiguity, self-occlusion, and complex hand-object interactions. Prior methods mitigate these issues by scaling training data or adding auxiliary cues, but they often struggle in unseen contexts. We present EgoHandICL, the first in-context learning (ICL) framework for 3D hand reconstruction that improves semantic alignment, visual consistency, and robustness under challenging egocentric conditions. EgoHandICL introduces complementary exemplar retrieval guided by vision-language models (VLMs), an ICL-tailored tokenizer for multimodal context, and a masked autoencoder (MAE)-based architecture trained with hand-guided geometric and perceptual objectives. Experiments on ARCTIC and EgoExo4D show consistent gains over state-of-the-art methods. We also demonstrate real-world generalization and improve EgoVLM hand-object interaction reasoning by using reconstructed hands as visual prompts. Code and data: this https URL"
    },
    {
      "title": "Rhombot: Rhombus-shaped Modular Robots for Stable, Medium-Independent Reconfiguration Motion",
      "url": "https://arxiv.org/abs/2601.19529",
      "date": "2026-01-27",
      "authors_text": "Jie Gu, Yirui Sun, Zhihao Xia, Tin Lun Lam, Chunxu Tian",
      "is_highlight": false,
      "score": 86.0,
      "summary": "Rhombot is a modular self-reconfigurable robot featuring rhombus-shaped modules that enable stable, medium-independent morphing and locomotion through a novel motion primitive called morphpivoting.",
      "debug_abstract": "In this paper, we present Rhombot, a novel deformable planar lattice modular self-reconfigurable robot (MSRR) with a rhombus shaped module. Each module consists of a parallelogram skeleton with a single centrally mounted actuator that enables folding and unfolding along its diagonal. The core design philosophy is to achieve essential MSRR functionalities such as morphing, docking, and locomotion with minimal control complexity. This enables a continuous and stable reconfiguration process that is independent of the surrounding medium, allowing the system to reliably form various configurations in diverse environments. To leverage the unique kinematics of Rhombot, we introduce morphpivoting, a novel motion primitive for reconfiguration that differs from advanced MSRR systems, and propose a strategy for its continuous execution. Finally, a series of physical experiments validate the module&#39;s stable reconfiguration ability, as well as its positional and docking accuracy."
    }
  ],
  "Multimedia Lab (MMLab)": [
    {
      "title": "EgoHandICL: Egocentric 3D Hand Reconstruction with In-Context Learning",
      "url": "https://arxiv.org/abs/2601.19850",
      "date": "2026-01-27",
      "authors_text": "Binzhu Xie, Shi Qiu, Sicheng Zhang, Yinqiao Wang, Hao Xu",
      "is_highlight": false,
      "score": 76.0,
      "summary": "EgoHandICL introduces an innovative in-context learning framework for robust 3D hand reconstruction in egocentric vision, enhancing performance through exemplar retrieval and multimodal context integration.",
      "debug_abstract": "Robust 3D hand reconstruction in egocentric vision is challenging due to depth ambiguity, self-occlusion, and complex hand-object interactions. Prior methods mitigate these issues by scaling training data or adding auxiliary cues, but they often struggle in unseen contexts. We present EgoHandICL, the first in-context learning (ICL) framework for 3D hand reconstruction that improves semantic alignment, visual consistency, and robustness under challenging egocentric conditions. EgoHandICL introduces complementary exemplar retrieval guided by vision-language models (VLMs), an ICL-tailored tokenizer for multimodal context, and a masked autoencoder (MAE)-based architecture trained with hand-guided geometric and perceptual objectives. Experiments on ARCTIC and EgoExo4D show consistent gains over state-of-the-art methods. We also demonstrate real-world generalization and improve EgoVLM hand-object interaction reasoning by using reconstructed hands as visual prompts. Code and data: this https URL"
    },
    {
      "title": "Rhombot: Rhombus-shaped Modular Robots for Stable, Medium-Independent Reconfiguration Motion",
      "url": "https://arxiv.org/abs/2601.19529",
      "date": "2026-01-27",
      "authors_text": "Jie Gu, Yirui Sun, Zhihao Xia, Tin Lun Lam, Chunxu Tian",
      "is_highlight": false,
      "score": 86.0,
      "summary": "Rhombot is a modular self-reconfigurable robot featuring rhombus-shaped modules that enable stable, medium-independent morphing and locomotion through a novel motion primitive called morphpivoting.",
      "debug_abstract": "In this paper, we present Rhombot, a novel deformable planar lattice modular self-reconfigurable robot (MSRR) with a rhombus shaped module. Each module consists of a parallelogram skeleton with a single centrally mounted actuator that enables folding and unfolding along its diagonal. The core design philosophy is to achieve essential MSRR functionalities such as morphing, docking, and locomotion with minimal control complexity. This enables a continuous and stable reconfiguration process that is independent of the surrounding medium, allowing the system to reliably form various configurations in diverse environments. To leverage the unique kinematics of Rhombot, we introduce morphpivoting, a novel motion primitive for reconfiguration that differs from advanced MSRR systems, and propose a strategy for its continuous execution. Finally, a series of physical experiments validate the module&#39;s stable reconfiguration ability, as well as its positional and docking accuracy."
    }
  ],
  "中国人民大学": [
    {
      "title": "AtomMem : Learnable Dynamic Agentic Memory with Atomic Memory Operation",
      "url": "https://arxiv.org/abs/2601.08323",
      "date": "2026-01-27",
      "authors_text": "Yupeng Huo, Yaxi Lu, Zhong Zhang, Haotian Chen, Yankai Lin",
      "is_highlight": false,
      "score": 82.0,
      "summary": "AtomMem introduces a learnable dynamic memory framework that optimizes memory management through atomic CRUD operations, significantly outperforming static methods in long-horizon tasks.",
      "debug_abstract": "Equipping agents with memory is essential for solving real-world long-horizon problems. However, most existing agent memory mechanisms rely on static and hand-crafted workflows. This limits the performance and generalization ability of these memory designs, which highlights the need for a more flexible, learning-based memory framework. In this paper, we propose AtomMem, which reframes memory management as a dynamic decision-making problem. We deconstruct high-level memory processes into fundamental atomic CRUD (Create, Read, Update, Delete) operations, transforming the memory workflow into a learnable decision process. By combining supervised fine-tuning with reinforcement learning, AtomMem learns an autonomous, task-aligned policy to orchestrate memory behaviors tailored to specific task demands. Experimental results across 3 long-context benchmarks demonstrate that the trained AtomMem-8B consistently outperforms prior static-workflow memory methods. Further analysis of training dynamics shows that our learning-based formulation enables the agent to discover structured, task-aligned memory management strategies, highlighting a key advantage over predefined routines."
    }
  ],
  "机器人系统实验室 (RSL)": [
    {
      "title": "DeFM: Learning Foundation Representations from Depth for Robotics",
      "url": "https://arxiv.org/abs/2601.18923",
      "date": "2026-01-26",
      "authors_text": "Manthan Patel, Jonas Frey, Mayank Mittal, Fan Yang, Alexander Hansson",
      "is_highlight": true,
      "score": 88.0,
      "summary": "DeFM is a self-supervised foundation model that learns robust geometric and semantic representations from depth images, achieving state-of-the-art performance across various robotic tasks.",
      "debug_abstract": "Depth sensors are widely deployed across robotic platforms, and advances in fast, high-fidelity depth simulation have enabled robotic policies trained on depth observations to achieve robust sim-to-real transfer for a wide range of tasks. Despite this, representation learning for depth modality remains underexplored compared to RGB, where large-scale foundation models now define the state of the art. To address this gap, we present DeFM, a self-supervised foundation model trained entirely on depth images for robotic applications. Using a DINO-style self-distillation objective on a curated dataset of 60M depth images, DeFM learns geometric and semantic representations that generalize to diverse environments, tasks, and sensors. To retain metric awareness across multiple scales, we introduce a novel input normalization strategy. We further distill DeFM into compact models suitable for resource-constrained robotic systems. When evaluated on depth-based classification, segmentation, navigation, locomotion, and manipulation benchmarks, DeFM achieves state-of-the-art performance and demonstrates strong generalization from simulation to real-world environments. We release all our pretrained models, which can be adopted off-the-shelf for depth-based robotic learning without task-specific fine-tuning. Webpage: this https URL"
    },
    {
      "title": "Safe Exploration via Policy Priors",
      "url": "https://arxiv.org/abs/2601.19612",
      "date": "2026-01-27",
      "authors_text": "Manuel Wendl, Yarden As, Manish Prajapat, Anton Pollak, Stelian Coros",
      "is_highlight": false,
      "score": 84.0,
      "summary": "The paper presents SOOPER, a safe exploration method for reinforcement learning that uses conservative policy priors to ensure safety and optimal policy convergence during online learning.",
      "debug_abstract": "Safe exploration is a key requirement for reinforcement learning (RL) agents to learn and adapt online, beyond controlled (e.g. simulated) environments. In this work, we tackle this challenge by utilizing suboptimal yet conservative policies (e.g., obtained from offline data or simulators) as priors. Our approach, SOOPER, uses probabilistic dynamics models to optimistically explore, yet pessimistically fall back to the conservative policy prior if needed. We prove that SOOPER guarantees safety throughout learning, and establish convergence to an optimal policy by bounding its cumulative regret. Extensive experiments on key safe RL benchmarks and real-world hardware demonstrate that SOOPER is scalable, outperforms the state-of-the-art and validate our theoretical guarantees in practice."
    }
  ],
  "Sensing, Interaction & Perception Lab (SIPLAB)": [
    {
      "title": "DeFM: Learning Foundation Representations from Depth for Robotics",
      "url": "https://arxiv.org/abs/2601.18923",
      "date": "2026-01-26",
      "authors_text": "Manthan Patel, Jonas Frey, Mayank Mittal, Fan Yang, Alexander Hansson",
      "is_highlight": true,
      "score": 88.0,
      "summary": "DeFM is a self-supervised foundation model that learns robust geometric and semantic representations from depth images, achieving state-of-the-art performance across various robotic tasks.",
      "debug_abstract": "Depth sensors are widely deployed across robotic platforms, and advances in fast, high-fidelity depth simulation have enabled robotic policies trained on depth observations to achieve robust sim-to-real transfer for a wide range of tasks. Despite this, representation learning for depth modality remains underexplored compared to RGB, where large-scale foundation models now define the state of the art. To address this gap, we present DeFM, a self-supervised foundation model trained entirely on depth images for robotic applications. Using a DINO-style self-distillation objective on a curated dataset of 60M depth images, DeFM learns geometric and semantic representations that generalize to diverse environments, tasks, and sensors. To retain metric awareness across multiple scales, we introduce a novel input normalization strategy. We further distill DeFM into compact models suitable for resource-constrained robotic systems. When evaluated on depth-based classification, segmentation, navigation, locomotion, and manipulation benchmarks, DeFM achieves state-of-the-art performance and demonstrates strong generalization from simulation to real-world environments. We release all our pretrained models, which can be adopted off-the-shelf for depth-based robotic learning without task-specific fine-tuning. Webpage: this https URL"
    },
    {
      "title": "Safe Exploration via Policy Priors",
      "url": "https://arxiv.org/abs/2601.19612",
      "date": "2026-01-27",
      "authors_text": "Manuel Wendl, Yarden As, Manish Prajapat, Anton Pollak, Stelian Coros",
      "is_highlight": false,
      "score": 84.0,
      "summary": "The paper presents SOOPER, a safe exploration method for reinforcement learning that uses conservative policy priors to ensure safety and optimal policy convergence during online learning.",
      "debug_abstract": "Safe exploration is a key requirement for reinforcement learning (RL) agents to learn and adapt online, beyond controlled (e.g. simulated) environments. In this work, we tackle this challenge by utilizing suboptimal yet conservative policies (e.g., obtained from offline data or simulators) as priors. Our approach, SOOPER, uses probabilistic dynamics models to optimistically explore, yet pessimistically fall back to the conservative policy prior if needed. We prove that SOOPER guarantees safety throughout learning, and establish convergence to an optimal policy by bounding its cumulative regret. Extensive experiments on key safe RL benchmarks and real-world hardware demonstrate that SOOPER is scalable, outperforms the state-of-the-art and validate our theoretical guarantees in practice."
    }
  ],
  "电子科技大学": [
    {
      "title": "AC^2-VLA: Action-Context-Aware Adaptive Computation in Vision-Language-Action Models for Efficient Robotic Manipulation",
      "url": "https://arxiv.org/abs/2601.19634",
      "date": "2026-01-27",
      "authors_text": "Wenda Yu, Tianshi Wang, Fengling Li, Jingjing Li, Lei Zhu",
      "is_highlight": false,
      "score": 94.0,
      "summary": "AC^2-VLA enhances robotic manipulation efficiency by adaptively optimizing computation based on action context, achieving significant speedup and reduced resource usage without sacrificing performance.",
      "debug_abstract": "Vision-Language-Action (VLA) models have demonstrated strong performance in robotic manipulation, yet their closed-loop deployment is hindered by the high latency and compute cost of repeatedly running large vision-language backbones at every timestep. We observe that VLA inference exhibits structured redundancies across temporal, spatial, and depth dimensions, and that most existing efficiency methods ignore action context, despite its central role in embodied tasks. To address this gap, we propose Action-Context-aware Adaptive Computation for VLA models (AC^2-VLA), a unified framework that conditions computation on current visual observations, language instructions, and previous action states. Based on this action-centric context, AC^2-VLA adaptively performs cognition reuse across timesteps, token pruning, and selective execution of model components within a unified mechanism. To train the adaptive policy, we introduce an action-guided self-distillation scheme that preserves the behavior of the dense VLA policy while enabling structured sparsification that transfers across tasks and settings. Extensive experiments on robotic manipulation benchmarks show that AC^2-VLA achieves up to a 1.79\\times speedup while reducing FLOPs to 29.4% of the dense baseline, with comparable task success."
    },
    {
      "title": "R^3: Replay, Reflection, and Ranking Rewards for LLM Reinforcement Learning",
      "url": "https://arxiv.org/abs/2601.19620",
      "date": "2026-01-27",
      "authors_text": "Zhizheng Jiang, Kang Zhao, Weikai Xu, Xinkui Lin, Wei Liu",
      "is_highlight": false,
      "score": 68.0,
      "summary": "The R^3 framework enhances reinforcement learning for large reasoning models by integrating cross-context replay, in-context self-reflection, and structural entropy ranking rewards, achieving state-of-the-art performance in math tasks.",
      "debug_abstract": "Large reasoning models (LRMs) aim to solve diverse and complex problems through structured reasoning. Recent advances in group-based policy optimization methods have shown promise in enabling stable advantage estimation without reliance on process-level annotations. However, these methods rely on advantage gaps induced by high-quality samples within the same batch, which makes the training process fragile and inefficient when intra-group advantages collapse under challenging tasks. To address these problems, we propose a reinforcement learning mechanism named \\emph{\\textbf{R^3}} that along three directions: (1) a \\emph{cross-context \\underline{\\textbf{R}}eplay} strategy that maintains the intra-group advantage by recalling valuable examples from historical trajectories of the same query, (2) an \\emph{in-context self-\\underline{\\textbf{R}}eflection} mechanism enabling models to refine outputs by leveraging past failures, and (3) a \\emph{structural entropy \\underline{\\textbf{R}}anking reward}, which assigns relative rewards to truncated or failed samples by ranking responses based on token-level entropy patterns, capturing both local exploration and global stability. We implement our method on Deepseek-R1-Distill-Qwen-1.5B and train it on the DeepscaleR-40k in the math domain. Experiments demonstrate our method achieves SoTA performance on several math benchmarks, representing significant improvements and fewer reasoning tokens over the base models. Code and model will be released."
    }
  ],
  "MMLab@NTU": [
    {
      "title": "RoamScene3D: Immersive Text-to-3D Scene Generation via Adaptive Object-aware Roaming",
      "url": "https://arxiv.org/abs/2601.19433",
      "date": "2026-01-27",
      "authors_text": "Jisheng Chu, Wenrui Li, Rui Zhao, Wangmeng Zuo, Shifeng Chen",
      "is_highlight": false,
      "score": 67.0,
      "summary": "RoamScene3D introduces a novel framework for immersive text-to-3D scene generation that enhances spatial awareness and object relations, outperforming existing methods in realism and consistency.",
      "debug_abstract": "Generating immersive 3D scenes from texts is a core task in computer vision, crucial for applications in virtual reality and game development. Despite the promise of leveraging 2D diffusion priors, existing methods suffer from spatial blindness and rely on predefined trajectories that fail to exploit the inner relationships among salient objects. Consequently, these approaches are unable to comprehend the semantic layout, preventing them from exploring the scene adaptively to infer occluded content. Moreover, current inpainting models operate in 2D image space, struggling to plausibly fill holes caused by camera motion. To address these limitations, we propose RoamScene3D, a novel framework that bridges the gap between semantic guidance and spatial generation. Our method reasons about the semantic relations among objects and produces consistent and photorealistic scenes. Specifically, we employ a vision-language model (VLM) to construct a scene graph that encodes object relations, guiding the camera to perceive salient object boundaries and plan an adaptive roaming trajectory. Furthermore, to mitigate the limitations of static 2D priors, we introduce a Motion-Injected Inpainting model that is fine-tuned on a synthetic panoramic dataset integrating authentic camera trajectories, making it adaptive to camera motion. Extensive experiments demonstrate that with semantic reasoning and geometric constraints, our method significantly outperforms state-of-the-art approaches in producing consistent and photorealistic scenes. Our code is available at this https URL."
    },
    {
      "title": "Video-KTR: Reinforcing Video Reasoning via Key Token Attribution",
      "url": "https://arxiv.org/abs/2601.19686",
      "date": "2026-01-27",
      "authors_text": "Ziyue Wang, Sheng Jin, Zhongrong Zuo, Jiawei Wu, Han Qiu",
      "is_highlight": false,
      "score": 66.0,
      "summary": "Video-KTR enhances video reasoning in multimodal models by selectively reinforcing key tokens through a novel attribution framework, achieving state-of-the-art performance across multiple benchmarks.",
      "debug_abstract": "Reinforcement learning (RL) has shown strong potential for enhancing reasoning in multimodal large language models, yet existing video reasoning methods often rely on coarse sequence-level rewards or single-factor token selection, neglecting fine-grained links among visual inputs, temporal dynamics, and linguistic outputs, limiting both accuracy and interpretability. We propose Video-KTR, a modality-aware policy shaping framework that performs selective, token-level RL by combining three attribution signals: (1) visual-aware tokens identified via counterfactual masking to reveal perceptual dependence; (2) temporal-aware tokens detected through frame shuffling to expose temporal sensitivity; and (3) high-entropy tokens signaling predictive uncertainty. By reinforcing only these key tokens, Video-KTR focuses learning on semantically informative, modality-sensitive content while filtering out low-value tokens. Across five challenging benchmarks, Video-KTR achieves state-of-the-art or highly competitive results, achieving 42.7\\% on Video-Holmes (surpassing GPT-4o) with consistent gains on both reasoning and general video understanding tasks. Ablation studies verify the complementary roles of the attribution signals and the robustness of targeted token-level updates. Overall, Video-KTR improves accuracy and interpretability, offering a simple, drop-in extension to RL for complex video reasoning. Our code and models are available at this https URL."
    }
  ],
  "PINE Lab": [
    {
      "title": "RoamScene3D: Immersive Text-to-3D Scene Generation via Adaptive Object-aware Roaming",
      "url": "https://arxiv.org/abs/2601.19433",
      "date": "2026-01-27",
      "authors_text": "Jisheng Chu, Wenrui Li, Rui Zhao, Wangmeng Zuo, Shifeng Chen",
      "is_highlight": false,
      "score": 67.0,
      "summary": "RoamScene3D introduces a novel framework for immersive text-to-3D scene generation that enhances spatial awareness and object relations, outperforming existing methods in realism and consistency.",
      "debug_abstract": "Generating immersive 3D scenes from texts is a core task in computer vision, crucial for applications in virtual reality and game development. Despite the promise of leveraging 2D diffusion priors, existing methods suffer from spatial blindness and rely on predefined trajectories that fail to exploit the inner relationships among salient objects. Consequently, these approaches are unable to comprehend the semantic layout, preventing them from exploring the scene adaptively to infer occluded content. Moreover, current inpainting models operate in 2D image space, struggling to plausibly fill holes caused by camera motion. To address these limitations, we propose RoamScene3D, a novel framework that bridges the gap between semantic guidance and spatial generation. Our method reasons about the semantic relations among objects and produces consistent and photorealistic scenes. Specifically, we employ a vision-language model (VLM) to construct a scene graph that encodes object relations, guiding the camera to perceive salient object boundaries and plan an adaptive roaming trajectory. Furthermore, to mitigate the limitations of static 2D priors, we introduce a Motion-Injected Inpainting model that is fine-tuned on a synthetic panoramic dataset integrating authentic camera trajectories, making it adaptive to camera motion. Extensive experiments demonstrate that with semantic reasoning and geometric constraints, our method significantly outperforms state-of-the-art approaches in producing consistent and photorealistic scenes. Our code is available at this https URL."
    },
    {
      "title": "Video-KTR: Reinforcing Video Reasoning via Key Token Attribution",
      "url": "https://arxiv.org/abs/2601.19686",
      "date": "2026-01-27",
      "authors_text": "Ziyue Wang, Sheng Jin, Zhongrong Zuo, Jiawei Wu, Han Qiu",
      "is_highlight": false,
      "score": 66.0,
      "summary": "Video-KTR enhances video reasoning in multimodal models by selectively reinforcing key tokens through a novel attribution framework, achieving state-of-the-art performance across multiple benchmarks.",
      "debug_abstract": "Reinforcement learning (RL) has shown strong potential for enhancing reasoning in multimodal large language models, yet existing video reasoning methods often rely on coarse sequence-level rewards or single-factor token selection, neglecting fine-grained links among visual inputs, temporal dynamics, and linguistic outputs, limiting both accuracy and interpretability. We propose Video-KTR, a modality-aware policy shaping framework that performs selective, token-level RL by combining three attribution signals: (1) visual-aware tokens identified via counterfactual masking to reveal perceptual dependence; (2) temporal-aware tokens detected through frame shuffling to expose temporal sensitivity; and (3) high-entropy tokens signaling predictive uncertainty. By reinforcing only these key tokens, Video-KTR focuses learning on semantically informative, modality-sensitive content while filtering out low-value tokens. Across five challenging benchmarks, Video-KTR achieves state-of-the-art or highly competitive results, achieving 42.7\\% on Video-Holmes (surpassing GPT-4o) with consistent gains on both reasoning and general video understanding tasks. Ablation studies verify the complementary roles of the attribution signals and the robustness of targeted token-level updates. Overall, Video-KTR improves accuracy and interpretability, offering a simple, drop-in extension to RL for complex video reasoning. Our code and models are available at this https URL."
    }
  ],
  "MReaLLab": [
    {
      "title": "RoamScene3D: Immersive Text-to-3D Scene Generation via Adaptive Object-aware Roaming",
      "url": "https://arxiv.org/abs/2601.19433",
      "date": "2026-01-27",
      "authors_text": "Jisheng Chu, Wenrui Li, Rui Zhao, Wangmeng Zuo, Shifeng Chen",
      "is_highlight": false,
      "score": 67.0,
      "summary": "RoamScene3D introduces a novel framework for immersive text-to-3D scene generation that enhances spatial awareness and object relations, outperforming existing methods in realism and consistency.",
      "debug_abstract": "Generating immersive 3D scenes from texts is a core task in computer vision, crucial for applications in virtual reality and game development. Despite the promise of leveraging 2D diffusion priors, existing methods suffer from spatial blindness and rely on predefined trajectories that fail to exploit the inner relationships among salient objects. Consequently, these approaches are unable to comprehend the semantic layout, preventing them from exploring the scene adaptively to infer occluded content. Moreover, current inpainting models operate in 2D image space, struggling to plausibly fill holes caused by camera motion. To address these limitations, we propose RoamScene3D, a novel framework that bridges the gap between semantic guidance and spatial generation. Our method reasons about the semantic relations among objects and produces consistent and photorealistic scenes. Specifically, we employ a vision-language model (VLM) to construct a scene graph that encodes object relations, guiding the camera to perceive salient object boundaries and plan an adaptive roaming trajectory. Furthermore, to mitigate the limitations of static 2D priors, we introduce a Motion-Injected Inpainting model that is fine-tuned on a synthetic panoramic dataset integrating authentic camera trajectories, making it adaptive to camera motion. Extensive experiments demonstrate that with semantic reasoning and geometric constraints, our method significantly outperforms state-of-the-art approaches in producing consistent and photorealistic scenes. Our code is available at this https URL."
    },
    {
      "title": "Video-KTR: Reinforcing Video Reasoning via Key Token Attribution",
      "url": "https://arxiv.org/abs/2601.19686",
      "date": "2026-01-27",
      "authors_text": "Ziyue Wang, Sheng Jin, Zhongrong Zuo, Jiawei Wu, Han Qiu",
      "is_highlight": false,
      "score": 66.0,
      "summary": "Video-KTR enhances video reasoning in multimodal models by selectively reinforcing key tokens through a novel attribution framework, achieving state-of-the-art performance across multiple benchmarks.",
      "debug_abstract": "Reinforcement learning (RL) has shown strong potential for enhancing reasoning in multimodal large language models, yet existing video reasoning methods often rely on coarse sequence-level rewards or single-factor token selection, neglecting fine-grained links among visual inputs, temporal dynamics, and linguistic outputs, limiting both accuracy and interpretability. We propose Video-KTR, a modality-aware policy shaping framework that performs selective, token-level RL by combining three attribution signals: (1) visual-aware tokens identified via counterfactual masking to reveal perceptual dependence; (2) temporal-aware tokens detected through frame shuffling to expose temporal sensitivity; and (3) high-entropy tokens signaling predictive uncertainty. By reinforcing only these key tokens, Video-KTR focuses learning on semantically informative, modality-sensitive content while filtering out low-value tokens. Across five challenging benchmarks, Video-KTR achieves state-of-the-art or highly competitive results, achieving 42.7\\% on Video-Holmes (surpassing GPT-4o) with consistent gains on both reasoning and general video understanding tasks. Ablation studies verify the complementary roles of the attribution signals and the robustness of targeted token-level updates. Overall, Video-KTR improves accuracy and interpretability, offering a simple, drop-in extension to RL for complex video reasoning. Our code and models are available at this https URL."
    }
  ],
  "S-Lab for Advanced Intelligence": [
    {
      "title": "RoamScene3D: Immersive Text-to-3D Scene Generation via Adaptive Object-aware Roaming",
      "url": "https://arxiv.org/abs/2601.19433",
      "date": "2026-01-27",
      "authors_text": "Jisheng Chu, Wenrui Li, Rui Zhao, Wangmeng Zuo, Shifeng Chen",
      "is_highlight": false,
      "score": 67.0,
      "summary": "RoamScene3D introduces a novel framework for immersive text-to-3D scene generation that enhances spatial awareness and object relations, outperforming existing methods in realism and consistency.",
      "debug_abstract": "Generating immersive 3D scenes from texts is a core task in computer vision, crucial for applications in virtual reality and game development. Despite the promise of leveraging 2D diffusion priors, existing methods suffer from spatial blindness and rely on predefined trajectories that fail to exploit the inner relationships among salient objects. Consequently, these approaches are unable to comprehend the semantic layout, preventing them from exploring the scene adaptively to infer occluded content. Moreover, current inpainting models operate in 2D image space, struggling to plausibly fill holes caused by camera motion. To address these limitations, we propose RoamScene3D, a novel framework that bridges the gap between semantic guidance and spatial generation. Our method reasons about the semantic relations among objects and produces consistent and photorealistic scenes. Specifically, we employ a vision-language model (VLM) to construct a scene graph that encodes object relations, guiding the camera to perceive salient object boundaries and plan an adaptive roaming trajectory. Furthermore, to mitigate the limitations of static 2D priors, we introduce a Motion-Injected Inpainting model that is fine-tuned on a synthetic panoramic dataset integrating authentic camera trajectories, making it adaptive to camera motion. Extensive experiments demonstrate that with semantic reasoning and geometric constraints, our method significantly outperforms state-of-the-art approaches in producing consistent and photorealistic scenes. Our code is available at this https URL."
    },
    {
      "title": "Video-KTR: Reinforcing Video Reasoning via Key Token Attribution",
      "url": "https://arxiv.org/abs/2601.19686",
      "date": "2026-01-27",
      "authors_text": "Ziyue Wang, Sheng Jin, Zhongrong Zuo, Jiawei Wu, Han Qiu",
      "is_highlight": false,
      "score": 66.0,
      "summary": "Video-KTR enhances video reasoning in multimodal models by selectively reinforcing key tokens through a novel attribution framework, achieving state-of-the-art performance across multiple benchmarks.",
      "debug_abstract": "Reinforcement learning (RL) has shown strong potential for enhancing reasoning in multimodal large language models, yet existing video reasoning methods often rely on coarse sequence-level rewards or single-factor token selection, neglecting fine-grained links among visual inputs, temporal dynamics, and linguistic outputs, limiting both accuracy and interpretability. We propose Video-KTR, a modality-aware policy shaping framework that performs selective, token-level RL by combining three attribution signals: (1) visual-aware tokens identified via counterfactual masking to reveal perceptual dependence; (2) temporal-aware tokens detected through frame shuffling to expose temporal sensitivity; and (3) high-entropy tokens signaling predictive uncertainty. By reinforcing only these key tokens, Video-KTR focuses learning on semantically informative, modality-sensitive content while filtering out low-value tokens. Across five challenging benchmarks, Video-KTR achieves state-of-the-art or highly competitive results, achieving 42.7\\% on Video-Holmes (surpassing GPT-4o) with consistent gains on both reasoning and general video understanding tasks. Ablation studies verify the complementary roles of the attribution signals and the robustness of targeted token-level updates. Overall, Video-KTR improves accuracy and interpretability, offering a simple, drop-in extension to RL for complex video reasoning. Our code and models are available at this https URL."
    }
  ],
  "MARS Lab": [
    {
      "title": "RoamScene3D: Immersive Text-to-3D Scene Generation via Adaptive Object-aware Roaming",
      "url": "https://arxiv.org/abs/2601.19433",
      "date": "2026-01-27",
      "authors_text": "Jisheng Chu, Wenrui Li, Rui Zhao, Wangmeng Zuo, Shifeng Chen",
      "is_highlight": false,
      "score": 67.0,
      "summary": "RoamScene3D introduces a novel framework for immersive text-to-3D scene generation that enhances spatial awareness and object relations, outperforming existing methods in realism and consistency.",
      "debug_abstract": "Generating immersive 3D scenes from texts is a core task in computer vision, crucial for applications in virtual reality and game development. Despite the promise of leveraging 2D diffusion priors, existing methods suffer from spatial blindness and rely on predefined trajectories that fail to exploit the inner relationships among salient objects. Consequently, these approaches are unable to comprehend the semantic layout, preventing them from exploring the scene adaptively to infer occluded content. Moreover, current inpainting models operate in 2D image space, struggling to plausibly fill holes caused by camera motion. To address these limitations, we propose RoamScene3D, a novel framework that bridges the gap between semantic guidance and spatial generation. Our method reasons about the semantic relations among objects and produces consistent and photorealistic scenes. Specifically, we employ a vision-language model (VLM) to construct a scene graph that encodes object relations, guiding the camera to perceive salient object boundaries and plan an adaptive roaming trajectory. Furthermore, to mitigate the limitations of static 2D priors, we introduce a Motion-Injected Inpainting model that is fine-tuned on a synthetic panoramic dataset integrating authentic camera trajectories, making it adaptive to camera motion. Extensive experiments demonstrate that with semantic reasoning and geometric constraints, our method significantly outperforms state-of-the-art approaches in producing consistent and photorealistic scenes. Our code is available at this https URL."
    },
    {
      "title": "Video-KTR: Reinforcing Video Reasoning via Key Token Attribution",
      "url": "https://arxiv.org/abs/2601.19686",
      "date": "2026-01-27",
      "authors_text": "Ziyue Wang, Sheng Jin, Zhongrong Zuo, Jiawei Wu, Han Qiu",
      "is_highlight": false,
      "score": 66.0,
      "summary": "Video-KTR enhances video reasoning in multimodal models by selectively reinforcing key tokens through a novel attribution framework, achieving state-of-the-art performance across multiple benchmarks.",
      "debug_abstract": "Reinforcement learning (RL) has shown strong potential for enhancing reasoning in multimodal large language models, yet existing video reasoning methods often rely on coarse sequence-level rewards or single-factor token selection, neglecting fine-grained links among visual inputs, temporal dynamics, and linguistic outputs, limiting both accuracy and interpretability. We propose Video-KTR, a modality-aware policy shaping framework that performs selective, token-level RL by combining three attribution signals: (1) visual-aware tokens identified via counterfactual masking to reveal perceptual dependence; (2) temporal-aware tokens detected through frame shuffling to expose temporal sensitivity; and (3) high-entropy tokens signaling predictive uncertainty. By reinforcing only these key tokens, Video-KTR focuses learning on semantically informative, modality-sensitive content while filtering out low-value tokens. Across five challenging benchmarks, Video-KTR achieves state-of-the-art or highly competitive results, achieving 42.7\\% on Video-Holmes (surpassing GPT-4o) with consistent gains on both reasoning and general video understanding tasks. Ablation studies verify the complementary roles of the attribution signals and the robustness of targeted token-level updates. Overall, Video-KTR improves accuracy and interpretability, offering a simple, drop-in extension to RL for complex video reasoning. Our code and models are available at this https URL."
    }
  ],
  "ROSE Lab": [
    {
      "title": "RoamScene3D: Immersive Text-to-3D Scene Generation via Adaptive Object-aware Roaming",
      "url": "https://arxiv.org/abs/2601.19433",
      "date": "2026-01-27",
      "authors_text": "Jisheng Chu, Wenrui Li, Rui Zhao, Wangmeng Zuo, Shifeng Chen",
      "is_highlight": false,
      "score": 67.0,
      "summary": "RoamScene3D introduces a novel framework for immersive text-to-3D scene generation that enhances spatial awareness and object relations, outperforming existing methods in realism and consistency.",
      "debug_abstract": "Generating immersive 3D scenes from texts is a core task in computer vision, crucial for applications in virtual reality and game development. Despite the promise of leveraging 2D diffusion priors, existing methods suffer from spatial blindness and rely on predefined trajectories that fail to exploit the inner relationships among salient objects. Consequently, these approaches are unable to comprehend the semantic layout, preventing them from exploring the scene adaptively to infer occluded content. Moreover, current inpainting models operate in 2D image space, struggling to plausibly fill holes caused by camera motion. To address these limitations, we propose RoamScene3D, a novel framework that bridges the gap between semantic guidance and spatial generation. Our method reasons about the semantic relations among objects and produces consistent and photorealistic scenes. Specifically, we employ a vision-language model (VLM) to construct a scene graph that encodes object relations, guiding the camera to perceive salient object boundaries and plan an adaptive roaming trajectory. Furthermore, to mitigate the limitations of static 2D priors, we introduce a Motion-Injected Inpainting model that is fine-tuned on a synthetic panoramic dataset integrating authentic camera trajectories, making it adaptive to camera motion. Extensive experiments demonstrate that with semantic reasoning and geometric constraints, our method significantly outperforms state-of-the-art approaches in producing consistent and photorealistic scenes. Our code is available at this https URL."
    },
    {
      "title": "Video-KTR: Reinforcing Video Reasoning via Key Token Attribution",
      "url": "https://arxiv.org/abs/2601.19686",
      "date": "2026-01-27",
      "authors_text": "Ziyue Wang, Sheng Jin, Zhongrong Zuo, Jiawei Wu, Han Qiu",
      "is_highlight": false,
      "score": 66.0,
      "summary": "Video-KTR enhances video reasoning in multimodal models by selectively reinforcing key tokens through a novel attribution framework, achieving state-of-the-art performance across multiple benchmarks.",
      "debug_abstract": "Reinforcement learning (RL) has shown strong potential for enhancing reasoning in multimodal large language models, yet existing video reasoning methods often rely on coarse sequence-level rewards or single-factor token selection, neglecting fine-grained links among visual inputs, temporal dynamics, and linguistic outputs, limiting both accuracy and interpretability. We propose Video-KTR, a modality-aware policy shaping framework that performs selective, token-level RL by combining three attribution signals: (1) visual-aware tokens identified via counterfactual masking to reveal perceptual dependence; (2) temporal-aware tokens detected through frame shuffling to expose temporal sensitivity; and (3) high-entropy tokens signaling predictive uncertainty. By reinforcing only these key tokens, Video-KTR focuses learning on semantically informative, modality-sensitive content while filtering out low-value tokens. Across five challenging benchmarks, Video-KTR achieves state-of-the-art or highly competitive results, achieving 42.7\\% on Video-Holmes (surpassing GPT-4o) with consistent gains on both reasoning and general video understanding tasks. Ablation studies verify the complementary roles of the attribution signals and the robustness of targeted token-level updates. Overall, Video-KTR improves accuracy and interpretability, offering a simple, drop-in extension to RL for complex video reasoning. Our code and models are available at this https URL."
    }
  ],
  "Affective Intelligence and Robotics Laboratory (AFAR)": [
    {
      "title": "Reimagining Social Robots as Recommender Systems: Foundations, Framework, and Applications",
      "url": "https://arxiv.org/abs/2601.19761",
      "date": "2026-01-27",
      "authors_text": "Jin Huang, Fethiye Irmak Doğan, Hatice Gunes",
      "is_highlight": true,
      "score": 74.0,
      "summary": "The paper proposes integrating recommender systems into social robots to enhance personalization by effectively modeling user preferences and ensuring ethical interactions.",
      "debug_abstract": "Personalization in social robots refers to the ability of the robot to meet the needs and/or preferences of an individual user. Existing approaches typically rely on large language models (LLMs) to generate context-aware responses based on user metadata and historical interactions or on adaptive methods such as reinforcement learning (RL) to learn from users&#39; immediate reactions in real time. However, these approaches fall short of comprehensively capturing user preferences-including long-term, short-term, and fine-grained aspects-, and of using them to rank and select actions, proactively personalize interactions, and ensure ethically responsible adaptations. To address the limitations, we propose drawing on recommender systems (RSs), which specialize in modeling user preferences and providing personalized recommendations. To ensure the integration of RS techniques is well-grounded and seamless throughout the social robot pipeline, we (i) align the paradigms underlying social robots and RSs, (ii) identify key techniques that can enhance personalization in social robots, and (iii) design them as modular, plug-and-play components. This work not only establishes a framework for integrating RS techniques into social robots but also opens a pathway for deep collaboration between the RS and HRI communities, accelerating innovation in both fields."
    }
  ],
  "Bio-Inspired Robotics Laboratory (BIRL)": [
    {
      "title": "Reimagining Social Robots as Recommender Systems: Foundations, Framework, and Applications",
      "url": "https://arxiv.org/abs/2601.19761",
      "date": "2026-01-27",
      "authors_text": "Jin Huang, Fethiye Irmak Doğan, Hatice Gunes",
      "is_highlight": true,
      "score": 74.0,
      "summary": "The paper proposes integrating recommender systems into social robots to enhance personalization by effectively modeling user preferences and ensuring ethical interactions.",
      "debug_abstract": "Personalization in social robots refers to the ability of the robot to meet the needs and/or preferences of an individual user. Existing approaches typically rely on large language models (LLMs) to generate context-aware responses based on user metadata and historical interactions or on adaptive methods such as reinforcement learning (RL) to learn from users&#39; immediate reactions in real time. However, these approaches fall short of comprehensively capturing user preferences-including long-term, short-term, and fine-grained aspects-, and of using them to rank and select actions, proactively personalize interactions, and ensure ethically responsible adaptations. To address the limitations, we propose drawing on recommender systems (RSs), which specialize in modeling user preferences and providing personalized recommendations. To ensure the integration of RS techniques is well-grounded and seamless throughout the social robot pipeline, we (i) align the paradigms underlying social robots and RSs, (ii) identify key techniques that can enhance personalization in social robots, and (iii) design them as modular, plug-and-play components. This work not only establishes a framework for integrating RS techniques into social robots but also opens a pathway for deep collaboration between the RS and HRI communities, accelerating innovation in both fields."
    }
  ],
  "南京大学": [
    {
      "title": "GeoDiff3D: Self-Supervised 3D Scene Generation with Geometry-Constrained 2D Diffusion Guidance",
      "url": "https://arxiv.org/abs/2601.19785",
      "date": "2026-01-27",
      "authors_text": "Haozhi Zhu, Miaomiao Zhao, Dingyao Liu, Runze Tian, Yan Zhang",
      "is_highlight": false,
      "score": 73.0,
      "summary": "GeoDiff3D is a self-supervised framework for efficient, high-quality 3D scene generation using geometry-constrained 2D diffusion, reducing reliance on labeled data and enhancing structural coherence.",
      "debug_abstract": "3D scene generation is a core technology for gaming, film/VFX, and VR/AR. Growing demand for rapid iteration, high-fidelity detail, and accessible content creation has further increased interest in this area. Existing methods broadly follow two paradigms - indirect 2D-to-3D reconstruction and direct 3D generation - but both are limited by weak structural modeling and heavy reliance on large-scale ground-truth supervision, often producing structural artifacts, geometric inconsistencies, and degraded high-frequency details in complex scenes. We propose GeoDiff3D, an efficient self-supervised framework that uses coarse geometry as a structural anchor and a geometry-constrained 2D diffusion model to provide texture-rich reference images. Importantly, GeoDiff3D does not require strict multi-view consistency of the diffusion-generated references and remains robust to the resulting noisy, inconsistent guidance. We further introduce voxel-aligned 3D feature aggregation and dual self-supervision to maintain scene coherence and fine details while substantially reducing dependence on labeled data. GeoDiff3D also trains with low computational cost and enables fast, high-quality 3D scene generation. Extensive experiments on challenging scenes show improved generalization and generation quality over existing baselines, offering a practical solution for accessible and efficient 3D scene construction."
    }
  ],
  "Robot Perception and Learning Lab": [
    {
      "title": "Unsupervised Learning of Efficient Exploration: Pre-training Adaptive Policies via Self-Imposed Goals",
      "url": "https://arxiv.org/abs/2601.19810",
      "date": "2026-01-27",
      "authors_text": "Octavio Pappalardo",
      "is_highlight": false,
      "score": 81.0,
      "summary": "The paper presents ULEE, an unsupervised meta-learning method that enhances exploration and adaptation in reinforcement learning by enabling agents to set and pursue self-imposed goals.",
      "debug_abstract": "Unsupervised pre-training can equip reinforcement learning agents with prior knowledge and accelerate learning in downstream tasks. A promising direction, grounded in human development, investigates agents that learn by setting and pursuing their own goals. The core challenge lies in how to effectively generate, select, and learn from such goals. Our focus is on broad distributions of downstream tasks where solving every task zero-shot is infeasible. Such settings naturally arise when the target tasks lie outside of the pre-training distribution or when their identities are unknown to the agent. In this work, we (i) optimize for efficient multi-episode exploration and adaptation within a meta-learning framework, and (ii) guide the training curriculum with evolving estimates of the agent&#39;s post-adaptation performance. We present ULEE, an unsupervised meta-learning method that combines an in-context learner with an adversarial goal-generation strategy that maintains training at the frontier of the agent&#39;s capabilities. On XLand-MiniGrid benchmarks, ULEE pre-training yields improved exploration and adaptation abilities that generalize to novel objectives, environment dynamics, and map structures. The resulting policy attains improved zero-shot and few-shot performance, and provides a strong initialization for longer fine-tuning processes. It outperforms learning from scratch, DIAYN pre-training, and alternative curricula."
    }
  ],
  "机器人技术与系统国家重点实验室": [
    {
      "title": "RoamScene3D: Immersive Text-to-3D Scene Generation via Adaptive Object-aware Roaming",
      "url": "https://arxiv.org/abs/2601.19433",
      "date": "2026-01-27",
      "authors_text": "Jisheng Chu, Wenrui Li, Rui Zhao, Wangmeng Zuo, Shifeng Chen",
      "is_highlight": false,
      "score": 67.0,
      "summary": "RoamScene3D introduces a novel framework for immersive text-to-3D scene generation that enhances spatial awareness and object relations, outperforming existing methods in realism and consistency.",
      "debug_abstract": "Generating immersive 3D scenes from texts is a core task in computer vision, crucial for applications in virtual reality and game development. Despite the promise of leveraging 2D diffusion priors, existing methods suffer from spatial blindness and rely on predefined trajectories that fail to exploit the inner relationships among salient objects. Consequently, these approaches are unable to comprehend the semantic layout, preventing them from exploring the scene adaptively to infer occluded content. Moreover, current inpainting models operate in 2D image space, struggling to plausibly fill holes caused by camera motion. To address these limitations, we propose RoamScene3D, a novel framework that bridges the gap between semantic guidance and spatial generation. Our method reasons about the semantic relations among objects and produces consistent and photorealistic scenes. Specifically, we employ a vision-language model (VLM) to construct a scene graph that encodes object relations, guiding the camera to perceive salient object boundaries and plan an adaptive roaming trajectory. Furthermore, to mitigate the limitations of static 2D priors, we introduce a Motion-Injected Inpainting model that is fine-tuned on a synthetic panoramic dataset integrating authentic camera trajectories, making it adaptive to camera motion. Extensive experiments demonstrate that with semantic reasoning and geometric constraints, our method significantly outperforms state-of-the-art approaches in producing consistent and photorealistic scenes. Our code is available at this https URL."
    }
  ],
  "多智能体与具身智能研究所": [
    {
      "title": "RoamScene3D: Immersive Text-to-3D Scene Generation via Adaptive Object-aware Roaming",
      "url": "https://arxiv.org/abs/2601.19433",
      "date": "2026-01-27",
      "authors_text": "Jisheng Chu, Wenrui Li, Rui Zhao, Wangmeng Zuo, Shifeng Chen",
      "is_highlight": false,
      "score": 67.0,
      "summary": "RoamScene3D introduces a novel framework for immersive text-to-3D scene generation that enhances spatial awareness and object relations, outperforming existing methods in realism and consistency.",
      "debug_abstract": "Generating immersive 3D scenes from texts is a core task in computer vision, crucial for applications in virtual reality and game development. Despite the promise of leveraging 2D diffusion priors, existing methods suffer from spatial blindness and rely on predefined trajectories that fail to exploit the inner relationships among salient objects. Consequently, these approaches are unable to comprehend the semantic layout, preventing them from exploring the scene adaptively to infer occluded content. Moreover, current inpainting models operate in 2D image space, struggling to plausibly fill holes caused by camera motion. To address these limitations, we propose RoamScene3D, a novel framework that bridges the gap between semantic guidance and spatial generation. Our method reasons about the semantic relations among objects and produces consistent and photorealistic scenes. Specifically, we employ a vision-language model (VLM) to construct a scene graph that encodes object relations, guiding the camera to perceive salient object boundaries and plan an adaptive roaming trajectory. Furthermore, to mitigate the limitations of static 2D priors, we introduce a Motion-Injected Inpainting model that is fine-tuned on a synthetic panoramic dataset integrating authentic camera trajectories, making it adaptive to camera motion. Extensive experiments demonstrate that with semantic reasoning and geometric constraints, our method significantly outperforms state-of-the-art approaches in producing consistent and photorealistic scenes. Our code is available at this https URL."
    }
  ],
  "Quest for Intelligence": [
    {
      "title": "VGGT-SLAM 2.0: Real time Dense Feed-forward Scene Reconstruction",
      "url": "https://arxiv.org/abs/2601.19887",
      "date": "2026-01-27",
      "authors_text": "Dominic Maggio, Luca Carlone",
      "is_highlight": false,
      "score": 92.0,
      "summary": "VGGT-SLAM 2.0 enhances real-time RGB SLAM by improving submap alignment, reducing drift, and enabling effective image retrieval verification, achieving superior accuracy in diverse environments.",
      "debug_abstract": "We present VGGT-SLAM 2.0, a real time RGB feed-forward SLAM system which substantially improves upon VGGT-SLAM for incrementally aligning submaps created from VGGT. Firstly, we remove high-dimensional 15-degree-of-freedom drift and planar degeneracy from VGGT-SLAM by creating a new factor graph design while still addressing the reconstruction ambiguity of VGGT given unknown camera intrinsics. Secondly, by studying the attention layers of VGGT, we show that one of the layers is well suited to assist in image retrieval verification for free without additional training, which enables both rejecting false positive matches and allows for completing more loop closures. Finally, we conduct a suite of experiments which includes showing VGGT-SLAM 2.0 can easily be adapted for open-set object detection and demonstrating real time performance while running online onboard a ground robot using a Jetson Thor. We also test in environments ranging from cluttered indoor apartments and office scenes to a 4,200 square foot barn, and we also demonstrate VGGT-SLAM 2.0 achieves the highest accuracy on the TUM dataset with about 23 percent less pose error than VGGT-SLAM. Code will be released upon publication."
    }
  ],
  "CSAIL Embodied Intelligence Labs": [
    {
      "title": "VGGT-SLAM 2.0: Real time Dense Feed-forward Scene Reconstruction",
      "url": "https://arxiv.org/abs/2601.19887",
      "date": "2026-01-27",
      "authors_text": "Dominic Maggio, Luca Carlone",
      "is_highlight": false,
      "score": 92.0,
      "summary": "VGGT-SLAM 2.0 enhances real-time RGB SLAM by improving submap alignment, reducing drift, and enabling effective image retrieval verification, achieving superior accuracy in diverse environments.",
      "debug_abstract": "We present VGGT-SLAM 2.0, a real time RGB feed-forward SLAM system which substantially improves upon VGGT-SLAM for incrementally aligning submaps created from VGGT. Firstly, we remove high-dimensional 15-degree-of-freedom drift and planar degeneracy from VGGT-SLAM by creating a new factor graph design while still addressing the reconstruction ambiguity of VGGT given unknown camera intrinsics. Secondly, by studying the attention layers of VGGT, we show that one of the layers is well suited to assist in image retrieval verification for free without additional training, which enables both rejecting false positive matches and allows for completing more loop closures. Finally, we conduct a suite of experiments which includes showing VGGT-SLAM 2.0 can easily be adapted for open-set object detection and demonstrating real time performance while running online onboard a ground robot using a Jetson Thor. We also test in environments ranging from cluttered indoor apartments and office scenes to a 4,200 square foot barn, and we also demonstrate VGGT-SLAM 2.0 achieves the highest accuracy on the TUM dataset with about 23 percent less pose error than VGGT-SLAM. Code will be released upon publication."
    }
  ]
}